{"title": [{"text": "Leveraging Small Multilingual Corpora for SMT Using Many Pivot Languages", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9891285300254822}]}], "abstractContent": [{"text": "We present our work on leveraging multilingual parallel corpora of small sizes for Statistical Machine Translation between Japanese and Hindi using multiple pivot languages.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 83, "end_pos": 114, "type": "TASK", "confidence": 0.8780966599782308}]}, {"text": "In our setting, the source and target part of the corpus remains the same, but we show that using several different pivot to extract phrase pairs from these source and target parts lead to large BLEU improvements.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 195, "end_pos": 199, "type": "METRIC", "confidence": 0.9972075819969177}]}, {"text": "We focus on a variety of ways to exploit phrase tables generated using multiple pivots to support a direct source-target phrase table.", "labels": [], "entities": []}, {"text": "Our main method uses the Multiple Decoding Paths (MDP) feature of Moses, which we empirically verify as the best compared to the other methods we used.", "labels": [], "entities": []}, {"text": "We compare and contrast our various results to show that one can overcome the limitations of small corpora by using as many pivot languages as possible in a multilingual setting.", "labels": [], "entities": []}, {"text": "Most importantly, we show that such pivoting aids in learning of additional phrase pairs which are not learned when the direct source-target corpus is small.", "labels": [], "entities": []}, {"text": "We obtained improvements of up to 3 BLEU points using multiple pivots for Japanese to Hindi translation compared to when only one pivot is used.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9996194839477539}, {"text": "Japanese to Hindi translation", "start_pos": 74, "end_pos": 103, "type": "TASK", "confidence": 0.5504658669233322}]}, {"text": "To the best of our knowledge, this work is also the first of its kind to attempt the simultaneous utilization of 7 pivot languages at decoding time.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the increasing size of parallel corpora it has become possible to achieve very high quality translation.", "labels": [], "entities": []}, {"text": "However, not all language pairs are blessed with the availability of large parallel corpora in the sizes of millions of lines.", "labels": [], "entities": []}, {"text": "With the exception of the major European languages and a few Asian languages like Chinese and Japanese, other languages have parallel corpora in the sizes of a few thousands of lines.", "labels": [], "entities": []}, {"text": "Since translation quality is related to the size of the parallel corpus, it is impossible to achieve the same level of translation quality as that in the case of resource rich languages.", "labels": [], "entities": []}, {"text": "To remedy this scenario, an intermediate resource rich language can be exploited.", "labels": [], "entities": []}, {"text": "Although, finding a direct parallel corpus between source and target languages might be difficult, there are higher odds of finding a pair of parallel corpora: one between the source language and an intermediate resource rich language (henceforth called pivot 1 ) and one between that pivot and the target language.", "labels": [], "entities": []}, {"text": "Using the methods developed for Pivot Based SMT ( one can use the source-pivot and pivot-target parallel corpora to develop a source-target translation system (henceforth called as pivot based system 2 ) . Moreover, if there exists a small source-target parallel corpus then the resulting system (henceforth called as direct system 3 ) can be supported by the pivot based source-target system to significantly improve the translation quality.", "labels": [], "entities": [{"text": "Pivot Based SMT", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.8084977269172668}]}, {"text": "Note that in this paper we use the terms \"translation system\" and \"phrase table\" interchangeably since the phrase table is the main component of the translation system.", "labels": [], "entities": []}, {"text": "Reordering tables are supplementary and can usually be replaced by a simple distortion model.", "labels": [], "entities": []}, {"text": "Major problems arise when source-pivot and pivot-target corpora belong to different domains leading to rather poor quality translations.", "labels": [], "entities": []}, {"text": "Even if the individual corpora are large, one will run into domain adaptation problems.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7141980081796646}]}, {"text": "In such a scenario the availability of a small size multilingual corpus of a few thousand lines belonging to a single domain can be beneficial.", "labels": [], "entities": []}, {"text": "The setting of this paper is: 1.", "labels": [], "entities": []}, {"text": "We suppose the existence of a multilingual corpus with sentences aligned across N 4 different languages.", "labels": [], "entities": []}, {"text": "2. We show using the other languages as additional pivots leads to the construction of better phrase tables and better translation results.", "labels": [], "entities": []}, {"text": "Note that this setting is realistic and differs from the majority of existing work on pivot languages, in which the source-pivot and pivot-target corpora are unrelated (or at least do not have equivalent sentences).", "labels": [], "entities": []}, {"text": "In addition to the well-known Europarl corpus, many other similar multilingual corpora exist.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.9887455105781555}]}, {"text": "For example, a multilingual parallel corpus for 9 major Indian Languages belonging to the Health and Tourism domain of approximately 50000 lines was used to develop basic SMT systems ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 171, "end_pos": 174, "type": "TASK", "confidence": 0.995762288570404}]}, {"text": "For our experiments we will use a recently released Bible domain multilingual parallel corpus) fora large number (over 25) of languages (other than Indian) including Japanese and Hindi (Japanese to Hindi translation being our focus) of approximately 30000 lines.", "labels": [], "entities": [{"text": "Bible domain multilingual parallel corpus", "start_pos": 52, "end_pos": 93, "type": "DATASET", "confidence": 0.6717624306678772}]}, {"text": "We chose this setting because we feel that this multilingual approach is especially important for lowresource language pairs.", "labels": [], "entities": []}, {"text": "Typically system combination methods like linear interpolation are used to combine the direct and pivot phrase tables by modifying the probabilities of phrase pairs leading to the modification of the underlying distribution which affects the resultant translation quality.", "labels": [], "entities": []}, {"text": "The Multiple Decoding Paths (Birch and Osborne, 2007) (MDP) feature has been used to combine two source-target phrase tables of different domains for domain adaptation but not so extensively in a pivot language scenario, especially when multiple pivots are involved (7 in our case).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.7353441715240479}]}, {"text": "Our work is different from other previous works in the following ways: \u2022 We work on a realistic low resource setting for translation between Japanese and Hindi in which we use small sized multilingual corpora containing translations of a sentence in multiple languages.", "labels": [], "entities": [{"text": "translation between Japanese and Hindi", "start_pos": 121, "end_pos": 159, "type": "TASK", "confidence": 0.8629528999328613}]}, {"text": "\u2022 We focus on the impact of using a relatively large number of pivot languages (7 to be precise) to improve the translation quality and compare this to when only one pivot language is used.", "labels": [], "entities": []}, {"text": "\u2022 Most works focus on obtaining pivot based phrase tables on relatively larger corpora than the ones used for the direct phrase table.", "labels": [], "entities": []}, {"text": "We use the same corpora sizes for the pivot as well as direct tables.", "labels": [], "entities": []}, {"text": "\u2022 We verify that Multiple Decoding Paths (MDP) feature of Moses is much more effective than plain linear interpolation, especially when more pivot languages are used together.", "labels": [], "entities": []}, {"text": "\u2022 We show that simply varying the pivot language leads to additional phrase pairs being acquired that impact translation quality.", "labels": [], "entities": []}, {"text": "Section 2 contains the related work.", "labels": [], "entities": []}, {"text": "Section 3 begins with a basic description about the languages involved, followed by the corpora details and the experimental methodology.", "labels": [], "entities": []}, {"text": "Section 4 consists of results, observations and discussions.", "labels": [], "entities": []}, {"text": "The paper ends with conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first describe the pivot languages and the corpora we use.", "labels": [], "entities": []}, {"text": "We follow this with a description of the triangulation method which we use to construct phrase tables using the pivot languages, the methods used to combine the constructed tables and then the experiments that use them.", "labels": [], "entities": []}, {"text": "Our experiments were centered around Phrase Based SMT (PBSMT).", "labels": [], "entities": [{"text": "Phrase Based SMT (PBSMT)", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.706373949845632}]}, {"text": "We used the open source Moses decoder () package (including Giza++) for word alignment, phrase table extraction and decoding for sentence translation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.8014117777347565}, {"text": "phrase table extraction", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.706620971361796}, {"text": "sentence translation", "start_pos": 129, "end_pos": 149, "type": "TASK", "confidence": 0.7361151725053787}]}, {"text": "We also used the Moses scripts for linear and fillup interpolation along with the multiple decoding paths (MDP) setting (by modifying the moses.ini files).", "labels": [], "entities": []}, {"text": "We performed MERT based tuning using the MIRA algorithm.", "labels": [], "entities": [{"text": "MERT based tuning", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.6308751503626505}, {"text": "MIRA", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.7804588079452515}]}, {"text": "We used BLEU () as our evaluation criteria and the bootstrapping method) for significance testing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9987082481384277}, {"text": "significance testing", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.9363820850849152}]}, {"text": "For the sake of comparison with previous methods, we experimented with sentence translation strategy () using 10 as the n-best list size for intermediate and target language translations.", "labels": [], "entities": [{"text": "sentence translation", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.7869009375572205}]}, {"text": "The experiments we performed are given below.", "labels": [], "entities": []}, {"text": "Each experiment involves either the creation of a phrase tables or combination of phrase tables.", "labels": [], "entities": []}, {"text": "We tune, test and evaluate these tables or combinations.", "labels": [], "entities": []}, {"text": "1. A src (source) to tgt (target) direct phrase table.", "labels": [], "entities": []}, {"text": "2. For piv in Pivot Languages Set; the set of pivot languages to be used 3.", "labels": [], "entities": []}, {"text": "Combine all the src-piv-tgt tables into a single table using linear (weights are ratios of BLEU scores) and fillup interpolation independently, giving the phrase tables: linear interp all and fill interp all respectively., rows 4 and 5. 4. Perform linear interpolation of the src-tgt and linear interp all tables using 9:1 weight ratio in equation 5 to get a combined table., row 6. 5. Perform linear interpolation of the src-tgt and all src-piv-tgt phrase tables using the ratio of their BLEU scores as weights in equation 5 to get a combined table., row 7. 6. Perform fillup interpolation of the src-tgt and all src-piv-tgt phrase tables.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9908906817436218}, {"text": "BLEU", "start_pos": 489, "end_pos": 493, "type": "METRIC", "confidence": 0.9919753670692444}]}, {"text": "The priority of the tables is given by the descending order of BLEU scores.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9586824178695679}]}, {"text": "for direct and 1 for each of the 7 pivots)., row 11. 9. Combine the top 3 pivot phrase tables with the src-piv-tgt phrase tables with the src-tgt phrase table using MDP (4 paths, 1 for direct and 1 for each of the 3 pivots).", "labels": [], "entities": []}, {"text": "The pivot tables with the 3 highest 8 standalone BLEU scores are selected., row 12.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9829793572425842}]}], "tableCaptions": [{"text": " Table 1: Japanese-Hindi Results Using Single Pivots", "labels": [], "entities": []}, {"text": " Table 2: Hindi-Japanese Results Using Single Pivots", "labels": [], "entities": []}, {"text": " Table 3: Results Using Multiple Pivots With Different Combination Methods", "labels": [], "entities": []}, {"text": " Table 4: Unique phrase pairs in each table (in millions of pairs)", "labels": [], "entities": []}, {"text": " Table 5: Number of improved translations (out of 500) using sentence level BLEU difference at various cutoffs", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.978239119052887}]}]}