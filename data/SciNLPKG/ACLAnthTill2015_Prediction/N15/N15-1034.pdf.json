{"title": [{"text": "Sign constraints on feature weights improve a joint model of word segmentation and phonology", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7227605581283569}]}], "abstractContent": [{"text": "This paper describes a joint model of word segmentation and phonological alternations, which takes unsegmented utterances as input and infers word segmentations and underlying phonological representations.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7104085385799408}]}, {"text": "The model is a Maximum Entropy or log-linear model, which can express a probabilistic version of Opti-mality Theory (OT; Prince and Smolensky (2004)), a standard phonological framework.", "labels": [], "entities": []}, {"text": "The features in our model are inspired by OT's Markedness and Faithfulness constraints.", "labels": [], "entities": []}, {"text": "Following the OT principle that such features indicate \"violations\", we require their weights to be non-positive.", "labels": [], "entities": []}, {"text": "We apply our model to a modified version of the Buckeye corpus (Pitt et al., 2007) in which the only phonological alternations are deletions of word-final /d/ and /t/ segments.", "labels": [], "entities": [{"text": "Buckeye corpus", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.9320096969604492}]}, {"text": "The model sets anew state-of-the-art for this corpus for word segmentation, identification of underlying forms, and identification of /d/ and /t/ deletions.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7586952745914459}, {"text": "identification of underlying forms", "start_pos": 76, "end_pos": 110, "type": "TASK", "confidence": 0.8602737933397293}, {"text": "identification of /d/ and /t/ deletions", "start_pos": 116, "end_pos": 155, "type": "TASK", "confidence": 0.7284802675247193}]}, {"text": "We also show that the OT-inspired sign constraints on feature weights are crucial for accurate identification of deleted /d/s; without them our model posits approximately 10 times more deleted underlying /d/s than appear in the manually annotated data.", "labels": [], "entities": [{"text": "identification of deleted /d/s", "start_pos": 95, "end_pos": 125, "type": "TASK", "confidence": 0.795212847845895}]}], "introductionContent": [{"text": "This paper unifies two different strands of research on word segmentation and phonological rule induction.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7542496919631958}, {"text": "phonological rule induction", "start_pos": 78, "end_pos": 105, "type": "TASK", "confidence": 0.6732060015201569}]}, {"text": "The word segmentation task is the task of segmenting utterances represented as sequences of phones into sequences of words.", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8062423865000407}]}, {"text": "This is an idealisation of the lexicon induction problem, since the resulting words are phonological forms for lexical entries.", "labels": [], "entities": []}, {"text": "In its simplest form, the data fora word segmentation task is obtained by looking up the words of an orthographic transcript (of, say, child-directed speech) in a pronouncing dictionary and concatenating the results.", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.8157228032747904}]}, {"text": "However, this formulation significantly oversimplifies the problem because it assumes that each token of a word type is pronounced identically in the form specified by the pronouncing dictionary (usually its citation form).", "labels": [], "entities": []}, {"text": "In reality there is usually a significant amount of pronunciation variation from token to token.", "labels": [], "entities": []}, {"text": "The Buckeye corpus, on which we base our experiments here, contains manually-annotated surface phonetic representations of each word as well as the corresponding underlying form.", "labels": [], "entities": [{"text": "Buckeye corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9536213576793671}]}, {"text": "For example, a token of the word \"lived\" has the underlying form /l.ih.v.d/ and could have the surface form [l.ah.v] (we follow standard phonological convention by writing underlying forms with slashes and surface forms with square brackets, and use the Buckeye transcription format).", "labels": [], "entities": [{"text": "Buckeye transcription format", "start_pos": 254, "end_pos": 282, "type": "DATASET", "confidence": 0.8734514514605204}]}, {"text": "There is a large body of work in the phonological literature on inferring phonological rules mapping underlying forms to their surface realisations.", "labels": [], "entities": []}, {"text": "While most of this work assumes that the underlying forms are available to the inference procedure, there is work that induces underlying forms as well as the phonological processes that map them to sur-face forms.", "labels": [], "entities": []}, {"text": "We present a model that takes a corpus of unsegmented surface representations of sentences and infers a word segmentation and underlying forms for each hypothesised word.", "labels": [], "entities": []}, {"text": "We test this model on data derived from the Buckeye corpus where the only phonological variation consists of word-final /d/ and /t/ deletions, and show that it outperforms a state-ofthe-art model that only handles word-final /t/ deletions.", "labels": [], "entities": [{"text": "Buckeye corpus", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.9599161744117737}]}, {"text": "Our model is a MaxEnt or log-linear model, which means that it is formally equivalent to a Harmonic Grammar, which is a continuous version of Optimality Theory (OT)).", "labels": [], "entities": []}, {"text": "We use features inspired by OT, and show that sign constraints on feature weights result in models that recover underlying /d/s significantly more accurately than models that don't include such contraints.", "labels": [], "entities": []}, {"text": "We present results suggesting that these constraints simplify the search problem that the learner faces.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "The next section describes related work, including previous work that this paper builds on.", "labels": [], "entities": []}, {"text": "Section 3 describes our model, while section 4 explains how we prepared the data, presents our experimental results and investigates the effects of design choices on model performance.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper and discusses possible future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the experiments we performed to evaluate the model just described.", "labels": [], "entities": []}, {"text": "We first describe how we prepared the data on which the model is trained and evaluated, and then we describe the performance of that model.", "labels": [], "entities": []}, {"text": "Finally we perform an analysis of how the model's performance varies as parameters of the model are changed.", "labels": [], "entities": []}, {"text": "We ran this model on data extracted from the Buckeye corpus of conversational speech which was modified so the only alternations it contained are final /d/ and /t/ deletions.", "labels": [], "entities": [{"text": "Buckeye corpus of conversational speech", "start_pos": 45, "end_pos": 84, "type": "DATASET", "confidence": 0.9542737364768982}]}, {"text": "The Buckeye corpus gives a surface realisation and an underlying form for each word token, and following, we prepared the data as follows.", "labels": [], "entities": [{"text": "Buckeye corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9762993454933167}]}, {"text": "We used the Buckeye underlying forms as our underlying forms.", "labels": [], "entities": [{"text": "Buckeye underlying forms", "start_pos": 12, "end_pos": 36, "type": "DATASET", "confidence": 0.9504153927167257}]}, {"text": "Our surface forms were also identical to the Buckeye underlying forms, except when the underlying form ends in either a /d/ or a /t/.", "labels": [], "entities": [{"text": "Buckeye underlying forms", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.8980080684026083}]}, {"text": "In this case, if the Buckeye surface form does not end in an allophonic variant of that segment, then our surface form consists of the Buckeye underlying form with that final segment deleted.", "labels": [], "entities": [{"text": "Buckeye surface form", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.8440981308619181}]}, {"text": "Thus the only phonological variation in our data are deletions of word-final /d/ and /t/ appearing in the Buckeye corpus, otherwise our surface forms are identical to Buckeye underlying forms.", "labels": [], "entities": [{"text": "Buckeye corpus", "start_pos": 106, "end_pos": 120, "type": "DATASET", "confidence": 0.9744365811347961}]}, {"text": "Our model considers all possible substrings of length 15 or less as a possible surface form of a word, yielding 4,803,734 possible word types and 5,292,040 possible surface/underlying word type pairs.", "labels": [], "entities": []}, {"text": "Taking the 3 contexts derived from the following word into account, there are 4,969,718 possible word+context types.", "labels": [], "entities": []}, {"text": "When all possible surface/underlying pairs are considered in all possible contexts there are 15,876,120 possible surface/underlying/context triples.", "labels": [], "entities": []}, {"text": "summarises the major experimental results for this model, and compares them to the results of.", "labels": [], "entities": []}, {"text": "Note that their model only recovers word-final /t/ deletions and was run on data without word-final /d/ deletions, so it is solving a simpler problem than the one studied here.", "labels": [], "entities": []}, {"text": "Even so, our model achieves higher overall accuracies.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9850469827651978}]}, {"text": "We also conducted experiments on several of the design choices in our model.", "labels": [], "entities": []}, {"text": "shows the effect of the sign constraints on feature weights discussed above.", "labels": [], "entities": []}, {"text": "This plot shows that the contraints on the weights of markedness and faithfulness features seems essential for good word segmentation performance.", "labels": [], "entities": [{"text": "word segmentation performance", "start_pos": 116, "end_pos": 145, "type": "TASK", "confidence": 0.8054743508497874}]}, {"text": "Interestingly, we found that the weight constraints make very little difference if the data does: Results summary for our model compared to that of the model.", "labels": [], "entities": []}, {"text": "Surface token fscore is the standard token f-score, while underlying type or \"lexicon\" f-score measures the accuracy with which the underlying word types are recovered.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.998924195766449}]}, {"text": "Deleted /t/ and /d/ f-scores measure the accuracy with which the model recovers segments that don't appear in the surface.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9995742440223694}]}, {"text": "These results are averaged over 40 runs (standard deviations in parentheses) with the word length penalty d = 1.525 applied to underlying forms; standard deviations are given in parentheses.", "labels": [], "entities": [{"text": "word length penalty d", "start_pos": 86, "end_pos": 107, "type": "METRIC", "confidence": 0.8660990595817566}]}, {"text": "not any /t/ or /d/ deletions (i.e., the case that Berg-.", "labels": [], "entities": []}, {"text": "Investigating this further, we found that the weight constraints on the markedness and faithfulness features has a dramatic effect on the recovery of underlying segments, particularly underlying /d/s. shows that with these constraints the model recovers approximately the correct number of deleted underlying segments, while without this constraint the model posits far too many underlying /d/s. shows that these constraints help the model find higher regularised likelihood sets of feature weights with fewer non-zero feature weights.", "labels": [], "entities": []}, {"text": "We examined how the number of non-zero feature weights (most of which are for underlying type features) relate to the number of underlying types posited by the model.", "labels": [], "entities": []}, {"text": "shows that the weight constraints on markedness and faithfulness constraints have great impact on the number of nonzero feature weights and on the number of underlying forms the model posits.", "labels": [], "entities": []}, {"text": "In all cases, the model recovers far more underlying forms than it finds nonzero weights.", "labels": [], "entities": []}, {"text": "The lexicon weight constraints have much less impact than the OT weight constraints.", "labels": [], "entities": []}, {"text": "As shows, without the OT weight constraints the models posit too many deleted /d/ and essentially no deleted /t/.", "labels": [], "entities": []}, {"text": "shows that OT weight constraints enable the model to find higher likelihood solutions, i.e., the OT weight constraints help search.", "labels": [], "entities": []}, {"text": "Inspired by a reviewer's comments, we studied typetoken ratios and the number of boundaries our models posit.", "labels": [], "entities": [{"text": "typetoken ratios", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.6842589974403381}]}, {"text": "We found that the models without OT weight constraints posit far too few word boundaries compared to the gold data, so the number of surface tokens is too low, so the words are too long, and the number of underlying types is too high.", "labels": [], "entities": []}, {"text": "This is consistent with Figures 4-5.", "labels": [], "entities": []}, {"text": "We also examined whether it is necessary to consider all surface/underlying pairs X in each context C, or whether it is possible to restrict attention to the much smaller sets X c that occur in each c \u2208 C (this dramatically reduces the amount of memory required and speeds the computation).", "labels": [], "entities": []}, {"text": "shows that working with the smaller, context-specific sets dramatically decreases the model's ability to recover deleted segments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results summary for our model compared to that  of the", "labels": [], "entities": []}]}