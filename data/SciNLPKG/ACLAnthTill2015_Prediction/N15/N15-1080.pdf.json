{"title": [], "abstractContent": [{"text": "We present anew algorithm for transforming dependency parse trees into phrase-structure parse trees.", "labels": [], "entities": [{"text": "dependency parse trees into phrase-structure parse trees", "start_pos": 43, "end_pos": 99, "type": "TASK", "confidence": 0.7199023451123919}]}, {"text": "We cast the problem as struc-tured prediction and learn a statistical model.", "labels": [], "entities": [{"text": "struc-tured prediction", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.6988596618175507}]}, {"text": "Our algorithm is faster than traditional phrase-structure parsing and achieves 90.4% English parsing accuracy and 82.4% Chinese parsing accuracy, near to the state of the art on both benchmarks.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.7649832963943481}, {"text": "English parsing", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.6266293674707413}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9290928244590759}, {"text": "Chinese parsing", "start_pos": 120, "end_pos": 135, "type": "TASK", "confidence": 0.4997027665376663}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9498703479766846}]}], "introductionContent": [{"text": "Natural language parsers typically produce phrasestructure (constituent) trees or dependency trees.", "labels": [], "entities": []}, {"text": "These representations capture some of the same syntactic phenomena, and the two can be produced jointly ().", "labels": [], "entities": []}, {"text": "Yet it appears to be completely unpredictable which will be preferred by a particular subcommunity or used in a particular application.", "labels": [], "entities": []}, {"text": "Both continue to receive the attention of parsing researchers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9784573912620544}]}, {"text": "Further, it appears to be a historical accident that phrase-structure syntax was used in annotating the Penn Treebank, and that English dependency annotations are largely derived through mechanical, rule-based transformations (reviewed in Section 2).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 104, "end_pos": 117, "type": "DATASET", "confidence": 0.9899324476718903}]}, {"text": "Indeed, despite extensive work on directto-dependency parsing algorithms (which we call dparsing), the most accurate dependency parsers for English still involve phrase-structure parsing (which we call c-parsing) followed by rule-based extraction of dependencies (.", "labels": [], "entities": [{"text": "directto-dependency parsing", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7392068803310394}, {"text": "phrase-structure parsing", "start_pos": 162, "end_pos": 186, "type": "TASK", "confidence": 0.7400657087564468}]}, {"text": "What if dependency annotations had come first?", "labels": [], "entities": []}, {"text": "Because d-parsers are generally much faster than c-parsers, we consider an alternate pipeline (Section 3): d-parse first, then transform the dependency representation into a phrase-structure tree constrained to be consistent with the dependency parse.", "labels": [], "entities": []}, {"text": "This idea was explored by and using hand-written rules.", "labels": [], "entities": []}, {"text": "Instead, we present a data-driven algorithm using the structured prediction framework (Section 4).", "labels": [], "entities": []}, {"text": "The approach can be understood as a specially-trained coarse-to-fine decoding algorithm where a d-parser provides \"coarse\" structure and the second stage refines it.", "labels": [], "entities": []}, {"text": "Our lexicalized phrase-structure parser, PAD, is asymptotically faster than parsing with a lexicalized context-free grammar: O(n 2 ) plus d-parsing, vs. O(n 5 ) worst case runtime in sentence length n, with the same grammar constant.", "labels": [], "entities": [{"text": "O", "start_pos": 125, "end_pos": 126, "type": "METRIC", "confidence": 0.9401406645774841}]}, {"text": "Experiments show that our approach achieves linear observable runtime, and accuracy similar to state-of-the-art phrase-structure parsers without reranking or semisupervised training (Section 7).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9993569254875183}]}], "datasetContent": [{"text": "We ran experiments to assess the accuracy of the method, its runtime efficiency, the effect of dependency parsing accuracy, and the effect of the amount of annotated phrase-structure data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9988983869552612}, {"text": "dependency parsing", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.6694705188274384}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.5927538275718689}]}, {"text": "compares the accuracy and speed of the phrase-structure trees produced by the parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9993093013763428}]}, {"text": "For these experiments we treat our system and the Zhang-Nivre parser as an independently trained, but complete end-to-end c-parser.", "labels": [], "entities": []}, {"text": "Runtime for these experiments includes both the time for dparsing and conversion.", "labels": [], "entities": []}, {"text": "Despite the fixed depen-  dency constraints, the English results show that the parser is comparable inaccuracy to many widelyused systems, and is significantly faster.", "labels": [], "entities": []}, {"text": "The parser most competitive in both speed and accuracy is that of, a fast shift-reduce phrasestructure parser.", "labels": [], "entities": [{"text": "speed", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.993459939956665}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9953887462615967}]}, {"text": "Furthermore, the Chinese results suggest that, even without making language-specific changes in the feature system we can still achieve competitive parsing accuracy.", "labels": [], "entities": [{"text": "competitive parsing", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.5453621745109558}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.8421180248260498}]}, {"text": "shows experiments comparing the effect of different input dparses.", "labels": [], "entities": []}, {"text": "For these experiments we used the same version of PAD with 11 different d-parsers of varying quality and speed.", "labels": [], "entities": []}, {"text": "We measure for each parser: its UAS, speed, and labeled F 1 when used with PAD and with an oracle converter.", "labels": [], "entities": [{"text": "UAS", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9965116381645203}, {"text": "speed", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9892654418945312}, {"text": "labeled F 1", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.8186653653780619}]}, {"text": "The paired figure shows that there is a direct correlation between the UAS of the inputs and labeled F 1 .", "labels": [], "entities": [{"text": "UAS", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9153831601142883}]}], "tableCaptions": [{"text": " Table 1: Comparison of three parsing setups: LEX CKY  *   is the complete lexicalized c-parser on Y(x), but limited to  only sentences less than 20 words for tractability, DEP CKY  is the constrained c-parser on Y(x, d), PRUNE1, PRUNE2, and  PRUNE1+2 are combinations of the pruning methods described  in Section 3.2. The oracle is the best labeled F1 achievable on  the development data ( \u00a722, see Section 7).", "labels": [], "entities": [{"text": "DEP", "start_pos": 173, "end_pos": 176, "type": "METRIC", "confidence": 0.9162098169326782}, {"text": "F1", "start_pos": 350, "end_pos": 352, "type": "METRIC", "confidence": 0.9845596551895142}]}, {"text": " Table 2: Comparison with the rule-based system of", "labels": [], "entities": []}, {"text": " Table 3: Accuracy and speed on PTB  \u00a723 and CTB 5.1 test  split. Comparisons are to state-of-the-art non-reranking super- vised phrase-structure parsers", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9982278943061829}, {"text": "speed", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9944207072257996}, {"text": "PTB  \u00a723", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9588072896003723}, {"text": "CTB 5.1 test  split", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.9303335249423981}]}, {"text": " Table 4: The effect of d-parsing accuracy (PTB  \u00a722) on PAD  and an oracle converter. Runtime includes d-parsing and c- parsing. Inputs include MaltParser (Nivre et al., 2006), the  RedShift and the Yara implementations of the parser of Zhang  and Nivre (2011) with various beam size, and three versions of  TurboParser trained with projective constraints (Martins et al.,  2013).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9773074388504028}, {"text": "PTB  \u00a722", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9407702684402466}]}, {"text": " Table 5: Error analysis of binary CFG rules. Rules used are split  into classes based on correct (+) identification of dependency  (h, m), span i, j, and split k. \"Count\" is the size of each  class. \"Acc.\" is the accuracy of span nonterminal identification.", "labels": [], "entities": [{"text": "Count", "start_pos": 165, "end_pos": 170, "type": "METRIC", "confidence": 0.9953992962837219}, {"text": "Acc.", "start_pos": 201, "end_pos": 205, "type": "METRIC", "confidence": 0.9988635778427124}, {"text": "accuracy", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.9995530247688293}, {"text": "span nonterminal identification", "start_pos": 226, "end_pos": 257, "type": "TASK", "confidence": 0.7033957839012146}]}]}