{"title": [{"text": "Ontologically Grounded Multi-sense Representation Learning for Semantic Vector Space Models", "labels": [], "entities": []}], "abstractContent": [{"text": "However, most approaches to representation learning for lexical semantics assign a single vector to every surface word type.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.9260676503181458}]}, {"text": "Meanwhile, lexical ontologies such as WordNet provide a source of complementary knowledge to distributional information , including a word sense inventory.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9595772624015808}]}, {"text": "In this paper we propose two novel and general approaches for generating sense-specific word embeddings that are grounded in an ontology.", "labels": [], "entities": []}, {"text": "The first applies graph smoothing as a post-processing step to tease the vectors of different senses apart, and is applicable to any vector space model.", "labels": [], "entities": []}, {"text": "The second adapts predictive maximum likelihood models that learn word embeddings with latent variables representing senses grounded in an specified ontology.", "labels": [], "entities": []}, {"text": "Empirical results on lexical semantic tasks show that our approaches effectively captures information from both the ontology and distribu-tional statistics.", "labels": [], "entities": []}, {"text": "Moreover, inmost cases our sense-specific models outperform other models we compare against.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector space models (VSMs) of word meaning play a central role in computational semantics.", "labels": [], "entities": [{"text": "Vector space models (VSMs) of word meaning", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.5443269312381744}]}, {"text": "These represent meanings of words as contextual feature vectors in a high-dimensional space) or some embedding thereof and are learned from unannotated corpora.", "labels": [], "entities": []}, {"text": "Word vectors in these continuous space representations can be used for meaningful semantic operations such as computing word similarity), performing analogical reasoning and discovering lexical relationships.", "labels": [], "entities": [{"text": "analogical reasoning", "start_pos": 149, "end_pos": 169, "type": "TASK", "confidence": 0.8417499959468842}]}, {"text": "They have also proved useful in downstream NLP applications such as information retrieval) and question answering (), among others.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.8358448445796967}, {"text": "question answering", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.9358861446380615}]}, {"text": "However, VSMs remain flawed because they assign a single vector to every word, thus ignoring the possibility that words may have more than one meaning.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.8974652886390686}]}, {"text": "For example, the word \"bank\" can either denote a financial institution or the shore of a river.", "labels": [], "entities": []}, {"text": "The ability to model multiple meanings is an important component of any NLP system, given how common polysemy is in language.", "labels": [], "entities": []}, {"text": "The lack of sense annotated corpora large enough to robustly train VSMs, and the absence of fast, high quality word sense disambiguation (WSD) systems makes handling polysemy difficult.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 111, "end_pos": 142, "type": "TASK", "confidence": 0.7144096046686172}]}, {"text": "Meanwhile, lexical ontologies, such as WordNet specifically catalog sense inventories and provide typologies that link these senses to one another.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9558952450752258}]}, {"text": "These hand-curated ontologies provide a complementary source of information to distributional statistics.", "labels": [], "entities": []}, {"text": "Recent research tries to leverage this information to train better VSMs (), but does not tackle the problem of polysemy.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 67, "end_pos": 71, "type": "TASK", "confidence": 0.8268676400184631}]}, {"text": "Parallely, work on polysemy for VSMs revolves primarily around techniques that cluster contexts to distinguish between different word senses (), but does not integrate ontologies in anyway.", "labels": [], "entities": []}, {"text": "In this paper we present two novel approaches to integrating ontological and distributional sources of information.", "labels": [], "entities": []}, {"text": "Our focus is on allowing already existing, proven techniques to be adapted to produce ontologically grounded word sense embeddings.", "labels": [], "entities": []}, {"text": "Our first technique is applicable to any sense-agnostic 683 VSM as a post-processing step that performs graph propagation on the structure of the ontology.", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7681273519992828}]}, {"text": "The second is applicable to the wide range of current techniques that learn word embeddings from predictive models that maximize the likelihood of a corpus.", "labels": [], "entities": []}, {"text": "Our technique adds a latent variable representing the word sense to each token in the corpus, and uses EM to find parameters.", "labels": [], "entities": [{"text": "EM", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9278125166893005}]}, {"text": "Using a structured regularizer based on the ontological graph, we learn grounded sense-specific vectors.", "labels": [], "entities": []}, {"text": "There are several reasons to prefer ontologies as distant sources of supervision for learning senseaware VSMs over previously proposed unsupervised context clustering techniques.", "labels": [], "entities": []}, {"text": "Clustering approaches must often parametrize the number of clusters (senses), which is neither known a priori nor constant across words.", "labels": [], "entities": []}, {"text": "Also the resulting vectors remain abstract and uninterpretable.", "labels": [], "entities": []}, {"text": "With ontologies, interpretable sense vectors can be used in downstream applications such as WSD, or for better human error analysis.", "labels": [], "entities": [{"text": "WSD", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.7512893080711365}, {"text": "human error analysis", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.6449028154214224}]}, {"text": "Moreover, clustering techniques operate on distributional similarity only whereas ontologies support other kinds of relationships between senses.", "labels": [], "entities": []}, {"text": "Finally, the existence of cross-lingual ontologies would permit learning multi-lingual vectors, without compounded errors from word alignment and context clustering.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 127, "end_pos": 141, "type": "TASK", "confidence": 0.732757568359375}]}, {"text": "We evaluate our methods on 3 lexical semantic tasks across 7 datasets and show that our sensespecific VSMs effectively integrate knowledge from the ontology with distributional statistics.", "labels": [], "entities": []}, {"text": "Empirically, this results in consistently and significantly better performance over baselines inmost cases.", "labels": [], "entities": []}, {"text": "In the more marginal cases, analysis reveals that our performance is a result of the deficient structure of the ontology.", "labels": [], "entities": []}, {"text": "We discuss and compare the two different approaches from the perspectives of performance, generalizability, flexibility and computational efficiency.", "labels": [], "entities": []}, {"text": "Finally, we qualitatively analyze the vectors and show that they indeed capture sensespecific semantics.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we detail experimental results on 3 lexical semantics tasks across 8 different datasets.", "labels": [], "entities": []}, {"text": "We begin by detailing the training and setup for our experiments.", "labels": [], "entities": []}, {"text": "We evaluate our models on 3 kinds of lexical semantic tasks: similarity scoring, synonym selection, and similarity scoring in context.", "labels": [], "entities": [{"text": "synonym selection", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.878746896982193}]}, {"text": "Similarity Scoring: This task involves using a semantic model to assign a score to pairs of words.", "labels": [], "entities": [{"text": "Similarity Scoring", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8762528896331787}]}, {"text": "We use the following 4 standard datasets in this evaluation: WS-353 (), RG-65 (, and MEN-3k ().", "labels": [], "entities": [{"text": "WS-353", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.8739967942237854}, {"text": "RG-65", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.6176773905754089}, {"text": "MEN-3k", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.723937451839447}]}, {"text": "Each dataset consists of pairs of words along with an averaged similarity score obtained from several human annotators.", "labels": [], "entities": []}, {"text": "For example an item in the WS-353 dataset is \"book, paper \u2192 7.46\".", "labels": [], "entities": [{"text": "WS-353 dataset", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.970344752073288}]}, {"text": "We use standard cosine similarity to assign a score to word pairs in single-sense VSMs, and the following average similarity score to multi-sense variants, as proposed by: The output of systems is evaluated against the gold standard using Spearman's rank correlation coefficient.", "labels": [], "entities": []}, {"text": "Synonym Selection: In this task, VSMs are used to select the semantically closest word to a target from a list of candidates.", "labels": [], "entities": [{"text": "Synonym Selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8597283959388733}]}, {"text": "We use the following 3 standard datasets in this evaluation: ESL-50), RD-300) and TOEFL-80).", "labels": [], "entities": [{"text": "ESL-50", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.6665231585502625}, {"text": "RD-300", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.7303012013435364}, {"text": "TOEFL-80", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.5944383144378662}]}, {"text": "These datasets consist of a list of target words that appear with several candidate lexical items.", "labels": [], "entities": []}, {"text": "An example from the TOEFL dataset is \"rug \u2192 sofa, ottoman, carpet, hallway\", with \"carpet\" being the most synonym-like candidate to the target.", "labels": [], "entities": [{"text": "TOEFL dataset", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.8983794450759888}]}, {"text": "We begin by scoring all pairs composed of the target and one of the candidates.", "labels": [], "entities": []}, {"text": "We use cosine similarity for single-sense VSMs, and max similarity for multisense models : These scores are then sorted in descending order, with the top-ranking score yielding the semantically closest candidate to the target.", "labels": [], "entities": []}, {"text": "Systems are evaluated on the basis of their accuracy at discriminating the top-ranked candidate.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9988024234771729}]}, {"text": "The results for similarity scoring and synonym selection are presented in table 1.", "labels": [], "entities": [{"text": "similarity scoring", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.6422813981771469}, {"text": "synonym selection", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.9729844331741333}]}, {"text": "On both tasks and on all datasets, with the partial exception of WS-353 and MEN-3k, our vectors (RETRO & EM+RETRO) consistently yield better results than other VSMs.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.9269695281982422}, {"text": "MEN-3k", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.8075677156448364}, {"text": "RETRO", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9712460041046143}, {"text": "RETRO", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.7704727649688721}]}, {"text": "Notably, both our techniques perform better than preprocessing a corpus with WSD information in unsupervised or supervised fashion (SG-WSD & SG-IMS).", "labels": [], "entities": []}, {"text": "Simple EM without an ontological prior to ground the vectors (SG-EM) also performs poorly.", "labels": [], "entities": []}, {"text": "We investigated the observed drop in performance on WS-353 and found that this dataset consists of two parts: a set of similar word pairs (e.g. \"tiger\" and \"cat\") and another set of related word pairs (e.g. \"weather\" and \"forecast\").", "labels": [], "entities": [{"text": "WS-353", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.9163358807563782}]}, {"text": "The synonym, hypernym and hyponym relations we use tend to encourage similarity to the detriment of relatedness.", "labels": [], "entities": []}, {"text": "We ran an auxiliary experiment to show this.", "labels": [], "entities": []}, {"text": "SG-EM+RETRO training also learns vectors for context words -which can bethought of as a proxy for relatedness.", "labels": [], "entities": [{"text": "RETRO", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.913129448890686}]}, {"text": "Using this VSM we scored a word pair by the average similarity of all the sense vectors of one word to the context vector of the other word, averaged over both words.", "labels": [], "entities": []}, {"text": "With this scoring function the correlation \u03c1 jumped from 0.321 to 0.493.", "labels": [], "entities": [{"text": "correlation \u03c1", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.9317619204521179}]}, {"text": "While still not as good as some of the other VSMs, it should be noted that this scoring function negatively influences the similar word pairs in the dataset.", "labels": [], "entities": []}, {"text": "The MEN-3k dataset is crowd-sourced and contains much diversity, with word pairs evidencing similarity as well as relatedness.", "labels": [], "entities": [{"text": "MEN-3k dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.913129448890686}]}, {"text": "However, we aren't sure why the performance for GC-RETRO improves greatly over GC-SINGLE for this dataset, while that of SG-RETRO and SG-RETRO+EM drops in relation to SG-SINGLE.", "labels": [], "entities": []}, {"text": "Similarity Scoring in Context: As outlined by, multi-sense VSMs can be used to consider context when computing similarity between words.", "labels": [], "entities": [{"text": "Similarity Scoring in Context", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8290290236473083}]}, {"text": "We use the SCWS dataset () in these experiments.", "labels": [], "entities": [{"text": "SCWS dataset", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.9451773762702942}]}, {"text": "This dataset is similar to the similarity scoring datasets, except that they additionally are presented in context.", "labels": [], "entities": []}, {"text": "For example an item involving the words \"bank\" and \"money\", gives the words in their respective contexts, \"along the east bank of the Des Moines River\" and \"the basis of all money laundering\" with a low averaged similarity score of 2.5 (on a scale of 1.0 to 10.0).", "labels": [], "entities": [{"text": "Des Moines River", "start_pos": 134, "end_pos": 150, "type": "DATASET", "confidence": 0.8286029100418091}, {"text": "money laundering", "start_pos": 174, "end_pos": 190, "type": "TASK", "confidence": 0.7740947306156158}, {"text": "similarity score", "start_pos": 212, "end_pos": 228, "type": "METRIC", "confidence": 0.9617715179920197}]}, {"text": "Following Reisinger and Mooney (2010) we use the following function to assign a score to word pairs in their respective contexts, given a multi-sense VSM: As with similarity scoring, the output of systems is evaluated against gold standard using Spearman's rank correlation coefficient.", "labels": [], "entities": []}, {"text": "The results are presented in table 2.", "labels": [], "entities": []}, {"text": "Preprocessing a corpus with WSD information in an unsupervised fashion (SG-WSD) yields poor results.", "labels": [], "entities": []}, {"text": "In comparison, the retrofitted vectors (SG-RETRO & GC-RETRO) already perform better, even though they do not have access to context vectors, and thus do not take contextual information into account.", "labels": [], "entities": []}, {"text": "Supervised sense vectors (SG-IMS) are also competent, scoring better than both retrofitting techniques.", "labels": [], "entities": []}, {"text": "Our EM vectors (SG-EM & SG-EM+RETRO) yield even better results and are able to capitalize on contextual information, however they still fall short of the pretrained GC-MULTI vectors.", "labels": [], "entities": [{"text": "RETRO", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.973042905330658}]}, {"text": "We were surprised that SG-EM+RETRO actually performed worse than SG-EM, given how poorly SG-EM performed in the other evaluations.", "labels": [], "entities": [{"text": "RETRO", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9909362196922302}]}, {"text": "However, an analysis again revealed that this was due to the kind of similarity encouraged by WordNet rather than an inability of the model to learn useful vectors.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.9726359844207764}]}, {"text": "The SCWS dataset, in addition to containing related  words -which we showed, hurt our performance on WS-353 -also contains word pairs with different POS tags.", "labels": [], "entities": [{"text": "SCWS dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9390764832496643}, {"text": "WS-353", "start_pos": 101, "end_pos": 107, "type": "DATASET", "confidence": 0.8643814921379089}]}, {"text": "WordNet synonymy, hypernymy and hyponymy relations are exclusively defined between lemmas of the same POS tag, which adversely affects performance further.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9242669343948364}]}], "tableCaptions": [{"text": " Table 1: Similarity scoring and synonym selection in English across several datasets involving different VSMs. Higher  scores are better; best scores within each category are in bold. In most cases our models consistently and significantly  outperform the other VSMs.", "labels": [], "entities": [{"text": "Similarity scoring", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8011275231838226}, {"text": "synonym selection", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9129427075386047}]}]}