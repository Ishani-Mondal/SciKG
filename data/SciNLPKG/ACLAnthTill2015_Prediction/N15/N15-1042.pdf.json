{"title": [{"text": "Data-driven sentence generation with non-isomorphic trees", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7007489204406738}]}], "abstractContent": [{"text": "structures from which the generation naturally starts often do not contain any functional nodes, while surface-syntactic structures or a chain of tokens in a linearized tree contain all of them.", "labels": [], "entities": []}, {"text": "Therefore, data-driven linguistic generation needs to be able to cope with the projection between non-isomorphic structures that differ in their topology and number of nodes.", "labels": [], "entities": []}, {"text": "So far, such a projection has been a challenge in data-driven generation and was largely avoided.", "labels": [], "entities": []}, {"text": "We present a fully stochastic generator that is able to cope with projection between non-isomorphic structures.", "labels": [], "entities": []}, {"text": "The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classifier based submodules that map in a series of transitions the input structures onto sentences.", "labels": [], "entities": []}, {"text": "The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered Ancora-UPF corpus.", "labels": [], "entities": [{"text": "Penn-Treebank", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9872379302978516}, {"text": "Ancora-UPF corpus", "start_pos": 103, "end_pos": 120, "type": "DATASET", "confidence": 0.8987282812595367}]}], "introductionContent": [{"text": "Applications such as machine translation that inherently draw upon sentence generation increasingly deal with deep meaning representations; see, e.g.,.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7311984151601791}, {"text": "sentence generation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7293891310691833}]}, {"text": "Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do.", "labels": [], "entities": []}, {"text": "This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7347244918346405}]}, {"text": "However, most of the recent work in datadriven sentence generation still avoids this challenge.", "labels": [], "entities": [{"text": "datadriven sentence generation", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.5501239697138468}]}, {"text": "Some systems focus on syntactic generation;) or linearization and inflection, and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures); and some adapt the meaning structures to be isomorphic with syntactic structures.", "labels": [], "entities": [{"text": "syntactic generation", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.736567959189415}]}, {"text": "However, it is obvious that a \"syntacticization\" of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability.", "labels": [], "entities": []}, {"text": "In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.", "labels": [], "entities": []}, {"text": "Such a generator can be used as a stand-alone application and also, e.g., in text simplification () or deep machine translation () (where the transfer is done at a deep level).", "labels": [], "entities": [{"text": "text simplification", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8236550092697144}, {"text": "deep machine translation", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.6312334438165029}]}, {"text": "In abstractive summarization, it facilitates the generation of the summaries, and in extractive summarization a better sentence fusion.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.5419214069843292}, {"text": "extractive summarization", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.6651504337787628}, {"text": "sentence fusion", "start_pos": 119, "end_pos": 134, "type": "TASK", "confidence": 0.719142809510231}]}, {"text": "The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by, consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions.", "labels": [], "entities": []}, {"text": "Following the idea presented in (), a separate SVM-classifier is defined for the mapping of each linguistic category.", "labels": [], "entities": []}, {"text": "The generator has been tested on Spanish with the multi-layered Ancora-UPF corpus () and on English with an extended version of the dependency Penn TreeBank (.", "labels": [], "entities": [{"text": "Ancora-UPF corpus", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.7488038241863251}]}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In the next section, we briefly outline the fundamentals of sentence generation as we view it in our work, focusing in particular on the most challenging part of it: the transition between the non-isomorphic predicateargument lexico-structural structures and surfacesyntactic structures.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7395828813314438}]}, {"text": "Section 3 outlines the setup of our system.", "labels": [], "entities": []}, {"text": "Section 4 discusses the experiments we carried out and the results we obtained.", "labels": [], "entities": []}, {"text": "In Section 5, we briefly summarize related work, before in Section 6 some conclusions are drawn and future work is outlined.", "labels": [], "entities": [{"text": "summarize related", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.8877033889293671}]}], "datasetContent": [{"text": "In our experiments, the Spanish treebank has been divided into: (i) a development set of 219 sentences, with 3,437 tokens in the DSyntS treebank and 4,799 tokens in the SSyntS treebank (with an average of 21.91 words by sentence in SSynt); (ii) a training set of 3,036 sentences, with 57,665 tokens in the DSyntS treebank and 84,668 tokens in the SSyntS treebank (with an average of 27.89 words by sentence in SSynt); and a (iii) a held-out test for evaluation of 258 sentences, with 5,878 tokens in the DSyntS treebank and 8,731 tokens in the SSyntS treebank (with an average of 33.84 words by sentence in SSynt).", "labels": [], "entities": [{"text": "Spanish treebank", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.9114956557750702}, {"text": "DSyntS treebank", "start_pos": 129, "end_pos": 144, "type": "DATASET", "confidence": 0.9816688597202301}, {"text": "SSyntS treebank", "start_pos": 169, "end_pos": 184, "type": "DATASET", "confidence": 0.9324291944503784}, {"text": "DSyntS treebank", "start_pos": 306, "end_pos": 321, "type": "DATASET", "confidence": 0.9777545034885406}, {"text": "SSyntS treebank", "start_pos": 347, "end_pos": 362, "type": "DATASET", "confidence": 0.941211462020874}, {"text": "DSyntS treebank", "start_pos": 504, "end_pos": 519, "type": "DATASET", "confidence": 0.9729703366756439}, {"text": "SSyntS treebank", "start_pos": 544, "end_pos": 559, "type": "DATASET", "confidence": 0.9252541065216064}]}, {"text": "For the English treebank, we used a classical split of (i) a training set of 39,279 sentences, with 724,828 tokens in the DSynt treebank and 958,167 tokens in the SSynt treebank (with an average of 24.39 words by sentence in SSynt); and (ii) a test set of 2,399 sentences, with 43,245 tokens in the DSynt treebank and 57,676 tokens in the SSynt treebank (with an average of 24.04 words by sentence in.", "labels": [], "entities": [{"text": "English treebank", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.9290471374988556}, {"text": "DSynt treebank", "start_pos": 122, "end_pos": 136, "type": "DATASET", "confidence": 0.9763429164886475}, {"text": "SSynt treebank", "start_pos": 163, "end_pos": 177, "type": "DATASET", "confidence": 0.9456243813037872}, {"text": "DSynt treebank", "start_pos": 299, "end_pos": 313, "type": "DATASET", "confidence": 0.9831072688102722}, {"text": "SSynt treebank", "start_pos": 339, "end_pos": 353, "type": "DATASET", "confidence": 0.9553000330924988}]}, {"text": "In what follows, we show the system performance on both treebanks.", "labels": [], "entities": []}, {"text": "The Spanish treebank was used for development and testing, while the English treebank was only used for testing.", "labels": [], "entities": [{"text": "Spanish treebank", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8972463607788086}, {"text": "English treebank", "start_pos": 69, "end_pos": 85, "type": "DATASET", "confidence": 0.9777737855911255}]}], "tableCaptions": [{"text": " Table 1: Results of the evaluation of the SVMs for the  non-isomorphic transition for the Spanish DSyntS devel- opment and test sets", "labels": [], "entities": [{"text": "Spanish DSyntS devel- opment", "start_pos": 91, "end_pos": 119, "type": "DATASET", "confidence": 0.8102121829986573}]}, {"text": " Table 2: Results of the evaluation of the SVMs for the  non-isomorphic transition for the English DSyntS test set", "labels": [], "entities": [{"text": "English DSyntS test set", "start_pos": 91, "end_pos": 114, "type": "DATASET", "confidence": 0.8720337450504303}]}, {"text": " Table 3: Overview of the results on the Spanish develop- ment and test sets excluding punctuation marks after the  linearization Test Set  BLEU NIST  Exact  surface gen.  0.91  15.26 56.02 %  baseline deep gen.  0.69  13.71 12.38 %  deep gen.  0.77  14.42 21.05 %", "labels": [], "entities": [{"text": "Spanish develop- ment and test sets", "start_pos": 41, "end_pos": 76, "type": "DATASET", "confidence": 0.8248275007520404}, {"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9479395151138306}, {"text": "NIST  Exact  surface gen", "start_pos": 145, "end_pos": 169, "type": "DATASET", "confidence": 0.5213210582733154}]}]}