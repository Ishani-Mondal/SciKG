{"title": [{"text": "Unsupervised Morphology Induction Using Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a language agnostic, unsupervised method for inducing morphological transformations between words.", "labels": [], "entities": []}, {"text": "The method relies on certain regularities manifest in high-dimensional vector spaces.", "labels": [], "entities": []}, {"text": "We show that this method is capable of discovering a wide range of morphological rules, which in turn are used to build morphological analyzers.", "labels": [], "entities": []}, {"text": "We evaluate this method across six different languages and nine datasets, and show significant improvements across all languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word representations obtained via neural networks () or specialized models () have been used to address various natural language processing tasks).", "labels": [], "entities": []}, {"text": "These vector representations capture various syntactic and semantic properties of natural language).", "labels": [], "entities": []}, {"text": "In many instances, natural language uses a small set of concepts to render a much larger set of meaning variations via morphology.", "labels": [], "entities": []}, {"text": "We show in this paper that morphological transformations can be captured by exploiting regularities present in wordrepresentations as the ones trained using the SkipGram model (.", "labels": [], "entities": [{"text": "SkipGram", "start_pos": 161, "end_pos": 169, "type": "DATASET", "confidence": 0.93825364112854}]}, {"text": "In contrast to previous approaches that combine morphology with vector-based word representations (), we do not rely on an external morphological analyzer, such as Morfessor (Creutz and La- * Work done at Google, now at Human Longevity Inc..", "labels": [], "entities": []}, {"text": "Instead, our method automatically induces morphological rules and transformations, represented as vectors in the same embedding space.", "labels": [], "entities": []}, {"text": "At the heart of our method is the SkipGram model described in).", "labels": [], "entities": []}, {"text": "We further exploit the observations made by, and further studied by, regarding the regularities exhibited by such embedding spaces.", "labels": [], "entities": []}, {"text": "These regularities have been shown to allow inferences of certain types (e.g., king is to man what queen is to woman).", "labels": [], "entities": []}, {"text": "Such regularities also hold for certain morphological relations (e.g., car is to cars what dog is to dogs).", "labels": [], "entities": []}, {"text": "In this paper, we show that one can exploit these regularities to model, in a principled way, prefix-and suffix-based morphology.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are as follows: 1.", "labels": [], "entities": []}, {"text": "provides a method by which morphological rules are learned in an unsupervised, languageagnostic fashion; 2.", "labels": [], "entities": []}, {"text": "provides a mechanism for applying these rules to known words (e.g., boldly is analyzed as bold+ly, while only is not); 3.", "labels": [], "entities": []}, {"text": "provides a mechanism for applying these rules to rare and unseen words; We show that this method improves state-of-the-art performance on a word-similarity rating task using standard datasets.", "labels": [], "entities": []}, {"text": "We also quantify the impact of our morphology treatment when using large amounts of training data (tens/hundreds of billions of words).", "labels": [], "entities": []}, {"text": "The technique we describe is capable of inducing transformations that cover both typical, regular morphological rules, such as adding suffix ed to verbs in English, as well as exceptions to such rules, such as the fact that pluralization of words that end in y require substituting it with ies.", "labels": [], "entities": []}, {"text": "Because each such transformation is represented in the high-dimensional embedding space, it therefore captures the semantics of the change.", "labels": [], "entities": []}, {"text": "Consequently, it allows us to build vector representations for any unseen word for which a morphological analysis is found, therefore covering an unbounded (albeit incomplete) vocabulary.", "labels": [], "entities": []}, {"text": "Our empirical evaluations show that this language-agnostic technique is capable of learning morphological transformations across various language families.", "labels": [], "entities": []}, {"text": "We present results for English, German, French, Spanish, Romanian, Arabic, and Uzbek.", "labels": [], "entities": []}, {"text": "The results indicate that the induced morphological analysis deals successfully with sophisticated morphological variations.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Examples of lexicalized morphological transformations evaluated in E n using rank and cosine.", "labels": [], "entities": []}, {"text": " Table 4: Performance of previously proposed methods, compared to SG and SG+Morph trained on Wiki1b. LSM2013  uses exactly the same training data for EN, whereas BB2014 uses the same training data for DE, FR, ES.", "labels": [], "entities": [{"text": "LSM2013", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.8444648385047913}, {"text": "BB2014", "start_pos": 162, "end_pos": 168, "type": "DATASET", "confidence": 0.8060599565505981}]}, {"text": " Table 6: Accuracy of Rare&OOV analysis.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9955175518989563}, {"text": "Rare&OOV analysis", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.5746256560087204}]}, {"text": " Table 5. For English, a 100x in-", "labels": [], "entities": []}]}