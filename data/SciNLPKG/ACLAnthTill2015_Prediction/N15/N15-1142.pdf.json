{"title": [{"text": "Two/Too Simple Adaptations of Word2Vec for Syntax Problems", "labels": [], "entities": []}], "abstractContent": [{"text": "We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax.", "labels": [], "entities": []}, {"text": "The main issue with the original models is the fact that they are insensitive to word order.", "labels": [], "entities": []}, {"text": "While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems.", "labels": [], "entities": []}, {"text": "We show improvements in part-of-speech tagging and dependency parsing using our proposed models.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.776509165763855}, {"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8767525553703308}]}], "introductionContent": [{"text": "Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging), dependency parsing) and machine translation ().", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.6994637250900269}, {"text": "dependency parsing", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.7924045622348785}, {"text": "machine translation", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.8347483277320862}]}, {"text": "These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text.", "labels": [], "entities": []}, {"text": "They are then incorporated as features alongside hand-engineered features (, or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available).", "labels": [], "entities": []}, {"text": "One of the most widely used tools for building word vectors are the models described in, implemented in the Word2Vec tool, in particular the \"skip-gram\" and the \"continuous bag-of-words\" (CBOW) models.", "labels": [], "entities": []}, {"text": "These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context.", "labels": [], "entities": []}, {"text": "Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks).", "labels": [], "entities": []}, {"text": "While more sophisticated approaches have been proposed), Word2Vec remains a popular choice due to their efficiency and simplicity.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9175251722335815}]}, {"text": "However, as these models are insensitive to word order, embeddings built using these models are suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 143, "end_pos": 164, "type": "TASK", "confidence": 0.7722772061824799}, {"text": "dependency parsing", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.8228421211242676}]}, {"text": "This is because syntax defines \"what words go where?\", while semantics than \"what words go together\".", "labels": [], "entities": []}, {"text": "Obviously, in a model where word order is discarded, the many syntactic relations between words cannot be captured properly.", "labels": [], "entities": []}, {"text": "For instance, while most words occur with the word the, only nouns tend to occur exactly afterwords (e.g. the cat).", "labels": [], "entities": []}, {"text": "This is supported by empirical evidence that suggests that order-insensitivity does indeed lead to substandard syntactic representations, where systems using pre-trained with Word2Vec models yield slight improvements while the computationally far more expensive which use word order information embeddings of yielded much better results.", "labels": [], "entities": []}, {"text": "In this work, we describe two simple modifications to Word2Vec, one for the skip-gram model and one for the CBOW model, that improve the quality of the embeddings for syntax-based tasks . Our goal is to improve the final embeddings while maintaining the simplicity and efficiency of the original models.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.9418479204177856}, {"text": "simplicity", "start_pos": 254, "end_pos": 264, "type": "METRIC", "confidence": 0.9783599376678467}]}, {"text": "We demonstrate the effectiveness of our approaches by training, on commodity hardware, on datasets containing more than 50 million sentences and over 1 billion words in less than a day, and show that our methods lead to improvements when used in state-of-the-art neural network systems for part-of-speech tagging and dependency parsing, relative to the original models.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 290, "end_pos": 312, "type": "TASK", "confidence": 0.7788265347480774}, {"text": "dependency parsing", "start_pos": 317, "end_pos": 335, "type": "TASK", "confidence": 0.8696559071540833}]}], "datasetContent": [{"text": "We conducted experiments in two mainstream syntax-based tasks part-of-speech Tagging and Dependency parsing.", "labels": [], "entities": [{"text": "part-of-speech Tagging", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.7956258058547974}, {"text": "Dependency parsing", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8479542434215546}]}, {"text": "Part-of-speech tagging is a word labeling task, where each word is to be labelled with its corresponding part-of-speech.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.775223433971405}, {"text": "word labeling task", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7778006295363108}]}, {"text": "In dependency parsing, the goal is to predict a tree built of syntactic relations between words.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8087977170944214}]}, {"text": "In both tasks, it has been shown that pre-trained embeddings can be used to achieve better generalization).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for part-of-speech tagging using differ- ent word embeddings (rows) on different datasets (PTB  and Twitter). Cells indicate the part-of-speech accuracy  of each experiment.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7638464570045471}, {"text": "PTB", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.9683399796485901}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9633064270019531}]}, {"text": " Table 3: Results for dependency parsing on PTB using  different word embeddings (rows). Columns UAS and  LAS indicate the labelled attachment score and the unla- belled parsing scores, respectively.", "labels": [], "entities": [{"text": "dependency parsing on PTB", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.758039727807045}, {"text": "UAS", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.8872714638710022}, {"text": "LAS", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9810693264007568}, {"text": "labelled attachment score", "start_pos": 123, "end_pos": 148, "type": "METRIC", "confidence": 0.7231578628222147}]}]}