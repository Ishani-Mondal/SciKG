{"title": [{"text": "Unsupervised Dependency Parsing: Let's Use Supervised Parsers", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a self-training approach to unsu-pervised dependency parsing that reuses existing supervised and unsupervised parsing algorithms.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.6811086237430573}]}, {"text": "Our approach, called 'iterated rerank-ing' (IR), starts with dependency trees generated by an unsupervised parser, and iteratively improves these trees using the richer probability models used in supervised parsing that are in turn trained on these trees.", "labels": [], "entities": [{"text": "iterated rerank-ing' (IR)", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6791892945766449}]}, {"text": "Our system achieves 1.8% accuracy higher than the state-of-the-part parser of Spitkovsky et al.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9994577765464783}]}, {"text": "(2013) on the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus.", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.933593213558197}]}], "introductionContent": [{"text": "Unsupervised dependency parsing and its supervised counterpart have many characteristics in common: they take as input raw sentences, produce dependency structures as output, and often use the same evaluation metric (DDA, or UAS, the percentage of tokens for which the system predicts the correct head).", "labels": [], "entities": [{"text": "Unsupervised dependency parsing", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6196137368679047}, {"text": "UAS", "start_pos": 225, "end_pos": 228, "type": "METRIC", "confidence": 0.9456261396408081}]}, {"text": "Unsurprisingly, there has been much more research on supervised parsing -producing a wealth of models, datasets and training techniques -than on unsupervised parsing, which is more difficult, much less accurate and generally uses very simple probability models.", "labels": [], "entities": []}, {"text": "Surprisingly, however, there have been no reported attempts to reuse supervised approaches to tackle the unsupervised parsing problem (an idea briefly mentioned in).", "labels": [], "entities": [{"text": "parsing problem", "start_pos": 118, "end_pos": 133, "type": "TASK", "confidence": 0.8881612122058868}]}, {"text": "There are, nevertheless, two aspects of supervised parsers that we would like to exploit in an unsupervised setting.", "labels": [], "entities": []}, {"text": "First, we can increase the model expressiveness in order to capture more linguistic regularities.", "labels": [], "entities": []}, {"text": "Many recent supervised parsers use thirdorder (or higher order) features ( to reach state-of-the-art (SOTA) performance.", "labels": [], "entities": []}, {"text": "In contrast, existing models for unsupervised parsing limit themselves to using simple features (e.g., conditioning on heads and valency variables) in order to reduce the computational cost, to identify consistent patterns in data, and to avoid overfitting.", "labels": [], "entities": []}, {"text": "Although this makes learning easier and more efficient, the disadvantage is that many useful linguistic regularities are missed: an upper bound on the performance of such simple models -estimated by using annotated data -is 76.3% on the WSJ corpus (), compared to over 93% actual performance of the SOTA supervised parsers.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 237, "end_pos": 247, "type": "DATASET", "confidence": 0.9729724526405334}, {"text": "SOTA supervised parsers", "start_pos": 299, "end_pos": 322, "type": "TASK", "confidence": 0.5753896435101827}]}, {"text": "Second, we would like to make use of information available from lexical semantics, as in,, and.", "labels": [], "entities": []}, {"text": "Lexical semantics is a source for handling rare words and syntactic ambiguities.", "labels": [], "entities": [{"text": "Lexical semantics", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8602953851222992}]}, {"text": "For instance, if a parser can identify that \"he\" is a dependent of \"walks\" in the sentence \"He walks\", then, even if \"she\" and \"runs\" do not appear in the training data, the parser may still be able to recognize that \"she\" should be a dependent of \"runs\" in the sentence \"she runs\".", "labels": [], "entities": []}, {"text": "Similarly, a parser can make use of the fact that \"sauce\" and \"John\" have very different meanings to decide that they have different heads in the two phrases \"ate spaghetti with sauce\" and \"ate spaghetti with John\".", "labels": [], "entities": []}, {"text": "However, applying existing supervised parsing techniques to the task of unsupervised parsing is, unfortunately, not trivial.", "labels": [], "entities": []}, {"text": "The reason is that those parsers are optimally designed for being trained on manually annotated data.", "labels": [], "entities": []}, {"text": "If we use existing unsupervised training methods (like EM), learning could be easily misled by a large amount of ambiguity naturally embedded in unannotated training data.", "labels": [], "entities": []}, {"text": "Moreover, the computational cost could rapidly increase if the training algorithm is not designed properly.", "labels": [], "entities": []}, {"text": "To overcome these difficulties we propose a framework, iterated reranking (IR), where existing supervised parsers are trained without the need of manually annotated data, starting with dependency trees provided by an existing unsupervised parser as initialiser.", "labels": [], "entities": []}, {"text": "Using this framework, we can employ the work of to build anew system that outperforms the SOTA unsupervised parser of on the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 125, "end_pos": 135, "type": "DATASET", "confidence": 0.9731981456279755}]}, {"text": "The contribution of this paper is twofold.", "labels": [], "entities": []}, {"text": "First, we show the benefit of using lexical semantics for the unsupervised parsing task.", "labels": [], "entities": []}, {"text": "Second, our work is abridge connecting the two research areas unsupervised parsing and its supervised counterpart.", "labels": [], "entities": []}, {"text": "Before going to the next section, in order to avoid confusion introduced by names, it is worth noting that we use un-trained existing supervised parsers which will be trained on automatically annotated treebanks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance on section 23 of the WSJ cor- pus (all sentences and up to length 10) for recent sys- tems and our system. MinEnc and MaxEnc denote  iters MST = 10 and iters MST = 1 respectively.", "labels": [], "entities": [{"text": "WSJ cor- pus", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.890406996011734}]}]}