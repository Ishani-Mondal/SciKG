{"title": [{"text": "Model Invertibility Regularization: Sequence Alignment With or Without Parallel Data", "labels": [], "entities": [{"text": "Model Invertibility Regularization", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.4911438822746277}, {"text": "Sequence Alignment", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.9422913193702698}]}], "abstractContent": [{"text": "We present Model Invertibility Regularization (MIR), a method that jointly trains two directional sequence alignment models, one in each direction, and takes into account the invertibil-ity of the alignment task.", "labels": [], "entities": [{"text": "Model Invertibility Regularization (MIR)", "start_pos": 11, "end_pos": 51, "type": "TASK", "confidence": 0.7765894035498301}]}, {"text": "By coupling the two models through their parameters (as opposed to through their inferences, as in Liang et al.'s Alignment by Agreement (ABA), and Ganchev et al.'s Posterior Regularization (PostCAT)), our method seamlessly extends to all IBM-style word alignment models as well as to alignment without parallel data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 249, "end_pos": 263, "type": "TASK", "confidence": 0.6592885702848434}]}, {"text": "Our proposed algorithm is mathematically sound and inherits convergence guarantees from EM.", "labels": [], "entities": [{"text": "convergence", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.9681813716888428}, {"text": "EM", "start_pos": 88, "end_pos": 90, "type": "DATASET", "confidence": 0.8419139981269836}]}, {"text": "We evaluate MIR on two tasks: (1) On word alignment , applying MIR on fertility based models we attain higher F-scores than ABA and PostCAT.", "labels": [], "entities": [{"text": "MIR", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9929596781730652}, {"text": "word alignment", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.8100926876068115}, {"text": "MIR", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9783108830451965}, {"text": "F-scores", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9968615770339966}, {"text": "ABA", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.9875246286392212}, {"text": "PostCAT", "start_pos": 132, "end_pos": 139, "type": "DATASET", "confidence": 0.7983905673027039}]}, {"text": "(2) On Japanese-to-English back-transliteration without parallel data, applied to the decipherment model of Ravi and Knight, MIR learns sparser models that close the gap in whole-name error rate by 33% relative to a model trained on parallel data, and further, beats a previous approach by Mylonakis et al.", "labels": [], "entities": [{"text": "MIR", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9705665707588196}, {"text": "whole-name error rate", "start_pos": 173, "end_pos": 194, "type": "METRIC", "confidence": 0.6282208959261576}]}], "introductionContent": [{"text": "The transfer of information between languages is a common natural language phenomenon that is intuitively invertible.", "labels": [], "entities": []}, {"text": "For example, in transliteration, a source-language word is mapped to a target language's writing system under a sound preserving mapping (for example, \"computer\" to Japanese Romaji, \"konpyutaa\").", "labels": [], "entities": []}, {"text": "The original word should then be recoverable from its transliterated version.", "labels": [], "entities": []}, {"text": "Similarly, in translation, the back-translation of the translation of a word is likely to be that same word itself.", "labels": [], "entities": [{"text": "translation", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9638725519180298}]}, {"text": "In NLP, however, commonly-used generative models describing such phenomena are directional, only concerned with the transfer of source-language symbols to target-language symbols or vice versa, but not both directions.", "labels": [], "entities": []}, {"text": "Left unchecked, independently training two such directional models (sourceto-target and target-to-source) often yields two models that diverge from this invertibility intuition.", "labels": [], "entities": []}, {"text": "In word alignment, this can lead to disagreements between alignments inferred by a model trained in one direction and those inferred by a model trained in the reverse direction.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 3, "end_pos": 17, "type": "TASK", "confidence": 0.7955467998981476}]}, {"text": "To remedy this disparity (and other shortcomings), it is common to turn to alignment symmetrization techniques such as growdiag-final-and () which heuristically combines alignments from both directions.", "labels": [], "entities": []}, {"text": "suggest a more fundamental approach they call Alignment by Agreement (ABA), which jointly trains two word alignment models by maximizing their data-likelihoods along with a regularizer that rewards agreement between their alignment posteriors (computed over each parallel sentence pair).", "labels": [], "entities": []}, {"text": "Although their EM-like optimization procedure is heuristic, it proves effective at jointly training bidirectional models.", "labels": [], "entities": [{"text": "EM-like optimization", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.9197052419185638}]}, {"text": "propose another approach for agreement between the directed models by adding constraints on the alignment posteriors.", "labels": [], "entities": []}, {"text": "Unlike ABA, their optimization is exact, but it can be computationally expensive, requiring multiple forward-backward inferences in each E-step.", "labels": [], "entities": []}, {"text": "In this paper we develop a different approach for jointly training general bidirectional sequence alignment models called Model Invertibility Regularization, or MIR (Section 3).", "labels": [], "entities": [{"text": "Model Invertibility Regularization", "start_pos": 122, "end_pos": 156, "type": "TASK", "confidence": 0.5743553837140402}]}, {"text": "Our approach has two key benefits over ABA and PostCAT: First, MIR can be applied to sequence alignment without parallel data.", "labels": [], "entities": [{"text": "MIR", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9351380467414856}, {"text": "sequence alignment", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.7324899137020111}]}, {"text": "Second, a single implementation seamlessly extends to all IBM models, including the fertility based models.", "labels": [], "entities": []}, {"text": "Furthermore, since MIR follows the MAP-EM framework, it inherits its desirable convergence guarantees.", "labels": [], "entities": [{"text": "MIR", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9778953194618225}]}, {"text": "The key idea facilitating the easy extension to complex models and to non-parallel data settings is in our regularizer, which operates on the model parameters as opposed to their inferences.", "labels": [], "entities": []}, {"text": "Specifically, MIR was designed to reward model pairs whose translation tables respect the invertibility intuition.", "labels": [], "entities": [{"text": "MIR", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9937064051628113}]}, {"text": "We tested MIR against competitive baselines on two sequence alignment tasks: word alignment (with parallel data) and back-transliteration decipherment (without parallel data).", "labels": [], "entities": [{"text": "MIR", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9918377995491028}, {"text": "word alignment", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.8023419976234436}]}, {"text": "On Czech-English and Chinese-English word alignment (Section 5), restricted to the HMM model, MIR attains F-and Bleu score improvements that are comparable to those of ABA and PostCAT.", "labels": [], "entities": [{"text": "Chinese-English word alignment", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.6019184092680613}, {"text": "MIR", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.7872185707092285}, {"text": "F-and Bleu score", "start_pos": 106, "end_pos": 122, "type": "METRIC", "confidence": 0.8785151243209839}, {"text": "ABA", "start_pos": 168, "end_pos": 171, "type": "METRIC", "confidence": 0.8689880967140198}, {"text": "PostCAT", "start_pos": 176, "end_pos": 183, "type": "DATASET", "confidence": 0.9186229705810547}]}, {"text": "We further apply MIR beyond HMM, on the fertility-based IBM Models, showing further gains in F-score compared to the baseline, ABAandPostCAT.", "labels": [], "entities": [{"text": "MIR", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.6101870536804199}, {"text": "IBM Models", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.9348257184028625}, {"text": "F-score", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9991746544837952}, {"text": "ABAandPostCAT", "start_pos": 127, "end_pos": 140, "type": "METRIC", "confidence": 0.6644999980926514}]}, {"text": "Interestingly, the HMM alignments obtained by ABA and MIR are qualitatively different, so that combining the two yields additive gains over each method by itself.", "labels": [], "entities": [{"text": "ABA", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9119364619255066}]}, {"text": "On English-Japanese back-transliteration decipherment (Section 6), we apply MIR to the cascade of wFSTs approach proposed by.", "labels": [], "entities": [{"text": "MIR", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9830284118652344}]}, {"text": "Using MIR, we are able to reduce the wholename error-rate relative to a model trained on parallel data by 33%, as well as significantly outperform the joint model proposed by.", "labels": [], "entities": [{"text": "MIR", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.6705708503723145}]}], "datasetContent": [{"text": "In this section, we compare MIR against standard EM training and ABA on Czech-English and Chinese-English word alignment and translation.", "labels": [], "entities": [{"text": "MIR", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9405542612075806}, {"text": "ABA", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9942030310630798}, {"text": "Czech-English and Chinese-English word alignment and translation", "start_pos": 72, "end_pos": 136, "type": "TASK", "confidence": 0.6071670992033822}]}, {"text": "We obtained HMM alignments by running either 5 or 10 iterations (optimized on a held-out validation set) of both IBM  We tuned MIR's \u03bb parameter to maximize alignment F-score on a validation set of 460 hand-aligned Czech-English and 1102 Chinese-English sentences.", "labels": [], "entities": [{"text": "HMM alignments", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.8112188577651978}, {"text": "F-score", "start_pos": 167, "end_pos": 174, "type": "METRIC", "confidence": 0.780200719833374}]}, {"text": "Alignment F-scores are reported in.", "labels": [], "entities": [{"text": "Alignment F-scores", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.8456777632236481}]}, {"text": "In particular, the best results were obtained by MIR, when applied to the fertility based IBM4 modelwe obtained gains of +2.1% (Chinese-English) and +0.3% (Czech-English) compared to the best competitor.", "labels": [], "entities": [{"text": "MIR", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.8373794555664062}]}, {"text": "We ran MT experiments using the Moses ( phrase-based translation system.", "labels": [], "entities": [{"text": "MT", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9860469698905945}, {"text": "phrase-based translation", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6898381263017654}]}, {"text": "The feature weights were trained discriminatively using MIRA (, and we used a 5-gram language model trained on the Xinhua portion of English Gigaword (LDC2007T07).", "labels": [], "entities": [{"text": "MIRA", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9487273097038269}, {"text": "Xinhua portion of English Gigaword (LDC2007T07)", "start_pos": 115, "end_pos": 162, "type": "DATASET", "confidence": 0.731208149343729}]}, {"text": "All other parameters remained with their default settings.", "labels": [], "entities": []}, {"text": "The development data used for discriminative training were: for Chinese-English, data from the NIST 2004 and NIST 2006 test sets; for Czech-English, 2051 sentences from the WMT 2010 shared task.", "labels": [], "entities": [{"text": "NIST 2004 and NIST 2006 test sets", "start_pos": 95, "end_pos": 128, "type": "DATASET", "confidence": 0.9141671742711749}, {"text": "WMT 2010 shared task", "start_pos": 173, "end_pos": 193, "type": "DATASET", "confidence": 0.7343237847089767}]}, {"text": "We used case-insensitive IBM Bleu (closest reference length) as our metric.", "labels": [], "entities": [{"text": "IBM Bleu (closest reference length)", "start_pos": 25, "end_pos": 60, "type": "METRIC", "confidence": 0.7662781732422965}]}, {"text": "On both language pairs, ABA, PostCAT and MIR outperform their respective EM baseline with comparable gains overall.", "labels": [], "entities": [{"text": "ABA", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9959546327590942}, {"text": "PostCAT", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.7438182830810547}, {"text": "MIR", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.7914251685142517}]}, {"text": "However, we noticed that ABA and MIR are not producing the same alignments.", "labels": [], "entities": [{"text": "ABA", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9589555859565735}, {"text": "MIR", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.6349796056747437}]}, {"text": "For example, by combining their HMM alignments (simply concatenating aligned bitexts) the total improvement reaches +1.5 Bleu on the Chinese-toEnglish task, a statistically significant improvement (p < 0.05) according to a bootstrap resampling significance test).", "labels": [], "entities": [{"text": "Bleu", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9712350964546204}]}, {"text": "Ravi and Knight (2009) consider the challenging task of learning a Japanese-English backtransliteration model without parallel data.", "labels": [], "entities": []}, {"text": "The goal is to correctly decode a list of 100 US senator names written in katakana script, without having access to parallel data.", "labels": [], "entities": []}, {"text": "In this section, we reproduce their decipherment experiment and show that applying MIR to their baseline model significantly outperforms both the baseline and the bi-EM method.", "labels": [], "entities": [{"text": "MIR", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9538334012031555}]}], "tableCaptions": [{"text": " Table 1: Word alignment F1 scores.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6987964659929276}, {"text": "F1", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.7415297031402588}]}, {"text": " Table 2: Bleu scores. Combining ABA and MIR HMM  alignments improves Bleu score significantly over all  other methods.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.8878457546234131}, {"text": "ABA", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9828857183456421}, {"text": "Bleu score", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9858534336090088}]}, {"text": " Table 3: MIR reduces error rates (WNER, NED) and  learns sparser models (number of t 1 parameters greater  than 0.01) compared to the other models.", "labels": [], "entities": [{"text": "MIR", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9377301931381226}, {"text": "error rates", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9676594138145447}]}]}