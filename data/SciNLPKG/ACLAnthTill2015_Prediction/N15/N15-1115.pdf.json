{"title": [{"text": "Encoding World Knowledge in the Evaluation of Local Coherence", "labels": [], "entities": []}], "abstractContent": [{"text": "Previous work on text coherence was primarily based on matching multiple mentions of the same entity in different parts of the text; therefore, it misses the contribution from semantically related but not necessarily coref-erential entities (e.g., Gates and Microsoft).", "labels": [], "entities": []}, {"text": "In this paper, we capture such semantic relat-edness by leveraging world knowledge (e.g., Gates is the person who created Microsoft), and use two existing evaluation frameworks.", "labels": [], "entities": []}, {"text": "First, in the unsupervised framework, we introduce semantic relatedness as an enrichment to the original graph-based model of Guin-audeau and Strube (2013).", "labels": [], "entities": []}, {"text": "In addition, we incorporate semantic relatedness as additional features into the popular entity-based model of Barzilay and Lapata (2008).", "labels": [], "entities": []}, {"text": "Across both frameworks, our enriched model with semantic relatedness outperforms the original methods , especially on short documents.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ina well-written document, sentences are organized and presented in a logical and coherent form, which makes the text fluent and easily understood.", "labels": [], "entities": []}, {"text": "Therefore, coherence is a fundamental aspect of high text quality, and the evaluation of coherence is a crucial component of many NLP applications, such as essay scoring (), story generation, and document summarization (.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 156, "end_pos": 169, "type": "TASK", "confidence": 0.7460744976997375}, {"text": "story generation", "start_pos": 174, "end_pos": 190, "type": "TASK", "confidence": 0.8288286626338959}, {"text": "document summarization", "start_pos": 196, "end_pos": 218, "type": "TASK", "confidence": 0.6841675043106079}]}, {"text": "A particularly popular model for evaluating text coherence is the entity-based local coherence model of, which extracts mentions of entities in adjacent sentences, and captures local coherence in terms of the transitions in the grammatical role of each mention.", "labels": [], "entities": []}, {"text": "Following this direction, a number of extensions have been proposed, the majority of which focus on enriching the original entity features.", "labels": [], "entities": []}, {"text": "An exception is the unsupervised model of, which converts the document into a graph of sentences, and evaluates the text coherence by computing the average out-degree over the entire graph.", "labels": [], "entities": []}, {"text": "However, despite the apparent success of these methods, they rely merely on matching mentions of the same entity, but neglect the contribution from semantically related but not necessarily coreferential entities.", "labels": [], "entities": []}, {"text": "For example, the text in has no common entity in s 2 and s 3 . However, the transition between them is perfectly coherent, because there exists close semantic relatedness between two distinct entities, Gates in s 2 and Microsoft in s 3 , which can be captured by the world knowledge that Gates is the person who created Microsoft (represented by Gates-create-Microsoft).", "labels": [], "entities": []}, {"text": "In fact, the issue of absence of common entities between adjacent sentences is quite prevalent.", "labels": [], "entities": []}, {"text": "Analyzing the CoNLL 2012 dataset (), we found that 42.34% of the time, adjacent sentences do not share common entities.", "labels": [], "entities": [{"text": "CoNLL 2012 dataset", "start_pos": 14, "end_pos": 32, "type": "DATASET", "confidence": 0.958603044350942}]}, {"text": "As a result, methods which rely on strict entity matching would fail on these cases.", "labels": [], "entities": [{"text": "entity matching", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7384874820709229}]}, {"text": "The corresponding entity grid: A news text fragment with its corresponding entity grid constructed following.", "labels": [], "entities": []}, {"text": "Although s 2 and s 3 share no entity, their transition is still coherent, because Gates and Microsoft are semantically related by the knowledge Gates-create-Microsoft.", "labels": [], "entities": []}, {"text": "We wish to incorporate semantic relatedness between different entities into existing models to tackle the problem described above.", "labels": [], "entities": []}, {"text": "In particular, we propose to capture such semantic relatedness between different entities with world knowledge represented as triples, e.g., Gates-create-Microsoft.", "labels": [], "entities": []}, {"text": "Given a text to be evaluated, we first retrieve relevant world knowledge from multiple sources.", "labels": [], "entities": []}, {"text": "For the unsupervised framework of G&S, we integrate knowledge into the original graph-based document representation, in which sentences are the nodes and edges are formed by shared entities and our world knowledge.", "labels": [], "entities": []}, {"text": "Then, we adopt a dynamic programming algorithm to produce a coherence score for the text.", "labels": [], "entities": []}, {"text": "For the supervised framework of B&L, we incorporate the world knowledge as a novel set of features into the original entity-based model, and train a model to discriminate different degrees of text coherence.", "labels": [], "entities": [{"text": "B&L", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.7602270245552063}]}, {"text": "To evaluate the impact of incorporating semantic relatedness, we conduct experiments on two datasets, each of which resembles areal sub-task in the text coherence modeling: sentence ordering and summary coherence rating.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 173, "end_pos": 190, "type": "TASK", "confidence": 0.6974087357521057}]}, {"text": "On both tasks, across two frameworks, supervised and unsupervised, we perform a direct comparison between our enhanced model and the original one.", "labels": [], "entities": []}, {"text": "On both tasks, our models are shown to be more powerful than the models relying on entity matching only.", "labels": [], "entities": []}, {"text": "Moreover, for sentence ordering, world knowledge is shown to be especially useful on short documents.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.789037436246872}]}], "datasetContent": [{"text": "To evaluate the impact of incorporating semantic relatedness, we conduct experiments on two datasets, each of which resembles areal sub-task in modeling text coherence: sentence ordering and summary coherence rating.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 169, "end_pos": 186, "type": "TASK", "confidence": 0.7073512077331543}]}, {"text": "Since text coherence is a relative concept rather than a binary distinction, in both tasks, we formulate the problem as pairwise preference ranking.", "labels": [], "entities": []}, {"text": "Specifically, given a set of texts with different degrees of coherence, we train a ranker to prefer the more coherent text over the less coherent one.", "labels": [], "entities": []}, {"text": "Performance is therefore measured as the fraction of correct pairwise rankings as recognized by the ranker.", "labels": [], "entities": []}, {"text": "We use SVM light) with the ranking configuration to train and evaluate our models, with all parameters set to default values.", "labels": [], "entities": []}, {"text": "On both tasks, across two frameworks, supervised and unsupervised, we directly compare our modified model against the original one, i.e., B&L in the supervised framework and G&S in the unsupervised framework.", "labels": [], "entities": []}, {"text": "In our experiments, we use the Stanford parser) to automatically extract the grammatical role for each entity mention.", "labels": [], "entities": []}, {"text": "In this section, we demonstrate the performance of our models with world knowledge encoded in one of the two ways: paths in a sentence graph or features in an entity grid.", "labels": [], "entities": []}, {"text": "We compare our models against the original graph-based model (G&S) and entitybased model (B&L).", "labels": [], "entities": []}, {"text": "The evaluation is conducted on the two tasks, sentence ordering and summary coherence rating, and the accuracy is the fraction of correct pairwise rankings.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7723385095596313}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.999576985836029}]}, {"text": "shows the performance of various models on both tasks.", "labels": [], "entities": []}, {"text": "The first section shows the results of G&S's graph-based local coherence model, including the performance reported in their original paper and that achieved by our re-implementation, repre-: Accuracies (%) of various models on the two tasks, sentence ordering (SO) and summary coherence rating (SCR).", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 191, "end_pos": 201, "type": "METRIC", "confidence": 0.9363359808921814}, {"text": "sentence ordering (SO", "start_pos": 242, "end_pos": 263, "type": "TASK", "confidence": 0.6950469315052032}, {"text": "summary coherence rating (SCR)", "start_pos": 269, "end_pos": 299, "type": "METRIC", "confidence": 0.8549696505069733}]}, {"text": "Models that perform significantly better than their corresponding reimplemented basic models are denoted by ** (p < .01) or * (p < .05), verified using paired t-test.", "labels": [], "entities": []}, {"text": "senting the effect with no world knowledge encoded.", "labels": [], "entities": []}, {"text": "The second section shows the performance of our two graph-based models with world knowledge encoded.", "labels": [], "entities": []}, {"text": "Graph model + K is the basic model with world knowledge encoded, but coherence is simply measured as the average out-degree as in G&S's approach.", "labels": [], "entities": []}, {"text": "Graph model + K + Avg R replaces the out-degree measurement by our average reachability score (described in Section 4.1.3), which measures coherence in a more sophisticated way.", "labels": [], "entities": [{"text": "Avg R", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.9451383352279663}]}, {"text": "The third section shows the results of B&L's entity-based local coherence model, including the originally reported performance and that obtained by our reimplementation, in which no world knowledge features are included.", "labels": [], "entities": [{"text": "B&L", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.8651045759518942}]}, {"text": "The last section, Entity model + K, shows the result of entity-based model with our world knowledge features encoded.", "labels": [], "entities": []}, {"text": "Note that the random baseline of both tasks is 50%.", "labels": [], "entities": []}, {"text": "Firstly, for graph-based models, our Graph model + K outperforms the original models, suggesting that world knowledge is truly helpful for capturing more coherence information 3 . Moreover, by intro-3 The large discrepancy between the performance reported by G&S and that of our re-implementation in Task 2 is due to the fact that G&S experimented with a set of specially formed summary pairs (see their paper for detail), which we have no access to.", "labels": [], "entities": []}, {"text": "They also did not give sufficient details about how they constructed those summary pairs, which has a great impact on the final result.", "labels": [], "entities": []}, {"text": "This made it difficult for us to fully reimplement their experiment.", "labels": [], "entities": []}, {"text": "So we use B&L's set of summary pairs, which are generated randomly and are more difficult to distinguish, which explains our differing results from theirs.", "labels": [], "entities": [{"text": "B&L's set", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.8639877915382386}]}, {"text": "ducing the scoring scheme of average reachability score, our Graph model + K + Avg R achieves the best performance among all graph-based models.", "labels": [], "entities": [{"text": "Avg R", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9492153525352478}]}, {"text": "Secondly, for entity-based models, our enhanced model with knowledge features encoded also achieves superior performance than our reimplemented model, again confirming the usefulness of world knowledge.", "labels": [], "entities": []}, {"text": "Interestingly, we observe that our re-implementation obtains higher accuracy compared to the performance reported by B&L.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9993457198143005}, {"text": "B&L", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.9207221865653992}]}, {"text": "This is partly due to the fact that the documents in our dataset have an average length of 31.5 sentences, which are longer than those used in B&L's experiments.", "labels": [], "entities": [{"text": "B&L", "start_pos": 143, "end_pos": 146, "type": "DATASET", "confidence": 0.8700491189956665}]}, {"text": "We will further discuss this problem in Section 5.4 and show that document length is an important factor to the overall performance.", "labels": [], "entities": []}, {"text": "However, on the task of summary coherence rating, the difference between our extended models and the original ones is generally not significant, primarily due to the fact that the sample size for this task is too small, i.e., 80 pairwise rankings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracies (%) of various models on  the two tasks, sentence ordering (SO) and sum- mary coherence rating (SCR). Models that per- form significantly better than their corresponding re- implemented basic models are denoted by ** (p <  .01) or * (p < .05), verified using paired t-test.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9968593120574951}, {"text": "sentence ordering (SO", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.6850918456912041}, {"text": "sum- mary coherence rating (SCR)", "start_pos": 89, "end_pos": 121, "type": "METRIC", "confidence": 0.8557193949818611}]}, {"text": " Table 2: Performance of various models with and  without world knowledge in the sentence ordering  task, tested on short documents with 1-5 sentences.", "labels": [], "entities": [{"text": "sentence ordering  task", "start_pos": 81, "end_pos": 104, "type": "TASK", "confidence": 0.8154396017392477}]}]}