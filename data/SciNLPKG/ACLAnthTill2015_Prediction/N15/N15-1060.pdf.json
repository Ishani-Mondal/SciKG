{"title": [{"text": "Towards a standard evaluation method for grammatical error detection and correction", "labels": [], "entities": [{"text": "grammatical error detection and correction", "start_pos": 41, "end_pos": 83, "type": "TASK", "confidence": 0.7655931353569031}]}], "abstractContent": [{"text": "We present a novel evaluation method for grammatical error correction that addresses problems with previous approaches and scores systems in terms of improvement on the original text.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.727344830830892}]}, {"text": "Our method evaluates corrections at the token level using a globally optimal alignment between the source, a system hypothesis, and a reference.", "labels": [], "entities": []}, {"text": "Unlike the M 2 Scorer, our method provides scores for both detection and correction and is sensitive to different types of edit operations.", "labels": [], "entities": [{"text": "M 2 Scorer", "start_pos": 11, "end_pos": 21, "type": "DATASET", "confidence": 0.6989312966664633}, {"text": "detection and correction", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.6399489541848501}]}], "introductionContent": [{"text": "A range of methods have been applied to evaluation of grammatical error correction, but no entirely satisfactory method has emerged as yet.", "labels": [], "entities": [{"text": "evaluation of grammatical error correction", "start_pos": 40, "end_pos": 82, "type": "TASK", "confidence": 0.7696633338928223}]}, {"text": "Standard metrics (such as accuracy, precision, recall and F -score) have been used, but they can lead to different results depending on the criteria used for their computation (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.999290943145752}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9981009364128113}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9981854557991028}, {"text": "F -score", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9951652487119039}]}, {"text": "Accuracy, for example, can only be computed in cases where we can enumerate all true negatives, which is why it has been mostly used for article and preposition errors.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9916714429855347}]}, {"text": "Extending this approach to other error types involves the identification of all relevant instances or positions where an error can occur, which is not always easy and renders the evaluation process costly, languagedependent, and possibly inexact.", "labels": [], "entities": []}, {"text": "Accuracy has also been criticised as being a poor indicator of predictive power, especially on unbalanced datasets).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9875615835189819}]}], "datasetContent": [{"text": "A better evaluation method should address the issues described above and use a metric that is meaningful and easy to interpret.", "labels": [], "entities": []}, {"text": "We examine these and other related problems, showing how they can be resolved.", "labels": [], "entities": []}, {"text": "The proposed method uses tokens as the unit of evaluation (instead of phrase-level edits), which provides a stable unit of comparison and facilitates the computation of true negatives.", "labels": [], "entities": []}, {"text": "In turn, this provides a solution for problems 1.(a), 1.(e), 1.(f) and 1.(g).", "labels": [], "entities": []}, {"text": "We tested our evaluation method by re-ranking systems in the CoNLL 2014 shared task on grammatical error correction.", "labels": [], "entities": [{"text": "CoNLL 2014 shared task", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.8753723353147507}, {"text": "grammatical error correction", "start_pos": 87, "end_pos": 115, "type": "TASK", "confidence": 0.5477989514668783}]}, {"text": "Re-ranking was limited to the 12 participating teams that made their system's output publicly available.", "labels": [], "entities": [{"text": "Re-ranking", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9306058883666992}]}, {"text": "For the gold standard, we used the shared task test set containing corrections from the two official annotators as well as alternative corrections provided by three participating teams.", "labels": [], "entities": []}, {"text": "This version allowed us to generate many more references than the original test set and thus reduce annotator bias.", "labels": [], "entities": []}, {"text": "The corrections extracted from the gold standard were automatically clustered into groups of independent errors based on token overlap.", "labels": [], "entities": []}, {"text": "This means that overlapping corrections from different annotators are considered to be mutually exclusive (i.e. alternative) corrections of the same error and are therefore grouped together (the error elements in Listing 1).", "labels": [], "entities": []}, {"text": "Provided the original annotations are correct, the combination of alternatives will generate all possible valid references.", "labels": [], "entities": []}, {"text": "Sentences containing corrections that could not be automatically clustered because they require human knowledge were excluded, leaving a subset of 711 sentences (out of 1,312).", "labels": [], "entities": [{"text": "Sentences containing corrections", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7395235300064087}]}, {"text": "We restrict our analysis to correction, since that is the only aspect reported by the M 2 Scorer.", "labels": [], "entities": [{"text": "correction", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9790674448013306}, {"text": "M 2 Scorer", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.7225873668988546}]}, {"text": "shows the results of the M 2 Scorer using the original annotations as well as a modified version containing mixed-and-matched corrections.", "labels": [], "entities": [{"text": "M 2 Scorer", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.5127741396427155}]}, {"text": "Results of our proposed evaluation method are included in.", "labels": [], "entities": []}, {"text": "As expected, rankings are clearly distinct between the two methods, as they use different units of evaluation (phrase-level edits vs tokens) and maximising metrics (F 0.5 vs WAcc).", "labels": [], "entities": [{"text": "F 0.5 vs WAcc)", "start_pos": 165, "end_pos": 179, "type": "METRIC", "confidence": 0.8587512731552124}]}, {"text": "Results show that only the UFC system is able to beat the baseline (by a small but statistically significant margin), being also the one with consistently highest P (much higher than the rest).", "labels": [], "entities": [{"text": "P", "start_pos": 163, "end_pos": 164, "type": "METRIC", "confidence": 0.9929584264755249}]}, {"text": "These rankings are affected by the fact that systems were probably optimised for F 0.5 during development, as it was the official evaluation metric   for the shared task.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9737066924571991}]}, {"text": "Rankings by F 0.5 are almost identical for the two methods (Spearman's rank correlation is 0.9835 with p < 0.01), suggesting that there is a statistically significant difference between phrase-level edits and tokens, despite phrases being only 1.12 tokens on average in this dataset.", "labels": [], "entities": [{"text": "F", "start_pos": 12, "end_pos": 13, "type": "METRIC", "confidence": 0.9911447167396545}]}, {"text": "Spearman's \u03c1 between both scorers (F 0.5 vs I) is \u22120.5330, which suggests they generally produce inverse rankings.", "labels": [], "entities": [{"text": "F 0.5 vs I)", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.933669102191925}]}, {"text": "Pearson's correlation between token-level F 0.5 and I is \u22120.5942, confirming the relationship between rankings and our intuition that F 0.5 is not a good indicator of overall correction quality.", "labels": [], "entities": [{"text": "F", "start_pos": 42, "end_pos": 43, "type": "METRIC", "confidence": 0.9283490180969238}, {"text": "I", "start_pos": 52, "end_pos": 53, "type": "METRIC", "confidence": 0.9820858240127563}, {"text": "F", "start_pos": 134, "end_pos": 135, "type": "METRIC", "confidence": 0.967445969581604}]}, {"text": "While the I-measure reflects improvement, F 0.5 indicates error manipulation.", "labels": [], "entities": [{"text": "I-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9896894693374634}, {"text": "F 0.5", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9789720475673676}, {"text": "error manipulation", "start_pos": 58, "end_pos": 76, "type": "METRIC", "confidence": 0.920476883649826}]}, {"text": "We argue that I is better suited to the needs of end-users (as it indicates whether the output of the system is better than the original text) whereas F 0.5 is more relevant to system developers (since they need to analyse P and R in order to tune their systems).", "labels": [], "entities": [{"text": "F", "start_pos": 151, "end_pos": 152, "type": "METRIC", "confidence": 0.9196411371231079}]}, {"text": "Lastly, we verify that mixing and matching corrections from different annotators improves R (see) and ensures systems are always assigned the maximum possible score.", "labels": [], "entities": [{"text": "R", "start_pos": 90, "end_pos": 91, "type": "METRIC", "confidence": 0.9864842295646667}]}], "tableCaptions": [{"text": " Table 1: The M 2 Scorer is unable to mix and match corrections from different annotators.", "labels": [], "entities": [{"text": "M 2 Scorer", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.45529810587565106}]}, {"text": " Table 2: Partial matches are ignored by the M 2 Scorer.", "labels": [], "entities": [{"text": "M 2 Scorer", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.5273236334323883}]}, {"text": " Table 3: The M 2 Scorer evaluates systems based on the number of edits, regardless of their length and their effect on  the final corrected sentence. The first hypothesis is better than the second despite having a lower F 0.5 -score.", "labels": [], "entities": [{"text": "M 2 Scorer", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.6595112582047781}, {"text": "F 0.5 -score", "start_pos": 221, "end_pos": 233, "type": "METRIC", "confidence": 0.9828815162181854}]}, {"text": " Table 7: S 1 outperforms S 2 in terms of overall F 0.5 but S 2  outperforms S 1 when evaluated on different references.", "labels": [], "entities": [{"text": "F", "start_pos": 50, "end_pos": 51, "type": "METRIC", "confidence": 0.9918445944786072}]}, {"text": " Table 8: An increase in P , R or F does not necessarily translate into an increase in Acc, assuming all systems are  evaluated on the same set of references.", "labels": [], "entities": [{"text": "Acc", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9990825653076172}]}, {"text": " Table 11: Results of our new evaluation method (in percentages). All values of I are statistically significant (two-tailed  paired T-test, p < 0.01).", "labels": [], "entities": [{"text": "I", "start_pos": 80, "end_pos": 81, "type": "METRIC", "confidence": 0.9528205394744873}]}, {"text": " Table 10: M 2 Scorer results (in percentages).", "labels": [], "entities": [{"text": "M 2 Scorer", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.8845646182696024}]}]}