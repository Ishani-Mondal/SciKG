{"title": [{"text": "Lifelong Machine Learning for Topic Modeling and Beyond", "labels": [], "entities": [{"text": "Topic Modeling", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.8701528906822205}]}], "abstractContent": [{"text": "Machine learning has been popularly used in numerous natural language processing tasks.", "labels": [], "entities": [{"text": "Machine learning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7893720865249634}, {"text": "natural language processing tasks", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.71324273198843}]}, {"text": "However, most machine learning models are built using a single dataset.", "labels": [], "entities": []}, {"text": "This is often referred to as one-shot learning.", "labels": [], "entities": []}, {"text": "Although this one-shot learning paradigm is very useful, it will never make an NLP system understand the natural language because it does not accumulate knowledge learned in the past and make use of the knowledge in future learning and problem solving.", "labels": [], "entities": [{"text": "problem solving", "start_pos": 236, "end_pos": 251, "type": "TASK", "confidence": 0.8400672674179077}]}, {"text": "In this thesis proposal , I first present a survey of lifelong machine learning (LML).", "labels": [], "entities": [{"text": "lifelong machine learning (LML)", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.7595893243948618}]}, {"text": "I then narrow down to one specific NLP task, i.e., topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.8535520136356354}]}, {"text": "I propose several approaches to apply lifelong learning idea in topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.7883029580116272}]}, {"text": "Such capability is essential to make an NLP system versatile and holistic.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine learning serves as a prevalent approach for research in many natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.7099049761891365}]}, {"text": "However, most of existing machine learning approaches are built using a single dataset, which is often referred to as one-shot learning.", "labels": [], "entities": []}, {"text": "This kind of one-shot approach is useful but it does not usually perform well to various datasets or tasks.", "labels": [], "entities": []}, {"text": "The main shortcoming of such one-short approach is the lack of continuous learning ability, i.e., learning and accumulating knowledge from past tasks and leveraging the knowledge for future tasks and problem solving in a lifelong manner.", "labels": [], "entities": [{"text": "problem solving", "start_pos": 200, "end_pos": 215, "type": "TASK", "confidence": 0.7154814004898071}]}, {"text": "To overcome the above shortcoming, lifelong machine learning (LML) has attracted researchers' attention.", "labels": [], "entities": [{"text": "lifelong machine learning (LML)", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.799452950557073}]}, {"text": "The term was initially introduced in 1990s.", "labels": [], "entities": []}, {"text": "LML aims to design and develop computational systems and algorithms that learn as humans do, i.e., retaining the results learned in the past, abstracting knowledge from them, and using the knowledge to help future learning.", "labels": [], "entities": []}, {"text": "The motivation is that when faced with anew situation, we humans always use our previous experience and learned knowledge to help deal with and learn from the new situation, i.e., we learn and accumulate knowledge continuously.", "labels": [], "entities": []}, {"text": "The same rationale can be applied to computational models.", "labels": [], "entities": []}, {"text": "When a model is built using a single dataset fora task, its performance is limited.", "labels": [], "entities": []}, {"text": "However, if the model sees more datasets from the same or similar tasks, it should be able to adjust its learning algorithm for better performance.", "labels": [], "entities": []}, {"text": "There are four components in a LML framework: knowledge representation, knowledge extraction, knowledge transfer, and knowledge retention and maintenance.", "labels": [], "entities": [{"text": "knowledge representation", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.7325666844844818}, {"text": "knowledge extraction", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.7472668290138245}, {"text": "knowledge transfer", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7650423645973206}, {"text": "knowledge retention and maintenance", "start_pos": 118, "end_pos": 153, "type": "TASK", "confidence": 0.6833396181464195}]}, {"text": "These components are closely connected.", "labels": [], "entities": []}, {"text": "I will illustrate each component using examples from topic modeling in Section 3.", "labels": [], "entities": []}, {"text": "Compared to the significant progress of machine learning theory and algorithm, there is relatively little study on lifelong machine learning.", "labels": [], "entities": [{"text": "machine learning theory", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.7116314371426901}]}, {"text": "One of the most notable works is Never-Ending Language Learner (NELL) which was proposed to extractor read information from the web to expand the knowledge base in an endless manner, aiming to achieve better performance in each day than the previous day.", "labels": [], "entities": [{"text": "Never-Ending Language Learner (NELL)", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.49388423562049866}, {"text": "extractor read information from the web", "start_pos": 92, "end_pos": 131, "type": "TASK", "confidence": 0.8571846385796865}]}, {"text": "Recently, we proposed lifelong Topic Modeling (LTM) that extracts knowledge from topic modeling results of many domains and utilizes the knowledge to generate coherent topics in the new domains).", "labels": [], "entities": [{"text": "Topic Modeling (LTM)", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.7913832604885102}]}, {"text": "In (, the authors proposed a method that tackles online multi-task learning in the lifelong learning setting. Some other LML related works include).", "labels": [], "entities": []}, {"text": "Note that LML is different from transfer learning which usually considers one single source domain where the knowledge is coming from and one target domain where the knowledge is applied on.", "labels": [], "entities": []}, {"text": "In this thesis proposal, I narrow down the scope and focus on LML in topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.6993730664253235}]}, {"text": "Topic modeling has been successfully applied to extract semantic topics from text data.", "labels": [], "entities": [{"text": "Topic modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8312605917453766}]}, {"text": "However, the majority of existing topic models (one exception is the LTM model mentioned before) belong to the one-shot approach, i.e., they are proposed to address a specific problem without any knowledge accumulation.", "labels": [], "entities": []}, {"text": "To leverage the idea of LML, I propose several new approaches to advance topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.7130549103021622}]}, {"text": "I believe that the proposed approaches can significantly advance LML in topic modeling.", "labels": [], "entities": [{"text": "LML", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9626728296279907}, {"text": "topic modeling", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.8251137435436249}]}, {"text": "More broadly, this thesis proposal aims to encourage the community to apply LML in a variety of NLP tasks.", "labels": [], "entities": []}, {"text": "This thesis proposal makes the following three contributions:).", "labels": [], "entities": []}, {"text": "In general, topic models assume that each document is a multinomial distribution over topics, where each topic is a multinomial distribution over words.", "labels": [], "entities": []}, {"text": "The two types of distributions in topic modeling are document-topic distributions and topic-word distributions respectively.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.8708370625972748}]}, {"text": "The intuition is that words are more or less likely to be present given the topics of a document.", "labels": [], "entities": []}, {"text": "For example, \"sport\" and \"player\" will appear more often in documents about sports, \"rain\" and \"cloud\" will appear more frequently in documents about weather.", "labels": [], "entities": []}, {"text": "My work is mainly related to knowledge-based topic models) which incorporate different types of prior knowledge into topic models.", "labels": [], "entities": []}, {"text": "Supervised label information was considered in.", "labels": [], "entities": []}, {"text": "Some works also enable the user to specify prior knowledge as seed words/terms for some topics.", "labels": [], "entities": []}, {"text": "Interactive topic modeling was proposed in () to improve topics with the interactive help from the user.", "labels": [], "entities": [{"text": "Interactive topic modeling", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6369064350922903}]}, {"text": "However, these works require labeled data or user manual guidance while my proposed approaches do not.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section proposes anew evaluation framework that suits our proposed approaches.", "labels": [], "entities": []}, {"text": "In (Chen and Liu, 2014b), the evaluation measurements are Topic Coherence () and Precision@n which asks annotators to label both topics and words.", "labels": [], "entities": [{"text": "Precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.960316002368927}]}, {"text": "A more comprehensive evaluation framework can contain the following two measurements: 1.", "labels": [], "entities": []}, {"text": "In order to evaluate each piece of knowledge (must-link or cannot-link) in the knowledge base, PMI score of both words using a large standard text corpus) can be applied.", "labels": [], "entities": [{"text": "PMI score", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.8753113746643066}]}, {"text": "Human annotation can also be used to label the correctness of each piece of knowledge.", "labels": [], "entities": []}, {"text": "This is to evaluate the effectiveness of knowledge handling in the model.", "labels": [], "entities": [{"text": "knowledge handling", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.6943923830986023}]}, {"text": "As mentioned in 5.3, not all the prior domains are suitable to anew domain.", "labels": [], "entities": []}, {"text": "It is important to evaluate the model performance by providing different sets of prior domains.", "labels": [], "entities": []}, {"text": "There could be three main sets of prior domains for an extensive evaluation: 1) all relevant; 2) all irrelevant; 3) a combination of both.", "labels": [], "entities": []}, {"text": "The relevance of domains should be defined by experts that are familiar with these domains.", "labels": [], "entities": []}], "tableCaptions": []}