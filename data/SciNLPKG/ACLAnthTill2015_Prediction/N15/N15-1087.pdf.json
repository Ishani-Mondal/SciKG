{"title": [], "abstractContent": [{"text": "Automatic analysis of impaired speech for screening or diagnosis is a growing research field; however there are still many barriers to a fully automated approach.", "labels": [], "entities": [{"text": "Automatic analysis of impaired speech", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7912804901599884}, {"text": "screening or diagnosis", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.6394560039043427}]}, {"text": "When automatic speech recognition is used to obtain the speech transcripts, sentence boundaries must be inserted before most measures of syntactic complexity can be computed.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7486938834190369}]}, {"text": "In this paper, we consider how language impairments can affect segmentation methods, and compare the results of computing syntactic complexity met-rics on automatically and manually segmented transcripts.", "labels": [], "entities": []}, {"text": "We find that the important boundary indicators and the resulting segmentation accuracy can vary depending on the type of impairment observed, but that results on patient data are generally similar to control data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9063145518302917}]}, {"text": "We also find that a number of syntactic complexity metrics are robust to the types of seg-mentation errors that are typically made.", "labels": [], "entities": []}], "introductionContent": [{"text": "The automatic analysis of speech samples is a promising direction for the screening and diagnosis of cognitive impairments.", "labels": [], "entities": []}, {"text": "For example, recent studies have shown that machine learning classifiers trained on speech and language features can detect, with reasonably high accuracy, whether a speaker has mild cognitive impairment), frontotemporal lobar degeneration (), primary progressive aphasia (), or Alzheimer's disease ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9935358762741089}]}, {"text": "These studies used manually transcribed samples of patient speech; however, it is turning to politics for al gore and george w bush another day of rehearsal in just over forty eight hours the two men will face off in their first of three debates for the first time voters will get a live unfiltered view of them together Turning to politics, for Al Gore and George W Bush another day of rehearsal.", "labels": [], "entities": []}, {"text": "In just over forty-eight hours the two men will face off in their first of three debates.", "labels": [], "entities": []}, {"text": "For the first time, voters will get a live, unfiltered view of them together.", "labels": [], "entities": []}, {"text": "clear that for such systems to be practical in the real world they must use automatic speech recognition (ASR).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 76, "end_pos": 110, "type": "TASK", "confidence": 0.7170177102088928}]}, {"text": "One issue that arises with ASR is the introduction of word recognition errors: insertions, deletions, and substitutions.", "labels": [], "entities": [{"text": "ASR", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.990990161895752}, {"text": "word recognition", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7703198194503784}]}, {"text": "This problem as it relates to impaired speech has been considered elsewhere), although more work is needed.", "labels": [], "entities": []}, {"text": "Another issue, which we address here, is how ASR transcripts are divided into sentences.", "labels": [], "entities": [{"text": "ASR transcripts", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.9255836009979248}]}, {"text": "The raw output from an ASR system is generally a stream of words, as shown in.", "labels": [], "entities": [{"text": "ASR", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9792045950889587}]}, {"text": "With some effort, it can be transformed into a format which is more readable by both humans and machines.", "labels": [], "entities": []}, {"text": "Many algorithms exist for the segmentation of the raw text stream into sentences.", "labels": [], "entities": [{"text": "segmentation of the raw text stream into sentences", "start_pos": 30, "end_pos": 80, "type": "TASK", "confidence": 0.7924613580107689}]}, {"text": "However, there has been no previous work on how those algorithms might be applied to impaired speech.", "labels": [], "entities": []}, {"text": "This problem must be addressed for two reasons: first, sentence boundaries are important when analyzing the syntactic complexity of speech, which can be a strong indicator of potential impairment.", "labels": [], "entities": []}, {"text": "Many measures of syntactic complexity are based on properties of the syntactic parse tree (e.g. Yngve depth, tree height), which first require the demarcation of individual sentences.", "labels": [], "entities": [{"text": "Yngve depth", "start_pos": 96, "end_pos": 107, "type": "METRIC", "confidence": 0.6688608974218369}]}, {"text": "Even very basic measures of syntactic complexity, such as the mean length of sentence, require this information.", "labels": [], "entities": []}, {"text": "Secondly, there are many reasons to believe that existing algorithms might not perform well on impaired speech, since assumptions about normal speech do not hold true in the impaired case.", "labels": [], "entities": []}, {"text": "For example, in normal speech, pausing is often used to indicate a boundary between syntactic units, whereas in some types of dementia or aphasia a pause may indicate word-finding difficulty instead.", "labels": [], "entities": []}, {"text": "Other indicators of sentence boundaries, such as prosody, filled pauses, and discourse markers, can also be affected by cognitive impairments.", "labels": [], "entities": []}, {"text": "Here we explore whether we can apply standard approaches to sentence segmentation to impaired speech, and compare our results to the segmentation of broadcast news.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.737940102815628}]}, {"text": "We then extract syntactic complexity features from the automatically segmented text, and compare the feature values with measurements taken on manually segmented text.", "labels": [], "entities": []}, {"text": "We assess which features are most robust to the noisy segmentation, and thus could be appropriate features for future work on automatic diagnostic interfaces.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: F score for the automatic segmentation  method on each data set. Boldface indicates best in  column.", "labels": [], "entities": [{"text": "F score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9812519252300262}, {"text": "segmentation", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.864433765411377}]}, {"text": " Table 3: Mean values of syntactic complexity metrics for the different patient groups. Features which show  no significant difference between the manual and automatic segmentation on all three clinical groups are  marked as \"NS\" (not significant).", "labels": [], "entities": [{"text": "NS", "start_pos": 226, "end_pos": 228, "type": "METRIC", "confidence": 0.8963841795921326}]}]}