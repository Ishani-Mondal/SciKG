{"title": [{"text": "A Compositional and Interpretable Semantic Space", "labels": [], "entities": []}], "abstractContent": [{"text": "Vector Space Models (VSMs) of Semantics are useful tools for exploring the semantics of single words, and the composition of words to make phrasal meaning.", "labels": [], "entities": []}, {"text": "While many methods can estimate the meaning (i.e. vector) of a phrase, few do so in an interpretable way.", "labels": [], "entities": []}, {"text": "We introduce anew method (CNNSE) that allows word and phrase vectors to adapt to the notion of composition.", "labels": [], "entities": [{"text": "CNNSE", "start_pos": 26, "end_pos": 31, "type": "TASK", "confidence": 0.5977985262870789}]}, {"text": "Our method learns a VSM that is both tailored to support a chosen semantic composition operation, and whose resulting features have an intuitive interpretation.", "labels": [], "entities": []}, {"text": "Interpretability allows for the exploration of phrasal semantics, which we leverage to analyze performance on a behavioral task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector Space Models (VSMs) are models of word semantics typically built with word usage statistics derived from corpora.", "labels": [], "entities": [{"text": "Vector Space Models (VSMs)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5875468552112579}]}, {"text": "VSMs have been shown to closely match human judgements of semantics (for an overview see, Chapter 5), and can be used to study semantic composition.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 127, "end_pos": 147, "type": "TASK", "confidence": 0.706381693482399}]}, {"text": "Composition has been explored with different types of composition functions) including higher order functions (such as matrices) (, and some have considered which corpus-derived information is most useful for semantic composition.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 209, "end_pos": 229, "type": "TASK", "confidence": 0.7525690793991089}]}, {"text": "Still, many VSMs act like a black box -it is unclear what VSM dimensions represent (save for broad classes of corpus statistic types) and what the application of a composition function to those dimensions entails.", "labels": [], "entities": []}, {"text": "Neural network (NN) models are becoming increasingly popular), and some model introspection has been attempted: examined connections between layers, and explored how shifts in VSM space encodes semantic relationships.", "labels": [], "entities": []}, {"text": "Still, interpreting NN VSM dimensions, or factors, remains elusive.", "labels": [], "entities": [{"text": "interpreting NN VSM", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.8027188380559286}]}, {"text": "This paper introduces anew method, Compositional Non-negative Sparse Embedding (CNNSE).", "labels": [], "entities": []}, {"text": "In contrast to many other VSMs, our method learns an interpretable VSM that is tailored to suit the semantic composition function.", "labels": [], "entities": []}, {"text": "Such interpretability allows for deeper exploration of semantic composition than previously possible.", "labels": [], "entities": []}, {"text": "We will begin with an overview of the CNNSE algorithm, and follow with empirical results which show that CNNSE produces: 1.", "labels": [], "entities": [{"text": "CNNSE", "start_pos": 38, "end_pos": 43, "type": "TASK", "confidence": 0.9694981575012207}, {"text": "CNNSE", "start_pos": 105, "end_pos": 110, "type": "TASK", "confidence": 0.7770667672157288}]}, {"text": "more interpretable dimensions than the typical VSM, 2.", "labels": [], "entities": [{"text": "VSM", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.5434089303016663}]}, {"text": "composed representations that outperform previous methods on a phrase similarity task.", "labels": [], "entities": [{"text": "phrase similarity task", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.8120290438334147}]}, {"text": "Compared to methods that do not consider composition when learning embeddings, CNNSE produces: 1.", "labels": [], "entities": [{"text": "CNNSE", "start_pos": 79, "end_pos": 84, "type": "TASK", "confidence": 0.8678730130195618}]}, {"text": "better approximations of phrasal semantics, 2.", "labels": [], "entities": []}, {"text": "phrasal representations with dimensions that more closely match phrase meaning.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the semantic vectors made available by, which were compiled from a 16 billion word subset of.", "labels": [], "entities": []}, {"text": "We used the 1000 dependency SVD dimensions, which were shown to perform well for composition tasks.", "labels": [], "entities": []}, {"text": "Dependency features are tuples consisting of two POS tagged words and their dependency relationship in a sentence; the feature value is the pointwise positive mutual information (PPMI) for the tuple.", "labels": [], "entities": []}, {"text": "The dataset is comprised of 54,454 words and phrases.", "labels": [], "entities": []}, {"text": "We randomly split the approximately 14,000 adjective noun phrases into a train (2/3) and From the test set we removed 200 randomly selected phrases as a development set for parameter tuning.", "labels": [], "entities": []}, {"text": "We did not lexically split the train and test sets, so many words appearing in training phrases also appear in test phrases.", "labels": [], "entities": []}, {"text": "For this reason we cannot make specific claims about the generalizability of our methods to unseen words.", "labels": [], "entities": []}, {"text": "NNSE has one parameter to tune (\u03bb 1 ); CNNSE has two: \u03bb 1 and \u03bb c . In general, these methods are not overly sensitive to parameter tuning, and searching over orders of magnitude will suffice.", "labels": [], "entities": [{"text": "NNSE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9575197100639343}, {"text": "CNNSE", "start_pos": 39, "end_pos": 44, "type": "TASK", "confidence": 0.511701226234436}, {"text": "parameter tuning", "start_pos": 122, "end_pos": 138, "type": "TASK", "confidence": 0.7454326450824738}]}, {"text": "We found the optimal settings for NNSE were \u03bb 1 = 0.05, and for CNNSE \u03bb 1 = 0.05, \u03bb c = 0.5.", "labels": [], "entities": [{"text": "NNSE", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.6118471026420593}, {"text": "CNNSE", "start_pos": 64, "end_pos": 69, "type": "TASK", "confidence": 0.6455510854721069}]}, {"text": "Too large \u03bb 1 leads to overly sparse solutions, too small reduces interpretability.", "labels": [], "entities": []}, {"text": "We set = 1000 for both NNSE and CNNSE and altered sparsity by tuning only \u03bb 1 .  We now compare the performance of various composition methods on an adjective-noun phrase similarity dataset.", "labels": [], "entities": [{"text": "CNNSE", "start_pos": 32, "end_pos": 37, "type": "TASK", "confidence": 0.4149344563484192}, {"text": "sparsity", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9636616110801697}]}, {"text": "This dataset is comprised of 108 adjective-noun phrase pairs split into high, medium and low similarity groups.", "labels": [], "entities": []}, {"text": "Similarity scores from 18 human subjects are averaged to create one similarity score per phrase pair.", "labels": [], "entities": []}, {"text": "We then compute the cosine similarity between the composed phrasal representations of each phrase pair under each compositional model.", "labels": [], "entities": []}, {"text": "As in, we report the correlation of the cosine similarity measures to the behavioral scores.", "labels": [], "entities": []}, {"text": "We withheld 12 of the 108 questions for parameter tuning, four randomly selected from each of the high, medium and low similarity groups.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.8061635792255402}]}, {"text": "shows the correlation of each model's similarity scores to behavioral similarity scores.", "labels": [], "entities": []}, {"text": "Again, Lexfunc performs poorly.", "labels": [], "entities": [{"text": "Lexfunc", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.7627398371696472}]}, {"text": "This is probably attributable to the fact that there are, on average, only 39 phrases available for training each adjective in the dataset, whereas the original Lexfunc study had at least 50 per adjective (.", "labels": [], "entities": [{"text": "Lexfunc study", "start_pos": 161, "end_pos": 174, "type": "DATASET", "confidence": 0.9631782472133636}]}, {"text": "CNNSE is the top performer, followed closely by weighted addition.", "labels": [], "entities": [{"text": "CNNSE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.736541211605072}]}, {"text": "Interestingly, weighted NNSE correlation is lower than CNNSE by nearly 0.15, which shows the value of allowing the learned latent space to conform to the desired composition function.", "labels": [], "entities": [{"text": "correlation", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.5344935059547424}, {"text": "CNNSE", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.8036010265350342}]}], "tableCaptions": [{"text": " Table 2: Median rank, mean reciprocal rank (MRR)  and percentage of test phrases ranked perfectly (i.e.  first in a sorted list of approx. 4,600 test phrases)  for four methods of estimating the test phrase vec- tors. w.add SVD is weighted addition of SVD vectors,  w.add NNSE is weighted addition of NNSE vectors.", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 23, "end_pos": 49, "type": "METRIC", "confidence": 0.9474790592988332}]}, {"text": " Table 3: A comparison of mistakes in phrase rank- ing across 4 composition methods. To evaluate mis- takes, we chose phrases for which all 4 models rank  a different (incorrect) phrase first. Mturk users were  asked to identify the phrase that was semantically  closest to the target phrase.", "labels": [], "entities": [{"text": "phrase rank- ing", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7825164049863815}]}, {"text": " Table 4: Quantifying the interpretability of learned  semantic representations via the intruder task. In- truders detected: % of questions for which the ma- jority response was the intruder. Mturk agreement:  the % of questions for which a majority of users  chose the same response.", "labels": [], "entities": [{"text": "interpretability of learned  semantic representations", "start_pos": 26, "end_pos": 79, "type": "TASK", "confidence": 0.7565964221954345}]}, {"text": " Table 4.  Consistent with previous studies, NNSE provides a  much more interpretable latent representation than  SVD. We find that the additional composition con- straint used in CNNSE has maintained the inter- pretability of the learned latent space. Because in- truders detected is higher for CNNSE, but agreement  amongst Mturk users is higher for NNSE, we con- sider the interpretability results for the two methods  to be equivalent. Note that SVD interpretability is  close to chance (1/6 = 16.7%).", "labels": [], "entities": [{"text": "CNNSE", "start_pos": 180, "end_pos": 185, "type": "TASK", "confidence": 0.8683692216873169}, {"text": "SVD interpretability", "start_pos": 450, "end_pos": 470, "type": "TASK", "confidence": 0.8746311068534851}]}, {"text": " Table 5: Comparing the coherence of phrase rep- resentations from CNNSE and NNSE. Mturk users  were shown the interpretable summarization for the  top scoring dimension of target phrases. Represen- tations from CNNSE and NNSE were shown side by  side and users were asked to choose the list (summa- rization) most related to the phrase, or that the lists  were equally good or bad.", "labels": [], "entities": [{"text": "NNSE", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8839470744132996}]}, {"text": " Table 6: Correlation of phrase similarity judgements  (Mitchell and Lapata, 2010) to pairwise distances in  several adjective-noun composition models.", "labels": [], "entities": []}]}