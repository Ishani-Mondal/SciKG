{"title": [{"text": "Echoes of Persuasion: The Effect of Euphony in Persuasive Communication", "labels": [], "entities": []}], "abstractContent": [{"text": "While the effect of various lexical, syntactic, semantic and stylistic features have been addressed in persuasive language from a computational point of view, the persuasive effect of phonetics has received little attention.", "labels": [], "entities": []}, {"text": "By modeling a notion of euphony and analyzing four datasets comprising persuasive and non-persuasive sentences in different domains (po-litical speeches, movie quotes, slogans and tweets), we explore the impact of sounds on different forms of persuasiveness.", "labels": [], "entities": []}, {"text": "We conduct a series of analyses and prediction experiments within and across datasets.", "labels": [], "entities": []}, {"text": "Our results highlight the positive role of phonetic devices on persuasion.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Phonetic N-Gram All CORPS 0.589 (-, 1) 0.733 * * * (4k, 1) 0.736 \u2020 (2k, 1) Movie 0.600 (-, 2) 0.694 * * * (1k, 1) 0.722 * * * (1k, 1) Slogan 0.700 (-, 2) 0.826 * * * (3k, 1) 0.883 * * * (5k, 1) Twitter 0.563 (-, 2) 0.732 * * * (5k, 1) 0.745 * * * (4k, 1): Results of the within-dataset experiments.", "labels": [], "entities": []}, {"text": "In this section, we describe the prediction tasks (both within and across datasets) that we carried out to in-vestigate the impact of the phonetic features on the detection of various forms of persuasiveness.", "labels": [], "entities": []}, {"text": "We compare three different sets of features, namely phonetic, n-grams and their combination to understand whether phonetic information can improve the performance of standard lexical approaches.", "labels": [], "entities": []}, {"text": "Similarly to Danescu-Niculescu-Mizil et al. and, we formulate a pairwise classification problem such that given a pair (s 1 , s 2 ) consisting of sentences s 1 and s 2 , the goal is to determine the more persuasive one (i.e., the one on the left or right).", "labels": [], "entities": []}, {"text": "We can consider this as a binary classification task where for each instance (i.e., pair) the possible labels are left or right.", "labels": [], "entities": []}, {"text": "For the prediction experiments, we used the four datasets described in Section 4 (i.e., CORPS, Twitter, Slogan and Movie), all of which consist of a persuasive sentence P and its non-persuasive counterpart (\u00acP ) labeled as either left or right.", "labels": [], "entities": []}, {"text": "To make the positions of the sentences in a pair irrelevant (i.e. to provide symmetry), for each instance occurring in the original datasets (e.g., (s 1 , s 2 ) with label left), we added another instance including the same sentence pair in reverse order (i.e., (s 2 , s 1 ) with label right).", "labels": [], "entities": []}, {"text": "As a preprocessing step, all the sentences were tokenized by using Stanford CoreNLP (Manning et al., 2014).", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.9128004312515259}]}, {"text": "For this set of experiments, we conducted a 10-fold cross validation on each dataset separately.", "labels": [], "entities": []}, {"text": "In Table 5, for each dataset listed in the first column, in the subsequent columns we report the performance of the best model obtained with 10-fold cross validation using i) only phonetic features (Phonetic), ii) only n-grams (N-Gram), iii) both phonetic and ngram features (All).", "labels": [], "entities": []}, {"text": "As mentioned previously, for each pair (s 1 , s 2 ) consisting of sentences s 1 and s 2 , our dataset contains another pair including the same sentences in reverse order (i.e., (s 2 , s 1 )), resulting in asymmetric and balanced dataset.", "labels": [], "entities": []}, {"text": "Therefore, classification performance is measured in terms of accuracy (i.e., the percentage of pairs of which labels were correctly predicted).", "labels": [], "entities": [{"text": "classification", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9689381122589111}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9990635514259338}]}, {"text": "For each accuracy value, we also report in parenthesis the number of features selected and the kernel degree of the corresponding model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9989873766899109}]}, {"text": "While the kernel degree did not make a big difference in the performance, the number of selected features had an important effect on the accuracy of the models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.999087929725647}]}, {"text": "As can be observed from these values, the best performance on all the datasets is achieved with a relatively small number of features.", "labels": [], "entities": []}, {"text": "Among the values reported in the table, the ones followed by * * * are significantly different (p < .001) Dataset N-Gram N-Gram+Rhyme N-Gram+Plosive N-Gram+Homogeneity N-Gram+Alliteration: Results of the cross-dataset prediction experiments optimized on the training set. from the ones to their left, while \u2020 represents no significance, as calculated according to McNemar's test.", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 364, "end_pos": 378, "type": "DATASET", "confidence": 0.8713523546854655}]}, {"text": "For each dataset, the weakest models (i.e. the ones using only the phonetic features in all cases) are still significantly (p < .001) more accurate than a random baseline (accuracy = 50%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9989277720451355}]}, {"text": "As can be observed from the table, the models using only n-grams significantly outperform the ones only based on phonetic features in all datasets.", "labels": [], "entities": []}, {"text": "However, while the phonetic features are not very strong by themselves, their combination with ngrams results in models outperforming the n-gram based models in all cases.", "labels": [], "entities": []}, {"text": "The difference is highly significant for all datasets except CORPS, where ngrams alone are sufficient to achieve a good performance.", "labels": [], "entities": [{"text": "CORPS", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.8992916941642761}]}, {"text": "We speculate that the kind of persuasiveness used in political speeches is more dependent on the lexical choices of the speaker and on the use of a specific set of semantically loaded words such as bless, victory, God and justice or military.", "labels": [], "entities": []}, {"text": "This is inline with the work of, who built a domain specific lexicon to study the persuasive impact of words in political speeches.", "labels": [], "entities": []}, {"text": "We also conducted an additional set of experiments to investigate if some phonetic features standout among the others, and to find out the contribution and importance of each phonetic feature in isolation.", "labels": [], "entities": []}, {"text": "To achieve that, for each dataset we conducted a 10-fold cross validation to obtain the best four models containing a single phonetic feature on top of n-gram features (i.e. N-Gram+Rhyme, N-Gram+Plosive, N-Gram+Homogeneity and NGram+Alliteration).", "labels": [], "entities": []}, {"text": "In, we report the accuracy of the n-gram model and these four models for each dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9996058344841003}]}, {"text": "Similarly to, for each accuracy value, we also report in parenthesis the number of features selected and the kernel degree of the corresponding model obtained with grid search.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9991212487220764}]}, {"text": "The results demonstrate that homegeneity is the most effective feature when added on top of n-grams, resulting in highly significant improvement against the basic n-gram models in three out of four datasets.", "labels": [], "entities": []}, {"text": "Alliteration and rhyme closely follow homogeneity by yielding models that significantly outperform the ngram models in three and two datasets respectively.", "labels": [], "entities": [{"text": "rhyme", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.9738792181015015}]}, {"text": "Finally, the models containing plosives do not improve over the n-gram models in any of the four datasets.", "labels": [], "entities": []}, {"text": "It is worth noting that in CORPS none of the n-gram models enriched with phonetic features improves over the basic n-gram models as inline with the results of the within-dataset experiments reported in.", "labels": [], "entities": [{"text": "CORPS", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.7586111426353455}]}, {"text": "After observing that the combination of phonetic and n-gram features can be effective in the withindataset prediction experiments, we took a further step and investigated the interaction of the three feature sets across datasets.", "labels": [], "entities": [{"text": "withindataset prediction", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.7743306159973145}]}, {"text": "More specifically, we classified each dataset with the best models (one for each feature set) trained on the other datasets.", "labels": [], "entities": []}, {"text": "With these experiments, we investigated the ability of phonetic features to generalize across the different lexicons of the datasets.", "labels": [], "entities": []}, {"text": "As we discussed previously, the four datasets represent different forms of persuasiveness.", "labels": [], "entities": []}, {"text": "In this respect, the results of the cross-dataset experiments can also be interpreted as a measure of the degree of compatibility among these kinds of persuasiveness.", "labels": [], "entities": []}, {"text": "In, we present the results of the crossdataset prediction experiments.", "labels": [], "entities": [{"text": "crossdataset prediction", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.8414847254753113}]}, {"text": "For each training and test set pair, we report the accuracy of the best models, one for each feature set, based on crossvalidation on the training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9992778897285461}]}, {"text": "As can be observed from the table, the figures are generally low and various domain adaptation techniques could be employed to improve the results.", "labels": [], "entities": []}, {"text": "However, the objective of this evaluation is not to train an optimized cross-domain classifier, but to assess the potential of the feature sets to model different kinds of persuasiveness.", "labels": [], "entities": []}, {"text": "As expected, n-gram features show poor performance due to the lexical and stylistic differences among the datasets.", "labels": [], "entities": []}, {"text": "In many cases, the phonetic models outperform the n-gram models, and in several cases the combination of the two feature sets deteriorates the performance of the phonetic features alone.", "labels": [], "entities": []}, {"text": "These findings support our hypothesis that phonetic features, due to their generality, have better correlation with different forms of persuasiveness than lexical features.", "labels": [], "entities": []}, {"text": "The experiments involving the CORPS dataset, both for training and testing, do not share this behavior.", "labels": [], "entities": [{"text": "CORPS dataset", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.9203272461891174}]}, {"text": "Indeed, when CORPS is used as a training or test dataset, the performance of the models is quite low (very close to or worse than the baseline in many cases) independently from the feature sets.", "labels": [], "entities": []}, {"text": "These results suggest that the notion of persuasiveness encoded in this dataset is remarkably different from the others, as previously discussed in the data analysis in Section 5.", "labels": [], "entities": []}, {"text": "As seen in the within dataset experiments (see), CORPS is the only dataset in which the combination of lexical and phonetic features do not improve the classification accuracy.", "labels": [], "entities": [{"text": "CORPS", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.7447441816329956}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.8858529925346375}]}, {"text": "This explains the inability of the phonetic features to improve the accuracy in cross-dataset experiments when this dataset is employed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9992166757583618}]}], "tableCaptions": [{"text": " Table 1: Criteria used in the construction of each  dataset and average token length of persuasive and non- persuasive pairs", "labels": [], "entities": []}, {"text": " Table 2: Euphonic examples of persuasive sentences from each dataset, along with their phonetic scores.", "labels": [], "entities": []}, {"text": " Table 3: Average phonetic scores for our datasets -***, p < .001; **, p < .01; *, p < .05;  \u2020, not significant", "labels": [], "entities": []}, {"text": " Table 4: Probability of examples above threshold, -***,  p < .001; **, p < .01; *, p < .05;  \u2020, not significant", "labels": [], "entities": []}, {"text": " Table 6: Contribution of the phonetic features.", "labels": [], "entities": [{"text": "Contribution", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.8392861485481262}]}, {"text": " Table 7: Results of the cross-dataset prediction experiments optimized on the training set.", "labels": [], "entities": [{"text": "cross-dataset prediction", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.7668849527835846}]}]}