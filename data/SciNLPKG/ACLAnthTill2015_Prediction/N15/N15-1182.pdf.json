{"title": [{"text": "Shared common ground influences information density in microblog texts", "labels": [], "entities": []}], "abstractContent": [{"text": "If speakers use language rationally, they should structure their messages to achieve approximately uniform information density (UID), in order to optimize transmission via a noisy channel.", "labels": [], "entities": [{"text": "information density (UID)", "start_pos": 107, "end_pos": 132, "type": "METRIC", "confidence": 0.6135600328445434}]}, {"text": "Previous work identified a consistent increase in linguistic information across sentences in text as a signature of the UID hypothesis.", "labels": [], "entities": []}, {"text": "This increase was derived from a predicted increase in context, but the context itself was not quantified.", "labels": [], "entities": []}, {"text": "We use microblog texts from Twitter, tied to a single shared event (the baseball World Series), to quantify both linguistic and non-linguistic context.", "labels": [], "entities": []}, {"text": "By tracking changes in contextual information, we predict and identify gradual and rapid changes in information content in response to in-game events.", "labels": [], "entities": []}, {"text": "These findings lend further support to the UID hypothesis and highlights the importance of non-linguistic common ground for language production and processing.", "labels": [], "entities": [{"text": "UID", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9005808234214783}]}], "introductionContent": [{"text": "There are many ways express a given message in natural language, so how do speakers decide between potential structures?", "labels": [], "entities": []}, {"text": "One prominent hypothesis is that they aim for structures that best convey the intendeed message in the context of the communication.", "labels": [], "entities": []}, {"text": "On this view, the use of natural languages is assumed to follow optimal information transmission results from information theory.", "labels": [], "entities": []}, {"text": "In particular, speakers should structure their messages to approximate uniform information density across symbols (words and phonemes), which is optimal for transmission of information through a noisy channel.", "labels": [], "entities": []}, {"text": "At least three lines of evidence suggest that speakers do make choices to increase the uniformity of information density across their utterances.", "labels": [], "entities": []}, {"text": "First, speakers phonologically reduce more predictable material (.", "labels": [], "entities": []}, {"text": "Second, they omit or reduce optional lexical material in cases where the subsequent syntactic information is relatively more predictable (.", "labels": [], "entities": []}, {"text": "Third, and most relevant to our current hypothesis, speakers appear to increase the complexity of their utterances as a discourse develops (.", "labels": [], "entities": []}, {"text": "We expand on this finding below.", "labels": [], "entities": []}, {"text": "Following the UID hypothesis, proposed that H(Y i ), the total entropy of part i of a message (e.g., a word) is constant.", "labels": [], "entities": []}, {"text": "They compute this expression by considering X i , the random variable representing the precise word that will appear at position i, conditioned on all the previously observed words.", "labels": [], "entities": []}, {"text": "They then further factor this expression into two terms: where the first term H(X i |L i ) is the dependence of the current word on only the local linguistic context (e.g. within the rest of the sentence Li ) and the second is the mutual information between the current word and the broader linguistic context Ci , given the rest of the current sentence.", "labels": [], "entities": []}, {"text": "On their logic, with greater amounts of contextual information, the predictability of linguistic material based on context, I(X i |C i , Li ), must go up.", "labels": [], "entities": []}, {"text": "Therefore, they predicted that H(X i |L i ) should also increase, so as to maintain a constant total amount of information.", "labels": [], "entities": [{"text": "H", "start_pos": 31, "end_pos": 32, "type": "METRIC", "confidence": 0.9714341759681702}]}, {"text": "Genzel and Charniak then approximated H(X i |L i ) using a number of methods and showed that it did increase systematically in documents.", "labels": [], "entities": [{"text": "H", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.8834905028343201}, {"text": "it", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9785812497138977}]}, {"text": "Later work showed that this increase was strongest within paragraphs and was general across document types and languages.", "labels": [], "entities": []}, {"text": "This work, however, did not attempt to measure shared context (and its influence on message expectations) directly.", "labels": [], "entities": []}, {"text": "This challenge is the focus of our current work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Example tweets, grouped by minutes since the first pitch.", "labels": [], "entities": []}, {"text": " Table 2: Example tweets, grouped by the per-minute tweet rate during each at-bat.", "labels": [], "entities": []}]}