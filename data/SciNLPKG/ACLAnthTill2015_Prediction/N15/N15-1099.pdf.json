{"title": [{"text": "A Word Embedding Approach to Predicting the Compositionality of Multiword Expressions", "labels": [], "entities": [{"text": "Predicting the Compositionality of Multiword Expressions", "start_pos": 29, "end_pos": 85, "type": "TASK", "confidence": 0.8494578699270884}]}], "abstractContent": [{"text": "This paper presents the first attempt to use word embeddings to predict the composition-ality of multiword expressions.", "labels": [], "entities": []}, {"text": "We consider both single-and multi-prototype word em-beddings.", "labels": [], "entities": []}, {"text": "Experimental results show that, in combination with a back-off method based on string similarity, word embeddings out-perform a method using count-based distribu-tional similarity.", "labels": [], "entities": []}, {"text": "Our best results are competitive with, or superior to, state-of-the-art methods over three standard compositionality datasets, which include two types of multi-word expressions and two languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multiword expressions (MWEs) are word combinations that display some form of idiomaticity, including semantic idiomaticity, wherein the semantics of the MWE (e.g. ivory tower) cannot be predicted from the semantics of the component words (e.g. ivory and tower).", "labels": [], "entities": [{"text": "Multiword expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6668211162090302}]}, {"text": "Recent NLP work on semantic idiomaticity has focused on the task of \"compositionality prediction\", in the form of a regression task whereby a given MWE is mapped onto a continuous-valued compositionality score, either for the MWE as a whole or for each of its component words; Schulte im.", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.8114895522594452}]}, {"text": "Separately in NLP, there has been a recent surge of interest in learning distributed representations of word meaning, in the form of \"word embeddings\") and composition over distributed representations ().", "labels": [], "entities": []}, {"text": "This paper is the first attempt to bring together the work on word embedding-style distributional analysis with compositionality prediction of MWEs.", "labels": [], "entities": [{"text": "word embedding-style distributional analysis", "start_pos": 62, "end_pos": 106, "type": "TASK", "confidence": 0.5898958295583725}, {"text": "compositionality prediction", "start_pos": 112, "end_pos": 139, "type": "TASK", "confidence": 0.815806120634079}]}, {"text": "In the context of compositionality prediction, our primary research questions here are: RQ1: Are word embeddings superior to conventional count-based models of distributional similarity?", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.9149152040481567}, {"text": "RQ1", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.5876290202140808}]}, {"text": "RQ2: How sensitive to parameter optimisation are different word embedding approaches?", "labels": [], "entities": [{"text": "RQ2", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8503628969192505}]}, {"text": "RQ3: Are multi-prototype word embeddings empirically superior to single-prototype word embeddings?", "labels": [], "entities": [{"text": "RQ3", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9265381693840027}]}, {"text": "We explore these questions relative to three compositionality prediction datasets spanning two MWE construction types (noun compounds and verb particle constructions) and two languages (English and German), and arrive at the following conclusions: (1) consistent with recent work over other NLP tasks, word embeddings are superior to countbased models of distributional similarity (and also translation-based string similarity); (2) the results are relatively stable under parameter optimisation fora given word embedding learning approach; and (3) based on two simple approaches to composition, single word embeddings are empirically slightly superior to multi-prototype word embeddings overall.", "labels": [], "entities": [{"text": "MWE construction types (noun compounds and verb particle constructions)", "start_pos": 95, "end_pos": 166, "type": "TASK", "confidence": 0.6333077279004183}]}], "datasetContent": [{"text": "We evaluate our methods over three datasets: (1) English noun compounds (\"ENCs\", e.g. spelling bee and swimming pool); (2) English verb particle constructions (\"EVPCs\", e.g. stand up and give away); and (3) German noun compounds (\"GNCs\", e.g. ahornblatt \"maple leaf\" and eidechse \"lizard\").", "labels": [], "entities": []}, {"text": "The ENC dataset consists of 90 binary English noun compounds, and is annotated on a continuous [0, 5] scale for both overall compositionality and the component-wise compositionality of each of the modifier and head noun).", "labels": [], "entities": [{"text": "ENC dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.93952876329422}]}, {"text": "The state-of-the-art method for this dataset () is a supervised support vector regression model, trained over the distributional method from Section 3.1 as applied to both English and 51 target languages (under word and MWE translation).", "labels": [], "entities": []}, {"text": "The EVPC dataset consists of 160 English verb particle constructions, and is manually annotated for compositionality on a binary scale for each of the head verb and particle).", "labels": [], "entities": [{"text": "EVPC dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9519207179546356}]}, {"text": "In order to translate the dataset into a regression task, we calculate the overall compositionality as the number of annotations of entailment for the verb, divided by the total number of verb annotations for that VPC.", "labels": [], "entities": []}, {"text": "The state-of-the-art method for this dataset () is a linear combination of: (1) the distributional method from Section 3.1; (2) the same method applied to 10 target languages (under word and MWE translation, selecting the languages using supervised learning); and (3) the string similarity method of.", "labels": [], "entities": [{"text": "word and MWE translation", "start_pos": 182, "end_pos": 206, "type": "TASK", "confidence": 0.6015082821249962}]}, {"text": "The GNC dataset consists of 246 German noun compounds, and is annotated on a continuous scale (von der Heide and Borgwaldt, 2009; Schulte im).", "labels": [], "entities": [{"text": "GNC dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8404154777526855}]}, {"text": "The state-of-the-art method for this dataset is a distributional similarity method applied to part-of-speech tagged and lemmatised data (Schulte im).", "labels": [], "entities": []}, {"text": "For all experiments, we train our models over raw text Wikipedia corpora for either English or German, depending on the language of the dataset.", "labels": [], "entities": []}, {"text": "The raw English and German corpora were preprocessed using the WP2TXT toolbox to eliminate XML and HTML tags and hyperlinks, and punctuation was removed.", "labels": [], "entities": [{"text": "WP2TXT", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.8970404863357544}]}, {"text": "Finally, word-tokenisation was performed based on simple whitespace delimitation, after which we greedily identified all string occurrences of the MWEs in each of our datasets and combined them into a single token.", "labels": [], "entities": []}, {"text": "The word embedding approaches are unable to generate vector representations for tokens which occur with frequency below a fixed cutoff.", "labels": [], "entities": []}, {"text": "In order to  generate a compositionality prediction back-off for the small numbers of MWEs in this category, we assign a default value, which is the mean of computed compositionality scores for other instances.", "labels": [], "entities": [{"text": "compositionality prediction", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.9080265164375305}]}, {"text": "As a baseline, we use the translation string similarity approach of, including the cross-validation-based method for selecting the 10 best languages to use for each dataset.", "labels": [], "entities": [{"text": "translation string similarity", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.7523141304651896}]}, {"text": "We further include a linear combination of the string similarity method with each of the various approaches based on word embeddings.", "labels": [], "entities": []}, {"text": "shows the results for the various methods, lack of lemmatisation.", "labels": [], "entities": []}, {"text": "We also experimented with using the string similarity approach as a back-off, which resulted in marginally lower results than what is reported in over a range of hyper-parameter settings for each of WORD2VEC (vector dimensionality d; we also present results for CBOW vs. C-SKIP) and MSSG (vector dimensionality d and window size w), informed by the experimental results in the respective publications.", "labels": [], "entities": [{"text": "WORD2VEC", "start_pos": 199, "end_pos": 207, "type": "DATASET", "confidence": 0.8621147274971008}]}, {"text": "Note that for EVPC, we don't use the vector for the particle, in keeping with; as such, there are no results for comp 2 . For comp 1 , \u03b1 is set to 1.0 for EVPC, and 0.7 for both ENC and GNC, also based on the findings of.", "labels": [], "entities": []}, {"text": "The results indicate that the approaches using both and MSSG outperform simple distributional and string similarity by a substantial margin.", "labels": [], "entities": [{"text": "MSSG", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.8322545289993286}]}, {"text": "Further, over a variety of parameteriza- Figure 1: The effect of the number of prototypes on the results with MSSG tions, they surpass the state-of-the-art methods for ENC and EVPC; in the case of GNC, the bestperforming method (WORD2VEC with d = 500 and C-SKIP) roughly matches the state-of-the-art.", "labels": [], "entities": []}, {"text": "Note that in each case, the state-of-the-art is achieved using varying levels of supervision over labelled data (ENC and EVPC) or language-specific preprocessing (GNC), whereas the word embedding methods use no labelled data.", "labels": [], "entities": []}, {"text": "As such, the answer to RQ1 would appear to be a resounding yes.", "labels": [], "entities": [{"text": "RQ1", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.5086953639984131}]}, {"text": "Looking to RQ2, the models are remarkably insensitive to hyper-parameter optimisation for EVPC, but there are slight deviations in the results for ENC and GNC.", "labels": [], "entities": [{"text": "RQ2", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.9025924205780029}]}, {"text": "Having said that, they are largely between the different word embedding approaches, and the results fora given approach under different parameter settings is relatively stable.", "labels": [], "entities": []}, {"text": "A large part of the cause of the drop in results and greater parameter sensitivity over GNC is the lower token frequencies, through a combination of the Wikipedia corpus being markedly smaller and our naive tokenisation strategy having low recall over German due to the richer morphology.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 153, "end_pos": 169, "type": "DATASET", "confidence": 0.9078004360198975}, {"text": "recall", "start_pos": 240, "end_pos": 246, "type": "METRIC", "confidence": 0.9967855215072632}]}, {"text": "As such, the answer would appear to be a tentative \"relatively insensitive, assuming high token frequencies\".", "labels": [], "entities": []}, {"text": "Finally, looking to RQ3, there was little separating WORD2VEC and MSSG over ENC, but over the other two datasets, WORD2VEC had a clear advantage.", "labels": [], "entities": [{"text": "RQ3", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.9449117183685303}]}, {"text": "Given the high levels of polysemy observed in high frequency English verb particle constructions (), this result for EVPC was particularly surprising, and suggests that, at least under our two basic forms of composition, multiprototype word embeddings are at best equal to, and in many cases, inferior to, single-prototype word embeddings.", "labels": [], "entities": []}, {"text": "According to the results, the string similarity approach complements all word-embedding approaches.", "labels": [], "entities": []}, {"text": "We hypothesise that this is because it is not based on any corpus, and is thus not biased by the frequency of token instances in the corpus.", "labels": [], "entities": []}, {"text": "In, the number of embeddings for MSSG was set to 2 prototypes, based on the default recommendations of.", "labels": [], "entities": []}, {"text": "To investigate the impact of this parameter on our results, we retrained MSSG over the range and reran our experiments for each set of embeddings over the three datasets (without string similarity, to isolate the effect of the number of embeddings), as shown in.", "labels": [], "entities": []}, {"text": "For both English datasets (ENC and EVPC), setting the number of prototypes to a value higher than 2 boosts the results slightly, with 5 prototypes appearing to be the optimal value.", "labels": [], "entities": [{"text": "English datasets", "start_pos": 9, "end_pos": 25, "type": "DATASET", "confidence": 0.7730383276939392}]}, {"text": "For the German dataset (GNC), on the other hand, the best results are actually achieved fora single prototype.", "labels": [], "entities": [{"text": "German dataset (GNC)", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.9082092642784119}]}, {"text": "Further research is required to better understand this effect.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson's correlation (r) for the different methods over the three datasets; the state-of-the-art for each dataset  is described in Section 4", "labels": [], "entities": [{"text": "Pearson's correlation (r)", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8509169816970825}]}]}