{"title": [{"text": "Fast and Accurate Preordering for SMT using Neural Networks", "labels": [], "entities": [{"text": "Accurate", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9905080795288086}, {"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9943655729293823}]}], "abstractContent": [{"text": "We propose the use of neural networks to model source-side preordering for faster and better statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 93, "end_pos": 124, "type": "TASK", "confidence": 0.6636013885339102}]}, {"text": "The neu-ral network trains a logistic regression model to predict whether two sibling nodes of the source-side parse tree should be swapped in order to obtain a more monotonic parallel corpus, based on samples extracted from the word-aligned parallel corpus.", "labels": [], "entities": []}, {"text": "For multiple language pairs and domains, we show that this yields the best reordering performance against other state-of-the-art techniques, resulting in improved translation quality and very fast decoding .", "labels": [], "entities": []}], "introductionContent": [{"text": "Preordering is a pre-processing task in translation that aims to reorder the source sentence so that it best resembles the order of the target sentence.", "labels": [], "entities": [{"text": "Preordering", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.7990167737007141}]}, {"text": "If done correctly, it has a doubly beneficial effect: it allows a better estimation of word alignment and translation models which results in higher translation quality for distant language pairs, and it speeds up decoding enormously as less word movement is required.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.7438971698284149}]}, {"text": "Preordering schemes can be automatically learnt from source-side parsed, word-aligned parallel corpora.", "labels": [], "entities": []}, {"text": "Recently Jehl et al (2014) described a scheme based on a feature-rich logistic regression model that predicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted.", "labels": [], "entities": []}, {"text": "Based on the node-pair swapping probability predictions of this model, a branch-and-bound search returns the best ordering of nodes in the tree.", "labels": [], "entities": []}, {"text": "We propose using a neural network (NN) to estimate this node-swapping probability.", "labels": [], "entities": []}, {"text": "We find that this straightforward change to their scheme has multiple advantages: 1.", "labels": [], "entities": []}, {"text": "The superior modeling capabilities of NNs achieve better performance at preordering and overall translation quality when using the same set of features.", "labels": [], "entities": []}, {"text": "2. There is no need to manually define which feature combinations are to be considered in training.", "labels": [], "entities": []}, {"text": "3. Preordering is even faster as a result of the previous point.", "labels": [], "entities": [{"text": "Preordering", "start_pos": 3, "end_pos": 14, "type": "METRIC", "confidence": 0.9955765008926392}]}, {"text": "Our results in translating from English to Japanese, Korean, Chinese, Arabic and Hindi support these findings by comparing against two other preordering schemes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the intrinsic preordering task on a random 5K-sentence subset of the training data which is excluded from model estimation.", "labels": [], "entities": []}, {"text": "We report the normalized crossing score c/s, where c is the number of crossing links) in the aligned parallel corpus, and sis the number of source (e.g. English) words.", "labels": [], "entities": []}, {"text": "Ideally we would like this metric to be zero, meaning a completely monotonic parallel corpus ; the more monotonic the corpus, the better the translation models will be and the faster decoding will run as less distortion will be needed.", "labels": [], "entities": []}, {"text": "Normalizing over the number of source words allows us to compare this metric across language pairs, and so the potential impact of preordering in translation performance becomes apparent.", "labels": [], "entities": []}, {"text": "for results across several language pairs.", "labels": [], "entities": []}, {"text": "In all cases our proposed NN-based preorderer achieves the lowest normalized crossing score among all preordering schemes.", "labels": [], "entities": [{"text": "normalized crossing score", "start_pos": 66, "end_pos": 91, "type": "METRIC", "confidence": 0.6942238807678223}]}], "tableCaptions": [{"text": " Table 1: Translation performance for various language pairs using no preordering (baseline), and three alternative  preordering systems. Average test BLEU score and standard deviation across 3 independent tuning runs. Speed ratio  is calculated with respect to the speed of the slower baseline that uses a d = 10. Stack size is 1000. For eng-jpn and  eng-chi, character-based BLEU is used.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9716785550117493}, {"text": "BLEU score", "start_pos": 151, "end_pos": 161, "type": "METRIC", "confidence": 0.9691891372203827}, {"text": "Speed ratio", "start_pos": 219, "end_pos": 230, "type": "METRIC", "confidence": 0.9371194541454315}, {"text": "BLEU", "start_pos": 377, "end_pos": 381, "type": "METRIC", "confidence": 0.976253092288971}]}]}