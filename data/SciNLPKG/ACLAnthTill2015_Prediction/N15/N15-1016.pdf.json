{"title": [{"text": "Combining Language and Vision with a Multimodal Skip-gram Model", "labels": [], "entities": []}], "abstractContent": [{"text": "We extend the SKIP-GRAM model of Mikolov et al.", "labels": [], "entities": []}, {"text": "(2013a) by taking visual information into account.", "labels": [], "entities": []}, {"text": "Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM) build vector-based word representations by learning to predict linguistic contexts in text corpora.", "labels": [], "entities": []}, {"text": "However, fora restricted set of words, the models are also exposed to visual representations of the objects they denote (extracted from natural images), and must predict linguistic and visual features jointly.", "labels": [], "entities": []}, {"text": "The MMSKIP-GRAM models achieve good performance on a variety of semantic benchmarks.", "labels": [], "entities": []}, {"text": "Moreover, since they propagate visual information to all words, we use them to improve image labeling and retrieval in the zero-shot setup, where the test concepts are never seen during model training.", "labels": [], "entities": [{"text": "image labeling", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.6708432734012604}]}, {"text": "Finally, the MMSKIP-GRAM models discover intriguing visual properties of abstract words, paving the way to realistic implementations of embodied theories of meaning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional semantic models (DSMs) derive vector-based representations of meaning from patterns of word co-occurrence in corpora.", "labels": [], "entities": []}, {"text": "DSMs have been very effectively applied to a variety of semantic tasks.", "labels": [], "entities": []}, {"text": "However, compared to human semantic knowledge, these purely textual models, just like traditional symbolic AI systems, are severely impoverished, suffering of lack of grounding in extra-linguistic modalities).", "labels": [], "entities": []}, {"text": "This observation has led to the development of multimodal distributional semantic models (MDSMs) (, that enrich linguistic vectors with perceptual information, most often in the form of visual features automatically induced from image collections.", "labels": [], "entities": []}, {"text": "MDSMs outperform state-of-the-art text-based approaches, not only in tasks that directly require access to visual knowledge (), but also on general semantic benchmarks ().", "labels": [], "entities": []}, {"text": "However, current MDSMs still have a number of drawbacks.", "labels": [], "entities": []}, {"text": "First, they are generally constructed by first separately building linguistic and visual representations of the same concepts, and then merging them.", "labels": [], "entities": []}, {"text": "This is obviously very different from how humans learn about concepts, by hearing words in a situated perceptual context.", "labels": [], "entities": []}, {"text": "Second, MDSMs assume that both linguistic and visual information is available for all words, with no generalization of knowledge across modalities.", "labels": [], "entities": []}, {"text": "Third, because of this latter assumption of full linguistic and visual coverage, current MDSMs, paradoxically, cannot be applied to computer vision tasks such as image labeling or retrieval, since they do not generalize to images or words beyond their training set.", "labels": [], "entities": [{"text": "image labeling or retrieval", "start_pos": 162, "end_pos": 189, "type": "TASK", "confidence": 0.675550289452076}]}, {"text": "We introduce the multimodal skip-gram models, two new MDSMs that address all the issues above.", "labels": [], "entities": []}, {"text": "The models build upon the very effective skip-gram approach of, that constructs vector representations by learning, incrementally, to predict the linguistic contexts in which target words occur in a corpus.", "labels": [], "entities": []}, {"text": "In our extension, fora subset of the target words, relevant visual evidence from natural images is presented together with the corpus contexts (just like humans hear words accompanied by concurrent perceptual stimuli).", "labels": [], "entities": []}, {"text": "The model must learn to predict these visual representations jointly with the linguistic features.", "labels": [], "entities": []}, {"text": "The joint objective encourages the propagation of visual information to representations of words for which no direct visual evidence was available in training.", "labels": [], "entities": []}, {"text": "The resulting multimodally-enhanced vectors achieve remarkably good performance both on traditional semantic benchmarks, and in their new application to the \"zero-shot\" image labeling and retrieval scenario.", "labels": [], "entities": [{"text": "zero-shot\" image labeling and retrieval", "start_pos": 158, "end_pos": 197, "type": "TASK", "confidence": 0.6665307978789011}]}, {"text": "Very interestingly, indirect visual evidence also affects the representation of abstract words, paving the way to ground-breaking cognitive studies and novel applications in computer vision.", "labels": [], "entities": []}], "datasetContent": [{"text": "The parameters of all models are estimated by backpropagation of error via stochastic gradient descent.", "labels": [], "entities": []}, {"text": "Our text corpus is a Wikipedia 2009 dump comprising approximately 800M tokens.", "labels": [], "entities": [{"text": "Wikipedia 2009 dump", "start_pos": 21, "end_pos": 40, "type": "DATASET", "confidence": 0.8656563957532247}]}, {"text": "To train the multimodal models, we add visual information for 5,100 words that have an entry in ImageNet (), occur at least 500 times in the corpus and have concreteness score \u2265 0.5 according to.", "labels": [], "entities": []}, {"text": "On average, about 5% tokens in the text corpus are associated to a visual representation.", "labels": [], "entities": []}, {"text": "To construct the visual representation of a word, we sample 100 pictures from its ImageNet entry, and extract a 4096-dimensional vector from each picture using the Caffe toolkit (), together with the pre-trained convolutional neural network of.", "labels": [], "entities": []}, {"text": "The vector corresponds to activation in the top (FC7) layer of the network.", "labels": [], "entities": []}, {"text": "Finally, we average the vectors of the 100 pictures associated to each word, deriving 5,100 aggregated visual representations.", "labels": [], "entities": []}, {"text": "Hyperparameters For both SKIP-GRAM and the MMSKIP-GRAM models, we fix hidden layer size to 300.", "labels": [], "entities": [{"text": "MMSKIP-GRAM", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.8573399782180786}]}, {"text": "To facilitate comparison between MMSKIP-GRAM-A and MMSKIP-GRAM-B, and since the former requires equal linguistic and visual dimensionality, we keep the first 300 dimensions of the visual vectors.", "labels": [], "entities": []}, {"text": "For the linguistic objective, we use hierarchical softmax with a Huffman frequency-based encoding tree, setting frequency subsampling option t = 0.001 and window size c = 5, without tuning.", "labels": [], "entities": []}, {"text": "The following hyperparameters were tuned on the text9 corpus: 2 MMSKIP-GRAM-A: k=20, \u03b3 =0.5; MMSKIP-GRAM-B: k=5, \u03b3=0.5, \u03bb=0.0001.", "labels": [], "entities": [{"text": "text9 corpus", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.9336508512496948}]}], "tableCaptions": [{"text": " Table 1: Spearman correlation between model-generated similarities and human judgments. Right columns  report correlation on visual-coverage subsets (percentage of original benchmark covered by subsets on first  row of respective columns). First block reports results for out-of-the-box models; second block for visual  and textual representations alone; third block for our implementation of multimodal models.", "labels": [], "entities": []}, {"text": " Table 3: Percentage precision@k results in the zero- shot image labeling task.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.665546715259552}, {"text": "image labeling", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.5780252069234848}]}, {"text": " Table 4: Percentage precision@k results in the zero- shot image retrieval task.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.6482818722724915}, {"text": "image retrieval", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.6740655452013016}]}, {"text": " Table 5: Subjects' preference for nearest visual  neighbour of words in Kiela et al. (2014) vs. random  pictures. Figure of merit is percentage proportion  of significant results in favor of nearest neighbour  across words. Results are reported for the whole set,  as well as for words above (concrete) and below (ab- stract) the concreteness rating median. The unseen  column reports results when words exposed to direct  visual evidence during training are discarded. The  words columns report set cardinality.", "labels": [], "entities": []}]}