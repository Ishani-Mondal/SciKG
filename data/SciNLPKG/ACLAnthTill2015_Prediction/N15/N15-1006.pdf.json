{"title": [{"text": "An Incremental Algorithm for Transition-based CCG Parsing", "labels": [], "entities": [{"text": "Transition-based CCG Parsing", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.5453069607416788}]}], "abstractContent": [{"text": "Incremental parsers have potential advantages for applications like language modeling for machine translation and speech recognition.", "labels": [], "entities": [{"text": "Incremental parsers", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.731557309627533}, {"text": "language modeling", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7327991724014282}, {"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7672656774520874}, {"text": "speech recognition", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.7733453810214996}]}, {"text": "We describe anew algorithm for incremental transition-based Combinatory Categorial Grammar parsing.", "labels": [], "entities": [{"text": "Combinatory Categorial Grammar parsing", "start_pos": 60, "end_pos": 98, "type": "TASK", "confidence": 0.6790602952241898}]}, {"text": "As English CCGbank derivations are mostly right branching and non-incremental, we design our algorithm based on the dependencies resolved rather than the derivation.", "labels": [], "entities": []}, {"text": "We introduce two new actions in the shift-reduce paradigm based on the idea of 'revealing' (Pareschi and Steedman, 1987) the required information during parsing.", "labels": [], "entities": []}, {"text": "On the standard CCGbank test data, our algorithm achieved improvements of 0.88% in labeled and 2.0% in unlabeled F-score over a greedy non-incremental shift-reduce parser.", "labels": [], "entities": [{"text": "CCGbank test data", "start_pos": 16, "end_pos": 33, "type": "DATASET", "confidence": 0.9709307352701823}, {"text": "F-score", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.9784198999404907}]}], "introductionContent": [{"text": "Combinatory Categorial Grammar (CCG)) is an efficiently parseable, yet linguistically expressive grammar formalism.", "labels": [], "entities": [{"text": "Combinatory Categorial Grammar (CCG))", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7464657773574194}]}, {"text": "In addition to predicate-argument structure, CCG elegantly captures the unbounded dependencies found in grammatical constructions like relativization, coordination etc.", "labels": [], "entities": []}, {"text": "Availability of the English CCGbank ( has enabled the creation of several robust and accurate wide-coverage CCG parsers.", "labels": [], "entities": [{"text": "English CCGbank", "start_pos": 20, "end_pos": 35, "type": "DATASET", "confidence": 0.943974494934082}]}, {"text": "While the majority of CCG parsers use chart-based approaches, there has been some work on developing shift-reduce parsers for CCG ().", "labels": [], "entities": []}, {"text": "Most of these parsers model normal-form CCG derivations, which are mostly right-branching trees : hence are not incremental in nature.", "labels": [], "entities": []}, {"text": "The dependency models of and model dependencies rather than derivations, but do not guarantee incremental analyses.", "labels": [], "entities": []}, {"text": "Besides being cognitively plausible, incremental parsing is more useful than non-incremental parsing for some applications.", "labels": [], "entities": [{"text": "incremental parsing", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.5203408300876617}]}, {"text": "For example, an incremental analysis is required for integrating syntactic and semantic information into language modeling for statistical machine translation (SMT) and automatic speech recognition (ASR);.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 127, "end_pos": 164, "type": "TASK", "confidence": 0.7866048713525137}, {"text": "automatic speech recognition (ASR)", "start_pos": 169, "end_pos": 203, "type": "TASK", "confidence": 0.80173559486866}]}, {"text": "This paper develops anew incremental shiftreduce algorithm for parsing CCG by building a dependency graph in addition to the CCG derivation as a representation.", "labels": [], "entities": [{"text": "parsing CCG", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.8355395793914795}]}, {"text": "The dependencies in the graph are extracted from the CCG derivation.", "labels": [], "entities": []}, {"text": "A node can have multiple parents, and hence we construct a dependency graph rather than a tree.", "labels": [], "entities": []}, {"text": "Two new actions are introduced in the shift-reduce paradigm for \"revealing\" unbuilt structure during parsing.", "labels": [], "entities": []}, {"text": "We build the dependency graph in parallel to the incremental CCG derivation and use this graph for revealing, via these two new actions.", "labels": [], "entities": []}, {"text": "On the standard CCGbank test data, our algorithm achieves improvements of 0.88% in labeled F-score and 2.0% in unlabeled F-score over a greedy non-incremental shift-reduce algorithm.", "labels": [], "entities": [{"text": "CCGbank test data", "start_pos": 16, "end_pos": 33, "type": "DATASET", "confidence": 0.9747118155161539}, {"text": "F-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.896948516368866}, {"text": "F-score", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9481134414672852}]}, {"text": "As our algorithm does not model derivations, but rather models transitions, we do not need a treebank John likes mangoes from India madly of incremental CCG derivations and can train on the dependencies in the existing treebank.", "labels": [], "entities": []}, {"text": "Our approach can therefore be adapted to other languages with dependency treebanks, since CCG lexical categories can be easily extracted from dependency treebanks.", "labels": [], "entities": []}, {"text": "The rest of the paper is arranged as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief introduction to related work in the areas of CCG parsing and incremental parsing.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.6744562685489655}, {"text": "incremental parsing", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.5513351112604141}]}, {"text": "In section 3, we describe our incremental shift-reduce parsing algorithm.", "labels": [], "entities": []}, {"text": "Details about the experiments, evaluation metrices and analysis of the results are in section 4.", "labels": [], "entities": []}, {"text": "We conclude with possible future directions in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We re-implemented Zhang and Clark (2011)'s model for our experiments.", "labels": [], "entities": []}, {"text": "We used their global linear model trained with the averaged perceptron).", "labels": [], "entities": []}, {"text": "We applied the early-update strategy of while training.", "labels": [], "entities": []}, {"text": "In this strategy, when we don't use abeam, decoding is stopped when the predicted action is different from the gold action and weights are updated accordingly.", "labels": [], "entities": []}, {"text": "We use the feature set of Zhang and Clark (2011) (Z&C) for the NonInc algorithm.", "labels": [], "entities": []}, {"text": "This feature set comprises of features over the top four nodes in the stack and the next four words in the input buffer.", "labels": [], "entities": []}, {"text": "Complete details of the feature set can be found in their paper.", "labels": [], "entities": []}, {"text": "For our own model, RevInc, in addition to these features used for NonInc, we also provide features based on the right periphery of top node in the stack.", "labels": [], "entities": [{"text": "RevInc", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.9580424427986145}]}, {"text": "For nodes in the right periphery, we provide uni-gram and bi-gram features based on the node's CCG category.", "labels": [], "entities": []}, {"text": "For example, if S0 is the node on the top of the stack, B1 is the bottom most node in the right periphery, and c represent the node's CCG category, then B1c, and B1cS0c are the uni-gram and bi-gram features respectively.", "labels": [], "entities": []}, {"text": "Unlike Z&C, we do not use abeam for our experiments, although we use abeam of 16 for comparison of our results with their parser.", "labels": [], "entities": [{"text": "abeam", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9710978269577026}]}, {"text": "The latter gives competitive results with the state-of-theart CCG parsers.", "labels": [], "entities": []}, {"text": "Z&C and, use C&C's generate script and unification mechanism respectively to extract dependencies for evaluation.", "labels": [], "entities": []}, {"text": "C&C's grammar doesn't coverall the lexical categories and binary rules in the CCGbank.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9648059010505676}]}, {"text": "To avoid this, we adapted Hockenmaier's scripts used for extracting dependencies from the CCGbank derivations.", "labels": [], "entities": [{"text": "CCGbank derivations", "start_pos": 90, "end_pos": 109, "type": "DATASET", "confidence": 0.9381331205368042}]}, {"text": "These are the two major divergences in our re-implementation from Z&C.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Connectedness and waiting time.", "labels": [], "entities": []}, {"text": " Table 2: Performance on the development data. *: These results are from the Z&C paper.", "labels": [], "entities": [{"text": "Z&C paper", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.942561611533165}]}, {"text": " Table 3: Label-wise F-score of RevInc and NonInc  parsers (both with beam=1). Argument slots in the  relation are in bold.", "labels": [], "entities": [{"text": "F-score", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.8861023783683777}, {"text": "RevInc", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.9678172469139099}]}, {"text": " Table 4: Performance on the test data. *: These results are from their paper.", "labels": [], "entities": []}]}