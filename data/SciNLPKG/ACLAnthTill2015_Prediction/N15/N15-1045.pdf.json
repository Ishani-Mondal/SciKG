{"title": [{"text": "An In-depth Analysis of the Effect of Text Normalization in Social Media", "labels": [], "entities": [{"text": "Text Normalization", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7194326519966125}]}], "abstractContent": [{"text": "Recent years have seen increased interest in text normalization in social media, as the informal writing styles found in Twitter and other social media data often cause problems for NLP applications.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7651946544647217}]}, {"text": "Unfortunately, most current approaches narrowly regard the nor-malization task as a \"one size fits all\" task of replacing non-standard words with their standard counterparts.", "labels": [], "entities": []}, {"text": "In this work we build a taxonomy of normalization edits and present a study of normalization to examine its effect on three different downstream applications (de-pendency parsing, named entity recognition, and text-to-speech synthesis).", "labels": [], "entities": [{"text": "normalization edits", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8186163902282715}, {"text": "de-pendency parsing", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.711137980222702}, {"text": "named entity recognition", "start_pos": 180, "end_pos": 204, "type": "TASK", "confidence": 0.6165636877218882}, {"text": "text-to-speech synthesis", "start_pos": 210, "end_pos": 234, "type": "TASK", "confidence": 0.7110100388526917}]}, {"text": "The results suggest that how the normalization task should be viewed is highly dependent on the targeted application.", "labels": [], "entities": [{"text": "normalization task", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.9176892936229706}]}, {"text": "The results also show that normal-ization must bethought of as more than word replacement in order to produce results comparable to those seen on clean text.", "labels": [], "entities": [{"text": "word replacement", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.7353180795907974}]}], "introductionContent": [{"text": "The informal writing style employed by authors of social media data is problematic for many natural language processing (NLP) tools, which are generally trained on clean, formal text such as newswire data.", "labels": [], "entities": []}, {"text": "One possible solution to this problem is normalization, in which the informal text is converted into a more standard formal form.", "labels": [], "entities": [{"text": "normalization", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.9689253568649292}]}, {"text": "Because of this, the rise of social media data has coincided with arise in interest in the normalization problem.", "labels": [], "entities": [{"text": "normalization", "start_pos": 91, "end_pos": 104, "type": "TASK", "confidence": 0.982956051826477}]}, {"text": "Unfortunately, while many approaches to the problem exist, there are notable limitations to the * Work was done while at IBM way in which normalization is examined.", "labels": [], "entities": []}, {"text": "First, although social media normalization is universally motivated by pointing to its role in helping downstream applications, most normalization work gives little to no insight into the effect of the normalization process on the downstream application of interest.", "labels": [], "entities": []}, {"text": "Further, the normalization process is generally seen to be agnostic of the downstream application, adopting a \"one size fits all\" view of how normalization should be performed.", "labels": [], "entities": [{"text": "normalization", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9784011840820312}]}, {"text": "This view seems intuitively problematic, as different information is likely to be of importance for different tasks.", "labels": [], "entities": []}, {"text": "For instance, while capitalization is important for resolving named entities, it is less important for other tasks, such as dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.8641059696674347}]}, {"text": "Some recent work has given credence to the idea that application-targeted normalization is appropriate (.", "labels": [], "entities": []}, {"text": "However, how certain normalization actions influence the overall performance of these applications is not well understood.", "labels": [], "entities": []}, {"text": "To address this, we design a taxonomy of possible normalization edits based on inspiration from previous work and an examination of annotated data.", "labels": [], "entities": [{"text": "normalization edits", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8896393775939941}]}, {"text": "We then use this taxonomy to examine the importance of individual normalization actions on three different downstream applications: dependency parsing, named entity recognition, and textto-speech synthesis.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.8643893301486969}, {"text": "named entity recognition", "start_pos": 152, "end_pos": 176, "type": "TASK", "confidence": 0.6377457777659098}, {"text": "textto-speech synthesis", "start_pos": 182, "end_pos": 205, "type": "TASK", "confidence": 0.8093602657318115}]}, {"text": "The results suggest that the importance of a given normalization edit is highly dependent on the task, making the \"one size fits all\" approach inappropriate.", "labels": [], "entities": []}, {"text": "The results also show that a narrow view of normalization as word replacement is insufficient, as many often-ignored normalization actions prove to be important for certain tasks.", "labels": [], "entities": [{"text": "word replacement", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7635702192783356}]}, {"text": "In the next section, we give an overview of previous work on the normalization problem.", "labels": [], "entities": [{"text": "normalization problem", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.9681625068187714}]}, {"text": "We then introduce our taxonomy of normalization edits in Section 3.", "labels": [], "entities": [{"text": "normalization edits", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.8437074720859528}]}, {"text": "In Section 4, we present our evaluation methodology and present results over the three applications, using Twitter data as a representative domain.", "labels": [], "entities": []}, {"text": "Finally, we discuss our results in Section 5 and conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "To facilitate our experiments, we collected and annotated a dataset of Twitter posts (tweets) from the TREC Twitter Corpus 1 . The TREC Twitter corpus is a collection of 16 million tweets posted in January and February of 2011.", "labels": [], "entities": [{"text": "TREC Twitter Corpus 1", "start_pos": 103, "end_pos": 124, "type": "DATASET", "confidence": 0.9232544898986816}, {"text": "TREC Twitter corpus", "start_pos": 131, "end_pos": 150, "type": "DATASET", "confidence": 0.9105293353398641}]}, {"text": "The corpus is designed to be a representative sample of Twitter usage, and as such includes both regular and spam tweets.", "labels": [], "entities": []}, {"text": "To build our dataset, we sampled 600 posts at random from the corpus.", "labels": [], "entities": []}, {"text": "The tweets were then manually filtered such that tweets that were not in English were replaced with those in English.", "labels": [], "entities": []}, {"text": "To produce our gold standard, two oDesk 2 contractors were asked to manually normalize each tweet in the dataset to its fully grammatical form, as would be found informal text.", "labels": [], "entities": []}, {"text": "Annotation guidelines stipulated that twitter-specific tokens should be retained if important to understanding the sentence, but modified or removed otherwise.", "labels": [], "entities": []}, {"text": "As noted, most previous work often stopped short of requiring full grammaticality.", "labels": [], "entities": []}, {"text": "However, argued that grammaticality should be the ideal end goal of normalization since the models used in downstream applications are typically trained on well-formed sentences.", "labels": [], "entities": []}, {"text": "We adopt this methodology here both because we agree with this assertion and because a fully grammatical form is appropriate for all of the downstream applications of interest, allowing fora single unified gold standard that can aid comparison across applications.", "labels": [], "entities": []}, {"text": "During gold standard creation, each normalization edit was labeled with its type, according to the above taxonomy.", "labels": [], "entities": []}, {"text": "The distribution of normalization edits in the dataset is given in.", "labels": [], "entities": []}, {"text": "As shown, normalization edits accounted for about 29% of all tokens.", "labels": [], "entities": [{"text": "normalization edits", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8597406446933746}]}, {"text": "Token replacements accounted for just over half of all edits (53%), while token addition (29%) was more common than token removal (18%).", "labels": [], "entities": [{"text": "Token replacements", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8223717212677002}, {"text": "token addition", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.8578994870185852}, {"text": "token removal", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.8290935456752777}]}, {"text": "One interesting observation is non-capitalization word replacement accounted for only 25% of all normalization edits, intuitively indicating potential drawbacks for the common definition of normalization as one of simple word replacement which ignores capitalization and punctuation.", "labels": [], "entities": [{"text": "word replacement", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.717630073428154}, {"text": "word replacement", "start_pos": 221, "end_pos": 237, "type": "TASK", "confidence": 0.7702068090438843}]}, {"text": "In this section, we present our examination of the effect of normalization edits on downstream NLP applications.", "labels": [], "entities": [{"text": "normalization edits", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.9259142279624939}]}, {"text": "To get abroad understanding of these effects, we examine three very different cases: dependency parsing, named entity recognition (NER), and text-to-speech (TTS) synthesis.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.8648553788661957}, {"text": "named entity recognition (NER)", "start_pos": 105, "end_pos": 135, "type": "TASK", "confidence": 0.7652730643749237}, {"text": "text-to-speech (TTS) synthesis", "start_pos": 141, "end_pos": 171, "type": "TASK", "confidence": 0.6383731484413147}]}, {"text": "We chose these tasks because they each require the extraction of different information from the text.", "labels": [], "entities": []}, {"text": "For instance, named entity recognition requires only a shallow syntactic analysis, in contrast to the deeper understanding required for dependency parsing.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.6683986981709799}, {"text": "dependency parsing", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.8048622608184814}]}, {"text": "Similarly, only speech synthesis requires phoneme production, while the other tasks do not.", "labels": [], "entities": [{"text": "speech synthesis", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.772587388753891}]}, {"text": "Despite their differences, each of these tasks is relevant to larger applications that would benefit from improved performance on Twitter data, and each has garnered attention in the normalization and Twitter-adaptation literature (Beaufort et al., 2010; Liu et al., 2011b;.", "labels": [], "entities": []}, {"text": "Although the differences in these tasks also dictates that they be evaluated somewhat differently, we examine them within a common evaluation structure.", "labels": [], "entities": []}, {"text": "In all cases, to examine the effects of each normalization edit we model our analyses as ablation studies.", "labels": [], "entities": []}, {"text": "That is, for every category in the taxonomy, we examine the effect of performing all normalization edits except the relevant case.", "labels": [], "entities": []}, {"text": "This allows us to measure the drop in performance solely attributable to each category; the greater the performance drop observed when a given normalization edit is not performed, the greater the importance of performing that edit.", "labels": [], "entities": []}, {"text": "To aid analysis, results are presented in two ways: 1) as raw performance numbers, and 2) as an error rate per-token.", "labels": [], "entities": []}, {"text": "These metrics give two different views of the relevance of each edit type.", "labels": [], "entities": []}, {"text": "The raw numbers give a sense of the overall impact of a given category, and as such maybe impacted by the size of the category, with common edits becoming more important simply by virtue of their frequency.", "labels": [], "entities": []}, {"text": "In contrast, the per-token error rate highlights the cost of failing to perform a single instance of a given normalization edit, independent of the frequency of the edit.", "labels": [], "entities": []}, {"text": "Both of these measures are likely to be relevant when attempting to improve the performance of a normalization system.", "labels": [], "entities": []}, {"text": "Note that since the first measure is one of overall performance, smaller numbers reflect larger performance drops when removing a given type of edit, so that the smaller the number the more critical the need to perform the given type of normalization.", "labels": [], "entities": []}, {"text": "In contrast, the latter judgment is one of error rate, and thus interpretation is reversed; the larger the error rate when it is removed, the more critical the normalization edit.", "labels": [], "entities": [{"text": "error rate", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.945550948381424}]}, {"text": "Another commonality among the analyses is that performance is measured relative to the top performance of the tool, not the task.", "labels": [], "entities": []}, {"text": "That is, following, we consider the output produced by the tool (e.g., the dependency parser) on the grammatically correct data to be gold standard performance.", "labels": [], "entities": []}, {"text": "This means that some output based on our gold standard may in fact be incorrect relative to human judgment, simply because the tool used does not have perfect performance even if the text if fully grammatical.", "labels": [], "entities": []}, {"text": "Since the goal is to understand how normalization edits impact the performance, this style of evaluation is appropriate; it considers mistakes attributable to normalization edits as erroneous, but ignores those mistakes attributable to the limitations of the tool.", "labels": [], "entities": []}, {"text": "Finally, to maximize the relevance of the analyses given here, in each case we employ publicly available and widely used tools.", "labels": [], "entities": []}, {"text": "To examine the effect of normalization on dependency parsing, we employ the Stanford dependency parser).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.8486707210540771}]}, {"text": "To produce the gold standard dependencies for comparison, the manually grammaticalized tweets (Section 3.3) were run through the parser.", "labels": [], "entities": []}, {"text": "To compare the ablation results to the gold standard parses, we adopt a variation of the evaluation method used by.", "labels": [], "entities": []}, {"text": "Given dependency parses from the gold standard and a candidate normalization, we define precision and recall as follows: Where SOV and SOV gold are the sets of subject, object, and verb dependencies in the candidate normalization and gold standard, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9991424083709717}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9988188147544861}]}, {"text": "While Zhang et al. chose to examine subjects and objects separately from verbs, we employ a unified metric to simplify interpretation.", "labels": [], "entities": []}, {"text": "In this section, we examine the effect of each normalization edit on a somewhat more shallow interpretation task, named entity recognition.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.7236197888851166}]}, {"text": "Unlike dependency parsing which requires an understanding of every token in the text, NER must only determine whether a given token is a named entity, and if so, discover its associated entity type.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.8024033904075623}, {"text": "NER", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.8992306590080261}]}, {"text": "Unlike the previous two tasks, the TTS problem is complicated by its need for speech production.", "labels": [], "entities": [{"text": "TTS", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9912024736404419}, {"text": "speech production", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.7156860530376434}]}, {"text": "Similarly, evaluation of speech synthesis is more difficult, as it requires human judgment about the overall quality of the output).", "labels": [], "entities": [{"text": "speech synthesis", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.6608483195304871}]}, {"text": "While speech synthesis evaluations often rate performance on a 5 point scale, we adopt a more restricted method, based on the comparison to gold standard methodology used in the previous evaluations.", "labels": [], "entities": [{"text": "speech synthesis", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.7477332055568695}]}, {"text": "For each tweet and each round of ablation, a synthesized audio file was produced from both the gold standard and ablated version of the tweet.", "labels": [], "entities": []}, {"text": "These audio snippets were randomized and presented to human judges who were asked to make a binary judgment as to whether the meaning and understandability of the ablated case was comparable to the gold standard.", "labels": [], "entities": []}, {"text": "The accuracy of a given round of ablation is then calculated to be the percentage of tweets judged  to be similar to the gold standard.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999291181564331}]}, {"text": "The eSpeak speech synthesizer 5 was used to produce audio files for all tweet variations in the ablation study.", "labels": [], "entities": []}, {"text": "As is common for speech synthesizers, eSpeak does perform some amount of TTS-specific normalization natively.", "labels": [], "entities": [{"text": "TTS-specific normalization", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.8711591958999634}]}, {"text": "While this does influence the normalizations produced, the comparison to gold standard methodology employed in this study helps us to focus on differences that are primarily attributable to the normalization edits we wish to examine, not those produced natively.", "labels": [], "entities": []}, {"text": "To obtain the gold standard, two native-English speaking judges were recruited via oDesk.", "labels": [], "entities": [{"text": "oDesk", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.9737407565116882}]}, {"text": "Inter-annotator agreement was moderate, \u03ba = 0.48.", "labels": [], "entities": [{"text": "agreement", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.8021794557571411}]}, {"text": "shows the results of the speech synthesis study.", "labels": [], "entities": [{"text": "speech synthesis", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7473499476909637}]}, {"text": "As shown, the removal of non-standard or out of place tokens was most critical to the production of a normalization that is clearly understandable to human listeners.", "labels": [], "entities": []}, {"text": "The aggregate results for token removals were comparable to or better than those of replacements at all levels of the taxonomy, in contrast to the results from the other two tasks, where the larger number of replacements led to the largest 5 Version 1.47.11, http://espeak.sourceforge.net/ performance hits.", "labels": [], "entities": [{"text": "token removals", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.8131816983222961}]}, {"text": "Meanwhile, word addition proved to be less essential overall.", "labels": [], "entities": [{"text": "word addition", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.8210403323173523}]}], "tableCaptions": [{"text": " Table 1: Token counts for each type of normalization edit.", "labels": [], "entities": []}, {"text": " Table 2: Dependency Parser Results.", "labels": [], "entities": [{"text": "Dependency Parser", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6697692275047302}]}, {"text": " Table 4: Text-To-Speech Synthesis Results.", "labels": [], "entities": [{"text": "Text-To-Speech Synthesis", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6680291444063187}]}]}