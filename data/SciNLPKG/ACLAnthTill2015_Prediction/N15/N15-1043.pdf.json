{"title": [{"text": "Latent Domain Word Alignment for Heterogeneous Corpora", "labels": [], "entities": [{"text": "Latent Domain Word Alignment", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6079306155443192}]}], "abstractContent": [{"text": "This work focuses on the insensitivity of existing word alignment models to domain differences , which often yields suboptimal results on large heterogeneous data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.7179698646068573}]}, {"text": "A novel latent domain word alignment model is proposed , which induces domain-conditioned lexical and alignment statistics.", "labels": [], "entities": [{"text": "domain word alignment", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.6772072712580363}]}, {"text": "We propose to train the model on a heterogeneous corpus under partial supervision, using a small number of seed samples from different domains.", "labels": [], "entities": []}, {"text": "The seed samples allow estimating sharper, domain-conditioned word alignment statistics for sentence pairs.", "labels": [], "entities": [{"text": "domain-conditioned word alignment", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.6469237009684244}]}, {"text": "Our experiments show that the derived domain-conditioned statistics, once combined together, produce notable improvements both in word alignment accuracy and in translation accuracy of their resulting SMT systems.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 130, "end_pos": 144, "type": "TASK", "confidence": 0.7324971854686737}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.8041560649871826}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.8119404315948486}, {"text": "SMT", "start_pos": 201, "end_pos": 204, "type": "TASK", "confidence": 0.9907174706459045}]}], "introductionContent": [{"text": "Word alignment currently constitutes the basis for phrase extraction and reordering in phrase-based systems, and its statistics provide lexical parameters used for smoothing the phrase pair estimates.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6893061548471451}, {"text": "phrase extraction", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.8388842046260834}]}, {"text": "For over two decades since IBM models and the HMM alignment model, word alignment remains an active research line, e.g., see recent work (.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.8876935839653015}, {"text": "word alignment", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.8433359265327454}]}, {"text": "During the past years we witnessed an increasing need to collect and use large heterogeneous parallel corpora from different domains and sources, e.g., News, Wikipedia, Parliament Proceedings.", "labels": [], "entities": [{"text": "Parliament Proceedings", "start_pos": 169, "end_pos": 191, "type": "DATASET", "confidence": 0.7860798537731171}]}, {"text": "It is tacitly assumed that assembling a larger corpus should improve a phrase-based system coverage and performance.", "labels": [], "entities": []}, {"text": "Recent work ( shows that this is not necessarily true as phrase translations as well as (bi-and monolingual) word co-occurrence statistics could differ across domains.", "labels": [], "entities": [{"text": "phrase translations", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7248144000768661}]}, {"text": "This suggests that the word alignment quality obtained from IBM and HMM alignment models might also be affected in heterogeneous corpora.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.6646533608436584}]}, {"text": "Intuitively, in heterogeneous data certain words are present across many domains, whereas others are more specific to few domains.", "labels": [], "entities": []}, {"text": "This suggests that the translation probabilities for words will be as fractioned as the diversity of its translations across the domains.", "labels": [], "entities": []}, {"text": "Furthermore, because the IBM and HMM alignment models use context-insensitive conditional probabilities, in heterogeneous corpora the estimates of these probabilities will be aggregated over different domains.", "labels": [], "entities": []}, {"text": "Both issues could lead to suboptimal word alignment quality.", "labels": [], "entities": [{"text": "word alignment quality", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.7213288247585297}]}, {"text": "Surprisingly, the insensitivity of the existing IBM and HMM alignment models to domain differences has not received much attention thus far (see the study of and for reference in the literature).", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.6361911594867706}]}, {"text": "We conjecture that this is because it is not fully clear how to define what constitutes a (sub)-domain.", "labels": [], "entities": []}, {"text": "In this paper we propose to exploit the contrast between the alignment statistics in a handful of seed samples from different domains in order to induce domain-conditioned probabilities for each sentence pair in the heterogeneous corpus.", "labels": [], "entities": []}, {"text": "Crucially, some sentence pairs will be more similar to a seed domain than others, whereas some sentence pairs might be dissimilar to all seed domains.", "labels": [], "entities": []}, {"text": "The number and choice of seed domains depends largely on the available resources but intuitively these seed domains are chosen to be relevant to parts of the heterogeneous corpus.", "labels": [], "entities": []}, {"text": "A small number of such seeds can be expected to notably improve word alignment accuracy.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.8127104640007019}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.7503244876861572}]}, {"text": "In fact, a single seed sample already allows us to exploit the contrast between two parts in the corpus: similar or dissimilar to the seed data.", "labels": [], "entities": []}, {"text": "Considering the small seed samples as partial supervision, in this paper we explore the question: how to obtain better word alignment in a heterogeneous, mix-of-domains corpus?", "labels": [], "entities": [{"text": "word alignment", "start_pos": 119, "end_pos": 133, "type": "TASK", "confidence": 0.7344889789819717}]}, {"text": "We present a novel latent domain HMM alignment model, which aims to tighten the probability estimates of the generative alignment process of a sentence pair, and of the probability estimates of the sentence pair itself fora specific domain.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.7828142642974854}, {"text": "generative alignment", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.9127218425273895}]}, {"text": "We also present an accompanying training regime guided by partial supervision using the seed samples, exploiting the contrast between the domain-conditioned alignment statistics in these samples.", "labels": [], "entities": []}, {"text": "This way we aim for an alignment model that is more domain-sensitive than the original HMM alignment model.", "labels": [], "entities": []}, {"text": "Once the domainconditioned statistics are induced, we discuss how to combine them together to express the probability of a sentence pair as a mixture over specific domains.", "labels": [], "entities": []}, {"text": "Finally, we report experimental results over heterogeneous corpora of 1M, 2M and 4M sentence pairs, where we are provided domain information for different samples of 10%, 5% and 2.5% of the heterogeneous data respectively.", "labels": [], "entities": []}, {"text": "A large number of experiments are reported, showing that the latent domain HMM model produces notable improvements in word alignment accuracy over the original HMM alignment model.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 118, "end_pos": 132, "type": "TASK", "confidence": 0.7091823220252991}, {"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.8723741769790649}]}, {"text": "Furthermore, the translation accuracy of the resulting SMT systems is significantly improved across four different translation tasks.", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9381902813911438}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9137967228889465}, {"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.989723801612854}]}], "datasetContent": [{"text": "In the following experiments, we use three heterogeneous English-Spanish corpora consisting of 1M , 2M and 4M sentence pairs respectively.", "labels": [], "entities": []}, {"text": "These corpora combine two parts.", "labels": [], "entities": []}, {"text": "The first part respectively 0.7M , 1.7M and 3.7M is collected from multiple domains and resources including EuroParl (  For alignment accuracy evaluation, we use a data set of 100 sentence pairs with their \"golden\" alignment from.", "labels": [], "entities": [{"text": "EuroParl", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.9789199829101562}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.851769208908081}]}, {"text": "Here, the golden alignment consists of sure links (S) and possible links (P ) for each sentence pair.", "labels": [], "entities": []}, {"text": "Counting the set of generating alignment links (A), we report the word alignment accuracy by precision ( |A\u2229P | |P | ), recall ( |A\u2229S| |S| ), alignment error rate (AER) (1 \u2212 |A\u2229P |+|A\u2229S| |A|+|S| ) (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.8774574995040894}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.999329686164856}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.999685525894165}, {"text": "alignment error rate", "start_pos": 142, "end_pos": 162, "type": "METRIC", "confidence": 0.9643785754839579}, {"text": "AER)", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9090434312820435}]}, {"text": "For all experiments, we use the same training configuration for both the baseline/the latent domain alignment model: 5 iterations for IBM model 1/the latent domain model; 3 iterations for HMM alignment model/the latent domain model.", "labels": [], "entities": []}, {"text": "For evaluation, we first align the sentence pairs in both directions and then symmetrize them using the growdiag-final heuristic ().", "labels": [], "entities": []}, {"text": "For reference we also report the performance of a considerably more expressive Model 4, capable of capturing more structure, but at the expense of intractable inference.", "labels": [], "entities": []}, {"text": "Using MGIZA++ (), we run 5 iterations for training Model 1, 3 iterations for training the HMM alignment model, Model 3 and Model 4.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.6875503957271576}]}, {"text": "In this section, we investigate the contribution of our model in terms of the translation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.8618348240852356}]}, {"text": "Here, we run experiments on the heterogeneous corpora of 1M, 2M, and 4M sentence pairs, testing the translation accuracy over four different domain-specific test sets related to News, Pharmacy, Legal, and Hardware.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.8774044513702393}]}, {"text": "We use a standard state-of-the-art phrase-based system as the baseline.", "labels": [], "entities": []}, {"text": "Our dense features include MOSES () baseline features, plus hierarchical lexicalized reordering model features (, and the word-level feature derived from IBM model 1 score, c.f.,).", "labels": [], "entities": []}, {"text": "The interpolated 5-grams LMs with Kneser-Ney are trained on a very large monolingual corpus of 2B words.", "labels": [], "entities": []}, {"text": "We tune the systems using kbest batch MIRA ().", "labels": [], "entities": [{"text": "MIRA", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9577462077140808}]}, {"text": "Finally, we use MOSES ( as decoder.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.6170272827148438}]}, {"text": "Our system has exactly the same setting with the baseline, except: (1) To learn the translation, we use the alignment result derived from our latent domain HMM alignment model, rather than the HMM alignment model; and (2) We replace the word-level feature with our four domain-conditioned word-level features derived from the latent domain IBM model 1.", "labels": [], "entities": []}, {"text": "Here, note that our latent model is learned with the supervision from the combining domain knowledge of all three domain-specific seed samples.", "labels": [], "entities": []}, {"text": "Note that similar results are also observed for training, in which a soft domain assignment using soft EM produces better alignment accuracy than a hard domain assignment using hard EM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9413127899169922}]}, {"text": "(See () for reference to hard domain assignment to training data.)", "labels": [], "entities": []}, {"text": "This is perhaps due to the characteristics of the data we use.", "labels": [], "entities": []}, {"text": "For instance, News sentence pairs are useful for translating Legal, Financial or EuroParl to varying degrees.", "labels": [], "entities": [{"text": "translating Legal, Financial or EuroParl", "start_pos": 49, "end_pos": 89, "type": "TASK", "confidence": 0.7682052552700043}]}, {"text": "11 For every phrase pair\u02dcfpair\u02dc pair\u02dcf , \u02dc e with their length of m \u02dc f and l\u02dcel\u02dce respectively, the lexical feature estimates a probability in Model 1 style between their word pairs fj, ei (i.e. .", "labels": [], "entities": []}, {"text": "Note that adding word-level features from both translation sides does not help much, as observed by).", "labels": [], "entities": []}, {"text": "We thus add only an one from a translation side.", "labels": [], "entities": []}, {"text": "For the News translation task, we tune systems on the News-test 2008 of 2, 051 sentence pairs and test them on the News-test 2013 of 3, 000 sentence pairs from the WMT 2013 shared task (.", "labels": [], "entities": [{"text": "News translation task", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.8004036347071329}, {"text": "News-test 2008", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.9765803217887878}, {"text": "News-test 2013", "start_pos": 115, "end_pos": 129, "type": "DATASET", "confidence": 0.9650441706180573}, {"text": "WMT 2013 shared task", "start_pos": 164, "end_pos": 184, "type": "DATASET", "confidence": 0.894862711429596}]}, {"text": "For the Pharmacy, Legal, and Hardware translation tasks, we tune systems on three domain-specific dev sets of 1, 000 sentence pairs and test them on three domain-specific test sets of 1, 016, 1, 326 and 1, 721 sentence pairs.", "labels": [], "entities": [{"text": "Hardware translation", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.6391107887029648}]}, {"text": "We report three metrics -BLEU (), METEOR (Denkowski and Lavie, 2011) and TER (), with statistical significance at 95% confidence interval under paired bootstrap re-sampling.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9989641904830933}, {"text": "METEOR", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9963309168815613}, {"text": "TER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9987989664077759}]}, {"text": "For every system reported, we run the optimizer three times, before running) for resampling and significance testing.", "labels": [], "entities": [{"text": "resampling", "start_pos": 81, "end_pos": 91, "type": "TASK", "confidence": 0.9314081072807312}]}, {"text": "+0.7 +0.3 -0.5: Averaged improvements across the tasks.", "labels": [], "entities": []}, {"text": "Results are in, showing significant improvements across four different test sets over different heterogeneous corpora sizes.", "labels": [], "entities": []}, {"text": "gives a summary of the improvements.", "labels": [], "entities": []}, {"text": "On average, over heterogeneous corpora of 1M, 2M and 4M sentence pairs, our system outperforms the baseline by 1.0 BLEU, 1.4 BLEU and 0.7 BLEU, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9979864358901978}, {"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9848666787147522}, {"text": "BLEU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9839472770690918}]}, {"text": "Finally, we observe that our system produces comparably good performance to the MGIZA++-based system.", "labels": [], "entities": [{"text": "MGIZA++-", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.8374917805194855}]}, {"text": "When 1M data is considered, on three of four tasks, our system produces at least compatible translation accuracy to the corresponding MGIZA++-based system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9047943353652954}, {"text": "MGIZA++-", "start_pos": 134, "end_pos": 142, "type": "DATASET", "confidence": 0.8197928071022034}]}, {"text": "Further analysis reveals that the improvement is due to not only the reduction in alignment error rate, but also the use of the domain-sensitive lexical features.", "labels": [], "entities": [{"text": "alignment error rate", "start_pos": 82, "end_pos": 102, "type": "METRIC", "confidence": 0.7503182490666708}]}, {"text": "Moreover, the domain-sensitive lexical features is particularly useful when the domain of the test data matches with the domain of seed samplers.", "labels": [], "entities": []}, {"text": "This is also widely observed in the literature, e.g., see).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Alignment accuracy over heterogeneous corpora.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.8554388880729675}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9147326350212097}]}, {"text": " Table 2: Conditional entropy of the statistics.", "labels": [], "entities": []}, {"text": " Table 3: Domain-conditioned statistics combination  for Viterbi decoding. The reported results are for the  heterogeneous corpus of 1M sentence pairs. Similar  results are observed for other training data.", "labels": [], "entities": []}, {"text": " Table 4: Metric scores for the systems, which are  averages over multiple runs. Bold results indicate  that the comparison is significant over the baseline.", "labels": [], "entities": []}]}