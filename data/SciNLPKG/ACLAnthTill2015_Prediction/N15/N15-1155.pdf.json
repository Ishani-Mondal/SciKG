{"title": [{"text": "Combining Word Embeddings and Feature Embeddings for Fine-grained Relation Extraction", "labels": [], "entities": [{"text": "Fine-grained Relation Extraction", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.6314294934272766}]}], "abstractContent": [{"text": "Compositional embedding models build a representation fora linguistic structure based on its component word embeddings.", "labels": [], "entities": []}, {"text": "While recent work has combined these word embed-dings with hand crafted features for improved performance, it was restricted to a small number of features due to model complexity, thus limiting its applicability.", "labels": [], "entities": []}, {"text": "We propose anew model that conjoins features and word em-beddings while maintaing a small number of parameters by learning feature embeddings jointly with the parameters of a compositional model.", "labels": [], "entities": []}, {"text": "The result is a method that can scale to more features and more labels, while avoiding overfitting.", "labels": [], "entities": []}, {"text": "We demonstrate that our model attains state-of-the-art results on ACE and ERE fine-grained relation extraction.", "labels": [], "entities": [{"text": "ERE fine-grained relation extraction", "start_pos": 74, "end_pos": 110, "type": "TASK", "confidence": 0.5363755747675896}]}], "introductionContent": [{"text": "Word embeddings represent words in some lowdimensional space, where each dimension might intuitively correspond to some syntactic or semantic property of the word.", "labels": [], "entities": []}, {"text": "1 These embeddings can be used to create novel features, and can also be treated as model parameters * The work was done while the author was visiting JHU.", "labels": [], "entities": [{"text": "JHU", "start_pos": 151, "end_pos": 154, "type": "DATASET", "confidence": 0.9851328134536743}]}, {"text": "1 Such embeddings have along history in NLP, such as term co-occurrence frequency matrices and their low-dimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF) and word clusters.", "labels": [], "entities": []}, {"text": "Recently, neural networks have become popular methods for obtaining such embeddings ( to build representations for higher-level structures in some compositional embedding models).", "labels": [], "entities": []}, {"text": "Applications of embedding have boosted the performance of many NLP tasks, including syntax (), semantics), question answering () and machine translation).", "labels": [], "entities": [{"text": "question answering", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.845856785774231}, {"text": "machine translation", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.791172593832016}]}, {"text": "While compositional models aim to learn higherlevel structure representations, composition of embeddings alone may not capture important syntactic or semantic patterns.", "labels": [], "entities": []}, {"text": "Consider the task of relation extraction, where decisions require examining long-distance dependencies in a sentence.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8686107993125916}]}, {"text": "For the sentence in, \"driving\" is a strong indicator of the \"ART\" (ACE) relation because it appears on the dependency path between a person and a vehicle.", "labels": [], "entities": [{"text": "ART\" (ACE) relation", "start_pos": 61, "end_pos": 80, "type": "METRIC", "confidence": 0.8450812300046285}]}, {"text": "Yet such conjunctions of different syntactic/semantic annotations (dependency and NER) are typically not available in compositional models.", "labels": [], "entities": []}, {"text": "In contrast, hand-crafted features can easily capture this information, e.g. feature f i3.", "labels": [], "entities": []}, {"text": "Therefore, engineered features should be combined with learned representations in compositional models.", "labels": [], "entities": []}, {"text": "One approach is to use the features to select specific transformations fora sub-structure, which can conjoin features and word embeddings, but is impractical as the numbers of transformations will exponentially increase with additional features.", "labels": [], "entities": []}, {"text": "Typically, less than 10 features are used.", "labels": [], "entities": []}, {"text": "A solution 1374 -.5 .3 .8 .7 0 0 0 0 -.5 .3 .8 .7 0 0 0 0 0 0 0 0 -.5 .3 .8 .7 -.5 .3 .8 .7 -.5 .3 .8 .7  is provided by the recent work of, which reduces this complexity by using a tensor to transform the input feature vectors to a matrix transformation.", "labels": [], "entities": []}, {"text": "The model is equivalent to treating the outer product between word embeddings and features as input to a parameter tensor, thus model parameters increase linearly with the number of features.", "labels": [], "entities": []}, {"text": "Yet this model also uses too many parameters when a large number of features (e.g. over 1000) are used.", "labels": [], "entities": []}, {"text": "This limits the applicability of their method to settings where there area large number of training examples.", "labels": [], "entities": []}, {"text": "For smaller training sets, the variance of their estimator will be high resulting in increased generalization error on test data.", "labels": [], "entities": []}, {"text": "We seek to use many more features (based on rich annotations such as syntactic parsing and NER) and larger label sets, which further exacerbates the problem of overfitting.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7789019346237183}]}, {"text": "We propose anew method of learning interactions between engineered features and word embeddings by combining the idea of the outer product in FCM () with learning feature embeddings).", "labels": [], "entities": []}, {"text": "Our model jointly learns feature embeddings and a tensor-based classifier which relies on the outer product between features embeddings and word embeddings.", "labels": [], "entities": []}, {"text": "Therefore, the number of parameters are dramatically reduced since features are only represented as low-dimensional embeddings, which alleviates problems with overfitting.", "labels": [], "entities": []}, {"text": "The resulting model benefits from both approaches: conjunctions between feature and word embeddings allow model expressiveness, while keeping the number of parameters small.", "labels": [], "entities": []}, {"text": "This is especially beneficial when considering tasks with many labels, such as fine-grained relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.6953979134559631}]}, {"text": "We demonstrate these advantages on two relation extraction tasks: the well studied ACE 2005 dataset and the new ERE relation extraction task.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8414998352527618}, {"text": "ACE 2005 dataset", "start_pos": 83, "end_pos": 99, "type": "DATASET", "confidence": 0.982933779557546}, {"text": "ERE relation extraction", "start_pos": 112, "end_pos": 135, "type": "TASK", "confidence": 0.6531128684679667}]}, {"text": "We consider both coarse and finegrained relations, the latter of which has been largely unexplored in previous work.", "labels": [], "entities": [{"text": "coarse", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9500139355659485}]}], "datasetContent": [{"text": "Datasets We consider two relation extraction datasets: ACE2005 and ERE, both of which contain two sets of relations: coarse relation types and fine relation (sub-)types.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7345889508724213}, {"text": "ACE2005", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.9166761636734009}, {"text": "ERE", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.8499783277511597}]}, {"text": "Prior work on English ACE 2005 has focused only on coarse relations; to the best of our knowledge, this paper establishes the first baselines for the other datasets.", "labels": [], "entities": [{"text": "English ACE 2005", "start_pos": 14, "end_pos": 30, "type": "DATASET", "confidence": 0.7282622853914896}]}, {"text": "Since the fine-grained relations require a large number of parameters, they will test the ability: Results on test for ACE and ERE where only the entity spans (S) are known (top) and where both the entity spans and types are known (ST).", "labels": [], "entities": [{"text": "ERE", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.8410423994064331}]}, {"text": "PM'13 is an embedding method.", "labels": [], "entities": [{"text": "PM'13", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9554643034934998}]}, {"text": "The sizes of relation sets are indicated by |L|. of LRFCM to scale and generalize.", "labels": [], "entities": []}, {"text": "As is standard, we report precision, recall, and F1 for all tasks.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9937412142753601}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9994878768920898}, {"text": "F1", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9998283386230469}]}, {"text": "ACE 2005 We use the English portion of the ACE 2005 corpus ().", "labels": [], "entities": [{"text": "ACE 2005", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9755667746067047}, {"text": "ACE 2005 corpus", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.902152399222056}]}, {"text": "Following, we train on the union of the news domains (Newswire and Broadcast News), holdout half of the Broadcast Conversation (bc) domain as development data, and evaluate on the remainder of bc.", "labels": [], "entities": [{"text": "Newswire", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.9662717580795288}]}, {"text": "There are 11 coarse types and 32 fine (sub-)type classes in total.", "labels": [], "entities": []}, {"text": "In order to compare with traditional feature-based methods), we report results in which the gold entity spans and types are available at both train and test time.", "labels": [], "entities": []}, {"text": "We train the models with all pairs of entity mentions in the training set to yield 43,518 classification instances.", "labels": [], "entities": []}, {"text": "Furthermore, for comparison with prior work on embeddings for relation extraction, we report results using gold entity spans but no types, and generate negative relation instances from all pairs of entities within each sentence with three or fewer intervening entities.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.809005081653595}]}, {"text": "ERE We use the third release of the ERE annotations from Phase 1 of DEFT (LDC, 2013) . We divided the proxy reports summarizing news articles (pr) into training (56,889 relations), development (6,804 relations) and test data (6,911 relations).", "labels": [], "entities": [{"text": "DEFT (LDC, 2013)", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.9135530292987823}]}, {"text": "We run experiments under both the settings with and without gold entity types, while generating negative relation instances just as in ACE with the gold entity types setting.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to report results on this task.", "labels": [], "entities": []}, {"text": "Following the annotation guidelines of ERE relations, we treat all relations, except for \"social.business\", \"social.family\" and \"social.unspecified\", as asymmetric relations.", "labels": [], "entities": []}, {"text": "For coarse relation task, we treat all relations as asymmetric, including the \"social\" relation.", "labels": [], "entities": []}, {"text": "The reason is that the asymmetric subtype, \"social.role\", dominates the class: 679 of 834 total \"social\" relations.", "labels": [], "entities": []}, {"text": "Setup We randomly initialize the feature embeddings W f and pre-train 200-dimensional word embeddings on the NYT portion of Gigaword 5.0 (Parker et al., 2011) with word2vec (default setting of the toolkit) ().", "labels": [], "entities": [{"text": "NYT portion of Gigaword 5.0", "start_pos": 109, "end_pos": 136, "type": "DATASET", "confidence": 0.9319605350494384}]}, {"text": "Dependency parses are obtained from the Stanford Parser).", "labels": [], "entities": [{"text": "Stanford Parser", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.9731919169425964}]}, {"text": "We use the same feature templates as.", "labels": [], "entities": []}, {"text": "When gold entity types are unavailable, we replace them with WordNet tags annotated by.", "labels": [], "entities": [{"text": "WordNet tags", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.9103799164295197}]}, {"text": "Learning rates, weights of L2-regularizations, the number of iterations and the size of the feature embeddings dare tuned on dev sets.", "labels": [], "entities": []}, {"text": "We selected d from {12, 15, 20, 25, 30, 40}.", "labels": [], "entities": []}, {"text": "We used d=30 for feature embeddings for fine-grained ACE without gold types, and d=20 otherwise.", "labels": [], "entities": []}, {"text": "For ERE, we have d=15.", "labels": [], "entities": [{"text": "ERE", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9942261576652527}]}, {"text": "The weights of L2 \u03bb was selected from {1e-3, 5e-4, 1e-4}.", "labels": [], "entities": []}, {"text": "As in prior work (  Analysis To highlight differences in the results we provide the confusion matrix of the two models on ERE fine relations.", "labels": [], "entities": []}, {"text": "shows that the two models are complementary to each other to a certain degree.", "labels": [], "entities": []}, {"text": "It indicates that the combination of FCM and LRFCM may further boost the performance.", "labels": [], "entities": [{"text": "FCM", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.6094514727592468}, {"text": "LRFCM", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9767959117889404}]}, {"text": "We leave the combination of FCM and LRFCM, as well as their combination with the baseline method, to future work.", "labels": [], "entities": [{"text": "FCM", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.8372741937637329}, {"text": "LRFCM", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.5821150541305542}]}], "tableCaptions": [{"text": " Table 1: Results on test for ACE and ERE where only the entity spans (S) are known (top) and where both the entity", "labels": [], "entities": []}, {"text": " Table 2: Confusion Matrix between the results of FCM", "labels": [], "entities": [{"text": "FCM", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.7130106091499329}]}]}