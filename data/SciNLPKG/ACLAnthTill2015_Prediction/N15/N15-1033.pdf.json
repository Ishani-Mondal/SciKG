{"title": [{"text": "Multi-Target Machine Translation with Multi-Synchronous Context-free Grammars", "labels": [], "entities": [{"text": "Multi-Target Machine Translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6190987825393677}]}], "abstractContent": [{"text": "We propose a method for simultaneously translating from a single source language to multiple target languages T1, T2, etc.", "labels": [], "entities": []}, {"text": "The motivation behind this method is that if we only have a weak language model for T1 and translations in T1 and T2 are associated, we can use the information from a strong language model over T2 to disambiguate the translations in T1, providing better translation results.", "labels": [], "entities": []}, {"text": "As a specific framework to realize multi-target translation , we expand the formalism of synchronous context-free grammars to handle multiple targets , and describe methods for rule extraction, scoring, pruning, and search with these models.", "labels": [], "entities": [{"text": "multi-target translation", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.734888106584549}, {"text": "rule extraction", "start_pos": 177, "end_pos": 192, "type": "TASK", "confidence": 0.766762763261795}]}, {"text": "Experiments find that multi-target translation with a strong language model in a similar second target language can provide gains of up to 0.8-1.5 BLEU points.", "labels": [], "entities": [{"text": "multi-target translation", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.6882282197475433}, {"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9992596507072449}]}], "introductionContent": [{"text": "In statistical machine translation (SMT), the great majority of work focuses on translation of a single language pair, from the source F to the target E.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.8341835091511408}]}, {"text": "However, in many actual translation situations, identical documents are translated not from one language to another, but between a large number of different languages.", "labels": [], "entities": []}, {"text": "Examples of this abound in commercial translation, and prominent open data sets used widely by the MT community include UN documents in 6 languages (  (, and video subtitles on TED in as many as 50 languages (.", "labels": [], "entities": [{"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.9442386031150818}]}], "datasetContent": [{"text": "We evaluate the proposed multi-target translation method through translation experiments on the MultiUN corpus.", "labels": [], "entities": [{"text": "multi-target translation", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.657909482717514}, {"text": "MultiUN corpus", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.9694208204746246}]}, {"text": "We choose this corpus as it contains a large number of parallel documents in Arabic (ar), English (en), Spanish (es), French (fr), Russian (ru), and Chinese (zh), languages with varying degrees of similarity.", "labels": [], "entities": []}, {"text": "We use English as our source sentence in all cases, as it is the most common actual source language for UN documents.", "labels": [], "entities": []}, {"text": "To prepare the data, we first deduplicate the sentences in the corpus, then holdout 1,500 sentences each for tuning and test.", "labels": [], "entities": []}, {"text": "In our basic training setup, we use 100k sentences for training both the TM and the T1 LM.", "labels": [], "entities": []}, {"text": "This somewhat small number is to simulate a T1 language that has relatively few resources.", "labels": [], "entities": []}, {"text": "For the T2 language, we assume we have a large language model trained on all of the UN data, amounting to 3.5M sentences total.", "labels": [], "entities": [{"text": "UN data", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.8958413302898407}]}, {"text": "As a decoder, we use the Travatar (Neubig, 2013) toolkit, and implement all necessary extensions to the decoder and rule extraction code to allow for multiple targets.", "labels": [], "entities": [{"text": "Travatar (Neubig, 2013) toolkit", "start_pos": 25, "end_pos": 56, "type": "DATASET", "confidence": 0.7872332419667926}, {"text": "rule extraction", "start_pos": 116, "end_pos": 131, "type": "TASK", "confidence": 0.6999965161085129}]}, {"text": "Unless otherwise specified, we use joint search with a pop limit of 2,000, and T1 rule pruning with a limit of 10 rules per source rule.", "labels": [], "entities": [{"text": "T1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9373897314071655}]}, {"text": "BLEU is used for both tuning and evaluating all models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9823291897773743}]}, {"text": "In particular, we tune and evaluate all models based on T1 BLEU, simulating a situation similar to that in the introduction, where we want to use a large LM in T2 to help translation in T1.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.908449113368988}]}, {"text": "In order to control for optimizer instability, we follow's recommendation of performing tuning 3 times, and reporting the average of the runs along with statistical significance obtained by pairwise bootstrap resampling).", "labels": [], "entities": []}, {"text": "In this section we first perform experiments to investigate the effectiveness of the overall framework of multi-target translation.", "labels": [], "entities": [{"text": "multi-target translation", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.7441928386688232}]}, {"text": "We assess four models, starting with standard single-target SCFGs and moving gradually towards our full MSCFG model: SCFG: A standard SCFG grammar with only the source and T1.", "labels": [], "entities": []}, {"text": "SCFG+T2Al: SCFG constrained during rule extraction to only extract rules that also match the T2 alignments.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.8058053851127625}]}, {"text": "This will help measure the effect, if any, of being limited by T2 alignments in rule extraction.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.8587532937526703}]}], "tableCaptions": [{"text": " Table 1: Results for standard Hiero (SCFG), SCFG with  T2 extraction constraints (SCFG+T2Al), a multi-SCFG  minus the T2 LM (MSCFG-T2LM), and full multi-target  translation (MSCFG). Bold indicates the highest BLEU  score, and daggers indicate statistically significant gains  over SCFG ( \u2020: p < 0.05,  \u2021: p < 0.01)", "labels": [], "entities": [{"text": "BLEU  score", "start_pos": 210, "end_pos": 221, "type": "METRIC", "confidence": 0.9793895781040192}]}, {"text": " Table 2: Differences in rule table sizes for a T1 of French.", "labels": [], "entities": [{"text": "French", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.8661565780639648}]}, {"text": " Table 3: BLEU scores by pruning criterion. Columns  indicate T1 (fr or zh) and the pruning criterion (T1+T2  joint probability, or T1 probability plus max T2). Rows  indicate T2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9963323473930359}, {"text": "T1", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.9667114019393921}]}]}