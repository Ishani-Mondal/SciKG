{"title": [{"text": "Embedding a Semantic Network in a Word Space", "labels": [], "entities": [{"text": "Embedding a Semantic Network in a Word Space", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.6310993544757366}]}], "abstractContent": [{"text": "We present a framework for using continuous-space vector representations of word meaning to derive new vectors representing the meaning of senses listed in a semantic network.", "labels": [], "entities": []}, {"text": "It is a post-processing approach that can be applied to several types of word vector representations.", "labels": [], "entities": []}, {"text": "It uses two ideas: first, that vectors for polysemous words can be decomposed into a convex combination of sense vectors; secondly , that the vector fora sense is kept similar to those of its neighbors in the network.", "labels": [], "entities": []}, {"text": "This leads to a constrained optimization problem , and we present an approximation for the case when the distance function is the squared Euclidean.", "labels": [], "entities": []}, {"text": "We applied this algorithm on a Swedish semantic network, and we evaluate the quality of the resulting sense representations extrinsi-cally by showing that they give large improvements when used in a classifier that creates lexical units for FrameNet frames.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing word meaning computationally is central in natural language processing.", "labels": [], "entities": [{"text": "Representing word meaning computationally", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8912559151649475}, {"text": "natural language processing", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.6404646833737692}]}, {"text": "Manual, knowledge-based approaches to meaning representation maps word strings to symbolic concepts, which can be described using any knowledge representation framework; using the relations between concepts defined in the knowledge base, we can infer implicit facts from the information stated in a text: a mouse is a rodent, so it has prominent teeth.", "labels": [], "entities": [{"text": "meaning representation", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.8025443851947784}]}, {"text": "Conversely, data-driven meaning representation approaches rely on cooccurrence patterns to derive a vector representation.", "labels": [], "entities": [{"text": "meaning representation", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.714250847697258}]}, {"text": "There are two classes of methods that compute word vectors: context-counting and context-predicting; while the latter has seen much interest lately, their respective strengths and weaknesses are still being debated ().", "labels": [], "entities": []}, {"text": "The most important relation defined in a vector space between the meaning of two words is similarity: a mouse is something quite similar to a rat.", "labels": [], "entities": [{"text": "similarity", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9811663031578064}]}, {"text": "Similarity of meaning is operationalized in terms of geometry, by defining a distance metric.", "labels": [], "entities": []}, {"text": "Symbolic representations seem to have an advantage in describing word sense ambiguity: when a surface form corresponds to more than one concept.", "labels": [], "entities": [{"text": "word sense ambiguity", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.7328999837239584}]}, {"text": "For instance, the word mouse can refer to a rodent or an electronic device.", "labels": [], "entities": []}, {"text": "Vector-space representations typically represent surface forms only, which makes it hard to search e.g. fora group of words similar to the rodent sense of mouse or to reliably use the vectors in classifiers that rely on the semantics of the word.", "labels": [], "entities": []}, {"text": "There have been several attempts to create vectors representing senses, most of them based on some variant of the idea first proposed by: that senses can be seen as clusters of similar contexts.", "labels": [], "entities": []}, {"text": "Recent examples in this tradition include the work by and.", "labels": [], "entities": []}, {"text": "However, because sense distributions are often highly imbalanced, it is not clear that context clusters can be reliably created for senses that occur rarely.", "labels": [], "entities": []}, {"text": "These approaches also lack interpretability: if we are interested in the rodent sense of mouse, which of the vectors should we use?", "labels": [], "entities": []}, {"text": "In this work, we instead derive sense vectors by embedding the graph structure of a semantic network in the word space.", "labels": [], "entities": []}, {"text": "By combining two complementary sources of information -corpus statistics and network structure -we derive useful vectors also for concepts that occur rarely.", "labels": [], "entities": []}, {"text": "The method, which can be applied to context-counting as well as context-predicting spaces, works by decompos-ing word vectors as linear combinations of sense vectors, and by pushing the sense vectors towards their neighbors in the semantic network.", "labels": [], "entities": []}, {"text": "This intuition leads to a constrained optimization problem, for which we present an approximate algorithm.", "labels": [], "entities": []}, {"text": "We applied the algorithm to derive vectors for the senses in a Swedish semantic network, and we evaluated their quality extrinsically by using them as features in a semantic classification task -mapping senses to their corresponding FrameNet frames.", "labels": [], "entities": []}, {"text": "When using the sense vectors in this task, we saw a large improvement over using word vectors.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluating intrinsically using e.g. a correlation between a graph-based similarity measure and geometric similarity would be problematic, since this is in some sense what our algorithm optimizes.", "labels": [], "entities": []}, {"text": "We therefore evaluate extrinsically, by using the sense vectors in a classifier that maps senses to semantic classes defined by FrameNet (.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 128, "end_pos": 136, "type": "DATASET", "confidence": 0.9134789705276489}]}, {"text": "FrameNet is a semantic database consisting of two parts: first, an ontology of semantic framesthe classes -and secondly a lexicon that maps word senses to frames.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.844740092754364}]}, {"text": "In standard FrameNet terminology, the senses assigned to a frame are called its lexical units (LUs).", "labels": [], "entities": []}, {"text": "Coverage is often a problem in frame-semantic lexicons, and this has a negative impact on the quality of NLP systems using FrameNet.", "labels": [], "entities": []}, {"text": "The task of finding LUs for frames is thus a useful testbed for evaluating lemma and sense vectors.", "labels": [], "entities": []}, {"text": "To evaluate, we used 567 frames from the Swedish FrameNet (Friberg Heppin and Toporowska Gronostaj, 2012); in total we had 28,842 verb, noun, adjective, and adverb LUs, which we split into training (67%) and test sets (33%).", "labels": [], "entities": [{"text": "Swedish FrameNet", "start_pos": 41, "end_pos": 57, "type": "DATASET", "confidence": 0.7635753452777863}]}, {"text": "For each frame, we trained a SVM with LIBLINEAR, using the LUs in that frame as positive training instances, and all other LUs as negative instances.", "labels": [], "entities": [{"text": "LIBLINEAR", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9825855493545532}]}, {"text": "Each LU was represented as a vector: its lemma or sense embedding normalized to unit length.", "labels": [], "entities": []}, {"text": "shows the precision, recall, and Fmeasure for the classifiers for the five frames with most LUs, and finally the micro-average overall frames.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9997604489326477}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.999657154083252}, {"text": "Fmeasure", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9998101592063904}]}, {"text": "In the overall evaluation as well as in four out of the five largest frames, the classifiers using sense vectors clearly outperform those using lemma vectors.", "labels": [], "entities": []}, {"text": "The frame where we do not see any improvement by introducing sense distinctions, PEO-PLE BY VOCATION, contains terms for professions such as painter and builder; since SALDO derives such terms from their corresponding verbs rather than from a common hypernym (e.g. worker), they do not form a coherent subnetwork in SALDO or subregion in the embedding space.", "labels": [], "entities": [{"text": "PEO-PLE BY VOCATION", "start_pos": 81, "end_pos": 100, "type": "METRIC", "confidence": 0.9140827655792236}]}], "tableCaptions": [{"text": " Table 3: FrameNet lexical unit classification.", "labels": [], "entities": [{"text": "FrameNet lexical unit classification", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.6393001973628998}]}]}