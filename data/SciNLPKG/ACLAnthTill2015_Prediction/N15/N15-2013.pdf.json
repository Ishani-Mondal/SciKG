{"title": [{"text": "Relation Extraction from Community Generated Question-Answer Pairs", "labels": [], "entities": [{"text": "Relation Extraction from Community Generated Question-Answer Pairs", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.8805880376270839}]}], "abstractContent": [{"text": "Community question answering (CQA) web-sites contain millions of question and answer (QnA) pairs that represent real users' interests.", "labels": [], "entities": [{"text": "Community question answering (CQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.764279971520106}]}, {"text": "Traditional methods for relation extraction from natural language text operate over individual sentences.", "labels": [], "entities": [{"text": "relation extraction from natural language text", "start_pos": 24, "end_pos": 70, "type": "TASK", "confidence": 0.9131607413291931}]}, {"text": "However answer text is sometimes hard to understand without knowing the question, e.g., it may not name the subject or relation of the question.", "labels": [], "entities": []}, {"text": "This work presents a novel model for relation extraction from CQA data, which uses discourse of QnA pairs to predict relations between entities mentioned in question and answer sentences.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8509333729743958}]}, {"text": "Experiments on 2 publicly available datasets demonstrate that the model can extract from \u223c20% to \u223c40% additional relation triples, not extracted by existing sentence-based models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently all major search companies have adopted knowledge bases (KB), and as a result users now can get rich structured data as answers to some of their questions.", "labels": [], "entities": []}, {"text": "However, even the largest existing knowledge bases, such as Freebase (,), NELL, Google Knowledge Graph etc., which store billions of facts about millions of entities, are far from being complete (.", "labels": [], "entities": []}, {"text": "A lot of information is hidden in unstructured data, such as natural language text, and extracting this information for knowledge base population (KBP) is an active area of research (.", "labels": [], "entities": []}, {"text": "One particularly interesting source of unstructured text data is CQA websites (e.g. Yahoo!", "labels": [], "entities": []}, {"text": "Answers, 1 Answers.com, 2 etc.), which became very popular resources for question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.9080249071121216}]}, {"text": "The information expressed there can be very useful, for example, to answer future questions, which makes it attractive for knowledge base population.", "labels": [], "entities": []}, {"text": "Although some of the facts mentioned in QnA pairs can also be found in some other text documents, another part might be unique (e.g. in Clueweb 3 about 10% of entity pairs with existing Freebase relations mentioned in Yahoo!Answers documents cannot be found in other documents).", "labels": [], "entities": []}, {"text": "There are certain limitations in applying existing relation extraction algorithms to CQA data, i.e., they typically consider sentences independently and ignore the discourse of QnA pair text.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7321925908327103}]}, {"text": "However, often it is impossible to understand the answer without knowing the question.", "labels": [], "entities": []}, {"text": "For example, in many cases users simply give the answer to the question without stating it in a narrative sentence (e.g. \"What does \"xoxo\" stand for?", "labels": [], "entities": []}, {"text": "Hugs and kisses.\"), in some other cases the answer contains a statement, but some important information is omitted (e.g. \"What's the capital city of Bolivia?", "labels": [], "entities": []}, {"text": "Sucre is the legal capital, though the government sits in La Paz\").", "labels": [], "entities": []}, {"text": "In this work we propose a novel model for relation extraction from CQA data, that uses discourse of a QnA pair to extract facts between entities mentioned in question and entities mentioned in answer sentences.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8468749523162842}]}, {"text": "The conducted experiments confirm that many of such facts cannot be extracted by existing sentence-based techniques and thus it is beneficial to combine their outputs with the output of our model.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we use a subset of 29 Freebase predicates that have enough unique instances annotated in our corpus, e.g. date of birth, profession, nationality, education institution, date of death, disease symptoms and treatments, book author, artist album, etc.", "labels": [], "entities": []}, {"text": "We train and test the models on each dataset separately.", "labels": [], "entities": []}, {"text": "Each corpus is randomly split for training (75%) and testing (25%).", "labels": [], "entities": []}, {"text": "Knowledge base facts are also split into training and testing sets (50% each).", "labels": [], "entities": []}, {"text": "QnA and sentence-based models predict labels for each entity pair mention, and we aggregate mention predictions by taking the maximum score for each predicate.", "labels": [], "entities": []}, {"text": "We do the same aggregation to produce a combination of QnA-and sentence-based models, i.e., all extractions produced by the models are combined and if there are multiple extractions of the same fact we take the maximum score as the final confidence.", "labels": [], "entities": []}, {"text": "The precision and recall of extractions are evaluated on a test set of Freebase triples, i.e. an extracted triple is considered correct if it belongs to the test set of Freebase triples, which are not used for training (triples used for training are simply ignored).", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994799494743347}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9987300038337708}, {"text": "Freebase", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.957470715045929}, {"text": "Freebase", "start_pos": 169, "end_pos": 177, "type": "DATASET", "confidence": 0.9524226188659668}]}, {"text": "Note, that this only provides a lower bound on the model performance as some of the predicted facts can be correct and simply missing in Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 137, "end_pos": 145, "type": "DATASET", "confidence": 0.9648409485816956}]}, {"text": "shows Precision-Recall curves for QnAbased and sentence-based baseline models and some numeric results are given in.", "labels": [], "entities": [{"text": "Precision-Recall", "start_pos": 6, "end_pos": 22, "type": "METRIC", "confidence": 0.9850552082061768}]}, {"text": "As 100% recall we took all pairs of entities that can be extracted by either model.", "labels": [], "entities": [{"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.992733359336853}]}, {"text": "It is important to note, that since some entity pairs occur exclusively inside the answer sentences and some in pairs of question and answer sentences, none of the individual models is capable of achieving 100% recall, and maximum possible recalls for QnA-and sentence-based models are different.", "labels": [], "entities": [{"text": "recall", "start_pos": 211, "end_pos": 217, "type": "METRIC", "confidence": 0.9973258972167969}]}], "tableCaptions": [{"text": " Table 1: Examples of features used for relation extraction for \"When was Mariah Carey born? Mariah  Carey was born 27 March 1970\"", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.9119395613670349}, {"text": "Mariah Carey born? Mariah  Carey was born 27 March 1970", "start_pos": 74, "end_pos": 129, "type": "DATASET", "confidence": 0.6808215298435905}]}, {"text": " Table 2: Yahoo! Answers and WikiAnswers datasets statistics", "labels": [], "entities": [{"text": "Yahoo! Answers and WikiAnswers datasets", "start_pos": 10, "end_pos": 49, "type": "DATASET", "confidence": 0.795599122842153}]}, {"text": " Table 3: Extraction results for QnA-and sentence-based models on both datasets", "labels": [], "entities": []}]}