{"title": [{"text": "A Corpus and Model Integrating Multiword Expressions and Supersenses", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces a task of identifying and semantically classifying lexical expressions in running text.", "labels": [], "entities": []}, {"text": "We investigate the online reviews genre, adding semantic supersense annotations to a 55,000 word English corpus that was previously annotated for multiword expressions.", "labels": [], "entities": []}, {"text": "The noun and verb supersenses apply to full lexical expressions, whether single-or mul-tiword.", "labels": [], "entities": []}, {"text": "We then present a sequence tagging model that jointly infers lexical expressions and their supersenses.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.7022461295127869}]}, {"text": "Results show that even with our relatively small training corpus in a noisy domain, the joint task can be performed to attain 70% class labeling F 1 .", "labels": [], "entities": [{"text": "class labeling F 1", "start_pos": 130, "end_pos": 148, "type": "METRIC", "confidence": 0.5762919448316097}]}], "introductionContent": [{"text": "The central challenge in computational lexical semantics for text corpora is to develop and apply abstractions that characterize word meanings beyond what can be derived superficially from the orthography.", "labels": [], "entities": []}, {"text": "Such abstractions can be found in type-level human-curated lexical resources such as WordNet), but such intricate resources are expensive to build and difficult to annotate with at the token level, hindering their applicability beyond a narrow selection of languages and domains.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.970607340335846}]}, {"text": "A more portable and scalable-yet still linguisticallygrounded-way to represent lexical meanings is with coarse-grained semantic classes.", "labels": [], "entities": []}, {"text": "Here we build on prior work with an inventory of semantic classes (for nouns and verbs) known as supersenses.", "labels": [], "entities": []}, {"text": "The 41 supersenses resemble the types used for named entities (PERSON, LOCATION, etc.), but are more general, with semantic categories relevant to common nouns and verbs as well.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9204651713371277}]}, {"text": "As a result, their application to sentences is dense (describing a large proportion of tokens), in contrast to annotations that only describe named entities.", "labels": [], "entities": []}, {"text": "Because most supersense tagging studies have worked with data originally annotated for finegrained WordNet senses, then automatically mapped to supersenses, the resulting systems have been tied to the lexical coverage of WordNet. and overcame this limitation in part by annotating supersenses directly in text; thus, nouns and verbs not in WordNet were not neglected.", "labels": [], "entities": [{"text": "supersense tagging", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7473241686820984}, {"text": "WordNet.", "start_pos": 221, "end_pos": 229, "type": "DATASET", "confidence": 0.9741595983505249}, {"text": "WordNet", "start_pos": 340, "end_pos": 347, "type": "DATASET", "confidence": 0.9568384289741516}]}, {"text": "However, the issue of which units ought to receive supersenses has not been addressed satisfactorily.", "labels": [], "entities": []}, {"text": "We argue that the semantically holistic nature of multiword expressions (MWEs) including idioms, light verb constructions, verb-particle constructions, and many compounds ( means that they should be considered as units for manual and automatic supersense tagging.", "labels": [], "entities": [{"text": "supersense tagging", "start_pos": 244, "end_pos": 262, "type": "TASK", "confidence": 0.6723736673593521}]}, {"text": "Below, we motivate the need for an integrated representation for broad-coverage lexical semantic analysis that identifies MWEs and labels single-and multiword noun and verb expressions with supersenses ( \u00a72).", "labels": [], "entities": [{"text": "broad-coverage lexical semantic analysis", "start_pos": 65, "end_pos": 105, "type": "TASK", "confidence": 0.7326159626245499}]}, {"text": "By annotating supersenses directly on sentences with existing comprehensive MWE annotations, we circumvent WordNet's spotty coverage of many kinds of MWEs ( \u00a73).", "labels": [], "entities": []}, {"text": "Then we demonstrate that the two kinds of information are readily combined in a discriminative sequence tagging model ( \u00a74).", "labels": [], "entities": []}, {"text": "Notably, our analyzer handles gappy expressions that are ignored by existing supersense taggers, and it marks miscellaneous MWEs even though they do not receive a noun or verb supersense.", "labels": [], "entities": []}, {"text": "Our annotations of the REVIEWS section of the English Web Treebank (, which", "labels": [], "entities": [{"text": "REVIEWS section of the English Web Treebank", "start_pos": 23, "end_pos": 66, "type": "DATASET", "confidence": 0.8709993575300489}]}], "datasetContent": [{"text": "Our setup mostly echoes that of.", "labels": [], "entities": []}, {"text": "We adopt their train (3312 sentences/ 48k words) vs. test (500 sentences/7k words) split, and tune hyperparameters by 8-fold cross-validation on train.", "labels": [], "entities": []}, {"text": "By this procedure we chose a percept cutoff of 5 to use throughout, and tuned the number of training iterations for each experimental condition (early stopping within each cross-validation fold so as to greedily maximize tagging accuracy on the held-out portion, and averaging the best number of iterations across folds).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 229, "end_pos": 237, "type": "METRIC", "confidence": 0.886102557182312}]}, {"text": "For simplicity, we use oracle POS tags in our experiments and do not use recall-oriented cost function.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 73, "end_pos": 88, "type": "METRIC", "confidence": 0.9898713231086731}]}, {"text": "Experiments were managed with Jonathan Clark's ducttape tool.", "labels": [], "entities": []}, {"text": "shows full supersense tagging results, separating the MWE identification performance (measured by link-based precision, recall, and F 1 ; see) from the precision, recall, and F 1 of class labels on the first token of each expression (segments with no class label are ignored).", "labels": [], "entities": [{"text": "supersense tagging", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.6467147022485733}, {"text": "MWE identification", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.927295058965683}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.7361684441566467}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9956876635551453}, {"text": "F 1", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9963213205337524}, {"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9959813356399536}, {"text": "recall", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.9948996901512146}, {"text": "F 1", "start_pos": 175, "end_pos": 178, "type": "METRIC", "confidence": 0.9912627041339874}]}, {"text": "Exact tagging accuracy (last column) is higher because it rewards true negatives, i.e. single-word segments with no nominal or verbal class label (the O and o tags).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.983834981918335}]}, {"text": "The sequence tagging framework makes it simple to model MWE identification jointly with supersense tagging: this is accomplished by packing information about both kinds of output into the tags.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.689931258559227}, {"text": "MWE identification", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.984740674495697}]}, {"text": "But there is always a risk that a larger tag space will impair the model's ability to generalize.", "labels": [], "entities": []}, {"text": "By comparing the first two rows of the results, we can see that jointly modeling supersenses along with multiword expressions results in only a minor decrease (<2 F 1 points) in MWE identification performance under the most basic feature set.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.9673048257827759}]}, {"text": "Further, we see that most of that decrease is recovered with richer features.", "labels": [], "entities": []}, {"text": "Thus, we conclude that it is empirically reasonable to model these phenomena together.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on test for lexical semantic analysis of noun and verb supersenses and MWEs with increasingly  complex models. Class labeling performance is given in aggregate, and class labeling recall is further broken down into  noun supersense tagging (NSST), verb supersense tagging (VSST), and auxiliary verb tagging. All of these results  use a percept cutoff of 5. The first result row uses a collapsed tagset (just the MWE status) rather than predicting full  supersense labels, as described in  \u00a74.4. The number of training iterations M was tuned by cross-validation on train.  The best result in each column and section is bolded.", "labels": [], "entities": [{"text": "lexical semantic analysis of noun and verb supersenses and MWEs", "start_pos": 30, "end_pos": 93, "type": "TASK", "confidence": 0.6144184470176697}, {"text": "noun supersense tagging", "start_pos": 234, "end_pos": 257, "type": "TASK", "confidence": 0.6605562170346578}, {"text": "verb supersense tagging", "start_pos": 266, "end_pos": 289, "type": "TASK", "confidence": 0.6583418945471445}, {"text": "auxiliary verb tagging", "start_pos": 302, "end_pos": 324, "type": "TASK", "confidence": 0.7951882481575012}]}, {"text": " Table 3: Four polysemous lemmas and counts of their  gold vs. predicted supersenses in test (limited to cases  where both the gold standard tag and the predicted tag  included a supersense). The distribution of gold super- senses for take, for example, is V:SOCIAL: 8, V:MOTION: 7,  V:POSSESSION: 1, V:STATIVE: 4, V:EMOTION: 1.", "labels": [], "entities": []}]}