{"title": [{"text": "Bag-of-Words Forced Decoding for Cross-Lingual Information Retrieval", "labels": [], "entities": [{"text": "Cross-Lingual Information Retrieval", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.735693077246348}]}], "abstractContent": [{"text": "Current approaches to cross-lingual information retrieval (CLIR) rely on standard retrieval models into which query translations by statistical machine translation (SMT) are integrated at varying degree.", "labels": [], "entities": [{"text": "cross-lingual information retrieval (CLIR)", "start_pos": 22, "end_pos": 64, "type": "TASK", "confidence": 0.7787705461184183}, {"text": "statistical machine translation (SMT)", "start_pos": 132, "end_pos": 169, "type": "TASK", "confidence": 0.7674119671185812}]}, {"text": "In this paper, we present an attempt to turn this situation on its head: Instead of the retrieval aspect, we emphasize the translation component in CLIR.", "labels": [], "entities": [{"text": "translation", "start_pos": 123, "end_pos": 134, "type": "TASK", "confidence": 0.9679294228553772}]}, {"text": "We perform search by using an SMT decoder in forced decoding mode to produce a bag-of-words representation of the target documents to be ranked.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.875723659992218}]}, {"text": "The SMT model is extended by retrieval-specific features that are optimized jointly with standard translation features fora ranking objective.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9914506077766418}]}, {"text": "We find significant gains over the state-of-the-art in a large-scale evaluation on cross-lingual search in the domains patents and Wikipedia.", "labels": [], "entities": []}], "introductionContent": [{"text": "Approaches to CLIR have been plentiful and diverse.", "labels": [], "entities": []}, {"text": "While simple word translation probabilities are easily integrated into term-based retrieval models), state-of-the-art SMT systems are complex statistical models on their own.", "labels": [], "entities": [{"text": "word translation probabilities", "start_pos": 13, "end_pos": 43, "type": "TASK", "confidence": 0.7797165811061859}, {"text": "SMT", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.9914532899856567}]}, {"text": "The use of established translation models for context-aware translation of query strings, effectively reducing the problem of CLIR to a pipeline of translation and monolingual retrieval, has been shown to work well in the past (.", "labels": [], "entities": []}, {"text": "Only recently, approaches have been presented to include (weighted) translation alternatives into the query structure to allow a more generalized term matching ().", "labels": [], "entities": []}, {"text": "However, this integration of SMT remains agnostic about its use for CLIR and is instead optimized to match fluent, human reference translations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9842779040336609}]}, {"text": "In contrast, retrieval systems often use bag-of-word representations, stopword filtering, and stemming techniques during document scoring, and queries are rarely fluent, grammatical natural language queries ().", "labels": [], "entities": [{"text": "document scoring", "start_pos": 121, "end_pos": 137, "type": "TASK", "confidence": 0.7079712897539139}]}, {"text": "Thus, most of a translation's structural information is lost during retrieval, and lexical choices may not be optimal for the retrieval task.", "labels": [], "entities": []}, {"text": "Furthermore, the nature of modeling translation and retrieval separately requires that a single query translation is selected, which is usually done by choosing the most probable SMT output.", "labels": [], "entities": [{"text": "SMT", "start_pos": 179, "end_pos": 182, "type": "TASK", "confidence": 0.9606599807739258}]}, {"text": "Attempts to inform the SMT system about its use for retrieval by optimizing its parameters towards a retrieval objective have been presented in the form or re-ranking () or ranking ( . In this paper, we take this idea a step further and directly integrate the task of scoring documents with respect to the query into the process of translation decoding.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9915453791618347}, {"text": "translation decoding", "start_pos": 332, "end_pos": 352, "type": "TASK", "confidence": 0.8746364414691925}]}, {"text": "We make the full expressiveness of the translation search space available to the retrieval model, without enumerating all possible translation alternatives.", "labels": [], "entities": []}, {"text": "This is done by augmenting the linear model of the SMT system with features that relate partial translation hypotheses to documents in the retrieval collection.", "labels": [], "entities": [{"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9928755760192871}]}, {"text": "These retrieval-specific features decompose over partial translation hypotheses and thus allow efficient decoding using standard dynamic programming techniques.", "labels": [], "entities": []}, {"text": "Furthermore, we apply learning-to-rank to jointly optimize translation and retrieval for the ob-jective of retrieving relevant documents, and use decoding over the weighted translation hypergraph directly to perform cross-lingual search.", "labels": [], "entities": []}, {"text": "Since high weights on retrieval features for words in the bagof-words (BOW) representation of documents force the decoder to prefer relevant documents with high probability, by a slight abuse of terminology, we call our approach BOW Forced Decoding.", "labels": [], "entities": []}, {"text": "One of the key features of our approach is the use of context-sensitive information such as the language model and reordering information.", "labels": [], "entities": []}, {"text": "We show that the use of such a translation-benign search space is crucial to outperform state-of-the-art CLIR approaches.", "labels": [], "entities": []}, {"text": "Our experimental evaluation of retrieval performance is done on Wikipedia cross-lingual article retrieval ( and patent prior art search ().", "labels": [], "entities": [{"text": "cross-lingual article retrieval", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.6478766798973083}]}, {"text": "On both datasets, we show substantial improvements over the CLIR baselines of direct translation or Probabilistic Structured Queries (), with and without further parameter tuning using learning-to-rank techniques and extended feature sets.", "labels": [], "entities": [{"text": "direct translation", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.6778507828712463}]}, {"text": "From our results we conclude, that, in spite of algorithmic complexity, it is central to model translation and retrieval jointly to create more powerful CLIR models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on two large-scale CLIR tasks, namely German-English Wikipedia cross-lingual article retrieval 4 (, and patent prior art search with Japanese-English patent abstracts), comparing retrieval performance of BOW-FD against the state-of-the-art SMT-based CLIR baselines of Direct Translation (DT) and cross-lingual Probabilistic Structured Queries (PSQ) (.", "labels": [], "entities": [{"text": "cross-lingual article retrieval", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.5406992534796397}, {"text": "BOW-FD", "start_pos": 229, "end_pos": 235, "type": "METRIC", "confidence": 0.8619770407676697}, {"text": "Direct Translation (DT)", "start_pos": 293, "end_pos": 316, "type": "TASK", "confidence": 0.8223073244094848}]}, {"text": "The SMT models, as well as baseline evaluation scores were taken from ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9773895144462585}]}, {"text": "We present results for BOW-FD using a default weight v optimized on the development sets, and for models with parameters trained using pairwise learning-to-rank.", "labels": [], "entities": [{"text": "BOW-FD", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.782630205154419}]}, {"text": "We compute MAP, NDCG and PRES () scores on the top 1,000 returned documents to provide an extensive evaluation across precision-, and recall-oriented measures.", "labels": [], "entities": [{"text": "MAP", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.8373644948005676}, {"text": "PRES", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.8954982757568359}, {"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.998075008392334}, {"text": "recall-oriented", "start_pos": 134, "end_pos": 149, "type": "METRIC", "confidence": 0.9903144240379333}]}, {"text": "Differences in evaluation scores between two systems were tested for statistical significance using paired randomization tests.", "labels": [], "entities": []}, {"text": "Significance levels are either indicated as superscripts, or provided in the captions of the respective tables.", "labels": [], "entities": [{"text": "Significance", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8981638550758362}]}, {"text": "Baseline SMT systems and BOW-FD share the hierarchical phrase-based SMT systems built with cdec ( ).", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.8204510807991028}, {"text": "BOW-FD", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.5425522327423096}, {"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.8724257946014404}]}, {"text": "For German-English cross-lingual article retrieval on Wikipedia, we built a system analogously to from parallel training data (over 104M words) consisting of the Europarl corpus () in version 7, the News Commentary corpus, and the Common Crawl corpus ( . Word alignments were created with fast align.", "labels": [], "entities": [{"text": "cross-lingual article retrieval", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.57533926765124}, {"text": "Europarl corpus", "start_pos": 162, "end_pos": 177, "type": "DATASET", "confidence": 0.9816255271434784}, {"text": "News Commentary corpus", "start_pos": 199, "end_pos": 221, "type": "DATASET", "confidence": 0.8790546258290609}, {"text": "Common Crawl corpus", "start_pos": 231, "end_pos": 250, "type": "DATASET", "confidence": 0.8017760117848715}]}, {"text": "The 4-gram language model was trained with the KenLM toolkit (Heafield, 2011) on the English side of the training data and the English Wikipedia articles.", "labels": [], "entities": [{"text": "KenLM toolkit (Heafield, 2011)", "start_pos": 47, "end_pos": 77, "type": "DATASET", "confidence": 0.9163810610771179}]}, {"text": "Language model scores are added to the search spaces using the cube pruning algorithm) with poplimit = 200.", "labels": [], "entities": []}, {"text": "SMT Model parameters were optimized using MIRA () on the WMT2011 news test set (3003 sentences).", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.960335373878479}, {"text": "MIRA", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9909761548042297}, {"text": "WMT2011 news test set", "start_pos": 57, "end_pos": 78, "type": "DATASET", "confidence": 0.9788685142993927}]}, {"text": "The parameters for the baseline PSQ model were found on a development set consisting of 10,000 German queries using 1,000-best lists: interpolation parameter \u03bb = 0.4, lower threshold L = 0, and cumulative threshold C = 1.", "labels": [], "entities": []}, {"text": "For the task of Japanese-English patent prior-art search, we use a system analog to and.", "labels": [], "entities": [{"text": "Japanese-English patent prior-art search", "start_pos": 16, "end_pos": 56, "type": "TASK", "confidence": 0.5480717122554779}]}, {"text": "Its SMT features are trained on 1.8M parallel sentences of NTCIR-7 data () and weights were tuned on the NTCIR-8 test collection (2,000 sentences) using MIRA (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9896573424339294}, {"text": "NTCIR-7 data", "start_pos": 59, "end_pos": 71, "type": "DATASET", "confidence": 0.953400194644928}, {"text": "NTCIR-8 test collection", "start_pos": 105, "end_pos": 128, "type": "DATASET", "confidence": 0.9731582601865133}, {"text": "MIRA", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9493523240089417}]}, {"text": "A 5-gram language model on the English side of the training data was trained with the KenLM toolkit (Heafield, 2011).", "labels": [], "entities": [{"text": "KenLM toolkit (Heafield, 2011)", "start_pos": 86, "end_pos": 116, "type": "DATASET", "confidence": 0.8998351352555412}]}, {"text": "The system uses a cube pruning poplimit of 30.", "labels": [], "entities": []}, {"text": "Parameters for the baseline PSQ model were found on a development set of 2,000 patent abstract queries and set to n-best list size = 1000, \u03bb = 1.0, L = 0.005, C = 0.95 Experimental Results.", "labels": [], "entities": []}, {"text": "We first find a default weight v using grid search within v = and v = [0, 2] on the development sets for Wikipedia and patents, respectively.", "labels": [], "entities": []}, {"text": "v controls the balance between the retrieval and translation features and with larger v, the model is more likely to produce query derivations diverging from the SMT 1-best translation.", "labels": [], "entities": [{"text": "SMT 1-best translation", "start_pos": 162, "end_pos": 184, "type": "TASK", "confidence": 0.8381372094154358}]}, {"text": "For Wikipedia, we sample 1,000 out of 10,000 queries to reduce the time of the grid search.", "labels": [], "entities": []}, {"text": "For patents we use the full development set of 2,000 queries with 8,381 sentences.", "labels": [], "entities": []}, {"text": "We combine rankings for single-sentence queries from multi-sentence patent abstracts using the product method as previously described.", "labels": [], "entities": []}, {"text": "Well performing values were found at v = 1.6 for Wikipedia, and v = 0.8 for patents, respectively.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.9173808693885803}]}, {"text": "shows test set performance of DT and PSQ baselines versus BOW-FD.", "labels": [], "entities": [{"text": "DT", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.5995317697525024}, {"text": "BOW-FD", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9913369417190552}]}, {"text": "Scores for DT and PSQ are as reported in.", "labels": [], "entities": [{"text": "DT", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.5430713295936584}, {"text": "PSQ", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.5732411742210388}]}, {"text": "We observe that BOW-FD significantly outperforms both baselines by over 2 points on Wikipedia and patents under all three evaluation measures.", "labels": [], "entities": [{"text": "BOW-FD", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9617807865142822}, {"text": "Wikipedia", "start_pos": 84, "end_pos": 93, "type": "DATASET", "confidence": 0.9512293338775635}]}, {"text": "While the cube pruning poplimit was set to 200 for the Wikipedia experiments, it is set to 30 for patents.", "labels": [], "entities": []}, {"text": "This may reduce the diversity of the search space considerably.", "labels": [], "entities": []}, {"text": "Increasing the poplimit from 30 to 200 yielded another significant gain (MAP=0.2893, NDCG=0.5807, PRES=0.6172) on this dataset.", "labels": [], "entities": [{"text": "MAP", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9985994696617126}, {"text": "NDCG", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9948758482933044}, {"text": "PRES", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9943599104881287}]}, {"text": "We learned the weights of the BOW-FD model starting from IR default weights optimized by grid search, and from SMT feature weights \"pre-trained\" on parallel data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.9591943025588989}]}, {"text": "We furthermore found improvements over BOW-FD in precision-oriented metrics (MAP and NDCG) by freezing SMT weights.", "labels": [], "entities": [{"text": "BOW-FD", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9760003685951233}, {"text": "SMT", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9820063710212708}]}, {"text": "shows that BOW-FD+(LEX+)LTR models significantly outperform BOW-FD on both data sets, with the largest improvement for PRES.", "labels": [], "entities": [{"text": "BOW-FD+(LEX+)LTR", "start_pos": 11, "end_pos": 27, "type": "METRIC", "confidence": 0.8931966900825501}, {"text": "PRES", "start_pos": 119, "end_pos": 123, "type": "DATASET", "confidence": 0.5462893843650818}]}, {"text": "Differences between models with and without lexical alignment features are not statistically significant.", "labels": [], "entities": []}, {"text": "We conjecture that LTR models mostly optimize recall rather than precision, i.e. placing more relevant document in the ranking.", "labels": [], "entities": [{"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9975341558456421}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9985438585281372}]}, {"text": "This is supported by the fact that BOW-FD+LTR retrieves 70.1% of the relevant documents in the test set, compared to 68.0% by BOW-FD, while Mean Reciprocal Rank (MRR) hardly differs (0.7344 vs. 0.7332).", "labels": [], "entities": [{"text": "BOW-FD+LTR", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.7400745948155721}, {"text": "BOW-FD", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.6932589411735535}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 140, "end_pos": 166, "type": "METRIC", "confidence": 0.9610371390978495}]}, {"text": "An experiment with no pre-trained SMT or default IR weights, performed worse, indicating the importance of translation-benign search spaces and IR default weights for generalization to unseen terms.", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9756633639335632}]}, {"text": "Importance of Language Model for Retrieval. and claim that computationally expensive SMT feature functions such as language models have only minor impact on CLIR performance of SMT-based models.", "labels": [], "entities": [{"text": "SMT feature", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.9033920168876648}, {"text": "SMT-based", "start_pos": 177, "end_pos": 186, "type": "TASK", "confidence": 0.9785223603248596}]}, {"text": "We found that such context-sensitive information present in single 1-best query translations (DT), weighted translation alternatives from the n-best list (PSQ), and forced decoding in a \"translationbenign\" search space (BOW-FD) is crucial for retrieval performance in the experiments reported this paper.", "labels": [], "entities": []}, {"text": "In order to investigate the question of the importance of context-sensitive information such as language model scores for retrieval we conducted an experiment in which the language model information is removed from all three SMT-based models.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 225, "end_pos": 234, "type": "TASK", "confidence": 0.9823663234710693}]}, {"text": "For the PSQ models, we also set the parameter \u03bb to 1.0 to disable interpolation with the context-free lexical translation table).", "labels": [], "entities": []}, {"text": "shows that retrieval performance drops significantly for all models.", "labels": [], "entities": []}, {"text": "The drop in performance for the two baseline models is comparable on both data sets.", "labels": [], "entities": []}, {"text": "Removing the language model for BOW-FD hurts performance the most (with an average drop of 6 points in MAP and NDCG scores for Wikipedia, and over 11 points in all measures for patents).", "labels": [], "entities": [{"text": "MAP", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.7961874008178711}]}, {"text": "However, scores for recall-oriented PRES on Wikipedia remains relatively stable for BOW-FD with and without a language model.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 20, "end_pos": 35, "type": "METRIC", "confidence": 0.9831560850143433}, {"text": "PRES", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.8312621712684631}]}, {"text": "A closer analysis on the rankings for BOW-FD on Wikipedia shows that the -LM model returns 1,589 (out of 86,994) relevant documents less than the +LM model.", "labels": [], "entities": [{"text": "BOW-FD", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.5793489813804626}]}, {"text": "However, only 2 documents with relevance level 3, i.e., directly linked cross-lingual \"mates\", were no longer retrieved, suggesting that excluding the language model from the system mostly affects the retrieval of \"non-mates\", i.e. documents that are linked by, or link to the cross-lingual mate.", "labels": [], "entities": []}, {"text": "We explain this behavior as follows: Cross-lingual mates are likely to contain words that are close to an adequate query translation, since they constitute the beginning of a Wikipedia article with the same topic as the query.", "labels": [], "entities": []}, {"text": "Derivations generated for these documents are such that both translation model features (with or without the LM) and retrieval features agree on a path close to the SMT Viterbi translation.", "labels": [], "entities": [{"text": "SMT Viterbi translation", "start_pos": 165, "end_pos": 188, "type": "TASK", "confidence": 0.6935368975003561}]}, {"text": "In contrast, other relevant documents require more non-standard lexical choices that are harder to achieve in a +LM search space, since the strong weight on the language model, plus a language model-driven pruning technique, strongly favor lexical choices that agree with the language model's concept of fluency.", "labels": [], "entities": []}, {"text": "Ina -LM search space, disfluent derivations are easily reached by IR feature activations whose default weight is much larger in relation to the remaining SMT features.", "labels": [], "entities": [{"text": "SMT", "start_pos": 154, "end_pos": 157, "type": "TASK", "confidence": 0.9458145499229431}]}, {"text": "The use of \"glue rules\" allowing leftto-right concatenation of partial translations along with loosely extracted synchronous grammar rules give hierarchical MT models large degrees of freedom in producing very disfluent translations in the -LM space.", "labels": [], "entities": [{"text": "MT", "start_pos": 157, "end_pos": 159, "type": "TASK", "confidence": 0.9440913796424866}]}, {"text": "If a language model is not ensuring a more or less \"translation-benign\" search space, the \"reachability\" of terms in irrelevant documents is increased causing them to interfere with the ranking of relevant documents that maybe closer translations of the query.", "labels": [], "entities": []}, {"text": "This behavior immediately affects precision-oriented scores such as MAP and NDCG, while PRES is only affected if its recall cutoff parameter, N max , is lowered, as shown in.", "labels": [], "entities": [{"text": "precision-oriented", "start_pos": 34, "end_pos": 52, "type": "METRIC", "confidence": 0.9901837706565857}, {"text": "PRES", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.7419953346252441}, {"text": "recall cutoff parameter, N max", "start_pos": 117, "end_pos": 147, "type": "METRIC", "confidence": 0.8910066584746043}]}, {"text": "The major drop in performance for patent data maybe explained with the way multiple sentence queries are evaluated: A language model limits diversity of translation options for multiple sentences.", "labels": [], "entities": []}, {"text": "Without a language model, the sets of documents retrieved by each sentence are almost disjoint, i.e. the sentences do not agree on a common set of documents.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Retrieval results of baseline systems and BOW-FD with default weight v = 1.6 for Wikipedia and v = 0.8 for  patents, respectively. Baseline and BOW-FD models use the same SMT system. Significant differences at p = 10 \u22124  with respect to baselines are indicated with  *  . Significant differences at p = 10 \u22126 of learning-to-rank-based models  (LTR) with respect to BOW-FD are indicated with  \u2020 .", "labels": [], "entities": [{"text": "SMT", "start_pos": 181, "end_pos": 184, "type": "TASK", "confidence": 0.9600995779037476}]}]}