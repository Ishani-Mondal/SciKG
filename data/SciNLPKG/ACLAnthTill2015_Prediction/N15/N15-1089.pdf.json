{"title": [{"text": "Early Gains Matter: A Case for Preferring Generative over Discriminative Crowdsourcing Models", "labels": [], "entities": [{"text": "Preferring Generative over Discriminative Crowdsourcing", "start_pos": 31, "end_pos": 86, "type": "TASK", "confidence": 0.8434582114219665}]}], "abstractContent": [{"text": "In modern practice, labeling a dataset often involves aggregating annotator judgments obtained from crowdsourcing.", "labels": [], "entities": []}, {"text": "State-of-the-art aggregation is performed via inference on probabilistic models, some of which are data-aware, meaning that they leverage features of the data (e.g., words in a document) in addition to annotator judgments.", "labels": [], "entities": []}, {"text": "Previous work largely prefers discriminatively trained conditional models.", "labels": [], "entities": []}, {"text": "This paper demonstrates that a data-aware crowdsourcing model incorporating a generative multinomial data model enjoys a strong competitive advantage over its discriminative log-linear counterpart in the typical crowdsourcing setting.", "labels": [], "entities": []}, {"text": "That is, the generative approach is better except when the annotators are highly accurate in which case simple majority vote is often sufficient.", "labels": [], "entities": [{"text": "generative", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.9823768138885498}]}, {"text": "Additionally , we present a novel mean-field vari-ational inference algorithm for the generative model that significantly improves on the previously reported state-of-the-art for that model.", "labels": [], "entities": []}, {"text": "We validate our conclusions on six text classification datasets with both human-generated and synthetic annotations.", "labels": [], "entities": [{"text": "text classification", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7002950608730316}]}], "introductionContent": [{"text": "The success of supervised machine learning has created an urgent need for manually-labeled training datasets.", "labels": [], "entities": []}, {"text": "Crowdsourcing allows human label judgments to be obtained rapidly and at relatively low cost.", "labels": [], "entities": [{"text": "human label judgments", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.6411904692649841}]}, {"text": "Micro-task markets such as Amazon's Mechanical Turk and CrowdFlower have popularized crowdsourcing by reducing the overhead required to distribute a job to a community of annotators (the \"crowd\").", "labels": [], "entities": []}, {"text": "However, crowdsourced judgments often suffer from high error rates.", "labels": [], "entities": []}, {"text": "A common solution to this problem is to obtain multiple redundant human judgments, or annotations, 1 relying on the observation that, in aggregate, the ability of non-experts often rivals or exceeds that of experts by averaging over individual error patterns.", "labels": [], "entities": []}, {"text": "For the purposes of this paper a crowdsourcing model is a model that infers, at a minimum, class labels y based on the evidence of one or more imperfect annotations a.", "labels": [], "entities": []}, {"text": "A common baseline method aggregates annotations by majority vote but by so doing ignores important information.", "labels": [], "entities": []}, {"text": "For example, some annotators are more reliable than others, and their judgments ought to be weighted accordingly.", "labels": [], "entities": []}, {"text": "State-of-the-art crowdsourcing methods formulate probabilistic models that account for such side information and then apply standard inference techniques to the task of inferring ground truth labels from imperfect annotations.", "labels": [], "entities": []}, {"text": "Data-aware crowdsourcing models additionally account for the features x comprising each data instance (e.g., words in a document).", "labels": [], "entities": []}, {"text": "The data can be modeled generatively by proposing a joint distribution p(y, x, a).", "labels": [], "entities": []}, {"text": "However, because of the challenge of accurately modeling complex data x, most previous work uses a discriminatively trained conditional model p(y, a|x), hereafter referred to as a discriminative model.", "labels": [], "entities": []}, {"text": "As explain, maximizing conditional log likelihood is a compu-tationally convenient approximation to minimizing a discriminative 0-1 loss objective, giving rise to the common practice of referring to conditional models as discriminative.", "labels": [], "entities": []}, {"text": "This paper challenges the popular preference for discriminative data models in the crowdsourcing literature by demonstrating that in typical crowdsourcing scenarios a generative model enjoys a strong advantage over its discriminative counterpart.", "labels": [], "entities": []}, {"text": "We conduct, on both real and synthetic annotations, the first empirical comparison of structurally comparable generative and discriminative crowdsourcing models.", "labels": [], "entities": [{"text": "generative and discriminative crowdsourcing", "start_pos": 110, "end_pos": 153, "type": "TASK", "confidence": 0.8223138749599457}]}, {"text": "The comparison is made fair by developing similar mean-field variational inference algorithms for both models.", "labels": [], "entities": []}, {"text": "The generative model is considerably improved by our variational algorithm compared with the previously reported state-of-the-art for that model.", "labels": [], "entities": [{"text": "generative", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9641262292861938}]}], "datasetContent": [{"text": "Models which learn from error-prone annotations can be challenging to evaluate in a systematic way.", "labels": [], "entities": []}, {"text": "Simulated annotations allow us to systematically control annotator behavior and measure the performance of our models in each configuration.", "labels": [], "entities": []}, {"text": "We simulate the annotator pools from on each of six text classification datasets.", "labels": [], "entities": [{"text": "text classification", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7165688276290894}]}, {"text": "The datasets 20 Newsgroups, WebKB, Cade12, Reuters8, and Reuters52 are described by.", "labels": [], "entities": [{"text": "WebKB", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.9483581185340881}, {"text": "Cade12", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.8767293691635132}, {"text": "Reuters8", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.9111781716346741}, {"text": "Reuters52", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.9524893760681152}]}, {"text": "The LDC-labeled Enron emails dataset is described by.", "labels": [], "entities": [{"text": "Enron emails dataset", "start_pos": 16, "end_pos": 36, "type": "DATASET", "confidence": 0.8455140590667725}]}, {"text": "Each dataset is preprocessed via Porter stemming and by removal of the stopwords from MALLET's stopword list.", "labels": [], "entities": [{"text": "Porter stemming", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.6873409152030945}, {"text": "MALLET's stopword list", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.8187048584222794}]}, {"text": "Features occurring fewer than 5 times in the corpus are discarded.", "labels": [], "entities": []}, {"text": "Features are fractionally scaled so that |x i | 1 is equal to the average document length since document scaling has been shown to be beneficial for multinomial document models).", "labels": [], "entities": []}, {"text": "Each dataset is annotated according to the following process: an instance is selected at random (without replacement) and annotated by three annotators selected at random (without replacement).", "labels": [], "entities": []}, {"text": "Because annotation simulation is a stochastic process, each simulation is repeated five times.", "labels": [], "entities": []}, {"text": "In the previous section we used simulations to control annotator error.", "labels": [], "entities": []}, {"text": "In this section we relax that control.", "labels": [], "entities": []}, {"text": "To assess the effect of real-world annotation error on MOMRESP and LOGRESP, we selected 1000 instances at random from 20 Newsgroups and paid annotators on CrowdFlower to annotate them with the 20 Newsgroups categories, presented as humanreadable names (e.g., \"Atheism\" for alt.atheism", "labels": [], "entities": [{"text": "LOGRESP", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.8531232476234436}]}], "tableCaptions": [{"text": " Table 1: For each simulated annotator quality pool  (HIGH, MED, LOW, CONFLICT), annotators A1- A5 are assigned an accuracy.  \u2020 indicates that errors  are systematically in conflict as described in the text.", "labels": [], "entities": [{"text": "HIGH", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9098272323608398}, {"text": "MED", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9109846353530884}, {"text": "LOW", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.7674413323402405}, {"text": "CONFLICT", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9103124737739563}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9979010820388794}]}, {"text": " Table 2: The percentage of the dataset that must be  annotated (three-deep) before the generative model  MOMRESP is surpassed by LOGRESP. indicates  that MOMRESP dominates the entire learning curve;  0% indicates that LOGRESP dominates. NA indi- cates high variance cases that were too close to call.", "labels": [], "entities": []}]}