{"title": [{"text": "A Transition-based Algorithm for AMR Parsing", "labels": [], "entities": [{"text": "AMR Parsing", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9691399037837982}]}], "abstractContent": [{"text": "We present a two-stage framework to parse a sentence into its Abstract Meaning Representation (AMR).", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.6408918797969818}]}, {"text": "We first use a dependency parser to generate a dependency tree for the sentence.", "labels": [], "entities": []}, {"text": "In the second stage, we design a novel transition-based algorithm that transforms the dependency tree to an AMR graph.", "labels": [], "entities": []}, {"text": "There are several advantages with this approach.", "labels": [], "entities": []}, {"text": "First, the dependency parser can be trained on a training set much larger than the training set for the tree-to-graph algorithm, resulting in a more accurate AMR parser overall.", "labels": [], "entities": [{"text": "AMR parser", "start_pos": 158, "end_pos": 168, "type": "TASK", "confidence": 0.8224166929721832}]}, {"text": "Our parser yields an improvement of 5% absolute in F-measure over the best previous result.", "labels": [], "entities": [{"text": "absolute", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9773426055908203}, {"text": "F-measure", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.994792640209198}]}, {"text": "Second, the actions that we design are linguistically intuitive and capture the regularities in the mapping between the dependency structure and the AMR of a sentence.", "labels": [], "entities": []}, {"text": "Third, our parser runs in nearly linear time in practice in spite of a worst-case complexity of O(n 2).", "labels": [], "entities": [{"text": "O", "start_pos": 96, "end_pos": 97, "type": "METRIC", "confidence": 0.9345333576202393}]}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) is a rooted, directed, edge-labeled and leaf-labeled graph that is used to represent the meaning of a sentence.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8606067498524984}]}, {"text": "The AMR formalism has been used to annotate the AMR Annotation Corpus (), a corpus of over 10 thousand sentences that is still undergoing expansion.", "labels": [], "entities": [{"text": "AMR Annotation Corpus", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.907921572526296}]}, {"text": "The building blocks for an AMR representation are concepts and relations between them.", "labels": [], "entities": [{"text": "AMR representation", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.935399055480957}]}, {"text": "Understanding these concepts and their relations is crucial to understanding the meaning of a sentence and could potentially benefit a number of natural language applications such as Information Extraction, Question Answering and Machine Translation.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 183, "end_pos": 205, "type": "TASK", "confidence": 0.8120752274990082}, {"text": "Question Answering", "start_pos": 207, "end_pos": 225, "type": "TASK", "confidence": 0.8647111654281616}, {"text": "Machine Translation", "start_pos": 230, "end_pos": 249, "type": "TASK", "confidence": 0.8493199944496155}]}, {"text": "The property that makes AMR a graph instead of a tree is that AMR allows reentrancy, meaning that the same concept can participate in multiple relations.", "labels": [], "entities": []}, {"text": "Parsing a sentence into an AMR would seem to require graph-based algorithms, but moving to graph-based algorithms from the typical tree-based algorithms that we are familiar with is a big step in terms of computational complexity.", "labels": [], "entities": [{"text": "Parsing a sentence into an AMR", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8285032510757446}]}, {"text": "Indeed, quite a bit of effort has gone into developing grammars and efficient graph-based algorithms that can be used to parse AMRs (.", "labels": [], "entities": [{"text": "parse AMRs", "start_pos": 121, "end_pos": 131, "type": "TASK", "confidence": 0.7637990117073059}]}, {"text": "Linguistically, however, there are many similarities between an AMR and the dependency structure of a sentence.", "labels": [], "entities": []}, {"text": "Both describe relations as holding between ahead and its dependent, or between a parent and its child.", "labels": [], "entities": []}, {"text": "AMR concepts and relations abstract away from actual word tokens, but there are regularities in their mappings.", "labels": [], "entities": []}, {"text": "Content words generally be-come concepts while function words either become relations or get omitted if they do not contribute to the meaning of a sentence.", "labels": [], "entities": []}, {"text": "This is illustrated in, where 'the' and 'to' in the dependency tree are omitted from the AMR and the preposition 'in' becomes a relation of type location.", "labels": [], "entities": []}, {"text": "In AMR, reentrancy is also used to represent co-reference, but this only happens in some limited contexts.", "labels": [], "entities": [{"text": "AMR", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.8958728909492493}]}, {"text": "In, 'police' is both an argument of 'arrest' and 'want' as the result of a control structure.", "labels": [], "entities": []}, {"text": "This suggests that it is possible to transform a dependency tree into an AMR with a limited number of actions and learn a model to determine which action to take given pairs of aligned dependency trees and AMRs as training data.", "labels": [], "entities": []}, {"text": "This is the approach we adopt in the present work, and we present a transition-based framework in which we parse a sentence into an AMR by taking the dependency tree of that sentence as input and transforming it to an AMR representation via a series of actions.", "labels": [], "entities": []}, {"text": "This means that a sentence is parsed into an AMR in two steps.", "labels": [], "entities": []}, {"text": "In the first step the sentence is parsed into a dependency tree with a dependency parser, and in the second step the dependency tree is transformed into an AMR graph.", "labels": [], "entities": []}, {"text": "One advantage of this approach is that the dependency parser does not have to be trained on the same data set as the dependency to AMR transducer.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7582925856113434}]}, {"text": "This allows us to use more accurate dependency parsers trained on data sets much larger than the AMR Annotation Corpus and have a more advantageous starting point.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7268048822879791}, {"text": "AMR Annotation Corpus", "start_pos": 97, "end_pos": 118, "type": "DATASET", "confidence": 0.9159348607063293}]}, {"text": "Our experiments show that this approach is very effective and yields an improvement of 5% absolute over the previously reported best result) in F-score, as measure by the Smatch metric . The rest of the paper is as follows.", "labels": [], "entities": [{"text": "F-score", "start_pos": 144, "end_pos": 151, "type": "METRIC", "confidence": 0.996837854385376}]}, {"text": "In \u00a72, we describe how we align the word tokens in a sentence with its AMR to create a span graph based on which we extract contextual information as features and perform actions.", "labels": [], "entities": []}, {"text": "In \u00a73, we present our transition-based parsing algorithm and describe the actions used to transform the dependency tree of a sentence into an AMR.", "labels": [], "entities": []}, {"text": "In \u00a74, we present the learning algorithm and the features we extract to train the transition model.", "labels": [], "entities": []}, {"text": "In \u00a75, we present experimental results.", "labels": [], "entities": []}, {"text": "\u00a76 describes related work, and we conclude in \u00a77.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are conducted on the newswire section of AMR Annotation Corpus (LDC2013E117) ().", "labels": [], "entities": [{"text": "newswire section of AMR Annotation Corpus (LDC2013E117)", "start_pos": 37, "end_pos": 92, "type": "DATASET", "confidence": 0.8888681862089369}]}, {"text": "We follow insetting up the train/development/test splits 1 for easy comparison: 4.0k sentences with document years 1995-2006 as the training set; 2.1k sentences with document year 2007 as the development set; 2.1k sentences with document year 2008 as the test set, and only using AMRs that are tagged ::preferred.", "labels": [], "entities": []}, {"text": "Each sentence w is preprocessed with the Stanford CoreNLP toolkit () to get partof-speech tags, name entity information, and basic dependencies.", "labels": [], "entities": [{"text": "Stanford CoreNLP toolkit", "start_pos": 41, "end_pos": 65, "type": "DATASET", "confidence": 0.9324057896931967}]}, {"text": "We have verified that there is no overlap between the training data for the Stanford CoreNLP toolkit 2 and the AMR Annotation Corpus.", "labels": [], "entities": [{"text": "Stanford CoreNLP toolkit 2", "start_pos": 76, "end_pos": 102, "type": "DATASET", "confidence": 0.9489876925945282}, {"text": "AMR Annotation Corpus", "start_pos": 111, "end_pos": 132, "type": "DATASET", "confidence": 0.9267242749532064}]}, {"text": "We evaluate our parser with the Smatch tool , which seeks to maximize the semantic overlap between two AMR annotations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results on the test set. Here, l gc -gold  concept label; l gr -gold relation label; l grc -gold  concept label and gold relation label.", "labels": [], "entities": []}]}