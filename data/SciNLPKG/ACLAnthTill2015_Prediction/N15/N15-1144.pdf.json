{"title": [{"text": "Unsupervised POS Induction with Word Embeddings", "labels": [], "entities": [{"text": "POS Induction", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.6824172139167786}]}], "abstractContent": [{"text": "Unsupervised word embeddings have been shown to be valuable as features in supervised learning problems; however, their role in unsu-pervised problems has been less thoroughly explored.", "labels": [], "entities": []}, {"text": "In this paper, we show that embeddings can likewise add value to the problem of unsu-pervised POS induction.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 94, "end_pos": 107, "type": "TASK", "confidence": 0.8411656618118286}]}, {"text": "In two representative models of POS induction, we replace multi-nomial distributions over the vocabulary with multivariate Gaussian distributions over word embeddings and observe consistent improvements in eight languages.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9809368252754211}]}, {"text": "We also analyze the effect of various choices while inducing word embeddings on \"downstream\" POS induction results.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 93, "end_pos": 106, "type": "TASK", "confidence": 0.7938260138034821}]}], "introductionContent": [{"text": "Unsupervised POS induction is the problem of assigning word tokens to syntactic categories given only a corpus of untagged text.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9100666642189026}]}, {"text": "In this paper we explore the effect of replacing words with their vector space embeddings1 in two POS induction models: the classic first-order HMM ( and the newly introduced conditional random field autoencoder ().", "labels": [], "entities": []}, {"text": "In each model, instead of using a conditional multinomial distribution2 to generate a word token w i \u2208 V given a POS tag ti \u2208 T, we use a conditional Gaussian distribution and generate a d-dimensional word embedding v w i \u2208 Rd given ti . 1Unlike, we leverage easily obtainable and widely used embeddings of word types.", "labels": [], "entities": []}, {"text": "2Also known as a categorical distribution.", "labels": [], "entities": []}, {"text": "Our findings suggest that, in both models, substantial improvements are possible when word embeddings are used rather than opaque word types.", "labels": [], "entities": []}, {"text": "However, the independence assumptions made by the model used to induce embeddings strongly determines its effectiveness for POS induction: embedding models that model short-range context are more effective than those that model longer-range contexts.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.9763072431087494}]}, {"text": "This result is unsurprising, but it illustrates the lack of an evaluation metric that measures the syntactic (rather than semantic) information in word embeddings.", "labels": [], "entities": []}, {"text": "Our results also confirm the conclusions of who were likewise able to improve POS induction results, albeit using a custom clustering model based on the the distance-dependent Chinese restaurant process ().", "labels": [], "entities": [{"text": "POS induction", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.8695010840892792}]}, {"text": "Our contributions are as follows: (i) reparameterization of token-level POS induction models to use word embeddings; and (ii) a systematic evaluation of word embeddings with respect to the syntactic information they contain.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we attempt to answer the following questions: \u2022 \u00a74.1: Do syntactically-informed word embeddings improve POS induction?", "labels": [], "entities": [{"text": "POS induction", "start_pos": 121, "end_pos": 134, "type": "TASK", "confidence": 0.9502716064453125}]}, {"text": "\u2022 \u00a74.2: What kind of word embeddings are suitable for POS induction?", "labels": [], "entities": [{"text": "POS induction", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.969839870929718}]}], "tableCaptions": []}