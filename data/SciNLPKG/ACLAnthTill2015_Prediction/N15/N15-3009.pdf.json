{"title": [{"text": "Ckylark: A More Robust PCFG-LA Parser", "labels": [], "entities": [{"text": "Ckylark", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8618953227996826}, {"text": "PCFG-LA Parser", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.8072493374347687}]}], "abstractContent": [{"text": "This paper describes Ckylark, a PCFG-LA style phrase structure parser that is more robust than other parsers in the genre.", "labels": [], "entities": [{"text": "PCFG-LA style phrase structure parser", "start_pos": 32, "end_pos": 69, "type": "TASK", "confidence": 0.6356164693832398}]}, {"text": "PCFG-LA parsers are known to achieve highly competitive performance, but sometimes the parsing process fails completely, and no parses can be generated.", "labels": [], "entities": [{"text": "PCFG-LA parsers", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6912378370761871}]}, {"text": "Ckylark introduces three new techniques that prevent possible causes for parsing failure: outputting intermediate results when coarse-to-fine analysis fails, smoothing lexicon probabilities, and scaling probabilities to avoid underflow.", "labels": [], "entities": [{"text": "parsing", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.9672328233718872}]}, {"text": "An experiment shows that this allows millions of sentences can be parsed without any failures, in contrast to other publicly available PCFG-LA parsers.", "labels": [], "entities": []}, {"text": "Ckylark is implemented in C++, and is available open-source under the LGPL license.", "labels": [], "entities": [{"text": "Ckylark", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9309493899345398}]}], "introductionContent": [{"text": "Parsing accuracy has been shown to have a significant effect on downstream applications such as textual entailment) and machine translation, and most work on parsing evaluates accuracy to some extent.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.8462082743644714}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.8702090382575989}, {"text": "textual entailment", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7450886964797974}, {"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.8310896754264832}, {"text": "parsing", "start_pos": 158, "end_pos": 165, "type": "TASK", "confidence": 0.9648420810699463}, {"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9979853630065918}]}, {"text": "However, one element that is equally, or perhaps even more, important from the view of downstream applications is parser robustness, or the ability to return at least some parse regardless of the input.", "labels": [], "entities": [{"text": "parser", "start_pos": 114, "end_pos": 120, "type": "TASK", "confidence": 0.9510598182678223}]}, {"text": "Every failed parse is a sentence for which downstream applications have no chance of even performing processing in the normal way, and application developers must perform http://github.com/odashi/ckylark special checks that detect these sentences and either give up entirely, or fallback to some alternative processing scheme.", "labels": [], "entities": []}, {"text": "Among the various methods for phrase-structure parsing, the probabilistic context free grammar with latent annotations (PCFG-LA, ()) framework is among the most popular for several reasons.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.7869168221950531}]}, {"text": "The first is that it boasts competitive accuracy, both in intrisinic measures such as F1-score on the Penn Treebank, and extrinsic measures (it achieved the highest textual entailment and machine translation accuracy in the papers cited above).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9994369149208069}, {"text": "F1-score", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.998299777507782}, {"text": "Penn Treebank", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9713509380817413}, {"text": "machine translation", "start_pos": 188, "end_pos": 207, "type": "TASK", "confidence": 0.6411876380443573}, {"text": "accuracy", "start_pos": 208, "end_pos": 216, "type": "METRIC", "confidence": 0.7055124044418335}]}, {"text": "The second is the availablity of easy-to-use tools, most notably the Berkeley Parser, 2 but also including Egret, and BUBS Parser.", "labels": [], "entities": [{"text": "availablity", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9061628580093384}, {"text": "BUBS Parser", "start_pos": 118, "end_pos": 129, "type": "DATASET", "confidence": 0.9007441699504852}]}, {"text": "However, from the point of view of robustness, existing tools for PCFG-LA parsing leave something to be desired; to our knowledge, all existing tools produce a certain number of failed parses when run on large data sets.", "labels": [], "entities": [{"text": "PCFG-LA parsing", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7493372559547424}]}, {"text": "In this paper, we introduce Ckylark, anew PCFG-LA parser specifically designed for robustness.", "labels": [], "entities": []}, {"text": "Specifically, Ckylark makes the following contributions: \u2022 Based on our analysis of three reasons why conventional PCFG-LA parsing models fail (Section 2), Ckylark implements three improvements over the conventional PCFG-LA parsing method to remedy these problems (Section 3).", "labels": [], "entities": [{"text": "PCFG-LA parsing", "start_pos": 115, "end_pos": 130, "type": "TASK", "confidence": 0.540353462100029}, {"text": "PCFG-LA parsing", "start_pos": 216, "end_pos": 231, "type": "TASK", "confidence": 0.5542125403881073}]}, {"text": "\u2022 An experimental evaluation (Section 4) shows that Ckylark achieves competitive accuracy with other PCFG-LA parsers, and can robustly parse large datasets where other parsers fail.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9951745867729187}]}, {"text": "\u2022 Ckylark is implemented in C++, and released under the LGPL license, allowing for free research or commercial use.", "labels": [], "entities": []}, {"text": "It is also available in library format, which means that it can be incorporated directly into other programs.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated parsing accuracies of our parser Ckylark and conventional PCFG-LA parsers: Berkeley Parser and Egret.", "labels": [], "entities": []}, {"text": "Berkeley Parser is a conventional PCFG-LA parser written in Java with some additional optimization techniques.", "labels": [], "entities": []}, {"text": "Egret is also a conventional PCFG-LA parser in C++ which can generate a parsing forest that can be used in downstream application such forest based machine translation ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 148, "end_pos": 167, "type": "TASK", "confidence": 0.7744221091270447}]}, {"text": "shows summaries of each dataset.", "labels": [], "entities": []}, {"text": "We used GrammarTrainer in the Berkeley Parser to train a PCFG-LA grammar with the Penn Treebank WSJ dataset section 2 to 22 (WSJ-train/dev).", "labels": [], "entities": [{"text": "Penn Treebank WSJ dataset section 2", "start_pos": 82, "end_pos": 117, "type": "DATASET", "confidence": 0.9488980372746786}]}, {"text": "Egret and Ckylark can use the same model as the Berkeley Parser so we can evaluate only the performance of the parsers using the same grammar.", "labels": [], "entities": []}, {"text": "Each parser is run on a Debian 7.1 machine with an Intel Core i7 CPU (3.40GHz, 4 cores, 8MB caches) and 4GB RAM.", "labels": [], "entities": []}, {"text": "We chose 2 datasets to evaluate the performances of each parser.", "labels": [], "entities": []}, {"text": "First, WSJ-test, the Penn Treebank WSJ dataset section 23, is a standard dataset 43.", "labels": [], "entities": [{"text": "WSJ-test", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.8601496815681458}, {"text": "Penn Treebank WSJ dataset section 23", "start_pos": 21, "end_pos": 57, "type": "DATASET", "confidence": 0.9771467943986257}]}, {"text": "Input sentences of each parser must be tokenized in advance, so we used a tokenization algorithm equivalent to the Stanford Tokenizer 5 for tokenizing the NTCIR dataset.", "labels": [], "entities": [{"text": "NTCIR dataset", "start_pos": 155, "end_pos": 168, "type": "DATASET", "confidence": 0.9363302886486053}]}, {"text": "shows the bracketing F1 scores 6 of parse trees for each parser on the WSJ-test dataset and Table 3 also shows the part-of-speech tagging accuracies.", "labels": [], "entities": [{"text": "F1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9659191370010376}, {"text": "WSJ-test dataset", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.980836033821106}, {"text": "part-of-speech tagging", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.7169083952903748}]}, {"text": "We show 2 results for Ckylark with pruning threshold \u03f5 as 10 \u22125 and 10 \u22127 . These tables show that the result of Ckylark with \u03f5 = 10 \u22127 achieves nearly the same parsing accuracy as the Berkeley Parser.", "labels": [], "entities": [{"text": "parsing", "start_pos": 161, "end_pos": 168, "type": "TASK", "confidence": 0.9414553046226501}, {"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9458456039428711}]}, {"text": "shows calculation times of each parser on the WSJ-test dataset.", "labels": [], "entities": [{"text": "WSJ-test dataset", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9678487479686737}]}, {"text": "When the pruning threshold \u03f5 is smaller, parsing takes longer, but in all cases Ckylark is faster than Egret while achieving higher accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9830789566040039}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9970226883888245}]}, {"text": "Berkeley Parser is the fastest of all parsers, a result of optimizations not included in the standard PCFG-LA parsing algorithm.", "labels": [], "entities": [{"text": "Berkeley Parser", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.8996690809726715}, {"text": "PCFG-LA parsing", "start_pos": 102, "end_pos": 117, "type": "TASK", "confidence": 0.7254106998443604}]}, {"text": "Incorporating these techniques into Ckylark is future work.", "labels": [], "entities": []}, {"text": "0.01% and 0.5% of sentences could not be parsed with the Berkeley Parser and Egret respectively.", "labels": [], "entities": [{"text": "Berkeley Parser", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.8483147621154785}]}, {"text": "In contrast, our parser does not fail a single time.", "labels": [], "entities": []}, {"text": "shows the number of failures of Ckylark with \u03f5 = 10 \u22125 and without the stopping approach; if the parser failed at the level l analysis then it returns the result of the l \u2212 1 level.", "labels": [], "entities": [{"text": "Ckylark", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9036480188369751}]}, {"text": "Thus, the stopping approach will never generate any failure, unless failure occurs at the initial level.", "labels": [], "entities": []}, {"text": "The reason for failure at the initial level is only due to model mismatch, as no pruning has been performed.", "labels": [], "entities": []}, {"text": "These errors can be prevented by lexicon smoothing at parsing time as shown in the case of level 0 with \u03bb = 10 \u221210 in the table.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Bracketing F1 scores of each parser.", "labels": [], "entities": [{"text": "Bracketing", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9899249076843262}, {"text": "F1 scores", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9112568795681}]}, {"text": " Table 3: Tagging accuracies of each parser.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9823231101036072}]}, {"text": " Table 4: Calculation times of each parser.", "labels": [], "entities": []}, {"text": " Table 5: Frequencies of parsing failure of each parser.", "labels": [], "entities": [{"text": "Frequencies", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9109009504318237}, {"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.955232560634613}]}, {"text": " Table 6: Number of failures of each coarse-to-fine level.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9806554317474365}]}]}