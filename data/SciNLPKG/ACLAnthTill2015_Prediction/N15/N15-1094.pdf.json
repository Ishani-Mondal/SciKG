{"title": [{"text": "Penalized Expectation Propagation for Graphical Models over Strings *", "labels": [], "entities": [{"text": "Penalized Expectation Propagation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6644251346588135}]}], "abstractContent": [{"text": "We present penalized expectation propagation (PEP), a novel algorithm for approximate inference in graphical models.", "labels": [], "entities": [{"text": "penalized expectation propagation (PEP)", "start_pos": 11, "end_pos": 50, "type": "TASK", "confidence": 0.6628392785787582}]}, {"text": "Expectation propagation is a variant of loopy belief propagation that keeps messages tractable by projecting them back into a given family of functions.", "labels": [], "entities": [{"text": "Expectation propagation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7894413769245148}, {"text": "loopy belief propagation", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.7702064315478007}]}, {"text": "Our extension, PEP, uses a structured-sparsity penalty to encourage simple messages , thus balancing speed and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9948624968528748}]}, {"text": "We specifically show how to instantiate PEP in the case of string-valued random variables, where we adaptively approximate finite-state distributions by variable-order n-gram models.", "labels": [], "entities": [{"text": "instantiate PEP", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.6434579938650131}]}, {"text": "On phonological inference problems, we obtain substantial speedup over previous related algorithms with no significant loss inaccuracy.", "labels": [], "entities": []}], "introductionContent": [{"text": "Graphical models are well-suited to reasoning about linguistic structure in the presence of uncertainty.", "labels": [], "entities": []}, {"text": "Such models typically use discrete random variables, where each variable ranges over a finite set of values such as words or tags.", "labels": [], "entities": []}, {"text": "But a variable can also be allowed to range over an infinite space of discrete structures-in particular, the set of all strings, a case first explored by.", "labels": [], "entities": []}, {"text": "This setting arises because human languages make use of many word forms.", "labels": [], "entities": []}, {"text": "These strings are systematically related in their spellings due to linguistic processes such as morphology, phonology, abbreviation, copying error and historical change.", "labels": [], "entities": []}, {"text": "To analyze or predict novel strings, we can model the joint distribution of many related strings at once.", "labels": [], "entities": []}, {"text": "Under a graphical model, the joint probability of an assignment tuple is modeled as a product of potentials on sub-tuples, each of which is usually modeled in turn by a weighted finite-state machine.", "labels": [], "entities": []}, {"text": "In general, we wish to infer the values of unknown strings in the graphical model.", "labels": [], "entities": []}, {"text": "Deterministic * This material is based upon work supported by the National Science Foundation under Grant No. 1423276, and by a Fulbright Research Scholarship to the first author.", "labels": [], "entities": []}, {"text": "approaches to this problem have focused on belief propagation (BP), a message-passing algorithm that is exact on acyclic graphical models and approximate on cyclic (\"loopy\") ones ().", "labels": [], "entities": [{"text": "belief propagation (BP)", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.8479006886482239}]}, {"text": "But in both cases, further heuristic approximations of the BP messages are generally used for speed.", "labels": [], "entities": [{"text": "speed", "start_pos": 94, "end_pos": 99, "type": "METRIC", "confidence": 0.9819506406784058}]}, {"text": "In this paper, we develop a more principled and flexible way to approximate the messages, using variable-order n-gram models.", "labels": [], "entities": []}, {"text": "We first develop aversion of expectation propagation (EP) for string-valued variables.", "labels": [], "entities": [{"text": "aversion of expectation propagation (EP)", "start_pos": 17, "end_pos": 57, "type": "TASK", "confidence": 0.5857294031551906}]}, {"text": "EP offers a principled way to approximate BP messages by distributions from a fixed family-e.g., by trigram models.", "labels": [], "entities": [{"text": "EP", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8754159808158875}]}, {"text": "Each message update is found by minimizing a certain KL-divergence.", "labels": [], "entities": []}, {"text": "Second, we generalize to variable-order models.", "labels": [], "entities": []}, {"text": "To do this, we augment EP's minimization problem with a novel penalty term that keeps the number of n-grams finite.", "labels": [], "entities": []}, {"text": "In general, we advocate penalizing more \"complex\" messages (in our setting, large finite-state acceptors).", "labels": [], "entities": []}, {"text": "Complex messages are slower to construct, and slower to use in later steps.", "labels": [], "entities": []}, {"text": "Our penalty term is formally similar to regularizers that encourage structured sparsity ().", "labels": [], "entities": []}, {"text": "Like a regularizer, it lets us use a more expressive family of distributions, secure in the knowledge that we will use only as many of the parameters as we really need fora \"pretty good\" fit.", "labels": [], "entities": []}, {"text": "But why avoid using more parameters?", "labels": [], "entities": []}, {"text": "Regularization seeks better generalization by not overfitting the model to the data.", "labels": [], "entities": [{"text": "Regularization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9234830737113953}]}, {"text": "By contrast, we already have a model and are merely doing inference.", "labels": [], "entities": []}, {"text": "We seek better runtime by not over-fussing about capturing the model's marginal distributions.", "labels": [], "entities": []}, {"text": "Our \"penalized EP\" (PEP) inference strategy is applicable to any graphical model with complex messages.", "labels": [], "entities": []}, {"text": "In this paper, we focus on strings, and show how PEP speeds up inference on the computational phonology model of.", "labels": [], "entities": [{"text": "PEP", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.7929770350456238}]}, {"text": "We provide further details, tutorial material, and results in the appendices (supplementary material).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experimental design aims to answer three questions.", "labels": [], "entities": []}, {"text": "(1) Is our algorithm able to beat a strong baseline (adaptive pruning) in a non-trivial model?", "labels": [], "entities": []}, {"text": "(2) Is PEP actually better than ordinary EP, given that the structured sparsity penalty makes it more algorithmically complex?", "labels": [], "entities": []}, {"text": "(3) Does the \u03bb parameter successfully trade off between speed and accuracy?", "labels": [], "entities": [{"text": "speed", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9912108182907104}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.986973762512207}]}, {"text": "All experiments took place using the graphical model over strings for the discovery of underlying phonological forms introduced in In their Bayes net, morpheme underlying forms are latent variables, while word surface forms are observed variables.", "labels": [], "entities": []}, {"text": "The factors model underlyingto-surface phonological changes.", "labels": [], "entities": []}, {"text": "They learn the factors by Expectation Maximization (EM).", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.6862028002738952}]}, {"text": "Their first E step presents the hardest inference problem because the factors initially contribute no knowledge of the language; so that is the setting we test on here.", "labels": [], "entities": []}, {"text": "Their data are surface phonological forms from the CELEX database ().", "labels": [], "entities": [{"text": "CELEX database", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.9798387885093689}]}, {"text": "For each of 3 languages, we run 5 experiments, by observing the surface forms of 100 to 500 words and running EP to infer the underlying forms of their morphemes.", "labels": [], "entities": []}, {"text": "Each of the 15 factor graphs has \u2248 150-700 latent variables, joined by 500-2200 edges to 200-1200 factors of degree 1-3.", "labels": [], "entities": []}, {"text": "Variables representing suffixes can have extremely high degree (> 100).", "labels": [], "entities": [{"text": "degree", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9962756633758545}]}, {"text": "We compare PEP with other approximate inference methods.", "labels": [], "entities": []}, {"text": "As our main baseline, we take the approximation scheme actually used by, which restricts the domain of a belief to that of the union of 20-best strings of its incoming messages (section 5).We also compare to unpenalized EP with unigram, bigram, and trigram features.", "labels": [], "entities": []}, {"text": "We report both speed and accuracy for all methods.", "labels": [], "entities": [{"text": "speed", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9994364380836487}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9995405673980713}]}, {"text": "Speed is reported in seconds.", "labels": [], "entities": []}, {"text": "Judging accuracy is a bit trickier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9926274418830872}]}, {"text": "The best metric would to be to measure our beliefs' distance from the true marginals or even from the beliefs computed by vanilla loopy BP.", "labels": [], "entities": []}, {"text": "Obtaining these quantities, however, would be extremely expensive-even Gibbs sampling is infeasible in our setting, let alone 100-way WFSA intersections.", "labels": [], "entities": []}, {"text": "Luckily, provide goldstandard values for the latent variables (underlying forms).", "labels": [], "entities": []}, {"text": "shows the negated log-probabilities of these gold strings according to our beliefs, averaged over variables in a given factor graph.", "labels": [], "entities": []}, {"text": "Our accuracy is weaker than because we are doing inference with their initial (untrained) parameters, a more challenging problem.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995260238647461}]}, {"text": "Each update to \u03b8 V consisted of a single step of (proximal) gradient descent: starting at the current value, improve (2) with a gradient step of size \u03b7 = 0.05, then (in the adaptive case) apply the proximal operator of (9) with \u03bb = 0.01.", "labels": [], "entities": []}, {"text": "We chose these values by preliminary exploration, taking \u03b7 small enough to avoid backtracking (section 6.1).", "labels": [], "entities": []}, {"text": "We repeatedly visit variables and factors (section 4.4) in the forward-backward order used by.", "labels": [], "entities": []}, {"text": "For the first few iterations, when we visit a variable we make K = 20 passes over its incoming messages, updating them iteratively to ensure that the high probability strings in the initial approximations are \"in the ballpark\".", "labels": [], "entities": []}, {"text": "For subsequent iterations of message passing we take K = 1.", "labels": [], "entities": [{"text": "message passing", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.8146380484104156}]}, {"text": "For similar reasons, we constrained PEP to use only unigram features on the first iteration, when there are still many viable candidates for each morph.", "labels": [], "entities": []}], "tableCaptions": []}