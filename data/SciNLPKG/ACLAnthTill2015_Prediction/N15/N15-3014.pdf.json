{"title": [{"text": "Lean Question Answering over Freebase from Scratch", "labels": [], "entities": [{"text": "Lean Question Answering", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6744213203589121}]}], "abstractContent": [{"text": "For the task of question answering (QA) over Freebase on the WEBQUESTIONS dataset (Berant et al., 2013), we found that 85% of all questions (in the training set) can be directly answered via a single binary relation.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.8661002159118653}, {"text": "WEBQUESTIONS dataset", "start_pos": 61, "end_pos": 81, "type": "DATASET", "confidence": 0.9845181405544281}]}, {"text": "Thus we turned this task into slot-filling for <question topic, relation, answer> tuples: predicting relations to get answers given a question's topic.", "labels": [], "entities": []}, {"text": "We design efficient data structures to identify question topics organically from 46 million Freebase topic names, without employing any NLP processing tools.", "labels": [], "entities": []}, {"text": "Then we present a lean QA system that runs in real time (in offline batch testing it answered two thousand questions in 51 seconds on a laptop).", "labels": [], "entities": []}, {"text": "The system also achieved 7.8% better F 1 score (harmonic mean of average precision and recall) than the previous state of the art.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9905306498209635}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.983415961265564}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9987327456474304}]}], "introductionContent": [{"text": "Large-scale open-domain question answering from structured Knowledge Base (KB) provides a good balance of precision and recall in everyday QA tasks, executed by search engines and personal assistant applications.", "labels": [], "entities": [{"text": "question answering", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.6989504992961884}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9992166757583618}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.998600423336029}]}, {"text": "The release of WEBQUESTIONS dataset) has drawn a lot of interest from both academia and industry.", "labels": [], "entities": [{"text": "WEBQUESTIONS dataset", "start_pos": 15, "end_pos": 35, "type": "DATASET", "confidence": 0.8564038276672363}]}, {"text": "One tendency to notice is that the general trend of research is becoming more complex, utilizing various techniques such as semantic parsing and deep neural networks.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.7853486835956573}]}, {"text": "We took a radically different approach by heading for the other direction: simplifying the task as much as possible with no compromise on speed and accuracy.", "labels": [], "entities": [{"text": "speed", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9884806871414185}, {"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9775940179824829}]}, {"text": "We treat the task of QA from Freebase * Incubated by the Allen Institute for Artificial Intelligence.", "labels": [], "entities": []}, {"text": "as a two-step problem: identifying the correct topic (search problem) and predicting the correct answer (prediction problem).", "labels": [], "entities": []}, {"text": "The common approach to the first problem is applying basic linguistic processing, such as part-of-speech (POS) tagging and chunking to identify noun phrases, and named entity recognition (NER) for interesting topics.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 90, "end_pos": 118, "type": "TASK", "confidence": 0.7194325089454651}, {"text": "named entity recognition (NER)", "start_pos": 162, "end_pos": 192, "type": "TASK", "confidence": 0.7888484547535578}]}, {"text": "The common approach to the second problem is detailed question analysis, which usually involves parsing.", "labels": [], "entities": [{"text": "question analysis", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.672937661409378}, {"text": "parsing", "start_pos": 96, "end_pos": 103, "type": "TASK", "confidence": 0.9633836150169373}]}, {"text": "In any case, various components from the natural language processing (NLP) pipeline are usually applied.", "labels": [], "entities": []}, {"text": "With an emphasis on real-time prediction (usually making a prediction within 100 milliseconds after seeing the question), we chose not to use any NLP preprocessing -not even POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 174, "end_pos": 185, "type": "TASK", "confidence": 0.5996717214584351}]}, {"text": "Instead we design efficient data structures to help identify named entities to tackle the search problem.", "labels": [], "entities": []}, {"text": "For the prediction problem, we found that given a question and its topic, simply predicting the KB relation between the topic and the answer is sufficient.", "labels": [], "entities": [{"text": "prediction", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.9634255766868591}]}, {"text": "In other words, we turned QA from Freebase into a slot-filling problem in the form of <topic, relation, answer> tuples: given a question, the task is to find the answer, while the search problem is to find the topic and the prediction problem is to find the relation.", "labels": [], "entities": []}, {"text": "For instance, given the question what's sweden's currency?, the task can be turned into a tuple of <Sweden, /location/country/currency_used, Swedish krona>.", "labels": [], "entities": []}, {"text": "In Section 3 we address how to identify the topic (Sweden) and in Section 4 how to predict the relation (/location/country/currency_used).", "labels": [], "entities": []}, {"text": "There are obvious limitations in this task format, which are discussed in Section 6.", "labels": [], "entities": []}, {"text": "Going beyond reporting evaluation scores, we describe in details our design principle and also report performance in speed.", "labels": [], "entities": [{"text": "speed", "start_pos": 117, "end_pos": 122, "type": "METRIC", "confidence": 0.9935058355331421}]}, {"text": "This paper makes the follow-66 ing technical contributions to QA from KB: \u2022 We design and compare several data structures to help identify question topics using the KB resource itself.", "labels": [], "entities": []}, {"text": "The key to success is to search through 46 million Freebase topics efficiently while still being robust against noise (such as typographical or speech recognition errors).", "labels": [], "entities": []}, {"text": "\u2022 Our algorithm is high-performance, real-time, and simple enough to replicate.", "labels": [], "entities": []}, {"text": "It achieved state-of-the-art result on the WEBQUESTIONS dataset.", "labels": [], "entities": [{"text": "WEBQUESTIONS dataset", "start_pos": 43, "end_pos": 63, "type": "DATASET", "confidence": 0.9673336446285248}]}, {"text": "Training time in total is less than 5 minutes and testing on 2032 questions takes less than 1 minute.", "labels": [], "entities": []}, {"text": "There are no external NLP library dependencies: the only preprocessing is lowercasing.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results on the WEBQUESTIONS test set.", "labels": [], "entities": [{"text": "WEBQUESTIONS test set", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.8213838140169779}]}]}