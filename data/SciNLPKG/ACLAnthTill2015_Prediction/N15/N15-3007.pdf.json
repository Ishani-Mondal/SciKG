{"title": [{"text": "ICE: Rapid Information Extraction Customization for NLP Novices", "labels": [], "entities": [{"text": "ICE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9077842235565186}, {"text": "Rapid Information Extraction Customization", "start_pos": 5, "end_pos": 47, "type": "TASK", "confidence": 0.6720069572329521}]}], "abstractContent": [{"text": "We showcase ICE, an Integrated Customiza-tion Environment for Information Extraction.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.7602254748344421}]}, {"text": "ICE is an easy tool for non-NLP experts to rapidly build customized IE systems fora new domain.", "labels": [], "entities": [{"text": "ICE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6283805966377258}]}], "introductionContent": [{"text": "Creating an information extraction (IE) system fora new domain, with new vocabulary and new classes of entities and relations, remains a task requiring substantial time, training, and expense.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.873360025882721}]}, {"text": "This has been an obstacle to the wider use of IE technology.", "labels": [], "entities": [{"text": "IE", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9853841662406921}]}, {"text": "The tools which have been developed for this task typically do not take full advantage of linguistic analysis and available learning methods to provide guidance to the user in building the IE system.", "labels": [], "entities": [{"text": "IE", "start_pos": 189, "end_pos": 191, "type": "TASK", "confidence": 0.9593993425369263}]}, {"text": "They also generally require some understanding of system internals and data representations.", "labels": [], "entities": []}, {"text": "We have created ICE [the Integrated Customization Environment], which lowers the barriers to IE system development by providing guidance while letting the user retain control, and by allowing the user to interact in terms of the words and phrases of the domain, with a minimum of formal notation.", "labels": [], "entities": [{"text": "IE system development", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.9223827918370565}]}, {"text": "In this paper, we review related systems and explain the technologies behind ICE.", "labels": [], "entities": [{"text": "ICE", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9625234007835388}]}, {"text": "The code, documentation, and a demo video of ICE can be found at http://nlp.cs.nyu.edu/ice/", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform end-to-end relation extraction experiments to evaluate the utility of ICE: we start from: End-to-end relation extraction using large rule sets.", "labels": [], "entities": [{"text": "end-to-end relation extraction", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.6195562382539114}, {"text": "ICE", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9328930377960205}, {"text": "End-to-end relation extraction", "start_pos": 101, "end_pos": 131, "type": "TASK", "confidence": 0.5955375035603842}]}, {"text": "Same configurations as plain text, extract named entities, and finally extract drug names and relations with models built by ICE.", "labels": [], "entities": [{"text": "ICE", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.8156547546386719}]}, {"text": "We collect approximately 5,000 web news posts from the U.S. Drug Enforcement Administration 3 (DEA) for our experiments.", "labels": [], "entities": [{"text": "U.S. Drug Enforcement Administration 3 (DEA)", "start_pos": 55, "end_pos": 99, "type": "DATASET", "confidence": 0.6409872733056545}]}, {"text": "In our first experiment, we extracted 3,703 terms from this corpus and manually identified 119 DRUGS names and 97 law enforcement agent (AGENTS) mentions, which we use as the \"gold standard\" sets.", "labels": [], "entities": [{"text": "AGENTS", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.8597267866134644}]}, {"text": "We then ran our customizer in the following manner: 1) we provided the entity set expansion program with two seeds (\"methamphetamine\" and \"oxycodone\" for DRUGS; \"special agents\" and \"law enforcement officers\" for AGENTS); 2) the program produced a ranked list of terms; 3) in each iteration, we examined the top 20 terms that had not been examined in previous iterations; 4) if a term is in the gold standard set, we added it to the expander as a positive seed, otherwise, we added it as a negative seed; 5) we continued the expansion with the updated seed set, repeating the process for 10 iterations.", "labels": [], "entities": [{"text": "DRUGS", "start_pos": 154, "end_pos": 159, "type": "DATASET", "confidence": 0.8670377731323242}, {"text": "AGENTS", "start_pos": 213, "end_pos": 219, "type": "METRIC", "confidence": 0.8106454610824585}]}, {"text": "This process produced high-recall dictionary-based entity taggers (74% for drugs, 82% for agents) in just a few minutes.", "labels": [], "entities": []}, {"text": "With the ICE-built DRUGS dictionary, we performed end-to-end extraction of two relations: SELL, in which a PERSON sells DRUGS, using \"PERSON sell DRUGS\" as seed, and RESIDENT-OF, which indicates that a PERSON resides in a GPE 4 , using \"PERSON of GPE\" as seed.", "labels": [], "entities": [{"text": "ICE-built DRUGS dictionary", "start_pos": 9, "end_pos": 35, "type": "DATASET", "confidence": 0.8248058756192526}, {"text": "SELL", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9951883554458618}, {"text": "RESIDENT-OF", "start_pos": 166, "end_pos": 177, "type": "METRIC", "confidence": 0.9971115589141846}]}, {"text": "We manually annotated 51 documents from the DEA collection.", "labels": [], "entities": [{"text": "DEA collection", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.9429983794689178}]}, {"text": "There are 110 SELL relations and 45 RESIDENT-OF relations in the annotated corpus.", "labels": [], "entities": [{"text": "SELL", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.8735800385475159}, {"text": "RESIDENT-OF", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.958034873008728}]}, {"text": "We first extracted small rule sets.", "labels": [], "entities": []}, {"text": "For both relations, we asked a user to review the presented LDPs on the first screen (20 LDPs in total) and then ran bootstrapping using the expanded seeds.", "labels": [], "entities": []}, {"text": "We did this for 3 iterations, so the user evaluated 60 LDPs, which took less than half an hour.", "labels": [], "entities": []}, {"text": "We report the results in.", "labels": [], "entities": []}, {"text": "Note that these are end-to-end scores, reflecting in part errors of entity extraction.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7450859546661377}]}, {"text": "After entity tagging and coreference resolution, the recall of entity mentions is 0.76 in our experiments.", "labels": [], "entities": [{"text": "entity tagging", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.733921617269516}, {"text": "coreference resolution", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.9583499431610107}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9989497065544128}]}, {"text": "We observe that fuzzy LDP match with negative rule sets obtains best results for both relations.", "labels": [], "entities": []}, {"text": "If we remove the negative rule set, the precision of RESIDENT-OF is hurt more severely than the SELL relations.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9997530579566956}, {"text": "RESIDENT-OF", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9838441014289856}, {"text": "SELL", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9928315877914429}]}, {"text": "On the other hand, if we require exact match, the recall of SELL will decrease very significantly.", "labels": [], "entities": [{"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9997995495796204}, {"text": "SELL", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.8592961430549622}]}, {"text": "This discrepancy in performance is due to the nature of the two relations.", "labels": [], "entities": []}, {"text": "RESIDENT-OF is a relatively closed binary relation, with fewer lexical variations: the small RESIDENT-OF model covers around 50% of the relation mentions with 7 positive LDPs, so it is easier to rule out false positives than to further boost recall.", "labels": [], "entities": [{"text": "RESIDENT-OF", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9223518967628479}, {"text": "RESIDENT-OF", "start_pos": 93, "end_pos": 104, "type": "METRIC", "confidence": 0.9543817639350891}, {"text": "recall", "start_pos": 242, "end_pos": 248, "type": "METRIC", "confidence": 0.997226893901825}]}, {"text": "SELL, in contrast, can be expressed in many different ways, and fuzzy LDP match is essential for reasonable recall.", "labels": [], "entities": [{"text": "SELL", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5332444906234741}, {"text": "fuzzy LDP match", "start_pos": 64, "end_pos": 79, "type": "METRIC", "confidence": 0.7002261678377787}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9959598183631897}]}, {"text": "We report experimental results on larger rule sets in.", "labels": [], "entities": []}, {"text": "The large rule sets were bootstrapped in 3 iterations as well, but the user reviewed 250 LDPs in each iteration.", "labels": [], "entities": []}, {"text": "The best score in this setting improves to 0.4 F-score for SELL and 0.62 F-score for RESIDENT-OF, as we have more LDP rules.", "labels": [], "entities": [{"text": "F-score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9987579584121704}, {"text": "SELL", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.8895173072814941}, {"text": "F-score", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9981034994125366}, {"text": "RESIDENT-OF", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.9601734280586243}]}, {"text": "The exact match extractor performs better than the fuzzy match extractor for RESIDENT-OF, as the latter is hurt by false positives.", "labels": [], "entities": [{"text": "RESIDENT-OF", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.8413252234458923}]}], "tableCaptions": [{"text": " Table 1: End-to-end relation extraction using small rule  sets. Fuzzy: fuzzy match relation extractor with negative  rule set; -neg: fuzzy match extractor without negative rule  set; Exact: exact match extractor; P / R / F: Precision /  Recall / F-score", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.6911190450191498}, {"text": "Fuzzy", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9917900562286377}, {"text": "F", "start_pos": 222, "end_pos": 223, "type": "METRIC", "confidence": 0.8220460414886475}, {"text": "Precision /  Recall / F-score", "start_pos": 225, "end_pos": 254, "type": "METRIC", "confidence": 0.6887916803359986}]}, {"text": " Table 2: End-to-end relation extraction using large rule  sets. Same configurations as", "labels": [], "entities": [{"text": "End-to-end relation extraction", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.5715564092000326}]}]}