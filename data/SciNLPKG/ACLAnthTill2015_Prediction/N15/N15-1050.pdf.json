{"title": [{"text": "Modeling Word Meaning in Context with Substitute Vectors", "labels": [], "entities": [{"text": "Modeling Word Meaning in Context with Substitute Vectors", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.8360819295048714}]}], "abstractContent": [{"text": "Context representations area key element in distributional models of word meaning.", "labels": [], "entities": [{"text": "Context representations", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8587341904640198}]}, {"text": "In contrast to typical representations based on neighboring words, a recently proposed approach suggests to represent a context of a target word by a substitute vector, comprising the potential fillers for the target word slot in that context.", "labels": [], "entities": []}, {"text": "In this work we first propose a variant of substitute vectors, which we find particularly suitable for measuring context similarity.", "labels": [], "entities": []}, {"text": "Then, we propose a novel model for representing word meaning in context based on this context representation.", "labels": [], "entities": []}, {"text": "Our model outper-forms state-of-the-art results on lexical substitution tasks in an unsupervised setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Following the distributional hypothesis, distributional models represent the meaning of a word type as an aggregation of its contexts.", "labels": [], "entities": []}, {"text": "A recent line of work addresses polysemy of word types by representing the meaning (or sense) of each word instance individually as induced by its particular context.", "labels": [], "entities": []}, {"text": "The context-sensitive meaning of a word instance is commonly called the word meaning in context, as opposed to the word meaning out-ofcontext of a word type.", "labels": [], "entities": []}, {"text": "A key element of distributional models is the choice of context representation.", "labels": [], "entities": []}, {"text": "A context of a word instance is typically represented by an unordered collection of its first-order neighboring words, called bag-of-words (BOW).", "labels": [], "entities": []}, {"text": "In contrast,  includes the potential filler words for the target word slot, weighted according to how 'fit' they are to fill the target slot given the neighboring words.", "labels": [], "entities": []}, {"text": "For example, the substitute vector representing the context \"I my smartphone.\"", "labels": [], "entities": []}, {"text": "(target slot underlined), would typically include potential slot fillers such as love, lost, upgraded, etc.", "labels": [], "entities": []}, {"text": "argued that substitute vectors are potentially more informative than traditional context representations since the fitness of the fillers is estimated using an n-gram language model, thereby capturing information embedded in the neighboring word order.", "labels": [], "entities": []}, {"text": "They showed promising results on measuring word similarity out-of-context with a distributional model based on this approach.", "labels": [], "entities": []}, {"text": "In this paper we first propose a variant of substitute vectors as a context representation, which we find particularly suitable for measuring context similarity.", "labels": [], "entities": []}, {"text": "Then, we extend the work in by proposing a novel distributional model for representing word meaning in context, based on this context representation.", "labels": [], "entities": []}, {"text": "Like sub-stitute vectors, the word representations learned by our model are second-order vectors, which we call paraphrase vectors.", "labels": [], "entities": []}, {"text": "illustrates the difference between BOW and substitute vector context representations, as well as our representation fora word in context.", "labels": [], "entities": [{"text": "BOW", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.8630661964416504}]}, {"text": "Our model outperforms state-of-the-art results on lexical substitution tasks while learning from plain text in an unsupervised setting.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset introduced in the lexical substitution task of, denoted here LS07, is the most widely used for the evaluation of lexical substitution.", "labels": [], "entities": [{"text": "lexical substitution task", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.7366079489390055}]}, {"text": "It consists of 10 sentences extracted from a web corpus for each of 201 target words (nouns, verbs, adjectives and adverbs), or altogether 2,010 word instances in sentential context, split into 300 trial sentences and 1,710 test sentences.", "labels": [], "entities": []}, {"text": "The gold standard provided with this dataset is a weighted lemmatized paraphrase list for each word instance, based on manual annotations.", "labels": [], "entities": []}, {"text": "A more recent dataset (), denoted LS14, provides the same kind of data as LS07, but instead of target words that were specifically selected to be ambiguous as in LS07, the target words here are simply all the content words in text documents extracted from news and fiction corpora.", "labels": [], "entities": []}, {"text": "LS14 is also much larger than LS07 with over 15K target word instances.", "labels": [], "entities": [{"text": "LS14", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8257893919944763}]}], "tableCaptions": [{"text": " Table 4: Precision values for compared context similarity  measures. Only the best performing configurations for  BOW and CBOW are shown.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.995860755443573}, {"text": "BOW", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.4847725033760071}, {"text": "CBOW", "start_pos": 123, "end_pos": 127, "type": "DATASET", "confidence": 0.741615355014801}]}, {"text": " Table 5: best and oot subtasks scores for all compared  methods. best-m and oot-m stand for the mode scores.", "labels": [], "entities": []}, {"text": " Table 6. Looking first  at the results on the LS07 dataset, we see that not  surprisingly both our out-of-context method, P out ,  and the word2vec baseline, w2v out  skip,2w , which ig- nore the given context, achieve relatively low results.  S cond,1000 that considers only the context compati- bility performs a little better. Next, we see that our  in-context method, P in,100%", "labels": [], "entities": [{"text": "LS07 dataset", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.9562257528305054}]}, {"text": " Table 6:  GAP scores for compared methods.  UW = ukWaC; GW = Gigaword; WP = Wikipedia;  WN = WordNet; BN = British National Corpus (", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.8516994118690491}, {"text": "Wikipedia", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.9448477625846863}, {"text": "WordNet", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.9027546048164368}, {"text": "BN", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.8907984495162964}, {"text": "British National Corpus", "start_pos": 108, "end_pos": 131, "type": "DATASET", "confidence": 0.9499902923901876}]}, {"text": " Table 7: The effect of context clustering on our model's  performance on LS07. Orig stands for best configuration  of our model with no clustering, and 1000/100 stand for  the same configuration with that number of clusters.", "labels": [], "entities": []}]}