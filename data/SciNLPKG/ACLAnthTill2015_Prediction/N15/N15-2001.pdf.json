{"title": [{"text": "Cache-Augmented Latent Topic Language Models for Speech Retrieval", "labels": [], "entities": [{"text": "Speech Retrieval", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.7488783597946167}]}], "abstractContent": [{"text": "We aim to improve speech retrieval performance by augmenting traditional N-gram language models with different types of topic context.", "labels": [], "entities": [{"text": "speech retrieval", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.752464085817337}]}, {"text": "We present a latent topic model framework that treats documents as arising from an underlying topic sequence combined with a cache-based repetition model.", "labels": [], "entities": []}, {"text": "We analyze our proposed model both for its ability to capture word repetition via the cache and for its suitability as a language model for speech recognition and retrieval.", "labels": [], "entities": [{"text": "word repetition", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.6767826080322266}, {"text": "speech recognition and retrieval", "start_pos": 140, "end_pos": 172, "type": "TASK", "confidence": 0.7440625876188278}]}, {"text": "We show this model, augmented with the cache, captures intuitive repetition behavior across languages and exhibits lower perplexity than regular LDA on held out data in multiple languages.", "labels": [], "entities": []}, {"text": "Lastly, we show that our joint model improves speech retrieval performance beyond N-grams or latent topics alone, when applied to a term detection task in all languages considered.", "labels": [], "entities": [{"text": "speech retrieval", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.700577586889267}, {"text": "term detection task", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7934292455514272}]}], "introductionContent": [{"text": "The availability of spoken digital media continues to expand at an astounding pace.", "labels": [], "entities": []}, {"text": "According to YouTube's publicly released statistics, between August 2013 and February 2015 content upload rates have tripled from 100 to 300 hours of video per minute).", "labels": [], "entities": []}, {"text": "Yet the information content therein, while accessible via links, tags, or other user-supplied metadata, is largely inaccessible via content search within the speech.", "labels": [], "entities": []}, {"text": "Speech retrieval systems typically rely on Large Vocabulary Continuous Speech Recognition (LVSCR) to generate a lattice of word hypotheses for each document, indexed for fast search.", "labels": [], "entities": [{"text": "Speech retrieval", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7531568706035614}, {"text": "Large Vocabulary Continuous Speech Recognition (LVSCR)", "start_pos": 43, "end_pos": 97, "type": "TASK", "confidence": 0.7113830745220184}]}, {"text": "However, for sites like YouTube, localized in over 60 languages, the likelihood of high accuracy speech recognition inmost languages is quite low.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9664241671562195}, {"text": "speech recognition", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.6722307056188583}]}, {"text": "Our proposed solution is to focus on topic information in spoken language as a means of dealing with errorful speech recognition output in many languages.", "labels": [], "entities": [{"text": "speech recognition output", "start_pos": 110, "end_pos": 135, "type": "TASK", "confidence": 0.7677589257558187}]}, {"text": "It has been repeatedly shown that a task like topic classification is robust to high (40-60%) word error rate systems.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.8378305435180664}, {"text": "word error rate", "start_pos": 94, "end_pos": 109, "type": "METRIC", "confidence": 0.6937899390856425}]}, {"text": "We would leverage the topic signal's strength for retrieval in a high volume, multilingual digital media processing environment.", "labels": [], "entities": []}, {"text": "The English word topic, defined as a particular 'subject of discourse'), arises from the Greek root, \u03c4 o\u03c0o\u03c2, meaning a physical 'place' or 'location'.", "labels": [], "entities": []}, {"text": "However, the semantic concepts of a particular subject are not disjoint from the physical location of the words themselves.", "labels": [], "entities": []}, {"text": "The goal of this particular work is to jointly model two aspects of topic information, local context (repetition) and broad context (subject matter), which we previously treated in an ad hoc manner) in a latent topic framework.", "labels": [], "entities": []}, {"text": "We show that in doing so we can achieve better word retrieval performance than language models with only N-gram context on a diverse set of spoken languages.", "labels": [], "entities": [{"text": "word retrieval", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.7429496943950653}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Mean \u03ba (d) inferred from 10 hour development  data, by number of latent topics", "labels": [], "entities": [{"text": "Mean \u03ba (d) inferred", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9286670585473379}]}, {"text": " Table 2: Perplexities of topic unigram mixtures on held- out data, with and without cache.", "labels": [], "entities": []}, {"text": " Table 3: Best KWS accuracy (TWV) is each language.", "labels": [], "entities": [{"text": "accuracy (TWV)", "start_pos": 19, "end_pos": 33, "type": "METRIC", "confidence": 0.9033301323652267}]}]}