{"title": [{"text": "Speeding Document Annotation with Topic Models", "labels": [], "entities": [{"text": "Speeding Document Annotation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.80240265528361}]}], "abstractContent": [{"text": "Document classification and topic models are useful tools for managing and understanding large corpora.", "labels": [], "entities": [{"text": "Document classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8547624349594116}]}, {"text": "Topic models are used to uncover underlying semantic and structure of document collections.", "labels": [], "entities": []}, {"text": "Categorizing large collection of documents requires hand-labeled training data, which is time consuming and needs human expertise.", "labels": [], "entities": []}, {"text": "We believe engaging user in the process of document labeling helps reduce annotation time and address user needs.", "labels": [], "entities": [{"text": "document labeling", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.6688903421163559}]}, {"text": "We present an interactive tool for document labeling.", "labels": [], "entities": [{"text": "document labeling", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7067801058292389}]}, {"text": "We use topic models to help users in this procedure.", "labels": [], "entities": []}, {"text": "Our preliminary results show that users can more effectively and efficiently apply labels to documents using topic model information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many fields depend on texts labeled by human experts; computational linguistics uses such annotation to determine word senses and sentiment (); social science uses \"coding\" to scale up and systemetize content analysis).", "labels": [], "entities": [{"text": "systemetize content analysis", "start_pos": 189, "end_pos": 217, "type": "TASK", "confidence": 0.6693203250567118}]}, {"text": "In general text classification is a standard tool for managing large document collections.", "labels": [], "entities": [{"text": "text classification", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7772685289382935}]}, {"text": "However, these labeled data have to come from somewhere.", "labels": [], "entities": []}, {"text": "The process for creating a broadly applicable, consistent, and generalizable label set and then applying them to the dataset is long and difficult, requiring expensive annotators to examine large swaths of the data.", "labels": [], "entities": []}, {"text": "We present a user interactive tool for document labeling that uses topic models to help users assign appropriate labels to documents (Section 2).", "labels": [], "entities": [{"text": "document labeling", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.6682579517364502}]}, {"text": "In Section 3, we describe our user interface and experiments on Congressional Bills data set.", "labels": [], "entities": [{"text": "Congressional Bills data set", "start_pos": 64, "end_pos": 92, "type": "DATASET", "confidence": 0.937715470790863}]}, {"text": "We also explain an evaluation metric to assess the quality of assigned document labels.", "labels": [], "entities": []}, {"text": "In preliminary results, we show that annotators can more quickly label a document collection given a topic modeling overview.", "labels": [], "entities": []}, {"text": "While engaging user in the process of content-analysis has been studied before(as we discuss in Section 4), in Section 4 we describe how our new framework allows for more flexibility and interactivity.", "labels": [], "entities": []}, {"text": "Finally, in Section 5, we discuss the limitation of our framework and how we plan to extend it in future.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data: In our experiments, we need a labeled corpus to be able to assess the quality of usergenerated labels.", "labels": [], "entities": []}, {"text": "We chose US Congressional Bills corpus).", "labels": [], "entities": [{"text": "US Congressional Bills", "start_pos": 9, "end_pos": 31, "type": "DATASET", "confidence": 0.9348605871200562}]}, {"text": "GovTrack provides bill texts along with the discussed congressional issues as labels.", "labels": [], "entities": [{"text": "GovTrack", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9267612099647522}]}, {"text": "Example of labels are \"education\", \"agriculture\", \"health\", and \"defense\".", "labels": [], "entities": []}, {"text": "There are total of 19 unique labels.", "labels": [], "entities": []}, {"text": "We use the 112 th congress, which has 12274 documents.", "labels": [], "entities": []}, {"text": "We remove bills with no assigned gold label or that are short.", "labels": [], "entities": []}, {"text": "We end with 6528 documents.", "labels": [], "entities": []}, {"text": "Topic Modeling: To generate topics, we use Mallet) to apply lda on the data.", "labels": [], "entities": [{"text": "Topic Modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8059966266155243}]}, {"text": "A set of extra stop words are generated based on tf-idf scores to avoid displaying noninformative words to the user.", "labels": [], "entities": []}, {"text": "Features and Classification: A crucial step for text classification is to extract useful features to represent documents.", "labels": [], "entities": [{"text": "Features and Classification", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6239169339338938}, {"text": "text classification", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.754014402627945}]}, {"text": "Some common features for text classification are n-grams, which makes the dimensionality very high and classification slower.", "labels": [], "entities": [{"text": "text classification", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8513737618923187}]}, {"text": "Since response time is very important in user interactive systems, instead of n-grams, we nature: An example of topic words and the labels user has assigned to top documents for that topic.", "labels": [], "entities": []}, {"text": "use topic probabilities as features, which reduces the dimensionality and classification time significantly.", "labels": [], "entities": []}, {"text": "User can choose 10, 15, 25, or 50 topics.", "labels": [], "entities": []}, {"text": "We want to show the label probabilities generated by classifier to users.", "labels": [], "entities": []}, {"text": "We use) to run L2 regularized logistic regression for classifying documents and generating label probabilities.", "labels": [], "entities": []}, {"text": "Interface: We start with the web-based interface of for interactive topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.707505464553833}]}, {"text": "The existing interface starts with asking user information, corpus name, and number of topics they want to explore.", "labels": [], "entities": []}, {"text": "Then it displays topic words and the most relevant documents for each topic.", "labels": [], "entities": []}, {"text": "Also, the user can seethe content of documents.", "labels": [], "entities": []}, {"text": "Users can create new labels and/or edit/delete an existing label.", "labels": [], "entities": []}, {"text": "When seeing a document, user has 3 options: 1.", "labels": [], "entities": []}, {"text": "Create anew label and assign that label to the document.", "labels": [], "entities": []}, {"text": "2. Choose an existing label for the document.", "labels": [], "entities": []}, {"text": "At any point, the user can run the classifier.", "labels": [], "entities": []}, {"text": "After classification is finished, the predicted labels along with the certainty is shown for each document.", "labels": [], "entities": [{"text": "certainty", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9982172846794128}]}, {"text": "User can edit/delete document labels and re-run classifier as many times as they desire.", "labels": [], "entities": []}, {"text": "We Refer to this task as Topic Guided Annotation(TGA).", "labels": [], "entities": []}, {"text": "shows a screenshot of the interface when choosing a label fora document.", "labels": [], "entities": []}, {"text": "We introduce an interactive framework for document labeling using topic models.", "labels": [], "entities": [{"text": "document labeling", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.6299939006567001}]}, {"text": "In this section, we evaluate our system.", "labels": [], "entities": []}, {"text": "Our goal is to measure whether showing users a topic modeling overview of the corpus helps them apply labels to documents more effectively and efficiently.", "labels": [], "entities": []}, {"text": "Thus, we compare user-generated labels (considering labels assigned by user and classifier altogether) with gold labels of US Congressional Bills provided by GovTrack.", "labels": [], "entities": []}, {"text": "Since user labels can be more specific than gold labels, we want each user label to be \"pure\" in gold labels.", "labels": [], "entities": []}, {"text": "Thus, we use the purity score) to measure how many gold labels are associated with each user label.", "labels": [], "entities": [{"text": "purity score", "start_pos": 17, "end_pos": 29, "type": "METRIC", "confidence": 0.9815588593482971}]}, {"text": "Purity score is where U = {U 1 , U 2 , ..., UK } is the user clustering of documents, G = {G 1 , G 2 , ..., G J } is gold clustering of documents, and N is the total number of documents.", "labels": [], "entities": [{"text": "Purity score", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9479636549949646}]}, {"text": "Moreover, we interpret U k and G j as the set of documents in user cluster UK or gold cluster G j . shows an example of purity calculation fora clustering, given gold labels.", "labels": [], "entities": []}, {"text": "Purity is an external metric for cluster evaluation.", "labels": [], "entities": [{"text": "Purity", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8510183691978455}]}, {"text": "Avery bad labeling has a purity score close to 0 and a perfect labeling has purity score of 1.", "labels": [], "entities": [{"text": "purity score", "start_pos": 25, "end_pos": 37, "type": "METRIC", "confidence": 0.9864553809165955}, {"text": "purity score", "start_pos": 76, "end_pos": 88, "type": "METRIC", "confidence": 0.9904578924179077}]}, {"text": "The higher this score, the higher the quality of user labels.", "labels": [], "entities": [{"text": "quality", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9765704274177551}]}, {"text": "To evaluate TGA, We did a study on two different users.", "labels": [], "entities": [{"text": "TGA", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.5276886820793152}]}, {"text": "For User 1, we chose 15 topics and for User 2, we chose 25 topics.", "labels": [], "entities": []}, {"text": "They were asked to stop labeling whenever they were satisfied with the predicted document labels.", "labels": [], "entities": []}, {"text": "We compare the user study results with a baseline.", "labels": [], "entities": []}, {"text": "Our baseline ignores topic modeling information for choosing documents to labels.", "labels": [], "entities": []}, {"text": "It considers the scenario when users are given a large document collection and are asked to categorize the documents without any other information.", "labels": [], "entities": []}, {"text": "Thus, we show randomly chosen documents to users and want them to apply label to them.", "labels": [], "entities": []}, {"text": "All users can go back and editor delete document labels, or refuse to label a document if they find it confusing.", "labels": [], "entities": []}, {"text": "After each single labeling, we use the same features and classifier that we used for user study with topic models to classify documents.", "labels": [], "entities": []}, {"text": "Then we calculate purity for user labels with respect to gold labels.", "labels": [], "entities": [{"text": "purity", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9959967136383057}]}, {"text": "shows the purity score over different number of labeled documents for User 1, User 2, and baseline.", "labels": [], "entities": [{"text": "purity score", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9821552932262421}]}, {"text": "User 1 did the labeling in 6 rounds, whereas User 2 did total of 7 rounds.", "labels": [], "entities": [{"text": "labeling", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.9713752269744873}]}, {"text": "User 1 ended with 116 labeled documents and user 2 had 42 labeled documents in the end.", "labels": [], "entities": []}, {"text": "User 2 starts with a label set of size 9 and labels 11 documents.", "labels": [], "entities": []}, {"text": "Two documents are labeled as \"wildlife\", other two are labeled as \"tax\", and all other documents have unique labels.", "labels": [], "entities": []}, {"text": "This means that even if there are very few instance per label, baseline is outperformed.", "labels": [], "entities": [{"text": "baseline", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9662268757820129}]}, {"text": "This is an evidence of   choosing informative documents to assign labels with the help of topic models.", "labels": [], "entities": []}, {"text": "On the other hand, User 1 starts with a label set of size 7 and labels 36 documents and is outperformed by baseline significantly.", "labels": [], "entities": []}, {"text": "One reason for this is that assigning too many documents relevant to a topic, with the same label doesn't provide any new information to the classifier and thus the user could get the same purity score with a lower number of labeled documents, which would lead to outperforming baseline.", "labels": [], "entities": [{"text": "purity score", "start_pos": 189, "end_pos": 201, "type": "METRIC", "confidence": 0.9582635462284088}]}, {"text": "User 1 outperforms the baseline in the second (8 labels and 50 labeled documents) and third round (9 labels and 58 labeled documents) slightly.", "labels": [], "entities": []}, {"text": "In the fourth round, user creates more labels.", "labels": [], "entities": []}, {"text": "With total of 13 labels and 82 labeled documents, the gap between user's purity score and baseline gets larger.", "labels": [], "entities": [{"text": "purity score", "start_pos": 73, "end_pos": 85, "type": "METRIC", "confidence": 0.9709497094154358}]}, {"text": "Both users outperform baseline in the final round.", "labels": [], "entities": []}, {"text": "To see how topic models help speedup labeling process, we compare the number of user labeled documents with the approximate number of required labeled documents to get the same purity score in baseline.", "labels": [], "entities": [{"text": "purity score", "start_pos": 177, "end_pos": 189, "type": "METRIC", "confidence": 0.9563497602939606}]}, {"text": "shows the results for User 1 and User 2.", "labels": [], "entities": []}, {"text": "User 1 starts with man labeled documents and baseline can achieve the same performance with one third of the labeled documents.", "labels": [], "entities": []}, {"text": "As the user keeps labeling more documents, the performance improves and baseline needs more labeled documents to get the same level of purity.", "labels": [], "entities": [{"text": "purity", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9634854197502136}]}, {"text": "For User 2, baseline on average needs over two times as many labeled documents to achieve the same purity score as user labels.", "labels": [], "entities": [{"text": "purity score", "start_pos": 99, "end_pos": 111, "type": "METRIC", "confidence": 0.97347691655159}]}, {"text": "These tables indicate that topic models help users choose documents to assign labels to and achieve an acceptable performance with fewer labeled documents.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The number of required labeled documents  for baseline to get the same purity score as (a) User  1 (b) User 2, in each round", "labels": [], "entities": [{"text": "purity score", "start_pos": 81, "end_pos": 93, "type": "METRIC", "confidence": 0.9738157689571381}]}]}