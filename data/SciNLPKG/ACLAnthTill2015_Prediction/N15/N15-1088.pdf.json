{"title": [{"text": "Semantic parsing of speech using grammars learned with weak supervision", "labels": [], "entities": [{"text": "Semantic parsing of speech", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8896727561950684}]}], "abstractContent": [{"text": "Semantic grammars can be applied both as a language model fora speech recognizer and for semantic parsing, e.g. in order to map the output of a speech recognizer into formal meaning representations.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.7623022794723511}]}, {"text": "Semantic speech recognition grammars are, however, typically created manually or learned in a supervised fashion, requiring extensive manual effort in both cases.", "labels": [], "entities": [{"text": "Semantic speech recognition grammars", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.758385568857193}]}, {"text": "Aiming to reduce this effort, in this paper we investigate the induction of semantic speech recognition grammars under weak supervision.", "labels": [], "entities": [{"text": "semantic speech recognition grammars", "start_pos": 76, "end_pos": 112, "type": "TASK", "confidence": 0.6650199964642525}]}, {"text": "We present empirical results, indicating that the induced grammars support semantic parsing of speech with a rather low loss in performance when compared to parsing of input without recognition errors.", "labels": [], "entities": [{"text": "semantic parsing of speech", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.7984154671430588}]}, {"text": "Further , we show improved parsing performance compared to applying n-gram models as language models and demonstrate how our semantic speech recognition grammars can be enhanced by weights based on occurrence frequencies , yielding an improvement in parsing performance over applying unweighted grammars .", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9671956896781921}, {"text": "semantic speech recognition grammars", "start_pos": 125, "end_pos": 161, "type": "TASK", "confidence": 0.751430332660675}, {"text": "parsing", "start_pos": 250, "end_pos": 257, "type": "TASK", "confidence": 0.9616962671279907}]}], "introductionContent": [], "datasetContent": [{"text": "We evaluated the word error rate (WER) as well as parsing accuracy for different language models and combinations thereof.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 17, "end_pos": 38, "type": "METRIC", "confidence": 0.8242314805587133}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9733092784881592}]}, {"text": "In particular, in case of applying recognition grammars we applied these also in combination with an n-gram back off LM.", "labels": [], "entities": []}, {"text": "In particular, the n-gram model was applied in case of utterances which were rejected by the recognizer as out of grammar (OOG), as these might still be parsed subsequently by applying approximate matching.", "labels": [], "entities": []}, {"text": "Notice, however, that for our experiments we did not apply both LMs at a time but combined the output of two recognizers for further processing.", "labels": [], "entities": []}, {"text": "Notice further that most speech recognizers can only be applied using either a recognition grammar or an n-gram model at a time, but one can assume that two recognizers might be configured to run in parallel.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.6504047513008118}]}, {"text": "As mentioned previously, we performed 4-fold cross-validation on the four RoboCup games.", "labels": [], "entities": []}, {"text": "For each fold, learning semantic parsers and creation of language models was performed using the ambiguous written training data for three games and the spoken gold standard for the forth game for testing.", "labels": [], "entities": [{"text": "learning semantic parsers", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.6391721069812775}]}, {"text": "In the following, we will discuss results for applying our induced grammars as an LM compared to using standard trigrams models (solely trained on the indomain data) as a baseline, since these yielded the best results.", "labels": [], "entities": []}, {"text": "In particular, we do not discuss the results achieved by the grammars induced in a purely syntactic manner as they performed worse than semantic grammars in all experiments, and we do not discuss the experiments for the interpolated/adapted trigram models as they performed worse than the in-domain trigram models with the exception of a very slight improvement when applied as aback off model for SLU.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Semantic parsing results on written text and on  speech transcribed using different language models", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.9135417640209198}]}]}