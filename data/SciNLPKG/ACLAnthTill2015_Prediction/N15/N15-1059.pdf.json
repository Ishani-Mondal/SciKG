{"title": [{"text": "NASARI: a Novel Approach to a Semantically-Aware Representation of Items", "labels": [], "entities": [{"text": "NASARI", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7687724232673645}, {"text": "Semantically-Aware Representation of Items", "start_pos": 30, "end_pos": 72, "type": "TASK", "confidence": 0.7636549323797226}]}], "abstractContent": [{"text": "The semantic representation of individual word senses and concepts is of fundamental importance to several applications in Natural Language Processing.", "labels": [], "entities": [{"text": "semantic representation of individual word senses and concepts", "start_pos": 4, "end_pos": 66, "type": "TASK", "confidence": 0.7411506325006485}]}, {"text": "To date, concept modeling techniques have in the main based their representation either on lexicographic resources , such as WordNet, or on encyclope-dic resources, such as Wikipedia.", "labels": [], "entities": [{"text": "concept modeling", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7311089634895325}, {"text": "WordNet", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.955880880355835}]}, {"text": "We propose a vector representation technique that combines the complementary knowledge of both these types of resource.", "labels": [], "entities": []}, {"text": "Thanks to its use of explicit semantics combined with a novel cluster-based dimensionality reduction and an effective weighting scheme, our representation attains state-of-the-art performance on multiple datasets in two standard benchmarks: word similarity and sense clustering.", "labels": [], "entities": [{"text": "sense clustering", "start_pos": 261, "end_pos": 277, "type": "TASK", "confidence": 0.6898589581251144}]}, {"text": "We are releasing our vector representations at http://lcl.uniroma1.it/nasari/.", "labels": [], "entities": []}], "introductionContent": [{"text": "Obtaining accurate semantic representations of individual word senses or concepts is vital for several applications in Natural Language Processing (NLP) such as, for example, Word Sense Disambiguation, Entity Linking (, semantic similarity), Information Extraction (, and resource linking and integration.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 175, "end_pos": 200, "type": "TASK", "confidence": 0.6975071032842001}, {"text": "Entity Linking", "start_pos": 202, "end_pos": 216, "type": "TASK", "confidence": 0.7381941676139832}, {"text": "semantic similarity)", "start_pos": 220, "end_pos": 240, "type": "TASK", "confidence": 0.7603340446949005}, {"text": "Information Extraction", "start_pos": 242, "end_pos": 264, "type": "TASK", "confidence": 0.8507824838161469}, {"text": "resource linking and integration", "start_pos": 272, "end_pos": 304, "type": "TASK", "confidence": 0.6959844157099724}]}, {"text": "One prominent semantic representation approach is the distributional semantic model, which represents lexical items as vectors in a semantic space.", "labels": [], "entities": []}, {"text": "The weights in these vectors were traditionally computed on the basis of co-occurrence statistics (, whereas for the more recent generation of distributional models weight computation is viewed as a context prediction problem, often to be solved by using neural networks.", "labels": [], "entities": [{"text": "context prediction", "start_pos": 199, "end_pos": 217, "type": "TASK", "confidence": 0.71664759516716}]}, {"text": "Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector.", "labels": [], "entities": []}, {"text": "Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained from various lexical resources.", "labels": [], "entities": [{"text": "sense modeling", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.8201654851436615}]}, {"text": "However, these techniques mainly utilize the knowledge derived from either WordNet () or Wikipedia (), which are, respectively, the most widely-used lexicographic and encyclopedic resources in lexical semantics (.", "labels": [], "entities": []}, {"text": "This restriction to a single resource brings about two main limitations: (1) the sense modeling does not benefit from the complementary knowledge of different resources, and (2) the obtained representations are resource-specific and cannot be used across settings.", "labels": [], "entities": [{"text": "sense modeling", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.7455205619335175}]}, {"text": "In this paper we put forward a novel concept representation technique, called NASARI, which exploits the knowledge available in both types of resource in order to obtain effective representations of arbitrary concepts.", "labels": [], "entities": [{"text": "NASARI", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.6246535181999207}]}, {"text": "The contributions of this paper are threefold.", "labels": [], "entities": []}, {"text": "First, we propose a novel technique for rich semantic representation of arbitrary WordNet synsets or Wikipedia pages.", "labels": [], "entities": [{"text": "rich semantic representation of arbitrary WordNet synsets or Wikipedia pages", "start_pos": 40, "end_pos": 116, "type": "TASK", "confidence": 0.5821488440036774}]}, {"text": "Second, we provide improvements over the conventional tf-idf weighting scheme by applying lexical specificity, a statistical measure mainly used for term extraction, to the task of computing vector weights in a vector representation.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 149, "end_pos": 164, "type": "TASK", "confidence": 0.7151636034250259}]}, {"text": "Third, we propose a semantically-aware dimensionality reduction technique that transforms a lexical item's representation from a semantic space of words to one of WordNet synsets, simultaneously providing an implicit disambiguation and a distribution smoothing.", "labels": [], "entities": [{"text": "semantically-aware dimensionality reduction", "start_pos": 20, "end_pos": 63, "type": "TASK", "confidence": 0.6516448259353638}]}, {"text": "We demonstrate that our representation achieves stateof-the-art performance on two different tasks: (1) word similarity on multiple standard datasets: MC-30, RG-65, and WordSim-353 similarity, and (2) Wikipedia sense clustering, in which our unsupervised system surpasses the performance of a stateof-the-art supervised technique that exploits knowledge available in Wikipedia in several languages.", "labels": [], "entities": [{"text": "Wikipedia sense clustering", "start_pos": 201, "end_pos": 227, "type": "TASK", "confidence": 0.5363422532876333}]}], "datasetContent": [{"text": "We evaluated NASARI on two different tasks that require the computation of semantic similarity between words or concepts: word similarity (Section 4.1) and sense clustering (Section 4.2).", "labels": [], "entities": [{"text": "NASARI", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.8383808135986328}, {"text": "sense clustering", "start_pos": 156, "end_pos": 172, "type": "TASK", "confidence": 0.6956191509962082}]}, {"text": "For the sense clustering task, we take as our benchmark the two datasets created by.", "labels": [], "entities": [{"text": "sense clustering", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.8332956731319427}]}, {"text": "In these datasets, clustering has been viewed as a binary classification problem in which all possible pairings of senses of a word are annotated whether they ought to be clustered or not.", "labels": [], "entities": []}, {"text": "The first dataset contains 500 pairs, 357 of which are set to clustered and the remaining 143 to not clustered.", "labels": [], "entities": []}, {"text": "The second dataset, referred to as the SemEval dataset, is based on a set of highly ambiguous words taken from SemEval evaluations) and consists of 925 pairs, 162 of which are positively labeled, i.e., clustered.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.7587820291519165}]}, {"text": "In this task we use the procedure explained in Section 3.1 for measuring the similarity of concepts.", "labels": [], "entities": []}, {"text": "A pair of pages is set to belong to the same cluster if their similarity exceeds the middle point in our similarity scale, i.e., 0.5 in the scale of.", "labels": [], "entities": []}, {"text": "We compare our results with the state-of-the-art systems of that perform clustering by exploiting the structure and content of an English page (monolingual variant), or several pages in different languages (multilingual variant that uses English, German, Spanish and Italian pages).", "labels": [], "entities": []}, {"text": "These systems are essentially multi-feature Support Vector Machine classifiers that use an automaticallylabeled dataset for their training.", "labels": [], "entities": []}, {"text": "lists the results of NASARI as well as the state-of-the-art systems of.", "labels": [], "entities": [{"text": "NASARI", "start_pos": 21, "end_pos": 27, "type": "DATASET", "confidence": 0.9201164841651917}]}, {"text": "We also report the results fora baseline system that sets all pairs as not clustered.", "labels": [], "entities": []}, {"text": "As can be seen from the table, our system proves to be highly robust and competitive by outperforming, in an unsupervised setting, the supervised monolingual and multilingual systems of on both datasets.", "labels": [], "entities": []}, {"text": "As regards the F1, we obtain 72.0% and 64.2% on the 500-pair and SemEval datasets, respectively, a measure that is not reported by.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9971789121627808}, {"text": "SemEval datasets", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.8618322312831879}]}], "tableCaptions": [{"text": " Table 1: Pearson correlation of different similarity mea- sures on RG-65, MC-30, and WordSim-353 similarity  (WS-Sim) datasets. Results for Lin and ESA on RG-65  and MC-30 are taken from (Hassan and Mihalcea, 2011).  We show the best performance obtained by Baroni et al.  (2014) out of 48 configurations specifically tested on RG- 65 (highlighted by ) and across different datasets includ- ing WS-Sim (highlighted by  \u2021).", "labels": [], "entities": [{"text": "similarity mea- sures", "start_pos": 43, "end_pos": 64, "type": "METRIC", "confidence": 0.7174753770232201}, {"text": "WordSim-353 similarity  (WS-Sim) datasets", "start_pos": 86, "end_pos": 127, "type": "DATASET", "confidence": 0.6653021176656088}]}, {"text": " Table 2: Accuracy of different systems on two manually- annotated English datasets for sense clustering in  Wikipedia. Dan-mono and Dan-multi are the monolin- gual and multilingual systems of Dandala et al. (2013).", "labels": [], "entities": [{"text": "sense clustering", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.7187611758708954}]}, {"text": " Table 3: Performance of NASARI and its individual vector representations for different weight computation schemes,  i.e., lexical specificity and tf-idf, and for different vector comparison techniques, i.e., cosine and Weighted Overlap  (WO), in terms of Pearson correlation (word similarity) and accuracy (sense clustering). The scores highlighted by  are the ones obtained using our default NASARI setting, and the ones highlighted by  \u2020 correspond to the setting of our  system using Wikipedia as its only knowledge source.", "labels": [], "entities": [{"text": "NASARI", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.8868381977081299}, {"text": "Pearson correlation", "start_pos": 256, "end_pos": 275, "type": "METRIC", "confidence": 0.9358782172203064}, {"text": "accuracy", "start_pos": 298, "end_pos": 306, "type": "METRIC", "confidence": 0.99907386302948}]}]}