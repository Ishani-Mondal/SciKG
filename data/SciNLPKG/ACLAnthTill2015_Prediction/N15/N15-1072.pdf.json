{"title": [], "abstractContent": [{"text": "Crowdsourcing makes it possible to create translations at much lower cost than hiring professional translators.", "labels": [], "entities": []}, {"text": "However, it is still expensive to obtain the millions of translations that are needed to train statistical machine translation systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 95, "end_pos": 126, "type": "TASK", "confidence": 0.6394406060377756}]}, {"text": "We propose two mechanisms to reduce the cost of crowdsourc-ing while maintaining high translation quality.", "labels": [], "entities": []}, {"text": "First, we develop a method to reduce redundant translations.", "labels": [], "entities": []}, {"text": "We train a linear model to evaluate the translation quality on a sentence-by-sentence basis, and fit a threshold between acceptable and unacceptable translations.", "labels": [], "entities": []}, {"text": "Unlike past work, which always paid fora fixed number of translations for each source sentence and then chose the best from them, we can stop earlier and pay less when we receive a translation that is good enough.", "labels": [], "entities": []}, {"text": "Second, we introduce a method to reduce the pool of translators by quickly identifying bad translators after they have translated only a few sentences.", "labels": [], "entities": []}, {"text": "This also allows us to rank translators, so that we re-hire only good translators to reduce cost.", "labels": [], "entities": []}], "introductionContent": [{"text": "Crowdsourcing is a promising new mechanism for collecting large volumes of annotated data at low cost.", "labels": [], "entities": []}, {"text": "Many NLP researchers have started creating speech and language data through crowdsourcing (for example,, and others).", "labels": [], "entities": []}, {"text": "One NLP application that has been the focus of crowdsourced data collection is statistical machine translation which requires large bilingual sentence-aligned parallel corpora to train translation models.", "labels": [], "entities": [{"text": "crowdsourced data collection", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.7121744354565939}, {"text": "statistical machine translation", "start_pos": 79, "end_pos": 110, "type": "TASK", "confidence": 0.6784273087978363}]}, {"text": "Crowdsourcing's low cost has made it possible to hire people to create sufficient volumes of translation in order to train SMT systems (for example,,,,).", "labels": [], "entities": [{"text": "SMT", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.9883970618247986}]}, {"text": "However, crowdsourcing is not perfect, and one of its most pressing challenges is how to ensure the quality of the data that is created by it.", "labels": [], "entities": []}, {"text": "Unlike in more traditional employment scenarios, where annotators are pre-vetted and their skills are clear, in crowdsourcing very little is known about the annotators.", "labels": [], "entities": []}, {"text": "They are not professional translators, and there are no built-in mechanisms for testing their language skills.", "labels": [], "entities": []}, {"text": "They complete tasks without any oversight.", "labels": [], "entities": []}, {"text": "Thus, translations produced via crowdousrcing maybe low quality.", "labels": [], "entities": []}, {"text": "Previous work has addressed this problem, showing that non-professional translators hired on Amazon Mechanical Turk (MTurk) can achieve professional-level quality, by soliciting multiple translations of each source sentence and then choosing the best translation).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk)", "start_pos": 93, "end_pos": 123, "type": "DATASET", "confidence": 0.894091377655665}]}, {"text": "In this paper we focus on a different aspect of crowdsourcing than.", "labels": [], "entities": []}, {"text": "We attempt to achieve the same high quality while minimizing the associated costs.", "labels": [], "entities": []}, {"text": "We propose two complementary methods: (1) We reduce the number of translations that we solicit for each source sentence.", "labels": [], "entities": []}, {"text": "Instead of soliciting a fixed number of translations for each foreign sentence, we stop soliciting translations after we get an acceptable one.", "labels": [], "entities": []}, {"text": "We do so by building models to distinguish between acceptable translations and unacceptable ones.", "labels": [], "entities": []}, {"text": "We reduce the number of workers we hire, and retain only high quality translators by quickly identifying and filtering out workers who produce low quality translations.", "labels": [], "entities": []}, {"text": "Our work stands in contrast with Zaidan and Callison-Burch (2011) who always solicited and paid fora fixed number of translations for each source sentence, and who had no model of annotator quality.", "labels": [], "entities": []}, {"text": "In this paper we demonstrate that: \u2022 Our model can predict whether a given translation is acceptable with high accuracy, substantially reducing the number of redundant translations needed for every source segment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9945191740989685}]}, {"text": "\u2022 Translators can be ranked well even when observing only small amounts of data.", "labels": [], "entities": []}, {"text": "Compared with a gold standard ranking, we achieve a correlation of 0.94 after seeing the translations of only 20 sentences from each worker.", "labels": [], "entities": []}, {"text": "Therefore, bad workers can be filtered out quickly.", "labels": [], "entities": []}, {"text": "\u2022 We can achieve a similar BLEU score as Zaidan and Callison-Burch (2011) at half the cost using our cost optimizing methods.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.979208379983902}]}], "datasetContent": [{"text": "After we rank workers, we keep top-ranked workers and select the best translation only from their translations.", "labels": [], "entities": []}, {"text": "For both ranking approaches, we vary the number of good workers that we retain.", "labels": [], "entities": []}, {"text": "We report both rankings' correlation with the gold standard ranking.", "labels": [], "entities": []}, {"text": "Since the top worker threshold is varied and since we change the value of kin first k sentence ranking, we have a different test set in different settings.", "labels": [], "entities": []}, {"text": "Each test set excludes any items which were used to rank the workers, or which did not have any translations from the top workers according to our rankings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The relationship between \u03b4 (the allowable devia- tion from the expected upper bound on BLEU score), the  BLEU score for translations selected by models from par- tial sets and the average number of translation candidates  set for each source sentence (# Trans).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9794688820838928}, {"text": "BLEU score", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.9807897210121155}]}, {"text": " Table 2: Pearson Correlations for calibration data in dif- ferent proportion. The percentage column shows what  proportion of the whole data set is used for calibration.", "labels": [], "entities": []}, {"text": " Table 3: Correlation (\u03c1) and translation quality for the  various features used by our model. Translation quality is  computed by selecting best translations based on model- predicted ranking for workers (rank) and model-predicted  scores for translations (score). Here we do not filter out  bad workers when selecting the best translation.", "labels": [], "entities": [{"text": "Correlation (\u03c1)", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9202236980199814}]}, {"text": " Table 4: A comparison of the translation quality when we  retain the top translators under different rankings. The  rankings shown are random, the model's ranking (using  all features from Table 3) and the gold ranking. \u2206 is the  difference between the BLEU scores for the gold ranking  and the model ranking. # Trans is the average number of  translations needed for each source sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 254, "end_pos": 258, "type": "METRIC", "confidence": 0.9989416003227234}]}]}