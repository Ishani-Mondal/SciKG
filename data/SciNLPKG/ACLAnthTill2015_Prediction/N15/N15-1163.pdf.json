{"title": [], "abstractContent": [{"text": "A common approach to dependency parsing is scoring a parse via a linear function of a set of indicator features.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8401088714599609}]}, {"text": "These features are typically manually constructed from templates that are applied to parts of the parse tree.", "labels": [], "entities": []}, {"text": "The templates define which properties of apart should combine to create features.", "labels": [], "entities": []}, {"text": "Existing approaches consider only a small subset of the possible combinations, due to statistical and computational efficiency considerations.", "labels": [], "entities": []}, {"text": "In this work we present a novel kernel which facilitates efficient parsing with feature representations corresponding to a much larger set of combinations.", "labels": [], "entities": []}, {"text": "We integrate the kernel into a parse reranking system and demonstrate its effectiveness on four languages from the CoNLL-X shared task.", "labels": [], "entities": [{"text": "CoNLL-X shared task", "start_pos": 115, "end_pos": 134, "type": "DATASET", "confidence": 0.7506504456202189}]}], "introductionContent": [{"text": "Dependency parsing is the task of labeling a sentence x with a syntactic dependency tree y \u2208 Y (x), where Y (x) denotes the space of valid trees over x.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8776606917381287}]}, {"text": "Each word in x is represented as a list of linguistic properties (e.g. word form, part of speech, base form, gender, number, etc.).", "labels": [], "entities": []}, {"text": "In the graph based approach () parsing is cast as a structured linear prediction problem: where \u03a6 (x, y) \u2208 Rd is a feature representation defined over a sentence and its parse tree, and v \u2208 Rd is a vector of parameters.", "labels": [], "entities": []}, {"text": "To construct an effective representation, \u03a6 (x, y) is typically decomposed into local representations See https://bitbucket.org/hillel/templatekernels for implementation.", "labels": [], "entities": []}, {"text": "over parts p of the tree y: Standard decompositions include different types of parts: arcs, sibling arcs, grandparent arcs, etc.", "labels": [], "entities": []}, {"text": "Feature templates are then applied to the parts to construct the local representations.", "labels": [], "entities": []}, {"text": "The templates determine how the linguistic properties of the words in each part should combine to create features (see Section 2).", "labels": [], "entities": []}, {"text": "Substantial effort has been dedicated to the manual construction of feature templates.", "labels": [], "entities": []}, {"text": "Still, for both computational and statistical reasons, existing templates consider only a small subset of the possible combinations of properties.", "labels": [], "entities": []}, {"text": "From a computational perspective, solving Eq.", "labels": [], "entities": []}, {"text": "1 involves applying the templates toy and calculating a dot product in the effective dimension of \u03a6.", "labels": [], "entities": []}, {"text": "The use of many templates thus quickly leads to computational infeasibility (the dimensionality of v, as well as the number of non-zero features in \u03a6, become very large).", "labels": [], "entities": []}, {"text": "From a statistical perspective, the use of a large number of feature templates can lead to overfitting.", "labels": [], "entities": []}, {"text": "Several recent works have proposed solutions to the above problem.", "labels": [], "entities": []}, {"text": "represented the space of all possible property combinations in an arc-factored model as a third order tensor and learned the parameter matrix for the tensor under a low rank assumption.", "labels": [], "entities": []}, {"text": "In the context of transition parsers, have implemented a neural network that uses dense representations of words and parts of speech as its input and implicitly considers combinations in its inner layers.", "labels": [], "entities": []}, {"text": "Earlier work on transition-based dependency parsing used SVM classifiers with 2nd order polynomial kernels to achieve similar effects (Hall: Feature template over the second order consecutive siblings part type.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 16, "end_pos": 51, "type": "TASK", "confidence": 0.6305759946505228}]}, {"text": "The part type contains slots for the head (h), sibling (s) and modifier (m) words, as well as for the two edges (e1 and e2).", "labels": [], "entities": []}, {"text": "Each slot is associated with a set of properties.", "labels": [], "entities": []}, {"text": "The directed path skips over the edge properties and defines the partial template <h-cpos=?; s-cpos=?; m-gender=?>.", "labels": [], "entities": []}, {"text": "While training greedy transition-based parsers such as the ones used in and) amounts to training a multiclass classifier, the graph-based parsing framework explored in (  and in the present work is a more involved structured-learning task.", "labels": [], "entities": []}, {"text": "In this paper we present a kernel based approach to automated feature generation in the context of graph-based parsing.", "labels": [], "entities": [{"text": "automated feature generation", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.700425406297048}]}, {"text": "Compared to tensors and neural networks, kernel methods have the attractive properties of a convex objective and well understood generalization bounds).", "labels": [], "entities": []}, {"text": "We introduce a kernel that allows us to learn the parameters fora representation similar to the tensor representation in ( ) but without the low rank assumption, and without explicitly instantiating the exponentially many possible features.", "labels": [], "entities": []}, {"text": "In contrast to previous works on parsing with kernels (), in which the kernels are defined over trees and count the number of shared subtrees, our focus is on feature combinations.", "labels": [], "entities": []}, {"text": "In that sense our work is more closely related to work on tree kernels for relation extraction (), but the kernel we propose is designed to generate combinations of properties within selected part types and does not involve the all-subtrees representation.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7569388747215271}]}], "datasetContent": [{"text": "Datasets We test our system on 4 languages from the CoNLL 2006 shared task, all with rich morphological features.", "labels": [], "entities": [{"text": "CoNLL 2006 shared task", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.9131780564785004}]}, {"text": "The properties provided for each word in these datasets are its form, part of speech (pos), coarse part of speech (cpos), lemma and morph features (number, gender, person, etc. around 10-20 feats in total).", "labels": [], "entities": []}, {"text": "We use 20-fold jackknifing to create the k-best lists for the reranker).", "labels": [], "entities": []}, {"text": "Base Parser The base parser used in experiments was the sampling parser of , augmented to produce the k-best trees encountered during sampling.", "labels": [], "entities": []}, {"text": "The parser was set to use feature templates over third order part types, but its tensor component and global templates were deactivated.", "labels": [], "entities": []}, {"text": "Features The manual features \u03a6 g were based on first to third order templates from . For the kernel features \u03a6 k we annotated the nodes and edges in each tree with the properties in.", "labels": [], "entities": []}, {"text": "We used a first order template kernel to train a model using all the the possible combinations of head, edge and modifier properties.", "labels": [], "entities": []}, {"text": "Our kernel also produces all the property combinations of the head and modifier words (disregarding the edge properties).", "labels": [], "entities": []}, {"text": "Results For each language we train a Kernel Reranker by running Alg 1 for 10 iterations over the training set, using k-best lists of size 25 and C set to infinity.", "labels": [], "entities": []}, {"text": "As baseline, we train a Base Reranker in the same setup but with kernel features turned off.", "labels": [], "entities": []}, {"text": "shows the results for the two systems.", "labels": [], "entities": []}, {"text": "Even though they use the same feature set, the base-reranker lags behind the base-parser.", "labels": [], "entities": []}, {"text": "We attribute this to the fact that the reranker explores a much smaller fraction of the search space, and that the gold parse tree may not be available to it in either train or test time.", "labels": [], "entities": []}, {"text": "However, the kernel-reranker significantly improves over the base-reranker.", "labels": [], "entities": []}, {"text": "In Bulgarian and Danish, the kernel-reranker outperforms the baseparser.", "labels": [], "entities": []}, {"text": "This is not the case for Slovene and Arabic, which we attribute to the low oracle accuracy of the k-best lists in these languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9810725450515747}]}, {"text": "As is common in reranking), our final system incorporates the scores assigned to sentences by the base parser: i.e. score final (x, y) = \u03b2score base (x, y) + score reranker (x, y).", "labels": [], "entities": []}, {"text": "\u03b2 is tuned per language on a development set.", "labels": [], "entities": []}, {"text": "8 Our final system outperforms the base parser, as well as TurboParser (), a parser based on manually constructed feature templates over up to third order parts.", "labels": [], "entities": []}, {"text": "The system lags slightly behind the sampling parser of  which additionally uses global features (not used by our system) and a tensor component for property combinations.", "labels": [], "entities": []}, {"text": "Another important difference between the systems is that our search is severely restricted by the use of a reranker.", "labels": [], "entities": []}, {"text": "It is likely that using our kernel in a graph-based parser will further improve its reason we did not select the English treebank.", "labels": [], "entities": [{"text": "English treebank", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.9008212685585022}]}, {"text": "8 To obtain a development set we further split the reranker training sets into tuning training and a development sets (90/10).", "labels": [], "entities": []}, {"text": "We then tune \u03b2 per language on the respective development sets by selecting the best value from a list of {0, 0.05, . .", "labels": [], "entities": []}, {"text": "Performance lists the performance metrics of our system on the four evaluation treebanks.", "labels": [], "entities": []}, {"text": "While training times are reasonable even for large datasets, the increase in support size causes prediction to become slow for medium and large training sets.", "labels": [], "entities": []}, {"text": "The number of support instances is a general problem with kernel methods.", "labels": [], "entities": []}, {"text": "It has been addressed using techniques like feature maps) and bounded online algorithms).", "labels": [], "entities": []}, {"text": "The application of these techniques to template kernels is a topic for future research.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: System Performance (UAS excluding punctua- tion). TurboParser is (Martins et al., 2013), Zhang et al.  is (Zhang et al., 2014)", "labels": [], "entities": []}, {"text": " Table 3: Runtime statistics, measured on a standard Mac- book Pro 2.8 GHz Core i7 using 8 threads.", "labels": [], "entities": [{"text": "Mac- book Pro 2.8 GHz Core i7", "start_pos": 53, "end_pos": 82, "type": "DATASET", "confidence": 0.8891966417431831}]}]}