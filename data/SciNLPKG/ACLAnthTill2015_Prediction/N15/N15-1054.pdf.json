{"title": [{"text": "Inferring Missing Entity Type Instances for Knowledge Base Completion: New Dataset and Methods", "labels": [], "entities": [{"text": "Inferring Missing Entity Type Instances", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8500722289085388}, {"text": "Knowledge Base Completion", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.5612103343009949}]}], "abstractContent": [{"text": "Most of previous work in knowledge base (KB) completion has focused on the problem of relation extraction.", "labels": [], "entities": [{"text": "knowledge base (KB) completion", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.6597538938124975}, {"text": "relation extraction", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.8865643739700317}]}, {"text": "In this work, we focus on the task of inferring missing entity type instances in a KB, a fundamental task for KB competition yet receives little attention.", "labels": [], "entities": [{"text": "inferring missing entity type instances in a KB", "start_pos": 38, "end_pos": 85, "type": "TASK", "confidence": 0.7518356367945671}]}, {"text": "Due to the novelty of this task, we construct a large-scale dataset and design an automatic evaluation methodology.", "labels": [], "entities": []}, {"text": "Our knowledge base completion method uses information within the existing KB and external information from Wikipedia.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.615971028804779}, {"text": "KB", "start_pos": 74, "end_pos": 76, "type": "DATASET", "confidence": 0.9236549139022827}]}, {"text": "We show that individual methods trained with a global objective that considers unobserved cells from both the entity and the type side gives consistently higher quality predictions compared to baseline methods.", "labels": [], "entities": []}, {"text": "We also perform manual evaluation on a small subset of the data to verify the effectiveness of our knowledge base completion methods and the correctness of our proposed automatic evaluation method.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is now increasing interest in the construction of knowledge bases like Freebase () and NELL () in the natural language processing community.", "labels": [], "entities": []}, {"text": "KBs contain facts such as Tiger Woods is an athlete, and Barack Obama is the president of USA.", "labels": [], "entities": [{"text": "USA", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.9751173853874207}]}, {"text": "However, one of the main drawbacks in existing KBs is that they are incomplete and are missing important facts (West et * Most of the research conducted during summer internship at al., 2014), jeopardizing their usefulness in downstream tasks such as question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 251, "end_pos": 269, "type": "TASK", "confidence": 0.8984272480010986}]}, {"text": "This has led to the task of completing the knowledge base entries, or Knowledge Base Completion (KBC) extremely important.", "labels": [], "entities": []}, {"text": "In this paper, we address an important subproblem of knowledge base completion-inferring missing entity type instances.", "labels": [], "entities": [{"text": "knowledge base completion-inferring missing entity type instances", "start_pos": 53, "end_pos": 118, "type": "TASK", "confidence": 0.8162691720894405}]}, {"text": "Most of previous work in KB completion has only focused on the problem of relation extraction (.", "labels": [], "entities": [{"text": "KB completion", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.9565170407295227}, {"text": "relation extraction", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.8445366621017456}]}, {"text": "Entity type information is crucial in KBs and is widely used in many NLP tasks such as relation extraction ), coreference resolution (, entity linking), semantic parsing () and question answering ().", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8105684518814087}, {"text": "coreference resolution", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.9304154813289642}, {"text": "entity linking", "start_pos": 136, "end_pos": 150, "type": "TASK", "confidence": 0.7186840176582336}, {"text": "semantic parsing", "start_pos": 153, "end_pos": 169, "type": "TASK", "confidence": 0.7495817542076111}, {"text": "question answering", "start_pos": 177, "end_pos": 195, "type": "TASK", "confidence": 0.9028519988059998}]}, {"text": "For example, adding entity type information improves relation extraction by 3% ( ) and entity linking by 4.2 F1 points (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8297653496265411}, {"text": "entity linking", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.7464815974235535}, {"text": "F1", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9923642873764038}]}, {"text": "Despite their importance, there is surprisingly little previous work on this problem and, there are no datasets publicly available for evaluation.", "labels": [], "entities": []}, {"text": "We construct a large-scale dataset for the task of inferring missing entity type instances in a KB.", "labels": [], "entities": []}, {"text": "Most of previous KBC datasets () are constructed using a single snapshot of the KB and methods are evaluated on a subset of facts that are hidden during training.", "labels": [], "entities": [{"text": "KBC datasets", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.8249562382698059}]}, {"text": "Hence, the methods could be potentially evaluated by their ability to predict easy facts that the KB already contains.", "labels": [], "entities": []}, {"text": "Moreover, the methods are not directly evaluated: Freebase description of Jean Metellus can be used to infer that the entity has the type /book/author.", "labels": [], "entities": [{"text": "Freebase description of Jean Metellus", "start_pos": 50, "end_pos": 87, "type": "DATASET", "confidence": 0.8187696099281311}]}, {"text": "This missing fact is found by our algorithm and is still missing in the latest version of Freebase when the paper is written.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.9779806137084961}]}, {"text": "on their ability to predict missing facts.", "labels": [], "entities": [{"text": "predict missing facts", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.8814377586046854}]}, {"text": "To overcome these drawbacks we construct the train and test data using two snapshots of the KB and evaluate the methods on predicting facts that are added to the more recent snapshot, enabling a more realistic and challenging evaluation.", "labels": [], "entities": []}, {"text": "Standard evaluation metrics for KBC methods are generally type-based (, measuring the quality of the predictions by aggregating scores computed within a type.", "labels": [], "entities": []}, {"text": "This is not ideal because: (1) it treats every entity type equally not considering the distribution of types, (2) it does not measure the ability of the methods to rank predictions across types.", "labels": [], "entities": []}, {"text": "Therefore, we additionally use a global evaluation metric, where the quality of predictions is measured within and across types, and also accounts for the high variance in type distribution.", "labels": [], "entities": []}, {"text": "In our experiments, we show that models trained with negative examples from the entity side perform better on type-based metrics, while when trained with negative examples from the type side perform better on the global metric.", "labels": [], "entities": []}, {"text": "In order to design methods that can rank predictions both within and across entity (or relation) types, we propose a global objective to train the models.", "labels": [], "entities": []}, {"text": "Our proposed method combines the advantages of previous approaches by using negative examples from both the entity and the type side.", "labels": [], "entities": []}, {"text": "When considering the same number of negative examples, we find that the linear classifiers and the low-dimensional embedding models trained with the global objective produce better quality ranking within and across entity types when compared to training with negatives examples only from entity or type side.", "labels": [], "entities": []}, {"text": "Additionally compared to prior methods, the model trained on the proposed global objective can more reliably suggest confident entity-type pair candidates that could be added into the given knowledge base.", "labels": [], "entities": []}, {"text": "Our contributions are summarized as follows: \u2022 We develop an evaluation framework comprising of methods for dataset construction and evaluation metrics to evaluate KBC approaches for missing entity type instances.", "labels": [], "entities": [{"text": "dataset construction", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.7600672245025635}]}, {"text": "The dataset and evaluation scripts are publicly available at http://research.", "labels": [], "entities": []}, {"text": "\u2022 We propose a global training objective for KBC methods.", "labels": [], "entities": []}, {"text": "The experimental results show that both linear classifiers and low-dimensional embedding models achieve best overall performance when trained with the global objective function.", "labels": [], "entities": []}, {"text": "\u2022 We conduct extensive studies on models for inferring missing type instances studying the impact of using various features and models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we propose an evaluation methodology for the task of inferring missing entity type instances in a KB.", "labels": [], "entities": []}, {"text": "While we focus on recovering entity types, the proposed framework can be easily adapted to relation extraction as well.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.866857498884201}]}, {"text": "First, we discuss our two-snapshot dataset construction strategy.", "labels": [], "entities": []}, {"text": "Then we motivate the importance of evaluating KBC algorithms globally and describe the evaluation metrics we employ.", "labels": [], "entities": []}, {"text": "Mean average precision (MAP) () is now commonly used to evaluate KB completion methods (.", "labels": [], "entities": [{"text": "Mean average precision (MAP)", "start_pos": 0, "end_pos": 28, "type": "METRIC", "confidence": 0.9491986533006033}, {"text": "KB completion", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.6716675460338593}]}, {"text": "MAP is defined as the mean of average precision overall entity (or relation) types.", "labels": [], "entities": []}, {"text": "MAP treats each entity type equally (not explicitly accounting for their distribution).", "labels": [], "entities": []}, {"text": "However, some types occur much more frequently than others.", "labels": [], "entities": []}, {"text": "For example, in our large-scale experiment with 500 entity types, there are many entity types with only 5 instances in the test set while the most frequent entity type has tens of thousands of missing instances.", "labels": [], "entities": []}, {"text": "Moreover, MAP only measures the ability of the methods to correctly rank predictions within a type.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7117293477058411}]}, {"text": "To account for the high variance in the distribution of entity types and measure the ability of the methods to correctly rank predictions across types we use global average precision (GAP) (similarly to micro-F1) as an additional evaluation metric for KB completion.", "labels": [], "entities": [{"text": "global average precision (GAP)", "start_pos": 158, "end_pos": 188, "type": "METRIC", "confidence": 0.8450384239355723}, {"text": "KB completion", "start_pos": 252, "end_pos": 265, "type": "TASK", "confidence": 0.8555917739868164}]}, {"text": "We convert the multi-label classification problem to a binary classification problem where the label of an entity and type pair is true if the entity has that type in Freebase and false otherwise.", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.7613006830215454}]}, {"text": "GAP is the average precision of this transformed problem which can measure the ability of the methods to rank predictions both within and across entity types.", "labels": [], "entities": [{"text": "GAP", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.6073429584503174}, {"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9771459102630615}]}, {"text": "Prior to us, use mean reciprocal rank as a global evaluation metric fora KBC task.", "labels": [], "entities": []}, {"text": "We use average precision instead of mean reciprocal rank since MRR could be biased to the top predictions of the method While GAP captures global ordering, it would be beneficial to measure the quality of the top k predictions of the model for bootstrapping and active learning scenarios ().", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.806503415107727}]}, {"text": "We report G@k, GAP measured on the top k predictions (similarly to Precision@k and Hits@k).", "labels": [], "entities": [{"text": "G", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9651820063591003}, {"text": "GAP", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9657678008079529}]}, {"text": "This metric can be reliably used to measure the overall quality of the top k predictions.", "labels": [], "entities": []}, {"text": "In this section, we give details about our dataset and discuss our experimental results.", "labels": [], "entities": []}, {"text": "Finally, we perform manual evaluation on a small subset of the data.", "labels": [], "entities": []}, {"text": "Adagrad Vs DCD We first study the linear models by comparing Linear.DCD and Linear.AdaGrad.", "labels": [], "entities": [{"text": "Adagrad Vs DCD", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8634901841481527}]}, {"text": "shows that Linear.AdaGrad consistently performs better for our task.", "labels": [], "entities": []}, {"text": "Impact of Features We compare the effect of different features on the final performance using Linear.AdaGrad in.", "labels": [], "entities": []}, {"text": "Types are represented by boolean features while Freebase description and Wikipedia full text are represented using tfidf weighting.", "labels": [], "entities": []}, {"text": "The best MAP results are obtained by using all the information (T+D+W) while best GAP results are obtained by using the Freebase description and Wikipedia article of the entity.", "labels": [], "entities": [{"text": "GAP", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.6728627681732178}]}, {"text": "Note that the features are simply concatenated when multiple resources are used.", "labels": [], "entities": []}, {"text": "We tried to use idf weighting on type features and on all features, but they did not yield improvements.", "labels": [], "entities": []}, {"text": "The Importance of Global Objective and 2d compares global training objective with NE and NT training objective.", "labels": [], "entities": [{"text": "Importance", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9820055961608887}, {"text": "2d", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9524782299995422}]}, {"text": "Note that all the three methods use the same number of negative examples.", "labels": [], "entities": []}, {"text": "More precisely, for each (e, t) \u2208 \u039b 0 , |N E (e, t)| + |N T (e, t)| = m + n = 2.", "labels": [], "entities": []}, {"text": "The results show that the global training objective achieves best scores on both MAP and GAP for classifiers and lowdimensional embedding models.", "labels": [], "entities": [{"text": "MAP", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.7673830389976501}, {"text": "GAP", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.6392001509666443}]}, {"text": "Among NE and NT, NE performs better on the type-based metric while NT performs better on the global metric.", "labels": [], "entities": []}, {"text": "Linear Model Vs Embedding Model Finally, we compare the linear classifier model with the embedding model in.", "labels": [], "entities": []}, {"text": "The linear classifier model performs better than the embedding model in both MAP and GAP.", "labels": [], "entities": []}, {"text": "We perform large-scale evaluation on 500 types with the description features (as experiments are expensive) and the results are shown in  One might expect that with the increased number of types, the embedding model would perform better than the classifier since they share parameters across types.", "labels": [], "entities": []}, {"text": "However, despite the recent popularity of embedding models in NLP, linear model still performs better in our task.", "labels": [], "entities": []}, {"text": "To verify the effectiveness of our KBC algorithms, and the correctness of our automatic evaluation method, we perform manual evaluation on the top 100 predictions of the output obtained from two different experimental setting and the results are shown in: Manual vs. Automatic evaluation of top 100 predictions on 70 types.", "labels": [], "entities": []}, {"text": "Predictions are obtained by training a linear classifier using Adagrad with global training objective (m=1, n=1).", "labels": [], "entities": [{"text": "Adagrad", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9208813309669495}]}, {"text": "G@100-M and Accuracy-M are computed by manual evaluation.", "labels": [], "entities": [{"text": "G", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9501574039459229}, {"text": "Accuracy-M", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9992762207984924}]}], "tableCaptions": [{"text": " Table 1: Statistics of our dataset. \u039b 0 is our training snap- shot and \u039b is our test snapshot. An example is an entity- type pair.", "labels": [], "entities": []}, {"text": " Table 2: Automatic Evaluation Results. Note that m = |N E (e, t)| and n = |N T (e, t)|.", "labels": [], "entities": []}, {"text": " Table 3: Manual vs. Automatic evaluation of top 100 pre- dictions on 70 types. Predictions are obtained by train- ing a linear classifier using Adagrad with global training  objective (m=1, n=1). G@100-M and Accuracy-M are  computed by manual evaluation.", "labels": [], "entities": [{"text": "Accuracy-M", "start_pos": 209, "end_pos": 219, "type": "METRIC", "confidence": 0.9971188306808472}]}]}