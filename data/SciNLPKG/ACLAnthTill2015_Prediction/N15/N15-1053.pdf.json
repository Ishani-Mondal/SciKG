{"title": [{"text": "D\u00e9j\u00e0 Image-Captions: A Corpus of Expressive Descriptions in Repetition", "labels": [], "entities": [{"text": "Repetition", "start_pos": 60, "end_pos": 70, "type": "TASK", "confidence": 0.7297038435935974}]}], "abstractContent": [{"text": "We present anew approach to harvesting a large-scale, high quality image-caption corpus that makes a better use of already existing web data with no additional human efforts.", "labels": [], "entities": []}, {"text": "The key idea is to focus on D\u00e9j\u00e0 Image-Captions: naturally existing image descriptions that are repeated almost verbatim-by more than one individual for different images.", "labels": [], "entities": []}, {"text": "The resulting corpus provides association structure between 4 million images with 180K unique captions, capturing a rich spectrum of everyday narratives including figurative and pragmatic language.", "labels": [], "entities": []}, {"text": "Exploring the use of the new corpus, we also present new conceptual tasks of visually situated paraphrasing, creative image cap-tioning, and creative visual paraphrasing.", "labels": [], "entities": []}], "introductionContent": [{"text": "The use of multimodal web data has been a recurring theme in many recent studies integrating language and vision, e.g., image captioning (), text-based image retrieval (, and entry-level categorization (.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 120, "end_pos": 136, "type": "TASK", "confidence": 0.7179270535707474}, {"text": "text-based image retrieval", "start_pos": 141, "end_pos": 167, "type": "TASK", "confidence": 0.6220886905988058}]}, {"text": "However, much research integrating complex textual descriptions to date has been based on datasets that rely on substantial human curation or annotation (), rather than using the web data in the wild as is ().", "labels": [], "entities": []}, {"text": "The need for human curation limits the potential scale of the multimodal dataset.", "labels": [], "entities": []}, {"text": "Without human curation, however, the web data introduces significant noise.", "labels": [], "entities": []}, {"text": "In particular, everyday captions often contain extraneous information that is not directly relevant to what the image shows (.", "labels": [], "entities": []}, {"text": "In this paper, we present anew approach to harvesting a large-scale, high quality image-caption corpus that makes a better use of already existing web data with no additional human efforts.", "labels": [], "entities": []}, {"text": "shows sample captions in the resulting corpus, e.g., \"butterfly resting on a flower\" and \"evening walk along the beach\".", "labels": [], "entities": []}, {"text": "Notably, some of these are figurative, e.g., \"rippled sky\" and \"sun is going to bed.\"", "labels": [], "entities": []}, {"text": "The key idea is to focus on D\u00e9j\u00e0 Image-Captions, i.e., naturally existing image captions that are repeated almost verbatim by more than one individual for different images.", "labels": [], "entities": []}, {"text": "The hypothesis is that such captions represent common visual content across multiple images, hence are more likely to be free of unwanted extraneous information (e.g., specific names, time, or any other personal information) and better represent visual concepts.", "labels": [], "entities": []}, {"text": "A surprising aspect of our study is that such a strict data filtration scheme can still result in a large-scale corpus; sifting through 760 million image-caption pairs, we harvest as many as 4 million image-caption pairs with 180K unique captions.", "labels": [], "entities": []}, {"text": "The resulting corpus, D\u00e9j\u00e0 Image Captions, provides several unique properties that complement human-curated or crowd-sourced datasets.", "labels": [], "entities": [{"text": "D\u00e9j\u00e0 Image Captions", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.6695241530736288}]}, {"text": "First, as our approach is fully automated, it can be readily applied to harvesting anew dataset from the ever changing multimodal web data.", "labels": [], "entities": []}, {"text": "Indeed, a recent internet report estimates that billions of new photographs are being uploaded daily.", "labels": [], "entities": []}, {"text": "In contrast, human-annotated datasets are costly to scale to different domains.", "labels": [], "entities": []}, {"text": "Second, datasets that are harvested from the web Butterflies are self propelled flowers butterfly resting on a flower After the sun has set (9) Sun is going to bed can you spot the butterfly (88) The sky looks like it is on fire (58) The sun sets for another day Evening walk along the beach Chillaxing at the beach (20) Walk by the beach (557) Rippled sky (44) In the sky (1013): The image-caption association graph of D\u00e9j\u00e0 Image-Captions.", "labels": [], "entities": [{"text": "D\u00e9j\u00e0 Image-Captions", "start_pos": 420, "end_pos": 439, "type": "DATASET", "confidence": 0.7924795746803284}]}, {"text": "Solid lines represent original captions and dotted lines represent paraphrase captions.", "labels": [], "entities": []}, {"text": "This corpus reflects a rich spectrum of everyday narratives people use in online activities including figurative language (e.g., \"Sun is going to bed\"), casual language (e.g., Chillaxing at the beach\"), and conversational language (e.g., \"Can you spot the butterfly\").", "labels": [], "entities": []}, {"text": "The numbers in the parenthesis show the cardinality of images associated with each caption.", "labels": [], "entities": []}, {"text": "Surprisingly, some of these descriptions are highly expressive, almost creative, and yet not unique -as all these captions are repeated almost verbatim by different individuals describing different images.", "labels": [], "entities": []}, {"text": "can complement those based on prompted human annotations.", "labels": [], "entities": []}, {"text": "The latter in general are literal and mechanical readings of the visual scenes, while the former reflect a rich spectrum of natural language utterances in everyday narratives, including figurative, pragmatic, and conversational language, e.g., \"can you spot the butterfly\").", "labels": [], "entities": []}, {"text": "Therefore, this dataset offers unique opportunities for grounding figurative and metaphoric expressions using visual context.", "labels": [], "entities": []}, {"text": "In conjunction with the new corpus, publicly shared at http://www.cs.stonybrook.", "labels": [], "entities": []}, {"text": "edu/ \u02dc jianchen/deja.html, we also present three new tasks: visually situated paraphrases ( \u00a75); creative image captioning ( \u00a77), and creative visual paraphrasing ( \u00a77).", "labels": [], "entities": [{"text": "creative image captioning", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.6305289566516876}, {"text": "creative visual paraphrasing", "start_pos": 134, "end_pos": 162, "type": "TASK", "confidence": 0.6883031924565634}]}, {"text": "The central algorithm component in addressing all these tasks is a simple and yet effective approach to image caption transfer that exploits the unique association structure of the resulting corpus ( \u00a73).", "labels": [], "entities": [{"text": "image caption transfer", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.8154291411240896}]}, {"text": "Our empirical results collectively demonstrate that when the web data is available at such scale, it is possible to obtain a large-scale, high-quality dataset with significantly less noise.", "labels": [], "entities": []}, {"text": "We hope that our approach would be only one of the first attempts, and inspire future research to develop better ways of making use of ever-growing multimodal web data.", "labels": [], "entities": []}, {"text": "Although it is unlikely that the automatically gathered datasets can completely replace the curated descriptions written in a controlled setting, our hope is to find ways to complement human annotated datasets in terms of both the scale and also the diversity of the domain and language.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "First we describe the dataset collection procedure and insights ( \u00a72).", "labels": [], "entities": [{"text": "dataset collection", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.6398753970861435}]}, {"text": "We then present anew approach to image caption transfer based on the association structure of the corpus ( \u00a73) followed by experimental results ( \u00a74).", "labels": [], "entities": [{"text": "image caption transfer", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.8625402450561523}]}, {"text": "After then we present new conceptual tasks: visual paraphrasing ( \u00a75), creative image captioning, and creative visual paraphrasing ( \u00a77), interleaved with corresponding experimental results ( \u00a76, \u00a78).", "labels": [], "entities": [{"text": "creative image captioning", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.6078436573346456}]}], "datasetContent": [{"text": "Our corpus consists of three components MAIN SET The first step is to crawl as many image-caption pairs as possible.", "labels": [], "entities": [{"text": "MAIN SET", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.6327405869960785}]}, {"text": "We use flickr.com search API to crawl 760 million pairs in total.", "labels": [], "entities": []}, {"text": "The API allows searching images within a given time window, which enables exhaustive search over anytime span.", "labels": [], "entities": []}, {"text": "To ensure visual correspondence between images and captions, we set query terms using 693 most frequent nouns from the dataset of, and systematically slide time windows over the year 2013.", "labels": [], "entities": []}, {"text": "1 For each image, we segment its title and the first line of its description into sentences.", "labels": [], "entities": []}, {"text": "The crawled dataset at this point includes a lot of noise in the captions.", "labels": [], "entities": []}, {"text": "Hence we apply initial filtering rules to reduce the noise.", "labels": [], "entities": []}, {"text": "We retain only those image-sentence pairs in which the sentence contains the query noun, and does not contain personal information indicators such as first-person pronouns.", "labels": [], "entities": []}, {"text": "We  want captions that are more than simple keywords, thus we discard trivial captions that do not include at least one verb, preposition, or adjective.", "labels": [], "entities": []}, {"text": "The next step is to find captions in repetition.", "labels": [], "entities": []}, {"text": "For this purpose, we transform captions into canonical forms.", "labels": [], "entities": []}, {"text": "We lemmatize all words, convert prepositions to a special token \"IN\" 2 , and discard function words, numbers, and punctuations.", "labels": [], "entities": []}, {"text": "For instance, \"The bird flies in blue sky\" and \"A bird flying into the blue sky\" have the same canonical form, \"bird fly IN blue sky\".", "labels": [], "entities": []}, {"text": "We then retain only those captions that are repeated with respect to their canonical forms by more than one user, and for distinctly different images to ensure the generality of the captions.", "labels": [], "entities": []}, {"text": "Retaining only captions that are repeated verbatim may seem overly restrictive.", "labels": [], "entities": []}, {"text": "Nonetheless, because we start with as many as 760 million pairs, this procedure yields nearly 180K unique captions associated with nearly 4M images.", "labels": [], "entities": []}, {"text": "What is more surprising, as will be shown later, is that many of these captions are highly expressive.", "labels": [], "entities": []}, {"text": "shows the distribution of the number of images associated with each caption.", "labels": [], "entities": []}, {"text": "The median and mean are 10 and 22.4 respectively, showing a high degree of connectivities between captions and images.", "labels": [], "entities": [{"text": "mean", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9713663458824158}]}, {"text": "There(is  The early bird gets the worm   and images.", "labels": [], "entities": []}, {"text": "To extend these relations to many-tomany, we introduce visually-situated paraphrases (or visual paraphrases for shorthand) ( \u00a75).", "labels": [], "entities": []}, {"text": "A visual paraphrase relation is a triple (i, c, p), where image i has an original caption c, caption p is the visual paraphrase for c situated in image i.", "labels": [], "entities": []}, {"text": "We collect visual paraphrases for sample images in our dataset, using both crowd sourcing (7,570 triples) and an automatic algorithm (353,560 triples) (see \u00a75 for details).", "labels": [], "entities": [{"text": "crowd sourcing", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.6635640114545822}]}, {"text": "Formally, our corpus represents a bipartite graph G = (T, V, E), in which the set of captions T and the set of images V are connected by typed edges e(c, i, t), where caption c \u2208 T , image i \u2208 V , and edge type t \u2208 {original, paraphrase}, which denotes whether the image-caption association is given by the original caption or by a visual paraphrase.", "labels": [], "entities": []}, {"text": "FIGURATIVE SET We find that many repeating captions are surprisingly lengthy and expressive, most of which turnout to be idiomatic expressions and quotations, e.g., \"faith is the bird that feels the light when the dawn is still dark\" from Tagore's poem.", "labels": [], "entities": [{"text": "FIGURATIVE", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.5477637052536011}, {"text": "SET", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.6574499607086182}]}, {"text": "We lookup goodreads.com   and brainyquotes.com to identify 6K quotation captions illustrated by 180K images.", "labels": [], "entities": [{"text": "goodreads.com", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.9364265203475952}, {"text": "quotation captions", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.863273561000824}]}, {"text": "We also present a manual labeling on a small subset of the data to provide better insights into the degree and types of figurative speech used in natural captions.", "labels": [], "entities": []}, {"text": "Using these labels we build a classifier ( \u00a77) to further detect 18K figurative captions associated with 410K images.", "labels": [], "entities": []}, {"text": "INSIGHTS As additional insights into the dataset, shows statistics of the visual content, Table 5 shows syntactic types of the captions, and Table 4 shows positive and negative sentiment in captions.", "labels": [], "entities": [{"text": "INSIGHTS", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.5343123078346252}]}, {"text": "The experimental configuration basically follows \u00a74.", "labels": [], "entities": []}, {"text": "We compare ASSOC para , the visual-paraphrase augmented approach, to the vanilla ASSOC approach.", "labels": [], "entities": [{"text": "ASSOC", "start_pos": 11, "end_pos": 16, "type": "TASK", "confidence": 0.8779377937316895}]}, {"text": "The image feature setting is the one with which the ASSOC approach performs best.", "labels": [], "entities": [{"text": "ASSOC", "start_pos": 52, "end_pos": 57, "type": "TASK", "confidence": 0.9647095799446106}]}, {"text": "Both approaches use the GIST+Tinyimage feature to prepare candidate captions, then use either the GIST or Tinyimage feature for reranking.", "labels": [], "entities": [{"text": "GIST", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.9177154302597046}]}, {"text": "shows that the ASSOC para approach significantly improves the vanilla ASSOC method under both automatic and human evaluation.", "labels": [], "entities": [{"text": "ASSOC", "start_pos": 15, "end_pos": 20, "type": "TASK", "confidence": 0.9569911360740662}]}, {"text": "As a reference, the first row shows the performance of the INSTANCE method ( \u00a74).", "labels": [], "entities": [{"text": "INSTANCE", "start_pos": 59, "end_pos": 67, "type": "TASK", "confidence": 0.45408377051353455}]}, {"text": "The ASSOC method significantly improves over the INSTANCE method.", "labels": [], "entities": [{"text": "ASSOC", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.7030125260353088}]}, {"text": "On a similar vein, the ASSOC para method further improves over the ASSOC method, as automatic paraphrases provide a better visual neighborhood.", "labels": [], "entities": [{"text": "ASSOC para", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.6737402677536011}]}, {"text": "This improvement is remarkable since the paraphrasing association is added automatically without any supervised training.", "labels": [], "entities": []}, {"text": "This demonstrates the usefulness of the bipartite association structure of our corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Percentiles of the image count associated with", "labels": [], "entities": [{"text": "Percentiles", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9395731687545776}]}, {"text": " Table 4: Distribution of caption sentiment. The polar-", "labels": [], "entities": [{"text": "Distribution of caption sentiment", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.8366362154483795}]}, {"text": " Table 5: Statistics on the syntactic composition of cap-", "labels": [], "entities": [{"text": "syntactic composition of cap-", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.5595363020896912}]}, {"text": " Table 7: Human evaluation for image captioning: the", "labels": [], "entities": [{"text": "image captioning", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7585574686527252}]}, {"text": " Table 8: Automatic and human evaluation of exploit-", "labels": [], "entities": []}, {"text": " Table 10: Human eval for creative visual paraphrasing", "labels": [], "entities": [{"text": "creative visual paraphrasing", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.7199262976646423}]}]}