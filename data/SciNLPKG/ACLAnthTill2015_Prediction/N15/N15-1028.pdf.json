{"title": [{"text": "Deep Multilingual Correlation for Improved Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings have been found useful for many NLP tasks, including part-of-speech tagging, named entity recognition, and parsing.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.7202079147100449}, {"text": "named entity recognition", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.6657189627488455}]}, {"text": "Adding multilingual context when learning embeddings can improve their quality, for example via canonical correlation analysis (CCA) on embeddings from two languages.", "labels": [], "entities": [{"text": "canonical correlation analysis (CCA)", "start_pos": 96, "end_pos": 132, "type": "TASK", "confidence": 0.7243675043185552}]}, {"text": "In this paper, we extend this idea to learn deep non-linear transformations of word embed-dings of the two languages, using the recently proposed deep canonical correlation analysis.", "labels": [], "entities": []}, {"text": "The resulting embeddings, when evaluated on multiple word and bigram similarity tasks, consistently improve over monolin-gual embeddings and over embeddings transformed with linear CCA.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learned word representations are widely used in NLP tasks such as tagging, named entity recognition, and parsing).", "labels": [], "entities": [{"text": "tagging", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.9611189961433411}, {"text": "named entity recognition", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.5808221201101939}]}, {"text": "The idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space.", "labels": [], "entities": []}, {"text": "Continuous representations are learned with neural language models () or spectral methods).", "labels": [], "entities": []}, {"text": "The context used to learn these representations is typically the set of nearby words of each word occurrence.", "labels": [], "entities": []}, {"text": "Prior work has found that adding translational context results in better representations (.", "labels": [], "entities": []}, {"text": "Recently, applied canonical correlation analysis (CCA) to word embeddings of two languages, and found that the resulting embeddings represent word similarities better than the original monolingual embeddings.", "labels": [], "entities": [{"text": "canonical correlation analysis (CCA)", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.7222259442011515}]}, {"text": "In this paper, we follow the same intuition as but rather than learning linear transformations with CCA, we permit the correlated information to lie in nonlinear subspaces of the original embeddings.", "labels": [], "entities": []}, {"text": "We use the recently proposed deep canonical correlation analysis (DCCA) technique of to learn nonlinear transformations of two languages' embeddings that are highly correlated.", "labels": [], "entities": [{"text": "deep canonical correlation analysis (DCCA)", "start_pos": 29, "end_pos": 71, "type": "TASK", "confidence": 0.7765594124794006}]}, {"text": "We evaluate our DCCA-transformed embeddings on word similarity tasks like and, and also on the bigram similarity task of (using additive composition), obtaining consistent improvements over the original embeddings and over linear CCA.", "labels": [], "entities": []}, {"text": "We also compare tuning criteria and ensemble methods for these architectures.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use English and German as our two languages.", "labels": [], "entities": []}, {"text": "Our original monolingual word vectors are the same as those used by.", "labels": [], "entities": []}, {"text": "They are 640-dimensional and are estimated via latent semantic analysis on the WMT 2011 monolingual news corpora.", "labels": [], "entities": [{"text": "WMT 2011 monolingual news corpora", "start_pos": 79, "end_pos": 112, "type": "DATASET", "confidence": 0.9575656652450562}]}, {"text": "We use German-English translation pairs as the input to CCA and DCCA, using the same set of 36K pairs as used by Faruqui and Dyer.", "labels": [], "entities": [{"text": "CCA", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.9139094948768616}, {"text": "DCCA", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.7692088484764099}]}, {"text": "These pairs contain, for each of 36K English word types, the single most frequently aligned German word.", "labels": [], "entities": []}, {"text": "They were obtained using the word aligner in cdec () run on the WMT06-10 news commentary corpora and Europarl.", "labels": [], "entities": [{"text": "WMT06-10 news commentary corpora", "start_pos": 64, "end_pos": 96, "type": "DATASET", "confidence": 0.9820382297039032}, {"text": "Europarl", "start_pos": 101, "end_pos": 109, "type": "DATASET", "confidence": 0.5106565356254578}]}, {"text": "After training, we apply the learned CCA/DCCA projection mappings to the original English word embeddings (180K words) and use these transformed embeddings for our evaluation tasks.", "labels": [], "entities": [{"text": "CCA/DCCA projection mappings", "start_pos": 37, "end_pos": 65, "type": "DATASET", "confidence": 0.7634854674339294}]}, {"text": "We compare our DCCA-based embeddings to the original word vectors and to CCA-based em-beddings on several tasks.", "labels": [], "entities": []}, {"text": "We use WordSim-353 (), which contains 353 English word pairs with human similarity ratings.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.95790034532547}]}, {"text": "It is divided into WS-SIM and WS-REL by to measure similarity and relatedness.", "labels": [], "entities": [{"text": "WS-SIM", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.7804361581802368}, {"text": "WS-REL", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.7917446494102478}, {"text": "similarity", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9416950345039368}]}, {"text": "We also use SimLex-999 (), anew similarity-focused dataset consisting of 666 noun pairs, 222 verb pairs, and 111 adjective pairs.", "labels": [], "entities": []}, {"text": "Finally, we use the bigram similarity dataset from Mitchell and Lapata (2010) which has 3 subsets, adjective-noun (AN), noun-noun (NN), and verbobject (VN), and dev and test sets for each.", "labels": [], "entities": [{"text": "bigram similarity dataset from Mitchell and Lapata (2010)", "start_pos": 20, "end_pos": 77, "type": "DATASET", "confidence": 0.7985092967748642}]}, {"text": "For the bigram task, we simply add the word vectors output by CCA or DCCA to get bigram vectors.", "labels": [], "entities": []}, {"text": "All task datasets contain pairs with human similarity ratings.", "labels": [], "entities": []}, {"text": "To evaluate embeddings, we compute cosine similarity between the two vectors in each pair, order the pairs by similarity, and compute Spearman's correlation (\u03c1) between the model's ranking and human ranking.", "labels": [], "entities": [{"text": "Spearman's correlation (\u03c1)", "start_pos": 134, "end_pos": 160, "type": "METRIC", "confidence": 0.8220843076705933}]}], "tableCaptions": [{"text": " Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.", "labels": [], "entities": [{"text": "word and bigram similarity tasks", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.6570457279682159}, {"text": "NN results", "start_pos": 283, "end_pos": 293, "type": "DATASET", "confidence": 0.8004069328308105}]}, {"text": " Table 2: Bigram results, tuned on bigram dev sets.", "labels": [], "entities": []}]}