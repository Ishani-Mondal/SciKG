{"title": [{"text": "Diamonds in the Rough: Event Extraction from Imperfect Microblog Data", "labels": [], "entities": [{"text": "Event Extraction", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.6465430855751038}, {"text": "Imperfect Microblog Data", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.8024371862411499}]}], "abstractContent": [{"text": "We introduce a distantly supervised event extraction approach that extracts complex event templates from microblogs.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7435502409934998}]}, {"text": "We show that this near real-time data source is more challenging than news because it contains information that is both approximate (e.g., with values that are close but different from the gold truth) and ambiguous (due to the brevity of the texts), impacting both the evaluation and extraction methods.", "labels": [], "entities": []}, {"text": "For the former, we propose a novel, \"soft\", F1 metric that incorporates similarity between extracted fillers and the gold truth, giving partial credit to different but similar values.", "labels": [], "entities": [{"text": "F1", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.997771680355072}]}, {"text": "With respect to extraction methodology , we propose two extensions to the distant supervision paradigm: to address approximate information, we allow positive training examples to be generated from information that is similar but not identical to gold values; to address ambiguity, we aggregate contexts across tweets discussing the same event.", "labels": [], "entities": []}, {"text": "We evaluate our contributions on the complex domain of earthquakes, with events with up to 20 arguments.", "labels": [], "entities": []}, {"text": "Our results indicate that, despite their simplicity, our contributions yield a statistically-significant improvement of 33% (relative) over a strong distantly-supervised system.", "labels": [], "entities": []}, {"text": "The dataset containing the knowledge base, relevant tweets and manual annotations is publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Twitter is an excellent source of near real-time data on recent events, motivating the need for information extraction (IE) systems that operate on tweets rather than traditional news articles.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 96, "end_pos": 123, "type": "TASK", "confidence": 0.8474730014801025}]}, {"text": "However, using this data comes with its own challenges: tweets tend to use colloquial speech, noisy syntax and discourse, and, more importantly, the information reported is often inaccurate (e.g., reporting a different but similar magnitude for an earthquake) and ambiguous (e.g., reporting multiple potential earthquake locations, with insufficient context to guess which is the correct one).", "labels": [], "entities": []}, {"text": "The top rows in show examples of these problems for an actual event in our dataset on earthquakes.", "labels": [], "entities": []}, {"text": "This comes in contrast with \"traditional\" IE work on newswire documents, where information is considerably more accurate than microblog material, and none of the above observations hold (.", "labels": [], "entities": [{"text": "IE", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9795263409614563}]}, {"text": "As an example of the benefits of event extraction from a near real-time social-media resource, the last row in lists a motivating example, where our system extracts the correct depth of an earthquake from the text tweeted by the U.S. Geological Survey, which is novel information that is missing in our manually-curated knowledge base.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7382392883300781}]}, {"text": "In this work we take a classic event extraction (EE) task, where events are defined by templates containing a predefined set of arguments, and implement it using data from Twitter.", "labels": [], "entities": [{"text": "event extraction (EE) task", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.8839705487092336}]}, {"text": "We avoid the prohibitive cost of manual annotation through distant supervision (DS): we automatically generate train-Earthquake in Honduras.", "labels": [], "entities": []}, {"text": "So strong it Approximate strong it was felt in Guatemala information as well.: Challenges and opportunities for event extraction from Twitter.", "labels": [], "entities": [{"text": "Approximate", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9892885684967041}, {"text": "event extraction", "start_pos": 112, "end_pos": 128, "type": "TASK", "confidence": 0.745190292596817}]}, {"text": "The first row shows a tweet with approximate information (in bold); the correct magnitude is 7.3 (cf.).", "labels": [], "entities": []}, {"text": "The second row shows a first tweet with ambiguous information, which leads our baseline model to extract the incorrect country (in bold; correct country is Peru).", "labels": [], "entities": []}, {"text": "The following two tweets help disambiguate the context.", "labels": [], "entities": []}, {"text": "The last row shows a tweet containing information (in bold) that is missing in the knowledge base.", "labels": [], "entities": []}, {"text": "ing data by aligning a knowledge base of known event instances with tweets (), which is then used to train a supervised extraction model (sequence tagger in our case).", "labels": [], "entities": []}, {"text": "In seminal work on event extraction,) applied DS to both detect tweets about local events and then extracted values about two arguments (artist and venue).", "labels": [], "entities": [{"text": "event extraction", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.712088942527771}]}, {"text": "In our work, we work on automatically selected tweets, and scale the task to complex events with a large number of arguments.", "labels": [], "entities": []}, {"text": "We focus on the domain of earthquakes, where each event has up to 20 arguments.", "labels": [], "entities": []}, {"text": "The contributions of this work are the following: 1.", "labels": [], "entities": []}, {"text": "To our knowledge, this is one of the first works that analyzes the problem of distantly supervised extraction of complex events with many arguments from microblogs.", "labels": [], "entities": []}, {"text": "2. Our analysis shows (Section 3) that the biggest barrier is that information on Twitter can be inaccurate (containing approximately correct event argument values) and ambiguous (with insufficient context for accurate extraction).", "labels": [], "entities": []}, {"text": "The top two blocks in show an example of each.", "labels": [], "entities": []}, {"text": "These challenges impact both evaluation and system development.", "labels": [], "entities": []}, {"text": "3. The analysis also highlights the need to adapt evaluation metrics to approximately correct information, which may appear both in text and in the knowledge base itself.", "labels": [], "entities": []}, {"text": "For example, fora particular earthquake, the USGS reports a depth of 22 km., while NOAA reports 25 km 2 . We propose anew evaluation metric that gives partial credit to extracted argument values based on their similarity to existing values in the knowledge base.", "labels": [], "entities": []}, {"text": "4. We introduce two simple strategies that address the above barriers for system development: approximate matching, which addresses inaccurate values by allowing the distant supervision process to map values from the knowledge base to text even when they do not match exactly; and feature aggregation, which responds to small, ambiguous contexts by aggregating information across multiple tweets for the same event.", "labels": [], "entities": [{"text": "approximate matching", "start_pos": 94, "end_pos": 114, "type": "TASK", "confidence": 0.7060647457838058}]}, {"text": "For example, the first strategy considers the 7.1 magnitude in the first tweet in Table 1 as a training example because it is close to the value in the knowledge base (7.3).", "labels": [], "entities": []}, {"text": "The second strategy classifies all instances of Peru jointly using a single set of features, extracted from all available tweets for the corresponding earthquake.", "labels": [], "entities": []}, {"text": "For example, this feature set contains three values for the feature previous-word (:, rocks, and in).", "labels": [], "entities": []}, {"text": "Each approach yields 19% relative improvement, 33% in combination.", "labels": [], "entities": []}, {"text": "5. We release a public dataset containing a knowledge base of earthquake instances and corresponding tweets for each earthquake 3 .", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we detail the creation of the knowledge base of earthquake events, the collection process for potentially-relevant tweets, and, lastly, our distant supervision framework, which serves as a platform for our contributions (Sections 5 and 6).", "labels": [], "entities": []}, {"text": "The knowledge base (KB) was created from the list of globally significant earthquakes during the 21st century, as reported by Wikipedia.", "labels": [], "entities": []}, {"text": "We se-   Where necessary, argument values were normalized.", "labels": [], "entities": []}, {"text": "See fora summary and an example.", "labels": [], "entities": []}, {"text": "We used the Topsy API 6 to search for tweets that are potentially relevant for each earthquake.", "labels": [], "entities": []}, {"text": "We formed a query using the word \"earthquake\" plus the location, encoded as a disjunction of city, region, and country arguments.", "labels": [], "entities": []}, {"text": "We retrieved tweets from the day before the date and time of the earthquake, up to seven days after.", "labels": [], "entities": []}, {"text": "This procedure might also retrieve tweets about aftershocks, which we consider to be different events.", "labels": [], "entities": []}, {"text": "We applied an aggressive method to discard aftershock tweets: we only kept  We sorted the list of earthquakes in the KB chronologically, and chose the earliest 75% of the earthquakes as the training dataset, and the most recent (25%) for testing.", "labels": [], "entities": []}, {"text": "The training set contained 81 earthquakes and their corresponding 6078 tweets, while the testing set contained 27 earthquakes and 1763 tweets.", "labels": [], "entities": []}, {"text": "All development experiments were performed using 5-fold cross-validation over the training partition, where the folds were organized randomly by earthquake.", "labels": [], "entities": []}, {"text": "Each fold contained tweets for around 15 earthquakes, but the number of tweets varied widely, with one fold having 585 tweets and another 2229.", "labels": [], "entities": []}, {"text": "The evaluation compares the argument values induced by our system with those in the gold KB, and computes precision, recall and F1 using the official scorer from the Knowledge Base Population (KBP) Slot Filling (SF) shared task.", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9984096884727478}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9995468258857727}, {"text": "F1", "start_pos": 128, "end_pos": 130, "type": "METRIC", "confidence": 0.999541163444519}]}, {"text": "We also incorporated the notion of equivalence classes proposed in the SF task.", "labels": [], "entities": [{"text": "SF task", "start_pos": 71, "end_pos": 78, "type": "TASK", "confidence": 0.7967552840709686}]}, {"text": "For instance, if the system predicted Guerrero State for the argument region, when the KB contains just Guerrero, we consider this result correct because the two strings are equivalent in this context.", "labels": [], "entities": []}, {"text": "Our equivalence classes also include countries, regions, and cities with hashtags, unnormalized temporal expressions, etc.", "labels": [], "entities": []}, {"text": "Where applicable, we checked statistical significance of performance differences using the bootstrap resampling technique proposed in, in which we draw many simulated test sets by sampling with replacement from the set of earthquakes in the test partition.", "labels": [], "entities": []}, {"text": "The previous analysis suggests that traditional evaluation measures unnecessarily penalize arguments containing values that do not match the gold truth exactly.", "labels": [], "entities": []}, {"text": "Rather than giving no credit when predicted values are different from gold ones, we devised a simple extension to the KBP evaluation measures that take into account the similarity between the values of system and gold arguments, where the similarity depends on the type of each slot (cf..", "labels": [], "entities": []}, {"text": "For numeric values, we use the following formula, where x is the predicted value, and g the gold value: For example, given a gold value of 7.3, a system value of 7.2 would have a similarity of 0.98, and a system value of 14.6 or larger would have a similarity 0.", "labels": [], "entities": []}, {"text": "If both values are equal, similarity is 1.", "labels": [], "entities": [{"text": "similarity", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9991356730461121}]}, {"text": "For the other slot types, the similarity function is discrete, with values set to 1 (proposed slot is correct) or 0 (incorrect) as follows.", "labels": [], "entities": []}, {"text": "We consider a proposed temporal argument as correct if it is within a span of 5 minutes of the corresponding gold temporal value.", "labels": [], "entities": []}, {"text": "Durations are judged as correct if they are within 10 seconds of the gold values.", "labels": [], "entities": [{"text": "Durations", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9640527963638306}]}, {"text": "We considered proposed dates as correct if they differ by at most one day from the gold date.", "labels": [], "entities": []}, {"text": "For location arguments, we use GeoNames 13 to obtain the coordinates of the locations produced by the system that do not match the information in the KB.", "labels": [], "entities": []}, {"text": "Based on the average size of countries, regions, and cities, we consider these additional locations as correct if they are at the following distance (or closer) from the gold locations: 500 kms for countries, 50 kms for regions, and 10 kms for cities.", "labels": [], "entities": []}, {"text": "The original KBP scorer increases the value of True Positives (TP) by 1 every time a predicted argument matches its gold value.", "labels": [], "entities": [{"text": "True Positives (TP)", "start_pos": 47, "end_pos": 66, "type": "METRIC", "confidence": 0.7769177198410034}]}, {"text": "In the proposed lenient scorer, TP is increased by the similarity between the predicted and gold values.", "labels": [], "entities": [{"text": "TP", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9990594983100891}]}, {"text": "The precision and recall will be thus calculated as follows (SYS for number of predicted argument values, GOLD for number of gold argument values): The right block in lists the results under this lenient evaluation for the experiment initially reported in the left block in the same table.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995405673980713}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9996398687362671}, {"text": "SYS", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.992051362991333}, {"text": "GOLD", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9989651441574097}]}, {"text": "As expected, these results are higher than the ones using the strict measure, but maintain the relative order of the systems in each of the evaluation measures.", "labels": [], "entities": []}, {"text": "The difference in precision between DS-CRF and MA-CRF decreases, indicating that the new measure assigns partial credit to the larger amount of argument values extracted by MA-CRF.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9994208812713623}]}, {"text": "The difference in re- These thresholds might change in other domains, but adjusting these values is trivial.", "labels": [], "entities": [{"text": "re- These thresholds", "start_pos": 18, "end_pos": 38, "type": "METRIC", "confidence": 0.9746163636445999}]}, {"text": "We address this in the next section.", "labels": [], "entities": []}, {"text": "Designing relevant measures for lenient evaluations, such as the one discussed here, is an open research issue.", "labels": [], "entities": []}, {"text": "For example, the method proposed in Section 4 gives partial credit to all reported (positive) numeric values in the interval, where g is the correct value for the corresponding slot (see the equation in Section 4).", "labels": [], "entities": []}, {"text": "But other, stricter, measures are certainly possible.", "labels": [], "entities": []}, {"text": "For example, one stricter variant of our proposed measure would assign partial credit only for predicted values that have a similarity of 0.95 or higher with the gold truth (inline with our approximate DS training process).", "labels": [], "entities": []}, {"text": "For example, for the same gold numeric value g, the measure assigns partial credit only for predicted values in the interval [0.95g, 1.05g].", "labels": [], "entities": []}, {"text": "We repeated the experiments in using this alternate evaluation measure.", "labels": [], "entities": []}, {"text": "The result are summarized in.", "labels": [], "entities": []}, {"text": "The results reported in do not alter the findings of the paper.", "labels": [], "entities": []}, {"text": "In fact, under this stricter evaluation measure, our results are stronger: DS comb -CRF, which combines both our ideas, approaches with nearly 1 F1 point MA-CRF, which trains on manually annotated data.", "labels": [], "entities": [{"text": "DS comb -CRF", "start_pos": 75, "end_pos": 87, "type": "METRIC", "confidence": 0.8936314433813095}, {"text": "F1 point MA-CRF", "start_pos": 145, "end_pos": 160, "type": "METRIC", "confidence": 0.8010172446568807}]}], "tableCaptions": [{"text": " Table 1: Challenges and opportunities for event extrac- tion from Twitter. The first row shows a tweet with ap- proximate information (in bold); the correct magnitude is  7.3 (cf.", "labels": [], "entities": []}, {"text": " Table 2: Event arguments and types in the earthquake do- main (first and second column), summary statistics for  the knowledge base, i.e., the gold truth (third column),  and values for one example earthquake (4th column). (*)  indicates multi-valued arguments (all other are single- valued). The two rightmost columns give statistics for  the number of mentions in the tweets per argument, as  obtained through manual annotation (MA) or distant su- pervision (DS) (cf. Section 2.4). The argument types are  the following: D date, T time, L location, N numeric, and  B boolean.", "labels": [], "entities": [{"text": "distant su- pervision (DS)", "start_pos": 439, "end_pos": 465, "type": "METRIC", "confidence": 0.6356453171798161}]}, {"text": " Table 3: Development: Results for the distant supervision  system (DS-CRF). We also include results for the same  CRF trained on manual annotations (MA-CRF). The reg- ular evaluation is shown in the left columns and lenient  evaluation (cf. Section 4) in the right.", "labels": [], "entities": []}, {"text": " Table 4: Test: Regular (DS-CRF) and approximate DS  (DS appr -CRF) results, with lenient evaluation.  \u2020 indicates  statistically significant improvement over DS-CRF (p <  0.05).", "labels": [], "entities": [{"text": "approximate DS  (DS appr -CRF)", "start_pos": 37, "end_pos": 67, "type": "METRIC", "confidence": 0.91718390583992}]}, {"text": " Table 6: Test: Replica of the experiments in", "labels": [], "entities": []}, {"text": " Table 5.   \u2020 indicates statistically significant improvement over DS- CRF (p < 0.05).", "labels": [], "entities": [{"text": "DS- CRF", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.5601430336634318}]}]}