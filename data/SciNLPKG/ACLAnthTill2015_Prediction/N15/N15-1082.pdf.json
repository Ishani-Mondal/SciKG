{"title": [], "abstractContent": [{"text": "Coreference resolution is a key problem in natural language understanding that still escapes reliable solutions.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9374534487724304}, {"text": "natural language understanding", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.651588241259257}]}, {"text": "One fundamental difficulty has been that of resolving instances involving pronouns since they often require deep language understanding and use of background knowledge.", "labels": [], "entities": []}, {"text": "In this paper we propose an algorithmic solution that involves anew representation for the knowledge required to address hard coreference problems, along with a constrained optimization framework that uses this knowledge in coreference decision making.", "labels": [], "entities": [{"text": "coreference decision making", "start_pos": 224, "end_pos": 251, "type": "TASK", "confidence": 0.8192710479100546}]}, {"text": "Our representation, Predicate Schemas, is instantiated with knowledge acquired in an unsupervised way, and is compiled automatically into constraints that impact the coreference decision.", "labels": [], "entities": []}, {"text": "We present a general coreference resolution system that significantly improves state-of-the-art performance on hard, Winograd-style, pronoun resolution cases, while still performing at the state-of-the-art level on standard coreference resolution datasets.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.8905260562896729}, {"text": "Winograd-style, pronoun resolution cases", "start_pos": 117, "end_pos": 157, "type": "TASK", "confidence": 0.671519112586975}, {"text": "coreference resolution", "start_pos": 224, "end_pos": 246, "type": "TASK", "confidence": 0.7864082455635071}]}], "introductionContent": [{"text": "Coreference resolution is one of the most important tasks in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9242884516716003}, {"text": "Natural Language Processing (NLP)", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.7030931313832601}]}, {"text": "Although there is a plethora of works on this task (, it is still deemed an unsolved problem due to intricate and ambiguous nature of natural language * These authors contributed equally to this text.", "labels": [], "entities": []}, {"text": "Existing methods perform particularly poorly on pronouns, specifically when gender or plurality information cannot help.", "labels": [], "entities": []}, {"text": "In this paper, we aim to improve coreference resolution by addressing these hard problems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.9764328598976135}]}, {"text": "Consider the following examples: Ex.", "labels": [], "entities": []}, {"text": "In both examples, one cannot resolve the pronouns based on only gender or plurality information.", "labels": [], "entities": []}, {"text": "Recently, Rahman and Ng (2012) gathered a dataset containing 1886 sentences of such challenging pronoun resolution problems (referred to later as the Winograd dataset, following and).", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8387764692306519}, {"text": "Winograd dataset", "start_pos": 150, "end_pos": 166, "type": "DATASET", "confidence": 0.9355837106704712}]}, {"text": "As an indication to the difficulty of these instances, we note that a state-ofthe-art coreference resolution system () achieves precision of 53.26% on it.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.7888365983963013}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9995143413543701}]}, {"text": "A special purpose classifier) trained on this data set achieves 73.05%.", "labels": [], "entities": [{"text": "73.05", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9810612201690674}]}, {"text": "The key contribution of this paper is a general purpose, state-of-theart coreference approach which, at the same time, achieves precision of 76.76% on these hard cases.", "labels": [], "entities": [{"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9994338154792786}]}, {"text": "Addressing these hard coreference problems requires significant amounts of background knowledge, along with an inference paradigm that can make use of it in supporting the coreference decision.", "labels": [], "entities": []}, {"text": "Specifically, in Ex.1 one needs to know that \"a limb bends\" is more likely than \"a bird bends\".", "labels": [], "entities": [{"text": "Ex.1", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.9352620244026184}]}, {"text": "In Ex.2 one needs to know that the subject of the verb \"rob\" is more likely to be the object of \"arrest\" than the object of the verb \"rob\" is.", "labels": [], "entities": []}, {"text": "The knowledge required is, naturally, centered around the key predicates in the sentence, motivating the central notion proposed in this paper, that of Predicate Schemas.", "labels": [], "entities": []}, {"text": "In this paper, we develop the notion of Predicate Schemas, instantiate them with automatically acquired knowledge, and show how to compile it into constraints that are used to resolve coreference within a general Integer Linear Programming (ILP) driven approach to coreference resolution.", "labels": [], "entities": [{"text": "Predicate Schemas", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.8119950890541077}, {"text": "coreference resolution", "start_pos": 265, "end_pos": 287, "type": "TASK", "confidence": 0.9537887573242188}]}, {"text": "Specifically, we study two types of Predicate Schemas that, as we show, cover a large fraction of the challenging cases.", "labels": [], "entities": [{"text": "Predicate Schemas", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.9546966254711151}]}, {"text": "The first specifies one predicate with its subject and object, thus providing information on the subject and object preferences of a given predicate.", "labels": [], "entities": []}, {"text": "The second specifies two predicates with a semantically shared argument (either subject or object), thus specifies role preferences of one predicate, among roles of the other.", "labels": [], "entities": []}, {"text": "We instantiate these schemas by acquiring statistics in an unsupervised way from multiple resources including the Gigaword corpus, Wikipedia, Web Queries and polarity information.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 114, "end_pos": 129, "type": "DATASET", "confidence": 0.9312038421630859}]}, {"text": "A lot of recent work has attempted to utilize similar types of resources to improve coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.9671320915222168}]}, {"text": "The common approach has been to inject knowledge as features.", "labels": [], "entities": []}, {"text": "However, these pieces of knowledge provide relatively strong evidence that loses impact in standard training due to sparsity.", "labels": [], "entities": []}, {"text": "Instead, we compile our Predicate Schemas knowledge automatically, at inference time, into constraints, and make use of an ILP driven framework () to make decisions.", "labels": [], "entities": []}, {"text": "Using constraints is also beneficial when the interaction between multiple pronouns is taken into account when making global decisions.", "labels": [], "entities": []}, {"text": "Consider the following example: Ex In order to correctly resolve the pronouns in Ex.3, one needs to have the knowledge that \"he asks him\" indicates that he and him refer to different entities (because they are subject and object of the same predicate; otherwise, himself should be used instead of him).", "labels": [], "entities": []}, {"text": "This knowledge, which can be easily represented as constraints during inference, then impacts other pronoun decisions in a global decision with respect to all pronouns: pro 3 is likely to be different from pro 2 , and is likely to refer toe 2 . This type of inference can be easily represented as a constraint during inference, but hard to inject as a feature.", "labels": [], "entities": []}, {"text": "We then incorporate all constraints into a general coreference system () utilizing the mention-pair model.", "labels": [], "entities": []}, {"text": "A classifier learns a pairwise metric between mentions, and during inference, we follow the framework proposed in Chang et al.", "labels": [], "entities": []}, {"text": "(2011) using ILP.", "labels": [], "entities": []}, {"text": "The main contributions of this paper can be summarized as follows: 1.", "labels": [], "entities": []}, {"text": "We propose the Predicate Schemas representation and study two specific schemas that are important for coreference.", "labels": [], "entities": []}, {"text": "2. We show how, in a given context, Predicate Schemas can be automatically compiled into constraints and affect inference.", "labels": [], "entities": []}, {"text": "3. Consequently, we address hard pronoun resolution problems as a standard coreference problem and develop a system 1 which shows significant improvement for hard coreference problems while achieving the same state-of-the-art level of performance on standard coreference problems.", "labels": [], "entities": [{"text": "hard pronoun resolution", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.747897187868754}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We describe our Predicate Schemas in Section 2 and explain the inference framework and automatic constraint generation in Section 3.", "labels": [], "entities": [{"text": "automatic constraint generation", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.7126107811927795}]}, {"text": "A summary of our knowledge acquisition steps is given in Section 4.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.8437179327011108}]}, {"text": "We report our experimental results and analysis in Section 5, and review related work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our system for both hard coreference problems and general coreference problems, and provide detailed anaylsis on the impact of our proposed Predicate Schemas.", "labels": [], "entities": []}, {"text": "Since we treat resolving hard pronouns as part of the general coreference problems, we extend the Winograd dataset with a more complete annotation to get anew dataset.", "labels": [], "entities": [{"text": "Winograd dataset", "start_pos": 98, "end_pos": 114, "type": "DATASET", "confidence": 0.9611870646476746}]}, {"text": "We evaluate our system on both datasets, and show significant improvemnt over the baseline system and over the results reported in.", "labels": [], "entities": []}, {"text": "Moreover, we show that, at the same time, our system achieves the state-of-art performance on standard coreference datasets.", "labels": [], "entities": []}, {"text": "Datasets: Since we aim to solve hard coreference problems, we choose to test our system on the Winograd dataset 12.", "labels": [], "entities": [{"text": "Winograd dataset 12", "start_pos": 95, "end_pos": 114, "type": "DATASET", "confidence": 0.9748815695444742}]}, {"text": "It is a challenging pronoun resolution dataset which consists of sentence pairs based on Winograd schemas.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7768934965133667}]}, {"text": "The original annotation only specifies one pronoun and two entites in each sentence, and it is considered as a binary decision for each pronoun.", "labels": [], "entities": []}, {"text": "As our target is to model and solve them as general coreference problems, we expand the annotation to include all pronouns and their linked entities as mentions (We call this new re-annotated dataset WinoCoref 13 ).", "labels": [], "entities": []}, {"text": "Ex.3 in Section 1 is from the Winograd dataset.", "labels": [], "entities": [{"text": "Winograd dataset", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.9884207546710968}]}, {"text": "It originally only specifies he as the pronoun in question, and we added him and his as additional target pronouns.", "labels": [], "entities": []}, {"text": "We also use two standard coreference resolution   We inject Predicate Schemas knowledge as mentionpair features and retrain the system (KnowFeat).", "labels": [], "entities": []}, {"text": "We use the original coreference model and Predicate Schemas knowledge as constraints during inference (KnowCons).", "labels": [], "entities": []}, {"text": "We also have a combined system (KnowComb), which uses the schema knowledge to add features for learning as well as constraints for inference.", "labels": [], "entities": []}, {"text": "A summary of all systems is provided in  Suppose there are k pronouns in the dataset, and each pronoun has n 1 , n 2 , \u00b7 \u00b7 \u00b7 , n k antecedents, respectively.", "labels": [], "entities": []}, {"text": "We can view predicted coreference clusters as binary decisions on each antecedent-pronoun pair (linked or not).", "labels": [], "entities": []}, {"text": "The total number of binary decisions is k i=1 n i . We then meaure how many binary decisions among them are correct; let m be the number of correct decisions, then AnrePre is computed as: m k i=1 n i .", "labels": [], "entities": [{"text": "AnrePre", "start_pos": 164, "end_pos": 171, "type": "METRIC", "confidence": 0.9973210692405701}]}], "tableCaptions": [{"text": " Table 1: Example sentences for each schema category. The annotated entities and pronouns are hard coref- erence problems.", "labels": [], "entities": []}, {"text": " Table 2: 1) For Type 1 schema, S(u, v) \u2261  S(pred v (m = u, a = a v )) 6 2) For Type 2 schema,", "labels": [], "entities": []}, {"text": " Table 5: Statistics of Winograd, WinoCoref, ACE and OntoNotes. We give the total number of mentions and  pronouns, while the number of predictions for pronoun is specific for the test data. We added 746 mentions  (709 among them are pronouns) to WinoCoref compared to Winograd.", "labels": [], "entities": [{"text": "ACE", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8232766389846802}]}, {"text": " Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on  WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by  over than 20% on Winograd and over 15% on WinoCoref.", "labels": [], "entities": [{"text": "Winograd", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9138675928115845}, {"text": "WinoCoref datasets", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.7383109778165817}, {"text": "Winograd", "start_pos": 225, "end_pos": 233, "type": "DATASET", "confidence": 0.9452009201049805}]}, {"text": " Table 9: Distribution of instances in Winograd  dataset of each category. Cat1/Cat2 is the subset of  instances that require Type 1/Type 2 schema knowl- edge, respectively. All other instances are put into  Cat3. Cat1 and Cat2 instances can be covered by  our proposed Predicate Schemas.", "labels": [], "entities": [{"text": "Winograd  dataset", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.9579402804374695}]}, {"text": " Table 10: Ablation Study of Knowledge Schemas on  WinoCoref. The first line specifies the preformance  for KnowComb with only Type 1 schema knowl- edge tested on all data while the third line speci- fies the preformance using the same model but tested  on Cat1 data. The second line specifies the prefor- mance results for KnowComb system with only Type  2 schema knowledge on all data while the fourth line  specifies the preformance using the same model but  tested on Cat2 data.", "labels": [], "entities": []}]}