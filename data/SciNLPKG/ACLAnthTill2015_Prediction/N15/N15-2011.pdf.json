{"title": [{"text": "Learning Kernels for Semantic Clustering: A Deep Approach", "labels": [], "entities": []}], "abstractContent": [{"text": "In this thesis proposal we present a novel semantic embedding method, which aims at consistently performing semantic clustering at sentence level.", "labels": [], "entities": [{"text": "semantic clustering", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7289918065071106}]}, {"text": "Taking into account special aspects of Vector Space Models (VSMs), we propose to learn reproducing kernels in classification tasks.", "labels": [], "entities": []}, {"text": "By this way, capturing spectral features from data is possible.", "labels": [], "entities": []}, {"text": "These features make it theoretically plausible to model semantic similarity criteria in Hilbert spaces, i.e. the embedding spaces.", "labels": [], "entities": []}, {"text": "We could improve the semantic assessment over embed-dings, which are criterion-derived representations from traditional semantic vectors.", "labels": [], "entities": []}, {"text": "The learned kernel could be easily transferred to clustering methods, where the Multi-Class Imbalance Problem is considered (e.g. semantic clustering of definitions of terms).", "labels": [], "entities": [{"text": "semantic clustering of definitions of terms", "start_pos": 130, "end_pos": 173, "type": "TASK", "confidence": 0.8335597018400828}]}], "introductionContent": [{"text": "Overall in Machine Learning algorithms, knowledge is statistically embedded via the Vector Space Model (VSM), which is also named the semantic space (.", "labels": [], "entities": []}, {"text": "Contrarily to it is usually conceived in text data analysis, not any data set is suitable to embed into p metric spaces, including euclidean spaces (p = 2) (.", "labels": [], "entities": []}, {"text": "This implies that, in particular, clustering algorithms are being adapted to some p -derived metric, but not to semantic vector sets (clusters) ().", "labels": [], "entities": []}, {"text": "The above implication also means that semantic similarity measures are commonly not consistent, e.g. the cosine similarity or transformationbased distances).", "labels": [], "entities": []}, {"text": "These are mainly based on the concept of triangle.", "labels": [], "entities": []}, {"text": "Thus if the triangle inequality does not hold (which induces norms for Hilbert spaces exclusively), then the case of the cosine similarity becomes mathematically inconsistent . Despite VSMs are sometimes not mathematically analyzed, traditional algorithms work well enough for global semantic analysis (hereinafter global analysis, i.e. at document level where Zipf's law holds).", "labels": [], "entities": [{"text": "global semantic analysis", "start_pos": 277, "end_pos": 301, "type": "TASK", "confidence": 0.8118767142295837}]}, {"text": "Nevertheless, for local analysis (hereinafter local analysis, i.e., at sentence, phrase or word level) the issue remains still open (.", "labels": [], "entities": []}, {"text": "In this thesis proposal, we will address the main difficulties raised from traditional VSMs for local analysis of text data.", "labels": [], "entities": []}, {"text": "We consider the latter as an illposed problem (which implies unstable algorithms) in the sense of some explicit semantic similarity criterion (hereinafter criterion), e.g. topic, concept, etc.).", "labels": [], "entities": []}, {"text": "The following feasible reformulation is proposed.", "labels": [], "entities": []}, {"text": "By learning a kernel in classification tasks, we want to induce an embedding space.", "labels": [], "entities": []}, {"text": "In this space, we will consider relevance (weighting) of spectral features of data, which are in turn related to the shape of semantic vector sets ().", "labels": [], "entities": []}, {"text": "These vectors would be derived from different Statistical Language Models (SLMs); i.e. countable things, e.g. n-grams, bag-ofwords (BoW), etc.; which in turn encode language aspects (e.g. semantics, syntax, morphology, etc.).", "labels": [], "entities": []}, {"text": "Learned kernels are susceptible to be transferred to clustering methods (, where spectral features would be properly filtered from text ().", "labels": [], "entities": []}, {"text": "When both learning and clustering processes are performed, the kernel approach is tolerant enough for data scarcity.", "labels": [], "entities": []}, {"text": "Thus, eventually, we could have any criterion-derived amount of semantic clusters regardless of the Multi-Class Imbalance Problem (MCIP).", "labels": [], "entities": []}, {"text": "It is a rarely studied problem in Natural Language Processing (NLP), however, contributions can be helpful in a number of tasks such as IE, topic modeling, QA systems, opinion mining, Natural Language Understanding, etc.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.6884361306826273}, {"text": "IE", "start_pos": 136, "end_pos": 138, "type": "TASK", "confidence": 0.9896463751792908}, {"text": "topic modeling", "start_pos": 140, "end_pos": 154, "type": "TASK", "confidence": 0.8076822459697723}, {"text": "opinion mining", "start_pos": 168, "end_pos": 182, "type": "TASK", "confidence": 0.7946726381778717}, {"text": "Natural Language Understanding", "start_pos": 184, "end_pos": 214, "type": "TASK", "confidence": 0.6907919645309448}]}, {"text": "This paper is organized as follows: In Section 2 we show our case study.", "labels": [], "entities": []}, {"text": "In Section 3 we show the embedding framework.", "labels": [], "entities": []}, {"text": "In Section 4 we present our learning problem.", "labels": [], "entities": []}, {"text": "Sections 5 and 6 respectively show research directions and related work.", "labels": [], "entities": []}, {"text": "In Section 7, conclusions and future work are presented.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}