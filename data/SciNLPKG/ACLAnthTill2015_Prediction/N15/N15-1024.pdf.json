{"title": [], "abstractContent": [{"text": "Research on entity linking has considered abroad range of text, including newswire, blogs and web documents in multiple languages.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7133204787969589}]}, {"text": "However, the problem of entity linking for spoken language remains unexplored.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7269739806652069}]}, {"text": "Spoken language obtained from automatic speech recognition systems poses different types of challenges for entity linking; transcription errors can distort the context, and named entities tend to have high error rates.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 107, "end_pos": 121, "type": "TASK", "confidence": 0.7228277176618576}]}, {"text": "We propose features to mitigate these errors and evaluate the impact of ASR errors on entity linking using anew corpus of entity linked broadcast news transcripts.", "labels": [], "entities": [{"text": "ASR", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9898722171783447}]}], "introductionContent": [{"text": "Entity linking identifies for each textual mention of an entity a corresponding entry contained in a knowledge base, or indicates when no such entry exits (NIL).", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7014814168214798}]}, {"text": "Numerous studies have explored entity linking in a wide range of domains, including newswire (, blog posts (, web pages), social media (), email () and multi-lingual documents.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.7139086723327637}]}, {"text": "A common theme across all these settings requires addressing two difficulties in linking decisions: matching the textual name mention to the form contained in the knowledge base, and using contextual clues to disambiguate similar entities.", "labels": [], "entities": []}, {"text": "However, all of these studies have focused on written language, while linking of spoken language remains untested.", "labels": [], "entities": []}, {"text": "Yet many intended applications of entity linking, such as supporting search) and identifying relevant sources for reports), linking of spoken language is critical.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7240848243236542}]}, {"text": "Search results regularly include audio content (e.g. YouTube) and numerous information sources are audio recordings (e.g. media reports.)", "labels": [], "entities": []}, {"text": "An evaluation of entity linking for spoken language can help clarify issues and challenges in this domain.", "labels": [], "entities": []}, {"text": "In addition to the two main challenges discussed above, audio entity linking presents two parallel difficulties that arise from automatic transcription (ASR) of speech.", "labels": [], "entities": [{"text": "audio entity linking", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.6759931643803915}, {"text": "automatic transcription (ASR) of speech", "start_pos": 128, "end_pos": 167, "type": "TASK", "confidence": 0.7526156221117292}]}, {"text": "First, the context can be both shorter (than newswire formats) and contain ASR errors, which can make the context of the mention less like supporting material in the knowledge base.", "labels": [], "entities": []}, {"text": "Second, named entities are often more difficult to recognize; they are often out-of-vocabulary and less common overall in training data.", "labels": [], "entities": []}, {"text": "This can mislead the name matching techniques on which most entity linking systems depend.", "labels": [], "entities": [{"text": "name matching", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.8161835968494415}]}, {"text": "In this paper we consider the task of entity linking for spoken language by evaluating linking on transcripts of broadcast news.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.7344754338264465}]}, {"text": "We select broadcast news as a comparable domain of spoken language to newswire documents, which have been the focus of considerable research for entity linking.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 145, "end_pos": 159, "type": "TASK", "confidence": 0.7483022511005402}]}, {"text": "We chose this comparable domain to focus on issues introduced because of a transition to spoken language from written, as opposed to issues that arise from a general domain shift associated with conversational speech, an issue that has been previously studied in the shift from written news to written conversations (weblogs, social media, etc.)", "labels": [], "entities": []}, {"text": "(. We proceed as follows.", "labels": [], "entities": []}, {"text": "We first introduce anew broadcast news dataset annotated for entity linking.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.7756260633468628}]}, {"text": "We then propose new features based on ASR output to address the two sources of error specific to spoken language: 1) context errors and shortening, 2) name mention transcription errors.", "labels": [], "entities": [{"text": "ASR", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9698230028152466}]}, {"text": "We then test our features on the automated output of an ASR system to validate our findings.", "labels": [], "entities": [{"text": "ASR", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9532865285873413}]}], "datasetContent": [{"text": "We evaluate reference transcripts and output from two ASR systems run on our dataset (HUB4).", "labels": [], "entities": [{"text": "ASR", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9371307492256165}, {"text": "HUB4", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.9717546701431274}]}, {"text": "We use) trained on the spoken version of the Wall Street Journal corpus.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 45, "end_pos": 71, "type": "DATASET", "confidence": 0.9279905259609222}]}, {"text": "The first system (mono) relies on an HMM whose hidden states are context-independent phones.", "labels": [], "entities": []}, {"text": "The second system (tri4b) uses an HMM that outputs phones dependent on their immediate left and right contexts.", "labels": [], "entities": []}, {"text": "These systems respectively achieve 70.6% and 50.7% WER over our training set, where high error rates are likely due to a shift in domain from primarily financial news to the wider  news variety in HUB4.", "labels": [], "entities": [{"text": "WER", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9994438290596008}, {"text": "error", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9653379321098328}, {"text": "HUB4", "start_pos": 197, "end_pos": 201, "type": "DATASET", "confidence": 0.9501540660858154}]}, {"text": "These higher error settings test the limits of entity linking in noisy ASR.", "labels": [], "entities": [{"text": "ASR", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9282673597335815}]}, {"text": "To find the ASR-corrupted mention string used for the query q we align the ASR transcript by tokenlevel edit distance -additions and deletions cost 1, while substitutions cost the Jaccard distance between two tokens.", "labels": [], "entities": []}, {"text": "This is done at both training and test time, and allows us to evaluate performance of entity linking features without worrying about errors introduced by a named entity recognizer on the transcripts.", "labels": [], "entities": []}, {"text": "Entity Linking System Training The entity linking system relies on a linear Ranking SVM objective), and the optimal slack parameter C was chosen using 5-fold cross validation over the training set (C varied from 1 and 5 \u00d710 \u22125...3 ).", "labels": [], "entities": []}, {"text": "During cross-validation, mention string types were kept disjoint between the train and development folds.", "labels": [], "entities": []}, {"text": "Ranking was performed over the (up to) 1000 candidates produced by triage selected from the TAC-KBP 2009/10 KB (.", "labels": [], "entities": [{"text": "TAC-KBP 2009/10 KB", "start_pos": 92, "end_pos": 110, "type": "DATASET", "confidence": 0.9394831776618957}]}, {"text": "Using the selected C we trained over the entire training set and evaluated on the test set.", "labels": [], "entities": []}, {"text": "reports both the average accuracy for 5-fold cross validation (CV) on train and for the best tuned system from CV on test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9992722868919373}]}, {"text": "The reference test accuracy is relatively high, but lags behind person entity linking for written language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9652289748191833}]}, {"text": "When accurate transcripts are available, entity linking for spoken language, while harder, achieves just a little behind written language.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.7653345465660095}]}, {"text": "However, on ASR transcripts, accuracy drops considerably: 0.77 reference to 0.48 (mono, 71% WER) or 0.55 (tri4b, 51% WER).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9994497895240784}, {"text": "WER", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9247400164604187}]}, {"text": "Our features improve accuracy for both ASR systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9993224143981934}, {"text": "ASR", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.984327495098114}]}, {"text": "Metaphone features do better than Phone features.", "labels": [], "entities": []}, {"text": "Lattice do not show significant improvements, likely because they help with context but not mentions (see below.)", "labels": [], "entities": []}, {"text": "When combined with text, both metaphone and phone features do similarly.", "labels": [], "entities": []}, {"text": "The majority of our improvement comes from improvements to recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9974409341812134}]}, {"text": "shows the accuracy of queries for which the triager found the correct candidate in both the reference transcript and tri4b, providing a consistent set for comparison.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994725584983826}]}, {"text": "For these queries, tri4b is much closer to the results obtained on reference and much higher than the best results in.", "labels": [], "entities": [{"text": "tri4b", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9865269064903259}]}, {"text": "This is encouraging, especially given the 50% WER of tri4b; entity linking accuracy is not seriously impacted by noisy transcripts, provided that the correct candidate is recalled for ranking.", "labels": [], "entities": [{"text": "WER", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.997780978679657}, {"text": "entity linking", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.7169490456581116}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9400429725646973}]}, {"text": "shows the recall of the triager on the reference, tri4b and mono transcripts.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9990788698196411}]}, {"text": "Reference recall is quite high, while recall for the ASR systems is much lower.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9541328549385071}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9994507431983948}, {"text": "ASR", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.8337426781654358}]}, {"text": "Here, our features dramatically improve the recall, giving the ranker an opportunity to correctly score these queries.", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9994258880615234}]}, {"text": "The challenge of recall is that many of the mention strings are incorrectly recognized.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9913380146026611}]}, {"text": "shows the WER of the mention string and the number of mention strings for which the recognition is completely correct.", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9991249442100525}]}, {"text": "Unsurprisingly, error rates for mentions are higher than the overall WER.", "labels": [], "entities": [{"text": "error rates", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9797172844409943}, {"text": "WER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9379312992095947}]}, {"text": "In short, success on ASR transcripts is primarily dictated by the effectiveness of finding candidates in triage, which is much harder given the low recognition rate.", "labels": [], "entities": [{"text": "ASR transcripts", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.9213502407073975}]}, {"text": "Our features most benefit overall accuracy by improving recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9984910488128662}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9991356730461121}]}], "tableCaptions": [{"text": " Table 2: Overall/non-NIL triager recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9921189546585083}]}, {"text": " Table 3: Cross validation accuracy evaluated over only  those queries whose correct candidate was output by the  triager for both tri4b and reference.", "labels": [], "entities": [{"text": "Cross validation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6189620196819305}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9712355136871338}]}]}