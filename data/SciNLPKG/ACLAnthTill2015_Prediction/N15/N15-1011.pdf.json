{"title": [{"text": "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks", "labels": [], "entities": []}], "abstractContent": [{"text": "Convolutional neural network (CNN) is a neu-ral network that can make use of the internal structure of data such as the 2D structure of image data.", "labels": [], "entities": [{"text": "Convolutional neural network (CNN)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6357825398445129}]}, {"text": "This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction.", "labels": [], "entities": []}, {"text": "Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embedding of small text regions for use in classification.", "labels": [], "entities": []}, {"text": "In addition to a straightforward adaptation of CNN from image to text, a simple but new variation which employs bag-of-word conversion in the convolution layer is proposed.", "labels": [], "entities": []}, {"text": "An extension to combine multiple convolution layers is also explored for higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9927771091461182}]}, {"text": "The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text categorization is the task of automatically assigning pre-defined categories to documents written in natural languages.", "labels": [], "entities": [{"text": "Text categorization", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7413330674171448}]}, {"text": "Several types of text categorization have been studied, each of which deals with different types of documents and categories, such as topic categorization to detect discussed topics (e.g., sports, politics), spam detection (, and sentiment classification (;) to determine the sentiment typically in product or movie reviews.", "labels": [], "entities": [{"text": "spam detection", "start_pos": 208, "end_pos": 222, "type": "TASK", "confidence": 0.8943556845188141}, {"text": "sentiment classification", "start_pos": 230, "end_pos": 254, "type": "TASK", "confidence": 0.8490041494369507}]}, {"text": "A standard approach to text categorization is to represent documents by bag-of-word vectors, namely, vectors that indicate which words appear in the documents but do not preserve word order, and use classification models such as SVM.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7138191163539886}]}, {"text": "It has been noted that loss of word order caused by bag-of-word vectors (bow vectors) is particularly problematic on sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.9517724514007568}]}, {"text": "A simple remedy is to use word bi-grams in addition to unigrams (.", "labels": [], "entities": []}, {"text": "However, use of word n-grams with n > 1 on text categorization in general is not always effective; e.g., on topic categorization, simply adding phrases or n-grams is not effective (see, e.g., references in ().", "labels": [], "entities": [{"text": "topic categorization", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.73246631026268}]}, {"text": "To benefit from word order on text categorization, we take a different approach, which employs convolutional neural networks (CNN) (.", "labels": [], "entities": []}, {"text": "CNN is a neural network that can make use of the internal structure of data such as the 2D structure of image data through convolution layers, where each computation unit responds to a small region of input data (e.g., a small square of a large image).", "labels": [], "entities": []}, {"text": "We apply CNN to text categorization to make use of the 1D structure (word order) of document data so that each unit in the convolution layer responds to a small region of a document (a sequence of words).", "labels": [], "entities": []}, {"text": "CNN has been very successful on image classification; see e.g., the winning solutions of ImageNet Large Scale Visual Recognition Challenge ().", "labels": [], "entities": [{"text": "CNN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9154177308082581}, {"text": "image classification", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.7774494290351868}, {"text": "ImageNet Large Scale Visual Recognition Challenge", "start_pos": 89, "end_pos": 138, "type": "TASK", "confidence": 0.7146566808223724}]}, {"text": "On text, since the work on token-level applications (e.g., POS tagging) by, CNN has been used in systems for entity search, sentence modeling, word embedding learning, product feature mining, and soon (.", "labels": [], "entities": [{"text": "POS tagging)", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.7802308201789856}, {"text": "entity search", "start_pos": 109, "end_pos": 122, "type": "TASK", "confidence": 0.7523070275783539}, {"text": "sentence modeling", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.7583412528038025}, {"text": "word embedding learning", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.7360321084658304}, {"text": "product feature mining", "start_pos": 168, "end_pos": 190, "type": "TASK", "confidence": 0.6184508502483368}]}, {"text": "Notably, in many of these CNN studies on text, the first layer of the network converts words in sentences to word vectors by table lookup.", "labels": [], "entities": []}, {"text": "The word vectors are either trained as part of CNN training, or fixed to those learned by some other method (e.g.,) from an additional large corpus.", "labels": [], "entities": []}, {"text": "The latter is a form of semi-supervised learning, which we study elsewhere.", "labels": [], "entities": []}, {"text": "We are interested in the effectiveness of CNN itself without aid of additional resources; therefore, word vectors should be trained as part of network training if word vector lookup is to be done.", "labels": [], "entities": []}, {"text": "A question arises, however, whether word vector lookup in a purely supervised setting is really useful for text categorization.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.8066637814044952}]}, {"text": "The essence of convolution layers is to convert text regions of a fixed size (e.g., \"am so happy\" with size 3) to feature vectors, as described later.", "labels": [], "entities": []}, {"text": "In that sense, a word vector learning layer is a special (and unusual) case of convolution layer with region size one.", "labels": [], "entities": [{"text": "word vector learning layer", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.7143127918243408}]}, {"text": "Why is size one appropriate if bi-grams are more discriminating than unigrams?", "labels": [], "entities": []}, {"text": "Hence, we take a different approach.", "labels": [], "entities": []}, {"text": "We directly apply CNN to high-dimensional one-hot vectors; i.e., we directly learn embedding 1 of text regions without going through word embedding learning.", "labels": [], "entities": []}, {"text": "This approach is made possible by solving the computational issue 2 through efficient handling of high-dimensional sparse data on GPU, and it turned out to have the merits of improving accuracy with fast training/prediction and simplifying the system (fewer hyper-parameters to tune).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9987545013427734}]}, {"text": "Our CNN code for text is publicly available on the internet 3 . We study the effectiveness of CNN on text categorization and explain why CNN is suitable for the task.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7008085548877716}]}, {"text": "Two types of CNN are tested: seq-CNN is a straightforward adaptation of CNN from image to text, and bow-CNN is a simple but new variation of CNN that employs bag-of-word conversion in the convolution layer.", "labels": [], "entities": []}, {"text": "The experiments show that seq-CNN outperforms bow-CNN on sentiment classi- We use the term 'embedding' loosely to mean a structurepreserving function, in particular, a function that generates lowdimensional features that preserve the predictive structure.", "labels": [], "entities": []}, {"text": "2 CNN implemented for image would not handle sparse data efficiently, and without efficient handling of sparse data, convolution over high-dimensional one-hot vectors would be computationally infeasible.", "labels": [], "entities": []}, {"text": "3 riejohnson.com/cnn_download.html   fication, vice versa on topic classification, and the winner generally outperforms the conventional bagof-n-gram vector-based methods, as well as previous CNN models for text which are more complex.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.7461956739425659}]}, {"text": "In particular, to our knowledge, this is the first work that has successfully used word order to improve topic classification performance.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.8293542265892029}]}, {"text": "A simple extension that combines multiple convolution layers (thus combining multiple types of text region embedding) leads to further improvement.", "labels": [], "entities": []}, {"text": "Through empirical analysis, we will show that CNN can make effective use of high-order n-grams when conventional methods fail.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with CNN on two tasks, topic classification and sentiment classification.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8438321352005005}, {"text": "sentiment classification", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.9358125925064087}]}, {"text": "Detailed information for reproducing the results is available on the internet along with our code.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: RCV1 data summary.", "labels": [], "entities": [{"text": "RCV1 data summary", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.9586687286694845}]}, {"text": " Table 2: Error rate (%) comparison with bag-of-n-gram-", "labels": [], "entities": [{"text": "Error rate", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9855526983737946}]}, {"text": " Table 4: RCV1 micro-averaged and macro-averaged F-", "labels": [], "entities": [{"text": "F", "start_pos": 49, "end_pos": 50, "type": "METRIC", "confidence": 0.9490975141525269}]}]}