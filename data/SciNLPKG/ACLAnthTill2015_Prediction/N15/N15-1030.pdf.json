{"title": [{"text": "Empty Category Detection With Joint Context-Label Embeddings", "labels": [], "entities": [{"text": "Empty Category Detection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.4906766613324483}]}], "abstractContent": [{"text": "This paper presents a novel technique for empty category (EC) detection using distributed word representations.", "labels": [], "entities": [{"text": "empty category (EC) detection", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.6915933390458425}]}, {"text": "A joint model is learned from the labeled data to map both the distributed representations of the contexts of ECs and EC types to a low dimensional space.", "labels": [], "entities": []}, {"text": "In the testing phase, the context of possible EC positions will be projected into the same space for empty category detection.", "labels": [], "entities": [{"text": "empty category detection", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.6968840559323629}]}, {"text": "Experiments on Chinese Treebank prove the effectiveness of the proposed method.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 15, "end_pos": 31, "type": "DATASET", "confidence": 0.9746495485305786}]}, {"text": "We improve the precision by about 6 points on a subset of Chinese Treebank, which is anew state-of-the-art performance on CTB.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9996734857559204}, {"text": "Chinese Treebank", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.9917432069778442}, {"text": "CTB", "start_pos": 122, "end_pos": 125, "type": "DATASET", "confidence": 0.9837797284126282}]}], "introductionContent": [{"text": "The empty category (EC) is an important concept in linguistic theories.", "labels": [], "entities": []}, {"text": "It is used to describe nominal words that do not have explicit phonological forms (they are also called \"covert nouns\").", "labels": [], "entities": []}, {"text": "This kind of grammatical phenomenons is usually caused by the omission or dislocation of nouns or pronouns.", "labels": [], "entities": []}, {"text": "Empty categories are the \"hidden\" parts of text and are essential for syntactic parsing (;.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7421841621398926}]}, {"text": "As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (;), long distance dependency relation analysis).", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.7393681704998016}, {"text": "long distance dependency relation analysis", "start_pos": 138, "end_pos": 180, "type": "TASK", "confidence": 0.5573852360248566}]}, {"text": "Research also uncovers the important role of ECs in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8162890374660492}]}, {"text": "Some recent work) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation.", "labels": [], "entities": [{"text": "EC detection", "start_pos": 78, "end_pos": 90, "type": "TASK", "confidence": 0.8457336723804474}]}, {"text": "To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible).", "labels": [], "entities": []}, {"text": "Existing research mainly focuses on the first problem which is referred to as EC detection, and so is this paper.", "labels": [], "entities": [{"text": "EC detection", "start_pos": 78, "end_pos": 90, "type": "TASK", "confidence": 0.9183586835861206}]}, {"text": "As ECs are words or phrases inferable from their context, previous work mainly designs features mining the contexts of ECs and then trains classification models or parsers using these features;.", "labels": [], "entities": []}, {"text": "One problem with these human-developed features are that they are not fully capable of representing the semantics and syntax of contexts.", "labels": [], "entities": []}, {"text": "Besides, the feature engineering is also time consuming and labor intensive.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.8423980474472046}]}, {"text": "Recently neural network models have proven their superiority in capturing features using low dense vector compared with traditional manually designed features in dozens of NLP tasks (.", "labels": [], "entities": []}, {"text": "This paper demonstrates the advantages of distributed representations and neural networks in predicting the locations and types of ECs.", "labels": [], "entities": []}, {"text": "We formulate the EC detection as an annotation task, to assign predefined labels (EC types) to given contexts.", "labels": [], "entities": [{"text": "EC detection", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.8656010329723358}]}, {"text": "Recently,  proposed a system taking advantages of the hidden representations of neural networks for image annotation which is to annotate images with a set of textual words.", "labels": [], "entities": [{"text": "image annotation", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.7239968478679657}]}, {"text": "Following the work, we design a novel method for EC detection.", "labels": [], "entities": [{"text": "EC detection", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.9655482769012451}]}, {"text": "We represent possible EC positions using the word embeddings of their contexts and then map them to a low dimension space for EC detection.", "labels": [], "entities": [{"text": "EC detection", "start_pos": 126, "end_pos": 138, "type": "TASK", "confidence": 0.8656923174858093}]}, {"text": "Experiments on Chinese Treebank show that the proposed model obtains significant improvements over the previous state-of-the-art methods based on strict evaluation metrics.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 15, "end_pos": 31, "type": "DATASET", "confidence": 0.9814874529838562}]}, {"text": "We also identify the dependency relations between ECs and their heads, which is not reported in previous work.", "labels": [], "entities": []}, {"text": "The dependency relations can help us with the resolution of ECs and benefit other tasks, such as full parsing and machine translation in practice.", "labels": [], "entities": [{"text": "resolution of ECs", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.9111731648445129}, {"text": "machine translation", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7565450966358185}]}], "datasetContent": [{"text": "Initialization Parameter Tuning To optimize the parameters, firstly, we set the dimension of word vectors to be 80, the dimension of hidden space to be 50.", "labels": [], "entities": [{"text": "Initialization Parameter Tuning", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7901035149892172}]}, {"text": "We search for the suitable learning rate in {10 \u22121 , 10 \u22122 , 10 \u22124 }.", "labels": [], "entities": []}, {"text": "Then we deal with the dimension of word vectors {80, 100, 200}.", "labels": [], "entities": []}, {"text": "Finally we tune the dimension of hidden space in {50, 200, 500} against the F-1 scores.", "labels": [], "entities": [{"text": "F-1", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.8884406685829163}]}, {"text": "Those underlined figures are the value of the parameters after optimization.", "labels": [], "entities": []}, {"text": "We use the stochastic gradient descent algorithm to optimize the model.", "labels": [], "entities": [{"text": "stochastic gradient descent", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.6933283011118571}]}, {"text": "The details can be checked here ).", "labels": [], "entities": []}, {"text": "The maximum iteration number we used is 10K.", "labels": [], "entities": []}, {"text": "In the following experiments, we set the parameters to be learning rate=10 \u22121 , word vector dimension=80 and hidden layer dimension=500.", "labels": [], "entities": [{"text": "word vector dimension", "start_pos": 80, "end_pos": 101, "type": "METRIC", "confidence": 0.7255361080169678}]}, {"text": "From the experiments for parameter tuning, we find that for the word embeddings in the proposed model, low dimension vectors are better than high dimensions one for low dimension vectors are better in sharing meanings.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7383152097463608}]}, {"text": "For the hidden space which represents inputs as uninterpreted vectors, high dimensional vectors are better than low dimensional vectors.", "labels": [], "entities": []}, {"text": "The learning rates also have an impact on the performance.", "labels": [], "entities": []}, {"text": "If the learning rate is too small, we need more iterations to achieve convergence.", "labels": [], "entities": []}, {"text": "If we stop iterations too early, we will suffer under-fitting.", "labels": [], "entities": []}, {"text": "Previous work reports results based on different evaluation metrics.", "labels": [], "entities": []}, {"text": "Some work uses linear positions to describe ECs.", "labels": [], "entities": []}, {"text": "ECs are judged on a \"whether there is an EC of type A before a certain token in the text\" basis).", "labels": [], "entities": []}, {"text": "Collapsing ECs before the same token to one, has 1352 ECs in the test data. has stated that some ECs that share adjacent positions have different heads in the parse tree.", "labels": [], "entities": []}, {"text": "They judge ECs on a \"whether there is an EC of type A with a certain headword and a certain following token in the text\" basis.", "labels": [], "entities": []}, {"text": "Using this kind of metric, they gets 1765 ECs.", "labels": [], "entities": [{"text": "ECs", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.8651655316352844}]}, {"text": "Here we use the same evaluation metric with.", "labels": [], "entities": []}, {"text": "Note that we still cannot describe all the 1838 ECs in the corpora, for on some occasions ECs preceding the same token share the same headword.", "labels": [], "entities": []}, {"text": "We also omit some ECs which cause cycles in dependency trees as described in the previous sections.", "labels": [], "entities": []}, {"text": "We have 1748 ECs, 95% of all the ECs in the test data, very close to 1765 used by.", "labels": [], "entities": []}, {"text": "The total number of ECs has an impact on the recall.", "labels": [], "entities": [{"text": "ECs", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.8045681715011597}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9939475059509277}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We present the results for each kind of EC and compare our results with two previous state-of-the-art methods.", "labels": [], "entities": []}, {"text": "The proposed method yields the newest stateof-the-art performances on CTB as far as we know.", "labels": [], "entities": [{"text": "CTB", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.9545074701309204}]}, {"text": "We also identify the dependency types between ECs and their heads.", "labels": [], "entities": []}, {"text": "Some ECs, such as pro and PRO, are latent subjects of sentences.", "labels": [], "entities": []}, {"text": "They usually serve as SBJ with very few exceptions.", "labels": [], "entities": [{"text": "SBJ", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9210135340690613}]}, {"text": "While the others may play various roles.", "labels": [], "entities": []}, {"text": "There are 31 possible (EC, Dep) pairs.", "labels": [], "entities": [{"text": "EC", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9438937902450562}, {"text": "Dep", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.836062490940094}]}, {"text": "Using the same model, the overall result is p = 0.701, r = 0.703, f = 0.702.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data Division and EC Distribution", "labels": [], "entities": [{"text": "Data Division", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8323515951633453}, {"text": "EC Distribution", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.6315175294876099}]}, {"text": " Table 2: EC Distribution in the Test Data", "labels": [], "entities": [{"text": "EC Distribution", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9384844303131104}, {"text": "Test Data", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.7078539282083511}]}, {"text": " Table 3: Performance on the CTB Test Data", "labels": [], "entities": [{"text": "CTB Test Data", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9778750936190287}]}, {"text": " Table 3. We present  the results for each kind of EC and compare our  results with two previous state-of-the-art meth- ods", "labels": [], "entities": []}]}