{"title": [], "abstractContent": [{"text": "Natural language processing research increasingly relies on the output of a variety of syntactic and semantic analytics.", "labels": [], "entities": [{"text": "Natural language processing research", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7563981711864471}]}, {"text": "Yet integrating output from multiple analytics into a single framework can be time consuming and slow research progress.", "labels": [], "entities": []}, {"text": "We present a CONCRETE Chinese NLP Pipeline: an NLP stack built using a series of open source systems integrated based on the CONCRETE data schema.", "labels": [], "entities": [{"text": "CONCRETE Chinese NLP Pipeline", "start_pos": 13, "end_pos": 42, "type": "DATASET", "confidence": 0.6830859407782555}, {"text": "CONCRETE data schema", "start_pos": 125, "end_pos": 145, "type": "DATASET", "confidence": 0.8391072948773702}]}, {"text": "Our pipeline includes data ingest, word seg-mentation, part of speech tagging, parsing, named entity recognition, relation extraction and cross document coreference resolution.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.715755045413971}, {"text": "parsing", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.9585263133049011}, {"text": "named entity recognition", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.5963239073753357}, {"text": "relation extraction", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.8632756769657135}, {"text": "cross document coreference resolution", "start_pos": 138, "end_pos": 175, "type": "TASK", "confidence": 0.7446368336677551}]}, {"text": "Additionally, we integrate a tool for visualizing these annotations as well as allowing for the manual annotation of new data.", "labels": [], "entities": []}, {"text": "We release our pipeline to the research community to facilitate work on Chinese language tasks that require rich linguistic annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past few years, the natural language processing community has shifted its attention towards the Chinese language, with numerous papers covering a range of NLP tasks for Chinese.", "labels": [], "entities": []}, {"text": "Last year's EMNLP and ACL alone featured two dozen papers focused primarily on Chinese data , not including many others that considered Chinese language data within a broader context.", "labels": [], "entities": [{"text": "EMNLP and ACL", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.8419178128242493}]}, {"text": "The large number of Chinese speakers, coupled with the unique challenges of Chinese compared to well studied Romance and Excluding the Chinese Restaurant Process.", "labels": [], "entities": []}, {"text": "Germanic languages, have driven these research efforts.", "labels": [], "entities": []}, {"text": "This focus has given rise to new NLP systems that enable the automated processing of Chinese data.", "labels": [], "entities": []}, {"text": "While some pipelines cover multiple tasks, such as Stanford CoreNLP (), other tasks such as relation extraction are not included.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.9171012938022614}, {"text": "relation extraction", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8629169464111328}]}, {"text": "Modern NLP research, including research focused on Chinese, often relies on automatically produced analytics, or annotations, from multiple stages of linguistic analysis.", "labels": [], "entities": []}, {"text": "Downstream systems, such as sentiment analysis and question answering, assume that data has been pre-processed by a variety of syntactic and semantic analytics.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.9812041521072388}, {"text": "question answering", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8913100063800812}]}, {"text": "Consider the task of knowledge base population (KBP), in which information is extracted from text corpora for inclusion in a knowledge base.", "labels": [], "entities": []}, {"text": "Associated information extraction systems rely on various NLP analytics run on the data of interest, such as relation extractors that require the identification of named entities and syntactically parsed text.", "labels": [], "entities": [{"text": "Associated information extraction", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6420089999834696}, {"text": "relation extractors", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.7395066916942596}]}, {"text": "Similarly, entity linking typically assumes the presence of within document coreference resolution, named entity identification and relation extraction.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7830984890460968}, {"text": "document coreference resolution", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.6308299700419108}, {"text": "named entity identification", "start_pos": 100, "end_pos": 127, "type": "TASK", "confidence": 0.6818441649278005}, {"text": "relation extraction", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.745695024728775}]}, {"text": "These analytics themselves rely on other core NLP systems, such as part of speech tagging and syntactic parsing.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.7081303596496582}, {"text": "syntactic parsing", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7351965010166168}]}, {"text": "While each of these tasks have received extensive attention and have associated research software for producing annotations, the output of these components must be integrated into a single cohesive framework for use in a downstream task.", "labels": [], "entities": []}, {"text": "This integration faces a wide variety of challenges resulting from the simple fact that most research systems are designed to produce good performance on an eval-86 uation metric, but are not designed for integration in a pipeline.", "labels": [], "entities": []}, {"text": "Beyond the production of integrated NLP pipelines, research groups often produce resources of corpora annotated by multiple systems, such as the Annotated Gigaword Corpus (.", "labels": [], "entities": [{"text": "Annotated Gigaword Corpus", "start_pos": 145, "end_pos": 170, "type": "DATASET", "confidence": 0.7714578708012899}]}, {"text": "Effective sharing of these corpora requires a common standard.", "labels": [], "entities": []}, {"text": "These factors lead to the recent development of CONCRETE, a data schema that represents numerous types of linguistic annotations produced by a variety of NLP systems.", "labels": [], "entities": []}, {"text": "CONCRETE enables interoperability between NLP systems, facilitates the development of large scale research systems, and aids sharing of richly annotated corpora.", "labels": [], "entities": []}, {"text": "This paper describes a Chinese NLP pipeline that ingests Chinese text to produce richly annotated data.", "labels": [], "entities": []}, {"text": "The pipeline relies on existing Chinese NLP systems that encompass a variety of syntactic and semantic tasks.", "labels": [], "entities": []}, {"text": "Our pipeline is built on the CON-CRETE data schema to produce output in a structured, coherent and shareable format.", "labels": [], "entities": [{"text": "CON-CRETE data schema", "start_pos": 29, "end_pos": 50, "type": "DATASET", "confidence": 0.878011683622996}]}, {"text": "To be clear, our goal is not the development of new methods or research systems.", "labels": [], "entities": []}, {"text": "Rather, our focus is the integration of multiple tools into a single pipeline.", "labels": [], "entities": []}, {"text": "The advantages of this newly integrated pipeline lie in the fact that the components of the pipeline communicate through a unified data schema: CONCRETE.", "labels": [], "entities": [{"text": "CONCRETE", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.7644839286804199}]}, {"text": "By doing so, we can \u2022 easily switch each component of the pipeline to any state-of-the-art model; \u2022 keep several annotations of the same type generated by different tools; and \u2022 easily share the annotated corpora.", "labels": [], "entities": []}, {"text": "Furthermore, we integrate a visualization tool for viewing and editing the annotated corpora.", "labels": [], "entities": []}, {"text": "We posit all the above benefits as the contributions of this paper and hope the efforts can facilitate ongoing Chinese focused research and aid in the construction and distribution of annotated corpora.", "labels": [], "entities": []}, {"text": "Our code is available at http://hltcoe.github.io.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}