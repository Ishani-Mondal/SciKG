{"title": [{"text": "Weakly Supervised Slot Tagging with Partially Labeled Sequences from Web Search Click Logs", "labels": [], "entities": [{"text": "Slot Tagging", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.8652820885181427}]}], "abstractContent": [{"text": "In this paper, we apply a weakly-supervised learning approach for slot tagging using conditional random fields by exploiting web search click logs.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 66, "end_pos": 78, "type": "TASK", "confidence": 0.9017029106616974}]}, {"text": "We extend the constrained lattice training of T\u00e4ckstr\u00f6m et al.", "labels": [], "entities": []}, {"text": "(2013) to non-linear conditional random fields in which latent variables mediate between observations and labels.", "labels": [], "entities": []}, {"text": "When combined with a novel initialization scheme that leverages unlabeled data, we show that our method gives significant improvement over strong supervised and weakly-supervised baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "A key problem in natural language processing (NLP) is to effectively utilize large amounts of unlabeled and partially labeled data in situations where little or no annotations are available fora task of interest.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.8208698034286499}]}, {"text": "Many recent work tackled this problem mostly in the context of part-of-speech (POS) tagging by transferring POS tags from a supervised language via automatic alignment and/or constructing tag dictionaries from the web (.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.676516306400299}]}, {"text": "In this work, we attack this problem in the context of slot tagging, where the goal is to find correct semantic segmentation of a given query, which is an important task for information extraction and natural language understanding.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 55, "end_pos": 67, "type": "TASK", "confidence": 0.765986979007721}, {"text": "information extraction", "start_pos": 174, "end_pos": 196, "type": "TASK", "confidence": 0.8172145783901215}, {"text": "natural language understanding", "start_pos": 201, "end_pos": 231, "type": "TASK", "confidence": 0.6413058340549469}]}, {"text": "For instance, answering the question \"when is the new bill murray movie release date?\" requires recognizing and labeling key phrases: e.g., \"bill murray\" as actor and \"movie\" as media type.", "labels": [], "entities": []}, {"text": "The standard approach to slot tagging involves training a sequence model such as a conditional random field (CRF) on manually annotated data.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.9288890063762665}]}, {"text": "An obvious limitation of this approach is that it relies on fully labeled data, which is both difficult to adapt and changing tasks and schemas.", "labels": [], "entities": []}, {"text": "Certain films, songs, and books become more or less popular overtime, and the performance of models trained on outdated data will degrade.", "labels": [], "entities": []}, {"text": "If not updated, models trained on live data feeds such as movies, songs and books become obsolete overtime and their accuracy will degrade.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9986263513565063}]}, {"text": "In order to achieve high accuracy continuously data and even model schemas have to be refreshed on a regular basis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9770136475563049}]}, {"text": "To remedy this limitation, we propose a weakly supervised framework that utilizes the information available in web click logs.", "labels": [], "entities": []}, {"text": "A web click log is a mapping from a user query to URL link.", "labels": [], "entities": []}, {"text": "For example, users issuing queries about movies tend to click on links from the IMDB.com or rottentomatoes.com, which provide rich structured data for entities such as title of the movie (\"The Matrix\"), the director (\"The Wachowski Brothers\"), and the release date.", "labels": [], "entities": [{"text": "IMDB.com", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.9437229633331299}]}, {"text": "Web click logs present an opportunity to learn semantic tagging models from large-scale and naturally occurring user interaction data ().", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7123556137084961}]}, {"text": "While some previous works () have applied a similar strategy to incorporate click logs in slot tagging, they do not employ recent advances in machine learning to effectively leverage the incomplete annotations.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 90, "end_pos": 102, "type": "TASK", "confidence": 0.8440361618995667}]}, {"text": "In this paper, we pursue and extend learning from partially labeled sequences, in particular the approach of.", "labels": [], "entities": []}, {"text": "Instead of projecting labels from a high-resource to a low-resource languages via parallel text and word alignment, we project annotations from structured data found in click logs.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.7196510583162308}]}, {"text": "This can be seen as a benefit since typically a much larger volume of click log data is available than parallel text for low-resource languages.", "labels": [], "entities": []}, {"text": "We also extend the constrained lattice training method of from linear CRFs to non-linear CRFs.", "labels": [], "entities": []}, {"text": "We propose a perceptron training method for hidden unit) that allows us to train with partially labeled sequences.", "labels": [], "entities": []}, {"text": "We show that combined with a novel pretraining methodology that leverages large quantities of unlabeled data, this training method achieves significant improvements over several strong baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the effectiveness of our approach, we perform experiments on a suite of three entertainment domains for slot tagging: queries about movies, music, and games.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 112, "end_pos": 124, "type": "TASK", "confidence": 0.9149898290634155}]}, {"text": "For each domain, we have two types of data: engineered data and log data.", "labels": [], "entities": []}, {"text": "Engineered data is a set of synthetic queries to mimic the behavior of users.", "labels": [], "entities": []}, {"text": "This data is created during development at which time no log data is available.", "labels": [], "entities": []}, {"text": "Log data is a set of queries created by actual users using deployed spoken dialogue systems: thus it is directly transcribed from users' voice commands with automatic speech recognition (ASR).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 157, "end_pos": 191, "type": "TASK", "confidence": 0.6951848020156225}]}, {"text": "In general we found log data to be fairly noisy, containing many ASR and grammatical errors, whereas engineered data consisted of clean, well-formed text.", "labels": [], "entities": [{"text": "ASR", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9294323325157166}]}, {"text": "Not surprisingly, synthetic queries in engineered data are not necessarily representative of real queries in log data since it is difficult to accurately simulate what users' queries will be before a fully functioning system is available and real user data can be gathered.", "labels": [], "entities": []}, {"text": "Hence this setting can greatly benefit from weakly-supervised learning methods such as ours since it is critical to learn from new incoming log data.", "labels": [], "entities": []}, {"text": "We use search engine log data to project lattice constraints for weakly supervised learning.", "labels": [], "entities": []}, {"text": "In this setup, a user issues a natural language query to retrieve movies, music titles, games and/or information thereof.", "labels": [], "entities": []}, {"text": "For instance, a user could say \"play the latest batman movie\" or \"find beyonce's music\".", "labels": [], "entities": []}, {"text": "Our slot sequence tagger is trained with variants of CRF using lexical features, gazetteers, Brown clusters and context words.", "labels": [], "entities": []}, {"text": "The domains consist of 35 slot types for movies, 25 for music and 24 for games.", "labels": [], "entities": []}, {"text": "Slot types correspond to both named entities (e.g., game name, music title, movie name) as well as more general categories (genre, media type, description).", "labels": [], "entities": []}, {"text": "shows the size of the datasets used in our experiments.: The difference in F1 performance of CRF models trained only on engineered data but tested on both engineered and log data.", "labels": [], "entities": [{"text": "F1", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9989598989486694}]}, {"text": "Our main contribution is to leverage search log data to improve slot tagging in spoken dialogue systems.", "labels": [], "entities": [{"text": "slot tagging", "start_pos": 64, "end_pos": 76, "type": "TASK", "confidence": 0.8840527534484863}]}, {"text": "In this section, we assume that we have no log data in training slot taggers.", "labels": [], "entities": [{"text": "training slot taggers", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.6006498634815216}]}, {"text": "We did not see a significant difference between perceptron and LBFGS inaccuracy, but perceptron is faster and thus favorable for training complex HUCRF models.", "labels": [], "entities": []}, {"text": "We used 100 as the maximum iteration count and 1.0 for the L2 regularization parameter.", "labels": [], "entities": []}, {"text": "The number of hidden variables per token is set to 300.", "labels": [], "entities": []}, {"text": "The same features described in the previous section are used here.", "labels": [], "entities": []}, {"text": "We perform experiments with the following CRF variants (see Section 2): \u2022 CRF: A fully supervised linear-chain CRF trained with manually labeled engineered samples.", "labels": [], "entities": []}, {"text": "\u2022 POCRF: A partially observed CRF of trained with both manually labeled engineered samples and click logs.", "labels": [], "entities": []}, {"text": "\u2022 POHUCRF: A partially observed hidden unit CRF () trained with both manually labeled engineered samples and click logs.", "labels": [], "entities": [{"text": "POHUCRF", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.8672106862068176}]}, {"text": "\u2022 POHUCRF+: POHUCRF with pre-training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Labeled data set size for games, movies and mu- sic domains partitioned into training and test set.", "labels": [], "entities": []}, {"text": " Table 2: The difference in F1 performance of CRF mod- els trained only on engineered data but tested on both en- gineered and log data.", "labels": [], "entities": [{"text": "F1", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9992192983627319}]}, {"text": " Table 3: The F1 performance of variants of CRF across  three domains, test on log data", "labels": [], "entities": [{"text": "F1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9991397857666016}]}]}