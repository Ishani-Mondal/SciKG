{"title": [{"text": "Effective Feature Integration for Automated Short Answer Scoring *", "labels": [], "entities": [{"text": "Short Answer Scoring", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.5530358056227366}]}], "abstractContent": [{"text": "A major opportunity for NLP to have a real-world impact is in helping educators score student writing, particularly content-based writing (i.e., the task of automated short answer scoring).", "labels": [], "entities": [{"text": "automated short answer scoring)", "start_pos": 157, "end_pos": 188, "type": "TASK", "confidence": 0.6716974556446076}]}, {"text": "A major challenge in this enterprise is that scored responses to a particular question (i.e., labeled data) are valuable for mod-eling but limited in quantity.", "labels": [], "entities": []}, {"text": "Additional information from the scoring guidelines for humans , such as exemplars for each score level and descriptions of key concepts, can also be used.", "labels": [], "entities": []}, {"text": "Here, we explore methods for integrating scoring guidelines and labeled responses , and we find that stacked generalization (Wolpert, 1992) improves performance, especially for small training sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Educational applications of NLP have considerable potential for real-world impact, particularly in helping to score responses to assessments, which could allow educators to focus more on instruction.", "labels": [], "entities": []}, {"text": "We focus on the task of analyzing short, contentfocused responses from an assessment of reading comprehension, following previous work on short answer scoring ().", "labels": [], "entities": []}, {"text": "This task is typically defined as a text regression or classification problem: we label student responses that consist of one or more sentences with scores on an * Work done when Keisuke Sakaguchi was an intern at ETS.", "labels": [], "entities": [{"text": "text regression or classification", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.8025671318173409}, {"text": "ETS", "start_pos": 214, "end_pos": 217, "type": "DATASET", "confidence": 0.9725488424301147}]}, {"text": "Michael Heilman is now a data scientist at Civis Analytics.", "labels": [], "entities": []}, {"text": "ordinal scale (e.g. correct, partially correct, or incorrect; 1-5 score range, etc.).", "labels": [], "entities": []}, {"text": "Importantly, in addition to the student response itself, we may also have available other information such as reference answers or descriptions of key concepts from the scoring guidelines for human scorers.", "labels": [], "entities": []}, {"text": "Such information can be cheap to acquire since it is often generated as part of the assessment development process.", "labels": [], "entities": []}, {"text": "Generally speaking, most work on short answer scoring takes one of the following approaches: \u2022 A response-based approach uses detailed features extracted from the student response itself (e.g., word n-grams, etc.) and learns a scoring function from human-scored responses.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.6397106051445007}]}, {"text": "\u2022 A reference-based approach compares the student response to reference texts, such as exemplars for each score level, or specifications of required content from the assessment's scoring guidelines.", "labels": [], "entities": []}, {"text": "Various text similarity methods ( can be used.", "labels": [], "entities": [{"text": "text similarity", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.6879216134548187}]}, {"text": "These two approaches can, of course, be combined.", "labels": [], "entities": []}, {"text": "However, to our knowledge, the issues of how to combine the approaches and when that is likely to be useful have not been thoroughly studied.", "labels": [], "entities": []}, {"text": "A challenge in combining the approaches is that the response-based approach produces a large set of sparse features (e.g., word n-gram indicators), while the reference-based approach produces a small set of continuous features (e.g., similarity scores between the response and exemplars for different score levels).", "labels": [], "entities": []}, {"text": "A simple combination method is to train a model on the union of the feature sets ( \u00a73.3).", "labels": [], "entities": []}, {"text": "However, the dense reference features maybe lost among the many sparse response features.", "labels": [], "entities": []}, {"text": "Therefore, we apply stacked generalization (i.e. stacking) to build an ensemble of the response-and reference-based approaches.", "labels": [], "entities": []}, {"text": "To our knowledge, there is little if any research investigating the value of stacking for NLP applications such as automated scoring.", "labels": [], "entities": [{"text": "automated scoring", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.6172311455011368}]}, {"text": "The contributions of this paper are as follows: (1) we investigate various reference-based features for short answer scoring, (2) we apply stacking) in order to combine the referenceand response-based methods, and (3) we demonstrate that the stacked combination outperforms other models, especially for small training sets.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 104, "end_pos": 124, "type": "TASK", "confidence": 0.6918780207633972}]}], "datasetContent": [{"text": "We conduct our experiments on short-answer questions that are developed under the Reading for Understanding (RfU) assessment framework.", "labels": [], "entities": [{"text": "Reading for Understanding (RfU) assessment", "start_pos": 82, "end_pos": 124, "type": "TASK", "confidence": 0.53354064481599}]}, {"text": "This framework is designed to measure the reading comprehension skills of students from grades 6 through 9 by attempting to assess whether the reader has formed a coherent mental model consistent with the text discourse.", "labels": [], "entities": []}, {"text": "A more detailed description is provided by.", "labels": [], "entities": []}, {"text": "We use 4 short-answer questions based on two different reading passages.", "labels": [], "entities": []}, {"text": "The first passage is a 1300-word short story.", "labels": [], "entities": []}, {"text": "A single question (\"Q1\" hereafter) asks the reader to read the story and write a 5-7 sentence synopsis in her own words that includes all the main characters and action from the story but does not include any opinions or information from outside the story.", "labels": [], "entities": []}, {"text": "The second passage is a 700-word article that describes the experiences of European immigrants in the late 19th and early 20th centuries.", "labels": [], "entities": []}, {"text": "There are 3 questions associated with this passage: two that ask the reader to summarize one section each in the article (\"Q2\" and \"Q4\") and a third that asks to summarize the entire article (\"Q3\").", "labels": [], "entities": [{"text": "summarize", "start_pos": 162, "end_pos": 171, "type": "TASK", "confidence": 0.9803832173347473}]}, {"text": "These 3 questions ask the reader to restrict his or her responses to 1-2 sentences each.", "labels": [], "entities": []}, {"text": "Each question includes the following: \u2022 scored responses: short responses written by students, scored on a 0 to 4 scale for the first question, and 0 to 3 for the other 3.", "labels": [], "entities": []}, {"text": "\u2022 exemplars: one or two exemplar responses for each score level, and \u2022 key concepts: several (\u2264 10) sentences briefly expressing key concepts in a correct answer.", "labels": [], "entities": []}, {"text": "The data for each question is split into a training and testing sets.", "labels": [], "entities": []}, {"text": "For each question, we have about 2,000 scored student responses.", "labels": [], "entities": []}, {"text": "Following previous work on automatic scoring (Shermis and Burstein, 2013), we evaluate performance using the quadratically weighted \u03ba) between human and machine scores (rounded and trimmed to the range of the training scores).", "labels": [], "entities": []}, {"text": "This section describes two experiments: an evaluation of reference-based similarity metrics, and an evaluation of methods for combining the referenceand response-based features by stacking.", "labels": [], "entities": []}, {"text": "As mentioned in \u00a72, we evaluate performance using Another possible combination approach would be to use the combination method from \u00a73.3 but apply less regularization to the reference-based features, or, equivalently, scale them by a large constant.", "labels": [], "entities": []}, {"text": "We only briefly explored this through training set cross-validation.", "labels": [], "entities": []}, {"text": "The stacking approach seemed to perform at least as well in general.", "labels": [], "entities": [{"text": "stacking", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.9930277466773987}]}, {"text": "8 It would also be possible to also make a lower-layer model for the reference-based features, though doing this did not show benefits in preliminary experiments.", "labels": [], "entities": []}, {"text": ".78 .52 .66 .59 length .68 .42 .59 .51 response-based (\"resp\") .82 .72 .75 .74: Training set cross-validation performance of reference-based models, in quadratically weighted \u03ba, with baselines for comparison.", "labels": [], "entities": [{"text": "length", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9971703886985779}]}, {"text": "The response-based (\"resp\") model is a stronger baseline as described in \u00a73.3.", "labels": [], "entities": []}, {"text": "Note that each reference-based model includes the length bin features fora fair comparison to \"resp\".", "labels": [], "entities": []}, {"text": "quadratically weighted \u03ba between the human and predicted scores.", "labels": [], "entities": []}], "tableCaptions": []}