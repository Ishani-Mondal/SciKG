{"title": [{"text": "Learning to parse with IAA-weighted loss", "labels": [], "entities": [{"text": "parse", "start_pos": 12, "end_pos": 17, "type": "TASK", "confidence": 0.9613453149795532}, {"text": "IAA-weighted loss", "start_pos": 23, "end_pos": 40, "type": "METRIC", "confidence": 0.8556556701660156}]}], "abstractContent": [{"text": "Natural language processing (NLP) annotation projects employ guidelines to maximize inter-annotator agreement (IAA), and models are estimated assuming that there is one single ground truth.", "labels": [], "entities": [{"text": "Natural language processing (NLP) annotation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.741961521761758}, {"text": "inter-annotator agreement (IAA)", "start_pos": 84, "end_pos": 115, "type": "METRIC", "confidence": 0.8215203881263733}]}, {"text": "However, not all disagreement is noise, and in fact some of it may contain valuable linguistic information.", "labels": [], "entities": []}, {"text": "We integrate such information in the training of a cost-sensitive dependency parser.", "labels": [], "entities": []}, {"text": "We introduce five different factorizations of IAA and the corresponding loss functions, and evaluate these across six different languages.", "labels": [], "entities": []}, {"text": "We obtain robust improvements across the board using a factoriza-tion that considers dependency labels and di-rectionality.", "labels": [], "entities": []}, {"text": "The best method-dataset combination reaches an average overall error reduction of 6.4% in labeled attachment score.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 63, "end_pos": 78, "type": "METRIC", "confidence": 0.9698854982852936}]}], "introductionContent": [{"text": "Typically, NLP annotation projects employ guidelines to maximize inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Possible inconsistencies are resolved by adjudication, and models are induced assuming there is one single ground truth.", "labels": [], "entities": []}, {"text": "However, there exist linguistically hard cases where there is no clear answer, and incorporating such disagreements into the training of a model has proven helpful for POS tagging).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 168, "end_pos": 179, "type": "TASK", "confidence": 0.936212033033371}]}, {"text": "Inter-annotator agreement (IAA) is straightforward to calculate for POS, but not for dependency trees.", "labels": [], "entities": [{"text": "Inter-annotator agreement (IAA)", "start_pos": 0, "end_pos": 31, "type": "METRIC", "confidence": 0.8516582727432251}]}, {"text": "There is no well-established standard for computing agreement on trees).", "labels": [], "entities": []}, {"text": "For a dependency tree, annotators can disagree in attachment, labeling, or both.", "labels": [], "entities": []}, {"text": "We implement different strategies, i.e., factorizations ( \u00a72), to capture disagreement on specific syntactic phenomena.", "labels": [], "entities": []}, {"text": "Our hypothesis is that a dependency parser can be informed of disagreements to regularize over annotators' biases.", "labels": [], "entities": []}, {"text": "Testing our hypothesis requires the availability of doubly-annotated data, and involves two steps: i) how to factorize attachment or labeling disagreements; and ii) how to inform the parser of them during learning ( \u00a73).", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we use redshift, 1 a transition-based arc-eager dependency parser that implements the dynamic oracle (Goldberg and Nivre, 2012) with averaged perceptron training.", "labels": [], "entities": []}, {"text": "We modified the parser 2 to read confusion matrices and weigh the updates with the respective \u03b3.", "labels": [], "entities": []}, {"text": "We compare the five ( \u00a72) factorized systems to a baseline system that does not take confusion probabilities into account, i.e., standard redshift.", "labels": [], "entities": []}, {"text": "Throughout the experiments, we fix the number of iterations to 5, and we use pseudo-projectivization (Nivre and).", "labels": [], "entities": []}, {"text": "The parser does not include morphological features, which lowers performance for morphological rich languages like FI.", "labels": [], "entities": []}, {"text": "We report labeled attachment scores (LAS) incl. punctuation.", "labels": [], "entities": [{"text": "labeled attachment scores (LAS)", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.8044126033782959}]}, {"text": "We use bootstrap sampling in all our experiments in order to get more reliable results.", "labels": [], "entities": []}, {"text": "This method allows abstracting away from biases-in sampling and annotation-of training and test splits.", "labels": [], "entities": []}, {"text": "We use two complementary evaluation methods: crossvalidation within the training data, and learning curves against the test set.", "labels": [], "entities": []}, {"text": "We calculate significance using the approximate randomization test) with 10k iterations.", "labels": [], "entities": [{"text": "significance", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.8092876672744751}]}, {"text": "Cross-validation In this setup, we perform 50 runs of 5-fold cross validation on bootstrap-based samples of the training data.", "labels": [], "entities": []}, {"text": "This allows us to gauge the effect of our factorization without committing to a certain test set.", "labels": [], "entities": []}, {"text": "We report on the average of the total of 250 runs.", "labels": [], "entities": []}, {"text": "Learning curve To calculate the learning curves, we train the parser on increasing amounts of training data, bootstrap-sampled in steps of 10%, and evaluate against the test set.", "labels": [], "entities": []}, {"text": "Each 10% increment is repeated k = 50 times.", "labels": [], "entities": []}, {"text": "We finally report average overall error reduction over the baseline.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 34, "end_pos": 49, "type": "METRIC", "confidence": 0.9356893002986908}]}], "tableCaptions": [{"text": " Table 1: Data statistics: number of sentences/tokens, de- pendency labels l, POS tags p for NO (Norwegian), EN  (English), DA (Danish), CA (Catalan), Croatian (HR)  and Finnish (FI);  \u2020=canonical test split available.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of the doubly-annotated data.", "labels": [], "entities": []}, {"text": " Table 3: Crossvalidation results (in LAS incl. punctuation). Gray: below baseline. Best factorization per language in  boldface. Significance at p < 0.01 (computed over runs and wrt baseline) is indicated by  *  .", "labels": [], "entities": []}, {"text": " Table 4: Overall avg. error red. across learning curves.", "labels": [], "entities": []}]}