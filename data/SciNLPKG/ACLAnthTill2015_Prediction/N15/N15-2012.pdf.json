{"title": [{"text": "Narrowing the Loop: Integration of Resources and Linguistic Dataset Development with Interactive Machine Learning", "labels": [], "entities": [{"text": "Linguistic Dataset Development", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.62490181128184}]}], "abstractContent": [{"text": "This thesis proposal sheds light on the role of interactive machine learning and implicit user feedback for manual annotation tasks and semantic writing aid applications.", "labels": [], "entities": []}, {"text": "First we focus on the cost-effective annotation of training data using an interactive machine learning approach by conducting an experiment for sequence tagging of German named entity recognition.", "labels": [], "entities": [{"text": "sequence tagging of German named entity recognition", "start_pos": 144, "end_pos": 195, "type": "TASK", "confidence": 0.7241861692496708}]}, {"text": "To show the effectiveness of the approach, we further carryout a sequence tagging task on Amharic part-of-speech and are able to significantly reduce time used for annotation.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.6952077001333237}]}, {"text": "The second research direction is to systematically integrate different NLP resources for our new semantic writing aid tool using again an interactive machine learning approach to provide contextual paraphrase suggestions.", "labels": [], "entities": [{"text": "semantic writing aid", "start_pos": 97, "end_pos": 117, "type": "TASK", "confidence": 0.7637108365694681}]}, {"text": "We develop a baseline system where three lexical resources are combined to provide paraphrasing in context and show that combining resources is a promising direction.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine learning applications require considerable amounts of annotated data in order to achieve a good prediction performance.", "labels": [], "entities": []}, {"text": "Nevertheless, the development of such annotated data is labor-intensive and requires a certain degree of human expertise.", "labels": [], "entities": []}, {"text": "Also, such annotated data produced by expert annotators has limitations, such as 1) it usually does not scale very well since annotation of a very large data set is prohibitively expensive, and 2) for applications which should reflect dynamic changes of data overtime, static training data will not serve its purpose.", "labels": [], "entities": []}, {"text": "This issue is commonly known as concept drift (.", "labels": [], "entities": []}, {"text": "There has been a lot of effort in automatically expanding training data and lexical resources using different techniques.", "labels": [], "entities": []}, {"text": "One approach is the use of active learning) which aims at reducing the amount of labeled training data required by selecting most informative data to be annotated.", "labels": [], "entities": []}, {"text": "For example it selects the instances from the training dataset about which the machine learning model is least certain how to label (.", "labels": [], "entities": []}, {"text": "Another recent approach to alleviate bottleneck in collecting training data is the usage of crowdsourcing services) to collect large amount of annotations from non-expert crowds at comparably low cost.", "labels": [], "entities": []}, {"text": "In an interactive machine learning approach, the application might start with minimal or no training data.", "labels": [], "entities": []}, {"text": "During runtime, the user provides simple feedback to the machine learning process interactively by correcting suggestions or adding new annotations and integrating background knowledge into the modeling stage ().", "labels": [], "entities": []}, {"text": "Similarly, natural language processing (NLP) tasks, such as information retrieval, word sense disambiguation, sentiment analysis and question answering require comprehensive external knowledge sources (electronic dictionaries, ontologies, or thesauri) in order to attain a satisfactory performance.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.7920514146486918}, {"text": "information retrieval", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.7600052654743195}, {"text": "word sense disambiguation", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.6745150883992513}, {"text": "sentiment analysis", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.9545340240001678}, {"text": "question answering", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.817742109298706}]}, {"text": "Lexical resources such as) also suffer from the same limitations that the machine learning training data faces.", "labels": [], "entities": []}, {"text": "This proposal focuses on the development and enhancement of training data as well as on systematic combinations of different NLP resources fora semantic writing aid application.", "labels": [], "entities": []}, {"text": "More specifically we address the following issues: 1) How can we produce annotated data of high quality using an interactive machine learning approach?", "labels": [], "entities": []}, {"text": "2) How can we systematically integrate different NLP resources?", "labels": [], "entities": []}, {"text": "3) How can we integrate user interaction and feedback into the interactive machine learning system?", "labels": [], "entities": []}, {"text": "Moreover, we will explore the different paradigms of interactions (when should the machine learning produce anew model, how to provide useful suggestions to users, and how to control annotators behavior in the automation process ).", "labels": [], "entities": []}, {"text": "To tackle these problems, we will look at two applications, 1) an annotation task using a web-based annotation tool and 2) a semantic writing aid application, a tool with an online interface that provides users with paraphrase detection and prediction capability fora varying writing style.", "labels": [], "entities": [{"text": "paraphrase detection and prediction", "start_pos": 216, "end_pos": 251, "type": "TASK", "confidence": 0.7564023584127426}]}, {"text": "In principle, the two applications have similar nature except that the ultimate goal of the annotation task is to produce a fully annotated data whereas the semantic writing aid will use the improved classifier model instantly.", "labels": [], "entities": []}, {"text": "We have identified a sequence tagging and a paraphrasing setup to explore the aforementioned applications.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.6797634661197662}]}, {"text": "Sequence tagging setup: We will employ an annotation tool similar to) in order to facilitate the automatic acquisition of training data for machine learning applications.", "labels": [], "entities": [{"text": "Sequence tagging setup", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9114288886388143}]}, {"text": "Our goal is to fully annotate documents sequentially but interactively using the machine learning support in contrast to an active learning setup where the system presents portions of the document at a time.", "labels": [], "entities": []}, {"text": "Paraphrasing setup: The semantic writing aid tool is envisioned to improve readability of documents and provide varied writing styles by suggesting semantically equivalent paraphrases and remove redundant or overused words or phrases.", "labels": [], "entities": []}, {"text": "Using several lexical resources, the system will detect and provide alternative contextual paraphrases as shown in.", "labels": [], "entities": []}, {"text": "Such paraphrasing will substitute words or phrases in context with appropriate synonyms when they form valid collocations with the surrounding words () based on the lexical resource suggestion or using statistics gathered from large corpora.", "labels": [], "entities": []}, {"text": "While the work of shows that there are different approaches of paraphrasing or quasi-paraphrasing based on syntactical analysis, we will also further explore context-aware paraphrasing using distributional semantics ( and machine learning classifiers for contextual similarity.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now describe several experimental setups that evaluate the effectiveness of our current system, the quality of training data obtained, and user satisfaction in using the system.", "labels": [], "entities": []}, {"text": "We have already conducted some preliminary experiments and simulated evaluations towards some of the tasks.", "labels": [], "entities": []}, {"text": "For the semantic writing aid tool, we need to create a paraphrasing component (see.", "labels": [], "entities": [{"text": "semantic writing aid", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.8049781123797098}]}, {"text": "We conduct an evaluation by comparing automatic paraphrases against existing paraphrase corpora).", "labels": [], "entities": []}, {"text": "The Microsoft Research Paraphrase Corpus (MSRPC) () dataset, PPDB, and the DIRT paraphrase collections (Lin and Pantel, 2001) will be used for phrase-level evaluations.", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase Corpus (MSRPC) () dataset", "start_pos": 4, "end_pos": 59, "type": "DATASET", "confidence": 0.8036273452970717}, {"text": "DIRT paraphrase collections (Lin and Pantel, 2001", "start_pos": 75, "end_pos": 124, "type": "DATASET", "confidence": 0.8440072337786356}]}, {"text": "The TWSI dataset (Biemann, 2012) will be used for the word level paraphrase evaluation.", "labels": [], "entities": [{"text": "TWSI dataset (Biemann, 2012)", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.8932544759341648}, {"text": "word level paraphrase evaluation", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.6904736235737801}]}, {"text": "We will use precision, recall, and machine translation metrics BLEU for evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9994818568229675}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9964989423751831}, {"text": "machine translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.6626933813095093}, {"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9206992983818054}]}, {"text": "Once the basic paraphrasing system is in place and evaluated, the next step will be the improvement of the paraphrasing system using syntagmatic and paradigmatic structures of language as features.", "labels": [], "entities": []}, {"text": "The process will incorporate the implementation of distributional similarity based on syntactic structures such as POS tagging, dependency parsing, token n-grams, and patterns, resulting in a context-aware paraphrasing system, which offers paraphrases in context.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 115, "end_pos": 126, "type": "TASK", "confidence": 0.6866073906421661}, {"text": "dependency parsing", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.7058092951774597}]}, {"text": "Furthermore, interactive machine learning can be employed to train a model that can be used to provide context-dependent paraphrasing.", "labels": [], "entities": []}, {"text": "We have conducted preliminary experiments fora semantic writing aid system, employing the LanguageTools) user interface to display paraphrase suggestions.", "labels": [], "entities": []}, {"text": "We have used WordNet, PPDB, and JobimText DT 3 to provide paraphrase suggestions.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9753892421722412}, {"text": "JobimText DT 3", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.9173927108446757}]}, {"text": "Paraphrases are first obtained from each individual resources and irrelevant or out-ofcontext paraphrases are discarded by ranking alternatives using an n-gram language model.", "labels": [], "entities": []}, {"text": "Paraphrases suggested by most of the underlining resources (at least 2 out of 3) are provided as suggestions.", "labels": [], "entities": []}, {"text": "shows an online interface displaying paraphrase suggestions based on our approach . We have conducted experimental evaluation to assess the performance of the system using recall as a metric (recall = s r where sis the number of tokens in the source (paraphrased) sentence and r is the number of tokens in the reference sentence).", "labels": [], "entities": [{"text": "recall", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.9953277111053467}, {"text": "recall", "start_pos": 192, "end_pos": 198, "type": "METRIC", "confidence": 0.9941160678863525}]}, {"text": "We have used 100 sentences of paraphrase pairs (source and reference sentences) from the MSRPC dataset.", "labels": [], "entities": [{"text": "MSRPC dataset", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.9749382734298706}]}, {"text": "The baseline result is computed using the original paraphrase pairs of sentences which gives us a recall of 59%.", "labels": [], "entities": [{"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9997215867042542}]}, {"text": "We took the source sentence and applied our paraphrasing technique for words that are not in the reference sentence and computed recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9881579279899597}]}, {"text": "shows results for different settings, such as taking the first, top 5, and top 10 suggestions from the candidate paraphrases which outperforms the baseline result.", "labels": [], "entities": []}, {"text": "The combination of different resources improves the performance of the paraphrasing system.: Recall values for paraphrasing using different NLP resources and techniques.", "labels": [], "entities": []}, {"text": "Top 1 is where we consider only the best suggestion and compute the score.", "labels": [], "entities": []}, {"text": "top 5 and 10 considers the Top 5 and 10 suggestions provided by the system respectively.", "labels": [], "entities": []}, {"text": "The row 2in3 shows the result where we consider a paraphrase suggestion to be a candidate when it appears at least in two of the three resources.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation result for the German named entity  recognition task using an interactive online learning ap- proach with different sizes of training dataset tested on  the fixed development dataset.", "labels": [], "entities": [{"text": "German named entity  recognition task", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.5936137616634369}]}, {"text": " Table 2: Experimentation of interactive machine learning  for different precision and recall levels for Amharic POS  tagging task. The cell with the precision/recall intersec- tion records the total time (in minutes) required to fully  annotate the dataset with the help of interactive automa- tion. Without automation (no Auto.), annotation of all  sentences took 67 minutes.", "labels": [], "entities": [{"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9976446032524109}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9914152026176453}, {"text": "Amharic POS  tagging task", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.6791277006268501}, {"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9890387058258057}, {"text": "recall intersec-", "start_pos": 160, "end_pos": 176, "type": "METRIC", "confidence": 0.8591034611066183}]}, {"text": " Table 3: Recall values for paraphrasing using different  NLP resources and techniques. Top 1 is where we con- sider only the best suggestion and compute the score. top  5 and 10 considers the Top 5 and 10 suggestions provided  by the system respectively. The row 2in3 shows the result  where we consider a paraphrase suggestion to be a candi- date when it appears at least in two of the three resources.", "labels": [], "entities": []}]}