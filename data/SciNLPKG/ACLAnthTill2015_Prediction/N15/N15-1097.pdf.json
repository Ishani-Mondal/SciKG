{"title": [{"text": "So similar and yet incompatible: Toward automated identification of semantically compatible words", "labels": [], "entities": [{"text": "automated identification of semantically compatible words", "start_pos": 40, "end_pos": 97, "type": "TASK", "confidence": 0.7404641906420389}]}], "abstractContent": [{"text": "We introduce the challenge of detecting semantically compatible words, that is, words that can potentially refer to the same thing (cat and hindrance are compatible, cat and dog are not), arguing for its central role in many semantic tasks.", "labels": [], "entities": []}, {"text": "We present a publicly available data-set of human compatibility ratings, and a neural-network model that takes distributional embeddings of words as input and learns alternative embeddings that perform the compatibility detection task quite well.", "labels": [], "entities": [{"text": "compatibility detection", "start_pos": 206, "end_pos": 229, "type": "TASK", "confidence": 0.8281529545783997}]}], "introductionContent": [{"text": "Vectors encoding distributional information extracted from large text corpora provide very effective estimates of semantic similarity or, more generally, relatedness between words.", "labels": [], "entities": []}, {"text": "Semantic relatedness is undoubtedly a core property of word understanding, and indeed current vector-based distributional semantic models (DSMs) provide an impressive approximation to human judgments in many tasks ().", "labels": [], "entities": [{"text": "Semantic relatedness", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7813568711280823}, {"text": "word understanding", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8098320662975311}]}, {"text": "However, relatedness alone is too general a notion to truly capture the nuances of human conceptual knowledge.", "labels": [], "entities": []}, {"text": "The terms animal, puppy, and cat are all closely related to dog, but the nature of their relation is very different, each affording different inferences: If you tell me that Fido is a dog, I will also conclude that he's an animal, that he is not a cat, and that he might or might not be a puppy.", "labels": [], "entities": []}, {"text": "The previous examples hint at a fundamental semantic property that is only partially linked to relatedness, namely compatibility, that we define, for our current purposes, as follows: Linguistic expressions w 1 and w 2 are compatible iff, in a reasonably normal state of affairs, they can both truthfully refer to the same thing.", "labels": [], "entities": [{"text": "compatibility", "start_pos": 115, "end_pos": 128, "type": "METRIC", "confidence": 0.9584384560585022}]}, {"text": "If they cannot, then they are incompatible.", "labels": [], "entities": []}, {"text": "We realize that the notion of a \"reasonably normal sate of affairs\" is dangerously vague, but we want to exclude science-fiction scenarios in which dogs mutate into cats.", "labels": [], "entities": []}, {"text": "And we use thing as a catchall term for anything words (or other linguistic expressions) can refer to (entities, events, collections, etc.).", "labels": [], "entities": [{"text": "catchall term for anything words (or other linguistic expressions) can refer to (entities, events, collections, etc.)", "start_pos": 22, "end_pos": 139, "type": "Description", "confidence": 0.7787808153940283}]}, {"text": "The notions of compatibility and incompatibility have been introduced in theoretical semantics before.", "labels": [], "entities": []}, {"text": "The definition that we give here for compatibility is related, but different from the one by Cruse.", "labels": [], "entities": [{"text": "compatibility", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.7027168869972229}]}, {"text": "For example, subsuming pairs are out of the scope of compatibility under his definition, whereas we include them.", "labels": [], "entities": []}, {"text": "Murphy defines incompatibility similarly to us, but she does not define compatibility.", "labels": [], "entities": []}, {"text": "We are not aware, on the other hand, of any earlier systematic attempt to study the phenomenon empirically, nor to model it computationally.", "labels": [], "entities": []}, {"text": "In general, compatible terms will be semantically related (dog and animal).", "labels": [], "entities": []}, {"text": "However, relatedness does not suffice: many semantically related, even very similar terms are not compatible (dog and cat).", "labels": [], "entities": []}, {"text": "Relatedness is not even a necessary condition: A husband can be a hindrance in an all-too-normal state of affairs, but the concepts of husband and hindrance are not semantically close.", "labels": [], "entities": []}, {"text": "Moreover, compatibility does not reduce to (a set of) more commonly studied semantic relations.", "labels": [], "entities": [{"text": "compatibility", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.7203463912010193}]}, {"text": "While it relates to hy-pernymy, synonymy and co-hyponymy, there are cases, such as husband/hindrance, that do not naturally map to any of these relations.", "labels": [], "entities": []}, {"text": "Also, although many incompatibles among closely related pairs are co-hyponyms, this is not necessarily the case: You cannot be both a dog and a cat, but you can be a violinist and a drummer.", "labels": [], "entities": []}, {"text": "We argue that, since knowing what's compatible plays a central role inhuman semantic reasoning, algorithms that determine compatibility automatically will help in many domains that require human-like semantic knowledge.", "labels": [], "entities": [{"text": "semantic reasoning", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.754523903131485}]}, {"text": "Most obviously, compatibility is a necessary (although not sufficient) prerequisite for coreference.", "labels": [], "entities": [{"text": "compatibility", "start_pos": 16, "end_pos": 29, "type": "METRIC", "confidence": 0.9286649227142334}, {"text": "coreference", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.9590113162994385}]}, {"text": "Dog and puppy could belong to the same coreference chain, whereas dog and cat do not.", "labels": [], "entities": []}, {"text": "We conjecture that the relatively disappointing performance of DSMs in support of coreference resolution () is at least partially due to the inability of standard DSMs to distinguish compatible and incompatible terms.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.95749631524086}]}, {"text": "Compatibility is also central to recognizing entailment (and contradiction): Standard DSMs are of relatively little use in recognizing entailment as they treat antonymous, contradictory words such as dead and alive as highly related, with catastrophic results for the inferences that can be drawn (antonyms are just the tip of the incompatibility iceberg: dog and cat are not antonyms, but one still contradicts the other).", "labels": [], "entities": []}, {"text": "Knowing what's compatible might also help in tasks that require recognizing (distant) paraphrases, such as question answering, document summarization or even machine translation (the violinist also played the drum might corefer with the drummer also played the violin, whereas the dog was killed and the cat was killed must refer to different events).", "labels": [], "entities": [{"text": "question answering", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8738203346729279}, {"text": "document summarization", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.6265043318271637}, {"text": "machine translation", "start_pos": 158, "end_pos": 177, "type": "TASK", "confidence": 0.7229870408773422}]}, {"text": "Other applications could include modeling semantic plausibility of a nominal phrase, where the goal is to accept expressions like coastal mosquito, but reject parlamentary tomato.", "labels": [], "entities": []}, {"text": "Finally, the notion of incompatibility relates to (certain kinds of) negation.", "labels": [], "entities": []}, {"text": "Negation is notoriously difficult to model with DSMs (, and compatibility might offer anew angle into it.", "labels": [], "entities": [{"text": "Negation", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9623709917068481}]}, {"text": "In this paper, we introduce anew, large benchmark to evaluate computational models on compatibility detection.", "labels": [], "entities": [{"text": "compatibility detection", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.959355354309082}]}, {"text": "We then present a supervised neural-network based model that takes distributional semantic vectors as input and embeds them into a space that is optimized for compatibility detection.", "labels": [], "entities": [{"text": "compatibility detection", "start_pos": 159, "end_pos": 182, "type": "TASK", "confidence": 0.8826904594898224}]}, {"text": "The model performs significantly better than direct DSM relatedness, and achieves high scores in absolute terms.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since compatibility is asymmetric relation, we first duplicated each pair in the benchmark by swapping the two words.", "labels": [], "entities": []}, {"text": "We then split it into training, testing and development sections.", "labels": [], "entities": []}, {"text": "To make the task more challenging, we enforced disjoint vocabularies in each of them.", "labels": [], "entities": []}, {"text": "For example, drummer only occurs in the training set, while ant, only in the test set.", "labels": [], "entities": [{"text": "drummer", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9556320905685425}]}, {"text": "We use about 1/10th of the vocabulary (29 words) on the development set and the rest was split equally between train and test (135 words each).", "labels": [], "entities": []}, {"text": "The resulting partitions contain 7,228 (train), 7,336 (test) and 312 (development) pairs, respectively.", "labels": [], "entities": []}, {"text": "To train the models, we used the scores they generate in three sub-tasks: approximation of average ratings, classification of compatibles and classification of incompatibles.", "labels": [], "entities": []}, {"text": "We used mean square error as cost function for the first sub-task, cross-entropy for the latter two.", "labels": [], "entities": [{"text": "mean square error", "start_pos": 8, "end_pos": 25, "type": "METRIC", "confidence": 0.7880623737970988}]}, {"text": "We implemented the models in Torch7 (Collobert et al., 2011).", "labels": [], "entities": [{"text": "Torch7", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9431807398796082}]}, {"text": "We trained them for 120 epochs with adagrad, with a batch size of 150 items and adopting an emphasizing scheme (, where compatibles, incompatibles and middle-ground items appear in equal proportions.", "labels": [], "entities": []}, {"text": "We fixed hidden-layer size to 100 dimensions, while we tuned a coefficient fora L2-norm regularization term on the development data.", "labels": [], "entities": []}, {"text": "We evaluated the models ability to predict human compatibility ratings as well as to detect compatible and incompatible items.", "labels": [], "entities": []}, {"text": "We compared the supervised measures to the cosine of pairs directly represented by their DSM vectors (with thresholds tuned on the training set).", "labels": [], "entities": []}, {"text": "We expected this baseline to fare relatively well on incompatibility detection, since many of our randomly generated pairs were both incompatible and dissimilar (e.g., bag/bus).", "labels": [], "entities": [{"text": "incompatibility detection", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7410214245319366}]}, {"text": "Also, we controlled for the portion of the data that can be accounted just by looking atone of the words of the relation (for example, the presence of a word might indicate that the relation is incompatible).", "labels": [], "entities": []}, {"text": "To this end, we included two models that look at only one of the words in the pair.", "labels": [], "entities": []}, {"text": "1L mono is a logistic regression model that only looks at the first word of the pair while 2L mono is an analogous neural network with one hidden layer.", "labels": [], "entities": []}, {"text": "As it can be seen, all the supervised models from strongly outperform the cosine (that, as expected, is nevertheless quite good at detecting incompatibles).", "labels": [], "entities": []}, {"text": "Also, they outperform the mono models (with the only exception of 1L direct on incompatibility), showing that the data they account for cannot be reduced to properties of individual lexical items.", "labels": [], "entities": []}, {"text": "Importantly, the 2L interaction model is way ahead of all other models, confirming our expectations.", "labels": [], "entities": []}, {"text": "To gain some insight into the features learned by the best model, we labeled the words of our input vocabulary with one of the following general category tags: animal, artefact, general-function, human, organic-and-food and place.", "labels": [], "entities": []}, {"text": "The distribution, where no obvious pattern emerges.", "labels": [], "entities": []}, {"text": "If instead we plot the output vectors of 2L interaction mapping in the same way, we obtain the heatmap in.", "labels": [], "entities": []}, {"text": "It is evident that the mapping produces vectors that are similar within most categories, and very different across them.", "labels": [], "entities": []}, {"text": "Thus, the 2L interaction model clearly learned the relevance of general categories in capturing compatibility judgments.", "labels": [], "entities": []}, {"text": "The fact that this model produced the best results hints at the importance of exploiting this source of information, confirming the intuition we used in designing it, that compatibility can be characterized by a combination of general relatedness and category-specific cues.", "labels": [], "entities": []}, {"text": "Finally, we explored to what extent the data can be accounted by co-hyponymy, an idea briefly introduced in the introductory discussion of Section 1.", "labels": [], "entities": []}, {"text": "For simplicity purposes, we take the same category tags we just introduced as a word's hypernym.", "labels": [], "entities": []}, {"text": "Classifying co-hyponyms as incompatibles and noncohyponyms as compatibles performs very poorly (7 and 18 F1-scores for compatibility and incompatibility, respectively).", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9986968636512756}]}, {"text": "On the other hand, the opposite strategy -co-hyponyms as compatibles and noncohyponyms as incompatibles -works much better (62 and 84 F1), even outperforming many supervised models.", "labels": [], "entities": [{"text": "F1", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.9890763759613037}]}, {"text": "Yet, this strategy does not suffice.", "labels": [], "entities": []}, {"text": "For example, all animal pairs would be treated as compatibles, whereas 54% of them are actually incompatible.", "labels": [], "entities": []}, {"text": "By contrast the L2 interaction model gets 78% of these incompatible pairs right.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results.  Correlation  with human ratings measured by Pearson r.  (In)compatibility detection scored by the F1 mea- sure.", "labels": [], "entities": [{"text": "Pearson r", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9769190847873688}, {"text": "compatibility detection", "start_pos": 93, "end_pos": 116, "type": "METRIC", "confidence": 0.740810364484787}, {"text": "F1 mea- sure", "start_pos": 131, "end_pos": 143, "type": "METRIC", "confidence": 0.9358872175216675}]}]}