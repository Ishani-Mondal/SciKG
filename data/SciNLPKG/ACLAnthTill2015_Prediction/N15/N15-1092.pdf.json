{"title": [{"text": "Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval", "labels": [], "entities": [{"text": "Representation Learning", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9549594521522522}, {"text": "Semantic Classification", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.7550664842128754}, {"text": "Information Retrieval", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.6937078535556793}]}], "abstractContent": [{"text": "Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks.", "labels": [], "entities": []}, {"text": "However, inmost previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or single-task supervised objectives, which often suffer from insufficient training data.", "labels": [], "entities": []}, {"text": "We develop a multi-task DNN for learning representations across multiple tasks, not only leverag-ing large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains.", "labels": [], "entities": []}, {"text": "Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation.", "labels": [], "entities": [{"text": "query classification", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.7378543019294739}, {"text": "information retrieval", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.7173123806715012}]}], "introductionContent": [{"text": "Recent advances in deep neural networks (DNNs) have demonstrated the importance of learning vector-space representations of text, e.g., words and sentences, fora number of natural language processing tasks.", "labels": [], "entities": []}, {"text": "For example, the study reported in) demonstrated significant accuracy gains in tagging, named entity recognition, and semantic role labeling when using vector space word * This research was conducted during the author's internship at Microsoft Research.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9975554347038269}, {"text": "named entity recognition", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.6294434567292532}, {"text": "semantic role labeling", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.6374769608179728}]}, {"text": "representations learned from large corpora.", "labels": [], "entities": []}, {"text": "Further, since these representations are usually in a lowdimensional vector space, they result in more compact models than those built from surface-form features.", "labels": [], "entities": []}, {"text": "A recent successful example is the parser by), which is not only accurate but also fast.", "labels": [], "entities": [{"text": "accurate", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9794559478759766}]}, {"text": "However, existing vector-space representation learning methods are far from optimal.", "labels": [], "entities": [{"text": "vector-space representation learning", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.7200442751248678}]}, {"text": "Most previous methods are based on unsupervised objectives such as word prediction for training ().", "labels": [], "entities": [{"text": "word prediction", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7688145041465759}]}, {"text": "Other methods use supervised training objectives on a single task, e.g. (, and thus are often constrained by limited amounts of training data.", "labels": [], "entities": []}, {"text": "Motivated by the success of multi-task learning, we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.9461487829685211}]}, {"text": "In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks.", "labels": [], "entities": []}, {"text": "Our contributions are of two-folds: First, we propose a multi-task deep neural network for representation learning, in particular focusing on semantic classification (query classification) and semantic information retrieval (ranking for web search) tasks.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.9366268515586853}, {"text": "semantic classification (query classification)", "start_pos": 142, "end_pos": 188, "type": "TASK", "confidence": 0.7740904291470846}, {"text": "semantic information retrieval (ranking for web search) tasks", "start_pos": 193, "end_pos": 254, "type": "TASK", "confidence": 0.7478847056627274}]}, {"text": "Our model learns to map arbitrary text queries and documents into semantic vector representations in a low dimensional latent space.", "labels": [], "entities": []}, {"text": "While the general concept of multi-task neural nets is not new, our model is novel in that it successfully combines tasks as disparate as operations necessary for classifica-tion or ranking.", "labels": [], "entities": []}, {"text": "Second, we demonstrate strong results on query classification and web search.", "labels": [], "entities": [{"text": "query classification", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.8485607504844666}]}, {"text": "Our multi-task representation learning consistently outperforms stateof-the-art baselines.", "labels": [], "entities": []}, {"text": "Meanwhile, we show that our model is not only compact but it also enables agile deployment into new domains.", "labels": [], "entities": []}, {"text": "This is because the learned representations allow domain adaptation with substantially fewer in-domain labels.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7726560533046722}]}], "datasetContent": [{"text": "We employ large-scale, real data sets in our evaluation.", "labels": [], "entities": []}, {"text": "The test data for query classification were sampled from one-year log files of a commercial search engine with labels (yes or no) judged by humans.", "labels": [], "entities": [{"text": "query classification", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.9271295070648193}]}, {"text": "The test data for web search contains 12,071 English queries, where each query-document pair has a relevance label manually annotated on a 5-level relevance scale: bad, fair,).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the data sets used in the experiments.", "labels": [], "entities": []}, {"text": " Table 2: Query Classification AUC results.", "labels": [], "entities": [{"text": "Query Classification AUC", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6613980730374655}]}, {"text": " Table 3: Web Search NDCG results. Here, * indicates statistical significance improvement compared to the  best baseline (DSSM) measured by t-test at p-value of 0.05.", "labels": [], "entities": [{"text": "Web Search NDCG", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.6420626242955526}]}]}