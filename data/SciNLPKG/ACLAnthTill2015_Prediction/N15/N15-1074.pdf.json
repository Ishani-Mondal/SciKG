{"title": [{"text": "Incorporating Word Correlation Knowledge into Topic Modeling", "labels": [], "entities": [{"text": "Incorporating Word Correlation Knowledge", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7920199781656265}, {"text": "Topic Modeling", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.739600270986557}]}], "abstractContent": [{"text": "This paper studies how to incorporate the external word correlation knowledge to improve the coherence of topic modeling.", "labels": [], "entities": []}, {"text": "Existing topic models assume words are generated independently and lack the mechanism to utilize the rich similarity relationships among words to learn coherent topics.", "labels": [], "entities": []}, {"text": "To solve this problem , we build a Markov Random Field (MRF) regularized Latent Dirichlet Allocation (LDA) model, which defines a MRF on the latent topic layer of LDA to encourage words labeled as similar to share the same topic label.", "labels": [], "entities": []}, {"text": "Under our model, the topic assignment of each word is not independent, but rather affected by the topic labels of its correlated words.", "labels": [], "entities": []}, {"text": "Similar words have better chance to be put into the same topic due to the regularization of MRF, hence the coherence of topics can be boosted.", "labels": [], "entities": [{"text": "MRF", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.48721325397491455}]}, {"text": "In addition, our model can accommodate the subtlety that whether two words are similar depends on which topic they appear in, which allows word with multiple senses to be put into different topics properly.", "labels": [], "entities": []}, {"text": "We derive a vari-ational inference method to infer the posterior probabilities and learn model parameters and present techniques to deal with the hard-to-compute partition function in MRF.", "labels": [], "entities": [{"text": "MRF", "start_pos": 184, "end_pos": 187, "type": "DATASET", "confidence": 0.7768682837486267}]}, {"text": "Experiments on two datasets demonstrate the effectiveness of our model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic topic models (PTM), such as probabilistic latent semantic indexing(PLSI)) and latent Dirichlet allocation(LDA) ( have shown great success in documents modeling and analysis.", "labels": [], "entities": []}, {"text": "Topic models posit document collection exhibits multiple latent semantic topics where each topic is represented as a multinomial distribution over a given vocabulary and each document is a mixture of hidden topics.", "labels": [], "entities": []}, {"text": "To generate a document d, PTM first samples a topic proportion vector, then for each word win d, samples a topic indicator z and generates w from the topic-word multinomial corresponding to topic z.", "labels": [], "entities": [{"text": "PTM", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.8485087752342224}]}, {"text": "A key limitation of the existing PTMs is that words are assumed to be uncorrelated and generated independently.", "labels": [], "entities": []}, {"text": "The topic assignment for each word is irrelevant to all other words.", "labels": [], "entities": []}, {"text": "While this assumption facilitates computational efficiency, it loses the rich correlations between words.", "labels": [], "entities": []}, {"text": "In many applications, users have external knowledge regarding word correlation, which can betaken into account to improve the semantic coherence of topic modeling.", "labels": [], "entities": [{"text": "word correlation", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.7655251920223236}]}, {"text": "For example, WordNet presents a large amount of synonym relationships between words, Wikipedia 1 provides a knowledge graph by linking correlated concepts together and named entity recognizer identifies the categories of entity mentions.", "labels": [], "entities": []}, {"text": "All of these external knowledge can be leveraged to learn more coherent topics if we can design a mechanism to encourage similar words, correlated concepts, entities of the same category to be assigned to the same topic.", "labels": [], "entities": []}, {"text": "Many approaches () have attempted to solve this problem by enforcing hard and topic-independent rules that similar words should have similar probabilities in all topics, which is questionable in that two words with similar representativeness of one topic are not necessarily of equal importance for another topic.", "labels": [], "entities": []}, {"text": "For example, in the fruit topic, the words apple and orange have similar representativeness, while in an IT company topic, apple has much higher importance than orange.", "labels": [], "entities": []}, {"text": "As another example, church and bible are similarly relevant to a religion topic, whereas their relevance to an architecture topic are vastly different.", "labels": [], "entities": []}, {"text": "Exiting approaches are unable to differentiate the subtleties of word sense across topics and would falsely put irrelevant words into the same topic.", "labels": [], "entities": []}, {"text": "For instance, since orange and microsoft are both labeled as similar to apple and are required to have similar probabilities in all topics as apple has, in the end, they will be unreasonably allocated to the same topic.", "labels": [], "entities": []}, {"text": "The existing approaches fail to properly use the word correlation knowledge, which is usually a list of word pairs labeled as similar.", "labels": [], "entities": [{"text": "word correlation", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.7141534686088562}]}, {"text": "The similarity is computed based on statistics such as co-occurrence which are unable to accommodate the subtlety that whether two words labeled as similar are truly similar depends on which topic they appear in, as explained by the aforementioned examples.", "labels": [], "entities": []}, {"text": "Ideally, the knowledge would be word A and B are similar under topic C.", "labels": [], "entities": []}, {"text": "However, in reality, we only know two words are similar, but not under which topic.", "labels": [], "entities": []}, {"text": "In this paper, we aim to abridge this gap.", "labels": [], "entities": []}, {"text": "Gaining insights from, we design a Markov Random Field regularized LDA model (MRF-LDA) which utilizes the external knowledge in a soft and topic-dependent manner to improve the coherence of topic modeling.", "labels": [], "entities": []}, {"text": "We define a MRF on the latent topic layer of LDA to encode word correlations.", "labels": [], "entities": []}, {"text": "Within a document, if two words are labeled as similar according to the external knowledge, their latent topic nodes will be connected by an undirected edge and a binary potential function is defined to encourage them to share the same topic label.", "labels": [], "entities": []}, {"text": "This mechanism gives correlated words a better chance to be put into the same topic, thereby, improves the coherence of the learned topics.", "labels": [], "entities": []}, {"text": "Our model provides a mechanism to automatically decide under which topic, two words labeled as similar are truly similar.", "labels": [], "entities": []}, {"text": "We encourage words labeled as similar to share the same topic label, but do not specify which topic label they should share, and leave this to be decided by data.", "labels": [], "entities": []}, {"text": "In the above mentioned apple, orange, microsoft example, we encourage apple and orange to share the same topic label A and try to push apple and microsoft to the same topic B.", "labels": [], "entities": []}, {"text": "But A and B are not necessarily the same and they will be inferred according to the fitness of data.", "labels": [], "entities": []}, {"text": "Different from the existing approaches which directly use the word similarities to control the topic-word distributions in a hard and topic-independent way, our method imposes constraints on the latent topic layer by which the topic-word multinomials are influenced indirectly and softly and are topic-aware.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce related work.", "labels": [], "entities": []}, {"text": "In Section 3, we propose the MRF-LDA model and present the variational inference method.", "labels": [], "entities": []}, {"text": "Section 4 gives experimental results.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we corroborate the effectiveness of our model by comparing it with three baseline methods on two datasets.", "labels": [], "entities": []}, {"text": "\u2022 Dataset: We use two datasets in the experiments: 20-Newsgroups 2 and NIPS 3 . Their statistics are summarized in.", "labels": [], "entities": [{"text": "NIPS", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.8614641427993774}]}, {"text": "\u2022 External Knowledge: We extract word correlation knowledge from Web Eigenwords 4 , where each word has a real-valued vector capturing the semantic meaning of this word based on distributional similarity.", "labels": [], "entities": []}, {"text": "Two words are regarded as correlated if their representation vectors are similar enough.", "labels": [], "entities": []}, {"text": "It is worth mentioning that, other sources of external word correlation knowledge, such as Word2Vec () and Glove (), can be readily incorporated into MRF-LDA.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.9769894480705261}, {"text": "MRF-LDA", "start_pos": 150, "end_pos": 157, "type": "DATASET", "confidence": 0.7736137509346008}]}, {"text": "\u2022 Baselines: We compare our model with three baseline methods: LDA (, DF-LDA () and Quad-LDA).", "labels": [], "entities": []}, {"text": "LDA is the most widely used topic model, but it is unable to incorporate external knowledge.", "labels": [], "entities": [{"text": "LDA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.637937605381012}]}, {"text": "DF-LDA and Quad-LDA are two models designed to incorporate word correlation to improve topic modeling.", "labels": [], "entities": [{"text": "DF-LDA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.784187912940979}, {"text": "topic modeling", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.7542585134506226}]}, {"text": "DF-LDA puts a Dirichlet Forest prior over the topic-word multinomials to encode the Must-Links and Cannot-Links between words.", "labels": [], "entities": [{"text": "DF-LDA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8882938027381897}]}, {"text": "Quad-LDA regularizes the topic-word distributions with a structured prior to incorporate word relation.", "labels": [], "entities": []}, {"text": "\u2022 Parameter Settings: For all methods, we learn 100 topics.", "labels": [], "entities": [{"text": "Parameter Settings", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.679297536611557}]}, {"text": "LDA parameters are set to their default settings in ().", "labels": [], "entities": []}, {"text": "For DF-LDA, we set its parameters as \u03b1 = 1, \u03b2 = 0.01 and \u03b7 = 100.", "labels": [], "entities": [{"text": "DF-LDA", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.7903447151184082}]}, {"text": "The Must/Cannot links between words are generated based on the cosine similarity of words' vector representations in Web Eigenwords.", "labels": [], "entities": []}, {"text": "Word pairs with similarity higher than 0.99 are set as Must-Links, and pairs with similarity lower than 0.1 are put into Cannot-Link set.", "labels": [], "entities": []}, {"text": "For Quad-LDA, \u03b2 is set as 0.01; \u03b1 is defined as 0.05\u00b7N D\u00b7T , where N is the total occurrences of all words in all documents, Dis the number of documents and T is topic number.", "labels": [], "entities": []}, {"text": "For MRF-LDA, word pairs with similarity higher than 0.99 are labeled as correlated.", "labels": [], "entities": []}, {"text": "Our method provides a mechanism to automatically decide under which topic, two words labeled as similar are truly similar.", "labels": [], "entities": []}, {"text": "The decision is made flexibly by data according to their fitness to the model, rather than by a hard rule adopted by DF-LDA and Quad-LDA.", "labels": [], "entities": [{"text": "DF-LDA", "start_pos": 117, "end_pos": 123, "type": "DATASET", "confidence": 0.847575843334198}, {"text": "Quad-LDA", "start_pos": 128, "end_pos": 136, "type": "DATASET", "confidence": 0.8962727189064026}]}, {"text": "For instance, according to the external knowledge, the word child is correlated with gun and with men simultaneously.", "labels": [], "entities": []}, {"text": "Under a crime topic, child and gun are truly correlated because they cooccur a lot in youth crime news, whereas, child and men are less correlated in this topic.", "labels": [], "entities": []}, {"text": "Under a sex topic, child and men are truly correlated whereas child and gun are not.", "labels": [], "entities": []}, {"text": "Our method can differentiate this subtlety and successfully put child and gun into the crime topic and put child and men into the sex topic.", "labels": [], "entities": []}, {"text": "This is because our method encourages child and gun to be put into the same topic A and encourages child and men to be put into the same topic B, but does not require A and B to be the same.", "labels": [], "entities": []}, {"text": "A and B are freely decided by data.", "labels": [], "entities": [{"text": "A", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.8901591300964355}]}, {"text": "shows some topics learned on NIPS dataset.", "labels": [], "entities": [{"text": "NIPS dataset", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.9564986228942871}]}, {"text": "The four topics correspond to vision, neural network, speech recognition and electronic circuits respectively.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7890809178352356}]}, {"text": "From this table, we observe that the topics learned by our method are better in coherence than those learned from the baseline methods, which again demonstrates the effectiveness of our model.", "labels": [], "entities": []}, {"text": "We also evaluate our method in a quantitative manner.", "labels": [], "entities": []}, {"text": "Similar to, we use the coherence measure (CM) to assess how coherent the learned topics are.", "labels": [], "entities": [{"text": "coherence measure (CM)", "start_pos": 23, "end_pos": 45, "type": "METRIC", "confidence": 0.818018662929535}]}, {"text": "For each topic, we pickup the top 10 candidate words and ask human annotators to judge whether they are relevant to the topic.", "labels": [], "entities": []}, {"text": "First, annotators needs to judge whether a topic is interpretable or not.", "labels": [], "entities": []}, {"text": "If not, the ten candidate words in this topic are automatically labeled as irrelevant.", "labels": [], "entities": []}, {"text": "Otherwise, annotators are asked to identify words that are relevant to this topic.", "labels": [], "entities": []}, {"text": "Coherence measure (CM) is defined as the ratio between the number of relevant words and total number of candidate words.", "labels": [], "entities": [{"text": "Coherence measure (CM)", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.7963710844516754}]}, {"text": "In our experiments, four graduate students participated the labeling.", "labels": [], "entities": [{"text": "labeling", "start_pos": 60, "end_pos": 68, "type": "TASK", "confidence": 0.9737425446510315}]}, {"text": "For each dataset and each method, 10% of topics were randomly chosen for labeling. and 5 summarize the coherence measure of topics learned on 20-Newsgroups dataset and NIPS dataset respectively.", "labels": [], "entities": [{"text": "NIPS dataset", "start_pos": 168, "end_pos": 180, "type": "DATASET", "confidence": 0.9827376902103424}]}, {"text": "As shown in the table, our method significantly outperforms the baseline methods with a large margin.", "labels": [], "entities": []}, {"text": "On the 20-Newsgroups dataset, our method achieves an average coherence measure of 60.8%, which is two times better than LDA.", "labels": [], "entities": [{"text": "20-Newsgroups dataset", "start_pos": 7, "end_pos": 28, "type": "DATASET", "confidence": 0.898884117603302}, {"text": "LDA", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9185062050819397}]}, {"text": "On the NIPS dataset, our method is also much better than the baselines.", "labels": [], "entities": [{"text": "NIPS dataset", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9665848910808563}]}, {"text": "In summary, we conclude that MRF-LDA produces much better results on both datasets compared to baselines, which demonstrates the effectiveness of our model in exploiting word correlation knowledge to improve the quality of topic modeling.", "labels": [], "entities": []}, {"text": "To assess the consistency of the labelings made by different annotators, we computed the intraclass correlation coefficient (ICC).", "labels": [], "entities": [{"text": "consistency", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9814419150352478}, {"text": "intraclass correlation coefficient (ICC)", "start_pos": 89, "end_pos": 129, "type": "METRIC", "confidence": 0.8969324827194214}]}, {"text": "The ICCs on 20-Newsgroups and NIPS dataset are 0.925 and 0.725 respectively, which indicate good agreement between different annotators.", "labels": [], "entities": [{"text": "ICCs", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9939437508583069}, {"text": "NIPS dataset", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9809972047805786}]}], "tableCaptions": [{"text": " Table 4: CM (%) on 20-Newsgroups Dataset", "labels": [], "entities": [{"text": "CM", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.7382798790931702}]}]}