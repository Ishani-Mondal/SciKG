{"title": [], "abstractContent": [{"text": "For instance , two words maybe similar with respect to semantics, syntax, or morphology inter alia.", "labels": [], "entities": []}, {"text": "Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree.", "labels": [], "entities": []}, {"text": "This work considers guiding word-embeddings with morphologically annotated data, a form of semi-supervised learning, encouraging the vectors to encode a word's morphology, i.e., words close in the embedded space share morphological features.", "labels": [], "entities": []}, {"text": "We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using Ger-man as a case study.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word representation is fundamental for NLP.", "labels": [], "entities": [{"text": "Word representation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7041761577129364}]}, {"text": "Recently, continuous word-embeddings have gained traction as a general-purpose representation framework.", "labels": [], "entities": []}, {"text": "While such embeddings have proven themselves useful, they typically treat words holistically, ignoring their internal structure.", "labels": [], "entities": []}, {"text": "For morphologically impoverished languages, i.e., languages with a low morpheme-per-word ratio such as English, this is often not a problem.", "labels": [], "entities": []}, {"text": "However, for the processing of morphologically-rich languages exploiting wordinternal structure is necessary.", "labels": [], "entities": []}, {"text": "Word-embeddings are typically trained to produce representations that capture linguistic similarity.", "labels": [], "entities": []}, {"text": "The general idea is that words that are close in the embedding space should be close in meaning.", "labels": [], "entities": []}, {"text": "A key issue, however, is that meaning is a multifaceted concept and thus there are multiple axes, along which two words can be similar.", "labels": [], "entities": []}, {"text": "For example, ice and cold are topically related, ice and fire are syntactically related as they are both nouns, and ice and icy are morphologically related as they are both derived from the same root.", "labels": [], "entities": []}, {"text": "In this work, we are interested in distinguishing between these various axes and guiding the embeddings such that similar embeddings are morphologically related.", "labels": [], "entities": []}, {"text": "We augment the log-bilinear model (LBL) of Mnih and Hinton (2007) with a multi-task objective.", "labels": [], "entities": []}, {"text": "In addition to raw text, our model is trained on a corpus annotated with morphological tags, encouraging the vectors to encode a word's morphology.", "labels": [], "entities": []}, {"text": "To be concrete, the first task is language modelingthe traditional use of the LBL-and the second is akin to unigram morphological tagging.", "labels": [], "entities": [{"text": "language modelingthe", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7592993974685669}, {"text": "unigram morphological tagging", "start_pos": 108, "end_pos": 137, "type": "TASK", "confidence": 0.6390802462895712}]}, {"text": "The LBL, described in section 3, is fundamentally a language model (LM)-word-embeddings fallout as low dimensional representations of context used to predict the next word.", "labels": [], "entities": []}, {"text": "We extend the model to jointly predict the next morphological tag along with the next word, encouraging the resulting embeddings to encode morphology.", "labels": [], "entities": []}, {"text": "We present a novel metric and experiments on German as a case study that demonstrates that our approach produces wordembeddings that better preserve morphological relationships.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our evaluation, we attempt to intrinsically determine whether it is indeed true that words similar in the embedding space are morphologically related.", "labels": [], "entities": []}, {"text": "Qualitative evaluation, shown in, indicates that this is the case.", "labels": [], "entities": [{"text": "Qualitative", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9799581170082092}]}, {"text": "To show the potential of our approach, we chose to perform a case study on German, a morphologicallyrich language.", "labels": [], "entities": []}, {"text": "We conducted experiments on the TIGER corpus of newspaper German ().", "labels": [], "entities": [{"text": "TIGER corpus of newspaper German", "start_pos": 32, "end_pos": 64, "type": "DATASET", "confidence": 0.8572320640087128}]}, {"text": "To the best of our knowledge, no previous word-embedding techniques have attempted to incorporate morphological tags into embeddings in a supervised fashion.", "labels": [], "entities": []}, {"text": "We note again that there has been recent work on incorporating morphological segmentations into embeddings-generally in a pipelined approach using a segmenter, e.g., MOR-FESSOR, as a preprocessing step, but we distinguish our model through its use of a different view on morphology.", "labels": [], "entities": []}, {"text": "We opted to compare Morph-LBL with two fully unsupervised models: the original LBL and WORD2VEC (code.google.com/p/word2vec/,).", "labels": [], "entities": []}, {"text": "All models were trained on the first 200k words of the train split of the TIGER corpus; Morph-LBL was given the correct morphological annotation for the first 100k words.", "labels": [], "entities": [{"text": "TIGER corpus", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.885980635881424}]}, {"text": "The LBL and Morph-LBL models were implemented in Python using THEANO).", "labels": [], "entities": [{"text": "THEANO", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.955868661403656}]}, {"text": "All vectors had dimensionality 200.", "labels": [], "entities": []}, {"text": "We used the SkipGram model of the WORD2VEC toolkit with context n = 5.", "labels": [], "entities": [{"text": "WORD2VEC toolkit", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.9193643629550934}]}, {"text": "We initialized parameters of LBL: We examined to what extent the individual embeddings store morphological information.", "labels": [], "entities": []}, {"text": "To quantify this, we treated the problem as supervised multi-way classification with the embedding as the input and the morphological tag as the output to predict.", "labels": [], "entities": []}, {"text": "Note that \"All Types\" refers to all types in the training corpus and \"No Tags\" refers to the subset of types, whose morphological tag was not seen by Morph-LBL at training time. and Morph-LBL randomly and trained them using stochastic gradient descent.", "labels": [], "entities": []}, {"text": "We used a history size of n = 4.", "labels": [], "entities": []}, {"text": "We first investigated whether the embeddings learned by Morph-LBL do indeed encode morphological information.", "labels": [], "entities": []}, {"text": "For each word, we selected the most frequently occurring morphological tag for that word (ties were broken randomly).", "labels": [], "entities": []}, {"text": "We then treated the problem of labeling a word-embedding with its most frequent morphological tag as a multiway classification problem.", "labels": [], "entities": []}, {"text": "We trained a k nearest neighbors classifier where k was optimized on development data.", "labels": [], "entities": []}, {"text": "We used the scikit-learn library (Pedregosa et al., 2011) on all types in the vocabulary with 10-fold cross-validation, holding out 10% of the data for testing at each fold and an additional 10% of training as a development set.", "labels": [], "entities": []}, {"text": "The results displayed in table 2 are broken down by whether MorphLBL observed the morphological tag at training time or not.", "labels": [], "entities": []}, {"text": "We see that embeddings from Morph-LBL do store the proper morphological analysis at a much higher rate than both the vanilla LBL and WORD2VEC.", "labels": [], "entities": [{"text": "WORD2VEC", "start_pos": 133, "end_pos": 141, "type": "DATASET", "confidence": 0.8601862192153931}]}, {"text": "Word-embeddings, however, are often trained on massive amounts of unlabeled data.", "labels": [], "entities": []}, {"text": "To this end, we also explored on how WORD2VEC itself encodes morphology, when trained on an order of magnitude more data.", "labels": [], "entities": [{"text": "WORD2VEC", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.7976555824279785}]}, {"text": "Using the same experimental setup as above, we trained WORD2VEC on the union of the TIGER German corpus and German section of Europarl () fora total of \u2248 45 million tokens.", "labels": [], "entities": [{"text": "TIGER German corpus", "start_pos": 84, "end_pos": 103, "type": "DATASET", "confidence": 0.8465606967608134}, {"text": "German section of Europarl", "start_pos": 108, "end_pos": 134, "type": "DATASET", "confidence": 0.7423527836799622}]}, {"text": "Looking only at those types found in TIGER, we found that the k-NN classifier predicted the cor- rect tag with \u2248 22% accuracy (not shown in the table).", "labels": [], "entities": [{"text": "TIGER", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.7290427684783936}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9990460276603699}]}, {"text": "We also evaluated the three types of embeddings using the MORPHODIST metric introduced in section 5.1.", "labels": [], "entities": [{"text": "MORPHODIST", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9639127850532532}]}, {"text": "This metric roughly tells us how similar each word is to its neighbors, where distance is measured in the Hamming distance between morphological tags.", "labels": [], "entities": []}, {"text": "We only evaluated on words that MorphLBL did not observe at training time to get a fair idea of how well our model has managed to encode morphology purely from the contextual signature.", "labels": [], "entities": []}, {"text": "reports results fork \u2208 {5, 10, 25, 50} nearest neighbors.", "labels": [], "entities": []}, {"text": "We see that the values of k studied do not affect the metric-the closest 5 words are about as similar as the closest 50 words.", "labels": [], "entities": []}, {"text": "We see again that the Morph-LBL embeddings generally encode morphology better than the baselines.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: We examined to what extent the individual em- beddings store morphological information. To quantify  this, we treated the problem as supervised multi-way  classification with the embedding as the input and the  morphological tag as the output to predict. Note that \"All  Types\" refers to all types in the training corpus and \"No  Tags\" refers to the subset of types, whose morphological  tag was not seen by Morph-LBL at training time.", "labels": [], "entities": []}]}