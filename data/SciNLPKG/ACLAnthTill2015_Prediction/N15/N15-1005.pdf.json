{"title": [{"text": "Randomized Greedy Inference for Joint Segmentation, POS Tagging and Dependency Parsing", "labels": [], "entities": [{"text": "Joint Segmentation", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.8148065209388733}, {"text": "POS Tagging", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.7678368091583252}, {"text": "Dependency Parsing", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.6804103404283524}]}], "abstractContent": [{"text": "In this paper, we introduce anew approach for joint segmentation, POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.8956210017204285}, {"text": "dependency parsing", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.836901992559433}]}, {"text": "While joint modeling of these tasks addresses the issue of error propagation inherent in traditional pipeline archi-tectures, it also complicates the inference task.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7072654515504837}]}, {"text": "Past research has addressed this challenge by placing constraints on the scoring function.", "labels": [], "entities": []}, {"text": "In contrast, we propose an approach that can handle arbitrarily complex scoring functions.", "labels": [], "entities": []}, {"text": "Specifically, we employ a randomized greedy algorithm that jointly predicts segmentations, POS tags and dependency trees.", "labels": [], "entities": []}, {"text": "Moreover, this architecture readily handles different seg-mentation tasks, such as morphological seg-mentation for Arabic and word segmentation for Chinese.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.7366209179162979}]}, {"text": "The joint model outperforms the state-of-the-art systems on three datasets, obtaining 2.1% TedEval absolute gain against the best published results in the 2013 SPMRL shared task.", "labels": [], "entities": [{"text": "TedEval absolute", "start_pos": 91, "end_pos": 107, "type": "METRIC", "confidence": 0.8868293166160583}, {"text": "SPMRL shared task", "start_pos": 160, "end_pos": 177, "type": "TASK", "confidence": 0.5875256458918253}]}], "introductionContent": [{"text": "Parsing accuracy is greatly impacted by the quality of preprocessing steps such as tagging and word segmentation.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.897936999797821}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9326065182685852}, {"text": "word segmentation", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7399024218320847}]}, {"text": "report that the difference between using the gold POS tags and using the automatic counterparts reaches about 6% in dependency accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9485523700714111}]}, {"text": "Prior research has demonstrated that joint prediction alleviates error propagation inherent in pipeline architectures, where mistakes cascade from one task to the next ().", "labels": [], "entities": [{"text": "joint prediction", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.7413584887981415}, {"text": "error propagation", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.6998185366392136}]}, {"text": "However, jointly modeling all the processing tasks inevitably increases inference complexity.", "labels": [], "entities": []}, {"text": "Prior work addressed this challenge by introducing constraints on scoring functions to keep inference tractable.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method for joint prediction that imposes no constraints on the scoring function.", "labels": [], "entities": [{"text": "joint prediction", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7104397118091583}]}, {"text": "The method is able to handle high-order and global features for each individual task (e.g., parsing), as well as features that capture interactions between tasks.", "labels": [], "entities": [{"text": "parsing)", "start_pos": 92, "end_pos": 100, "type": "TASK", "confidence": 0.892044723033905}]}, {"text": "The algorithm achieves this flexibility by operating overfull assignments that specify segmentation, POS tags and dependency tree, moving from one complete configuration to another.", "labels": [], "entities": []}, {"text": "Our approach is based on the randomized greedy algorithm from our earlier dependency parsing system ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7231853604316711}]}, {"text": "We extend this algorithm to jointly predict the segmentation and the POS tags in addition to the dependency parse.", "labels": [], "entities": []}, {"text": "The search space for the algorithm is a combination of parse trees and lattices that encode alternative morphological and POS analyses.", "labels": [], "entities": []}, {"text": "The inference algorithm greedily searches over this space, iteratively making local modifications to POS tags and dependency trees.", "labels": [], "entities": []}, {"text": "To overcome local optima, we employ multiple restarts.", "labels": [], "entities": []}, {"text": "This simple, yet powerful approach can be easily applied to a range of joint prediction tasks.", "labels": [], "entities": [{"text": "joint prediction tasks", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.7538925011952718}]}, {"text": "In prior work, joint models have been designed fora specific language.", "labels": [], "entities": []}, {"text": "For instance, joint models for Chinese are designed with word segmentation in mind (, while algorithms for processing Semitic languages are tailored for morpho-logical analysis.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7136324793100357}]}, {"text": "In contrast, we show that our algorithm can be effortlessly applied to all these distinct languages.", "labels": [], "entities": []}, {"text": "Language-specific characteristics drive the lattice construction and the feature selection, while the learning and inference methods are languageagnostic.", "labels": [], "entities": []}, {"text": "We evaluate our model on three datasets: SPMRL (Modern Standard Arabic), classical Arabic and CTB5 (Chinese).", "labels": [], "entities": [{"text": "SPMRL", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.6658958196640015}]}, {"text": "Our model consistently outperforms state-of-the-art systems designed for these languages.", "labels": [], "entities": []}, {"text": "We obtain a 2.1% TedEval gain against the best published results in the 2013 SPMRL shared task ().", "labels": [], "entities": [{"text": "TedEval", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9777336716651917}, {"text": "SPMRL shared task", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.5340436200300852}]}, {"text": "The joint model results in significant gains against its pipeline counterpart, yielding 2.4% absolute F-score increase in dependency parsing on the same dataset.", "labels": [], "entities": [{"text": "F-score", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9600632190704346}, {"text": "dependency parsing", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.649170309305191}]}, {"text": "Our analysis reveals that most of this gain comes from the improved prediction on OOV words.", "labels": [], "entities": []}], "datasetContent": [{"text": "We. summarizes the statistics of the datasets.", "labels": [], "entities": []}, {"text": "For the SPMRL test set, we follow the common practice which limits the sentence lengths up to 70).", "labels": [], "entities": [{"text": "SPMRL test set", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.8911880254745483}]}, {"text": "For classical Arabic and Chinese, we evaluate on all the test sentences.", "labels": [], "entities": []}, {"text": "Following standard practice in previous work), we use Fscore as the evaluation metric for segmentation, POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.8689262270927429}, {"text": "segmentation", "start_pos": 90, "end_pos": 102, "type": "TASK", "confidence": 0.9719268083572388}, {"text": "POS tagging", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.8873719871044159}, {"text": "dependency parsing", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.7688760757446289}]}, {"text": "We report the morpheme-level F-score for Arabic and the wordlevel F-score for Chinese.", "labels": [], "entities": [{"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9213899970054626}]}, {"text": "In addition, we use TedEval () to evaluate the joint prediction on the SPMRL dataset, because TedEval score is the only evaluation metric used in the official report.", "labels": [], "entities": [{"text": "TedEval", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.7986353635787964}, {"text": "SPMRL dataset", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.8586138188838959}, {"text": "TedEval score", "start_pos": 94, "end_pos": 107, "type": "METRIC", "confidence": 0.829483836889267}]}, {"text": "We directly use the evaluation tools provided on the SPMRL official website.", "labels": [], "entities": [{"text": "SPMRL official website", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.8076400558153788}]}, {"text": "8  Following our earlier work (), we train a first-order classifier to prune the dependency tree space.", "labels": [], "entities": []}, {"text": "Following common practice, we average parameters overall iterations after training with passive-aggressive online learning algorithm).", "labels": [], "entities": []}, {"text": "We use the same adaptive random restart strategy as in our earlier work () and set K = 300.", "labels": [], "entities": []}, {"text": "In addition, we also apply an aggressive early-stop strategy during training for efficiency.", "labels": [], "entities": []}, {"text": "If we have found a violation against the ground truth during the first 50 iterations, we immediately stop and update the We set the probability threshold to 0.05 and limit the number of candidate heads up to 20, which gives a 99.5% pruning recall on both the SPMRL and the CTB5 development sets.", "labels": [], "entities": [{"text": "recall", "start_pos": 240, "end_pos": 246, "type": "METRIC", "confidence": 0.5861840844154358}, {"text": "SPMRL", "start_pos": 259, "end_pos": 264, "type": "DATASET", "confidence": 0.7570331692695618}, {"text": "CTB5 development sets", "start_pos": 273, "end_pos": 294, "type": "DATASET", "confidence": 0.9733245770136515}]}, {"text": "parameters based on the current violation.", "labels": [], "entities": []}, {"text": "The reasoning behind this early-stop strategy is that weaker violations for some training sentences are already sufficient for separable training sets ().", "labels": [], "entities": []}, {"text": "Seg: F-score error reductions (%) of the joint model over the pipeline counterpart on seen and OOV words.", "labels": [], "entities": [{"text": "F-score error reductions", "start_pos": 5, "end_pos": 29, "type": "METRIC", "confidence": 0.9305866956710815}]}, {"text": "shows the break of the improvement based on seen and out-of-vocabulary (OOV) words.", "labels": [], "entities": [{"text": "break", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9800164103507996}]}, {"text": "As expected, across all languages OOV words benefit more from the joint prediction, as they constitute a common source of error propagation in a pipeline model.", "labels": [], "entities": []}, {"text": "The extent of improvement depends on the underlying accuracy of the preprocessing for segmentation and POS tagging on OOV words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9992314577102661}, {"text": "segmentation", "start_pos": 86, "end_pos": 98, "type": "TASK", "confidence": 0.9673525094985962}, {"text": "POS tagging", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.789822906255722}]}, {"text": "For instance, we observe a higher gain (7%) on Chinese OOV words which have a 61.5% accuracy when processed by the original stand-along POS tagger.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9974308609962463}]}, {"text": "On the SPMRL dataset, the gain on OOV words is lower (3%), while preprocessing accuracy is higher (82%).", "labels": [], "entities": [{"text": "SPMRL dataset", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.8013851046562195}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9060924053192139}]}, {"text": "Their error reductions on OOV words are nevertheless close to each other.", "labels": [], "entities": []}, {"text": "summarizes the results on F-score error reduction.", "labels": [], "entities": [{"text": "F-score error reduction", "start_pos": 26, "end_pos": 49, "type": "METRIC", "confidence": 0.8601510723431905}]}, {"text": "We also observe that the error reductions of OOV words/morphemes on the Chinese and the Classical Arabic dataset are larger than that of the invocabulary counterparts (e.g. 26% vs. 20% on Chinese word segmentation).", "labels": [], "entities": [{"text": "error reductions", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.9612807631492615}, {"text": "Classical Arabic dataset", "start_pos": 88, "end_pos": 112, "type": "DATASET", "confidence": 0.6856040954589844}, {"text": "Chinese word segmentation", "start_pos": 188, "end_pos": 213, "type": "TASK", "confidence": 0.6834908326466879}]}, {"text": "However, we have the opposite observation on the segmentation and POS tagging on the SPMRL dataset (28% vs. 48%).", "labels": [], "entities": [{"text": "segmentation", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.9484775066375732}, {"text": "POS tagging", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.8657479286193848}, {"text": "SPMRL dataset", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.9267550706863403}]}, {"text": "This can be explained by analyzing the oracle performance in which we select the best solution from possible candidates.", "labels": [], "entities": []}, {"text": "The oracle error reduction of OOV morphemes in the SPMRL dataset is relatively low (44%), compared to the 61% oracle error reduction of OOV morphemes in the Classical Arabic dataset.", "labels": [], "entities": [{"text": "SPMRL dataset", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9499697685241699}]}], "tableCaptions": [{"text": " Table 1: POS tag feature templates. t 0 and w 0 de- notes the POS tag and the word at the current posi- tion. t \u2212x and t x denote left and right context tags,  and similarly for words. s(\u00b7) denotes the score of  the POS tag produced by the preprocessing tagger.  The last row shows the \"Character\"-based features  for Chinese. pre 1 (\u00b7) and pre 2 (\u00b7) denote the word  prefixes with one and two characters respectively.  suf 1 (\u00b7) and suf 2 (\u00b7) denote the word suffixes simi- larly. c n (\u00b7) denotes the n-th character in the word.  len(\u00b7) denotes the length of the word, capped at 5 if  longer.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of datasets.", "labels": [], "entities": []}, {"text": " Table 3: Quality of the lattice structures on each  dataset. For SPMRL and CTB5, we show the statis- tics on the development sets. For classical Arabic,  we directly show the statistics on the testing set be- cause the development set is not available.", "labels": [], "entities": [{"text": "CTB5", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.8225535154342651}]}, {"text": " Table 5: F-score error reductions (%) of the joint  model over the pipeline counterpart on seen and  OOV words.", "labels": [], "entities": [{"text": "F-score error reductions", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.91754549741745}]}]}