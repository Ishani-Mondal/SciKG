{"title": [{"text": "Word Embedding-based Antonym Detection using Thesauri and Distributional Information", "labels": [], "entities": [{"text": "Antonym Detection", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.841040700674057}]}], "abstractContent": [{"text": "This paper proposes a novel approach to train word embeddings to capture antonyms.", "labels": [], "entities": []}, {"text": "Word embeddings have shown to capture synonyms and analogies.", "labels": [], "entities": []}, {"text": "Such word embeddings, however , cannot capture antonyms since they depend on the distributional hypothesis.", "labels": [], "entities": []}, {"text": "Our approach utilizes supervised synonym and antonym information from thesauri, as well as distributional information from large-scale unlabelled text data.", "labels": [], "entities": []}, {"text": "The evaluation results on the GRE antonym question task show that our model outperforms the state-of-the-art systems and it can answer the antonym questions in the F-score of 89%.", "labels": [], "entities": [{"text": "GRE antonym question task", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.5977354571223259}, {"text": "F-score", "start_pos": 164, "end_pos": 171, "type": "METRIC", "confidence": 0.9994369149208069}]}], "introductionContent": [{"text": "Word embeddings have shown to capture synonyms and analogies).", "labels": [], "entities": []}, {"text": "Word embeddings have also been effectively employed in several tasks such as named entity recognition (), adjectival scales (Kim and de Marneffe, 2013) and text classification (.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.6433978180090586}, {"text": "text classification", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.7970700860023499}]}, {"text": "Such embeddings trained based on distributional hypothesis, however, often fail to recognize antonyms since antonymous words, e.g. strong and weak, occur in similar contexts.", "labels": [], "entities": []}, {"text": "Recent studies focuses on learning word embeddings for specific tasks, such as sentiment analysis) and dependency parsing ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.9502387642860413}, {"text": "dependency parsing", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.8160971701145172}]}, {"text": "These motivate anew approach to learn word embeddings to capture antonyms.", "labels": [], "entities": []}, {"text": "Recent studies on antonym detection have shown that thesauri information are useful in distinguishing antonyms from synonyms.", "labels": [], "entities": [{"text": "antonym detection", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.8095205426216125}]}, {"text": "The state-of-the-art systems achieved over 80% in F-score on GRE antonym tests.", "labels": [], "entities": [{"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9995365142822266}, {"text": "GRE antonym tests", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.7396666208902994}]}, {"text": "proposed a Polarity Inducing Latent Semantic Analysis (PILSA) that incorporated polarity information in two thesauri in constructing a matrix for latent semantic analysis.", "labels": [], "entities": [{"text": "Polarity Inducing Latent Semantic Analysis (PILSA)", "start_pos": 11, "end_pos": 61, "type": "TASK", "confidence": 0.6926052384078503}, {"text": "latent semantic analysis", "start_pos": 146, "end_pos": 170, "type": "TASK", "confidence": 0.7045735617478689}]}, {"text": "They additionally used context vectors to cover the out-ofvocabulary words; however, they did not use word embeddings.", "labels": [], "entities": []}, {"text": "Recently, proposed a Bayesian Probabilistic Tensor Factorization (BPTF) model to combine thesauri information and existing word embeddings.", "labels": [], "entities": []}, {"text": "They showed that the usefulness of word embeddings but they used pretrained word embeddings.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach to construct word embeddings that can capture antonyms.", "labels": [], "entities": []}, {"text": "Unlike the previous approaches, our approach directly trains word embeddings to represent antonyms.", "labels": [], "entities": []}, {"text": "We propose two models: a Word Embedding on Thesauri information (WE-T) model and a Word Embeddings on Thesauri and Distributional information (WE-TD) model.", "labels": [], "entities": []}, {"text": "The WE-T model receives supervised information from synonym and antonym pairs in thesauri and infers the relations of the other word pairs in the thesauri from the supervised information.", "labels": [], "entities": []}, {"text": "The WE-TD model incorporates corpus-based contextual information (distributional information) into the WE-T model, which enables the calculation of the similarities among invocabulary and out-of-vocabulary words.: Overview of our approach.", "labels": [], "entities": []}, {"text": "When we use the thesauri directly, disperse and garner are known to be antonymous and disperse and scatter are known to be synonymous, but the remaining relations are unknown.", "labels": [], "entities": []}, {"text": "WE-T infers indirect relations among words in thesauri.", "labels": [], "entities": []}, {"text": "Furthermore, WE-TD incorporates distributional information, and the relatedness among in-vocabulary and out-of-vocabulary words (nucleate here) are obtained.", "labels": [], "entities": [{"text": "WE-TD", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.6434922814369202}]}], "datasetContent": [{"text": "This section explains the task setting, resource for training, parameter settings, and evaluation metrics.", "labels": [], "entities": []}, {"text": "We used the F-score as a primary evaluation metric following.", "labels": [], "entities": [{"text": "F-score", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.998521625995636}]}, {"text": "The F-score is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9955937266349792}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9994811415672302}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9964978694915771}]}, {"text": "Precision is the proportion of correctly answered questions over answered questions.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.988564133644104}]}, {"text": "Recall is the proportion of correctly answered questions over the questions.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.983704149723053}]}, {"text": "shows the results of our models on the GRE antonym question task.", "labels": [], "entities": [{"text": "GRE antonym question task", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.5910912603139877}]}, {"text": "This table also shows the results of previous systems) and models trained on Wikipedia without thesauri (WE-D) for the comparison.", "labels": [], "entities": [{"text": "WE-D", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.7657901644706726}]}], "tableCaptions": [{"text": " Table 1: Results on the GRE antonym question task.  \u2020 is from Yih et al. (2012),  \u2021 is from Zhang et al. (2014), and  \u00a7  is from Mohammad et al. (2013).  \u00b6 slightly differs from the result in Zhang et al. (2014) since thesauri can contain  multiple candidates as antonyms and the answer is randomly selected for the candidates.", "labels": [], "entities": [{"text": "GRE antonym question task", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.49679719656705856}]}]}