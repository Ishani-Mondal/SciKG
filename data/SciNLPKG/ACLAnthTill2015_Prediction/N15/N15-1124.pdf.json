{"title": [{"text": "Accurate Evaluation of Segment-level Machine Translation Metrics", "labels": [], "entities": [{"text": "Accurate", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9760259389877319}, {"text": "Segment-level Machine Translation Metrics", "start_pos": 23, "end_pos": 64, "type": "TASK", "confidence": 0.8496983349323273}]}], "abstractContent": [{"text": "Evaluation of segment-level machine translation metrics is currently hampered by: (1) low inter-annotator agreement levels inhuman assessments ; (2) lack of an effective mechanism for evaluation of translations of equal quality; and (3) lack of methods of significance testing improvements over a baseline.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.6708190441131592}]}, {"text": "In this paper, we provide solutions to each of these challenges and outline anew human evaluation methodology aimed specifically at assessment of segment-level metrics.", "labels": [], "entities": []}, {"text": "We replicate the human evaluation component of WMT-13 and reveal that the current state-of-the-art performance of segment-level metrics is better than previously believed.", "labels": [], "entities": [{"text": "WMT-13", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.7170844674110413}]}, {"text": "Three segment-level met-rics-METEOR, NLEPOR and SENTBLEU-MOSES-are found to correlate with human assessment at a level not significantly outper-formed by any other metric in both the individual language pair assessment for Spanish-to-English and the aggregated set of 9 language pairs.", "labels": [], "entities": [{"text": "NLEPOR", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9254412651062012}, {"text": "SENTBLEU-MOSES-are", "start_pos": 48, "end_pos": 66, "type": "METRIC", "confidence": 0.9527028799057007}]}], "introductionContent": [{"text": "Automatic segment-level machine translation (MT) metrics have the potential to greatly advance MT by providing more fine-grained error analysis, increasing efficiency of system tuning methods and leveraging techniques for system hybridization.", "labels": [], "entities": [{"text": "segment-level machine translation (MT)", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.7813277592261633}, {"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9941196441650391}]}, {"text": "However, a major obstacle currently hindering the development of segment-level metrics is their evaluation.", "labels": [], "entities": []}, {"text": "Human assessment is the gold standard against which metrics must be evaluated, but when it comes to the task of evaluating translation quality, human annotators are notoriously inconsistent.", "labels": [], "entities": []}, {"text": "For example, the main venue for evaluation of metrics, the annual Workshop on Statistical Machine Translation (WMT), reports disturbingly low inter-annotator agreement levels and highlights the need for better human assessment of MT.", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 78, "end_pos": 115, "type": "TASK", "confidence": 0.7760489583015442}, {"text": "MT", "start_pos": 230, "end_pos": 232, "type": "TASK", "confidence": 0.9841035604476929}]}, {"text": "WMT-13, for example, report Kappa coefficients ranging from 0.075 to 0.324 for assessors from crowd-sourcing services, only increasing to between 0.315 and 0.457 for MT researchers ().", "labels": [], "entities": [{"text": "WMT-13", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9821926951408386}, {"text": "Kappa", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9851176738739014}, {"text": "MT", "start_pos": 166, "end_pos": 168, "type": "TASK", "confidence": 0.9277829527854919}]}, {"text": "For evaluation of metrics that operate at the system or document-level such as BLEU, inconsistency in individual human judgments can, to some degree, be overcome by aggregation of individual human assessments over the segments within a document.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9749864339828491}]}, {"text": "However, for evaluation of segment-level metrics, there is no escaping the need to boost the consistency of human annotation of individual segments.", "labels": [], "entities": [{"text": "consistency", "start_pos": 93, "end_pos": 104, "type": "METRIC", "confidence": 0.9811761975288391}]}, {"text": "This motivates our analysis of current methods of human evaluation of segment-level metrics, and proposal of an alternative annotation mechanism.", "labels": [], "entities": []}, {"text": "We examine the accuracy of segment scores collected with our proposed method by replicating components of the WMT-13 human evaluation (, with the sole aim of optimizing agreement in segment scores to provide an effective gold standard for evaluating segment-level metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9959508180618286}, {"text": "WMT-13 human evaluation", "start_pos": 110, "end_pos": 133, "type": "DATASET", "confidence": 0.8505582412083944}]}, {"text": "Our method also supports the use of significance testing of segment-level metrics, and tests applied to the WMT-13 metrics over nine language pairs reveal for the first time which segment-level metrics outperform others.", "labels": [], "entities": [{"text": "WMT-13 metrics", "start_pos": 108, "end_pos": 122, "type": "DATASET", "confidence": 0.8180587291717529}]}, {"text": "We have made available code for acquiring accurate segment-level MT human evaluations from the crowd, in addition to significance testing competing segment-level metrics, at: https://github.com/ygraham/ segment-mteval", "labels": [], "entities": [{"text": "MT human evaluations", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.8751832445462545}]}], "datasetContent": [{"text": "Since 2008, the WMT workshop series has included a shared task for automatic metrics, and as with the translation shared task, human evaluation remains the official gold standard for evaluation.", "labels": [], "entities": [{"text": "WMT workshop series", "start_pos": 16, "end_pos": 35, "type": "DATASET", "confidence": 0.5948594212532043}]}, {"text": "In order to minimize the amount of annotation work and enforce consistency between the primary shared tasks in WMT, the same evaluations are used to evaluate MT systems in the shared translation task, as well as MT evaluation metrics in the document-level metrics and segment-level metrics tasks.", "labels": [], "entities": [{"text": "MT", "start_pos": 158, "end_pos": 160, "type": "TASK", "confidence": 0.9636103510856628}]}, {"text": "Although WMT have trialled several methods of human evaluation over the years, the prevailing method takes the form of ranking a set of five competing translations fora single source language (SL) input segment from best to worst.", "labels": [], "entities": []}, {"text": "A total often pairwise human relative preference judgments can be extracted from each set of five translations.", "labels": [], "entities": []}, {"text": "Performance of a segment-level metric is assessed by the degree to which it corresponds with human judgment, measured by the number of metric scores for pairs of translations that are either concordant (Con) or discordant (Dis) with those of a human assessor, which the organizers describe as \"Kendall's \u03c4 \": Pairs of translations deemed equally good by a human assessor are omitted from evaluation of segment-level metrics ( ).", "labels": [], "entities": [{"text": "Con) or discordant (Dis)", "start_pos": 203, "end_pos": 227, "type": "METRIC", "confidence": 0.7110676084245954}]}, {"text": "There is a mismatch between the human judgments data used to evaluate segment-level metrics and the standard conditions under which Kendall's \u03c4 is applied, however: Kendall's \u03c4 is used to measure the association between a set of observations of a single pair of joint random variables, X (e.g. the human rank of a translation) and Y (e.g. the metric score for the same translation).", "labels": [], "entities": []}, {"text": "A conventional application of Kendall's \u03c4 would be comparison of all pairs of values within X with each corresponding pair within Y . Since the human assessment data is, however, a large number of separately ranked sets of five competing translations and not a single ranking of all translations, it is not possible to compute a single Kendall's \u03c4 correlation.", "labels": [], "entities": []}, {"text": "The formula used to assess the performance of a metric in the task, therefore, is not what is ordinarily understood to be a Kendall's \u03c4 coefficient, but, in fact, equivalent to a weighted average of all Kendall's \u03c4 for each humanranked set of five translations.", "labels": [], "entities": []}, {"text": "A more significant problem, however, lies in the inconsistency of human relative preference judgments within data sets.", "labels": [], "entities": []}, {"text": "Since overall scores for metrics are described as correlations, possible values achievable by any metric could be expected to lie in the range [\u22121, 1] (or \"\u00b11\").", "labels": [], "entities": []}, {"text": "This is not the case, and achievements of metrics are obscured by contradictory human judgments.", "labels": [], "entities": []}, {"text": "Before any metric has provided scores for segments, for example, the maximum and minimum correlation achievable by a participating metric can be computed as, in the case of WMT-13: \u2022 Russian-to-English: \u00b10.92 \u2022 Spanish-to-English: \u00b10.90 \u2022 French-to-English: \u00b10.90 \u2022 German-to-English: \u00b10.92 \u2022 Czech-to-English: \u00b10.89 \u2022 English-to-Russian: \u00b10.90 \u2022 English-to-Spanish: \u00b10.90 \u2022 English-to-French: \u00b10.91 \u2022 English-to-German: \u00b10.90 \u2022 English-to-Czech: \u00b10.87 If we are interested in the relative performance of metrics and take a closer look at the formula used to contribute a score to metrics, we can effectively ignore the denominator (|Con| + |Dis|), as it is constant for all metrics.", "labels": [], "entities": [{"text": "WMT-13", "start_pos": 173, "end_pos": 179, "type": "DATASET", "confidence": 0.9005279541015625}]}, {"text": "The numerator (|Con| \u2212 |Dis|) is what determines our evaluation of the relative performance of metrics, and although the formula appears to be a straightforward subtraction of counts of concordant and discordant pairs, due to the large numbers of contradictory human relative preference judgments in data sets, what this number actually represents is not immediately obvious.", "labels": [], "entities": []}, {"text": "If, for example, translations A and B were scored by a metric such that metric score(A) > metric score(B), one might expect an addition or subtraction of 1 depending on whether or not the metric's scores agreed with those of a human.", "labels": [], "entities": [{"text": "metric score(A)", "start_pos": 72, "end_pos": 87, "type": "METRIC", "confidence": 0.8964898109436035}, {"text": "metric score(B)", "start_pos": 90, "end_pos": 105, "type": "METRIC", "confidence": 0.9290443897247315}]}, {"text": "Instead, however, the following is added: where: |A > B| = # human judgments where A was preferred over B |A < B| = # human judgments where B was For example, translations of segment 971 for Czechto-English systems uedin-heafield and uedin-wmt13 were compared by human assessors a total of 12 times: the first system was judged to be best 4 times, the second system was judged to be best 2 times, and the two systems were judged to be equal 6 times.", "labels": [], "entities": []}, {"text": "This results in a score of 4\u22122 fora system-level metric that scores the uedin-heafield translation higher than uedin-wmt13 (tied judgments are omitted), or score of 2 \u2212 4 in the converse case.", "labels": [], "entities": []}, {"text": "Another challenge is how to deal with relative preference judgments where two translations are deemed equal quality (as opposed to strictly better or worse).", "labels": [], "entities": []}, {"text": "In the current setup, tied translation pairs are excluded from the data, meaning that the ability for evaluation metrics to evaluate similar translations is not directly evaluated, and a metric that manages to score two equal quality translations closer, does not receive credit.", "labels": [], "entities": []}, {"text": "A segment-level metric that can accurately predict not just disparities between translations but also similarities is likely to have high utility for MT system optimization, and is possibly the strongest motivation for developing segment-level metrics in the first place.", "labels": [], "entities": [{"text": "MT system optimization", "start_pos": 150, "end_pos": 172, "type": "TASK", "confidence": 0.931719700495402}]}, {"text": "In WMT-13, however, 24% of all relative preference judgments were omitted on the basis of ties, broken down as follows: \u2022 Spanish-to-English: 28% \u2022 French-to-English: 26% \u2022 German-to-English: 27% \u2022 Czech-to-English: 25% \u2022 Russian-to-English: 24% \u2022 English-to-Spanish: 23% \u2022 English-to-French: 23% \u2022 English-to-German: 20% \u2022 English-to-Czech: 16% \u2022 English-to-Russian: 27% Although significance tests for evaluation of MT systems and document-level metrics have been identified, no such test has been proposed for segment-level metrics, and it is unfortunately common to conclude success without taking into account the fact that an increase in correlation can occur simply by chance.", "labels": [], "entities": [{"text": "WMT-13", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.7467004656791687}, {"text": "MT", "start_pos": 418, "end_pos": 420, "type": "TASK", "confidence": 0.9656683206558228}]}, {"text": "In the rare cases where significance tests have been applied, tests or confidence intervals for individual correlations form the basis for drawing conclusions).", "labels": [], "entities": []}, {"text": "However, such tests do not provide insight into whether or not a metric outperforms another, as all that's required for rejection of the null hypothesis with such a testis a likelihood that an individual metric's correlation with human judgment is not equal to zero.", "labels": [], "entities": []}, {"text": "In addition, data sets for evaluation in both document and segment-level metrics are not independent and the correlation that exists between pairs of metrics should also betaken into account by significance tests.", "labels": [], "entities": []}, {"text": "Many human evaluation methodologies attempt to elicit precisely the same quality judgment for individual translations from all assessors, and inevitably produce large numbers of conflicting assessments in the process, including from the same individual human judge).", "labels": [], "entities": []}, {"text": "An alternative approach is to take into account the fact that different judges may genuinely disagree, and allow assessments provided by individuals to each contribute to an overall estimate of the quality of a given translation.", "labels": [], "entities": []}, {"text": "In an ideal world in which we had access to assessments provided by the entire population of qualified human assessors, for example, the mean of those assessments would provide a statistic that, in theory at least, would provide a meaningful segment-level human score for translations.", "labels": [], "entities": [{"text": "translations", "start_pos": 272, "end_pos": 284, "type": "TASK", "confidence": 0.9785082936286926}]}, {"text": "If it were possible to collect assessments from the entire population we could directly compute the true mean score fora translation segment.", "labels": [], "entities": []}, {"text": "This is of course not possible, but thanks to the law of large numbers we can make the following assumption: Given a sufficiently large assessment sample fora given translation, the mean of assessments will provide a very good estimate of the true mean score of the translation sourced from the entire assessor population.", "labels": [], "entities": []}, {"text": "What the law of large numbers does not tell us, however, is, for our particular case of translation quality assessment, precisely how large the sample of assessments needs to be, so that the mean of scores provides a close enough estimate to the true mean score for any translation.", "labels": [], "entities": [{"text": "translation quality assessment", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.8007645408312479}]}, {"text": "For a sample mean for which the variance is known, the required sample size can be computed fora specified standard error.", "labels": [], "entities": []}, {"text": "However, due to the large number of distinct translations we deal with, the variance in sample score distributions may change considerably from one translation to the next.", "labels": [], "entities": []}, {"text": "In addition, the choice as to what exactly is an acceptable standard error in sample means would be somewhat arbitrary.", "labels": [], "entities": []}, {"text": "On the one hand, if we specify a standard error that's lower than is required, and subsequently collect more repeat assessments than is needed, we would be wasting resources that could, for example, be targeted at the annotation of additional translation segments.", "labels": [], "entities": []}, {"text": "Our solution is to empirically investigate the impact on sample size of repeat assessments on the mean score fora given segment, and base our determination of sample size on the findings.", "labels": [], "entities": []}, {"text": "Since we later motivate the use of Pearson's correlation to measure the linear association between human and metric scores (see Section 4), we base our investigation on Pearson's correlation.", "labels": [], "entities": []}, {"text": "We collect multiple assessments per segment to create score distributions for segments fora fixed set per language pair.", "labels": [], "entities": []}, {"text": "This is repeated twice over the same set of segments to generate two distinct sets of annotations: one set is used to estimate the true mean score, and the second set is randomly downsampled to simulate a set of assessments of fixed sample size.", "labels": [], "entities": [{"text": "true mean score", "start_pos": 131, "end_pos": 146, "type": "METRIC", "confidence": 0.6718313495318095}]}, {"text": "We measure the Pearson correlation between the true mean score and different numbers of  assessments fora given assessment, to ask the question: how many assessments must be collected fora given segment to obtain mean segment scores that truly reflects translation quality?", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 15, "end_pos": 34, "type": "METRIC", "confidence": 0.9819785356521606}]}, {"text": "Scores are sampled according to annotation time to simulate a realistic setting.", "labels": [], "entities": []}, {"text": "Since the scores generated by our method are continuous and segment-level metrics are also required to output continuous-valued scores, we can now compare the scores directly using Pearson's correlation.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 181, "end_pos": 202, "type": "METRIC", "confidence": 0.581974059343338}]}, {"text": "Pearson's correlation has three main advantages for this purpose.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.6612105568250021}]}, {"text": "Firstly, the measure is unit-free, so metrics do not have to produce scores on the same scale as the human assessments.", "labels": [], "entities": []}, {"text": "Secondly, scores are absolute as opposed to relative and therefore more intuitive and ultimately more powerful; for example, we are able to evaluate metrics over the 20% of translations of highest or lowest quality in the test set.", "labels": [], "entities": []}, {"text": "Finally, the use of Pearson's correlation facilitates the measurement of statistical significance in correlation differences.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 20, "end_pos": 41, "type": "METRIC", "confidence": 0.5769791603088379}]}, {"text": "It is important to point out, however, that moving from Kendall's \u03c4 over relative preference judgments to Pearson's rover absolute scores does, in fact, change the task required of metrics in one respect: previously, there was no direct evaluation of the scores generated by a metric, nor indeed did the evaluation ever directly compare translations for different source language inputs (as relative preference judgments were always relative to other translations for the same input).", "labels": [], "entities": []}, {"text": "Pearson's correlation, on the other hand, compares scores across the entire test set.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.6348711848258972}]}], "tableCaptions": [{"text": " Table 1: Datasets used to assess translation assessment  sample size", "labels": [], "entities": [{"text": "translation assessment", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.9071746468544006}]}, {"text": " Table 2: Pearson's correlation and Kendall's \u03c4 between  WMT-13 segment-level metrics and human assessment  for Spanish-to-English (ES-EN). Note that Kendall's \u03c4  is based on the WMT-13 formulation, and the preference  judgments from WMT-13.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.6122665603955587}, {"text": "Kendall's \u03c4", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.8824434081713358}, {"text": "WMT-13", "start_pos": 234, "end_pos": 240, "type": "DATASET", "confidence": 0.9721117615699768}]}]}