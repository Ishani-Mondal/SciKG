{"title": [{"text": "Unediting: Detecting Disfluencies Without Careful Transcripts", "labels": [], "entities": [{"text": "Detecting Disfluencies Without Careful Transcripts", "start_pos": 11, "end_pos": 61, "type": "TASK", "confidence": 0.8655577898025513}]}], "abstractContent": [{"text": "Speech transcripts often only capture semantic content, omitting disfluencies that can be useful for analyzing social dynamics of a discussion.", "labels": [], "entities": []}, {"text": "This work describes steps in building a model that can recover a large fraction of locations where disfluencies were present, by transforming carefully annotated text to match the standard transcription style, introducing a two-stage model for handling different types of disfluencies, and applying semi-supervised learning.", "labels": [], "entities": []}, {"text": "Experiments show improvement in disfluency detection on Supreme Court oral arguments , nearly 23% improvement in F1.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.8589735627174377}, {"text": "Supreme Court oral arguments", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.6412146091461182}, {"text": "F1", "start_pos": 113, "end_pos": 115, "type": "METRIC", "confidence": 0.9996203184127808}]}], "introductionContent": [{"text": "Many hearings, lectures, news broadcasts and other spoken proceedings are hand-transcribed and made available online for easier searching and increased accessability.", "labels": [], "entities": []}, {"text": "For speed and cost reasons, standard transcription services aim at representing semantic content only; thus, filled pauses (uh, um) and many disfluencies (repetitions and self corrections) are omitted, though not all.", "labels": [], "entities": []}, {"text": "Careful transcripts represent all the words (and word fragments spoken), as shown below with disfluent regions underlined.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the different sources/transformations of training data, self-training and the two-stage detection model on ANNOTATED OYEZ transcripts from three cases (\u223c30k words).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Disfluency detection of ANNOT OYEZ with  different training sets.", "labels": [], "entities": [{"text": "Disfluency detection", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9105775654315948}, {"text": "ANNOT", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9170127511024475}, {"text": "OYEZ", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.49290961027145386}]}, {"text": " Table 3: The combination of ANNOT OYEZ and SWBD  with different SWBD transformation steps.", "labels": [], "entities": [{"text": "ANNOT", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9528923630714417}, {"text": "OYEZ", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.5309880971908569}]}, {"text": " Table 4: Self-training performance using different initial  models.", "labels": [], "entities": []}, {"text": " Table 5: Baseline, two-stages and self-training methods,  comparison: baseline self-training method is trained on  ...., all the rest methods are trained on ANNOT OYEZ and  TRANSF SWBD. Our method, UNEDITOR combines self- training and two-stage models.", "labels": [], "entities": [{"text": "ANNOT", "start_pos": 158, "end_pos": 163, "type": "METRIC", "confidence": 0.8974212408065796}, {"text": "OYEZ", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.5124895572662354}, {"text": "TRANSF SWBD", "start_pos": 174, "end_pos": 185, "type": "DATASET", "confidence": 0.7560251951217651}]}]}