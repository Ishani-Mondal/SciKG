{"title": [{"text": "Simple task-specific bilingual word embeddings *", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce a simple wrapper method that uses off-the-shelf word embedding algorithms to learn task-specific bilingual word em-beddings.", "labels": [], "entities": []}, {"text": "We use a small dictionary of easily-obtainable task-specific word equivalence classes to produce mixed context-target pairs that we use to train off-the-shelf embedding models.", "labels": [], "entities": []}, {"text": "Our model has the advantage that it (a) is independent of the choice of embedding algorithm, (b) does not require parallel data, and (c) can be adapted to specific tasks by redefining the equivalence classes.", "labels": [], "entities": []}, {"text": "We show how our method outper-forms off-the-shelf bilingual embeddings on the task of unsupervised cross-language part-of-speech (POS) tagging, as well as on the task of semi-supervised cross-language super sense (SuS) tagging.", "labels": [], "entities": [{"text": "cross-language part-of-speech (POS) tagging", "start_pos": 99, "end_pos": 142, "type": "TASK", "confidence": 0.5980955163637797}, {"text": "cross-language super sense (SuS) tagging", "start_pos": 186, "end_pos": 226, "type": "TASK", "confidence": 0.668109838451658}]}], "introductionContent": [{"text": "Using multi-layered neural networks to learn word embeddings has become standard in NLP ().", "labels": [], "entities": []}, {"text": "While there is still some controversy whether such methods are superior to older methods (), there is little doubt that continuous word representations can potentially solve some of the data sparsity problems inherent in NLP.", "labels": [], "entities": []}, {"text": "Most research on word embeddings has focused on learning representations for the words in a single language, making syntactically or semantically similar words appear close in the embedding space.", "labels": [], "entities": []}, {"text": "Embeddings have been applied to many tasks, from * The authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "The second author is funded by the ERC Starting Grant LOWLANDS named entity recognition () to dependency parsing ().", "labels": [], "entities": [{"text": "ERC Starting Grant LOWLANDS", "start_pos": 35, "end_pos": 62, "type": "DATASET", "confidence": 0.879355326294899}, {"text": "entity recognition", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.6026616990566254}, {"text": "dependency parsing", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.8647448718547821}]}, {"text": "It has furthermore been shown that weakly supervised embedding algorithms can also lead to huge improvements for tasks like sentiment analysis (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.9582720100879669}]}, {"text": "In this work, we also use weak or distant supervision, relying on small dictionary seeds.", "labels": [], "entities": []}, {"text": "This paper, however, considers the problem of learning bilingual word embeddings, i.e., word embeddings such that similar words in two different languages end up close in the embedding space.", "labels": [], "entities": []}, {"text": "Such bilingual word embeddings can potentially be used for better cross-language transfer of NLP models, as we show in this paper.", "labels": [], "entities": [{"text": "cross-language transfer", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.7931643426418304}]}, {"text": "Previous work on bilingual word embeddings have defined similar words as translation equivalents and evaluated embeddings in the context of document classification tasks.", "labels": [], "entities": [{"text": "document classification tasks", "start_pos": 140, "end_pos": 169, "type": "TASK", "confidence": 0.7930923302968343}]}, {"text": "In this paper, we present a simple wrapper method to existing monolingual word embedding algorithms that can learn task-specific bilingual embeddings, e.g., for POS tagging, named entity recognition, or sentiment analysis.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 161, "end_pos": 172, "type": "TASK", "confidence": 0.9037296772003174}, {"text": "named entity recognition", "start_pos": 174, "end_pos": 198, "type": "TASK", "confidence": 0.6224724849065145}, {"text": "sentiment analysis", "start_pos": 203, "end_pos": 221, "type": "TASK", "confidence": 0.9404235780239105}]}, {"text": "Our algorithm is simpler and performs better on the tasks where we could compare performance to existing algorithms.", "labels": [], "entities": []}, {"text": "Also, we note that our approach, unlike existing algorithms), is as fast as learning monolingual embeddings.", "labels": [], "entities": []}, {"text": "Our contributions In this paper we introduce anew approach for learning bilingual word embeddings and revisit the task of unsupervised crosslanguage POS tagging ( . Our bilingual embedding model, which we call Bilingual Adaptive Reshuffling with Individual Stochastic Alternatives (BARISTA), takes two (non-parallel) corpora and a small dictionary as input.", "labels": [], "entities": [{"text": "crosslanguage POS tagging", "start_pos": 135, "end_pos": 160, "type": "TASK", "confidence": 0.6207612951596578}, {"text": "Bilingual Adaptive Reshuffling", "start_pos": 210, "end_pos": 240, "type": "TASK", "confidence": 0.6446512043476105}]}, {"text": "The dictio-nary is essentially a list of words in the two languages that are equivalent with respect to some task, e.g., English car and French maison ('house') are both nouns, and hence \"equivalent\" in POS tagging; English clerk and chauffeur are both persons, and hence \"equivalent\" in SuS tagging; house and maison are equivalent in machine translation.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 203, "end_pos": 214, "type": "TASK", "confidence": 0.6954903900623322}, {"text": "SuS tagging", "start_pos": 288, "end_pos": 299, "type": "TASK", "confidence": 0.9169079661369324}, {"text": "machine translation", "start_pos": 336, "end_pos": 355, "type": "TASK", "confidence": 0.7169444262981415}]}, {"text": "BARISTA has the advantage that it (a) is independent of the choice of embedding algorithm, (b) does not require parallel data, and (c) can be adapted to specific tasks by using appropriate dictionaries.", "labels": [], "entities": [{"text": "BARISTA", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5676160454750061}]}, {"text": "We use the bilingual embeddings directly to train a target language POS tagger on source language training data.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 68, "end_pos": 78, "type": "TASK", "confidence": 0.7230169773101807}]}, {"text": "Instead of lexical features, we use the bilingual embeddings.", "labels": [], "entities": []}, {"text": "We show our bilingual embedding method outperforms using off-the-shelf bilingual embeddings on this task, and that our system is competitive to state-of-the-art approaches for cross-language POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 191, "end_pos": 202, "type": "TASK", "confidence": 0.8234454989433289}]}, {"text": "Finally, we show that the same embeddings also lead to significantly better performance in semi-supervised cross-language SuS tagging.", "labels": [], "entities": [{"text": "SuS tagging", "start_pos": 122, "end_pos": 133, "type": "TASK", "confidence": 0.8908871412277222}]}, {"text": "The code will be made publicly available at https: //github.com/gouwsmeister/barista.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we balance the source and target corpora, by subsampling from the bigger corpus.", "labels": [], "entities": []}, {"text": "The vocabularies for all models are kept unrestricted, and result in around 1M words per language pair.", "labels": [], "entities": []}, {"text": "We train with a window of 4 words on either side of the target word, using linear discounting of the initial learning rate of 0.1.", "labels": [], "entities": []}, {"text": "These parameters were set on the Spanish POS data (see \u00a73.2).", "labels": [], "entities": [{"text": "Spanish POS data", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.8305236299832662}]}, {"text": "We use the CBOW model in word2vec for training the word embeddings.", "labels": [], "entities": []}, {"text": "Both our POS tagging evaluation datasets, as well as the Wiktionaries, 2 are mapped to Google's universal tagset ).", "labels": [], "entities": [{"text": "POS tagging evaluation datasets", "start_pos": 9, "end_pos": 40, "type": "DATASET", "confidence": 0.7172706425189972}]}, {"text": "We use the derived dictionaries for extracting bilingual POS equivalence classes.", "labels": [], "entities": [{"text": "extracting bilingual POS equivalence classes", "start_pos": 36, "end_pos": 80, "type": "TASK", "confidence": 0.7419644236564636}]}, {"text": "For SuS tagging, we only consider English-Danish and extract equivalence classes from Princeton WordNet and DanNet.", "labels": [], "entities": [{"text": "SuS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9822005033493042}, {"text": "Princeton WordNet", "start_pos": 86, "end_pos": 103, "type": "DATASET", "confidence": 0.8610658347606659}, {"text": "DanNet", "start_pos": 108, "end_pos": 114, "type": "DATASET", "confidence": 0.9178315997123718}]}, {"text": "For translation equivalents, we made use of Google Translate.", "labels": [], "entities": [{"text": "translation equivalents", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.9635398983955383}]}, {"text": "In our main experiments ( \u00a73.2-3.3), we use data from Wikipedia, but we first present a qualitative evaluation of English-German bilingual embeddings learned from the smaller Europarl corpus.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 175, "end_pos": 190, "type": "DATASET", "confidence": 0.9886682331562042}]}, {"text": "Note that while this is parallel data, we do not exploit its parallel nature.", "labels": [], "entities": []}, {"text": "POS classes The embeddings learned from English-German Europarl using POS classes from Wiktionary were visualized using the t-SNE technique (Van der Maaten and Hinton, 2008), and are shown in The model learns to cluster words very distinctively by their POS tag.", "labels": [], "entities": []}, {"text": "Monolingual word embedding models with short context windows typically cluster by POS, but here we seethe same effect for bilingual embeddings, i.e. words from both languages with the same POS tag cluster together.", "labels": [], "entities": []}, {"text": "Individual words, on the other hand, do not appear to retain the fine-grained relationships we normally observe in word embeddings (where similar words cluster closer together).", "labels": [], "entities": []}, {"text": "The question thus is whether such embeddings are more or less useful for cross-language POS tagging than bilingual embeddings based on translation equivalencies.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.8092528581619263}]}, {"text": "Translation classes Next, we induced EnglishGerman bilingual embeddings on Europarl using translations obtained from Google Translate.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9791799187660217}]}, {"text": "We derived a dictionary of the top 20k most frequent words in English, translated into German.", "labels": [], "entities": []}, {"text": "The embeddings are shown in The visualizations show that the models are able to extract very finegrained bilingual relationships, and some clusters still correspond to POS.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cross-language POS tagging. TC-Perc: type-constrained structured perceptron.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.7942230105400085}]}]}