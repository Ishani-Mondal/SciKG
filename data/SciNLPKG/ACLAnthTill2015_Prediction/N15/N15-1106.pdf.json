{"title": [{"text": "APRO: All-Pairs Ranking Optimization for MT Tuning", "labels": [], "entities": [{"text": "APRO", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6373475790023804}, {"text": "MT Tuning", "start_pos": 41, "end_pos": 50, "type": "TASK", "confidence": 0.9341393113136292}]}], "abstractContent": [{"text": "We present APRO, anew method for machine translation tuning that can handle large feature sets.", "labels": [], "entities": [{"text": "APRO", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.6478626728057861}, {"text": "machine translation tuning", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.8584587772687277}]}, {"text": "As opposed to other popular methods (e.g., MERT, MIRA, PRO), which involve ran-domness and require multiple runs to obtain a reliable result, APRO gives the same result on any run, given initial feature weights.", "labels": [], "entities": [{"text": "MERT", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.7141011953353882}, {"text": "MIRA", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.7681096196174622}, {"text": "APRO", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.5794024467468262}]}, {"text": "APRO follows the pairwise ranking approach of PRO (Hopkins and May, 2011), but instead of ranking a small sampled subset of pairs from the k-best list, APRO efficiently ranks all pairs.", "labels": [], "entities": [{"text": "APRO", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8700951337814331}, {"text": "PRO", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.7499645352363586}]}, {"text": "By obviating the need for manually determined sampling settings, we obtain more reliable results.", "labels": [], "entities": []}, {"text": "APRO converges more quickly than PRO and gives similar or better translation results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation tuning seeks to find feature weights that maximize translation quality.", "labels": [], "entities": [{"text": "Machine translation tuning", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.86141965786616}]}, {"text": "Recent efforts have focused on methods that scale to large numbers of features, and among these, PRO has gained popularity (Pairwise Ranking Optimization,).", "labels": [], "entities": [{"text": "PRO", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.8637063503265381}]}, {"text": "PRO's goal is to find feature weights such that the resulting k-best list entries are ranked in the same way that an evaluation function (e.g., BLEU,) ranks them.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9947433471679688}]}, {"text": "To do this, it labels pairs of translations for each sentence as positive or negative, depending on the gold ranking of the two pair elements given by BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.99481600522995}]}, {"text": "A binary classifier is trained on these labeled examples, resulting in new feature weights, and the procedure is iterated.", "labels": [], "entities": []}, {"text": "This * Markus Dreyer is now at Amazon, Inc., Seattle, WA.", "labels": [], "entities": []}, {"text": "procedure would ordinarily be too expensive since there are O(k 2 ) pairs per sentence, where both k and the number of sentences can be in the thousands, so billions of training examples would be produced per iteration.", "labels": [], "entities": []}, {"text": "Therefore, use subsampling to consider a small percentage of all pairs per sentence.", "labels": [], "entities": []}, {"text": "We present APRO (All-Pairs Ranking Optimization), a tuning approach that, like PRO, uses pairwise ranking for tuning.", "labels": [], "entities": [{"text": "APRO", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9716567397117615}]}, {"text": "Unlike PRO, it is not limited to optimizing a small percentage of pairs per sentence.", "labels": [], "entities": [{"text": "PRO", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9496080875396729}]}, {"text": "Based on an efficient ranking SVM formulation,), we find, in each iteration, feature weights that minimize ranking errors for all pairs of translations per sentence.", "labels": [], "entities": []}, {"text": "This tuning method inherits all the advantages of PRO-it is scalable, effective, easy to implement-and removes its limitations.", "labels": [], "entities": []}, {"text": "It does not require meta-tuning of sampling parameters since no sampling is used; it does not need to be run multiple times to obtain reliable results, like MERT, PRO, MIRA () and others, since it uses global optimization and is deterministic given initial feature weights; and it converges quickly.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.8285225033760071}]}], "datasetContent": [{"text": "We validate APRO on 6 diverse language pairs.", "labels": [], "entities": [{"text": "APRO", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.6231366395950317}]}, {"text": "For each one, we perform HMM-based word alignment () and phrase rule extraction on the training data.", "labels": [], "entities": [{"text": "HMM-based word alignment", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.8252032001813253}, {"text": "phrase rule extraction", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.6368359923362732}]}, {"text": "We use 20 standard features, incl.", "labels": [], "entities": []}, {"text": "8 reordering features, plus the sparse features listed for PBTM systems in.", "labels": [], "entities": [{"text": "PBTM", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.7946569919586182}]}, {"text": "For Ara-Eng and Chi-Eng, we use BOLT Y2 data sets.", "labels": [], "entities": [{"text": "BOLT Y2 data sets", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.8308437764644623}]}, {"text": "For all other languages, we sample train, dev, and test sets from in-house data.", "labels": [], "entities": []}, {"text": "describes the different data set sizes.", "labels": [], "entities": []}, {"text": "We use 5-gram LMs trained on the target side of the training data; for Ara-Eng and Chi-Eng, we add 2 LMs trained on English Gigaword and other sources.", "labels": [], "entities": [{"text": "English Gigaword", "start_pos": 116, "end_pos": 132, "type": "DATASET", "confidence": 0.8581684827804565}]}, {"text": "We tune on dev data.", "labels": [], "entities": []}, {"text": "In each tuning run, we use k = 500, except for Ara-Eng (k = 1500).", "labels": [], "entities": []}, {"text": "We use the same weight initialization for every tuning run, where most features are initialized to 0 and some dense features are initialized to 1 or -1.", "labels": [], "entities": []}, {"text": "During tuning, we use case-insensitive BLEU+1.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9776686429977417}]}, {"text": "We tune for  up to 30 iterations, 7 where we reset the accumulated k-best list after 10 iterations.", "labels": [], "entities": []}, {"text": "8 For PRO, we use \u0393=5000, \u039e=50, \u03b2=0.05, \u03a8=0.1, and (MegaM) regularization strength \u03bb=1 as described in.", "labels": [], "entities": [{"text": "PRO", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9104572534561157}]}, {"text": "For APRO, we use regularization strength C=0.01 and \u03a8=1, which effectively removes the weight interpolation step.", "labels": [], "entities": [{"text": "APRO", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.5037499666213989}]}, {"text": "We repeat each PRO tuning twice and report the mean of length ratios and case-sensitive BLEU scores on test data.", "labels": [], "entities": [{"text": "mean of length ratios", "start_pos": 47, "end_pos": 68, "type": "METRIC", "confidence": 0.86741803586483}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9809722900390625}]}, {"text": "For APRO, no repeated runs are necessary; it gives the same result on any run given initial feature weights.", "labels": [], "entities": [{"text": "APRO", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.45965373516082764}]}, {"text": "For APRO, we optimize using the implementation by Lee and Lin, which uses a truncated Newton method.", "labels": [], "entities": [{"text": "APRO", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.644457221031189}]}], "tableCaptions": []}