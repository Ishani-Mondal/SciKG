{"title": [{"text": "Discourse and Document-level Information for Evaluating Language Output Tasks", "labels": [], "entities": [{"text": "Evaluating Language Output Tasks", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.7519964277744293}]}], "abstractContent": [{"text": "Evaluating the quality of language output tasks such as Machine Translation (MT) and Automatic Summarisation (AS) is a challenging topic in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.8466331124305725}, {"text": "Automatic Summarisation (AS)", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.8327326774597168}, {"text": "Natural Language Processing (NLP)", "start_pos": 140, "end_pos": 173, "type": "TASK", "confidence": 0.6891032656033834}]}, {"text": "Recently, techniques focusing only on the use of outputs of the systems and source information have been investigated.", "labels": [], "entities": []}, {"text": "In MT, this is referred to as Quality Estimation (QE), an approach that uses machine learning techniques to predict the quality of unseen data, generalising from a few labelled data points.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9800931215286255}, {"text": "Quality Estimation (QE)", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.6871038794517517}]}, {"text": "Traditional QE research addresses sentence-level QE evaluation and prediction, disregarding document-level information.", "labels": [], "entities": [{"text": "QE evaluation and prediction", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.7654125243425369}]}, {"text": "Document-level QE requires a different setup from sentence-level, which makes the study of appropriate quality scores, features and models necessary.", "labels": [], "entities": [{"text": "Document-level QE", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.665526956319809}]}, {"text": "Our aim is to explore document-level QE of MT, focusing on discourse information.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.8278136849403381}]}, {"text": "However, the findings of this research can improve other NLP tasks, such as AS.", "labels": [], "entities": [{"text": "AS", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.9786049127578735}]}], "introductionContent": [{"text": "Evaluation metrics for Machine Translation (MT) and Automatic Summarisation (AS) tasks should be able to measure quality with respect to different aspects (e.g. fluency and adequacy) and they should be fast and scalable.", "labels": [], "entities": [{"text": "Machine Translation (MT) and Automatic Summarisation (AS) tasks", "start_pos": 23, "end_pos": 86, "type": "TASK", "confidence": 0.833337647219499}]}, {"text": "Human evaluation seems to be the most reliable (although it might introduce biases of reviewers).", "labels": [], "entities": []}, {"text": "However, it is expensive and cumbersome for large datasets; it is also not practical for certain scenarios, such as gisting in MT and summarisation of webpages.", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.8639498353004456}, {"text": "summarisation of webpages", "start_pos": 134, "end_pos": 159, "type": "TASK", "confidence": 0.9021874070167542}]}, {"text": "Automatic evaluation metrics (such as BLEU) and ROUGE (), based on human references, are widely used to evaluate MT and AS outputs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9990825653076172}, {"text": "ROUGE", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.9965699911117554}, {"text": "MT and AS outputs", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.7858307510614395}]}, {"text": "One limitation of these metrics is that if the MT or AS system outputs a translation or summary considerably different from the references, it does not really mean that it is a bad output.", "labels": [], "entities": [{"text": "MT or AS system outputs a translation or summary", "start_pos": 47, "end_pos": 95, "type": "TASK", "confidence": 0.5434429910447862}]}, {"text": "Another problem is that these metrics cannot be used in scenarios where the output of the system is to be used directly by end-users, for example a user reading the output of Google Translate 1 fora given news text cannot count on a reference for that translated text.", "labels": [], "entities": []}, {"text": "Quality Estimation (QE) approaches aim to predict the quality of MT systems without using references.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.737555855512619}, {"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9878250360488892}]}, {"text": "Instead, features (that maybe or may not be related to the MT system that produced this translations) are applied to source and target documents ().", "labels": [], "entities": []}, {"text": "The only requirement is data points with scores (e.g.: Humantargeted Translation Error Rate (HTER)) or even BLEU-style metrics).", "labels": [], "entities": [{"text": "Humantargeted Translation Error Rate (HTER))", "start_pos": 55, "end_pos": 99, "type": "METRIC", "confidence": 0.75219278889043}, {"text": "BLEU-style", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9956243634223938}]}, {"text": "These data points can be used to train supervised machine learning models (regressors or classifiers) to predict the scores of unseen data.", "labels": [], "entities": []}, {"text": "The advantage of these approaches is that we do not need to have all the words, sentences or documents of a task evaluated manually, we just need enough data points to train the machine learning model.", "labels": [], "entities": []}, {"text": "QE systems predict scores that reflect how good a translation is fora given scenario.", "labels": [], "entities": []}, {"text": "For example, a widely predicted score in QE is HTER, which measures the effort needed to post-edit a sentence.", "labels": [], "entities": [{"text": "HTER", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9953497648239136}]}, {"text": "A user of a QE system predicting HTER could decide whether to post-edit or translate sentences from scratch based on the score predicted for each sentence.", "labels": [], "entities": [{"text": "predicting HTER", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.5235448628664017}]}, {"text": "The vast majority of work done on QE is at sentence level.", "labels": [], "entities": [{"text": "QE", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9458461999893188}]}, {"text": "Document-level predictions, on the other hand, are interesting in scenarios where one wants to evaluate the overall score of an MT system or where the end-user is interested in the quality of the document as whole.", "labels": [], "entities": [{"text": "MT", "start_pos": 128, "end_pos": 130, "type": "TASK", "confidence": 0.9654526710510254}]}, {"text": "In addition, document-level features can also correlate well with quality scores, mainly because state-of-the-art MT systems translate documents at sentence level, disregarding discourse information.", "labels": [], "entities": [{"text": "MT", "start_pos": 114, "end_pos": 116, "type": "TASK", "confidence": 0.9646271467208862}]}, {"text": "Therefore, it is expected that the outputs of these systems may contain discourse problems.", "labels": [], "entities": []}, {"text": "In this work we focus on document-level QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.6542696356773376}]}, {"text": "Regarding features, discourse phenomena are being considered since they are linguistic phenomena that often manifest document-wide.", "labels": [], "entities": []}, {"text": "These phenomena are related to how sentences are connected, how genre and domain of a document are identified, anaphoric pronouns, etc.", "labels": [], "entities": []}, {"text": "Regarding document-level prediction, we focus on finding the ideal quality label for the task.", "labels": [], "entities": [{"text": "document-level prediction", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7573965489864349}]}, {"text": "Traditional evaluation metrics tend to yield similar scores for different documents.", "labels": [], "entities": []}, {"text": "This leads to low variation between the document quality scores with all these scores being close to the mean score.", "labels": [], "entities": []}, {"text": "Therefore, a quality label that captures document quality in a more sensitive way is needed.", "labels": [], "entities": []}, {"text": "Research on the use of linguistic features for QE and the use of discourse for improving MT and MT evaluation are presented in Section 2.", "labels": [], "entities": [{"text": "QE", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.910997211933136}, {"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9879383444786072}, {"text": "MT evaluation", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.9528017342090607}]}, {"text": "Section 3 presents the work done so far and the directions that we intend to follow.", "labels": [], "entities": []}, {"text": "Conclusions and future work are presented in Section 4 2 Document-level information for QE and MT Traditional systems translate documents at sentence level, disregarding document-wide information.", "labels": [], "entities": []}, {"text": "This means that sentences are translated without considering the relations in the whole document.", "labels": [], "entities": []}, {"text": "Therefore, information such as discourse structures can be lost in this process.", "labels": [], "entities": []}, {"text": "QE is also traditionally done at sentence level mainly because the majority of MT systems translate texts at this level.", "labels": [], "entities": [{"text": "QE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.948728084564209}]}, {"text": "Another reason is that sentencelevel approaches have more applications than other granularity levels, because they can explore the peculiarities of each sentence, being very useful for the post-edition task.", "labels": [], "entities": []}, {"text": "On the other hand, sentence-level approaches do not consider the document as a whole and information regarding discourse is disregarded.", "labels": [], "entities": []}, {"text": "Moreover, for scenarios in which post-edition is not possible, for example, gisting, quality predictions for the entire documents are more useful.", "labels": [], "entities": []}, {"text": "In this section we present related work on QE and the first research towards document-level QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9304249286651611}, {"text": "document-level QE", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.4280860424041748}]}, {"text": "Research on the use of discourse phenomena for MT improvement and MT evaluation are also presented.", "labels": [], "entities": [{"text": "MT improvement", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.98997563123703}, {"text": "MT evaluation", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.9881405532360077}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Average values of evaluation metrics in the WMT and LIG corpora", "labels": [], "entities": [{"text": "WMT and LIG corpora", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.6613087877631187}]}]}