{"title": [{"text": "Paradigm classification in supervised learning of morphology", "labels": [], "entities": [{"text": "Paradigm classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9023176431655884}]}], "abstractContent": [{"text": "Supervised morphological paradigm learning by identifying and aligning the longest common subsequence found in inflection tables has recently been proposed as a simple yet competitive way to induce morphological patterns.", "labels": [], "entities": [{"text": "morphological paradigm learning", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.7103784084320068}]}, {"text": "We combine this non-probabilistic strategy of inflection table generalization with a discriminative classifier to permit the reconstruction of complete inflection tables of unseen words.", "labels": [], "entities": [{"text": "inflection table generalization", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.6943944295247396}]}, {"text": "Our system learns morphological paradigms from labeled examples of inflection patterns (inflection tables) and then produces inflection tables from unseen lemmas or base forms.", "labels": [], "entities": []}, {"text": "We evaluate the approach on datasets covering 11 different languages and show that this approach results in consistently higher accuracies vis-` a-vis other methods on the same task, thus indicating that the general method is a viable approach to quickly creating high-accuracy morphological resources.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9690040349960327}]}], "introductionContent": [{"text": "Use of detailed and sophisticated morphological features has been found to be crucial for many downstream NLP tasks, including part-of-speech tagging and parsing.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.7648711800575256}]}, {"text": "However, creating an accurate widecoverage morphological analyzer fora new language that can be used in tandem with other higherlevel analyses is an arduous task.", "labels": [], "entities": [{"text": "widecoverage morphological analyzer", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.6446052690347036}]}, {"text": "Learning word inflection patterns by organizing related word-forms into morphological paradigms based on the longest common subsequence found in an inflection table has recently been proposed as a method for supervised and semisupervised induction of morphological processing tools from labeled data ().", "labels": [], "entities": []}, {"text": "Also, the argument that the LCS shared by different inflected forms of a word-even if discontinuous within a word-corresponds strongly to a crosslinguistic notion of a 'stem' has later been advanced independently on grounds of descriptive economy and minimum description length ().", "labels": [], "entities": []}, {"text": "We used this idea in () to create a relatively simple-to-implement system that learns paradigms from example inflection tables and is then able to reconstruct inflection tables for unseen words by comparing suffixes of new base forms to base forms seen during training.", "labels": [], "entities": []}, {"text": "The system performs well on available datasets and results in human-readable and editable output.", "labels": [], "entities": []}, {"text": "The longest common subsequence strategy itself shows little bias toward any specific morphological process such as prefixation, suffixation, or infixation.", "labels": [], "entities": []}, {"text": "Using the model, we argued, a selection of ready-inflected tables could be quickly provided by a linguist, allowing rapid development of morphological resources for languages for which few such resources exist.", "labels": [], "entities": []}, {"text": "Potentially, however, the model's commitment to a simple suffix-based learner is a weakness.", "labels": [], "entities": []}, {"text": "To assess this, we evaluate a similar LCS-based generalization system with a more refined discriminative classifier that takes advantage of substrings in the example data and performs careful feature selection.", "labels": [], "entities": []}, {"text": "We show that much higher accuracies can be achieved by combining the LCS paradigm generalization strategy with such a feature-based classi-fier that assigns unknown words to the LCS-learned paradigm based on substring features taken from word edges.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9787740111351013}, {"text": "LCS paradigm generalization", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.6131302316983541}]}, {"text": "This holds in particular for languages where paradigmatic behavior is triggered by material in the beginning of a word (e.g. German verbs).", "labels": [], "entities": []}, {"text": "We present experiments on 18 datasets in 11 languages varying in morphological complexity.", "labels": [], "entities": []}, {"text": "In all the experiments, the task is to reconstruct a complete inflection table from abase form, which usually corresponds to the lemma or dictionary form of a noun, verb, or adjective.", "labels": [], "entities": []}, {"text": "The experiments are divided into two sets.", "labels": [], "entities": []}, {"text": "In the first, we use an earlier dataset of Finnish, German, and Spanish to compare against other methods of paradigm learning.", "labels": [], "entities": []}, {"text": "In the second, we use a more comprehensive and complex dataset we have developed for 8 additional languages.", "labels": [], "entities": []}, {"text": "This new dataset is less regular and intended to be more realistic in that it also features defective or incomplete inflection tables and inflection tables containing various alternate forms, naturally making the classification task substantially more difficult.", "labels": [], "entities": []}, {"text": "Overall, supervised and semi-supervised learning of morphology by generalizing patterns from inflection tables is an active research field.", "labels": [], "entities": []}, {"text": "Recent work sharing our goals includes,, which works with a fully Bayesian model,,, which attempts to learn lexicons from morphologically annotated corpora, and, who train a discriminative model that learns transformation rules between word forms.", "labels": [], "entities": []}, {"text": "We directly compare our results against the last using the same dataset.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: section 2 contains the experimental setup, section 3 the datasets, and section 4 the results and discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "The accuracies obtained on the first three-language comparison experiment are shown in.", "labels": [], "entities": []}, {"text": "Here, we see a consistent improvement upon the maxsuff -strategy (AFH14) that simply picks the longest matching suffix among the base forms seen and assigns the unseen word to the same paradigm (breaking ties by paradigm frequency), as well as improvement over other learning strategies (D&DN13).", "labels": [], "entities": [{"text": "AFH14", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.4745377004146576}]}, {"text": "Particularly marked is the improved accuracy on German verbs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9990386962890625}]}, {"text": "We assume that this is because German verb prefixes, which are ignored in a suffix-based classifier, contain information that is useful in classifying verb behavior.", "labels": [], "entities": [{"text": "classifying verb behavior", "start_pos": 139, "end_pos": 164, "type": "TASK", "confidence": 0.8673842946688334}]}, {"text": "German verbs that contain socalled inseparable prefixes like miss-, ver-, widerdo not prefix a ge-in the past participle form.", "labels": [], "entities": []}, {"text": "For example: kaufen \u223c gekauft, brauchen \u223c gebraucht, legen \u223c gelegt, but verkaufen \u223c verkauft, widerlegen \u223c widerlegt, missbrauchen \u223c missbraucht, reflecting the replacement of the standard ge-by the inseparable prefix.", "labels": [], "entities": []}, {"text": "There are many such inseparable prefixes that immediately trigger this behavior (although some prefixes only occasionally show inseparable behavior), yet this information is lost when only looking at suffixes at classification time.", "labels": [], "entities": []}, {"text": "This analysis is supported by the fact that, during feature selection, German verbs was the only dataset in this first experiment where word prefixes were not removed by the feature selection process.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.8065772354602814}]}, {"text": "The results of the second experiment are given in tables 2 (per table accuracy) and 3 (per form accuracy).", "labels": [], "entities": []}, {"text": "The tables contain information about how many inflection tables were input on average over 5 folds to the learner (#tbl), how many paradigms this reduced to (#par), and how many forms (slots) each paradigm has (#forms).", "labels": [], "entities": []}, {"text": "The mfreq column is a baseline where the classifier always picks the most populated paradigm, i.e. the paradigm that resulted from combining the largest number of different inflection tables by the LCS process.", "labels": [], "entities": []}, {"text": "The AFH14 shows the performance of a maximal suffix matching classifier, identical to that used in.", "labels": [], "entities": [{"text": "AFH14", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.5913066864013672}]}], "tableCaptions": [{"text": " Table 1: Results on experiment 1. Here AFH14 stands for Ahlberg et al. (2014) and D&DN for Durrett and DeNero  (2013). The SVM-columns show the results of the current method.", "labels": [], "entities": [{"text": "AFH14", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.9977098703384399}, {"text": "D&DN", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9484571814537048}]}, {"text": " Table 1. Here,  we see a consistent improvement upon the max- suff -strategy (AFH14) that simply picks the longest  matching suffix among the base forms seen and as- signs the unseen word to the same paradigm (break- ing ties by paradigm frequency), as well as improve- ment over other learning strategies (D&DN13). Par- ticularly marked is the improved accuracy on Ger- man verbs. We assume that this is because German  verb prefixes, which are ignored in a suffix-based  classifier, contain information that is useful in clas- sifying verb behavior. German verbs that contain so- called inseparable prefixes like miss-, ver-, wider- do not prefix a ge-in the past participle form. For ex- ample: kaufen \u223c gekauft, brauchen \u223c gebraucht,  legen \u223c gelegt, but verkaufen \u223c verkauft, wider- legen \u223c widerlegt, missbrauchen \u223c missbraucht,  reflecting the replacement of the standard ge-by the  inseparable prefix. There are many such inseparable  prefixes that immediately trigger this behavior (al- though some prefixes only occasionally show insep- arable behavior), yet this information is lost when  only looking at suffixes at classification time. This  analysis is supported by the fact that, during feature", "labels": [], "entities": [{"text": "AFH14", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.5619964599609375}, {"text": "D&DN13", "start_pos": 308, "end_pos": 314, "type": "DATASET", "confidence": 0.8208560546239217}, {"text": "accuracy", "start_pos": 355, "end_pos": 363, "type": "METRIC", "confidence": 0.9989198446273804}]}, {"text": " Table 2: Per table accuracy results on the second exper- iment. 5-fold cross-validation is used throughout. The  #tbl-column shows the number of inflection tables input  to the LCS-learner and the #par column shows the num- ber of resulting unique paradigms. The mfreq-column il- lustrates a baseline of simply picking the most frequent  paradigm, while AFH14 is the strategy of finding the  longest suffix match to the base forms in the training data  (Ahlberg et al., 2014). The SVM-column shows the re- sults discussed in this paper.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9899056553840637}, {"text": "AFH14", "start_pos": 355, "end_pos": 360, "type": "METRIC", "confidence": 0.9543240070343018}]}, {"text": " Table 3: Per form accuracy results on the second exper- iment. 5-fold cross-validation is used throughout. The  #forms-column shows the number of different slots in the  paradigms. Other columns are as in table 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9384211897850037}]}]}