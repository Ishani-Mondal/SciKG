{"title": [{"text": "Improving the Inference of Implicit Discourse Relations via Classifying Explicit Discourse Connectives", "labels": [], "entities": [{"text": "Improving", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9824830293655396}]}], "abstractContent": [{"text": "Discourse relation classification is an important component for automatic discourse parsing and natural language understanding.", "labels": [], "entities": [{"text": "Discourse relation classification", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8358399868011475}, {"text": "automatic discourse parsing", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.606606662273407}, {"text": "natural language understanding", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.6589249968528748}]}, {"text": "The performance bottleneck of a discourse parser comes from implicit discourse relations , whose discourse connectives are not overtly present.", "labels": [], "entities": []}, {"text": "Explicit discourse connec-tives can potentially be exploited to collect more training data to collect more data and boost the performance.", "labels": [], "entities": []}, {"text": "However, using them indiscriminately has been shown to hurt the performance because not all discourse con-nectives can be dropped arbitrarily.", "labels": [], "entities": []}, {"text": "Based on this insight, we investigate the interaction between discourse connectives and the discourse relations and propose the criteria for selecting the discourse connectives that can be dropped independently of the context without changing the interpretation of the discourse.", "labels": [], "entities": []}, {"text": "Extra training data collected only by the freely omis-sible connectives improve the performance of the system without additional features.", "labels": [], "entities": []}], "introductionContent": [{"text": "The analysis of discourse-level structure has received increasing attention from the field in recent years.", "labels": [], "entities": [{"text": "analysis of discourse-level structure", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.7549228817224503}]}, {"text": "Discourse-level analysis is typically concerned with relations between clauses and sentences, linguistic units that go beyond sentence boundaries.", "labels": [], "entities": [{"text": "Discourse-level analysis", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.851169228553772}]}, {"text": "There area few conceptions of the discourse structure representation of a text such as a tree, or a graph ().", "labels": [], "entities": []}, {"text": "In the work we describe here, we adopt the view of the Penn Discourse Treebank (PDTB) (, which views a text as a series of local discourse relations, each of which consists of a discourse connective as a predicate taking two arguments.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 55, "end_pos": 85, "type": "DATASET", "confidence": 0.9556979636351267}]}, {"text": "Syntactically, these two arguments are typically realized as clauses or sentences.", "labels": [], "entities": []}, {"text": "The discourse connective (underlined) can either be explicit, as in (1), or implicit, as in (2) [So much of the stuff poured into its Austin, Texas, offices that its mail rooms there simply stopped delivering it] . Implicit=so [Now, thousands of mailers, catalogs and sales pitches go straight into the trash] Arg2 . Determining the sense of an explicit discourse relation such as (1) is straightforward since \"because\" is a strong indicator that the relation between the two arguments is CONTINGENCY.CAUSE.", "labels": [], "entities": [{"text": "Implicit", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.9892410635948181}, {"text": "Arg2", "start_pos": 310, "end_pos": 314, "type": "METRIC", "confidence": 0.9884771108627319}, {"text": "CONTINGENCY.CAUSE", "start_pos": 489, "end_pos": 506, "type": "METRIC", "confidence": 0.9315342903137207}]}, {"text": "This task effectively amounts to disambiguating the sense of discourse connective, which can be done with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9924126267433167}]}, {"text": "However, in the absence of an explicit discourse connective, inferring the sense of a discourse relation has proved to a very challenging task).", "labels": [], "entities": []}, {"text": "The sense is no longer localized on one or two discourse connectives and must now be inferred solely based on its two textual arguments.", "labels": [], "entities": []}, {"text": "Given the limited amount of annotated data in comparison to the number of features needed, the process of building a classifier is plagued by the data sparsity problem (.", "labels": [], "entities": []}, {"text": "As a result, the classification accuracy of implicit discourse relations remains much lower than that of explicit discourse relations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.988395094871521}]}, {"text": "One potential method for reducing the data sparsity problem is through a distantly supervised learning paradigm, which is the direction we take in this work.", "labels": [], "entities": []}, {"text": "Distant supervision approaches make use of prior knowledge or heuristics to cheaply obtain weakly labeled data, which potentially contain a small number of false labels.", "labels": [], "entities": []}, {"text": "Weakly labeled data can be collected from unannotated data and incorporated in the model training process to supplement manually labeled data.", "labels": [], "entities": []}, {"text": "For our task, we can collect instances of explicit discourse relations from unannotated data by some simple heuristics.", "labels": [], "entities": []}, {"text": "After dropping the discourse connectives, we should be able to treat them as additional implicit discourse relations.", "labels": [], "entities": []}, {"text": "The approach assumes that when the discourse connective is omitted, the discourse relation remains the same, which is a popular assumption in discourse analysis).", "labels": [], "entities": []}, {"text": "This assumption turns out to be too strong in many cases as illustrated in If \"Nonetheless\" is dropped in (3), one can no longer infer the COMPARISON relation.", "labels": [], "entities": [{"text": "COMPARISON", "start_pos": 139, "end_pos": 149, "type": "METRIC", "confidence": 0.8544436097145081}]}, {"text": "Instead, one would naturally infer a CONTINGENCY relation.", "labels": [], "entities": [{"text": "CONTINGENCY", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.974858283996582}]}, {"text": "Dropping the connective and adding the relation as a training sample adds noise to the training set and can only hurt the performance.", "labels": [], "entities": []}, {"text": "In addition, certain types of explicit discourse relations have no corresponding implicit discourse relations.", "labels": [], "entities": []}, {"text": "For example, discourse relations of the type CONTINGENY.CONDITION are almost always expressed with an explicit discourse connective and do not exist in implicit relations.", "labels": [], "entities": []}, {"text": "We believe this also explains the lack of success in previous attempts to boost the performance of implicit discourse relation detection with this approach..", "labels": [], "entities": [{"text": "discourse relation detection", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.6335595548152924}]}, {"text": "This suggests that in order for this approach to work, we need to identify instances of explicit discourse relations that closely match the characteristics of implicit discourse relations.", "labels": [], "entities": []}, {"text": "In this paper, we propose two criteria for selecting such explicit discourse relation instances: omission rate and context differential.", "labels": [], "entities": []}, {"text": "Our selection criteria first classify discourse connectives by their distributional properties and suggest that not all discourse connectives are truly optional and not all implicit and explicit discourse relations are equivalent, contrary to commonly held beliefs in previous studies of discourse connectives.", "labels": [], "entities": []}, {"text": "We show that only the freely omissible discourse connectives gather additional training instances that lead to significant performance gain against a strong baseline.", "labels": [], "entities": []}, {"text": "Our approach improves the performance of implicit discourse relations without additional feature engineering in many settings and opens doors to more sophisticated models that require more training data.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe the discourse connective selection criteria.", "labels": [], "entities": [{"text": "discourse connective selection", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.623808224995931}]}, {"text": "In Section 3, we present our discourse connective classification method and experimental results that demonstrate its impact on inferring implicit discourse relations.", "labels": [], "entities": [{"text": "discourse connective classification", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.6799633800983429}]}, {"text": "We discuss related work and conclude our findings in Section 4 and 5 respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use two datasets for the purposes of extracting and selecting weakly labeled explicit discourse relation instances: the Penn Discourse Treebank 2.0 () and the English Gigaword corpus version 3 (.", "labels": [], "entities": [{"text": "Penn Discourse Treebank 2.0", "start_pos": 123, "end_pos": 150, "type": "DATASET", "confidence": 0.9744464308023453}, {"text": "English Gigaword corpus version 3", "start_pos": 162, "end_pos": 195, "type": "DATASET", "confidence": 0.8712447881698608}]}, {"text": "The Penn Discourse Treebank (PDTB) is the largest manually annotated corpus of discourse relations on top of one million word tokens from the Wall Street Journal (.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.9391760428746542}, {"text": "Wall Street Journal", "start_pos": 142, "end_pos": 161, "type": "DATASET", "confidence": 0.8935808340708414}]}, {"text": "Each discourse relation in the PDTB is annotated with a semantic sense in the PDTB sense hierarchy, which has three levels: CLASS, TYPE and SUBTYPE.", "labels": [], "entities": [{"text": "TYPE", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9841946363449097}]}, {"text": "In this work, we are primarily concerned with the four top-level CLASS senses: EXPANSION, COMPARISON, CONTINGENCY, and TEMPORAL.", "labels": [], "entities": [{"text": "EXPANSION", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9253913760185242}, {"text": "CONTINGENCY", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.7151473760604858}, {"text": "TEMPORAL", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.8425227403640747}]}, {"text": "The distribution of top-level senses of implicit discourse relations is shown in.", "labels": [], "entities": []}, {"text": "The spans of text that participate in the discourse relation are also explicitly annotated.", "labels": [], "entities": []}, {"text": "These are called ARG1 or ARG2, depending on its relationship with the discourse connective.", "labels": [], "entities": [{"text": "ARG1", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.6322558522224426}, {"text": "ARG2", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.47282645106315613}]}, {"text": "The PDTB is our corpus of choice for its lexical groundedness.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9414677023887634}]}, {"text": "The existence of a discourse relation must be linked or grounded to a discourse connective.", "labels": [], "entities": []}, {"text": "More importantly, this applies to not only explicit discourse connectives that occur naturally as part of the text but also to implicit discourse relations where a discourse connective is added by annotators during the annotation process.", "labels": [], "entities": []}, {"text": "This is crucial to the work reported herein that it allows us to compare the distribution of the same connective in explicit and implicit discourse relations.", "labels": [], "entities": []}, {"text": "In the next subsection, we will explain in detail how we compute the comparison measures and apply them to the selection of explicit discourse connectives that can be used for collecting good weakly labeled data.", "labels": [], "entities": []}, {"text": "We use the Gigaword corpus, a large unannotated newswire corpus, to extract and select instances of explicit discourse discourse relations to supplement the manually annotated instances from the PDTB.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.9328466951847076}, {"text": "PDTB", "start_pos": 195, "end_pos": 199, "type": "DATASET", "confidence": 0.9312711954116821}]}, {"text": "The Gigaword corpus is used for its large size of 2.9 billion words and its similarity to the Wall Street Journal data from the PDTB.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.937368243932724}, {"text": "Wall Street Journal data from the PDTB", "start_pos": 94, "end_pos": 132, "type": "DATASET", "confidence": 0.9306145736149379}]}, {"text": "The source of the corpus is drawn from six distinct international sources of English newswire dating from 1994 -2006.", "labels": [], "entities": []}, {"text": "We use this corpus to extract weakly labeled data for the experiment.", "labels": [], "entities": []}, {"text": "We formulate the implicit relation classification task as a 4-way classification task in a departure from previous practice where the task is usually setup as four one vs other binary classification tasks so that the effect of adding the distant supervision from the weakly labeled data can be more easily studied.", "labels": [], "entities": [{"text": "relation classification task", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.7735369404157003}]}, {"text": "We also believe this setup is more natural in realistic settings.", "labels": [], "entities": []}, {"text": "Each classification instance consists of the two arguments of an implicit discourse relation, typically adjacent pairs of sentences in a text.", "labels": [], "entities": []}, {"text": "The distribution of the sense labels is shown in Table 2.", "labels": [], "entities": []}, {"text": "We follow the data split used in previous work fora consistent comparison).", "labels": [], "entities": []}, {"text": "The PDTB corpus is split into a training set, development set, and test set.", "labels": [], "entities": [{"text": "PDTB corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8765615820884705}]}, {"text": "Sections 2 to 20 are used to train classifiers.", "labels": [], "entities": []}, {"text": "Sections 0 and 1 are used for developing feature sets and tuning models.", "labels": [], "entities": []}, {"text": "Section 21 and 22 are used for testing the systems.", "labels": [], "entities": []}, {"text": "To evaluate our method for selecting explicit discourse relation instances, we extract weakly labeled discourse relations from the Gigaword corpus for each class of discourse connective such that the discourse connectives are equally represented within the class.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 131, "end_pos": 146, "type": "DATASET", "confidence": 0.9443099498748779}]}, {"text": "We train and test Maximum Entropy classifiers by adding varying number of randomly selected explicit discourse discourse relation instances to the manually annotated implicit discourse relations in the PDTB as training data.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 202, "end_pos": 206, "type": "DATASET", "confidence": 0.9354954361915588}]}, {"text": "We do this for each class of discourse connectives as presented in.", "labels": [], "entities": []}, {"text": "We perform 30 trials of this experiment and compute average accuracy rates to smooth out the variation from random shuffling of the weakly labeled data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9745696187019348}]}, {"text": "The statistical models used in this study are from the MALLET implementation with its default setting).", "labels": [], "entities": [{"text": "MALLET implementation", "start_pos": 55, "end_pos": 76, "type": "DATASET", "confidence": 0.8143536150455475}]}, {"text": "Features used in all experiments are taken from the state-of-the-art implicit discourse relation classification system).", "labels": [], "entities": [{"text": "implicit discourse relation classification", "start_pos": 69, "end_pos": 111, "type": "TASK", "confidence": 0.6307270005345345}]}, {"text": "The feature set consists of combinations of various lexical features, production rules, and Brown cluster pairs.", "labels": [], "entities": []}, {"text": "These features are described in greater detail by and.", "labels": [], "entities": []}, {"text": "Instance reweighting is required when using weakly labeled data because the training set no longer represents the natural distribution of the labels.", "labels": [], "entities": []}, {"text": "We reweight each instance such that the sums of the weights of all the instances of the same label are equal.", "labels": [], "entities": []}, {"text": "More precisely, if an instance i is from class j, then the weight for the instance w ij is equal to the inverse proportion of class j: where c j is the total number of instances from class j and k is the number of classes in the dataset of size n.", "labels": [], "entities": []}, {"text": "It is trivial to show that the sum of the weights for all instances from class j is exactly n k for all classes.", "labels": [], "entities": []}, {"text": "The impact of different classes of weakly labeled explicit discourse connective relations is illustrated in.", "labels": [], "entities": []}, {"text": "The results show that expicit discourse relations with freely omissible discourse connectives (high OR and low JSD) improve the performance on the standard test set and outperform the other classes of discourse connectives and the naive approach where all of the discourse connectives are used.", "labels": [], "entities": [{"text": "OR", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.9910157918930054}, {"text": "JSD", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9084125757217407}]}, {"text": "In addition, it shows that on average, the system with weakly labeled data from freely omissible discourse connectives continues to rise as we increase the number of samples unlike the other classes of discourse connectives, which show the opposite trend.", "labels": [], "entities": []}, {"text": "This suggests that discourse connectives must have both high omission rates and low context differential between implicit and explicit use of the connectives in order to be helpful to the inference of implicit discourse relations.", "labels": [], "entities": []}, {"text": "presents results that show, overall, our best performing system, the one using distant supervision from freely omissible discourse connectives, raises the accuracy rate from 0.550 to 0.571 (p < 0.05; bootstrap test) and the macro-average F 1 score from 0.384 to 0.405.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 155, "end_pos": 168, "type": "METRIC", "confidence": 0.9872640073299408}, {"text": "F 1 score", "start_pos": 238, "end_pos": 247, "type": "METRIC", "confidence": 0.9661097725232443}]}, {"text": "We achieve such performance after we tune the subset of weakly labeled data to maximize the performance on the development set.", "labels": [], "entities": []}, {"text": "Our distant supervision approach improves the performance by adding more weakly labeled data and no additional features.", "labels": [], "entities": []}, {"text": "For a more direct comparison with previous results, we also replicated the state-of-the-art system described in, who follows the practice of the first work on this topic () insetting up the task as four binary one vs. other classifiers.", "labels": [], "entities": []}, {"text": "The results are presented in: The performance of our approach on the binary classification task formulation.", "labels": [], "entities": [{"text": "binary classification task formulation", "start_pos": 69, "end_pos": 107, "type": "TASK", "confidence": 0.8177744895219803}]}, {"text": "did not improve as the Expansion class in the four-way classification).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The distribution of senses of implicit discourse  relations in the PDTB", "labels": [], "entities": [{"text": "PDTB", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.7456493973731995}]}, {"text": " Table 1: Classification of discourse connectives based on omission rate (OR) and Jensen-Shannon Divergence context  differential (JSD).", "labels": [], "entities": [{"text": "Classification of discourse connectives", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.796345129609108}, {"text": "omission rate (OR)", "start_pos": 59, "end_pos": 77, "type": "METRIC", "confidence": 0.8774120330810546}, {"text": "Jensen-Shannon Divergence context  differential (JSD)", "start_pos": 82, "end_pos": 135, "type": "METRIC", "confidence": 0.6965627031666892}]}, {"text": " Table 4. The results show that the  extra data extracted from the Gigaword Corpus is particu- larly helpful for minority classes such as Comparison vs.  Others and Temporal vs Others, where our current sys- tem significantly outperforms that of Rutherford and Xue  (2014). Interestingly, the Expansion vs. Others classifier", "labels": [], "entities": [{"text": "Gigaword Corpus", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.9278858006000519}]}, {"text": " Table 3: Our current 4-way classification system outper- forms the baseline overall. The difference in accuracy is  statistically significant (p < 0.05; bootstrap test).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9994606375694275}]}, {"text": " Table 4: The performance of our approach on the binary  classification task formulation.", "labels": [], "entities": [{"text": "binary  classification task formulation", "start_pos": 49, "end_pos": 88, "type": "TASK", "confidence": 0.826886922121048}]}, {"text": " Table 5: The accuracy rates for the freely omissible class  are higher than the ones for the other classes both when  using the Gigaword data alone and when using it in con- junction with the implicit relations in the PDTB.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995369911193848}, {"text": "Gigaword data", "start_pos": 129, "end_pos": 142, "type": "DATASET", "confidence": 0.9129861891269684}, {"text": "PDTB", "start_pos": 219, "end_pos": 223, "type": "DATASET", "confidence": 0.9658339619636536}]}, {"text": " Table 6: The sense distribution by connective class.", "labels": [], "entities": []}]}