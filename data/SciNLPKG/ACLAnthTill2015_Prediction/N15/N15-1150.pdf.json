{"title": [{"text": "Context-Dependent Automatic Response Generation Using Statistical Machine Translation Techniques", "labels": [], "entities": [{"text": "Context-Dependent Automatic Response Generation", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7872540205717087}, {"text": "Statistical Machine Translation", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6566994686921438}]}], "abstractContent": [{"text": "Developing a system that can automatically respond to a user's utterance has recently become a topic of research in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 116, "end_pos": 143, "type": "TASK", "confidence": 0.648791084686915}]}, {"text": "However, most works on the topic take into account only a single preceding utterance to generate a response.", "labels": [], "entities": []}, {"text": "Recent works demonstrate that the application of statistical machine translation (SMT) techniques towards monolingual dialogue setting, in which a response is treated as a translation of a stimulus , has a great potential, and we exploit the approach to tackle the context-dependent response generation task.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 49, "end_pos": 86, "type": "TASK", "confidence": 0.7804204722245535}, {"text": "monolingual dialogue setting", "start_pos": 106, "end_pos": 134, "type": "TASK", "confidence": 0.6562607983748118}, {"text": "context-dependent response generation task", "start_pos": 265, "end_pos": 307, "type": "TASK", "confidence": 0.6708281934261322}]}, {"text": "We attempt to extract relevant and significant information from the wider contextual scope of the conversation, and incorporate it into the SMT techniques.", "labels": [], "entities": [{"text": "SMT", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.9941390752792358}]}, {"text": "We also discuss the advantages and limitations of this approach through our experimental results .", "labels": [], "entities": []}], "introductionContent": [{"text": "Various approaches have been applied to the response generation task, each with its own merits and drawbacks.", "labels": [], "entities": [{"text": "response generation task", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.8960921367009481}]}, {"text": "While one of the main concerns on the topic has been the semantic relevance of the response, it has mostly been discussed in terms of a limited conversational scope, mostly a single utterance.", "labels": [], "entities": []}, {"text": "This provides us with a room for research on a wider scope of conversation, which reflects not only a single preceding utterance, but the overall context of the current conversation.", "labels": [], "entities": []}, {"text": "SMT-based data-driven approach to the response generation task was recently introduced by.", "labels": [], "entities": [{"text": "SMT-based data-driven", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8644275069236755}, {"text": "response generation task", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8620772957801819}]}, {"text": "They demonstrated that it was better-suited for response generation than some of the previous approaches, including information retrieval approach.", "labels": [], "entities": [{"text": "response generation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8997935354709625}, {"text": "information retrieval", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.8589935302734375}]}, {"text": "We exploit this model to address the above-mentioned problem of reflecting a wider scope of conversation.", "labels": [], "entities": []}, {"text": "We present a context-dependent model where we attempt to generate more semantically relevant and diverse responses by adding the semantically important words from previous utterances to the most recent one.", "labels": [], "entities": []}, {"text": "By doing so, we hope not only to diversify the responses, but also to be able to take semantics from broader scope of the conversation into account.", "labels": [], "entities": []}, {"text": "2 Response Generation using SMT 2.1 Overview remarked that stimulus-response pairs in the same language often have a strong structural resemblance, as shown in the example conversation below, that maybe exploited in SMT platforms.", "labels": [], "entities": [{"text": "SMT 2.1", "start_pos": 28, "end_pos": 35, "type": "TASK", "confidence": 0.869866132736206}, {"text": "SMT", "start_pos": 216, "end_pos": 219, "type": "TASK", "confidence": 0.9705411195755005}]}, {"text": "In the usual SMT setting, a string fin a source language is translated into a string e in a target language according to probability distribution p(e|f ) ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9956666231155396}]}, {"text": "Ritter et al. applied the SMT techniques to monolingual conversation setting, and treated the response as the translation of the stimulus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9925742149353027}, {"text": "monolingual conversation setting", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.6495053569475809}]}, {"text": "Stimulus: What is your hobby?", "labels": [], "entities": []}, {"text": "Response: My hobby is hiking.", "labels": [], "entities": []}], "datasetContent": [{"text": "One of the challenging aspects of the researches on conversation is its distinct nature in which there is an extremely wide range of acceptable candidate responses to a stimulus, unlike usual bilingual translation tasks where there are typically pre-set candidates to be referenced with high reliability.", "labels": [], "entities": []}, {"text": "Using the automatic evaluation metrics, we obtained slight improvements; for example, BLEU score (), with the actual responses from Twitter as the gold standard, increased from 0.82 for baseline to 0.89 for the pair-based approach.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9866276681423187}]}, {"text": "For the above-mentioned reason, however, we found it dubious whether a higher score in these metrics corresponds to better responses, and we thus resort to human manual evaluation as our primary source of evaluation.", "labels": [], "entities": []}, {"text": "We performed a human evaluation on Amazon Mechanical Turk (Buhrmester et al., 2011).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.94775919119517}]}, {"text": "The evaluation task consisted of four different sets of 100 questions, each set of which was handled by 10 workers.", "labels": [], "entities": []}, {"text": "Each question was a ranking task, and the workers were shown apart of conversation and were instructed to rank the responses that followed  the conversation in consideration of their relevance to the topic of the conversation.", "labels": [], "entities": []}, {"text": "For all questions, workers were given four responses; the actual response from Twitter, one generated by the baseline model, and two by each of our context-dependent models.", "labels": [], "entities": []}, {"text": "The order of responses was randomized for each question.", "labels": [], "entities": []}, {"text": "In addition, in order to filter out the workers who do not take the tasks seriously, generating noise answers, we selected 10 questions that had obvious answers, and rejected the answers by the workers who failed to achieve 70% or higher accuracy on those questions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.9951924085617065}]}, {"text": "As stated in Section 3.1, the threshold for length of source sentence to determine whether to add words or not was set to the average length of source sentences throughout the training data, which in our case was 10.", "labels": [], "entities": []}, {"text": "In roughly half of 400 questions, no words were added to the source sentence, and 1 to 6 words were added for roughly 25 to 30 questions respectively.", "labels": [], "entities": []}, {"text": "Beyond 6 words, the number of questions begins to decline.", "labels": [], "entities": []}, {"text": "shows how our models performed against the actual responses, the baseline model, and each other, in regards to the number of questions for which our models were ranked higher.", "labels": [], "entities": []}, {"text": "Overall, the table shows that our models were preferred over the baseline model, but performed poorly against the actual responses as expected.", "labels": [], "entities": []}, {"text": "Yet, it was able to perform better than the actual responses in roughly  15% of the questions, especially when the actual responses were grammatically poor, or irrelevant to the topic of the conversation.", "labels": [], "entities": []}, {"text": "There was no significant difference between the performances of our models.", "labels": [], "entities": []}, {"text": "It also shows the p-value and mutual agreement between two models.", "labels": [], "entities": []}, {"text": "Using S coefficient) as a measurement of agreement yields the following result.", "labels": [], "entities": [{"text": "agreement", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9073174595832825}]}, {"text": "Most of them fall into \"moderate agreement\" range of 0.4 to 0.6, except Token-based model against Pair-based model is slightly lower and falls into \"fair agreement\" range (.", "labels": [], "entities": []}, {"text": "shows the distribution of each model over each ranking and their average rankings.", "labels": [], "entities": []}, {"text": "Our models outperform the baseline model in higher rankings.", "labels": [], "entities": []}, {"text": "features examples of responses generated by each model and the actual responses on Twitter, along with their average ranking in the final evaluation.", "labels": [], "entities": []}, {"text": "In the first conversation, one of our models was ranked higher than both the baseline model and the actual response.", "labels": [], "entities": []}, {"text": "In other conversations, our models were ranked higher than the baseline model, but lower than the actual response.", "labels": [], "entities": []}, {"text": "Generally, our models have a wider range of topic-relevant vocabularies, and sound comparatively coherent than the baseline model, without too much grammatical violations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performances against Each Model", "labels": [], "entities": []}, {"text": " Table 2: Rankings from Human Evaluation", "labels": [], "entities": [{"text": "Human Evaluation", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.6797549575567245}]}, {"text": " Table 3: Examples of Responses", "labels": [], "entities": [{"text": "Responses", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.4719216823577881}]}]}