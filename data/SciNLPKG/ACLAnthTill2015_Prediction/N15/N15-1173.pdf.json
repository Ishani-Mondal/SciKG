{"title": [{"text": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "labels": [], "entities": [{"text": "Translating Videos to Natural Language", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8708387851715088}]}], "abstractContent": [{"text": "Solving the visual symbol grounding problem has long been a goal of artificial intelligence.", "labels": [], "entities": [{"text": "Solving the visual symbol grounding problem", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.6869606624046961}]}, {"text": "The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images.", "labels": [], "entities": [{"text": "natural language grounding in static images", "start_pos": 101, "end_pos": 144, "type": "TASK", "confidence": 0.7701201538244883}]}, {"text": "In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both con-volutional and recurrent structure.", "labels": [], "entities": []}, {"text": "Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words.", "labels": [], "entities": []}, {"text": "By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies.", "labels": [], "entities": []}, {"text": "We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.", "labels": [], "entities": [{"text": "subject, verb, and object prediction", "start_pos": 76, "end_pos": 112, "type": "TASK", "confidence": 0.6758962273597717}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.7036738395690918}]}], "introductionContent": [{"text": "For most people, watching a brief video and describing what happened (in words) is an easy task.", "labels": [], "entities": []}, {"text": "For machines, extracting the meaning from video pixels and generating natural-sounding language is a very complex problem.", "labels": [], "entities": [{"text": "extracting the meaning from video pixels", "start_pos": 14, "end_pos": 54, "type": "TASK", "confidence": 0.8349249958992004}]}, {"text": "Solutions have been proposed for narrow domains with a small set of known actions and objects, e.g., (), but generating descriptions for \"in-thewild\" videos such as the YouTube domain () remains an open challenge.", "labels": [], "entities": []}, {"text": "Progress in open-domain video description has been difficult in part due to large vocabularies and  very limited training data consisting of videos with associated descriptive sentences.", "labels": [], "entities": [{"text": "open-domain video description", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.6212183634440104}]}, {"text": "Another serious obstacle has been the lack of rich models that can capture the joint dependencies of a sequence of frames and a corresponding sequence of words.", "labels": [], "entities": []}, {"text": "Previous work has simplified the problem by detecting a fixed set of semantic roles, such as subject, verb, and object (, as an intermediate representation.", "labels": [], "entities": []}, {"text": "This fixed representation is problematic for large vocabularies and also leads to oversimplified rigid sentence templates which are unable to model the complex structures of natural language.", "labels": [], "entities": []}, {"text": "In this paper, we propose to translate from video pixels to natural language with a single deep neural network.", "labels": [], "entities": []}, {"text": "Deep NNs can learn powerful features (), but require a lot of supervised training data.", "labels": [], "entities": []}, {"text": "We address the problem by transferring knowledge from auxiliary tasks.", "labels": [], "entities": []}, {"text": "Each frame of the video is modeled by a convolutional (spatially-invariant) network pre-trained on 1.2M+ images with category labels ().", "labels": [], "entities": []}, {"text": "The meaning state and sequence of words is modeled by a recurrent (temporally invariant) deep network pre-trained on 100K+ Flickr) and COCO () images with associated sentence captions.", "labels": [], "entities": []}, {"text": "We show that such knowledge transfer significantly improves performance on the video task.", "labels": [], "entities": []}, {"text": "Our approach is inspired by recent breakthroughs reported by several research groups in image-to-text generation, in particular, the work by . They applied aversion of their model to video-to-text generation, but stopped short of proposing an end-to-end single network, using an intermediate role representation instead.", "labels": [], "entities": [{"text": "image-to-text generation", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.7305750846862793}, {"text": "video-to-text generation", "start_pos": 183, "end_pos": 207, "type": "TASK", "confidence": 0.7518804371356964}]}, {"text": "Also, they showed results only on the narrow domain of cooking videos with a small set of pre-defined objects and actors.", "labels": [], "entities": []}, {"text": "Inspired by their approach, we utilize a Long-Short Term Memory (LSTM) recurrent neural network) to model sequence dynamics, but connect it directly to a deep convolutional neural network to process incoming video frames, avoiding supervised intermediate representations altogether.", "labels": [], "entities": []}, {"text": "This model is similar to their image-to-text model, but we adapt it for video sequences.", "labels": [], "entities": []}, {"text": "Our proposed approach has several important advantages over existing video description work.", "labels": [], "entities": [{"text": "video description", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7140901684761047}]}, {"text": "The LSTM model, which has recently achieved state-ofthe-art results on machine translation tasks), effectively models the sequence generation task without requiring the use of fixed sentence templates as in previous work . Pre-training on image and text data naturally exploits related data to supplement the limited amount of descriptive video currently available.", "labels": [], "entities": [{"text": "machine translation tasks", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.7918533285458883}, {"text": "sequence generation task", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.7941709160804749}]}, {"text": "Finally, the deep convnet, the winner of the ILSVRC2012 () image classification competition, provides a strong visual representation of objects, actions and scenes depicted in the video.", "labels": [], "entities": [{"text": "ILSVRC2012 () image classification competition", "start_pos": 45, "end_pos": 91, "type": "TASK", "confidence": 0.8157421708106994}]}, {"text": "Our main contributions are as follows: \u2022 We present the first end-to-end deep model for video-to-text generation that simultaneously learns a latent \"meaning\" state, and a fluent grammatical model of the associated language.", "labels": [], "entities": [{"text": "video-to-text generation", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.7721300423145294}]}, {"text": "\u2022 We leverage still image classification and caption data and transfer deep networks learned on such data to the video domain.", "labels": [], "entities": [{"text": "still image classification and caption", "start_pos": 14, "end_pos": 52, "type": "TASK", "confidence": 0.6162665545940399}]}, {"text": "\u2022 We provide a detailed evaluation of our model on the popular YouTube corpus) and demonstrate a significant improvement over the state of the art.", "labels": [], "entities": [{"text": "YouTube corpus", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.728532075881958}]}], "datasetContent": [{"text": "Earlier works ( ) that reported results on the YouTube dataset compared their method based on how well their model could predict the subject, verb, and object (SVO) depicted in the video.", "labels": [], "entities": [{"text": "YouTube dataset", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.8448441922664642}]}, {"text": "Since these models first predicted the content (SVO triples) and then generated the sentences, the S,V,O accuracy captured the quality of the content generated by the models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.8478517532348633}]}, {"text": "However, in our case the sequential LSTM directly outputs the sentence, so we extract the S,V,O from the dependency parse of the generated sentence.", "labels": [], "entities": []}, {"text": "We present, in, the accuracy of S,V,O words comparing the performance of our model against any valid ground truth triple and the most frequent triple found inhuman description for each video.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9995148181915283}]}, {"text": "The latter evaluation was also reported by (), so we include it here for comparison.", "labels": [], "entities": []}, {"text": "To evaluate the generated sentences we use the BLEU () and METEOR () scores against all ground truth sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9992303848266602}, {"text": "METEOR", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9965751767158508}]}, {"text": "BLEU is the metric that is seen more commonly in image description literature, but a more recent study) has shown METEOR to be a better evaluation metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9884424805641174}, {"text": "image description", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7955361008644104}, {"text": "METEOR", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9258129596710205}]}, {"text": "However, since both metrics have been shown to correlate well with human eval-Model S% V% O% HVC 86.87 38.66 22.09 FGM 88: SVO accuracy: Binary SVO accuracy compared against any valid S,V,O triples in the ground truth descriptions.", "labels": [], "entities": [{"text": "HVC 86.87", "start_pos": 93, "end_pos": 102, "type": "DATASET", "confidence": 0.7441682517528534}, {"text": "FGM 88", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.481523334980011}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.7946444153785706}, {"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.8513599634170532}]}, {"text": "We extract S,V,O values from sentences output by our model using a dependency parser.", "labels": [], "entities": []}, {"text": "The model is correct if it identifies S,V, or O mentioned in anyone of the multiple human descriptions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SVO accuracy: Binary SVO accuracy compared  against any valid S,V,O triples in the ground truth descrip- tions. We extract S,V,O values from sentences output by  our model using a dependency parser. The model is cor- rect if it identifies S,V, or O mentioned in any one of the  multiple human descriptions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.8959474563598633}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9136869311332703}]}, {"text": " Table 2: SVO accuracy: Binary SVO accuracy compared  against most frequent S,V,O triple in the ground truth de- scriptions. We extract S,V,O values from parses of sen- tences output by our model using a dependency parser.  The model is correct only if it outputs the most frequently  mentioned S, V, O among the human descriptions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.883184552192688}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.8735995888710022}]}, {"text": " Table 3: Scores for BLEU at 4 (combined n-gram 1-4),  and METEOR scores from automated evaluation metrics  comparing the quality of the generation. All values are  reported as percentage (%).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9980385899543762}, {"text": "METEOR", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9988622665405273}]}, {"text": " Table 4: Human evaluation mean scores. Sentences were  uniquely ranked between 1 to 5 based on their relevance  to a given video. Sentences were rated between 1 to 5 for  grammatical correctness. Higher values are better.", "labels": [], "entities": []}, {"text": " Table 5: Scores for BLEU at 4 (combined n-gram 1-4),  and METEOR scores comparing the quality of sentence  generation by the models trained on Flickr30k and COCO  and tested on a random frame from the video. LSTM- YT-frame models were fine tuned on individual frames  from the Youtube video dataset. All values are reported  as percentage (%).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9978901743888855}, {"text": "METEOR", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9994658827781677}, {"text": "sentence  generation", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.7408499419689178}, {"text": "Flickr30k", "start_pos": 144, "end_pos": 153, "type": "DATASET", "confidence": 0.9607592225074768}, {"text": "Youtube video dataset", "start_pos": 278, "end_pos": 299, "type": "DATASET", "confidence": 0.9846052726109823}]}]}