{"title": [{"text": "Injecting Logical Background Knowledge into Embeddings for Relation Extraction", "labels": [], "entities": [{"text": "Injecting Logical Background Knowledge", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8927621841430664}, {"text": "Relation Extraction", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.9569532871246338}]}], "abstractContent": [{"text": "Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, and leverage unlabeled data.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.9323710501194}]}, {"text": "Unfortunately , these methods share a shortcoming with all other distantly supervised approaches: they cannot learn to extract target relations without existing data in the knowledge base, and likewise, these models are inaccurate for relations with sparse data.", "labels": [], "entities": []}, {"text": "Rule-based extractors, on the other hand, can be easily extended to novel relations and improved for existing but inaccurate relations, through first-order formu-lae that capture auxiliary domain knowledge.", "labels": [], "entities": [{"text": "Rule-based extractors", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6558098495006561}]}, {"text": "However, usually a large set of such formulae is necessary to achieve generalization.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a paradigm for learning low-dimensional embeddings of entity-pairs and relations that combine the advantages of matrix factorization with first-order logic domain knowledge.", "labels": [], "entities": []}, {"text": "We introduce simple approaches for estimating such embeddings, as well as a novel training algorithm to jointly optimize over factual and first-order logic information.", "labels": [], "entities": []}, {"text": "Our results show that this method is able to learn accurate extractors with little or no distant supervision alignments, while at the same time generalizing to textual patterns that do not appear in the formulae.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation extraction, the task of identifying relations between named entities, is a crucial component for information extraction.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.930136650800705}, {"text": "information extraction", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.9014312028884888}]}, {"text": "A recent successful approach () relies on two ideas: (a) unifying traditional canonical relations, such as those of the Freebase schema, with OpenIE surface form patterns in a universal schema, and (b) completing a knowledge base of such a schema using matrix factorization.", "labels": [], "entities": [{"text": "Freebase schema", "start_pos": 120, "end_pos": 135, "type": "DATASET", "confidence": 0.9470409154891968}]}, {"text": "This approach has several attractive properties.", "labels": [], "entities": []}, {"text": "First, for canonical relations it effectively performs distant supervision () and hence requires no textual annotations.", "labels": [], "entities": []}, {"text": "Second, in the spirit of OpenIE, a universal schema can use textual patterns as novel relations and thus increases the coverage of traditional schemas ().", "labels": [], "entities": []}, {"text": "Third, matrix factorization learns better embeddings for entity-pairs for which only surface form patterns are observed, and these can also lead to better extractions of canonical relations.", "labels": [], "entities": []}, {"text": "Unfortunately, populating a universal schema knowledge base using matrix factorization suffers from a problem all distantly-supervised techniques share: you can only reliably learn relations that appear frequently enough in the knowledge base.", "labels": [], "entities": []}, {"text": "In particular, for relations that do not appear in the knowledge base or for which no facts are known we cannot learn a predictor at all.", "labels": [], "entities": []}, {"text": "One way to overcome this problem is to incorporate additional domain knowledge, either specified manually or bootstrapped from auxiliary sources.", "labels": [], "entities": []}, {"text": "In fact, domain knowledge encoded as simple logic formulae over patterns and relations has been used in practice to directly specify relation extractors ().", "labels": [], "entities": []}, {"text": "However, these extractors can be brittle and obtain poor recall, since they are unable to generalize to textual patterns that are not found in given formulae.", "labels": [], "entities": [{"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9992625117301941}]}, {"text": "Hence, there is a need for learning extractors that are able to combine logical knowledge with benefits of factorization techniques to facilitate precise extractions and generalization to novel relations.", "labels": [], "entities": []}, {"text": "In this paper, we propose a paradigm for learning universal schema extractors by combining matrix factorization based relation extraction with additional information in the form of first-order logic knowledge.", "labels": [], "entities": [{"text": "learning universal schema extractors", "start_pos": 41, "end_pos": 77, "type": "TASK", "confidence": 0.6113954335451126}, {"text": "relation extraction", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7497073113918304}]}, {"text": "Our contributions are threefold: (i) We introduce simple baselines that enforce logic constraints through deterministic inference before and after matrix factorization ( \u00a73.1).", "labels": [], "entities": []}, {"text": "(ii) We propose a novel joint training algorithm that learns vector embeddings of relations and entity-pairs using both distant supervision and first-order logic formulae such that the factorization captures these formulae ( \u00a73.2).", "labels": [], "entities": []}, {"text": "(iii) We present an empirical evaluation using automatically mined rules that demonstrates the benefits of incorporating logical knowledge in relation extraction, in particular that joint factorization of distant and logic supervision is efficient, accurate, and robust to noise ( \u00a75).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.8499281406402588}]}], "datasetContent": [{"text": "There are two orthogonal question when evaluating the effectiveness of low-rank logic embeddings: a) does injection of logic formulae into the embeddings of entity-pairs and relations provide any benefits, and b) where do the background formulae come from?", "labels": [], "entities": []}, {"text": "The latter is a well-studied problem (.", "labels": [], "entities": []}, {"text": "In this paper we focus the evaluation on the ability of various approaches to benefit from formulae that we directly extract from the training data using a simple method.", "labels": [], "entities": []}, {"text": "Distant Supervision Evaluation We follow the procedure as used in Formulae Extraction and Annotation We use a simple technique for extracting formulae from the matrix factorization model.", "labels": [], "entities": [{"text": "Distant Supervision Evaluation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8893466591835022}, {"text": "Formulae Extraction", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.8560468554496765}]}, {"text": "We first run matrix factorization over the complete training data to learn accurate relation and entity-pair embeddings.", "labels": [], "entities": []}, {"text": "After training, we iterate overall pairs of relations (r s , rt ) where rt is a Freebase relation.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.9514521360397339}]}, {"text": "For every relationpair we iterate overall training atoms r s (e i , e j ), evaluate the score [r s (e i , e j ) \u21d2 rt (e i , e j )] as described in \u00a73.2.1, and calculate the average to arrive at a score for the formula.", "labels": [], "entities": []}, {"text": "Finally, we rank all formulae by their score and manually filter the top 100 formulae, which resulted in 36 annotated high-quality formulae (see for examples).", "labels": [], "entities": []}, {"text": "Note that our formula extraction approach does not observe the relations for test entity-pairs.", "labels": [], "entities": [{"text": "formula extraction", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.782307505607605}]}, {"text": "All models used in our experiments have access to these formulae, except for the matrix factorization baseline.", "labels": [], "entities": []}, {"text": "Methods Our proposed methods for injecting logic into relation embeddings are pre-factorization inference (Pre; \u00a73.1) which performs regular matrix factorization after propagating the logic formulae in a deterministic manner, and joint optimization (Joint; \u00a73.2) which maximizes an objective that combines terms from factual and first-order logic knowledge.", "labels": [], "entities": []}, {"text": "Additionally, we use the following three baselines.", "labels": [], "entities": []}, {"text": "The matrix factorization (MF; \u00a72.2) model uses only ground atoms to learn relation and entity-pair embed-Formula Score \u2200x, y : #2-unit-of-#1(x, y) \u21d2 org/parent/child(x, y) 0.97 \u2200x, y : #2-city-of-#1(x, y) \u21d2 location/containedby(x, y) 0.97 \u2200x, y : #2-minister-#1(x, y) \u21d2 person/nationality(x, y) 0.97 \u2200x, y : #2-executive-#1(x, y) \u21d2 person/company(x, y) 0.96 \u2200x, y : #2-co-founder-of-#1(x, y) \u21d2 company/founders(y, x) 0.96: Sample Extracted Formulae: Top implications of textual patterns to five different Freebase relations.", "labels": [], "entities": []}, {"text": "These implications were extracted from the matrix factorization model and manually annotated.", "labels": [], "entities": []}, {"text": "The premises of these implications are dependency paths, but we present a simplified version to make them more readable.", "labels": [], "entities": []}, {"text": "dings (i.e. it has no access to any formulae).", "labels": [], "entities": []}, {"text": "Furthermore, we consider pure logical inference (Inf).", "labels": [], "entities": [{"text": "Inf", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9093756079673767}]}, {"text": "Our final approach, post-factorization inference (Post), first runs matrix factorization and then performs logical inference on the known and predicted facts.", "labels": [], "entities": []}, {"text": "Postinference is computationally expensive, since for all premises of formulae we have to iterate overall rows (entity-pairs) in the matrix to assess whether the premise is true or not.", "labels": [], "entities": []}, {"text": "Parameters For every matrix factorization based method we use k = 100 as the dimension for the embeddings, \u03bb = 0.01 as parameter of 2 -regularization and \u03b1 = 0.1 as initial learning rate for AdaGrad, which we run for 200 epochs.", "labels": [], "entities": [{"text": "AdaGrad", "start_pos": 191, "end_pos": 198, "type": "DATASET", "confidence": 0.9022577404975891}]}, {"text": "Complexity Each AdaGrad update is defined over a single cell of the matrix, and thus training data can be streamed one ground atom at a time.", "labels": [], "entities": []}, {"text": "For matrix factorization, each AdaGrad epoch touches all the observed atoms once, and as many sampled negative atoms.", "labels": [], "entities": [{"text": "AdaGrad epoch", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.9246104955673218}]}, {"text": "With given formulae, it additionally revisits all the observed atoms that appear as an atom in the formula (and as many sampled negative atoms), and thus more general formulae will be more expensive.", "labels": [], "entities": []}, {"text": "However the updates over atoms are performed independently and thus not all the data needs to be stored in memory.", "labels": [], "entities": []}, {"text": "All presented models take less than 15 minutes to train on a 2.8 GHz Intel Core i7 machine.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Zero-shot Relation Learning: Average and", "labels": [], "entities": [{"text": "Zero-shot Relation Learning", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.5924260715643564}, {"text": "Average", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9871906638145447}]}]}