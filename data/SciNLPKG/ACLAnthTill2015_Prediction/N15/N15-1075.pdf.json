{"title": [{"text": "The Unreasonable Effectiveness of Word Representations for Twitter Named Entity Recognition", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6495537360509237}]}], "abstractContent": [{"text": "Named entity recognition (NER) systems trained on newswire perform very badly when tested on Twitter.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.76279749472936}]}, {"text": "Signals that were reliable in copy-edited text disappear almost entirely in Twitter's informal chatter, requiring the construction of specialized models.", "labels": [], "entities": []}, {"text": "Using well-understood techniques, we set out to improve Twitter NER performance when given a small set of annotated training tweets.", "labels": [], "entities": [{"text": "NER", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.7537376880645752}]}, {"text": "To leverage unlabeled tweets, we build Brown clusters and word vectors, enabling generalizations across distributionally similar words.", "labels": [], "entities": []}, {"text": "To leverage annotated newswire data, we employ an importance weighting scheme.", "labels": [], "entities": []}, {"text": "Taken all together, we establish anew state-of-the-art on two common test sets.", "labels": [], "entities": []}, {"text": "Though it is well-known that word representations are useful for NER, supporting experiments have thus far fo-cused on newswire data.", "labels": [], "entities": [{"text": "NER", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9800717830657959}]}, {"text": "We emphasize the effectiveness of representations on Twitter NER, and demonstrate that their inclusion can improve performance by up to 20 F1.", "labels": [], "entities": [{"text": "Twitter NER", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.7999856173992157}, {"text": "F1", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.9569936394691467}]}], "introductionContent": [{"text": "Named entity recognition (NER) is the task of finding rigid designators as they appear in free text and classifying them into coarse categories such as person or location (.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.794972687959671}]}, {"text": "NER enables many other information extraction tasks such as relation extraction () and entity linking).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.7421943545341492}, {"text": "relation extraction", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7997017502784729}, {"text": "entity linking", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.7791752815246582}]}, {"text": "There is considerable excitement at the prospect of porting information extraction technology to social media platforms such as Twitter.", "labels": [], "entities": [{"text": "porting information extraction", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.7838542660077413}]}, {"text": "Social media reacts to world events faster than traditional news sources, and its sub-communities pay close attention to topics that other sources might ignore.", "labels": [], "entities": []}, {"text": "An early example of the potential inherent in social information extraction is the Twitter Calendar (, which detects upcoming events (concerts, elections, video game releases, etc.) based on the anticipatory chatter of Twitter users.", "labels": [], "entities": [{"text": "social information extraction", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.8517906665802002}]}, {"text": "Unfortunately, processing social media text presents a unique set of challenges, especially for technologies designed for newswire: Twitter posts are short, the language is informal, capitalization is inconsistent at best, and spelling variations and abbreviations run rampant.", "labels": [], "entities": []}, {"text": "Armed with an affordable training set of 1,000 annotated tweets, we establish a strong baseline for Twitter NER using well-understood techniques.", "labels": [], "entities": [{"text": "NER", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.7269620895385742}]}, {"text": "We build two unsupervised word representations in order to leverage a large collection of unannotated tweets, while a data-weighting technique allows us to benefit from annotated newswire data.", "labels": [], "entities": []}, {"text": "Taken together, these two simple ideas establish anew state-of-the-art for both our test sets.", "labels": [], "entities": []}, {"text": "We rigorously test the impact of both continuous and cluster-based word representations on Twitter NER, emphasizing the dramatic improvement that they bring.", "labels": [], "entities": []}, {"text": "We also bring the experimental methodology of the domain adaptation community to Twitter NER, testing indomain, out-of-domain and combined training scenarios, and revealing that it is not trivial to benefit from out-of-domain training data.", "labels": [], "entities": [{"text": "Twitter NER", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.8962742686271667}]}, {"text": "Finally, an error analysis helps us begin to understand which social media challenges are being addressed by our adaptations, and which problems persist.", "labels": [], "entities": []}], "datasetContent": [{"text": "Vital statistics for all of our data sets are shown in.", "labels": [], "entities": []}, {"text": "For in-domain NER data, we use three collections of annotated tweets: Fin10 was originally crowd-sourced by, and was manually corrected by, while Rit11 (Ritter et al., 2011) and Fro14 () were built by expert annotators.", "labels": [], "entities": [{"text": "Fro14", "start_pos": 178, "end_pos": 183, "type": "DATASET", "confidence": 0.9128109812736511}]}, {"text": "We divide Fin10 temporally into a training set and a development set, and we consider Rit11 and Fro14 to be our test sets.", "labels": [], "entities": [{"text": "Fro14", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.8970643877983093}]}, {"text": "This reflects a plausible training scenario, with train and dev drawn from the same pool, but with distinct tests drawn from later in time.", "labels": [], "entities": []}, {"text": "These three data sets were collected and unified by, who normalized the tags into three entity classes: person (PER), location (LOC) and organization (ORG).", "labels": [], "entities": []}, {"text": "The source text has also been normalized; notably, all numbers are normalized to NUM-BER, and all URLs and Twitter @user names have been normalized to URL and @USER respectively.", "labels": [], "entities": []}, {"text": "In the gold-standard, we choose to reverse a tagging normalization performed by, who had post-processed the data so that all @user names are tagged as PER.", "labels": [], "entities": [{"text": "PER", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9189401865005493}]}, {"text": "These tags are trivial to replicate, and we found that they inflate scores quite dramatically.", "labels": [], "entities": []}, {"text": "Therefore, all @user names are untagged in both the gold standard and our system outputs.", "labels": [], "entities": []}, {"text": "We use the CoNLL 2003 newswire training set as a source of out-of-domain NER annotations).", "labels": [], "entities": [{"text": "CoNLL 2003 newswire training set", "start_pos": 11, "end_pos": 43, "type": "DATASET", "confidence": 0.9782527208328247}]}, {"text": "The source text has been normalized to match the Twitter NER data, and we have removed the MISC tag from the goldstandard, leaving PER, LOC and ORG.", "labels": [], "entities": [{"text": "Twitter NER data", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.8922834595044454}, {"text": "MISC", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.7577034831047058}, {"text": "PER", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9831148982048035}, {"text": "LOC", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.9437839984893799}, {"text": "ORG", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.9647975564002991}]}, {"text": "Finally, we also use a large corpus of unannotated tweets, collected from between May 2011 and April 2012.", "labels": [], "entities": []}, {"text": "It has been tokenized by the CMU Twokenizer, 3 but is otherwise unnormalized.", "labels": [], "entities": [{"text": "CMU Twokenizer", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.7852580845355988}]}], "tableCaptions": [{"text": " Table 3: Details of our NER-annotated corpora. A line is a tweet in Twitter and a sentence in newswire.", "labels": [], "entities": []}, {"text": " Table 4: Performance on newswire (CoNLL) data.", "labels": [], "entities": [{"text": "newswire (CoNLL) data", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.6713508784770965}]}, {"text": " Table 5: Impact of our components on Twitter NER per- formance, as measured by F1, under 3 data scenarios.", "labels": [], "entities": [{"text": "NER", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.6016416549682617}, {"text": "F1", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.9978129863739014}]}, {"text": " Table 4.  We achieve broadly comparable scores in both  settings. Our external knowledge features are not  as useful as theirs, which may be due to our lack of  Wikipedia gazetteers, or due to a domain mismatch  in our unannotated training data. Their clusters are  trained on the 1996 Reuters corpus, a superset of the  CoNLL data, matching it in both era and domain.  Conversely, our clusters and vectors are both trained  on tweets from 2011, so it is somewhat surprising  that they help to the extent that they do.", "labels": [], "entities": [{"text": "1996 Reuters corpus", "start_pos": 282, "end_pos": 301, "type": "DATASET", "confidence": 0.6903718312581381}, {"text": "CoNLL data", "start_pos": 322, "end_pos": 332, "type": "DATASET", "confidence": 0.9316570460796356}]}, {"text": " Table 7: F1 for our All Data+Reps+Weights system, or- ganized by entity class.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9983273148536682}]}, {"text": " Table 8: Adding linguistic resources to our baseline  and representation-enabled systems, as measured by F1  averaged over 3 test sets. All systems are trained on  CoNLL+Fin10, and all but Base+\u2205 use data weighting.", "labels": [], "entities": [{"text": "F1", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.9970098733901978}, {"text": "CoNLL+Fin10", "start_pos": 165, "end_pos": 176, "type": "DATASET", "confidence": 0.826903760433197}]}, {"text": " Table 8.  Comparing Base+\u2205 and Base+[P]+[G], we see  that linguistic resources boost the baseline's perfor- mance considerably. Turning to Reps+[P]+[G], we  see that adding word representations to linguistic re- sources provides another substantial boost of 7.9 F1.  Conversely, adding linguistic resources to a system  that already has representations increases F1 by only  1.1 points, indicating that not much new information  is being added. The per-feature analysis indicates  that much of this boost comes from the gazetteers. System  Rit11 Fro14  PHMS14 Baseline  77.4  82.1  PHMS14 Dict Web  78.5  83.9  All Data+Reps+Weights  82.3  86.4  All Data+Ling+Reps+Weights 82.6  86.9", "labels": [], "entities": [{"text": "F1", "start_pos": 263, "end_pos": 265, "type": "METRIC", "confidence": 0.9991427659988403}, {"text": "F1", "start_pos": 364, "end_pos": 366, "type": "METRIC", "confidence": 0.9990274906158447}, {"text": "Fro14  PHMS14 Baseline  77.4  82.1  PHMS14 Dict Web  78.5  83.9  All Data+Reps+Weights  82.3  86.4  All Data+Ling+Reps+Weights 82.6  86.9", "start_pos": 547, "end_pos": 684, "type": "DATASET", "confidence": 0.9015131401164191}]}]}