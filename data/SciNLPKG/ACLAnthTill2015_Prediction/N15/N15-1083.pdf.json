{"title": [{"text": "Pragmatic Neural Language Modelling in Machine Translation", "labels": [], "entities": [{"text": "Pragmatic Neural Language Modelling in Machine Translation", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.6333382597991398}]}], "abstractContent": [{"text": "This paper presents an in-depth investigation on integrating neural language models in translation systems.", "labels": [], "entities": []}, {"text": "Scaling neural language models is a difficult task, but crucial for real-world applications.", "labels": [], "entities": [{"text": "Scaling neural language models", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8952681422233582}]}, {"text": "This paper evaluates the impact on end-to-end MT quality of both new and existing scaling techniques.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9926177859306335}]}, {"text": "We show when explicitly normalising neu-ral models is necessary and what optimisa-tion tricks one should use in such scenarios.", "labels": [], "entities": []}, {"text": "We also focus on scalable training algorithms and investigate noise contrastive estimation and diagonal contexts as sources for further speed improvements.", "labels": [], "entities": [{"text": "noise contrastive estimation", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.7291416525840759}]}, {"text": "We explore the trade-offs between neural models and back-off n-gram models and find that neural models make strong candidates for natural language applications in memory constrained environments, yet still lag behind traditional models in raw translation quality.", "labels": [], "entities": []}, {"text": "We conclude with a set of recommendations one should follow to build a scalable neural language model for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.9940766096115112}]}], "introductionContent": [{"text": "Language models are used in translation systems to improve the fluency of the output translations.", "labels": [], "entities": []}, {"text": "The most popular language model implementation is a back-off n-gram model with Kneser-Ney smoothing).", "labels": [], "entities": []}, {"text": "Back-off n-gram models are conceptually simple, very efficient to construct and query, and are regarded as being extremely effective in translation systems.", "labels": [], "entities": []}, {"text": "Neural language models area more recent class of language models () that have been shown to outperform back-off n-gram models using intrinsic evaluations of heldout perplexity, or when used in addition to traditional models in natural language systems such as speech recognizers.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 260, "end_pos": 278, "type": "TASK", "confidence": 0.6996950060129166}]}, {"text": "Neural language models combat the problem of data sparsity inherent to traditional n-gram models by learning distributed representations for words in a continuous vector space.", "labels": [], "entities": []}, {"text": "It has been shown that neural language models can improve translation quality when used as additional features in a decoder ( or if used for n-best list rescoring.", "labels": [], "entities": []}, {"text": "These results show great promise and in this paper we continue this line of research by investigating the tradeoff between speed and accuracy when integrating neural language models in a decoder.", "labels": [], "entities": [{"text": "speed", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.9935801029205322}, {"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.993431031703949}]}, {"text": "We also focus on how effective these models are when used as the sole language model in a translation system.", "labels": [], "entities": []}, {"text": "This is important because our hypothesis is that most of the language modelling is done by the n-gram model, with the neural model only acting as a differentiating factor when the n-gram model cannot provide a decisive probability.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.6955165266990662}]}, {"text": "Furthermore, neural language models are considerably more compact and represent strong candidates for modelling language in memory constrained environments (e.g. mobile devices, commodity machines, etc.), where back-off n-gram models trained on large amounts of data do not fit into memory.", "labels": [], "entities": []}, {"text": "Our results show that a novel combination of noise contrastive estimation (Mnih and Teh, 2012) and factoring the softmax layer using Brown clusters) provides the most pragmatic solution for fast training and decoding.", "labels": [], "entities": [{"text": "noise contrastive estimation", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.7226099272569021}]}, {"text": "Further, we confirm that when evaluated purely on BLEU score, neural models are unable to match the benchmark Kneser-Ney models, even if trained with large hidden layers.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9740125238895416}]}, {"text": "However, when the evaluation is restricted to models that match a certain memory footprint, neural models clearly outperform the n-gram benchmarks, confirming that they represent a practical solution for memory constrained environments.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we use data from the 2014 ACL Workshop in Machine Translation.", "labels": [], "entities": [{"text": "2014 ACL Workshop in Machine Translation", "start_pos": 41, "end_pos": 81, "type": "TASK", "confidence": 0.6540236622095108}]}, {"text": "We train standard phrase-based translation systems for French \u2192 English, English \u2192 Czech and English \u2192 German using the Moses toolkit (.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.6253116726875305}]}, {"text": "We used the europarl and the news commentary corpora as parallel data for training", "labels": [], "entities": [{"text": "europarl", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.9863406419754028}]}], "tableCaptions": [{"text": " Table 5: Qualitative analysis of the proposed normalisa- tion schemes with an additional back-off n-gram model.", "labels": [], "entities": []}, {"text": " Table 6: Qualitative analysis of the proposed normal- isation schemes without an additional back-off n-gram  model.", "labels": [], "entities": []}, {"text": " Table 9: A comparison between stochastic gradient de- scent (SGD) and noise contrastive estimation (NCE) for  class factored models on the fr\u2192en data.", "labels": [], "entities": [{"text": "gradient de- scent (SGD)", "start_pos": 42, "end_pos": 66, "type": "METRIC", "confidence": 0.7188920591558728}, {"text": "noise contrastive estimation (NCE)", "start_pos": 71, "end_pos": 105, "type": "METRIC", "confidence": 0.7009067585070928}]}, {"text": " Table 11: A side by side comparison of class factored  models with and without diagonal contexts trained with  noise contrastive estimation on the fr\u2192en data.", "labels": [], "entities": []}]}