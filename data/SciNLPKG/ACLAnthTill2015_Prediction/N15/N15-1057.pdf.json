{"title": [{"text": "LCCT: A Semi-supervised Model for Sentiment Classification", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.9648702442646027}]}], "abstractContent": [{"text": "Analyzing public opinions towards products, services and social events is an important but challenging task.", "labels": [], "entities": [{"text": "Analyzing public opinions", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9056536555290222}]}, {"text": "An accurate sentiment an-alyzer should take both lexicon-level information and corpus-level information into account.", "labels": [], "entities": []}, {"text": "It also needs to exploit the domain-specific knowledge and utilize the common knowledge shared across domains.", "labels": [], "entities": []}, {"text": "In addition , we want the algorithm being able to deal with missing labels and learning from incomplete sentiment lexicons.", "labels": [], "entities": []}, {"text": "This paper presents a LCCT (Lexicon-based and Corpus-based, Co-Training) model for semi-supervised sentiment classification.", "labels": [], "entities": [{"text": "semi-supervised sentiment classification", "start_pos": 83, "end_pos": 123, "type": "TASK", "confidence": 0.6940212249755859}]}, {"text": "The proposed method combines the idea of lexicon-based learning and corpus-based learning in a unified co-training framework.", "labels": [], "entities": []}, {"text": "It is capable of incorporating both domain-specific and domain-independent knowledge.", "labels": [], "entities": []}, {"text": "Extensive experiments show that it achieves very competitive classification accuracy, even with a small portion of labeled data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9515745043754578}]}, {"text": "Comparing to state-of-the-art sentiment classification methods, the LCCT approach exhibits significantly better performances on a variety of datasets in both English and Chinese.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.8844256699085236}]}], "introductionContent": [{"text": "Due to the popularity of opinion-rich resources (e.g., online review sites, forums, blogs and the microblogging websites), people express their opinions allover the Internet.", "labels": [], "entities": []}, {"text": "Motivated by the demand of gleaning insights from such valuable data, a flurry of research devotes to the task of extracting people's opinions from online reviews.", "labels": [], "entities": [{"text": "extracting people's opinions from online reviews", "start_pos": 114, "end_pos": 162, "type": "TASK", "confidence": 0.8048691068376813}]}, {"text": "Such opinions could be expressed on products, services or policies, etc.", "labels": [], "entities": []}, {"text": "Existing sentiment analysis approaches can be divided into two categories based on the source of information they use: the lexiconbased approach and the corpus-based approach (.", "labels": [], "entities": [{"text": "Existing sentiment analysis", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7078779538472494}]}, {"text": "The lexicon-based approach counts positive and negative terms in a review based on the sentiment dictionary and classifies the document as positive if it contains more positive terms than negative ones.", "labels": [], "entities": []}, {"text": "On the contrary, the corpus-based approach uses supervised learning algorithms to train a sentiment classifier.", "labels": [], "entities": []}, {"text": "Further study ( shows that corpus-based and lexicon-based approaches have complementary performances.", "labels": [], "entities": []}, {"text": "Specifically, the corpus-based approach has high precision but low recall on positive instances, while the lexicon-based approach has high recall but low precision on positive instances.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.996669352054596}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9940589666366577}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9910969734191895}, {"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9725893139839172}]}, {"text": "In fact, corpusbased approaches are over conservative in classifying instances as positive, because positive reviews usually contain many neutral statements.", "labels": [], "entities": []}, {"text": "In contrast, the lexicon-based approaches tend to classify negative or neutral instances as positive when there area few positive words appear in the document.", "labels": [], "entities": []}, {"text": "It motivates us to develop anew approach that achieves good performance on both precision and recall evaluations.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9991017580032349}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9799566268920898}]}, {"text": "Besides reviews on products and services, another rich source of opinion data are social reviews in forums, blogs and microblogging websites.", "labels": [], "entities": []}, {"text": "Different from product reviews, the social reviews are not associated with numerical ratings, making it difficult to perform supervised classification.", "labels": [], "entities": []}, {"text": "Since manual labeling is time consuming and expensive, it is preferable to label a small portion of social reviews to perform semi-supervised learning, leveraging information from both labeled and unlabeled data.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach to handle the above two challenges.", "labels": [], "entities": []}, {"text": "We presents the LCCT Model (, which treats the lexicon-based information and the corpus-based information as two views, and combine them via co-training.", "labels": [], "entities": []}, {"text": "The algorithm naturally incorporates the framework of semi-supervised learning, as missing labels in each view can be estimated by the classifier trained from the other view.", "labels": [], "entities": []}, {"text": "The proposed LCCT model exploits the complementary performance associated with the lexicon-based and the corpus-based approaches, taking the best of each side to improve the overall performance.", "labels": [], "entities": []}, {"text": "We present a novel semi-supervised sentiment-aware LDA approach to build the lexicon-based classifier, which uses a minimal set of seed words (e.g., \"good\",\"happy\" as positive seeds) as well as document sentiment labels to construct a domain-specific sentiment lexicon.", "labels": [], "entities": []}, {"text": "This model reflects the domainspecific knowledge.", "labels": [], "entities": []}, {"text": "We employ the stacked denoising auto-encoder () to build the corpus-based classifier.", "labels": [], "entities": []}, {"text": "As) point out, the intermediate abstractions extracted in this way tend to reflect the domain-independent knowledge, unifying information across all domains.", "labels": [], "entities": []}, {"text": "Finally, we use a co-training algorithm to combine the corpus-based and lexiconbased classifiers and to combine the domain-specific knowledge and the domain-independent knowledge.", "labels": [], "entities": []}, {"text": "The main contributions of our approach are threefolded.", "labels": [], "entities": []}, {"text": "First, we propose a method that exploits both general domain-independent knowledge and specific domain-dependent knowledge, behaving like a human being when she analyzes the text.", "labels": [], "entities": []}, {"text": "Second, we complement the lexicon-based approach and the corpus-based approach to overcome their respective classification biases.", "labels": [], "entities": []}, {"text": "Third, our approach is capable of levering labeled and unlabeled data, unifying them into a semi-supervised learning framework.", "labels": [], "entities": []}, {"text": "We conduct extensive experiments to verify the effectiveness of the proposed approach on real-world social reviews.", "labels": [], "entities": []}, {"text": "The experiment results show that our model substantially outperforms the state-of-the-art methods in analyzing sentiments in online reviews.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare the proposed LCCT model with state-of-the-art methods in sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.9662817120552063}]}, {"text": "The experiment demonstrates the superior performance of our approach.", "labels": [], "entities": []}, {"text": "We conduct experiments on English and Chinese reviews from three datasets.", "labels": [], "entities": []}, {"text": "In this subsection, we describe the datasets.", "labels": [], "entities": []}, {"text": "Movie Review (MR) dataset in English The movie reviews are selected if the rating was stars or a numerical score.", "labels": [], "entities": [{"text": "Movie Review (MR) dataset", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.6159066557884216}]}, {"text": "In this paper, we use the Movie Review dataset containing 1000 positive examples and 1000 negative examples ().", "labels": [], "entities": [{"text": "Movie Review dataset", "start_pos": 26, "end_pos": 46, "type": "DATASET", "confidence": 0.9538770914077759}]}, {"text": "Positive labels were assigned to reviews that had a rating above 3.5 stars and negative labels were assigned to the rest ().", "labels": [], "entities": []}, {"text": "SemEval-2013 (SemEval) dataset in English This dataset is constructed for the Twitter sentiment analysis task (Task 2) in the Semantic Evaluation of Systems challenge.", "labels": [], "entities": [{"text": "SemEval-2013 (SemEval) dataset", "start_pos": 0, "end_pos": 30, "type": "DATASET", "confidence": 0.6713198304176331}, {"text": "Twitter sentiment analysis task", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.7149451896548271}, {"text": "Semantic Evaluation of Systems challenge", "start_pos": 126, "end_pos": 166, "type": "TASK", "confidence": 0.7803574562072754}]}, {"text": "All the tweets were manually annotated by 5 Amazon Mechanical Turk workers with negative, positive and neutral labels.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.9073712229728699}]}, {"text": "SemEval contains 13,975 tweets with 2,186 negative, 6,440 neutrals and 5,349 positives tweets.", "labels": [], "entities": []}, {"text": "We collect the 2,186 negative tweets and 5,349 positive tweets as the training data.", "labels": [], "entities": []}, {"text": "COAE-2009 (COAE) dataset in Chinese This dataset is provided by COAE 2009 1 (Task 4).", "labels": [], "entities": [{"text": "COAE-2009 (COAE) dataset", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.9089204430580139}, {"text": "COAE 2009 1", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.9007482926050822}]}, {"text": "The corpus consists of 39,976 documents and 50 topics.", "labels": [], "entities": []}, {"text": "The topics cover education, entertainment, finance, computer, etc.", "labels": [], "entities": []}, {"text": "In this paper, we select the 2202 negative and 1248 positive documents as our dataset.", "labels": [], "entities": []}, {"text": "In all experiments, data preprocessing is performed.", "labels": [], "entities": [{"text": "data preprocessing", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.6684588044881821}]}, {"text": "For English dataset, the texts are first tokenized using the natural language toolkit NLTK 2 . 1 http://ir-china.org.cn/coae2009.html 2 http://www.nltk.org Then, we remove non-alphabet characters, numbers, pronoun, punctuation and stop words from the text.", "labels": [], "entities": []}, {"text": "Finally, the WordNet stemmer 3 is applied to reduce the vocabulary size and settle the issue of data sparseness.", "labels": [], "entities": [{"text": "WordNet stemmer 3", "start_pos": 13, "end_pos": 30, "type": "DATASET", "confidence": 0.9136460423469543}]}, {"text": "For Chinese dataset, we first perform Chinese word segmentation with a popular Chinese auto-segmentation system ICTCLAS 4 . Then, the words about time, numeral words, pronoun and punctuation are removed as they are unrelated to the sentiment analysis task.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6164873639742533}, {"text": "sentiment analysis task", "start_pos": 232, "end_pos": 255, "type": "TASK", "confidence": 0.9037749568621317}]}, {"text": "For each dataset, we use 80% instances as the training data and the remaining are used for testing.", "labels": [], "entities": []}, {"text": "To test the performance of semi-supervised learning, we randomly select 10% of the training instances as labeled data and treat the remaining as unlabeled.", "labels": [], "entities": []}, {"text": "For fair comparison, the fully supervised SVM and Nguyen's method use the 10% labeled data for training.", "labels": [], "entities": []}, {"text": "We summarize the experiment results in.", "labels": [], "entities": []}, {"text": "According to, the proposed LCCT method substantially and consistently outperforms other methods on all the three datasets.", "labels": [], "entities": []}, {"text": "This verifies the effectiveness of the proposed approach and demonstrates its advantage in semi-supervised sentiment analysis where reviews are from different domains and different language.", "labels": [], "entities": [{"text": "semi-supervised sentiment analysis", "start_pos": 91, "end_pos": 125, "type": "TASK", "confidence": 0.668916662534078}]}, {"text": "For example, the overall accuracy of our algorithm is 5.3% higher than Dasgupta's method and 13.1% higher than TSVM on Movie Reviews dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9996459484100342}, {"text": "TSVM", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.7834953665733337}, {"text": "Movie Reviews dataset", "start_pos": 119, "end_pos": 140, "type": "DATASET", "confidence": 0.8940926790237427}]}, {"text": "On other datasets, we observe the similar results.", "labels": [], "entities": []}, {"text": "To verify that unlabeled data improves the performance, we compare the SVM and Nguyen's classifier trained on 10% of the labeled data with other semi-supervised classifiers.", "labels": [], "entities": []}, {"text": "shows that the semi-supervised learning methods greatly benefit from using unlabeled data, especially on the Movie Reviews and on the SemEval dataset.", "labels": [], "entities": [{"text": "Movie Reviews", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.8432071506977081}, {"text": "SemEval dataset", "start_pos": 134, "end_pos": 149, "type": "DATASET", "confidence": 0.8738216757774353}]}, {"text": "Surprisingly, on the COAE dataset, lexicon-based method turns out to outperform SVM, self-learning and TSVM.", "labels": [], "entities": [{"text": "COAE dataset", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.9600989520549774}]}, {"text": "The reason might be that the topics in the COAE dataset are pretty diverse.", "labels": [], "entities": [{"text": "COAE dataset", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.9188330769538879}]}, {"text": "Without sufficient labeled data or prior knowledge such as sentiment lexicon, the corpus-based classifiers tend to separate the documents into topical sub-clusters as opposed to sentiment classes.", "labels": [], "entities": []}, {"text": "To understand the performance of our algorithm with respect to different portions of labeled data, we compare our algorithm with baseline methods by varying the percentage of labeled data from 5% to 100%.", "labels": [], "entities": []}, {"text": "shows that our approach is robust and achieves excellent performance on different labeling percentages.", "labels": [], "entities": []}, {"text": "As expected, having more labeled data improves the performance.", "labels": [], "entities": []}, {"text": "The LCCT method achieves a relative high accuracy with 10% of the reviews labeled, better than SVM, TSVM and Self-learning with 100% of the reviews labeled.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9991623163223267}, {"text": "TSVM", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.6876257658004761}]}, {"text": "On the other hand, when all the training data are labeled, LCCT is still significantly more accurate than all the competitors except Nguyen's method.", "labels": [], "entities": [{"text": "LCCT", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.8725740909576416}, {"text": "accurate", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9707021713256836}]}, {"text": "Although, the accuracy of Nguyen's method is slightly better than ours on Movie Reviews dataset, it dosen't perform well on SemEval and COAE datasets since the rating-based features learned from score-associated product reviews cannot significantly benefit the social reviews in forums and blogs, etc.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995444416999817}, {"text": "Movie Reviews dataset", "start_pos": 74, "end_pos": 95, "type": "DATASET", "confidence": 0.9171806573867798}, {"text": "COAE datasets", "start_pos": 136, "end_pos": 149, "type": "DATASET", "confidence": 0.8992536664009094}]}, {"text": "The main advantage of our model comes from its capability of exploiting the complementary information from the lexicon-based approach and the corpusbased approach.", "labels": [], "entities": []}, {"text": "Another reason for the effectiveness of our approach is the way that we combine the domain-independent knowledge and the domainspecific knowledge.", "labels": [], "entities": []}, {"text": "It is known that both the corpus-based approach and the lexicon-based approach have classification biases (.", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of our algorithm in reducing the bias, we compare it with the classifier that only uses one view of the LCCT model: either using the corpus-based view or using the lexicon-based view.", "labels": [], "entities": []}, {"text": "The comparison is conducted on the Movie Review dataset.", "labels": [], "entities": [{"text": "Movie Review dataset", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.9686275919278463}]}, {"text": "As shows, our algorithm achieves good performance on both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9995085000991821}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9966075420379639}]}, {"text": "In contrast, the baseline methods either have high precision but low recall, or have high recall but low precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9978869557380676}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9984863996505737}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9986332058906555}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9942703247070312}]}, {"text": "The experiment result suggests that combining the two views is essential in eliminating the classification bias.", "labels": [], "entities": [{"text": "classification", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.950021505355835}]}], "tableCaptions": [{"text": " Table 1: Comparing classification accuracy with 10% labeled data. The LCCT model performs significantly better", "labels": [], "entities": [{"text": "classification", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.923161506652832}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9482110738754272}]}, {"text": " Table 2: Precision and recall on Movie reviews", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.998095691204071}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9992955923080444}]}]}