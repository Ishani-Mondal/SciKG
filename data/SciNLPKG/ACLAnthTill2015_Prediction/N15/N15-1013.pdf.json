{"title": [{"text": "Extractive Summarisation Based on Keyword Profile and Language Model", "labels": [], "entities": [{"text": "Extractive Summarisation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8175402879714966}]}], "abstractContent": [{"text": "We present a statistical framework to extract information-rich citation sentences that sum-marise the main contributions of a scientific paper.", "labels": [], "entities": []}, {"text": "Ina first stage, we automatically discover salient keywords from a paper's citation summary, keywords that characterise its main contributions.", "labels": [], "entities": []}, {"text": "Ina second stage, exploiting the results of the first stage, we identify citation sentences that best capture the paper's main contributions.", "labels": [], "entities": []}, {"text": "Experimental results show that our approach using methods rooted in quantitative statistics and information theory out-performs the current state-of-the-art systems in scientific paper summarisation.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Following, we use the pyramid method ( at sentence level to evaluate our system's performance.", "labels": [], "entities": []}, {"text": "The pyramid score is a fact-based evaluation method that has been especially popular in evaluating extractive summarisation systems.", "labels": [], "entities": [{"text": "extractive summarisation", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.60920450091362}]}, {"text": "It has been widely adopted because it incorporates both fact coverage and fact importance into the scoring process, which resonates well with the goals of summarisation ().", "labels": [], "entities": [{"text": "summarisation", "start_pos": 155, "end_pos": 168, "type": "TASK", "confidence": 0.9538155198097229}]}, {"text": "More specifically, the pyramid method scores a summary using the ratio between the total facts weights of the facts it covers and that of an optimal summary.", "labels": [], "entities": []}, {"text": "First a fact weights pyramid is built using some facts weighting method and each fact is subsequently put into its perspective pyramid tier.", "labels": [], "entities": []}, {"text": "built a weights pyramid for each paper and assigned each humanly discovered fact into a tier according to the number of citing sentences the fact occurs in that paper's citation summary.", "labels": [], "entities": []}, {"text": "For example, fact f i appearing in |f i | citing sentences in the citation summary of paper Dis assigned to the tier T |f i | in D's fact weights pyramid PD . Let F i denotes the number of facts in the summary ES in tier Ti of PD . The total facts weights ES covers is calculated as: where n is the highest tier of PD . Let ES optimal be the optimal summary for D w.r.t. the summary length limit (ES optimal can be found using heuristicdriven exhaustive search).", "labels": [], "entities": []}, {"text": "The pyramid score for ES is finally calculated as: Note again that we used exactly the same corpus and evaluation method as in, which makes our results directly comparable to those described in those papers.", "labels": [], "entities": [{"text": "ES", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.5140094757080078}]}, {"text": "Furthermore, both papers report on performance of various baseline methods which are also directly comparable to ours (see next section).", "labels": [], "entities": []}, {"text": "We compare our results with the current state-of-theart; readers are encouraged to refer to (Qazvinian Rank Summary KPLM without sentence re-ranking (Pyramid score: 0.23) 1 3.1 decoding mcdonald et al (2005b) use the chu-liuedmonds (cle) algorithm to solve the maximum spanning tree problem.", "labels": [], "entities": []}, {"text": "2 thus far, the formulation follows mcdonald et al (2005b) and corresponds to the maximum spanning tree (mst) problem.", "labels": [], "entities": []}, {"text": "3 while we have presented signi cant improvements using additional constraints, one may won5even when caching feature extraction during training mcdonald et al (2005a) still takes approximately 10 minutes to train.", "labels": [], "entities": [{"text": "caching feature extraction", "start_pos": 102, "end_pos": 128, "type": "TASK", "confidence": 0.6254568994045258}]}], "tableCaptions": [{"text": " Table 1: Gold standard key facts of P05-1013 (Qazvinian  and Radev, 2008) ordered by importance. The pyramid  tier might not be the sum of the occurrences of facts, as  multiple facts can appear in the same sentence.", "labels": [], "entities": []}, {"text": " Table 2: Extracted keywords for P05-1013, ranked by de- creasing Hypergeometric test significance.", "labels": [], "entities": [{"text": "P05-1013", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.7620339393615723}, {"text": "Hypergeometric test significance", "start_pos": 66, "end_pos": 98, "type": "METRIC", "confidence": 0.580304483572642}]}, {"text": " Table 3: Keyword profile language model built for an  imaginary document consists of only 5 distinct words.", "labels": [], "entities": []}, {"text": " Table 5: Summary pyramid score evaluation results with  summary length limit k = 5.", "labels": [], "entities": [{"text": "Summary pyramid score evaluation", "start_pos": 10, "end_pos": 42, "type": "METRIC", "confidence": 0.8181453496217728}, {"text": "summary length limit k", "start_pos": 57, "end_pos": 79, "type": "METRIC", "confidence": 0.9255324751138687}]}]}