{"title": [{"text": "Toward Abstractive Summarization Using Semantic Representations", "labels": [], "entities": [{"text": "Abstractive Summarization", "start_pos": 7, "end_pos": 32, "type": "TASK", "confidence": 0.6406923979520798}]}], "abstractContent": [{"text": "We present a novel abstractive summarization framework that draws on the recent development of a treebank for the Abstract Meaning Representation (AMR).", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.5443783700466156}, {"text": "Abstract Meaning Representation (AMR)", "start_pos": 114, "end_pos": 151, "type": "TASK", "confidence": 0.7745300531387329}]}, {"text": "In this framework, the source text is parsed to a set of AMR graphs, the graphs are transformed into a summary graph, and then text is generated from the summary graph.", "labels": [], "entities": []}, {"text": "We focus on the graph-to-graph transformation that reduces the source semantic graph into a summary graph, making use of an existing AMR parser and assuming the eventual availability of an AMR-to-text generator.", "labels": [], "entities": []}, {"text": "The framework is data-driven, trainable, and not specifically designed fora particular domain.", "labels": [], "entities": []}, {"text": "Experiments on gold-standard AMR annotations and system parses show promising results.", "labels": [], "entities": [{"text": "AMR annotations", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.6053095161914825}]}, {"text": "Code is available at: https://github.com/summarization", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstractive summarization is an elusive technological capability in which textual summaries of content are generated de novo.", "labels": [], "entities": [{"text": "Abstractive summarization", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8121431171894073}]}, {"text": "Demand is on the rise for high-quality summaries not just for lengthy texts (e.g., books; and texts known to be prohibitively difficult for people to understand (e.g., website privacy policies;), but also for non-textual media (e.g., videos and image collections;, where extractive and compressive summarization techniques simply do not suffice.", "labels": [], "entities": [{"text": "summaries", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.957511305809021}]}, {"text": "We believe that the challenge of abstractive summarization deserves renewed attention and propose that recent developments in semantic analysis have an important role to play.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.5534687936306}, {"text": "semantic analysis", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.9044474959373474}]}, {"text": "We conduct the first study exploring the feasibility of an abstractive summarization system based on transformations of semantic representations such as the Abstract Meaning Representation (AMR;.", "labels": [], "entities": []}, {"text": "Example sentences and their AMR graphs are shown in.", "labels": [], "entities": []}, {"text": "AMR has much in common with earlier formalisms; today an annotated corpus comprised of over 20,000 AMR-analyzed English sentences () and an automatic AMR parser (JAMR;) are available.", "labels": [], "entities": []}, {"text": "In our framework, summarization consists of three steps illustrated in: (1) parsing the input sentences to individual AMR graphs, (2) combining and transforming those graphs into a single summary AMR graph, and (3) generating text from the summary graph.", "labels": [], "entities": [{"text": "summarization", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.988946259021759}]}, {"text": "This paper focuses on step 2, treating it as a structured prediction problem.", "labels": [], "entities": []}, {"text": "We assume text documents as input and use JAMR for step 1.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.6162685751914978}]}, {"text": "We use a simple method to read a bag of words off the summary graph, allowing evaluation with ROUGE-1, and leave full text generation from AMR (step 3) to future work.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9828886985778809}, {"text": "full text generation", "start_pos": 113, "end_pos": 133, "type": "TASK", "confidence": 0.6402236521244049}, {"text": "AMR", "start_pos": 139, "end_pos": 142, "type": "DATASET", "confidence": 0.8201810121536255}]}, {"text": "The graph summarizer, described in \u00a74, first merges AMR graphs for each input sentence through a concept merging step, in which coreferent nodes of the graphs are merged; a sentence conjunction step, which connects the root of each sentence's AMR graph to a dummy \"ROOT\" node; and an optional Figure 1: A toy example.", "labels": [], "entities": []}, {"text": "Sentences are parsed into individual AMR graphs in step 1; step 2 conducts graph transformation that produces a single summary AMR graph; text is generated from the summary graph in step 3.", "labels": [], "entities": []}, {"text": "graph expansion step, where additional edges are added to create a fully dense graph on the sentencelevel.", "labels": [], "entities": [{"text": "graph expansion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7085563391447067}]}, {"text": "These steps result in a single connected source graph.", "labels": [], "entities": []}, {"text": "A subset of the nodes and arcs from the source graph are then selected for inclusion in the summary graph.", "labels": [], "entities": []}, {"text": "Ideally this is a condensed representation of the most salient semantic content from the source.", "labels": [], "entities": []}, {"text": "We briefly review AMR and JAMR ( \u00a72), then present the dataset used in this paper ( \u00a73).", "labels": [], "entities": [{"text": "AMR", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.46322953701019287}, {"text": "JAMR", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.7917439341545105}]}, {"text": "The main algorithm is presented in \u00a74, and we discuss our simple generation step in \u00a75.", "labels": [], "entities": []}, {"text": "Our experiments ( \u00a76) measure the intrinsic quality of the graph transformation algorithm as well as the quality of the terms selected for the summary (using ROUGE-1).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 158, "end_pos": 165, "type": "METRIC", "confidence": 0.9540796279907227}]}, {"text": "We explore variations on the transformation and the learning algorithm, and show oracle upper bounds of various kinds.", "labels": [], "entities": []}], "datasetContent": [{"text": "To build and evaluate our framework, we require a dataset that includes inputs and summaries, each with gold-standard AMR annotations.", "labels": [], "entities": []}, {"text": "This allows us to use a statistical model for step 2 (graph summarization) and to separate its errors from those in step 1 (AMR parsing), which is important in determining whether this approach is worth further investment.", "labels": [], "entities": [{"text": "graph summarization", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.6566534042358398}, {"text": "AMR parsing", "start_pos": 124, "end_pos": 135, "type": "TASK", "confidence": 0.6728226691484451}]}, {"text": "Fortunately, the \"proxy report\" section of the AMR Bank (  In, we report the performance of subgraph prediction and end-to-end summarization on the test set, using gold-standard and automatic AMR parses for the input.", "labels": [], "entities": [{"text": "AMR Bank", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.8956234455108643}, {"text": "subgraph prediction", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7629825174808502}]}, {"text": "Gold-standard AMR annotations are used for model training in all conditions.", "labels": [], "entities": [{"text": "model training", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.8402812480926514}]}, {"text": "During testing, we apply the trained model to source graphs constructed using either gold-standard or JAMR parses.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.8251941204071045}]}, {"text": "In all of these experiments, we use the number of edges in the gold-standard summary graph to fix the number of edges in the predicted subgraph, allowing direct comparison across conditions.", "labels": [], "entities": []}, {"text": "Subgraph prediction is evaluated against the goldstandard AMR graphs on summaries.", "labels": [], "entities": [{"text": "Subgraph prediction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8385818898677826}]}, {"text": "We report precision, recall, and F 1 for nodes, and F 1 for edges.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9967149496078491}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9996175765991211}, {"text": "F 1", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.994894415140152}, {"text": "F 1", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.992935836315155}]}, {"text": "Oracle results for the subgraph prediction stage are obtained using the ILP decoder to minimize the cost of the output graph, given the gold-standard.", "labels": [], "entities": [{"text": "subgraph prediction", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.8351702392101288}, {"text": "ILP decoder", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.8720403015613556}]}, {"text": "We assign wrong nodes and edges a score of \u22121, correct nodes and edges a score of 0, then decode with the same structural constraints as in subgraph prediction.", "labels": [], "entities": [{"text": "subgraph prediction", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.8114080429077148}]}, {"text": "The resulting graph is the best summary graph in the hypothesis space of our model, and provides an upper bound on performance achievable within our framework.", "labels": [], "entities": []}, {"text": "Oracle performance on node prediction is in the range of 80% when using gold-standard AMR annotations, and 70% when using JAMR output.", "labels": [], "entities": [{"text": "node prediction", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7741588652133942}]}, {"text": "Edge prediction has lower performance, yielding 52.2% for gold-standard and 31.1% for JAMR parses.", "labels": [], "entities": [{"text": "Edge prediction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7457269728183746}, {"text": "JAMR parses", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.48659731447696686}]}, {"text": "When graph expansion was applied, the numbers increased to 64% and 46.7%, respectively.", "labels": [], "entities": [{"text": "graph expansion", "start_pos": 5, "end_pos": 20, "type": "TASK", "confidence": 0.6553374528884888}]}, {"text": "The uncovered summary edge (i.e., those not covered by source graph) is a major source for low recall values on edge prediction (see); graph expansion slightly alleviates this issue.", "labels": [], "entities": [{"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9958360195159912}, {"text": "edge prediction", "start_pos": 112, "end_pos": 127, "type": "TASK", "confidence": 0.7542790472507477}]}, {"text": "Summarization is evaluated by comparing system summaries against reference summaries, using ROUGE-1 scores . System summaries are generated using the heuristic approach presented in \u00a75: given a predicted subgraph, the approach finds the most frequently aligned word span for each concept node, and then puts them together as a bag of words.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9690390825271606}, {"text": "ROUGE-1", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9908004403114319}]}, {"text": "ROUGE-1 is particularly usefully for eval-Structured perceptron loss: Since cost factors just like the scoring function, each max operation can be accomplished using a variant of ILP decoding ( \u00a74.2.1) in which the cost is incorporated into the linear objective while the constraints remain the same.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9368116855621338}]}], "tableCaptions": [{"text": " Table 1: Statistics of our dataset. \"Expand\" shows the  number of edges after performing graph expansion. The  numbers are averaged across all documents in the split.  We use the official split, dropping one training document  for which no summary sentences were annotated.", "labels": [], "entities": [{"text": "Expand", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9927012920379639}]}, {"text": " Table 2: Percentage of summary edges that can be cov- ered by an automatically constructed source graph.", "labels": [], "entities": []}, {"text": " Table 5: Subgraph prediction and summarization (to bag of words) results on test set. Gold-standard AMR annotations  are used for model training in all conditions. \"+ Expand\" means the result is obtained using source graph with  expansion; edge performance is measured ignoring labels.", "labels": [], "entities": [{"text": "Subgraph prediction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8823610246181488}]}]}