{"title": [{"text": "I Can Has Cheezburger? A Nonparanormal Approach to Combining Textual and Visual Information for Predicting and Generating Popular Meme Descriptions", "labels": [], "entities": [{"text": "Predicting and Generating Popular Meme Descriptions", "start_pos": 96, "end_pos": 147, "type": "TASK", "confidence": 0.862473060687383}]}], "abstractContent": [{"text": "The advent of social media has brought Inter-net memes, a unique social phenomenon, to the front stage of the Web.", "labels": [], "entities": []}, {"text": "Embodied in the form of images with text descriptions, little do we know about the \"language of memes\".", "labels": [], "entities": []}, {"text": "In this paper, we statistically study the correlations among popular memes and their wordings , and generate meme descriptions from raw images.", "labels": [], "entities": []}, {"text": "To do this, we take a multi-modal approach-we propose a robust non-paranormal model to learn the stochastic dependencies among the image, the candidate descriptions, and the popular votes.", "labels": [], "entities": []}, {"text": "In experiments , we show that combining text and vision helps identifying popular meme descriptions; that our nonparanormal model is able to learn dense and continuous vision features jointly with sparse and discrete text features in a prin-cipled manner, outperforming various competitive baselines; that our system can generate meme descriptions using a simple pipeline.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the past few years, Internet memes become anew, contagious social phenomenon: it all starts with an image with a witty, catchy, or sarcastic sentence, and people circulate it from friends to friends, colleagues to colleagues, and families to families.", "labels": [], "entities": []}, {"text": "Eventually, some of them go viral on the Internet.", "labels": [], "entities": []}, {"text": "Meme is not only about the funny picture, the Internet culture, or the emotion that passes along, but also about the richness and uniqueness of its language: it is often highly structured with special written style, and forms interesting and subtle connotations that resonate among the readers.", "labels": [], "entities": []}, {"text": "For example, the LOL cat memes (e.g.,) often include superimposed text with broken grammars and/or spellings.", "labels": [], "entities": [{"text": "LOL cat memes", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.7459174394607544}]}, {"text": "Even though the memes are popular over the Internet, the \"language of memes\" is still not wellunderstood: there are no systematic studies on predicting and generating popular Internet memes from the Natural Language Processing (NLP) and Computer Vision (CV) perspectives.", "labels": [], "entities": []}, {"text": "In this paper, we take a multimodal approach to predict and generate popular meme descriptions.", "labels": [], "entities": []}, {"text": "To do this, we collect a set of original meme images, a list of candidate descriptions, and the corresponding votes.", "labels": [], "entities": []}, {"text": "We propose a robust nonparanormal approach () to model the multimodal stochastic dependencies among images, text, and votes.", "labels": [], "entities": []}, {"text": "We then introduce a simple pipeline for generating meme descriptions combining reverse image search and traditional information retrieval approaches.", "labels": [], "entities": []}, {"text": "In empirical experiments, we show that our model outperforms strong discriminative baselines by very large margins in the regression/ranking experiments, and that in the generation experiment, the nonparanormal outperforms the second-best supervised baseline by 4.35 BLEU points, and obtains a BLEU score improvement of 4.48 over an unsupervised recurrent neural network language model trained on a large meme corpus that is almost 90 times larger.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 267, "end_pos": 271, "type": "METRIC", "confidence": 0.9978371262550354}, {"text": "BLEU", "start_pos": 294, "end_pos": 298, "type": "METRIC", "confidence": 0.9990286827087402}]}, {"text": "Our contributions are three-fold: \u2022 We are the first to study the \"language of memes\" combining NLP, CV, and machine learning techniques, and show that combining the visual and textual signals helps identifying popular meme descriptions; \u2022 Our approach empowers Internet users to select better wordings and generate new memes automatically; \u2022 Our proposed robust nonparanormal model outperforms competitive baselines for predicting and generating popular meme descriptions.", "labels": [], "entities": [{"text": "predicting and generating popular meme descriptions", "start_pos": 421, "end_pos": 472, "type": "TASK", "confidence": 0.7442837804555893}]}, {"text": "In the next section, we outline related work.", "labels": [], "entities": []}, {"text": "In Section 3, we introduce the theory of copula, and our nonparanormal approach.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the datasets.", "labels": [], "entities": []}, {"text": "We show the prediction and generation results in Section 5 and Section 6.", "labels": [], "entities": [{"text": "prediction", "start_pos": 12, "end_pos": 22, "type": "TASK", "confidence": 0.953177809715271}]}, {"text": "Finally, we conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collected meme images and text descriptions 6 from two popular meme websites 7 . In the prediction experiment, we use 3,008 image-description pairs for training, and 526 image-description pairs for testing.", "labels": [], "entities": []}, {"text": "In the generation experiment, we use 269,473 meme descriptions to index the meme search engine, and 50 randomly selected images for testing.", "labels": [], "entities": []}, {"text": "During training, we convert the raw counts of popular votes into reciprocal ranks (e.g., the most popular text descriptions will all have a reciprocal rank of 1, and n-th popular one will have a score of 1/n).", "labels": [], "entities": []}, {"text": "In the first experiment, we compare the proposed NPN with various baselines in a prediction task, since prior literature () also suggests using ranking based evaluation for associating images with text descriptions.", "labels": [], "entities": []}, {"text": "Throughout the experiment sections, we set = 10, and \u03b4 = 80 as the dropout hyperparameters.", "labels": [], "entities": []}, {"text": "Spearman's correlation and Kendall's tau have been widely used in many real-valued prediction (regression) problems in NLP, and here we use them to measure the quality of predicted values\u02c6yvalues\u02c6 values\u02c6y by comparing to the vector of ground truth y.", "labels": [], "entities": [{"text": "Spearman's correlation", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.6939327915509542}]}, {"text": "Kendall's tau is a nonparametric statistical metric that have shown to be inexpensive, robust, and representation independent).", "labels": [], "entities": []}, {"text": "We use paired two-tailed t-test to measure the statistical significance.", "labels": [], "entities": []}, {"text": "In this section, we investigate the performance of our meme generation system using 50 test meme images.", "labels": [], "entities": [{"text": "meme generation", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.872221827507019}]}, {"text": "To quantitatively evaluate our system, we compare with both unsupervised and supervised baselines.", "labels": [], "entities": []}, {"text": "For the unsupervised baselines, we compare with a compact recurrent neural network language model (RNNLM) (Mikolov, 2012) trained on the 3,008 text descriptions of our meme training set, as well as a full model of RNNLM trained on a large meme corpus of 269K sentences 9 . For the supervised baselines, all models are trained on the 3,008 training image-description pairs with labels.", "labels": [], "entities": []}, {"text": "All these models can be viewed as different re-ranking methods for the retrieved candidate descriptions.", "labels": [], "entities": []}, {"text": "We use BLEU score () as the evaluation metric, since the generation task can be viewed as translating raw images into sentences, and it is used in many caption generation studies ().", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.9880676567554474}, {"text": "caption generation", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.7902581691741943}]}, {"text": "The generation result is shown in  BLEU points over the full RNNLM, which is trained on a corpus that is \u223c90 times larger, in an unsupervised fashion.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9971792697906494}, {"text": "RNNLM", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.9152734875679016}]}, {"text": "When breaking down the results, we see that our NPN's advantage is on generating longer phrases, typically trigrams and four-grams, comparing to the other models.", "labels": [], "entities": []}, {"text": "This is very interesting, because generating high-quality long phrases is difficult, since the memes are often short.", "labels": [], "entities": []}, {"text": "We show some generation examples in.", "labels": [], "entities": []}, {"text": "We see that on the left column, the reference memes are the ones with top votes by the crowd.", "labels": [], "entities": []}, {"text": "The first chemistry cat meme includes puns, the second forever alone meme includes reference to the life simulation video game, while the last Batman meme has interesting conversations.", "labels": [], "entities": []}, {"text": "In the second column, we see that the memes generated by the full RNNLM model are short, which corresponds to the quantitative results in.", "labels": [], "entities": [{"text": "RNNLM model", "start_pos": 66, "end_pos": 77, "type": "DATASET", "confidence": 0.8114094436168671}]}, {"text": "In the third column, our NPN meme generator was able to generate longer descriptions.", "labels": [], "entities": []}, {"text": "Interestingly, it also creates a pun for the chemistry cat meme.", "labels": [], "entities": []}, {"text": "Our generation on the forever alone meme is also accurate.", "labels": [], "entities": []}, {"text": "In the Batman example, we show that the NPN model makes a sentence-image-mismatch type of error: although the generated sentence includes the entities Batman and Robin, as well as their slapping activity, it was originally created for the \"overly attached girlfriend\" meme 10 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The Spearman correlation (top table) and  Kendall's \u03c4 (bottom table) for comparing various text fea- tures and combining with vision features. The best results  of each row are highlighted in bold. * indicates p < .001  comparing to the second best result.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 14, "end_pos": 34, "type": "METRIC", "confidence": 0.6173608154058456}, {"text": "Kendall's \u03c4", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.7666625181833903}]}, {"text": " Table 2: The effects of dropout training for NPNs on  meme and other datasets. The best results of each row  are highlighted in bold. * indicates p < .001 comparing  to the no dropout setting.", "labels": [], "entities": []}, {"text": " Table 3: Top-30 linguistic features that are highly corre- lated with the popular votes.", "labels": [], "entities": []}, {"text": " Table 4. Note  that when combining B-1 to B-4 scores, BLEU in- cludes a brevity penalty as described in the original  BLEU paper. We see that our NPN model outper- forms the best supervised baseline by 4.35 BLEU  points, while also obtaining an advantage of 4.48", "labels": [], "entities": [{"text": "B-1", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9612200856208801}, {"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9984834790229797}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.8673262000083923}, {"text": "BLEU", "start_pos": 208, "end_pos": 212, "type": "METRIC", "confidence": 0.9955580830574036}]}, {"text": " Table 4: The BLEU scores for generating memes from  images. B-1 to B-4: BLEU unigram to four-grams. The  best BLEU results are highlighted in bold. * indicates  p < .001 comparing to the second best system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9979396462440491}, {"text": "B-1", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9455587863922119}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.996480405330658}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9862000942230225}]}]}