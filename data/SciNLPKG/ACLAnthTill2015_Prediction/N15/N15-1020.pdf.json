{"title": [{"text": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses", "labels": [], "entities": [{"text": "Context-Sensitive Generation of Conversational Responses", "start_pos": 29, "end_pos": 85, "type": "TASK", "confidence": 0.7924516916275024}]}], "abstractContent": [{"text": "We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations.", "labels": [], "entities": [{"text": "response generation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7197161763906479}]}, {"text": "A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances.", "labels": [], "entities": []}, {"text": "Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.", "labels": [], "entities": [{"text": "dynamic-context generative", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6600683331489563}, {"text": "Machine Translation and Information Retrieval", "start_pos": 114, "end_pos": 159, "type": "TASK", "confidence": 0.7863208770751953}]}], "introductionContent": [{"text": "Until recently, the goal of training open-domain conversational systems that emulate human conversation has seemed elusive.", "labels": [], "entities": []}, {"text": "However, the vast quantities of conversational exchanges now available on social media websites such as Twitter and Reddit raise the prospect of building data-driven models that can begin to communicate conversationally.", "labels": [], "entities": []}, {"text": "The work of, for example, demonstrates that a response generation system can be constructed from Twitter conversations using statistical machine translation techniques, where a status post by a Twitter user is \"translated\" into a plausible looking response.", "labels": [], "entities": [{"text": "response generation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.9009019136428833}, {"text": "statistical machine translation", "start_pos": 125, "end_pos": 156, "type": "TASK", "confidence": 0.6649314761161804}]}, {"text": "However, an approach such as that presented in   generating responses that are sensitive to the context of the conversation.", "labels": [], "entities": []}, {"text": "Broadly speaking, context maybe linguistic or involve grounding in the physical or virtual world, but we here focus on linguistic context.", "labels": [], "entities": []}, {"text": "The ability to take into account previous utterances is key to building dialog systems that can keep conversations active and engaging.", "labels": [], "entities": []}, {"text": "illustrates atypical Twitter dialog where the contextual information is crucial: the phrase \"good luck\" is plainly motivated by the reference to \"your game\" in the first utterance.", "labels": [], "entities": []}, {"text": "In the MT model, such contextual sensitivity is difficult to capture; moreover, naive injection of context information would entail unmanageable growth of the phrase table at the cost of increased sparsity, and skew towards rarely-seen context pairs.", "labels": [], "entities": [{"text": "MT", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9554067850112915}]}, {"text": "In most statistical approaches to machine translation, phrase pairs do not share statistical weights regardless of their intrinsic semantic commonality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7547748982906342}]}, {"text": "We propose to address the challenge of contextsensitive response generation by using continuous representations or embeddings of words and phrases to compactly encode semantic and syntactic similarity.", "labels": [], "entities": [{"text": "contextsensitive response generation", "start_pos": 39, "end_pos": 75, "type": "TASK", "confidence": 0.6402252912521362}]}, {"text": "We argue that embedding-based models af-ford flexibility to model the transitions between consecutive utterances and to capture long-span dependencies in a domain where traditional word and phrase alignment is difficult (.", "labels": [], "entities": [{"text": "word and phrase alignment", "start_pos": 181, "end_pos": 206, "type": "TASK", "confidence": 0.6400238946080208}]}, {"text": "To this end, we present two simple, context-sensitive response-generation models utilizing the Recurrent Neural Network Language Model (RLM) architecture of).", "labels": [], "entities": []}, {"text": "These models first encode past information in a hidden continuous representation, which is then decoded by the RLM to promote plausible responses that are simultaneously fluent and contextually relevant.", "labels": [], "entities": []}, {"text": "Unlike typical complex task-oriented multi-modular dialog systems), our architecture is completely data-driven and can easily be trained end-to-end using unstructured data without requiring human annotation, scripting, or automatic parsing.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions.", "labels": [], "entities": []}, {"text": "We present a neural network architecture for response generation that is both context-sensitive and datadriven.", "labels": [], "entities": [{"text": "response generation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8105067014694214}]}, {"text": "As such, it can be trained from end to end on massive amounts of social media data.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first application of a neural-network model to open-domain response generation, and we believe that the present work will lay groundwork for more complex models to come.", "labels": [], "entities": [{"text": "open-domain response generation", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.6044593552748362}]}, {"text": "We additionally introduce a novel multi-reference extraction technique that shows promise for automated evaluation.", "labels": [], "entities": [{"text": "multi-reference extraction", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.7017542719841003}]}], "datasetContent": [{"text": "For computational efficiency and to alleviate the burden of human evaluators, we restrict the context sequence c to a single sentence.", "labels": [], "entities": []}, {"text": "Hence, our dataset is composed of \"triples\" \u03c4 \u2261 (c \u03c4 , m \u03c4 , r \u03c4 ) consisting of three sentences.", "labels": [], "entities": []}, {"text": "We mined 127M context-messageresponse triples from the Twitter FireHose, covering the 3-month period June 2012 through August 2012.", "labels": [], "entities": []}, {"text": "Only those triples where context and response were generated by the same user were extracted.", "labels": [], "entities": []}, {"text": "To minimize noise, we selected triples that contained at least one frequent bigram that appeared more than 3 times in the corpus.", "labels": [], "entities": []}, {"text": "This produced a corpus of 29M Twitter triples.", "labels": [], "entities": []}, {"text": "Additionally, we hired crowdsourced raters to evaluate approximately 33K candidate triples.", "labels": [], "entities": []}, {"text": "Judgments on a 5-point scale were obtained from 3 raters apiece.", "labels": [], "entities": [{"text": "Judgments", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.763358473777771}]}, {"text": "This yielded a set of 4232 triples with a mean score of 4 or better that was then randomly binned into a tuning set of 2118 triples and a test set of 2114 triples . The mean length of responses in these sets was approximately 11.5 tokens, after cleanup (e.g., stripping of emoticons), including punctuation.", "labels": [], "entities": []}, {"text": "We evaluate all systems using BLEU () and METEOR (), and supplement these results with more targeted human pairwise comparisons in Section 6.3.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9988203644752502}, {"text": "METEOR", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9839359521865845}]}, {"text": "A major challenge in using these automated metrics for response generation is that the set of reasonable responses in our task is potentially vast and extremely diverse.", "labels": [], "entities": [{"text": "response generation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8460928201675415}]}, {"text": "The dataset construction method just described yields only a single reference for each status.", "labels": [], "entities": []}, {"text": "Accordingly, we extend the set of references using an IR approach to mine potential responses, after which we have human judges rate their appropriateness.", "labels": [], "entities": []}, {"text": "As we see in Section 6.3, it turns out that by optimizing systems towards BLEU using mined multi-references, BLEU rankings align well with human judgments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.7492635250091553}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.919461727142334}]}, {"text": "This lays groundwork for interesting future correlation studies.", "labels": [], "entities": []}, {"text": "Multi-reference extraction We use the following algorithm to better cover the space of reasonable responses.", "labels": [], "entities": [{"text": "Multi-reference extraction", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7584418654441833}]}, {"text": "Given a test triple \u03c4 \u2261 (c \u03c4 , m \u03c4 , r \u03c4 ), our goal is to mine other responses {r\u02dc\u03c4{r\u02dc {r\u02dc\u03c4 } that fit the context and message pair (c \u03c4 , m \u03c4 ).", "labels": [], "entities": []}, {"text": "To this end, we first select a set of 15 candidate triples {\u02dc\u03c4{\u02dc\u03c4 } using an IR system.", "labels": [], "entities": []}, {"text": "The IR system is calibrated in order to select candidate triples\u02dc\u03c4triples\u02dc triples\u02dc\u03c4 for which both the message m \u02dc \u03c4 and the response r \u02dc \u03c4 are similar to the original message m \u03c4 and response r \u03c4 . Formally, the score of a candidate triple is: where dis the bag-of-words BM25 similarity function (, \u03b1 controls the impact of the similarity between the responses and is a smoothing factor that avoids zero scores for candidate responses that do not share any words with the reference response.", "labels": [], "entities": [{"text": "IR", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8647336363792419}]}, {"text": "We found that this simple formula provided references that were both diverse and plausible.", "labels": [], "entities": []}, {"text": "Given a set of candidate triples {\u02dc\u03c4{\u02dc\u03c4 }, human evaluators are asked to rate the quality of the response within the new triples {(c \u03c4 , m \u03c4 , r \u02dc \u03c4 )}.", "labels": [], "entities": []}, {"text": "After human evaluation, we retain the references for which the score is 4 or better on a 5 point scale, resulting in 3.58 references per example on average.", "labels": [], "entities": []}, {"text": "The average lengths for the responses in the multi-reference tuning and test sets are 8.75 and 8.13 tokens respectively.", "labels": [], "entities": []}, {"text": "Human evaluation was conducted using crowdsourced annotators.", "labels": [], "entities": []}, {"text": "Annotators were asked to compare the quality of system output responses pairwise (\"Which is better?\") in relation to the context and message strings in the 2114 item test set.", "labels": [], "entities": [{"text": "2114 item test set", "start_pos": 156, "end_pos": 174, "type": "DATASET", "confidence": 0.7538740038871765}]}, {"text": "Identical strings were held out, so that the annotators only saw those outputs that differed.", "labels": [], "entities": []}, {"text": "Paired responses were presented in random order to the annotators, and each pair of responses was judged by 5 annotators.", "labels": [], "entities": []}, {"text": "summarizes the results of human evaluation, giving the difference in mean scores (pairwise preference margin) between systems and 95% confidence intervals generated using Welch's t-test.", "labels": [], "entities": [{"text": "pairwise preference margin)", "start_pos": 82, "end_pos": 109, "type": "METRIC", "confidence": 0.7773676365613937}]}, {"text": "Identical strings not shown to raters are incorporated with an automatically assigned score of 0.5.", "labels": [], "entities": []}, {"text": "The pattern in these results is clear and consistent: context-sensitive systems (+CMM) outperform non-context-sensitive systems, with preference gains as high as approximately 5.3% in the case of DCGM-II+CMM versus IR, and about 3.1% in the case of DCGM-II+CMM versus MT.", "labels": [], "entities": []}, {"text": "Similarly, context-sensitive DCGM systems outperform non-DCGM context-sensitive systems by 1.5% (MT) and 2.3% (IR).", "labels": [], "entities": []}, {"text": "These results are consistent with the automated BLEU rankings and confirm that our best performing DCGM models outperform both raw baseline and the context-sensitive baseline using CMM features.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9678163528442383}]}, {"text": "provides examples of responses generated on the tuning corpus by the MT-based DCGM-II+CMM system, our best system in terms of both BLEU and human evaluation.", "labels": [], "entities": [{"text": "MT-based DCGM-II+CMM", "start_pos": 69, "end_pos": 89, "type": "DATASET", "confidence": 0.8523527681827545}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.997386634349823}]}, {"text": "Responses from this system are on average shorter (8.95 tokens) than the original human responses in the tuning set (11.5 tokens).", "labels": [], "entities": []}, {"text": "Overall, the outputs tend to be generic or commonplace, but are often reasonably plausible in the context as in examples 1-3, especially where context and message contain common conversational elements.", "labels": [], "entities": []}, {"text": "Example 2 illustrates the impact of context-sensitivity: the word \"book\" in the response is not found in the message.", "labels": [], "entities": []}, {"text": "Nonetheless, longer generated responses are apt to degrade both syntactically and in terms of content.", "labels": [], "entities": []}, {"text": "We notice that longer responses are likely to present  information that conflicts either internally within the response itself, or is at odds with the context, as in examples 4-5.", "labels": [], "entities": []}, {"text": "This is not unsurprising, since our model lacks mechanisms both for reflecting agent intent in the response and for maintaining consistency with respect to sentiment polarity.", "labels": [], "entities": []}, {"text": "Longer context and message components may also result in responses that wander off-topic or lapse into incoherence as in 6-8, especially when relatively low frequency unigrams (\"bass\", \"threat\") are echoed in the response.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of triples, average, minimum and maxi- mum number of references for tuning and test corpora.", "labels": [], "entities": [{"text": "maxi- mum number", "start_pos": 50, "end_pos": 66, "type": "METRIC", "confidence": 0.9553936421871185}]}, {"text": " Table 3: Context-sensitive ranking results on both MT (left) and IR (right) n-best lists, n = 1000. The subscript feat.  indicates the number of features of the models. The log-linear weights are estimated by running one iteration of MERT.  We mark by (\u00b1%) the relative improvements with respect to the reference system (\u00a3).", "labels": [], "entities": [{"text": "feat", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9705861210823059}]}]}