{"title": [{"text": "HEADS: Headline Generation as Sequence Prediction Using an Abstract Feature-Rich Space", "labels": [], "entities": [{"text": "Headline Generation", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.8299108147621155}, {"text": "Sequence Prediction", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8311291933059692}]}], "abstractContent": [{"text": "Automatic headline generation is a sub-task of document summarization with many reported applications.", "labels": [], "entities": [{"text": "Automatic headline generation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6538985073566437}, {"text": "document summarization", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.6554917991161346}]}, {"text": "In this study we present a sequence-prediction technique for learning how editors title their news stories.", "labels": [], "entities": []}, {"text": "The introduced technique models the problem as a discrete optimization task in a feature-rich space.", "labels": [], "entities": []}, {"text": "In this space the global optimum can be found in polynomial time by means of dynamic programming.", "labels": [], "entities": []}, {"text": "We train and test our model on an extensive corpus of financial news, and compare it against a number of baselines by using standard metrics from the document sum-marization domain, as well as some new ones proposed in this work.", "labels": [], "entities": []}, {"text": "We also assess the readability and informativeness of the generated titles through human evaluation.", "labels": [], "entities": []}, {"text": "The obtained results are very appealing and substantiate the soundness of the approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Document summarization, also known as text summarization, is the process of automatically abridging text documents.", "labels": [], "entities": [{"text": "Document summarization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8491710126399994}, {"text": "text summarization", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7213465869426727}]}, {"text": "Although traditionally the final objective of text summarization is to produce a paragraph or abstract that summarizes a rather large collection of texts (, the task of producing a very short summary comprised of 10-15 words has also been broadly studied.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.6616128832101822}]}, {"text": "There have been many reported practical applications for this endeavor, most notably, efficient web browsing * Work done during an internship at Yahoo Labs.", "labels": [], "entities": []}, {"text": "on hand-held devices), generation of TV captions), digitization of newspaper articles that have uninformative headlines, and headline generation in one language based on news stories written in another ().", "labels": [], "entities": [{"text": "generation of TV captions", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.8308793753385544}, {"text": "headline generation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.8013345897197723}]}, {"text": "In general terms, a headline of a news article can be defined as a short statement that gives a reader a general idea about the main contents of the story it entitles.", "labels": [], "entities": []}, {"text": "The objective of our study is to develop a novel technique for generating informative headlines for news articles, albeit to conduct experiments we focused on finance articles written in English.", "labels": [], "entities": []}, {"text": "In this work we make a number of contributions concerning statistical models for headline generation, training of the models, and their evaluation, specifically: \u2022 We propose a model that learns how an editor generates headlines for news articles, where a headline is regarded as a compression of its article's text.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7861313819885254}]}, {"text": "Our model significantly differs from others in the way it represents possible headlines in a feature-rich space.", "labels": [], "entities": []}, {"text": "The model tries to learn how humans discern between good and bad compressions.", "labels": [], "entities": []}, {"text": "Furthermore, our model can be trained with any monolingual corpus consisting of titled articles, because it does not request special conditions on the headlines' structure or provenance.", "labels": [], "entities": []}, {"text": "\u2022 We suggest a slight change of the Margin Infused Relaxed Algorithm) to fit our model, which yields better empirical results.", "labels": [], "entities": []}, {"text": "\u2022 We present a simple and elegant algorithm that runs in polynomial time and finds the global optimum of our objective function.", "labels": [], "entities": []}, {"text": "This represents an important advantage of our proposal because many former techniques resort to heuristic-driven search algorithms that are not guaranteed to find the global optimum.", "labels": [], "entities": []}, {"text": "\u2022 With the intention of overcoming several problems suffered by traditional metrics for automatically evaluating the quality of proposed headlines, we propose two new evaluation metrics that correlate with ratings given by human annotators.", "labels": [], "entities": []}], "datasetContent": [{"text": "For performing an automatic evaluation of the headlines generated by our system we follow the path taken by related work such as and use a subset of the ROUGE metrics for comparing candidate headlines with reference ones, which have been proven to strongly correlate with human evaluations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 153, "end_pos": 158, "type": "METRIC", "confidence": 0.9250423908233643}]}, {"text": "We decide to use as metrics ROUGE-1, ROUGE-2, and ROUGE-SU.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.929517388343811}, {"text": "ROUGE-2", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9587453007698059}, {"text": "ROUGE-SU", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9588174819946289}]}, {"text": "We also propose as an experimental new metric a weighted version of ROUGE-SU, which we name ROUGE-WSU.", "labels": [], "entities": [{"text": "ROUGE-SU", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.956473708152771}]}, {"text": "The rationale of our proposal is that ROUGE-SU gives the same importance to all skip-bigrams extracted from a phrase no matter how far apart they are.", "labels": [], "entities": [{"text": "ROUGE-SU", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9780164957046509}]}, {"text": "We address the problem by weighting each shared skip-gram between phrases by the inverse of the token's average gap distance.", "labels": [], "entities": []}, {"text": "Formally: Where function dist H (a, b) returns the skip distance between tokens \"a\" and \"b\" in headline H, and su(H) returns all skip-bigrams in headline H.", "labels": [], "entities": []}, {"text": "With the objective of having a metric capable of detecting abstract concepts in phrases and comparing headlines at a semantic level, we resort to Latent Semantic Indexing (LSI)).", "labels": [], "entities": []}, {"text": "We use the method for extracting latent concepts from our training corpus so as to be able to represent text in an abstract latent space.", "labels": [], "entities": []}, {"text": "We then compute the similarity of a headline with respect to a news article by calculating the cosine similarity of their vector representations in latent space.", "labels": [], "entities": [{"text": "similarity", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9676134586334229}]}, {"text": "We trained our model with a corpus consisting of roughly 1.3 million financial news articles fetched from the web, written in English, and published on the second half of 2012.", "labels": [], "entities": []}, {"text": "We decided to add three important constraints to the learning algorithm which proved to yield positive empirical results: (1) Large news articles are simplified by eliminating their most redundant or least informative sentences.", "labels": [], "entities": []}, {"text": "For this end, the text ranking algorithm proposed by is used for discriminating salient sentences in the article.", "labels": [], "entities": [{"text": "text ranking", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.7030544728040695}]}, {"text": "Furthermore, a news article is considered large if it has more than 300 tokens, which corresponds to the average number of words per article in our training set.", "labels": [], "entities": []}, {"text": "(2) Because we observed that less than 2% of the headlines in the training set contained more than 15 tokens, we constraint the decoding algorithm to only generate headlines consisting of 15 or fewer tokens.", "labels": [], "entities": []}, {"text": "(3) We restrain the decoding algorithm from placing symbols such as commas, quotation marks, and question marks on headlines.", "labels": [], "entities": []}, {"text": "Nonetheless, only headlines that end with a period are considered as solutions; otherwise the model tends to generate non-conclusive phrases as titles.", "labels": [], "entities": []}, {"text": "For automated testing purposes we use a training set consisting of roughly 12,000 previously unseen articles, which were randomly extracted from the initial dataset before training.", "labels": [], "entities": []}, {"text": "The evaluation consisted in producing seven candidate headlines per article: one for each of the four baselines, plus one for each of the three variations of our model (each differing solely on the scheme used to learn the weight vector).", "labels": [], "entities": []}, {"text": "Then each candidate is compared against the article's reference headline by means of the five proposed metrics.", "labels": [], "entities": []}, {"text": "summarizes the obtained results of the models with respect to the ROUGE metrics.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.6980594992637634}]}, {"text": "The results show that our model, when trained with our proposed forced-MIRA update, outperforms all the other baselines on all metrics, except for ROUGE-2, where all differences are statistically significant when assessed via a paired t-test (p < 0.001).", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 147, "end_pos": 154, "type": "METRIC", "confidence": 0.9809844493865967}]}, {"text": "Also, as initially intended, the keywords baseline does produce better scores than all the other methods, therefore it is considered as a naive upper-bound.", "labels": [], "entities": []}, {"text": "It must be highlighted that all the numbers on the table are rather low, this occurs because, as noted by, humans tend to use a very different vocabulary and writing-style on headlines than on articles.", "labels": [], "entities": []}, {"text": "The effect of this is that our methods and baselines are not capable of producing headlines with wordings strongly similar to human-written ones, which as a consequence makes it almost impossible to obtain high ROUGE scores.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 211, "end_pos": 223, "type": "METRIC", "confidence": 0.9659114181995392}]}, {"text": "For having a more objective assessment of our proposal, we carried out a human evaluation of the headlines generated by our model when trained with the f-MIRA scheme and the word graphs approach by.", "labels": [], "entities": []}, {"text": "For this purpose, 100 articles were randomly extracted from the test set and their respective candidate headlines were generated.", "labels": [], "entities": []}, {"text": "Then different human raters were asked to evaluate on a Likert scale, from 1 to 5, both the grammaticality and informativeness of the titles.", "labels": [], "entities": []}, {"text": "Each article-headline pair was annotated by three different raters.", "labels": [], "entities": []}, {"text": "The median of their ratings was chosen as a final mark.", "labels": [], "entities": []}, {"text": "As a reference, the raters were also asked to annotate the actual human-generated headlines from the articles, although they were not informed about the provenance of the titles.", "labels": [], "entities": []}, {"text": "We measured inter-judge agreement by means of their IntraClass Correlation (ICC).", "labels": [], "entities": [{"text": "IntraClass Correlation (ICC)", "start_pos": 52, "end_pos": 80, "type": "METRIC", "confidence": 0.9678851246833802}]}, {"text": "The ICC for grammaticality was 0.51 \u00b1 0.07, which represents fair agreement, and the ICC for informativeness was 0.63 \u00b1 0.05, which represents substantial agreement.", "labels": [], "entities": [{"text": "ICC", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9950952529907227}, {"text": "ICC", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9894604682922363}]}, {"text": "respect to the LSI document similarity metric, and the human evaluations for grammaticality and informativeness.", "labels": [], "entities": [{"text": "LSI document similarity", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.5572546124458313}]}, {"text": "For exploratory purposes the table also contains the average length for the generated headlines of each of the models (which also counts the imposed final period).", "labels": [], "entities": []}, {"text": "The results in this table are satisfying: with respect to LSI document similarity, our model outperforms all of the baselines and its value is close to the one achieved by human-generated headlines.", "labels": [], "entities": [{"text": "LSI document similarity", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.8074161410331726}]}, {"text": "On the other hand, the human evaluations are middling: the word-graphs method produces more readable headlines, but our model proves to be more informative because it does better work at detecting abstract word relationships in the text.", "labels": [], "entities": []}, {"text": "All differences in this table are statistically significant when computed as paired t-tests (p < 0.001).", "labels": [], "entities": []}, {"text": "It is worth noting that the informativeness of human-generated headlines did not get a high score.", "labels": [], "entities": []}, {"text": "The reason for this is the fact that editors tend to produce rather sensationalist or partially informative titles so as to attract the attention of readers and engage them to read the whole article; human raters penalized the relevance of such headlines, which was reflected on this final score.", "labels": [], "entities": []}, {"text": "Finally, table 3 contains the mutual correlation between automated and manual metrics.", "labels": [], "entities": []}, {"text": "The first thing to note is that none of the used metrics proved to be good for assessing grammaticality of headlines.", "labels": [], "entities": [{"text": "assessing grammaticality of headlines", "start_pos": 79, "end_pos": 116, "type": "TASK", "confidence": 0.6985244601964951}]}, {"text": "It is also worth noting that our proposed metric ROUGE-WSU performs as well as the other ROUGE metrics, and that the proposed LSI document similarity does not prove to be as strong a metric as the others.", "labels": [], "entities": []}, {"text": "0.561 0.535 0.557 0.542 0.370: Spearman correlation between human-assessed metrics and automatic ones.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 31, "end_pos": 51, "type": "METRIC", "confidence": 0.8958886861801147}]}], "tableCaptions": [{"text": " Table 1: Result of the evaluation of our models and base- lines with respect to ROUGE metrics.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.7388874888420105}]}, {"text": " Table 2: Result of the evaluation of our models and base- lines with LSI document similarity, grammaticality and  informativeness as assessed by human raters, and aver- age headline length.", "labels": [], "entities": [{"text": "aver- age headline length", "start_pos": 164, "end_pos": 189, "type": "METRIC", "confidence": 0.8570967435836792}]}]}