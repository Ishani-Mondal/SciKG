{"title": [{"text": "Detecting Translation Direction: A Cross-Domain Study", "labels": [], "entities": [{"text": "Detecting Translation Direction", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9744071364402771}]}], "abstractContent": [{"text": "Parallel corpora are constructed by taking a document authored in one language and translating it into another language.", "labels": [], "entities": []}, {"text": "However, the information about the authored and translated sides of the corpus is usually not preserved.", "labels": [], "entities": []}, {"text": "When available, this information can be used to improve statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.7629232307275137}]}, {"text": "Existing statistical methods for translation direction detection have low accuracy when applied to the realistic out-of-domain setting, especially when the input texts are short.", "labels": [], "entities": [{"text": "translation direction detection", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.9892395536104838}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9985502362251282}]}, {"text": "Our contributions in this work are threefold: 1) We develop a multi-corpus parallel dataset with translation direction labels at the sentence level, 2) we perform a comparative evaluation of previously introduced features for translation direction detection in a cross-domain setting and 3) we generalize a previously introduced type of features to out-perform the best previously proposed features in detecting translation direction and achieve 0.80 precision with 0.85 recall.", "labels": [], "entities": [{"text": "translation direction detection", "start_pos": 226, "end_pos": 257, "type": "TASK", "confidence": 0.9124218225479126}, {"text": "detecting translation direction", "start_pos": 402, "end_pos": 433, "type": "TASK", "confidence": 0.8136864105860392}, {"text": "precision", "start_pos": 451, "end_pos": 460, "type": "METRIC", "confidence": 0.9969215989112854}, {"text": "recall", "start_pos": 471, "end_pos": 477, "type": "METRIC", "confidence": 0.9937924742698669}]}], "introductionContent": [{"text": "Translated text differs from authored text.", "labels": [], "entities": []}, {"text": "The main differences are simplification, explicitation, normalization and interference.", "labels": [], "entities": [{"text": "explicitation", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.9561923146247864}]}, {"text": "Statistical classifiers have been trained to detect Translationese . state two motivations for automatic detection of Translationese: empirical validation of Translationese linguistic theories and improving statistical machine translation ().", "labels": [], "entities": [{"text": "automatic detection of Translationese", "start_pos": 95, "end_pos": 132, "type": "TASK", "confidence": 0.6672024428844452}, {"text": "statistical machine translation", "start_pos": 207, "end_pos": 238, "type": "TASK", "confidence": 0.6212147076924642}]}, {"text": "Most of the prior work focus on in-domain Translationese detection (.", "labels": [], "entities": [{"text": "Translationese detection", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.957662045955658}]}, {"text": "That is, the training and test set come from the same, usually narrow, domain.", "labels": [], "entities": []}, {"text": "Cross-domain Translationese detection serves the two stated motivations better than in-domain detection.", "labels": [], "entities": [{"text": "Translationese detection", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.8343966901302338}]}, {"text": "First, automatic classification validates linguistic theories only if it works independent of the domain.", "labels": [], "entities": [{"text": "automatic classification", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.718439519405365}]}, {"text": "Otherwise, the classifier could perform well by memorizing lexical terms unique to a specific domain without using any linguistically meaningful generalizations.", "labels": [], "entities": []}, {"text": "Second, a Translationese classifier can improve statistical machine translation in two ways: 1) By labeling the parallel training data with translation direction 2 ; 2) By labeling input sentences to a decoder at translation time and use matching models.", "labels": [], "entities": [{"text": "Translationese classifier", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.896666556596756}, {"text": "statistical machine translation", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.6570688684781393}]}, {"text": "The accuracy of the classifier is the main factor determining its impact on statistical machine translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9992446899414062}, {"text": "statistical machine translation", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.6926590402921041}]}, {"text": "Most parallel or monolingual training data sources do not contain translation direction meta-data.", "labels": [], "entities": []}, {"text": "Also, the input sentences at translation time can be from any domain.", "labels": [], "entities": []}, {"text": "Therefore, a cross-domain setting for translation direction detection is more appropriate for improving statistical machine translation as well.", "labels": [], "entities": [{"text": "translation direction detection", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.973185122013092}, {"text": "statistical machine translation", "start_pos": 104, "end_pos": 135, "type": "TASK", "confidence": 0.6861232121785482}]}, {"text": "We develop a crossdomain training and test data set and compare some of the linguistically motivated features from prior work () in this setting.", "labels": [], "entities": []}, {"text": "In addition, we introduce anew bilingual feature that outperforms all prior work in both in-domain and cross-domain settings.", "labels": [], "entities": []}, {"text": "Our work also differs from many prior works by focusing on sentence level, rather than block level classification.", "labels": [], "entities": [{"text": "block level classification", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.7558097044626871}]}, {"text": "Although compare sentence level versus block level detection accuracy, most other research focuses on block level detection (.", "labels": [], "entities": [{"text": "block level detection", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.6670084297657013}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.8712415099143982}, {"text": "block level detection", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.6454562544822693}]}, {"text": "Sentence level classification serves the stated motivations above better than block level classification.", "labels": [], "entities": [{"text": "Sentence level classification", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9169873595237732}, {"text": "block level classification", "start_pos": 78, "end_pos": 104, "type": "TASK", "confidence": 0.6636344393094381}]}, {"text": "For empirical validation of linguistic theories, features that are detectable at the sentence level are more linguistically meaningful than block level statistics.", "labels": [], "entities": []}, {"text": "Sentence level detection is also more appropriate for labeling decoder input as well as some statistical machine translation training data.", "labels": [], "entities": [{"text": "Sentence level detection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9138651688893636}, {"text": "labeling decoder input", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.8824041485786438}, {"text": "statistical machine translation training", "start_pos": 93, "end_pos": 133, "type": "TASK", "confidence": 0.7021840363740921}]}, {"text": "In the rest of the paper, we first review prior work on sentence level and cross-domain translation direction detection.", "labels": [], "entities": [{"text": "cross-domain translation direction detection", "start_pos": 75, "end_pos": 119, "type": "TASK", "confidence": 0.871518075466156}]}, {"text": "In Section 3 we motivate the selection of features used in this study.", "labels": [], "entities": []}, {"text": "Next, we describe our cross-domain data set and the classification algorithm we use to build and evaluate models given a set of features.", "labels": [], "entities": []}, {"text": "Experimental results are presented in Section 5.2.", "labels": [], "entities": []}], "datasetContent": [{"text": "We chose the Vowpal Wabbit () (VW) online linear classifier since it is fast, scalable and it has special (bag of words and n-gram generation) options for text classification.", "labels": [], "entities": [{"text": "Vowpal Wabbit () (VW) online linear classifier", "start_pos": 13, "end_pos": 59, "type": "DATASET", "confidence": 0.8166638016700745}, {"text": "text classification", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.7586460411548615}]}, {"text": "We found that VW was comparable inaccuracy to a batch logistic regression classifier.", "labels": [], "entities": [{"text": "VW", "start_pos": 14, "end_pos": 16, "type": "DATASET", "confidence": 0.6305115222930908}]}, {"text": "For training and testing the classifier, we created balanced datasets with the same number of training examples in both directions.", "labels": [], "entities": []}, {"text": "This was achieved by randomly removing sentence pairs from the English to French direction until it matches the French to English direction.", "labels": [], "entities": []}, {"text": "For example, 636k sentence pairs are randomly chosen from the 2,930k sentence pairs in English to French Hansard-Committees corpus to match the number of examples in the French to English direction.", "labels": [], "entities": [{"text": "French Hansard-Committees corpus", "start_pos": 98, "end_pos": 130, "type": "DATASET", "confidence": 0.73048335313797}]}, {"text": "We are interested in comparing the performance of various feature sets in translation direction detection.", "labels": [], "entities": [{"text": "translation direction detection", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.9524282018343607}]}, {"text": "Performance evaluation of different classification features objectively is challenging in the absence of a downstream task.", "labels": [], "entities": []}, {"text": "Specifically, depending on the preferred balance between precision and recall, different features can be superior.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9991957545280457}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9980744123458862}]}, {"text": "Ideally an ROC graph) visualizes the tradeoff between precision and recall and can serve as an objective comparison between different classification feature sets.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9990283250808716}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.996044397354126}]}, {"text": "However, it is not practical to present ROC graphs for 360 experiments.", "labels": [], "entities": []}, {"text": "Hence, we resort to the Area Under the ROC graph (AUC) measure as a good measure to provide an objective comparison.", "labels": [], "entities": [{"text": "ROC graph (AUC) measure", "start_pos": 39, "end_pos": 62, "type": "DATASET", "confidence": 0.5329520603020986}]}, {"text": "Theoretically, the area under the curve can be interpreted as the probability that the classifier scores a random negative example higher than a random positive example).", "labels": [], "entities": []}, {"text": "As a point of reference, we also provide F-scores for experimental settings that are comparable to the prior work reviewed in Section 2.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9947947859764099}]}], "tableCaptions": []}