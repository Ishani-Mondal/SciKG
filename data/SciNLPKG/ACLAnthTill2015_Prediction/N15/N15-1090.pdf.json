{"title": [{"text": "Optimizing Multivariate Performance Measures for Learning Relation Extraction Models", "labels": [], "entities": [{"text": "Optimizing Multivariate Performance Measures", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.5698844119906425}, {"text": "Learning Relation Extraction", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.7701926032702128}]}], "abstractContent": [{"text": "We describe a novel max-margin learning approach to optimize non-linear performance measures for distantly-supervised relation extraction models.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7166222631931305}]}, {"text": "Our approach can be generally used to learn latent variable models under multivariate non-linear performance measures, such as F \u03b2-score.", "labels": [], "entities": []}, {"text": "Our approach interleaves Concave-Convex Procedure (CCCP) for populating latent variables with dual decomposition to factorize the original hard problem into smaller independent sub-problems.", "labels": [], "entities": []}, {"text": "The experimental results demonstrate that our learning algorithm is more effective than the ones commonly used in the literature for distant supervision of information extraction models.", "labels": [], "entities": [{"text": "distant supervision of information extraction", "start_pos": 133, "end_pos": 178, "type": "TASK", "confidence": 0.5832391321659088}]}, {"text": "On several data conditions, we show that our method outper-forms the baseline and results in up to 8.5% improvement in the F 1-score.", "labels": [], "entities": [{"text": "F 1-score", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9856675565242767}]}], "introductionContent": [{"text": "Rich models with latent variables are popular for many problems in natural language processing.", "labels": [], "entities": []}, {"text": "In information extraction, for example, one needs to predict the relation labels y that an entity-pair x can have based on the hidden relation mentions h, i.e., the relation labels for occurrences of the entity-pair in a given corpus.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.8356543183326721}]}, {"text": "However, these models are often trained by optimizing performance measures (such as conditional log-likelihood or error rate) that are not directly related to the task-specific non-linear performance measure, e.g., the F 1 -score.", "labels": [], "entities": [{"text": "error rate", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.9119399785995483}, {"text": "F 1 -score", "start_pos": 219, "end_pos": 229, "type": "METRIC", "confidence": 0.9607351720333099}]}, {"text": "However, better models maybe trained by optimizing the taskspecific performance measure while allowing latent variables to adapt their values accordingly.", "labels": [], "entities": []}, {"text": "We present a large-margin method to learn parameters of latent variable models fora wide range of non-linear multivariate performance measures such as F \u03b2 . Our method can be applied to learning graphical models that incorporate interdependencies among the output variables either directly, or indirectly through hidden variables.", "labels": [], "entities": []}, {"text": "Large-margin methods have been shown to be a compelling approach to learn rich models detailing the inter-dependencies among the output variables, via optimizing loss functions decomposable over the training instances) or non-decompasable loss functions ().", "labels": [], "entities": []}, {"text": "They have also been shown to be powerful when applied to latent variable models when optimizing for decomposable loss functions (.", "labels": [], "entities": []}, {"text": "Our large-margin method learns latent variable models via optimizing non-decomposable loss functions.", "labels": [], "entities": []}, {"text": "It interleaves the Concave-Convex Procedure (CCCP)) for populating latent variables with dual decomposition.", "labels": [], "entities": []}, {"text": "The latter factorizes the hard optimization problem (encountered in learning) into smaller independent sub-problems over the training instances.", "labels": [], "entities": []}, {"text": "We then present linear programming and local search methods for effective optimization of the sub-problems encountered in the dual decomposition.", "labels": [], "entities": []}, {"text": "Our local search algorithm leads to a speedup of 7,000 times compared to the exhaustive search used in the literature).", "labels": [], "entities": [{"text": "speedup", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9719954133033752}]}, {"text": "Our work is the first to make use of max-margin training in distant supervision of relation extraction models.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7489036619663239}]}, {"text": "We demonstrate the effectiveness of our proposed method compared to two strong baseline systems which optimize for the error rate and conditional likelihood, including a state-of-the-art system by.", "labels": [], "entities": [{"text": "error rate", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.9514362812042236}]}, {"text": "On several data conditions, we show that our method outperforms the baseline and results in up to 8.5% improvement in the F 1 -score.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 122, "end_pos": 132, "type": "METRIC", "confidence": 0.9895309954881668}]}], "datasetContent": [{"text": "Dataset: We use the challenging benchmark dataset created by for distant supervision of relation extraction models.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7364141792058945}]}, {"text": "It is created by aligning relations from Freebase 4 with the sentences in New York Times corpus.", "labels": [], "entities": [{"text": "New York Times corpus", "start_pos": 74, "end_pos": 95, "type": "DATASET", "confidence": 0.7399360537528992}]}, {"text": "The labels for the datapoints come from the Freebase database but Freebase is incomplete (.", "labels": [], "entities": [{"text": "Freebase database", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.9711617529392242}, {"text": "Freebase", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9582987427711487}]}, {"text": "So a data point is labeled nil when either no relation exists or the relation is absent in Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.9609699249267578}]}, {"text": "To avoid this ambiguity we train and evaluate the baseline and our algorithms on a subset of this dataset which consists of only non-nil relation labeled datapoints (termed as positive dataset).", "labels": [], "entities": []}, {"text": "For the sake of completeness, we do report the accuracies of the various approaches on the entire evaluation dataset.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9710978269577026}]}, {"text": "Systems and Baseline: Hoffmann et al. describe a state-of-the-art approach for this task.", "labels": [], "entities": []}, {"text": "They use a perceptron-style parameter update scheme adapted to handle latent variables; their training objective is the conditional likelihood.", "labels": [], "entities": []}, {"text": "Out of the two implementations of this algorithm, we use the better 5 of these two 6 , as our baseline (denoted by Hoffmann).", "labels": [], "entities": []}, {"text": "For a fair comparison, the training dataset and the set of features defined over it are common to all the experiments.", "labels": [], "entities": []}, {"text": "We discuss the results of two of our approaches.", "labels": [], "entities": []}, {"text": "One, is the LatentSVM max-margin formulation with the simple decomposable Hamming loss function which minimizes the error rate (denoted by MM-hamming).", "labels": [], "entities": [{"text": "error rate", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9734459817409515}]}, {"text": "The other is the LatentSVM maxmargin formulation with the non-decomposable loss function which minimizes the negative of F \u03b2 score (denoted by MM-F-loss) . Evaluation Measure: The performance measure is F \u03b2 which can be expressed in terms of false positives (FP) and false negatives (FN) as: where \u03b2 is the weight assigned to precision (and 1 \u2212 \u03b2 to recall).", "labels": [], "entities": [{"text": "F \u03b2 score", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.955525815486908}, {"text": "F \u03b2", "start_pos": 203, "end_pos": 206, "type": "METRIC", "confidence": 0.9538129568099976}, {"text": "false positives (FP) and false negatives (FN)", "start_pos": 242, "end_pos": 287, "type": "METRIC", "confidence": 0.7365474348718469}, {"text": "precision", "start_pos": 326, "end_pos": 335, "type": "METRIC", "confidence": 0.9954894185066223}, {"text": "recall", "start_pos": 350, "end_pos": 356, "type": "METRIC", "confidence": 0.9976429343223572}]}, {"text": "F P , F N and Np are defined as : It is not quite clear why the performance of the two implementations are different.", "labels": [], "entities": []}, {"text": "6 nlp.stanford.edu/software/mimlre.shtml We use a combination of F1 loss and hamming loss, as using only F1-loss overfits the training dataset, as observed from the experiments.", "labels": [], "entities": [{"text": "F1 loss", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9880204498767853}]}, {"text": "We use 1\u2212F \u03b2 as the expression for the multivariateloss.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average results on 10% Riedel datasets.", "labels": [], "entities": [{"text": "Riedel datasets", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.8251902461051941}]}, {"text": " Table 3: Overal results on the positive dataset.", "labels": [], "entities": [{"text": "Overal", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9683178067207336}]}, {"text": " Table 4: Increasing weight on Precision in F \u03b2 .", "labels": [], "entities": [{"text": "weight", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.6395914554595947}, {"text": "Precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.991701066493988}, {"text": "F \u03b2", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.8033885955810547}]}, {"text": " Table 5. Max- margin based approaches generally perform well  when trained only on the positive dataset compared  to Hoffmann. However, our F 1 -scores are \u223c8%  less when we train on the entire dataset consisting  of both nil and non-nil datapoints.", "labels": [], "entities": [{"text": "Max- margin", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8485268950462341}, {"text": "F 1 -scores", "start_pos": 141, "end_pos": 152, "type": "METRIC", "confidence": 0.9792377650737762}]}, {"text": " Table 5: F 1 -scores on the entire test set.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9622283279895782}]}]}