{"title": [{"text": "Inferring Temporally-Anchored Spatial Knowledge from Semantic Roles", "labels": [], "entities": [{"text": "Inferring Temporally-Anchored Spatial Knowledge from Semantic Roles", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.8175754206521171}]}], "abstractContent": [{"text": "This paper presents a framework to infer spatial knowledge from verbal semantic role representations.", "labels": [], "entities": []}, {"text": "First, we generate potential spatial knowledge deterministically.", "labels": [], "entities": []}, {"text": "Second, we determine whether it can be inferred and a degree of certainty.", "labels": [], "entities": [{"text": "certainty", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9983837604522705}]}, {"text": "Inferences capture that something is located or is not located somewhere , and temporally anchor this information.", "labels": [], "entities": []}, {"text": "An annotation effort shows that inferences are ubiquitous and intuitive to humans.", "labels": [], "entities": []}], "introductionContent": [{"text": "Extracting semantic relations from text is at the core of text understanding.", "labels": [], "entities": [{"text": "Extracting semantic relations from text", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.9010496258735656}, {"text": "text understanding", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7635931074619293}]}, {"text": "Semantic relations encode semantic connections between words.", "labels": [], "entities": []}, {"text": "For example, from (1) Bill couldn't handle the pressure and quit yesterday, one could extract that the CAUSE of quit was the pressure.", "labels": [], "entities": [{"text": "CAUSE", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9987010955810547}]}, {"text": "Doing so would help answering question Why did Bill quit? and determining that the pressure started before Bill quit.", "labels": [], "entities": [{"text": "answering question Why did Bill quit?", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.6823174357414246}]}, {"text": "In the past years, computational semantics has received a significant boost.", "labels": [], "entities": [{"text": "computational semantics", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.7380916476249695}]}, {"text": "But extracting all semantic relations in text-even in single sentences-is still an elusive goal.", "labels": [], "entities": [{"text": "extracting all semantic relations in text-even in single sentences-is", "start_pos": 4, "end_pos": 73, "type": "TASK", "confidence": 0.8563451104693942}]}, {"text": "Most existing approaches target either a single relation, e.g., PART-WHOLE (), or relations that hold between arguments following some syntactic construction, e.g., possessives ().", "labels": [], "entities": [{"text": "PART-WHOLE", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.6668280363082886}]}, {"text": "Among the latter kind, the task of verbal semantic role labeling focuses on extracting semantic links exclusively between verbs and their arguments.) is a popular corpus for this task, and tools to extract verbal semantic roles have been proposed for years).", "labels": [], "entities": [{"text": "verbal semantic role labeling", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6170042082667351}]}, {"text": "Some semantic relations hold forever, e.g., the CAUSE of event quit in example (1) above is pressure.", "labels": [], "entities": [{"text": "CAUSE", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.9888356328010559}]}, {"text": "Discussing when this CAUSE holds is somewhat artificial: at some point Bill quit, and he did so because of the pressure.", "labels": [], "entities": [{"text": "CAUSE", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.6436732411384583}]}, {"text": "But LOCATION and other semantic relations often do not hold forever.", "labels": [], "entities": [{"text": "LOCATION", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9036049246788025}]}, {"text": "For example, while buildings typically have one location during their existence, people and objects such as cars and books do not: they participate in events and as a result their locations change.", "labels": [], "entities": []}, {"text": "This paper presents a framework to infer temporally-anchored spatial knowledge from verbal semantic roles.", "labels": [], "entities": []}, {"text": "Specifically, our goal is to infer whether something is located somewhere or not located somewhere, and temporally anchor this spatial information.", "labels": [], "entities": []}, {"text": "Consider sentence (2) John was incarcerated at Shawshank prison and its semantic roles (, solid arrows).", "labels": [], "entities": [{"text": "Shawshank prison", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.9635024666786194}]}, {"text": "Given these roles, we aim at inferring that John had LOCATION Shawshank prison during event incarcerated, and that he (probably) did not have this LOCATION before and after (discontinuous arrow).", "labels": [], "entities": [{"text": "LOCATION", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9511339068412781}, {"text": "Shawshank prison", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.8909265100955963}, {"text": "LOCATION", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9402674436569214}]}, {"text": "Our intuition is that knowing that incarcerated has THEME John and LO-CATION Shawshank prison will help making these inferences.", "labels": [], "entities": [{"text": "THEME", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9875664710998535}, {"text": "LO-CATION", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9955739974975586}, {"text": "Shawshank prison", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.9193576574325562}]}, {"text": "As we shall discuss, sometimes we have evidence that something is (or is not) located somewhere, but cannot completely commit.", "labels": [], "entities": []}, {"text": "We target temporally-anchored spatial knowledge between intra-sentential arguments of verbs, not only between arguments of the same verb as exemplified in.", "labels": [], "entities": []}, {"text": "The main contributions are: (1) analysis of spatial knowledge inferable from PropBank-style semantic roles; (2) annotations of temporally-anchored LOCATION relations on top of OntoNotes; 1 (3) supervised models to infer the additional spatial knowledge; and (4) experiments detailing results using lexical, syntactic and semantic features.", "labels": [], "entities": []}, {"text": "The framework presented here infers over 44% spatial knowledge on top of the PropBank-style semantic roles annotated in OntoNotes (certYES and certNO labels, Section 3.3).", "labels": [], "entities": []}], "datasetContent": [{"text": "Results obtained with the test set using two baselines and models trained with several feature combinations are presented in.", "labels": [], "entities": []}, {"text": "The most frequent baseline always predicts certYES, and the most frequent per temporal anchor baseline predicts probNO, certYES and probYES for instances with temporal anchor before, during and after respectively.", "labels": [], "entities": []}, {"text": "The most frequent baseline obtains a weighted F-measure of 0.20, and most frequent per temporal anchor baseline 0.50.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9945210218429565}]}, {"text": "Results with supervised models are better, but we note that always predicting certYES for during instances obtains the same F-measure than using all features (0.65).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9978431463241577}]}, {"text": "The bottom block of presents results using all features.", "labels": [], "entities": []}, {"text": "The weighted F-measure is 0.55, and the highest F-measures are obtained with labels certYES (0.71) and probYES (0.60).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9672343730926514}, {"text": "F-measures", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9727874398231506}]}, {"text": "Results with certNO and probNO are lower (0.05 and 0.44), we believe this is due to the fact that few instances are annotated with this labels (8.72% and 19.75%, Ta-ble 3).", "labels": [], "entities": []}, {"text": "Results are higher (0.65) with during instances than with before and after instances (0.41 and 0.45).", "labels": [], "entities": []}, {"text": "These results are intuitive: certain events such as press and write require participants to be located where the event occurs only during the event.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Annotation counts. Over 44% of potential spatial knowledge can be inferred (certYES and certNO).", "labels": [], "entities": []}, {"text": " Table 3. First, it is worth not- ing that annotators used UNK to answer only 5.26%  of questions. Thus, over 94% of times ARGM-LOC  semantic role is found, additional spatial knowledge  can be inferred with some degree of certainty. Sec- ond, annotators were certain about the additional  spatial knowledge, i.e., labels certYES and certNO,  35.94% and 8.72% of times respectively. Thus,  44% of times one encounters ARGM-LOC seman-", "labels": [], "entities": []}, {"text": " Table 6: Results obtained with two baselines, and training with several feature combinations. Models are  trained with all instances (before, during and after).", "labels": [], "entities": []}]}