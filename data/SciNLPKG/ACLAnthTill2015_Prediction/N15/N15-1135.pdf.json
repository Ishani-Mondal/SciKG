{"title": [{"text": "Mining for unambiguous instances to adapt part-of-speech taggers to new domains", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a simple, yet effective approach to adapt part-of-speech (POS) taggers to new domains.", "labels": [], "entities": [{"text": "part-of-speech (POS) taggers", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.6074235081672669}]}, {"text": "Our approach only requires a dictionary and large amounts of unlabeled target data.", "labels": [], "entities": []}, {"text": "The idea is to use the dictionary to mine the unlabeled target data for unambigu-ous word sequences, thus effectively collecting labeled target data.", "labels": [], "entities": []}, {"text": "We add the mined instances to available labeled newswire data to train a POS tagger for the target domain.", "labels": [], "entities": []}, {"text": "The induced models significantly improve tagging accuracy on held-out test sets across three domains (Twitter, spoken language, and search queries).", "labels": [], "entities": [{"text": "tagging", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9480130076408386}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9114120006561279}]}, {"text": "We also present results for Dutch, Spanish and Portuguese Twitter data, and provide two novel manually-annotated test sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Part-of-speech (POS) taggers are typically trained on newswire and exhibit severe out-of-domain performance drops).", "labels": [], "entities": [{"text": "Part-of-speech (POS) taggers", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5870357811450958}]}, {"text": "When faced with anew domain, one option is to try to leverage available unlabeled data.", "labels": [], "entities": []}, {"text": "However, rather than resorting to pure self-training approaches (self-labeling), we here resort to another source of information.", "labels": [], "entities": []}, {"text": "One way to address the annotation problem is to use collaboratively created resources such as Wikipedia for distant supervision, or the automatically derived dictionaries called Wiktionary ().", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 94, "end_pos": 103, "type": "DATASET", "confidence": 0.9439438581466675}]}, {"text": "We show how to leverage these resources to create labeled training data.", "labels": [], "entities": []}, {"text": "It turns out that many entries in Wiktionary are actually unambiguous, i.e., there is only one possible tag for the word.", "labels": [], "entities": []}, {"text": "In fact, for English Wiktionary (, we find that 93% of the unigram types are unambiguous (cf..", "labels": [], "entities": []}, {"text": "Our idea here is simple: we mine for unlabeled sentences that contain only unambiguous items (according to Wiktionary), and use the resulting data as additional, labeled training material.", "labels": [], "entities": []}, {"text": "Concretely, we mine unannotated corpora of tweets, transcribed speech, and search queries for sentences that contain only unambiguous tokens, and combine those instances with newswire data to train POS models that adapt better to the respective domains.", "labels": [], "entities": []}, {"text": "We show that adding unambiguous data leads to considerable improvements over both unadapted and weakly-supervised baselines (.", "labels": [], "entities": []}, {"text": "Since Wiktionary has relatively low coverage for some of these domains, we also explore the use of Brown clusters to extend the coverage.", "labels": [], "entities": []}, {"text": "This enables us to generalize across spelling variations and synonyms.", "labels": [], "entities": []}, {"text": "Additionally, we evaluate our approach on Dutch, Portuguese and Spanish Twitter and present tow novel data sets for the latter two languages.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Characteristics of data sets used in this paper", "labels": [], "entities": []}, {"text": " Table 3: Tagging accuracies. TOOLS are off-the-shelf taggers (Stanford and TreeTagger), LI10/LI10 + the weakly  supervised models with and without embeddings, and CRF the model trained on newswire with in-domain word  clusters. Last two columns show results when extending with unambiguous data.  *  : Unlabeled data too small to  generate clusters with cut-off 100.", "labels": [], "entities": []}]}