{"title": [{"text": "Question Answering System using Multiple Information Source and Open Type Answer Merge", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7789908945560455}]}], "abstractContent": [{"text": "This paper presents a multi-strategy and multi-source question answering (QA) system that can use multiple strategies to both answer natural language (NL) questions and respond to keywords.", "labels": [], "entities": [{"text": "multi-source question answering (QA)", "start_pos": 41, "end_pos": 77, "type": "TASK", "confidence": 0.7962874670823415}]}, {"text": "We use multiple information sources including curated knowledge base, raw text, auto-generated triples, and NL processing results.", "labels": [], "entities": []}, {"text": "We develop open semantic answer type detector for answer merging and improve previous developed single QA modules such as knowledge base based QA, information retrieval based QA.", "labels": [], "entities": [{"text": "answer merging", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.8631339073181152}, {"text": "information retrieval based QA", "start_pos": 147, "end_pos": 177, "type": "TASK", "confidence": 0.7072797790169716}]}], "introductionContent": [{"text": "Several massive knowledge bases such as DBpedia () and Freebase () have been released.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.9266659021377563}, {"text": "Freebase", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.9680522084236145}]}, {"text": "To utilize these resources, various approaches to question answering (QA) on linked data have been proposed.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.8866839170455932}]}, {"text": "QA on linked data or on a knowledge base (KB) can give very high precision, but because KBs consist of fragmentary knowledge with no contextual information and is powered by community effort, they cannot coverall information needs of users.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9983032941818237}]}, {"text": "Furthermore, QA systems achieve low precision when disambiguating question sentences in to KB concepts; this flaw reduces QAs' performance ().", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9968427419662476}]}, {"text": "A QA system can understand a natural language (NL) question and return the answer.", "labels": [], "entities": []}, {"text": "In some ways, perfection of QA systems is the final goal of information retrieval (IR).", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.8522864699363708}]}, {"text": "Early QA systems were IR-based QAs (IRQAs).", "labels": [], "entities": []}, {"text": "However, as large KBs such as DBpedia and Freebase have been constructed, KB-based QA (KBQA) has become increasingly important (.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9366722106933594}, {"text": "Freebase", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.8770543336868286}]}, {"text": "These two kinds of QA systems use heterogeneous data; IRQA systems search raw text, whereas KBQA systems search KB.", "labels": [], "entities": []}, {"text": "KBQA systems give accurate answers because they search from KBs curated by humans.", "labels": [], "entities": []}, {"text": "However, they cannot utilize any contextual information of the answers.", "labels": [], "entities": []}, {"text": "The answers of IRQA are relatively less accurate than those of KBQA, but IRQA systems utilize the contextual information of the answers.", "labels": [], "entities": []}, {"text": "We assert that a successful QA system will require appropriate cooperation between a KBQA and an IRQA.", "labels": [], "entities": [{"text": "QA", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9715386033058167}, {"text": "IRQA", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.7632775902748108}]}, {"text": "We propose a method to merge the KBQA and the IRQA systems and to exploit the information in KB ontology-based open semantic answer type to merge the answers from the two systems, unlike previous systems that use a predetermined answer type.", "labels": [], "entities": [{"text": "KBQA", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.9322680830955505}]}, {"text": "We improve our previous system . Also we can answer not only complete NL sentence questions, and questions composed of only keywords, which are frequently asked in real life.", "labels": [], "entities": []}, {"text": "We suggest strategies and methods) to integrate KBQA, IRQA, and keyword QA.", "labels": [], "entities": [{"text": "IRQA", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.6266316175460815}]}, {"text": "Because the KBs (i.e., the information sources), are highly structured, the KBQA system can produce very pin-pointed answer sets.", "labels": [], "entities": []}, {"text": "We combined two approaches to make this system possible.", "labels": [], "entities": []}, {"text": "The first approach is based on semantic parsing, and the second is based on lexico-semantic pattern matching ().", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7110375463962555}, {"text": "lexico-semantic pattern matching", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.6948865056037903}]}, {"text": "In the semantic parsing approach, the system first generates candidate segments of the question sentence and tries to match KB vocabularies to the segments by combining use of string-similarity based methods and an automatically generated dictionary that consists of pairs of NL phrase and KB predicate).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7215018719434738}]}, {"text": "Finally the system generates query candidates by applying the segments to a small set of hand-crafted grammar rules to generate a single formal meaning representation.", "labels": [], "entities": []}, {"text": "In the lexico-semantic pattern approach, we use simple patterns paired with a formal query template.", "labels": [], "entities": []}, {"text": "The patterns consist of regular expression pattern that describes lexical, part-of-speech (PoS), and chunk-type patterns of a question sentence ().", "labels": [], "entities": []}, {"text": "Then the templates paired with these patterns are equipped with methods to extract information from the sentence and to fill the information into the template.", "labels": [], "entities": []}, {"text": "KBQA can assess the answers even when it has little or no additional contextual information, whereas other systems like IRQA systems can rely on the context from which it is retrieved ().", "labels": [], "entities": [{"text": "KBQA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8075791597366333}]}, {"text": "Instead, type information and its hierarchy defined in the KB are good sources of contextual information that the KBQA can exploit.", "labels": [], "entities": []}, {"text": "However, not all the entities defined in the KB have specific type information; therefore, relying only on the type information can reduce precision (.", "labels": [], "entities": [{"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9988448619842529}]}, {"text": "When KBQA systems fail, it is usually due to incorrect disambiguation of entities, or to incorrect disambiguation of predicate.", "labels": [], "entities": []}, {"text": "Both types of failures result in production of answers of the wrong types.", "labels": [], "entities": []}, {"text": "For example, fora question sentence \"What sport does the Toronto Maple Leafs play?\" evoke answers about the arena in which the team plays, instead of the sport that the team plays, when the KBQA system fails in disambiguation.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}