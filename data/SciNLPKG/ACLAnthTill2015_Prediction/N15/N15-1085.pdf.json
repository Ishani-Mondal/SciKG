{"title": [{"text": "Semantic Grounding in Dialogue for Complex Problem Solving", "labels": [], "entities": [{"text": "Complex Problem Solving", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.778404970963796}]}], "abstractContent": [{"text": "Dialogue systems that support users in complex problem solving must interpret user utterances within the context of a dynamically changing, user-created problem solving artifact.", "labels": [], "entities": [{"text": "problem solving", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7826110422611237}]}, {"text": "This paper presents a novel approach to semantic grounding of noun phrases within tutorial dialogue for computer programming.", "labels": [], "entities": [{"text": "semantic grounding of noun phrases within tutorial dialogue", "start_pos": 40, "end_pos": 99, "type": "TASK", "confidence": 0.8281129784882069}]}, {"text": "Our approach performs joint segmentation and labeling of the noun phrases to link them to attributes of entities within the problem-solving environment.", "labels": [], "entities": []}, {"text": "Evaluation results on a corpus of tutorial dialogue for Java programming demonstrate that a Conditional Random Field model performs well, achieving an accuracy of 89.3% for linking semantic segments to the correct entity attributes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9991569519042969}]}, {"text": "This work is a step toward enabling dialogue systems to support users in increasingly complex problem-solving tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the dialogue systems research community, there is growing recognition that dialogue systems need to support users in increasingly complex tasks.", "labels": [], "entities": []}, {"text": "To move in this direction, dialogue systems must perform natural language understanding within richer and richer contexts, and this understanding includes semantic interpretation of user utterances).", "labels": [], "entities": []}, {"text": "Previous approaches for semantic interpretation include domain-specific grammars () and open-domain parsers together with a domain-specific lexicon).", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.8623086810112}]}, {"text": "However, existing techniques are not sufficient to support increasingly complex problem-solving dialogues due to several challenges.", "labels": [], "entities": []}, {"text": "For example, domainspecific grammars become intractable when applied to more ill-formed domains, and opendomain parsers may not perform well across domains (.", "labels": [], "entities": []}, {"text": "The call for addressing these limitations is particularly strong for dialogue systems that help people learn, such as tutorial dialogue systems.", "labels": [], "entities": []}, {"text": "Today's tutorial dialogue systems engage in natural language dialogue in support of tasks such as solving qualitative physics problems (), understanding computer architecture and physics), and predicting behavior of electrical circuits.", "labels": [], "entities": [{"text": "predicting behavior of electrical circuits", "start_pos": 193, "end_pos": 235, "type": "TASK", "confidence": 0.854053509235382}]}, {"text": "Although these systems differ in many ways, they have an important commonality: in order to semantically interpret user dialogue utterances, these systems ground the utterances in a fixed domain description that is an integral part of the engineered system.", "labels": [], "entities": []}, {"text": "This characteristic is shared by most dialogue systems, which ground their dialogue in manually defined domain-specific ontologies, such as for the task of booking flights), checking bus schedules, and finding restaurants (.", "labels": [], "entities": []}, {"text": "These task-oriented domains, though they present a rich set of research challenges, stand in stark contrast to a complex problem-solving domain in which the user is creating an artifact to solve a problem.", "labels": [], "entities": []}, {"text": "Yet the psychology literature tells us that complex problem solving is an essential activity inhuman learning (.", "labels": [], "entities": [{"text": "problem solving", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.7638136446475983}]}, {"text": "In such a domain, understanding user dialogue utterances involves grounding them within an infinite set of possible user-created artifacts, not within a system ontology.", "labels": [], "entities": [{"text": "understanding user dialogue utterances", "start_pos": 18, "end_pos": 56, "type": "TASK", "confidence": 0.6971678882837296}]}, {"text": "This paper focuses on the complex problem-solving domain of introductory computer programming.", "labels": [], "entities": [{"text": "introductory computer programming", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.9033133188883463}]}, {"text": "In this domain the user might say, for example, \"Is myVariable supposed to bean int?\" where myVariable refers to the name of a variable within the computer program that the user has created.", "labels": [], "entities": []}, {"text": "The semantic interpretation task in this case is akin to situated dialogue where user utterances must be grounded within a physical environment (.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7220363169908524}]}, {"text": "However, even these situated dialogue models typically rely on a world defined by a limited number of entities (e.g., a chair or a cup).", "labels": [], "entities": []}, {"text": "To address these challenges, this paper presents a step toward semantic grounding for complex problem-solving dialogues, in which the number of potential entities (e.g., a Java variable or apiece of code) is infinite.", "labels": [], "entities": []}, {"text": "The present work focuses on the semantic understanding of noun phrases, which tend to bear significant semantic information for each utterance.", "labels": [], "entities": [{"text": "semantic understanding of noun phrases", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.8430614948272706}]}, {"text": "Although noun phrases are typically small in their number of tokens, their complexity and semantics vary in important ways.", "labels": [], "entities": []}, {"text": "For example, in the domain of computer programming, two similar noun phrases such as \"the 2 dimensional array\" and \"the 3 dimensional array\" refer to two different entities within the problem-solving artifact.", "labels": [], "entities": []}, {"text": "Inferring the semantic structure of the noun phrases is necessary to differentiate these two references within a dialogue, to ground them in the task, and to respond to them appropriately.", "labels": [], "entities": []}, {"text": "This noun phrase grounding task is similar to coreference resolution, which discovers the relationship between pairs of noun phrases in apiece of natural language text.", "labels": [], "entities": [{"text": "noun phrase grounding", "start_pos": 5, "end_pos": 26, "type": "TASK", "confidence": 0.6392226119836172}, {"text": "coreference resolution", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.9418970942497253}]}, {"text": "However, different from coreference resolution, noun phrase grounding links natural language expressions to entities in areal world environment.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.9481653869152069}, {"text": "noun phrase grounding", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.6340590318044027}]}, {"text": "The current approach leverages the structure of noun phrases, mapping their segments to attributes of entities to which they should be semantically linked.", "labels": [], "entities": []}, {"text": "In order to overcome the limitation of needing to fully enumerate the entities in the environment, we represent the entities as automatically extracted vectors of attributes.", "labels": [], "entities": []}, {"text": "We then perform joint segmentation and labeling of the noun phrases in user utterances to map them to the entity vectors (used to describe entities within the environment).", "labels": [], "entities": []}, {"text": "This mapping of noun phrases to realworld attributes is the grounding task focused on in this work.", "labels": [], "entities": []}, {"text": "The results show that a Conditional Random Field performs well for this task, achieving 89.3% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9952605366706848}]}, {"text": "Moreover, even in the absence of lexical features (using only dependency parse features and parts of speech), the model achieves 71.3% accuracy, indicating that it maybe tolerant to unseen words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9993929862976074}]}, {"text": "The flexibility of this approach is due in part to the fact that it does not rely on a syntactic parser's ability to accurately segment within noun phrases, but rather includes parse features as just one type of feature among several made available to the model.", "labels": [], "entities": []}, {"text": "Finally, in contrast to methods based on bag-of-words such as latent semantic analysis, the proposed approach models the structure of noun phrases to facilitate specific grounding within an artifact.", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.6525691548983256}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents related work on semantic interpretation and on natural language interpretation for tutorial dialogue.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7440959215164185}, {"text": "natural language interpretation", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.6853452324867249}]}, {"text": "Section 3 describes the corpus and highlights some of the characteristics of dialogue for complex problem solving.", "labels": [], "entities": [{"text": "problem solving", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.7745306491851807}]}, {"text": "The semantic interpretation approach is introduced in Section 4, with the experiments and results presented in Section 5.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8277222216129303}]}, {"text": "Section 6 concludes with important directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of the experiments is to determine how well the trained CRF can segment noun phrases and link these segments to the correct attribute of entities in the world.", "labels": [], "entities": []}, {"text": "This section presents the experiments using CRFs trained and tested on the Java programming tutorial dialogue corpus.", "labels": [], "entities": [{"text": "Java programming tutorial dialogue corpus", "start_pos": 75, "end_pos": 116, "type": "DATASET", "confidence": 0.5847213625907898}]}, {"text": "As described below, the results were evaluated by comparing with manually labeled data.", "labels": [], "entities": []}, {"text": "Noun phrases from the tutorial dialogues were first manually extracted and annotated as to their slots in the description vector described in Section 4.2.", "labels": [], "entities": []}, {"text": "There were 364 grounded noun phrases extracted manually from the six tutorial dialogue sessions used in the current work.", "labels": [], "entities": []}, {"text": "Each of these noun phrases extracted has one or multiple corresponding entities in the programming artifact.", "labels": [], "entities": []}, {"text": "Since each word in a noun phrase is linked to an element in the description vector, the indices in this vector were used as the label for each word.", "labels": [], "entities": []}, {"text": "Annotation of all 346 noun phrases was performed by one annotator, and 20% of the noun phrases (70 noun phrases) were doubly annotated by an independent second annotator.", "labels": [], "entities": []}, {"text": "The percent agreement was 85.3% and the Kappa was 0.765.", "labels": [], "entities": [{"text": "agreement", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.933182418346405}, {"text": "Kappa", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.9078887701034546}]}, {"text": "To extract features, the lemmatization and syntactic parsing were performed with the Stanford CoreNLP toolkit ().", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7179401814937592}, {"text": "Stanford CoreNLP toolkit", "start_pos": 85, "end_pos": 109, "type": "DATASET", "confidence": 0.9294490218162537}]}, {"text": "Then, a CRF was trained to predict the label for each word in anew noun phrase.", "labels": [], "entities": []}, {"text": "The training was performed with the crfChain toolbox.", "labels": [], "entities": [{"text": "crfChain toolbox", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.952465832233429}]}, {"text": "We use ten-fold cross-validation to evaluate the performance of the CRF in this problem.", "labels": [], "entities": []}, {"text": "Results with different feature combinations are shown in.", "labels": [], "entities": []}, {"text": "Manually labeled data were taken as ground truth for computing accuracy, which is defined as the percentage of segments correctly labeled.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9966530203819275}]}, {"text": "Recall that consecutive words with the same label in a noun phrase are treated as a segment.", "labels": [], "entities": []}, {"text": "Therefore, if a segment s CRF identified by the CRF has the same boundary and the same label as a segment s Human in the noun phrase containing s CRF , this segment s CRF will be counted as a correct segment.", "labels": [], "entities": []}, {"text": "Otherwise, s CRF will be counted as incorrect.", "labels": [], "entities": [{"text": "CRF", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.8246492743492126}, {"text": "incorrect", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9714434146881104}]}, {"text": "The accuracy is then calculated as the number of correct segments identified by the CRF divided by the number of segments annotated manually.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996190071105957}]}, {"text": "As can be seen in, all of the models perform substantially better than a minimal majority class baseline of 43%, which would result from taking each word as a segment and assigning it with the most frequent attribute label.", "labels": [], "entities": []}, {"text": "The results demonstrate important characteristics of the segmentation and labeling model.", "labels": [], "entities": [{"text": "segmentation and labeling", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.6382507185141245}]}, {"text": "First, unlike most previous semantic interpretation work, our semantic interpretation of noun phrases does not rely on accurate syntactic parse within noun phrases.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.7624403536319733}]}, {"text": "Rather, we use a dependency parse from an open-domain parser as only one of several types of features provided to the model.", "labels": [], "entities": []}, {"text": "These dependency features improved the model inmost feature combinations.", "labels": [], "entities": []}, {"text": "The feature combination of words, lemmas, and dependency parses achieved the best accuracy, which is 4.8% higher than the model that only used word features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9996028542518616}]}, {"text": "This difference is statistically significant  Notably, the combination of part-of-speech features and dependency parse features still performed at 71.3% accuracy, indicating that to some extent, the method maybe tolerant to unseen words.", "labels": [], "entities": [{"text": "dependency parse", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.6340504139661789}, {"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9993993043899536}]}], "tableCaptions": []}