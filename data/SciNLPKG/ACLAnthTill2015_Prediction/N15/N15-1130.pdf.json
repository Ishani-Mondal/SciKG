{"title": [{"text": "Automatic cognate identification with gap-weighted string subsequences", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we describe the problem of cog-nate identification in NLP.", "labels": [], "entities": [{"text": "cog-nate identification", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.7745504379272461}]}, {"text": "We introduce the idea of gap-weighted subsequences for discriminating cognates from non-cognates.", "labels": [], "entities": []}, {"text": "We also propose a scheme to integrate phonetic features into the feature vectors for cognate identification.", "labels": [], "entities": [{"text": "cognate identification", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.7943035960197449}]}, {"text": "We show that subsequence based features perform better than state-of-the-art classifier for the purpose of cognate identification.", "labels": [], "entities": [{"text": "cognate identification", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.7462799847126007}]}, {"text": "The contribution of this paper is the use of subsequence features for cognate identification.", "labels": [], "entities": [{"text": "cognate identification", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.7288079559803009}]}], "introductionContent": [{"text": "Cognates are words across languages whose origin can be traced back to a common ancestral language.", "labels": [], "entities": []}, {"text": "For example, English \u223c German night \u223c Nacht 'night' and English hound \u223c German Hund 'dog' are cognates whose origin can be traced back to Proto-Germanic.", "labels": [], "entities": []}, {"text": "Sometimes, cognates are not revealingly similar but have changed substantially overtime such that they do not share form similarity.", "labels": [], "entities": []}, {"text": "An example of such a cognate pair is the English wheel and Sanskrit chakra 'wheel', which can be traced back to Proto-Indo-European (PIE) * kw ek w elo.", "labels": [], "entities": []}, {"text": "Automatic cognate identification, in NLP, refers to the application of string similarity or phonetic similarity algorithms either independently, or in tandem with machine learning algorithms for determining if a given word pair is cognate or not).", "labels": [], "entities": [{"text": "Automatic cognate identification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6462119619051615}]}, {"text": "In NLP, even borrowed words (loanwords) are referred to as cognates.", "labels": [], "entities": []}, {"text": "In contrast, historical linguistics makes a stark distinction between loanwords and cognates.", "labels": [], "entities": []}, {"text": "For example, English beef is a loanword from Norman French.", "labels": [], "entities": []}, {"text": "In this paper, we use cognates to refer to those words whose origin can be traced back to a common ancestor.", "labels": [], "entities": []}, {"text": "We use string subsequence based features (motivated from string kernels) for automatic cognate identification.", "labels": [], "entities": []}, {"text": "We show that subsequencebased features outperform word similarity measures at the task of automatic cognate identification.", "labels": [], "entities": [{"text": "automatic cognate identification", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.6890009840329488}]}, {"text": "We motivate the use of subsequence based features in terms of linguistic examples and then proceed to formulate the subsequence based features that can be derived from string kernels).", "labels": [], "entities": []}, {"text": "In information retrieval literature, string subsequences go under the name of skipgrams ().", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.781714916229248}]}], "datasetContent": [{"text": "In this section, we will present the dataset, HK features, and results of our experiments.", "labels": [], "entities": []}, {"text": "We used the publicly available 3 IndoEuropean dataset) for our experiments.", "labels": [], "entities": [{"text": "IndoEuropean dataset", "start_pos": 33, "end_pos": 53, "type": "DATASET", "confidence": 0.8329153060913086}]}, {"text": "The dataset has 16, 520 lexical items for 200 concepts and 84 language varieties.", "labels": [], "entities": []}, {"text": "Each word form is assigned to a unique CCN (Cognate Class Number).", "labels": [], "entities": []}, {"text": "There are more than 200 identical non-cognate pairs in the dataset.", "labels": [], "entities": []}, {"text": "For the first experiment, we extracted all word pairs fora concept and assigned a positive label if the word pair has an identical CCN; a negative label, if the word pair has different CCNs.", "labels": [], "entities": []}, {"text": "We extracted a total of 619, 635 word pairs out of which 145, 145 are cognates.", "labels": [], "entities": []}, {"text": "The dataset is transcribed in abroad romanized phonetic alphabet.", "labels": [], "entities": []}, {"text": "We explored if we could use two other word list databases: ASJP ( and for our experiments.", "labels": [], "entities": [{"text": "ASJP", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.8187434673309326}]}, {"text": "Although the ASJP database has word lists for more than half of the world's languages, it has cognacy judgments for few selected languages and is limited to 40 concepts.", "labels": [], "entities": [{"text": "ASJP database", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.7163700014352798}]}, {"text": "Moreover, the ASJP database does not have cognacy judgments for Indo-European family.", "labels": [], "entities": [{"text": "ASJP database", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.9091721177101135}]}, {"text": "The other dataset of has items for 24 Indo-European languages which are transcribed in an orthographic format and not in a uniform phonetic alphabet.", "labels": [], "entities": []}, {"text": "Moreover, there area number of missing items for some of the languages.", "labels": [], "entities": []}, {"text": "Hence, we did not use Ringe et al.'s dataset in our experiments.", "labels": [], "entities": [{"text": "Ringe et al.'s dataset", "start_pos": 22, "end_pos": 44, "type": "DATASET", "confidence": 0.7861710886160532}]}, {"text": "In contrast, Dyen's dataset is much larger and transcribed in an uniform format.", "labels": [], "entities": [{"text": "Dyen's dataset", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.9203311999638876}]}, {"text": "Now, we proceed to describe the previous best-performing system.", "labels": [], "entities": []}, {"text": "We compare the performance of subsequence features against the SVM classifier system trained on the following word-similarity features from: \u2022 Edit distance.", "labels": [], "entities": []}, {"text": "\u2022 Length of longest common prefix.", "labels": [], "entities": [{"text": "Length", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9981095790863037}]}, {"text": "\u2022 Number of common bigrams.", "labels": [], "entities": []}, {"text": "\u2022 Lengths of individual words.", "labels": [], "entities": []}, {"text": "\u2022 Absolute difference between the lengths of the words.", "labels": [], "entities": [{"text": "Absolute difference", "start_pos": 2, "end_pos": 21, "type": "METRIC", "confidence": 0.9026444554328918}]}, {"text": "As a first step, we perform a random ten-fold cross-validation of the dataset and report the accuracies for various values of \u03bb and p.", "labels": [], "entities": []}, {"text": "The results of this experiment are shown in.", "labels": [], "entities": []}, {"text": "The best results are obtained at \u03bb = 0.8, p = 3.", "labels": [], "entities": []}, {"text": "The accuracies increase with an increment in the value of \u03bb until 0.8 for all p > 1 (non-unigram models).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9819027185440063}]}, {"text": "This experiment is mainly designed to test the robustness of subsequence features against random splits in the dataset which turns out to be robust.", "labels": [], "entities": []}, {"text": "In this experiment, we split our dataset into two sets by concepts; and train on one set and test on the other.", "labels": [], "entities": []}, {"text": "To replicate our dataset, we performed an alphabetical sort of the concepts and split the concepts into training and testing datasets with a ratio of 3 : 1.", "labels": [], "entities": []}, {"text": "Now, we extract positive and negative examples from each subset of concepts; and train and test on each concepts' subset.", "labels": [], "entities": []}, {"text": "We also performed a 3-fold cross-validation on the training set to tune c (SVM hyperparameter).", "labels": [], "entities": []}, {"text": "We observed that the value of c did not effect the crossvalidation accuracy on the training dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9800911545753479}]}, {"text": "Hence we fixed cat 1.", "labels": [], "entities": []}, {"text": "We also experimented with radialbasis function kernel and polynomial kernels but did not find any improvement over the linear kernel classifier.", "labels": [], "entities": []}, {"text": "The composition of the training and test sets is given in table 1.", "labels": [], "entities": []}, {"text": "In this experiment, we report the F 1 -score, defined as 2P RP +R (Precision and Recall), for different values of \u03bb and p.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9809686243534088}, {"text": "2P RP +R (Precision and Recall)", "start_pos": 57, "end_pos": 88, "type": "METRIC", "confidence": 0.7802838948037889}]}, {"text": "The results of this experiment are shown in figure 2.", "labels": [], "entities": []}, {"text": "The F 1 -score of the system of HK is 0.46 whereas the best performing subsequence system (\u03bb = 0.7, p = 2) has a score of 0.5.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9858859181404114}, {"text": "HK", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.6563650369644165}]}, {"text": "Our system performs better than the system of HK in terms of cross-validation accuracy as well as F 1 -score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9864451289176941}, {"text": "F 1 -score", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9846800416707993}]}, {"text": "Overall, all non-unigram models perform better than the system of HK at cross-validation and concepts experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of positive and negative examples in  the training and test sets. The ratio of positive to negative  examples is 1 : 3.62.", "labels": [], "entities": []}]}