{"title": [{"text": "Subsentential Sentiment on a Shoestring: A Crosslingual Analysis of Compositional Classification", "labels": [], "entities": [{"text": "Subsentential Sentiment on a Shoestring", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8610957026481628}, {"text": "Crosslingual Analysis of Compositional Classification", "start_pos": 43, "end_pos": 96, "type": "TASK", "confidence": 0.6375107288360595}]}], "abstractContent": [{"text": "Sentiment analysis has undergone a shift from document-level analysis, where labels expresses the sentiment of a whole document or whole sentence, to subsentential approaches, which assess the contribution of individual phrases, in particular including the composition of sentiment terms and phrases such as negators and intensifiers.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9587630033493042}]}, {"text": "Starting from a small sentiment treebank mod-eled after the Stanford Sentiment Treebank of Socher et al.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 60, "end_pos": 87, "type": "DATASET", "confidence": 0.8678646882375082}]}, {"text": "(2013), we investigate suitable methods to perform compositional sentiment classification for German in a data-scarce setting , harnessing cross-lingual methods as well as existing general-domain lexical resources.", "labels": [], "entities": [{"text": "compositional sentiment classification", "start_pos": 51, "end_pos": 89, "type": "TASK", "confidence": 0.8931295474370321}]}], "introductionContent": [{"text": "In sentiment classification, we find a general tendency from document-level classification towards more fine-grained approaches that yield a more detailed appraisal of the judgement performed in the text -in particular, using composition over syntactic structure to get a more detailed approach over phrases.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.9428347945213318}, {"text": "document-level classification", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.7192524969577789}]}, {"text": "For English movie reviews, work using the Stanford Sentiment Treebank (SSTb) has shown that such subsentential sentiment information can yield approaches with both very high accuracy) and precise information about the role of each phrase -information which can subsequently used for extracting or summarizing the sentiment expressed in the text.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank (SSTb)", "start_pos": 42, "end_pos": 76, "type": "DATASET", "confidence": 0.8493753969669342}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9908060431480408}, {"text": "extracting or summarizing the sentiment expressed in the text", "start_pos": 283, "end_pos": 344, "type": "TASK", "confidence": 0.870356924004025}]}, {"text": "The effort for creating a sentiment treebank such as the SSTb, however, seems prohibitive if we wanted to create such a resource for each pair of relevant domain and language: Compared to document-level annotations for sentiment, which are easy to come by (e.g., star ratings), annotating individual syntactic phrases requires considerable effort.", "labels": [], "entities": []}, {"text": "The main focus of this paper is the question if and how it is possible to reach sensible performance for compositional sentiment classification when we only have limited resources to spend on an in-language, in-domain sentiment treebank.", "labels": [], "entities": [{"text": "compositional sentiment classification", "start_pos": 105, "end_pos": 143, "type": "TASK", "confidence": 0.8175627589225769}]}, {"text": "For this goal, we use anew resource, the Heidelberg Sentiment Treebank (HeiST), which is a German-language counterpart to the Stanford Sentiment Treebank in the sense that it makes explicit the composition of sentiment expression over syntactic phrases.", "labels": [], "entities": [{"text": "Heidelberg Sentiment Treebank (HeiST)", "start_pos": 41, "end_pos": 78, "type": "DATASET", "confidence": 0.9301275213559469}, {"text": "Stanford Sentiment Treebank", "start_pos": 126, "end_pos": 153, "type": "DATASET", "confidence": 0.6097323099772135}]}, {"text": "Our experiments on HeiST provide a direct comparison of different techniques for harnessing cross-lingual, cross-domain, or cross-task information, and are the first of this kind to specifically target compositional sentiment analysis.", "labels": [], "entities": [{"text": "compositional sentiment analysis", "start_pos": 202, "end_pos": 234, "type": "TASK", "confidence": 0.76505579551061}]}, {"text": "(next page) shows a schematic overview of the experiments: beyond supervised baseline experiments using SVM classification and a supervised RNTN model (section 3), we evaluated crosslingual projection (section 4), lexicon-based approaches (section 5), as well as semi-supervised approaches based on word clusters (section 6).", "labels": [], "entities": [{"text": "SVM classification", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.8303617537021637}, {"text": "crosslingual projection", "start_pos": 177, "end_pos": 200, "type": "TASK", "confidence": 0.7288111448287964}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Filtering out objective phrases", "labels": [], "entities": []}, {"text": " Table 2: HeiST baseline, cross-lingual projection, SVM.", "labels": [], "entities": [{"text": "cross-lingual projection", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.7521675527095795}]}, {"text": " Table 3: Comparison figures on subsets of the Stanford  Sentiment Treebank", "labels": [], "entities": [{"text": "Stanford  Sentiment Treebank", "start_pos": 47, "end_pos": 75, "type": "DATASET", "confidence": 0.8770950039227804}]}, {"text": " Table 4: Lexicon-based phrase labeling", "labels": [], "entities": [{"text": "Lexicon-based phrase labeling", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.5864582061767578}]}, {"text": " Table 5: Incorporating additional information", "labels": [], "entities": [{"text": "Incorporating", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9830304980278015}]}, {"text": " Table 6: Rule types in SSTb and HeiST", "labels": [], "entities": [{"text": "HeiST", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.7126353979110718}]}, {"text": " Table 7: Precision of rules with non-neutral parent label (ID: daughters and parent have identical labels)", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9640657305717468}]}]}