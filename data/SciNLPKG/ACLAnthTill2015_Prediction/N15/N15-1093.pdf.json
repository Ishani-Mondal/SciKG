{"title": [{"text": "Inflection Generation as Discriminative String Transduction", "labels": [], "entities": [{"text": "Inflection Generation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8262355923652649}, {"text": "Discriminative String Transduction", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.6299800674120585}]}], "abstractContent": [{"text": "We approach the task of morphological inflection generation as discriminative string trans-duction.", "labels": [], "entities": [{"text": "morphological inflection generation", "start_pos": 24, "end_pos": 59, "type": "TASK", "confidence": 0.655636062224706}]}, {"text": "Our supervised system learns to generate word-forms from lemmas accompanied by morphological tags, and refines them by referring to the other forms within a paradigm.", "labels": [], "entities": []}, {"text": "Results of experiments on six diverse languages with varying amounts of training data demonstrate that our approach improves the state of the art in terms of predicting inflected word-forms.", "labels": [], "entities": [{"text": "predicting inflected word-forms", "start_pos": 158, "end_pos": 189, "type": "TASK", "confidence": 0.8629341522852579}]}], "introductionContent": [{"text": "Word-forms that correspond to the same lemma can be viewed as paradigmatically related instantiations of the lemma.", "labels": [], "entities": []}, {"text": "For example, take, takes, taking, took, and taken are the word-forms of the lemma take.", "labels": [], "entities": []}, {"text": "Many languages have complex morphology with dozens of different word-forms for any given lemma: verbs inflect for tense, mood, and person; nouns can vary depending on their role in a sentence, and adjectives agree with the nouns that they modify.", "labels": [], "entities": []}, {"text": "For such languages, many forms will not be attested even in a large corpus.", "labels": [], "entities": []}, {"text": "However, different lemmas often exhibit the same inflectional patterns, called paradigms, which are based on phonological, semantic, or morphological criteria.", "labels": [], "entities": []}, {"text": "The paradigm of a given lemma can be identified and used to generate unseen forms.", "labels": [], "entities": []}, {"text": "Inflection prediction has the potential to improve Statistical Machine Translation (SMT) into morphologically complex languages.", "labels": [], "entities": [{"text": "Inflection prediction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8732829391956329}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 51, "end_pos": 88, "type": "TASK", "confidence": 0.8504791160424551}]}, {"text": "In order to address data sparsity in the training bitext, and reduce diverse inflected forms in the target language into the corresponding base forms, or lemmas.", "labels": [], "entities": []}, {"text": "At test time, they predict an abstract inflection tag for each translated lemma, which is then transformed into a proper word-form.", "labels": [], "entities": []}, {"text": "Unfortunately, hand-crafted morphological generators such as the ones that they use for this purpose are available only fora small number of languages, and are expensive to create from scratch.", "labels": [], "entities": []}, {"text": "The supervised inflection generation models that we investigate in this paper can instead be trained on publicly available inflection tables.", "labels": [], "entities": []}, {"text": "The task of an inflection generator is to produce an inflected form given a base-form (e.g., an infinitive) and desired inflection, which can be specified as an abstract inflectional tag.", "labels": [], "entities": []}, {"text": "The generator is trained on a number of inflection tables, such as the one in, which enumerate inflection forms fora given lemma.", "labels": [], "entities": []}, {"text": "At test time, the generator predicts inflections for previously unseen base-forms.", "labels": [], "entities": []}, {"text": "For example, given the input atmen + 1SIA, where the tag stands for \"first person singular indicative preterite,\" it should output atmete.", "labels": [], "entities": []}, {"text": "Recently, and have proposed to model inflection generation as a two-stage process: an input base-form is first matched with rules corresponding to a paradigm seen during training, which is then used to generate all inflections for that base-form simultaneously.", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.8321847915649414}]}, {"text": "Although their methods are quite different, both systems account for paradigm-wide regularities by creating rules that span all inflections within a paradigm.", "labels": [], "entities": []}, {"text": "We analyze both approaches in greater detail in Section 2.", "labels": [], "entities": []}, {"text": "In this paper, we approach the task of supervised inflection generation as discriminative string transduction, in which character-level operations are applied to transform a lemma concatenated with an inflection tag into the correct surface word-form.", "labels": [], "entities": [{"text": "supervised inflection generation", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.6859971284866333}]}, {"text": "We carefully model the transformations carried out fora single inflection, taking into account source characters surrounding a rule, rule sequence patterns, and the shape of the resulting inflected word.", "labels": [], "entities": []}, {"text": "To take advantage of paradigmatic regularities, we perform a subsequent reranking of the top n word-forms produced by the transducer.", "labels": [], "entities": []}, {"text": "In the reranking model, soft constraints capture similarities between different inflection slots within a table.", "labels": [], "entities": []}, {"text": "Where previous work leveraged large, rigid rules to span paradigms, our work is characterized by small, flexible rules that can be applied to any inflection, with features determining what rule sequence works best for each pairing of a base-form with an inflection.", "labels": [], "entities": []}, {"text": "Since our target application is machine translation, we focus on maximizing inflection form accuracy, rather than complete table accuracy.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7293434739112854}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.8192036747932434}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9190415740013123}]}, {"text": "Unlike previous work, which aims at learning linguisticallycorrect paradigms from crowd-sourced data, our approach is designed to be robust with respect to incomplete and noisy training data, which could be extracted from digital lexicons and annotated corpora.", "labels": [], "entities": []}, {"text": "We conduct a series of experiments which demonstrate that our method can accurately learn complex morphological rules in languages with varying levels of morphological complexity.", "labels": [], "entities": []}, {"text": "In each experiment we either match or improve over the state of the art reported in previous work.", "labels": [], "entities": []}, {"text": "In addition to providing a detailed comparison of the available inflection prediction systems, we also contribute four new inflection datasets composed of Dutch and French verbs, and Czech verbs and nouns, which are made available for future research.", "labels": [], "entities": [{"text": "inflection prediction", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.7297734916210175}]}], "datasetContent": [{"text": "We perform five experiments that differ with respect to the amount and completeness of training data, and whether the training is performed on individual word-forms or entire inflection tables.", "labels": [], "entities": []}, {"text": "We follow the experimental settings established by previous work, as much as possible.", "labels": [], "entities": []}, {"text": "The parameters of our transducer and aligner were established on a development set of German nouns and verbs, and kept fixed in all experiments.", "labels": [], "entities": []}, {"text": "We limit stem alignments to 2-2, affix alignments to 2-4, source context to 8 characters, joint n-grams to 5 characters, and target Markov features to 2 characters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of base forms and inflections for  each dataset.", "labels": [], "entities": []}, {"text": " Table 4: Individual form accuracy of models trained on  complete inflection tables.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9609110951423645}]}, {"text": " Table 5: Complete table accuracy of models trained on  complete inflection tables.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9941002726554871}]}, {"text": " Table 6: Prediction accuracy of models trained on ob- served forms.", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8602412343025208}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9547895193099976}]}, {"text": " Table 7: Prediction accuracy of models trained on ob- served Czech forms.", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.97264164686203}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9565622210502625}]}, {"text": " Table 8: Prediction accuracy on German verb forms after training on a small number of seed inflection tables.", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9323461055755615}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.894478976726532}]}]}