{"title": [{"text": "What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task.", "labels": [], "entities": []}, {"text": "In particular, we focus on the cooking domain, where the instructions correspond to the recipe.", "labels": [], "entities": []}, {"text": "Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript.", "labels": [], "entities": []}, {"text": "We then refine this alignment using a state-of-the-art visual food detector, based on a deep convolutional neural network.", "labels": [], "entities": []}, {"text": "We show that our technique outperforms simpler techniques based on keyword spotting.", "labels": [], "entities": [{"text": "keyword spotting", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.7719407677650452}]}, {"text": "It also enables interesting applications, such as automatically illustrating recipes with keyframes, and searching within a video for events of interest .", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, there have been many successful attempts to build large \"knowledge bases\" (KBs), such as NELL (,),, and Google's Knowledge Graph/ Vault ().", "labels": [], "entities": []}, {"text": "These KBs mostly focus on declarative facts, such as \"Barack Obama was born in Hawaii\".", "labels": [], "entities": []}, {"text": "But human knowledge also encompasses procedural information not yet within the scope of such declarative KBs -instructions and demonstrations of how to dance the tango, for example, or how to change a tire on your car.", "labels": [], "entities": []}, {"text": "A KB for organizing and retrieving such procedural knowledge could be a valuable resource for helping people (and potentially even robotse.g.,) learn to perform various tasks.", "labels": [], "entities": []}, {"text": "In contrast to declarative information, procedural knowledge tends to be inherently multimodal.", "labels": [], "entities": []}, {"text": "In particular, both language and perceptual information are typically used to parsimoniously describe procedures, as evidenced by the large number of \"howto\" videos and illustrated guides on the open web.", "labels": [], "entities": []}, {"text": "To automatically construct a multimodal database of procedural knowledge, we thus need tools for extracting information from both textual and visual sources.", "labels": [], "entities": []}, {"text": "Crucially, we also need to figure out how these various kinds of information, which often complement and overlap each other, fit together to a form a structured knowledge base of procedures.", "labels": [], "entities": []}, {"text": "As a small step toward the broader goal of aligning language and perception, we focus in this paper on the problem of aligning video depictions of procedures to steps in an accompanying text that corresponds to the procedure.", "labels": [], "entities": []}, {"text": "We focus on the cooking domain due to the prevalence of cooking videos on the web and the relative ease of interpreting their recipes as linear sequences of canonical actions.", "labels": [], "entities": []}, {"text": "In this domain, the textual source is a user-uploaded recipe attached to the video showing the recipe's execution.", "labels": [], "entities": []}, {"text": "The individual steps of procedures are cooking actions like \"peel an onion\", \"slice an onion\", etc.", "labels": [], "entities": []}, {"text": "However, our techniques can be applied to any domain that has textual instructions and corresponding videos, including videos at sites such as youtube.com, howcast.com, howdini.com or videojug.com.", "labels": [], "entities": []}, {"text": "The approach we take in this paper leverages the fact that the speech signal in instructional videos is often closely related to the actions that the person is performing (which is not true in more general videos).", "labels": [], "entities": []}, {"text": "Thus we first align the instructional steps to the speech signal using an HMM, and then refine this alignment by using a state of the art computer vision system.", "labels": [], "entities": []}, {"text": "In summary, our contributions are as follows.", "labels": [], "entities": []}, {"text": "First, we propose a novel system that combines text, speech and vision to perform an alignment between textual instructions and instructional videos.", "labels": [], "entities": []}, {"text": "Second, we use our system to create a large corpus of 180k aligned recipe-video pairs, and an even larger corpus of 1.4M short video clips, each labeled with a cooking action and a noun phrase.", "labels": [], "entities": []}, {"text": "We evaluate the quality of our corpus using human raters.", "labels": [], "entities": []}, {"text": "Third, we show how we can use our methods to support applications such as within-video search and recipe auto-illustration.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we experimentally evaluate how well our methods work.", "labels": [], "entities": []}, {"text": "We then briefly demonstrate some prototype applications.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test set performance of text-based recipe classifier.", "labels": [], "entities": []}]}