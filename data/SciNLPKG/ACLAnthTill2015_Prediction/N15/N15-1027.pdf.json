{"title": [{"text": "When and why are log-linear models self-normalizing?", "labels": [], "entities": []}], "abstractContent": [{"text": "Several techniques have recently been proposed for training \"self-normalized\" discrimi-native models.", "labels": [], "entities": []}, {"text": "These attempt to find parameter settings for which unnormalized model scores approximate the true label probability.", "labels": [], "entities": []}, {"text": "However , the theoretical properties of such techniques (and of self-normalization generally) have not been investigated.", "labels": [], "entities": []}, {"text": "This paper examines the conditions under which we can expect self-normalization to work.", "labels": [], "entities": []}, {"text": "We characterize a general class of distributions that admit self-normalization, and prove generalization bounds for procedures that minimize empirical normalizer variance.", "labels": [], "entities": []}, {"text": "Motivated by these results, we describe a novel variant of an established procedure for training self-normalized models.", "labels": [], "entities": []}, {"text": "The new procedure avoids computing normalizers for most training examples, and decreases training time by as much as factor often while preserving model quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper investigates the theoretical properties of log-linear models trained to make their unnormalized scores approximately sum to one.", "labels": [], "entities": []}, {"text": "Recent years have seen a resurgence of interest in log-linear approaches to language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7686634361743927}]}, {"text": "This includes both conventional log-linear models) and neural networks with a log-linear output layer ().", "labels": [], "entities": []}, {"text": "On a variety of tasks, these LMs have produced substantial gains over conventional generative models based on counting n-grams.", "labels": [], "entities": []}, {"text": "Successes include machine translation) and speech recognition ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7800890207290649}, {"text": "speech recognition", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.850946307182312}]}, {"text": "However, log-linear LMs come at a significant cost for computational efficiency.", "labels": [], "entities": []}, {"text": "In order to output a well-formed probability distribution over words, such models must typically calculate a normalizing constant whose computational cost grows linearly in the size of the vocabulary.", "labels": [], "entities": []}, {"text": "Fortunately, many applications of LMs remain well-behaved even if LM scores do not actually correspond to probability distributions.", "labels": [], "entities": []}, {"text": "For example, if a machine translation decoder uses output from a pre-trained LM as a feature inside a larger model, it suffices to have all output scores on approximately the same scale, even if these do not sum to one for every LM context.", "labels": [], "entities": [{"text": "machine translation decoder", "start_pos": 18, "end_pos": 45, "type": "TASK", "confidence": 0.8048871755599976}]}, {"text": "There has thus been considerable research interest around training procedures capable of ensuring that unnormalized outputs for every context are \"close\" to a probability distribution.", "labels": [], "entities": []}, {"text": "We are aware of at least two such techniques: noisecontrastive estimation (NCE) ( and explicit penalization of the log-normalizer).", "labels": [], "entities": [{"text": "noisecontrastive estimation (NCE)", "start_pos": 46, "end_pos": 79, "type": "METRIC", "confidence": 0.7446865618228913}]}, {"text": "Both approaches have advantages and disadvantages.", "labels": [], "entities": []}, {"text": "NCE allows fast training by dispensing with the need to ever compute a normalizer.", "labels": [], "entities": [{"text": "NCE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9413377642631531}]}, {"text": "Explicit penalization requires full normalizers to be computed during training but parameterizes the relative importance of the likelihood and the \"sum-to-one\" constraint, allowing system designers to tune the objective for optimal performance.", "labels": [], "entities": []}, {"text": "While both NCE and explicit penalization are observed to work in practice, their theoretical properties have not been investigated.", "labels": [], "entities": []}, {"text": "It is a classical result that empirical minimization of classification error yields models whose predictions generalize well.", "labels": [], "entities": []}, {"text": "This paper instead investigates a notion of normalization error, and attempts to understand the conditions under which unnormalized model scores area reliable surrogate for probabilities.", "labels": [], "entities": []}, {"text": "While language modeling serves as a motivation and running example, our results apply to any log-linear model, and maybe of general use for efficient classification and decoding.", "labels": [], "entities": []}, {"text": "Our goals are twofold: primarily, to provide intuition about how self-normalization works, and why it behaves as observed; secondarily, to back these intuitions with formal guarantees, both about classes of normalizable distributions and parameter estimation procedures.", "labels": [], "entities": []}, {"text": "The paper is built around two questions: When can self-normalization work-for which distributions do good parameter settings exist?", "labels": [], "entities": []}, {"text": "And why should self-normalization work-how does variance of the normalizer on held-out data relate to variance of the normalizer during training?", "labels": [], "entities": []}, {"text": "Analysis of these questions suggests an improvement to the training procedure described by Devlin et al., and we conclude with empirical results demonstrating that our new procedure can reduce training time for self-normalized models by an order of magnitude.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Result of varying normalized fraction \u03b3, with  \u03b1 = 1. When no normalization is applied, the model's be- havior is pathological, but when normalizing only a small  fraction of the training set, performance on the down- stream translation task remains good.", "labels": [], "entities": []}, {"text": " Table 2: Result of varying normalization parameter \u03b1,", "labels": [], "entities": [{"text": "Result", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9502105712890625}]}]}