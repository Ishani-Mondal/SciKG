{"title": [{"text": "Incrementally Tracking Reference in Human/Human Dialogue Using Linguistic and Extra-Linguistic Information", "labels": [], "entities": [{"text": "Incrementally Tracking Reference in Human/Human Dialogue", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.7193375416100025}]}], "abstractContent": [{"text": "A large part of human communication involves referring to entities in the world and often these entities are objects that are visually present for the interlocutors.", "labels": [], "entities": []}, {"text": "A system that aims to resolve such references needs to tackle a complex task: objects and their visual features need to be determined, the referring expressions must be recognised, and extra-linguistic information such as eye gaze or pointing gestures need to be incorporated.", "labels": [], "entities": []}, {"text": "Systems that can make use of such information sources exist, but have so far only been tested under very constrained settings, such as WOz interactions.", "labels": [], "entities": []}, {"text": "In this paper, we apply to a more complex domain a reference resolution model that works incrementally (i.e., word by word), grounds words with visually present properties of objects (such as shape and size), and can incorporate extra-linguistic information.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7368666678667068}]}, {"text": "We find that the model works well compared to previous work on the same data, despite using fewer features.", "labels": [], "entities": []}, {"text": "We conclude that the model shows potential for use in a real-time interactive dialogue system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Referring to entities in the world via definite descriptions makes up a large part of human communication).", "labels": [], "entities": [{"text": "Referring to entities in the world via definite descriptions", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.802672459019555}]}, {"text": "In task-oriented situations, these references are often to entities that are visible in the shared environment.", "labels": [], "entities": []}, {"text": "This kind of reference has attracted attention in recent computational research, but the kinds of interactions studied are often fairly restricted in controlled lab situations or simulated human/computer interactions,.", "labels": [], "entities": []}, {"text": "In such task-oriented, co-located settings, interlocutors can make use of extra-linguistic cues such as gaze or pointing gestures.", "labels": [], "entities": []}, {"text": "Furthermore, listeners resolve references as they unfold, often identifying the referred entity before the end of the reference), however research in reference resolution has mostly focused on full, completed referring expressions.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 150, "end_pos": 170, "type": "TASK", "confidence": 0.7885181903839111}]}, {"text": "In this paper we make a first move towards addressing somewhat more complex domains.", "labels": [], "entities": []}, {"text": "We apply a model of reference resolution, which has been tested in a simpler setup, on more natural data coming from a corpus of human/human interactions.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.8291970789432526}]}, {"text": "The model is incremental in that it does not wait until the end of an utterance to process, rather it updates its interpretation at each word increment.", "labels": [], "entities": []}, {"text": "The model can also incorporate other modalities, such as gaze or pointing cues (deixis) incrementally.", "labels": [], "entities": []}, {"text": "We also model the saliency of the context, and show that the model can easily take such contextual information into account.", "labels": [], "entities": []}, {"text": "The model improves over previous work on reference resolution applied to the same data ().", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7197543978691101}]}, {"text": "The paper is structured as follows: in the following section we discuss related work on incremental resolution of referring expressions.", "labels": [], "entities": [{"text": "incremental resolution of referring expressions", "start_pos": 88, "end_pos": 135, "type": "TASK", "confidence": 0.7556440711021424}]}, {"text": "We explain the model that we use in Section 3 and the data we apply it to in Section 4.", "labels": [], "entities": []}, {"text": "We then describe the experiments and the results and provide a discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "Procedure The procedure for this experiment is as follows.", "labels": [], "entities": []}, {"text": "In order to compare our results directly with those of Iida et al., we provide our model with the same training and evaluation data, in a 10-fold cross-validation of the 1192 REs from 27 dialogues (the T2009-11 corpus in ).", "labels": [], "entities": [{"text": "T2009-11 corpus", "start_pos": 202, "end_pos": 217, "type": "DATASET", "confidence": 0.963524341583252}]}, {"text": "For development, we used a separate part of the REX corpus (N2009-11) that was structured similarly to the one used in our evaluation.", "labels": [], "entities": [{"text": "REX corpus (N2009-11)", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.8510334730148316}]}, {"text": "Task The task is RR.", "labels": [], "entities": [{"text": "RR", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.6850026845932007}]}, {"text": "At each increment, SIUM returns a distribution overall objects; the probability for each object represents the strength of the belief that it is the referred one.", "labels": [], "entities": []}, {"text": "The argmax of the distribution is chosen as the hypothesised referred object.", "labels": [], "entities": [{"text": "argmax", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9557285308837891}]}, {"text": "P(R|I) P (R|I) models the likelihood of selecting a property of a candidate object for verbalisation; this likelihood is assumed to be uniform for all the properties that the candidate object has.", "labels": [], "entities": []}, {"text": "We derive these properties from a representation of the scene; similar to how computed features to present to their classifier: namely Ling (linguistic features), TaskSp (task specific features), and Gaze (from SV only).", "labels": [], "entities": []}, {"text": "Some features were binary, others such as shape and size had more values.", "labels": [], "entities": []}, {"text": "shows all the properties that were used here.", "labels": [], "entities": []}, {"text": "Each will now be explained.", "labels": [], "entities": []}, {"text": "Ling Each object had a shape, size, and relative position to the other pieces.", "labels": [], "entities": []}, {"text": "We determined by hand  the shape and size properties which remained static through each dialogue.", "labels": [], "entities": []}, {"text": "The position properties were derived from the corpus logs.", "labels": [], "entities": []}, {"text": "For each object, the centroid of each object was computed.", "labels": [], "entities": []}, {"text": "Then, the vertical and horizontal range for all of the objects was calculated and then split into three even sections in each dimension (see).", "labels": [], "entities": []}, {"text": "An object with a centroid in the left-most section of the horizontal range received a left property, similarly middle and right properties were calculated for corresponding objects.", "labels": [], "entities": []}, {"text": "For vertical placement, top, center and bottom properties were given to objects in the respective vertical segments.", "labels": [], "entities": [{"text": "vertical placement", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7363138496875763}]}, {"text": "Each object had a vertical and a horizontal property at all times, however, moving an object could result in a change of one of these spatial properties as the dialogue progressed.", "labels": [], "entities": []}, {"text": "As an example, compare, which is a snapshot of the interaction towards the beginning, and, which shows a later stage of the game board; spatial layout changes throughout the dialogue.", "labels": [], "entities": []}, {"text": "These properties differ somewhat from the features for the Ling model presented in.", "labels": [], "entities": []}, {"text": "Three features that we did use as properties had to do with reference recency: the most recently referred object received the referred X properties, if an object was referred to in the past 5, 10, or 20 seconds.", "labels": [], "entities": []}, {"text": "(2011) used 14 task-specific features, three of which they found to be the most informative in their model.", "labels": [], "entities": []}, {"text": "Here, we will only use the two most informative features as properties (the third one, whether or not an object was being manipulated at the beginning of the RE, did not improve results in a held-out test): the object that was most recently moved received the most recent move property and objects that have the mouse cursor over them received the mouse pointed property (see; object 4 would receive both of these properties, but only for the duration that the mouse was actually over it).", "labels": [], "entities": []}, {"text": "Each of these properties can be extracted directly from the corpus annotations.", "labels": [], "entities": []}, {"text": "Gaze Similar to, we consider gaze during a window of 1500ms before the onset of the RE.", "labels": [], "entities": [{"text": "RE", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.7556538581848145}]}, {"text": "The object that was gazed at the longest during that time received a longest gazed at property, the object which was fixated upon most recently during that interval before the RE onset received a recent fixation property, and the object which had the most fixations received the most gazed at property.", "labels": [], "entities": [{"text": "RE", "start_pos": 176, "end_pos": 178, "type": "METRIC", "confidence": 0.6652830839157104}]}, {"text": "During a RE, an object received the gazed at in utt property if it is gazed at during the RE up until that point.", "labels": [], "entities": [{"text": "RE", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.8270894885063171}]}, {"text": "These properties can be extracted directly from the corpus annotations.", "labels": [], "entities": []}, {"text": "Other gaze features are not really accessible to an incremental model such as this, as gaze features extracted from gaze activity over the RE can only be computed when it is complete.", "labels": [], "entities": []}, {"text": "Our Gaze properties are made up of these 4 properties, as opposed to the 14 features in.", "labels": [], "entities": []}, {"text": "P(U|R) P (U |R) is the model that connects the property selected for verbalisation with away of verbalising it (a value for U ).", "labels": [], "entities": []}, {"text": "Instead of directly learning this model from data, which would suffer from data sparseness, we trained a naive Bayes model 276 for P (R|U ) (as, according to Bayes' rule, P (U |R) is equal to P (R|U )P (U ) 1 P (R) , which, plugged in into formula (2), cancels out 1 P (U ) ; further assuming the P (R) is uniform, we can directly replace P (U |R) with P (R|U ) here).", "labels": [], "entities": []}, {"text": "On the language side (the variable U in the model), we used n-grams over Japanese characters (we attempted tokenisation of the REs into words, but found that using characters worked just as well in the held-out set).", "labels": [], "entities": []}, {"text": "P(I) The prior P (I) is the posterior of the previously computed increment.", "labels": [], "entities": []}, {"text": "In the first increment, it can simply beset to a uniform distribution.", "labels": [], "entities": []}, {"text": "Here, we apply a more informative prior based on saliency.", "labels": [], "entities": []}, {"text": "We learn a context model which is queried when the first word begins, taking information about the context immediately before the beginning of the RE into account, producing a distribution over objects, which becomes P (I) of the first increment in the RE.", "labels": [], "entities": []}, {"text": "The context model itself is a simple application of the SIUM, where instead of being a word, U is a token that represents saliency.", "labels": [], "entities": []}, {"text": "The context model thus learns what properties are important to the pre-RE context and provides an up-to-date distribution over the objects as a RE begins.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of differences between PENTO and  REX tasks.", "labels": [], "entities": [{"text": "PENTO and  REX tasks", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.4296811819076538}]}, {"text": " Table 3: Applications of P (U |R), for some values of U  and R; we assume that this model is learned from data  (rows are excerpted from a larger distribution over all the  words in the vocabulary)", "labels": [], "entities": []}, {"text": " Table 4: P (R|I), for our example domain. The probabil- ity mass is distributed over the number of properties that  a candidate object actually has.", "labels": [], "entities": []}, {"text": " Table 5: Application of RE small triangle, where 1 is the  referred object", "labels": [], "entities": [{"text": "RE", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.7740648984909058}]}, {"text": " Table 6: Incremental results for SIUM, numbers represent  % into RE.", "labels": [], "entities": [{"text": "Incremental", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9557660818099976}, {"text": "SIUM", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.7019999027252197}, {"text": "RE", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9955390095710754}]}]}