{"title": [{"text": "Chinese Event Coreference Resolution: An Unsupervised Probabilistic Model Rivaling Supervised Resolvers", "labels": [], "entities": [{"text": "Chinese Event Coreference Resolution", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7493057250976562}]}], "abstractContent": [{"text": "Recent work has successfully leveraged the semantic information extracted from lexical knowledge bases such as WordNet and FrameNet to improve English event corefer-ence resolvers.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9527302384376526}, {"text": "English event corefer-ence resolvers", "start_pos": 143, "end_pos": 179, "type": "TASK", "confidence": 0.6321352943778038}]}, {"text": "The lack of comparable resources in other languages, however, has made the design of high-performance non-English event coreference resolvers, particularly those employing unsupervised models, very difficult.", "labels": [], "entities": [{"text": "event coreference resolvers", "start_pos": 114, "end_pos": 141, "type": "TASK", "confidence": 0.65115820368131}]}, {"text": "We propose a generative model for the under-studied task of Chinese event corefer-ence resolution that rivals its supervised counterparts in performance when evaluated on the ACE 2005 corpus.", "labels": [], "entities": [{"text": "Chinese event corefer-ence resolution", "start_pos": 60, "end_pos": 97, "type": "TASK", "confidence": 0.6292411759495735}, {"text": "ACE 2005 corpus", "start_pos": 175, "end_pos": 190, "type": "DATASET", "confidence": 0.9819587270418803}]}], "introductionContent": [{"text": "Event coreference resolution is the task of determining which event mentions in a text refer to the same real-world event.", "labels": [], "entities": [{"text": "Event coreference resolution", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.799228568871816}]}, {"text": "Compared to entity coreference, event coreference is not only much less studied, but it is arguably more challenging.", "labels": [], "entities": [{"text": "event coreference", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7737810909748077}]}, {"text": "Recall that for two event mentions to be coreferent, both their triggers (i.e., the words realizing the occurrence of the events) and their corresponding arguments (e.g., the times, places, and people involved in them) have to be compatible.", "labels": [], "entities": []}, {"text": "However, identifying potential arguments (which is typically performed by an entity extraction system), linking arguments to their event mentions (which is typically performed by an event extraction system), and determining the compatibility between two event arguments (which is the job of an entity coreference resolver), are all non-trivial tasks.", "labels": [], "entities": [{"text": "entity coreference resolver", "start_pos": 294, "end_pos": 321, "type": "TASK", "confidence": 0.6926574309666952}]}, {"text": "In other words, end-to-end event coreference resolution is complicated in part by the fact that an event coreference resolver has to rely on the noisy outputs produced by its upstream components in the standard information extraction (IE) pipeline.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.6738839546839396}, {"text": "event coreference resolver", "start_pos": 99, "end_pos": 125, "type": "TASK", "confidence": 0.7221212983131409}, {"text": "information extraction (IE) pipeline", "start_pos": 211, "end_pos": 247, "type": "TASK", "confidence": 0.8172914187113444}]}, {"text": "In this paper, we examine Chinese event coreference resolution.", "labels": [], "entities": [{"text": "Chinese event coreference resolution", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.7943555042147636}]}, {"text": "While English event coreference is under-investigated, Chinese event coreference is much less studied than English event coreference.", "labels": [], "entities": [{"text": "English event coreference", "start_pos": 6, "end_pos": 31, "type": "TASK", "confidence": 0.49573756257692975}]}, {"text": "In terms of task definition, there is no difference between English and Chinese event coreference.", "labels": [], "entities": [{"text": "Chinese event coreference", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.6207720935344696}]}, {"text": "However, the design of high-performance Chinese event coreference resolvers is complicated in part by the lack of large-scale lexical knowledge bases.", "labels": [], "entities": [{"text": "Chinese event coreference resolvers", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.6441592499613762}]}, {"text": "Recent work by has shown that the semantic information extracted from WordNet) and FrameNet () significantly contributed to the performance of their English event coreference resolver.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.9487263560295105}, {"text": "English event coreference resolver", "start_pos": 149, "end_pos": 183, "type": "TASK", "confidence": 0.6049196645617485}]}, {"text": "While the lack of comparable lexical knowledge bases in Chinese can be mitigated in part by the use of event coreference annotated data, we focus on a challenging version of the task ---unsupervised Chinese event coreference resolution.", "labels": [], "entities": [{"text": "Chinese event coreference resolution", "start_pos": 199, "end_pos": 235, "type": "TASK", "confidence": 0.5776844173669815}]}, {"text": "Specifically, our goal is to learn an event coreference model without using data annotated with event coreference links.", "labels": [], "entities": []}, {"text": "When evaluated on the Chinese portion of the ACE 2005 corpus, our unsupervised probabilistic model for event coreference resolution rivals its state-ofthe-art supervised counterpart in performance.", "labels": [], "entities": [{"text": "Chinese portion of the ACE 2005 corpus", "start_pos": 22, "end_pos": 60, "type": "DATASET", "confidence": 0.8716865948268345}, {"text": "event coreference resolution", "start_pos": 103, "end_pos": 131, "type": "TASK", "confidence": 0.8315100073814392}]}, {"text": "This, together with the fact that its underlying generative process is not language-dependent and does not rely on features extracted from lexical knowledge bases, potentially enables it to be applied to languages where neither annotated data nor large-scale knowledge bases are available.", "labels": [], "entities": []}, {"text": "Another feature of our model that deserves mention is that it performs joint event coreference resolution and anaphoricity determination.", "labels": [], "entities": [{"text": "joint event coreference resolution", "start_pos": 71, "end_pos": 105, "type": "TASK", "confidence": 0.6562595814466476}, {"text": "anaphoricity determination", "start_pos": 110, "end_pos": 136, "type": "TASK", "confidence": 0.65957410633564}]}, {"text": "Anaphoricity determination, the task of determining whether a mention is anaphoric and hence needs to be resolved, is an issue common to both entity and event coreference resolution.", "labels": [], "entities": [{"text": "Anaphoricity determination", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7159194052219391}, {"text": "event coreference resolution", "start_pos": 153, "end_pos": 181, "type": "TASK", "confidence": 0.7233606576919556}]}, {"text": "However, determining the anaphoricity of an event mention is arguably more difficult than determining the anaphoricity of a pronoun.", "labels": [], "entities": []}, {"text": "The reason is that while there exist lexical and syntactic cues that can be used to reliably identify pleonastic pronouns (, the lack of such cues in event mentions makes the identification of anaphoric event mentions challenging even in a supervised manner, let alone in an unsupervised manner.", "labels": [], "entities": []}, {"text": "Note that ignoring anaphoricity determination and having our model attempt to resolve every event mention is not a viable option, as only 24.4% of the Chinese event mentions in our evaluation corpus) are anaphoric.", "labels": [], "entities": []}, {"text": "Our decision to jointly model anaphoricity determination and event coreference resolution was inspired by the difficulty of designing a standalone system for determining the anaphoricity of event mentions.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6982795000076294}, {"text": "event coreference resolution", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.8351654609044393}]}], "datasetContent": [{"text": "For evaluation, we conduct five-fold cross-validation experiments on the 633 Chinese documents of the ACE 2005 training corpus.", "labels": [], "entities": [{"text": "Chinese documents of the ACE 2005 training corpus", "start_pos": 77, "end_pos": 126, "type": "DATASET", "confidence": 0.8682609125971794}]}, {"text": "Statistics on the corpus are shown in.", "labels": [], "entities": []}, {"text": "We report results in terms of recall (R), precision (P), and F-score (F) using the commonly-used coreference evaluation measures given by the CoNLL scorer, namely the link-based MUC scorer (, the mention-based B 3 scorer (, the entitybased version of the CEAF scorer (, and the Rand index-based BLANC scorer (Recasens and Hovy, 2011), after singleton event mentions are removed from the coreference partitions produced by our resolver.", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9506663829088211}, {"text": "precision (P)", "start_pos": 42, "end_pos": 55, "type": "METRIC", "confidence": 0.9427117854356766}, {"text": "F-score (F)", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9430275857448578}, {"text": "CoNLL scorer", "start_pos": 142, "end_pos": 154, "type": "DATASET", "confidence": 0.8982308804988861}, {"text": "mention-based B 3 scorer", "start_pos": 196, "end_pos": 220, "type": "METRIC", "confidence": 0.6838979870080948}, {"text": "CEAF scorer", "start_pos": 255, "end_pos": 266, "type": "DATASET", "confidence": 0.9120416045188904}, {"text": "Rand index-based BLANC scorer", "start_pos": 278, "end_pos": 307, "type": "METRIC", "confidence": 0.6112673282623291}]}, {"text": "We use the latest version (version 8) of the CoNLL scorer 2 , which fixes a bug in previous versions).", "labels": [], "entities": [{"text": "CoNLL scorer 2", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.9006725549697876}]}, {"text": "In addition, we report the CoNLL score, which is the unweighted average of the MUC, B 3 , and CEAF F-scores.", "labels": [], "entities": [{"text": "CoNLL score", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.951544851064682}, {"text": "MUC", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.8343379497528076}, {"text": "B 3", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.7046716213226318}, {"text": "CEAF", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.8819587826728821}, {"text": "F-scores", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.37569528818130493}]}, {"text": "We perform an end-to-end evaluation, as it can more accurately reflect the performance of an event coreference resolver when it is used in practice.", "labels": [], "entities": [{"text": "event coreference resolver", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.6822442809740702}]}, {"text": "More specifically, to extract the event mentions used in our evaluation, we employ SinoCoreferencer 3 , which, as mentioned before, is an end-to-end ACE-style Chinese IE system that achieves state-ofthe-art event coreference results.", "labels": [], "entities": [{"text": "SinoCoreferencer", "start_pos": 83, "end_pos": 99, "type": "DATASET", "confidence": 0.9051652550697327}]}, {"text": "Specifically, the event triggers needed to compute the trigger-based context features are extracted using SinoCoreferencer's event extraction subsystem.", "labels": [], "entities": [{"text": "SinoCoreferencer", "start_pos": 106, "end_pos": 122, "type": "DATASET", "confidence": 0.9613195657730103}]}, {"text": "The event subtypes needed to identify and filter out implausible candidate antecedents are also provided by its event extraction subsystem.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 112, "end_pos": 128, "type": "TASK", "confidence": 0.6986791342496872}]}, {"text": "The event arguments needed to compute the argument-based context features are first extracted and typed by its entity extraction subsystem, and then linked to their triggers by its event extraction subsystem.", "labels": [], "entities": []}, {"text": "Finally, the entity coreference links and the semantic roles needed to compute Feature 4 are provided by its entity coreference subsystem and its event extraction subsystem, respectively.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 146, "end_pos": 162, "type": "TASK", "confidence": 0.7084947526454926}]}, {"text": "Details of each of these subsystems can be found in Chen and Ng (2014).", "labels": [], "entities": []}, {"text": "Recall that in our model eight probability terms play a major role: P (e t |c t ), P (c|k), and P (f i c |l) for each We employ only those semantic roles that can be reliably determined by SinoCoreferencer's event extraction subsystem, namely, Agent, Adjudicator, Defendant, Giver, Person, Place, Position, Organization, Origin, and Recipient.  of the six context features.", "labels": [], "entities": [{"text": "SinoCoreferencer", "start_pos": 189, "end_pos": 205, "type": "DATASET", "confidence": 0.9114748239517212}]}, {"text": "To investigate the contribution of each probability term to overall performance, we conduct ablation experiments.", "labels": [], "entities": []}, {"text": "Specifically, in each ablation experiment, we remove exactly one term from the model and retrain it.", "labels": [], "entities": []}, {"text": "Ablation results are shown in.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9918419122695923}]}, {"text": "Each row contains the F-scores obtained via the five evaluation measures.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9975783228874207}]}, {"text": "To facilitate comparison, the scores of the model in which all eight probability terms are used is shown in row 1.", "labels": [], "entities": []}, {"text": "As we can see, Feature 1 is the most useful feature: its removal causes the CoNLL score to drop significantly by 5.3 points.", "labels": [], "entities": [{"text": "CoNLL score", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.8337517976760864}]}, {"text": "A closer examination reveals that the drop in the CoNLL score is caused by a significant drop in recall w.r.t. all scorers.", "labels": [], "entities": [{"text": "CoNLL score", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.7825583517551422}, {"text": "recall w.r.t. all scorers", "start_pos": 97, "end_pos": 122, "type": "METRIC", "confidence": 0.8343787342309952}]}, {"text": "Recall that this feature encodes the conditions under which two triggers are likely to be coreferent.", "labels": [], "entities": []}, {"text": "It is perhaps not surprising that its removal causes a significant drop in recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9983324408531189}]}, {"text": "The second most useful feature is P (c|k), which places zero probability mass on candidate antecedents whose event subtypes are different from that of the active event mention.", "labels": [], "entities": []}, {"text": "Its removal causes the CoNLL score to drop significantly by 1.6 points.", "labels": [], "entities": [{"text": "CoNLL score", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.4979298412799835}]}, {"text": "The removal of each other feature resulted in a small, insignificant drop in the CoNLL score.", "labels": [], "entities": [{"text": "CoNLL score", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.4822077751159668}]}], "tableCaptions": [{"text": " Table 2: Five-fold cross-validation event coreference results on the ACE 2005 corpus.", "labels": [], "entities": [{"text": "cross-validation event coreference", "start_pos": 20, "end_pos": 54, "type": "TASK", "confidence": 0.65004563331604}, {"text": "ACE 2005 corpus", "start_pos": 70, "end_pos": 85, "type": "DATASET", "confidence": 0.9801889856656393}]}, {"text": " Table 3. Each row  contains the F-scores obtained via the five evaluation  measures. To facilitate comparison, the scores of the  model in which all eight probability terms are used is  shown in row 1. As we can see, Feature 1 is the most  useful feature: its removal causes the CoNLL score  to drop significantly by 5.3 points. A closer exam- ination reveals that the drop in the CoNLL score is  caused by a significant drop in recall w.r.t. all scor- ers. Recall that this feature encodes the conditions  under which two triggers are likely to be coreferent.  It is perhaps not surprising that its removal causes a  significant drop in recall.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9756262302398682}, {"text": "CoNLL score", "start_pos": 280, "end_pos": 291, "type": "METRIC", "confidence": 0.8200861513614655}, {"text": "recall", "start_pos": 430, "end_pos": 436, "type": "METRIC", "confidence": 0.9956356883049011}, {"text": "recall", "start_pos": 639, "end_pos": 645, "type": "METRIC", "confidence": 0.9964423775672913}]}, {"text": " Table 3: Ablation results in terms of F-scores.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9984946250915527}, {"text": "F-scores", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9952735304832458}]}]}