{"title": [{"text": "WORKSHOP ON THE EVALUATION OF NATURAL LANGUAGE PROCESSING SYSTEMS", "labels": [], "entities": []}], "abstractContent": [], "introductionContent": [{"text": "In the past few years, the computational linguistics research community has begun to wrestle with the problem of how to evaluate its progress in developing natural language processing systems.", "labels": [], "entities": []}, {"text": "With the exception of natural language interfaces, there are few working systems in existence, and they tend to focus on very different tasks using equally different techniques.", "labels": [], "entities": []}, {"text": "There has been little agreement in the field about training sets and test sets, or about clearly defined subsets of problems that constitute standards for different levels of performance.", "labels": [], "entities": []}, {"text": "Even those groups that have attempted a measure of self-evaluation have often been reduced to discussing a system's performance in isolation---comparing its current performance to its previous performance rather than to another system.", "labels": [], "entities": []}, {"text": "As this technology begins to move slowly into the marketplace, the lack of useful evaluation techniques is becoming more and more painfully obvious.", "labels": [], "entities": []}, {"text": "In order to make progress in the difficult area of natural language evaluation, a Workshop on the Evaluation of Natural Language Processing Systems was held on December 7-9, 1988 at the Wayne Hotel in Wayne, Pennsylvania.", "labels": [], "entities": [{"text": "natural language evaluation", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6498032410939535}]}, {"text": "The workshop was organized by Martha Palmer (Unisys), assisted by a program committee consisting of Beth Sundheim (NOSC), Ed Hovy (ISI), Tim Finin (Unisys), Lynn Bates (BBN), and Mitch Marcus (Pennsylvania).", "labels": [], "entities": [{"text": "BBN", "start_pos": 169, "end_pos": 172, "type": "DATASET", "confidence": 0.8640760779380798}]}, {"text": "Approximately 50 people participated, drawn from universities, industry, and government.", "labels": [], "entities": []}, {"text": "The workshop received the generous support of the Rome Air Defense Center, the Association of Computational Linguistics, the American Association of Artificial Intelligence, and Unisys Defense Systems.", "labels": [], "entities": [{"text": "Rome Air Defense Center", "start_pos": 50, "end_pos": 73, "type": "DATASET", "confidence": 0.9394771903753281}]}, {"text": "The workshop was organized along two basic premises.", "labels": [], "entities": []}, {"text": "First, it should be possible to discuss system evaluation in general without having to state whether the purpose of the system is \"question-answering\" or \"text processing.\"", "labels": [], "entities": []}, {"text": "Evaluating a system requires the definition of an application task in terms of input/output pairs that are equally applicable to question-answering, text processing, or generation.", "labels": [], "entities": []}, {"text": "Second, there are two basic types of evaluation, black-box evaluation, which measures system performance on a given task in terms of well-defined input/output pairs, and glassbox evaluation, which examines the internal workings of the system.", "labels": [], "entities": []}, {"text": "For example, glass-box performance evaluation fora system that is supposed to perform semantic and pragmatic analysis should include the examination of predicate-argument relations, referents, and temporal and causal relations.", "labels": [], "entities": []}, {"text": "Since there are many different stages of development that a natural language system passes through before it is in a state where black-box evaluation is even possible (see), glass-box evaluation plays an especially important role in guiding the development at early stages.", "labels": [], "entities": []}, {"text": "With these premises in mind, the workshop was structured around the following three sessions: (i) defining the notions of \"black-box evaluation\" and \"glass-box evaluation\" and exploring their utility; (ii) defining criteria for \"black-box evaluation\"; and (iii) defining criteria for \"glass-box evaluation.\"", "labels": [], "entities": []}, {"text": "It was hoped that the workshop would shed light on the following questions.", "labels": [], "entities": []}, {"text": "\u2022 What are valid measures of\"black-box\" performance?", "labels": [], "entities": []}, {"text": "\u2022 What linguistic theories are relevant to developing test suites?", "labels": [], "entities": []}, {"text": "\u2022 How can we characterize efficiency?", "labels": [], "entities": [{"text": "characterize efficiency", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.9292193055152893}]}, {"text": "\u2022 What is a reasonable expectation for robustness?", "labels": [], "entities": []}, {"text": "\u2022 What would constitute valid training sets and test sets?", "labels": [], "entities": []}, {"text": "\u2022 How does all of this relate to measuring progress in the field?", "labels": [], "entities": []}], "datasetContent": [{"text": "Beth Sundheim (NOSC) proposed a black box evaluation of message understanding systems consisting of a training set of 100 messages from a specific domain, and two separate test sets, one consisting of 20 messages and another of 10.", "labels": [], "entities": [{"text": "message understanding", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.7714307606220245}]}, {"text": "The performance was to be evaluated with respect to a frame-filling task.", "labels": [], "entities": []}, {"text": "There was general agreement among the workshop participants that useful blackbox evaluations can be done for the message understanding and database question-answering task domains.", "labels": [], "entities": [{"text": "message understanding", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.8022578954696655}]}, {"text": "It was also agreed that more general systems aimed at text understanding and dialogue understanding were not good candidates for black-box evaluation due to the nascent stage of their development, although individual components from such systems might benefit from evaluation.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8465838432312012}, {"text": "dialogue understanding", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.7750262022018433}]}, {"text": "The workshop attendees were pleasantly surprised by the results of the generation group, which came up with a fairly concrete plan for comparing performance of generation systems, based on the message understanding proposal.", "labels": [], "entities": []}, {"text": "A perennial problem with all of these proposals, with the exception of the message understanding proposal, is the lack of funding.", "labels": [], "entities": [{"text": "message understanding", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.8903982639312744}]}, {"text": "Conferences and workshops need to be organized, systems need to be ported to the same domain so that they can be compared, etc., and there is very little financial support to make these things possible.", "labels": [], "entities": []}, {"text": "One of the primary goals of glass-box evaluations should be providing guidance to system developers--pinpointing gaps in coverage and imperfections in algorithms.", "labels": [], "entities": []}, {"text": "The glass-box evaluation task for the workshop, as outlined by Bonnie Webber (Penn), consisted of several stages.", "labels": [], "entities": [{"text": "glass-box evaluation task", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7792381246884664}]}, {"text": "The first stage was to define for each area a range of items that should be evaluated.", "labels": [], "entities": []}, {"text": "The next stage was to determine which items in the ranlge were suitable for evaluation and which were not.", "labels": [], "entities": []}, {"text": "For those that could be evaluated, appropriate methodologies (features and behaviors) and metrics (measures made on those features and behaviors) were to be specified.", "labels": [], "entities": []}, {"text": "For items o:r areas that were not yet ready, there should bean attempt to specify the necessary steps for improving their suitability for evaluation.", "labels": [], "entities": []}, {"text": "As explained in more detail below, the glass-box methodology most commonly suggested by the working groups was black-box evaluation of a single component.", "labels": [], "entities": []}, {"text": "The area that seemed the ripest for evaluation was syntax, with semantics being the farthest away from the level of consensus required for general evaluation standards.", "labels": [], "entities": []}, {"text": "Pragmatics and discourse heroically managed to specify a range of items and suggest a possible black-box evaluation methodology fora subset of those items.", "labels": [], "entities": []}, {"text": "Knowledge representation specified subtopics with associated evaluation techniques.", "labels": [], "entities": [{"text": "Knowledge representation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7398655265569687}]}], "tableCaptions": []}