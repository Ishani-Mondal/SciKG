{"title": [{"text": "BUAP: Evaluating Features for Multilingual and Cross-Level Semantic Textual Similarity", "labels": [], "entities": [{"text": "BUAP", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.47875016927719116}]}], "abstractContent": [{"text": "In this paper we present the evaluation of different features for multiligual and cross-level semantic textual similarity.", "labels": [], "entities": [{"text": "cross-level semantic textual similarity", "start_pos": 82, "end_pos": 121, "type": "TASK", "confidence": 0.5909384861588478}]}, {"text": "Three different types of features were used: lexical, knowledge-based and corpus-based.", "labels": [], "entities": []}, {"text": "The results obtained at the Semeval competition rank our approaches above the average of the rest of the teams highlighting the usefulness of the features presented in this paper.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic textual similarity aims to capture whether the meaning of two texts are similar.", "labels": [], "entities": [{"text": "Semantic textual similarity", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7351085543632507}]}, {"text": "This concept is somehow different from the textual similarity definition itself, because in the latter we are only interested in measuring the number of lexical components that the two texts share.", "labels": [], "entities": []}, {"text": "Therefore, textual similarity can range from exact semantic equivalence to a complete unrelatedness pair of texts.", "labels": [], "entities": []}, {"text": "Finding the semantic similarity between a pair of texts has become a big challenge for specialists in Natural Language Processing (NLP), because it has applications in some NLP task such as machine translation, automatic construction of summaries, authorship attribution, machine reading comprehension, information retrieval, among others, which usually need a manner to calculate degrees of similarity between two given texts.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 190, "end_pos": 209, "type": "TASK", "confidence": 0.7819567024707794}, {"text": "automatic construction of summaries", "start_pos": 211, "end_pos": 246, "type": "TASK", "confidence": 0.7577570825815201}, {"text": "authorship attribution", "start_pos": 248, "end_pos": 270, "type": "TASK", "confidence": 0.7598598301410675}, {"text": "information retrieval", "start_pos": 303, "end_pos": 324, "type": "TASK", "confidence": 0.8104572296142578}]}, {"text": "Semantic textual similarity can be calculated using texts of different sizes, for example between, a paragraph and a sentence, or a sentence and a phrase, or a phrase and a word, or even a word and a sense.", "labels": [], "entities": [{"text": "Semantic textual similarity", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7594265937805176}]}, {"text": "When we consider this difference, we say the task is called \"Cross-Level Semantic Similarity\", but when this distinction is not considered, then we call the task just as \"Semantic Textual Similarity\".", "labels": [], "entities": []}, {"text": "In this paper, we evaluate different features for determining those that obtain the best performances for calculating both, cross-level semantic similarity and multilingual semantic textual similarity.", "labels": [], "entities": []}, {"text": "The remaining of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the features used in both experiments.", "labels": [], "entities": []}, {"text": "Section 3 shows the manner we used the features for determining the degree of semantic textual similarity.", "labels": [], "entities": []}, {"text": "Section 4, on the other hand, shows the experiments we have carried out for determining cross-level semantic similarity.", "labels": [], "entities": [{"text": "cross-level semantic similarity", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.6414115031560262}]}, {"text": "Finally, in Section 5 the conclusions and findings are given.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Features used for calculating semantic textual similarity", "labels": [], "entities": [{"text": "calculating semantic textual similarity", "start_pos": 28, "end_pos": 67, "type": "TASK", "confidence": 0.6377042084932327}]}, {"text": " Table 2: Results obtained at the Task 10 of the Semeval competition for the English language", "labels": [], "entities": [{"text": "Semeval competition", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8423610627651215}]}, {"text": " Table 3: Results obtained at the Task 10 of the Semeval competition for the Spanish language (NOTE: The * symbol  denotes a system that used Wikipedia to build its model for the Wikipedia test dataset)", "labels": [], "entities": [{"text": "Semeval competition for the Spanish language", "start_pos": 49, "end_pos": 93, "type": "TASK", "confidence": 0.7561985750993093}, {"text": "NOTE", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.692626953125}, {"text": "Wikipedia test dataset", "start_pos": 179, "end_pos": 201, "type": "DATASET", "confidence": 0.8931844035784403}]}]}