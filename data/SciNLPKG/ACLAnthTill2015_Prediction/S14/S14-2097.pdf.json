{"title": [{"text": "ShrdLite: Semantic Parsing Using a Handmade Grammar", "labels": [], "entities": [{"text": "ShrdLite", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.8525303602218628}, {"text": "Semantic Parsing", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.688633531332016}]}], "abstractContent": [{"text": "This paper describes my approach for parsing robot commands, which was task 6 at SemEval 2014.", "labels": [], "entities": [{"text": "parsing robot commands", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.9019420345624288}]}, {"text": "My solution is to manually create a compact unification grammar.", "labels": [], "entities": []}, {"text": "The grammar is highly ambiguous , and relies heavily on filtering the parse results by checking their consistency with the current world.", "labels": [], "entities": []}, {"text": "The grammar is small, consisting of not more than 25 grammatical and 60 lexical rules.", "labels": [], "entities": []}, {"text": "The parser uses simple error correction together with a straightforward iterative deepening search.", "labels": [], "entities": []}, {"text": "Nevertheless, with these very basic algorithms, the system still managed to get 86.1% correctness on the evaluation data.", "labels": [], "entities": [{"text": "correctness", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.9970818161964417}]}, {"text": "Even more interesting is that by making the parser slightly more robust, the accuracy of the system rises to 93.5%, and by adding one single word to the lexicon, the accuracy is boosted to 98.0%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9993206262588501}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9994404911994934}]}], "introductionContent": [{"text": "SemEval 2014, task 6, was about parsing commands to a robot operating in a blocks world.", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6024955809116364}]}, {"text": "The goal is to parse utterances in natural language into commands in a formal language, the Robot Control Language (RCL).", "labels": [], "entities": [{"text": "parse utterances in natural language into commands", "start_pos": 15, "end_pos": 65, "type": "TASK", "confidence": 0.842305200440543}]}, {"text": "As a guide the system can use a spatial planner which can tell whether an RCL command is meaningful in a given blocks world.", "labels": [], "entities": []}, {"text": "The utterances are taken from the Robot Commands Treebank, which pairs 3409 sentences with semantic annotations consisting of an RCL command together with a description of This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": [{"text": "Robot Commands Treebank", "start_pos": 34, "end_pos": 57, "type": "DATASET", "confidence": 0.5618709524472555}]}, {"text": "Page numbers and proceedings footer are added by the organisers.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by/4.0/ a world where the command is meaningful.", "labels": [], "entities": []}, {"text": "The corpus was divided into 2500 training sentences and 909 evaluation sentences.", "labels": [], "entities": []}, {"text": "The system that is described in this paper, together with the evaluation data, is available online from GitHub: https://github.com/heatherleaf/semeval-2014-task6", "labels": [], "entities": []}], "datasetContent": [{"text": "As explained in section 3.1, the error analysis of the final evaluation revealed one construction and one lexical item that did not occur in the training corpus: \u2022 Utterances can start with 2-3 periods.", "labels": [], "entities": []}, {"text": "The reason why this was not caught by the robust parser is that each of these periods are considered a word of its own, and as mentioned in section 2.3.1, the penalty for skipping a lexicon word is 3 which means that the penalty for parsing a sentence with 2-3 initial periods is 6 or 9.", "labels": [], "entities": []}, {"text": "Unfortunately I had chosen a maximum penalty of 5 which meant that the original evaluation missed all these sentences.", "labels": [], "entities": []}, {"text": "By just increasing the maximum penalty from 5 to 9, the accuracy increased from 86.1% to 93.5%.", "labels": [], "entities": [{"text": "maximum penalty", "start_pos": 23, "end_pos": 38, "type": "METRIC", "confidence": 0.7447603940963745}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9997113347053528}]}, {"text": "\u2022 The word \"cell\" occurs in the evaluation data as a synonym for the entity type TILE, in addition to the existing tile words \"square\", \"grid\", \"space\", etc.", "labels": [], "entities": [{"text": "TILE", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9777218699455261}]}, {"text": "Unfortunately, the parser tries to correct \"cell\" into the Levenshtein-similar \"cube\", giving the wrong semantics.", "labels": [], "entities": []}, {"text": "By adding \"cell\" to the lexicon, the accuracy increased further from 93.5% to 98.0%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9996715784072876}]}, {"text": "The results of these minimal modifications are substantial, and are discussed further in section 3.2.", "labels": [], "entities": []}, {"text": "The system was evaluated on 909 sentences from the treebank, and I only tested for exact matches.", "labels": [], "entities": []}, {"text": "The result of the initial evaluation was that 86% of the sentences returned a correct result, when using the spatial planner as a guide for selecting parses.", "labels": [], "entities": []}, {"text": "Without the planner, the accuracy was only 51%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9998257756233215}]}, {"text": "The results are shown in the top rows in tables 1 and 2.", "labels": [], "entities": []}, {"text": "The grammar is ambiguous and the system relies heavily on the spatial planner to filter out candidates.", "labels": [], "entities": []}, {"text": "Without the planner, 42% of the utterances are ambiguous returning between 2 and 18 trees, but with the planner, only 4 utterances are ambiguous (i.e., 0.4%).", "labels": [], "entities": []}, {"text": "As already mentioned, the accuracy of the initial grammar was 86.1% with the spatial planner.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9997075200080872}]}, {"text": "The two minor modifications described in section 2.4 improve the results significantly, as can be seen in: Evaluation results without the spatial planner.", "labels": [], "entities": []}, {"text": "The final accuracy was therefore boosted to an impressive 98.0%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986159801483154}]}, {"text": "The columns in the result tables are as follows: Unique are the number of sentences for which the system returns one singletree which is correct.", "labels": [], "entities": []}, {"text": "Ambiguous are the number of sentences where the parser returns several trees, and the correct tree is among them: if the tree that the system selects (i.e., the smallest tree) is correct, it is counted as a correct ambiguous sentence, otherwise it is counted as incorrect.", "labels": [], "entities": []}, {"text": "Miss are the number of sentences where all the returned trees are incorrect, and Fail are the sentences for which the system could not find a tree at all.", "labels": [], "entities": [{"text": "Fail", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9981293082237244}]}, {"text": "shows that the modifications also improve the accuracy when the spatial planner is not used, but the improvement is not as impressive.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9993687272071838}]}, {"text": "The reason for this is that many of the failed sentences become ambiguous, and since the planner cannot be used for disambiguation, there is still a risk that the returned tree is not the correct one.", "labels": [], "entities": []}, {"text": "The number of sentences for which the system returns the correct tree somewhere among the results is the sum of all unique and ambiguous sentences, which amounts to 450+18+315 = 783 (i.e., 86.1%) for the original grammar and 498 + 24 + 366 = 888 (i.e., 97.7%) for the updated grammar.", "labels": [], "entities": []}, {"text": "Note that these are almost the same results as in table 1, which is consistent with the fact that the system uses the planner to filter out incorrect interpretations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results with the spatial planner.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results without the spatial planner.", "labels": [], "entities": []}]}