{"title": [{"text": "RTM-DCU: Referential Translation Machines for Semantic Similarity", "labels": [], "entities": [{"text": "Referential Translation Machines", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.8604020476341248}, {"text": "Similarity", "start_pos": 55, "end_pos": 65, "type": "TASK", "confidence": 0.8035920262336731}]}], "abstractContent": [{"text": "We use referential translation machines (RTMs) for predicting the semantic similarity of text.", "labels": [], "entities": [{"text": "referential translation machines (RTMs)", "start_pos": 7, "end_pos": 46, "type": "TASK", "confidence": 0.7833701968193054}, {"text": "predicting the semantic similarity of text", "start_pos": 51, "end_pos": 93, "type": "TASK", "confidence": 0.7780350347359976}]}, {"text": "RTMs area computational model for identifying the translation acts between any two data sets with respect to interpretants selected in the same domain , which are effective when making monolingual and bilingual similarity judgments.", "labels": [], "entities": []}, {"text": "RTMs judge the quality or the semantic similarity of text by using retrieved relevant training data as interpretants for reaching shared semantics.", "labels": [], "entities": [{"text": "RTMs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9279109239578247}]}, {"text": "We derive features measuring the closeness of the test sentences to the training data via inter-pretants, the difficulty of translating them, and the presence of the acts of translation , which may ubiquitously be observed in communication.", "labels": [], "entities": []}, {"text": "RTMs provide a language independent approach to all similarity tasks and achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) and good results in semantic relatedness and entail-ment (Task 1) and multilingual semantic textual similarity (STS) (Task 10).", "labels": [], "entities": [{"text": "predicting monolingual cross-level semantic similarity", "start_pos": 102, "end_pos": 156, "type": "TASK", "confidence": 0.6597867608070374}, {"text": "multilingual semantic textual similarity (STS", "start_pos": 236, "end_pos": 281, "type": "TASK", "confidence": 0.582496260603269}]}, {"text": "RTMs remove the need to access any task or domain specific information or resource.", "labels": [], "entities": [{"text": "RTMs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9306532740592957}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP).", "labels": [], "entities": [{"text": "parsing output", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.907271534204483}]}, {"text": " Table 2: Number of sentences in I (in thousands)  selected for each task.", "labels": [], "entities": []}, {"text": " Table 3.  ACC is entailment accuracy, r P is Pearson's corre- lation, r S is Spearman's correlation, MSE is mean  squared error, MAE is mean absolute error, and  RAE is relative absolute error. L uses the lem- matized corpora and R uses the true-cased corpora  corresponding to regular. R+L correspond to the  perspective using the features from both R and L,  which doubles the number of features. We com- pute the entailment by SVC.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9690549969673157}, {"text": "mean  squared error", "start_pos": 109, "end_pos": 128, "type": "METRIC", "confidence": 0.7758263746897379}, {"text": "MAE", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.9835458993911743}, {"text": "mean absolute error", "start_pos": 137, "end_pos": 156, "type": "METRIC", "confidence": 0.6968328058719635}, {"text": "RAE", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.9951006770133972}]}, {"text": " Table 3: SRE training results of the top 5 RTM  systems selected.", "labels": [], "entities": [{"text": "SRE training", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.5506086349487305}]}, {"text": " Table 5: CLSS training results of the top 3 RTM  systems for each subtask. Levels correspond to  paragraph to sentence (Par2S), sentence to phrase  (S2Phrase), and phrase to word (Phrase2W).", "labels": [], "entities": []}, {"text": " Table 5. RMSE is the root  mean squared error. As the compared text size de- crease, the performance decrease since it can be- come harder and more ambiguous to find the simi- larity using less context. RTM-DCU results on the  CLSS challenge test set are provided in", "labels": [], "entities": [{"text": "RMSE", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9856782555580139}, {"text": "root  mean squared error", "start_pos": 22, "end_pos": 46, "type": "METRIC", "confidence": 0.6803725734353065}, {"text": "CLSS challenge test set", "start_pos": 228, "end_pos": 251, "type": "DATASET", "confidence": 0.832908421754837}]}, {"text": " Table 6: RTM-DCU test results on CLSS for the  top 3 RTM systems for each subtask.", "labels": [], "entities": []}, {"text": " Table 7: RTM-DCU test results on CLSS.", "labels": [], "entities": [{"text": "RTM-DCU test", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.7093130648136139}]}, {"text": " Table 8: MSTS training results on the English, En- glish OnWN, and Spanish tasks.", "labels": [], "entities": [{"text": "MSTS", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9191291928291321}, {"text": "OnWN", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.6173644661903381}]}, {"text": " Table 9: RTM-DCU test results on MSTS for the  top 3 RTM systems for each subtask as well as  RTM results in STS 2013", "labels": [], "entities": [{"text": "STS 2013", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.8398235738277435}]}, {"text": " Table 11: RTM-DCU test results on MSTS Span- ish task. Rankings are calculated according to the  weighted Pearson's correlation.", "labels": [], "entities": [{"text": "MSTS Span- ish task", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.5983364105224609}, {"text": "Pearson's correlation", "start_pos": 107, "end_pos": 128, "type": "METRIC", "confidence": 0.9321914513905843}]}, {"text": " Table 10: RTM-DCU test results with ranks on MSTS English task.", "labels": [], "entities": [{"text": "RTM-DCU test", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.6034975349903107}, {"text": "MSTS English task", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.7987884680430094}]}, {"text": " Table 12: Best RTM-DCU RAE test results for different tasks and subtasks as well as STS 2013 re- sults (", "labels": [], "entities": [{"text": "RTM-DCU RAE", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.6597579419612885}, {"text": "STS 2013 re- sults", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.5151780664920806}]}]}