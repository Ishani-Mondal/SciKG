{"title": [{"text": "SemEval-2014 Task 2: Grammar Induction for Spoken Dialogue Systems", "labels": [], "entities": [{"text": "SemEval-2014 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8692262768745422}, {"text": "Grammar Induction", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7449613511562347}]}], "abstractContent": [{"text": "In this paper we present the SemEval-2014 Task 2 on spoken dialogue grammar induction.", "labels": [], "entities": [{"text": "spoken dialogue grammar induction", "start_pos": 52, "end_pos": 85, "type": "TASK", "confidence": 0.7686356827616692}]}, {"text": "The task is to classify a lexical fragment to the appropriate semantic category (grammar rule) in order to construct a grammar for spoken dialogue systems.", "labels": [], "entities": []}, {"text": "We describe four sub-tasks covering two languages, English and Greek, and three speech application domains , travel reservation, tourism and finance.", "labels": [], "entities": [{"text": "travel reservation", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7125954180955887}]}, {"text": "The classification results are compared against the groundtruth.", "labels": [], "entities": []}, {"text": "Weighted and unweighted precision, recall and f-measure are reported.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.995814859867096}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9997709393501282}, {"text": "f-measure", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9940037131309509}]}, {"text": "Three sites participated in the task with five systems, employing a variety of features and in some cases using external resources for training.", "labels": [], "entities": []}, {"text": "The submissions manage to significantly beat the baseline, achieving a f-measure of 0.69 in comparison to 0.56 for the base-line, averaged across all subtasks.", "labels": [], "entities": [{"text": "f-measure", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.996519923210144}]}], "introductionContent": [{"text": "This task aims to foster the application of computational models of lexical semantics to the field of spoken dialogue systems (SDS) for the problem of grammar induction.", "labels": [], "entities": [{"text": "spoken dialogue systems (SDS)", "start_pos": 102, "end_pos": 131, "type": "TASK", "confidence": 0.7011956771214803}, {"text": "grammar induction", "start_pos": 151, "end_pos": 168, "type": "TASK", "confidence": 0.7545175850391388}]}, {"text": "Grammars constitute a vital component of SDS representing the semantics of the domain of interest and allowing the system to correctly respond to a user's utterance.", "labels": [], "entities": [{"text": "SDS", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9871520400047302}]}, {"text": "The task has been developed in tight collaboration between the research community and commercial SDS grammar developers, under the auspices of the EU-IST PortDial project  project aims is to help automate the grammar development and localization process.", "labels": [], "entities": []}, {"text": "Unlike previous approaches () that have focused on full automation, PortDial adopts a human-in-the-loop approach were a developer bootstraps each grammar rule or request type with a few examples (use cases) and then machine learning algorithms are used to propose grammar rule enhancements to the developer.", "labels": [], "entities": []}, {"text": "The enhancements are post-edited by the developer and new grammar rule suggestions are proposed by the system, in an iterative fashion until a grammar of sufficient quality is achieved.", "labels": [], "entities": []}, {"text": "In this task, we focus on a snapshot of this process, where a portion of the grammar is already induced and post-edited by the developer and new candidate fragments are rolling in order to be classified to an existing rule (or rejected).", "labels": [], "entities": []}, {"text": "The goal is to develop machine learning algorithms for classifying candidate lexical fragments to the correct grammar rule (semantic category).", "labels": [], "entities": []}, {"text": "The task is equally relevant for both finite-state machine and statistical grammar induction.", "labels": [], "entities": [{"text": "statistical grammar induction", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.8380487561225891}]}, {"text": "In this task the semantic hierarchy of SDS grammars has two layers, namely, low-and highlevel.", "labels": [], "entities": [{"text": "SDS grammars", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.906732827425003}]}, {"text": "Low-level rules are similar to gazetteers referring to terminal concepts that can be as represented as sets of lexical entries.", "labels": [], "entities": []}, {"text": "For example, the concept of city name can be represented as <CITY> = (\"London\", \"Paris\", ...).", "labels": [], "entities": []}, {"text": "High-level rules are defined on top of low-level rules, while they can be lexicalized as textual fragments (or chunks), e.g., <TOCITY> = (\"fly to <CITY>\", ...).", "labels": [], "entities": []}, {"text": "Using the above examples the sentence \"I want to fly to Paris\" will be first parsed as \"I want to fly to <CITY>\" and finally as \"I want to <TOCITY>\".", "labels": [], "entities": []}, {"text": "In this task, we focus exclusively on high-level rule induction, assuming that the low-level rules are known.", "labels": [], "entities": [{"text": "rule induction", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.6906102150678635}]}, {"text": "The problem of fragment extraction and selection is simplified by investigating the binary classification of (already extracted) fragments into valid and non-valid.", "labels": [], "entities": [{"text": "fragment extraction", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8387318253517151}]}, {"text": "The task boils down mainly to a semantic similarity estimation problem for the assignment of valid fragments into high-level rules.", "labels": [], "entities": [{"text": "semantic similarity estimation", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.6081601679325104}]}], "datasetContent": [{"text": "We have provided four datasets, travel English, travel Greek, tourism English and finance English.", "labels": [], "entities": []}, {"text": "The travel domain grammar covers flight, car and hotel reservation utterances.", "labels": [], "entities": []}, {"text": "The tourism domain covers touristic information including accommodation, restaurants and movies.", "labels": [], "entities": []}, {"text": "The finance domain covers utterances of a bank client asking questions about his bank account as well as reporting problems.", "labels": [], "entities": []}, {"text": "In are presented typical examples of fragments for every subtask.", "labels": [], "entities": []}, {"text": "All grammars have been manually constructed by a grammar developer.", "labels": [], "entities": []}, {"text": "For the three English grammars, a small corpus (between 500 and 2000 sentences) was initially available.", "labels": [], "entities": []}, {"text": "The grammar developer first identified terminal concepts, which correspond to low-level rules.", "labels": [], "entities": []}, {"text": "Typical examples include city names for the travel domain, restaurant names for the tourism domain and credit card names in the finance domain.", "labels": [], "entities": []}, {"text": "After covering all low-level rules the grammar developer proceeded to identify high-level rules present in the corpus, like the departure city in the travel domain, or the user request type fora credit card.", "labels": [], "entities": []}, {"text": "The grammar developer was instructed to identify all rules present in the corpus, but also spend some effort to include rules not appearing in the corpus so that the resulting grammar better covers the domain at hand.", "labels": [], "entities": []}, {"text": "For the case of Greek travel grammar no corpus was initially available.", "labels": [], "entities": []}, {"text": "The Greek grammar was instead produced by manually translating the English one, accounting for the differences in syntax between the two languages.", "labels": [], "entities": []}, {"text": "The grammars have been developed as part of the PortDial FP7 project and are explained in detail in.", "labels": [], "entities": [{"text": "PortDial FP7 project", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.757675309975942}]}, {"text": "For the first three datasets that have been available from the beginning of the campaign we have split the release into train, development and test set.", "labels": [], "entities": []}, {"text": "For the finance domain which was announced when the test sets were released we only provided the train and test set, to simulate a resource poor scenario.", "labels": [], "entities": []}, {"text": "The statistics of the datasets for all language/domain pairs are given in.", "labels": [], "entities": []}, {"text": "In addition to the high-level rules we made available the low-level rules for each grammar, which although not used in the evaluation, can be useful for expanding the high-level rules to coverall lexicalizations expressed by the grammar.", "labels": [], "entities": []}, {"text": "For the evaluation of the task we have used precision, recall and f-measure, both weighted and unweighted.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9996143579483032}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9994592070579529}, {"text": "f-measure", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9673033952713013}]}, {"text": "If R j denotes the set of fragments for one rule and C j the set of fragments classified to this rule by a system then per-rule precision is computed by the equation: and per-rule recall by: Precision for all the J rules R j , 1 \u2264 j \u2264 J is computed by the following equation: In the unweighted case the weight w j has a fixed value for all rules, sow j = 1 J . Taking into account the fact that the rules are not balanced in terms of fragments, a better way to compute for the weight is w j = |R j | j |R j | . In the latter, weighted, case the total precision will better describe the results.", "labels": [], "entities": [{"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9768129587173462}, {"text": "recall", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.969070315361023}, {"text": "Precision", "start_pos": 191, "end_pos": 200, "type": "METRIC", "confidence": 0.9948773980140686}, {"text": "precision", "start_pos": 551, "end_pos": 560, "type": "METRIC", "confidence": 0.8712823390960693}]}, {"text": "Recall is similarly computed using the same weighting scheme as:", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7175496816635132}]}], "tableCaptions": [{"text": " Table 1: Example grammar fragments for each application domain.", "labels": [], "entities": []}, {"text": " Table 2: Number of rules in the training, development and test sets for each application domain.", "labels": [], "entities": []}, {"text": " Table 4: Weighted and unweighted precision, re- call and f-measure for all systems. Best perfor- mance per metric and dataset shown in bold.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9826326370239258}, {"text": "re- call", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9483514825503031}, {"text": "f-measure", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.8588969707489014}]}]}