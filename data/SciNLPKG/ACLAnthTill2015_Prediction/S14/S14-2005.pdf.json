{"title": [{"text": "SemEval-2014 Task 5: L2 Writing Assistant", "labels": [], "entities": [{"text": "SemEval-2014 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8799600303173065}]}], "abstractContent": [{"text": "We present anew cross-lingual task for SemEval concerning the translation of L1 fragments in an L2 context.", "labels": [], "entities": [{"text": "translation of L1 fragments", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.8781194537878036}]}, {"text": "The task is at the boundary of Cross-Lingual Word Sense Disambiguation and Machine Translation.", "labels": [], "entities": [{"text": "Cross-Lingual Word Sense Disambiguation", "start_pos": 31, "end_pos": 70, "type": "TASK", "confidence": 0.6671653836965561}, {"text": "Machine Translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.8215771913528442}]}, {"text": "It finds its application in the field of computer-assisted translation, particularly in the context of second language learning.", "labels": [], "entities": [{"text": "computer-assisted translation", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.7295946478843689}]}, {"text": "Translating L1 fragments in an L2 context allows language learners when writing in a target language (L2) to fallback to their native language (L1) whenever they are uncertain of the right word or phrase.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present anew cross-lingual and applicationoriented task for SemEval that is situated in the area where Word Sense Disambiguation and Machine Translation meet.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.7000007430712382}, {"text": "Machine Translation", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.8162116408348083}]}, {"text": "Finding the proper translation of a word or phrase in a given context is much like the problem of disambiguating between multiple senses.", "labels": [], "entities": []}, {"text": "In this task participants are asked to build a translation/writing assistance system that translates specifically marked L1 fragments in an L2 context to their proper L2 translation.", "labels": [], "entities": [{"text": "translation/writing assistance", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.8682916611433029}]}, {"text": "This type of translation can be applied in writing assistance systems for language learners in which users write in a target language, but are allowed to occasionally back off to their native L1 when they are uncertain of the proper lexical or grammatical form in L2.", "labels": [], "entities": []}, {"text": "The task concerns the NLP back-end rather than any user interface.", "labels": [], "entities": []}, {"text": "Full-on machine translation typically concerns the translation of complete sentences or texts from L1 to L2.", "labels": [], "entities": [{"text": "Full-on machine translation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7253133952617645}, {"text": "translation of complete sentences or texts from L1", "start_pos": 51, "end_pos": 101, "type": "TASK", "confidence": 0.7805004939436913}]}, {"text": "This task, in contrast, focuses on smaller fragments, side-tracking the problem of full word reordering.", "labels": [], "entities": [{"text": "full word reordering", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7032587031523386}]}, {"text": "We focus on the following language combinations of L1 and L2 pairs: English-German, English-Spanish, French-English and DutchEnglish.", "labels": [], "entities": []}, {"text": "Task participants could participate for all language pairs or any subset thereof.", "labels": [], "entities": []}], "datasetContent": [{"text": "Several metrics are available for automatic evaluation.", "labels": [], "entities": []}, {"text": "First, we measure the absolute accuracy a = c/n, where c is the number of fragment translations from the system output that precisely match the corresponding fragments in the reference translation, and n is the total number of translatable fragments, including those for which no translation was found.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9695467948913574}]}, {"text": "We also introduce a wordbased accuracy, which unlike the absolute accuracy gives some credits to mismatches that show partial overlap with the reference translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.8705966472625732}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.6499008536338806}]}, {"text": "It assigns a score according to the longest consecutive matching substring between output fragment and reference fragment and is computed as follows: wac = |longestsubmatch(output, ref erence)| max(|output|, |ref erence|) (1) The system with the highest word-based accuracy wins the competition.", "labels": [], "entities": [{"text": "ref erence)| max", "start_pos": 181, "end_pos": 197, "type": "METRIC", "confidence": 0.6898947507143021}, {"text": "accuracy", "start_pos": 265, "end_pos": 273, "type": "METRIC", "confidence": 0.9520717263221741}]}, {"text": "Systems may decide not to translate fragments if they cannot find a suitable translation.", "labels": [], "entities": []}, {"text": "A recall metric simply measures the number of fragments for which the system generated a translation, regardless of whether that translation is corrector not, as a proportion of the total number of fragments.", "labels": [], "entities": [{"text": "recall", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9809240102767944}]}, {"text": "In addition to these task-specific metrics, standard MT metrics such as BLEU, NIST, METEOR and error rates such as WER, PER and TER, are included in the evaluation script as well.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9572834372520447}, {"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9990363121032715}, {"text": "NIST", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.8145848512649536}, {"text": "METEOR", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9892576336860657}, {"text": "error rates", "start_pos": 95, "end_pos": 106, "type": "METRIC", "confidence": 0.9447283148765564}, {"text": "WER", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.969792902469635}, {"text": "PER", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.902601420879364}, {"text": "TER", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.9489759802818298}]}, {"text": "Scores such as BLEU will generally be high (> 0.95) when computed on the full sentence, as a large portion of the sentence is already translated and only a specific fragment remains to be evaluated.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9991255402565002}]}, {"text": "Nevertheless, these generic metrics are proven in our pilot study to follow the same trend as the more task-specific evaluation metrics, and will be omitted in the result section for brevity.", "labels": [], "entities": []}, {"text": "It regularly occurs that multiple translations are possible.", "labels": [], "entities": [{"text": "translations", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.9597646594047546}]}, {"text": "As stated, in the creation of the test set we have taken this into account by explicitly encoding valid alternatives.", "labels": [], "entities": []}, {"text": "A match with any alternative in the reference counts as a valid match.", "labels": [], "entities": []}, {"text": "For word accuracy, the highest word accuracy amongst all possible alternatives in the reference is taken.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.8472281098365784}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.7037203311920166}]}, {"text": "Likewise, participant system output may contain multiple alternatives as well, as we allowed two different types of runs, following the example of the Cross-Lingual Lexical Substitution and CrossLingual Word Sense Disambiguation tasks: \u2022 Best -The system may only output one, its best, translation; \u2022 Out of Five -The system may output up to five alternatives, effectively allowing 5 guesses.", "labels": [], "entities": [{"text": "CrossLingual Word Sense Disambiguation", "start_pos": 190, "end_pos": 228, "type": "TASK", "confidence": 0.5679763853549957}]}, {"text": "Only the best match is counted.", "labels": [], "entities": []}, {"text": "This metric does not count how many of the five are valid.", "labels": [], "entities": []}, {"text": "Participants could submit up to three runs per language pair and evaluation type.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. This table  shows the highest word accuracy achieved by the  participants, in which multiple system runs have  been aggregated. A ranking can quickly be dis- tilled from this, as the best score is marked in  bold. The system by the University of Edinburgh  emerges as the clear winner of the task. The full  results of the various system runs by the six par- ticipants are shown in", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.8583778142929077}]}, {"text": " Table 1: Highest word accuracy per team, per lan- guage pair, and per evaluation type (out-of-five is  include in the \"oof\" column). The best score in  each column is marked in bold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9385334253311157}]}, {"text": " Table 2: Full results for English-Spanish and  English-German.", "labels": [], "entities": []}, {"text": " Table 3: Full results for French-English and  Dutch-English.", "labels": [], "entities": []}]}