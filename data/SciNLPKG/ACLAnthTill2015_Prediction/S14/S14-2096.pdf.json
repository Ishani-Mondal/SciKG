{"title": [{"text": "SentiKLUE: Updating a Polarity Classifier in 48 Hours", "labels": [], "entities": []}], "abstractContent": [{"text": "SentiKLUE is an update of the KLUE polarity classifier-which achieved good and robust results in SemEval-2013 with a simple feature set-implemented in 48 hours.", "labels": [], "entities": []}], "introductionContent": [{"text": "The SemEval-2014 shared task on \"Sentiment Analysis in Twitter\" () is a rerun of the corresponding shared task from) with new test data.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.8437207341194153}]}, {"text": "It focuses on polarity classification in computermediated communication such as Twitter, other micro-blogging services, and SMS.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.7458183467388153}]}, {"text": "There are two subtasks: the goal of Message Polarity Classification (B) is to classify an entire SMS, tweet or other message as positive (pos), negative (neg) or neutral (ntr); in the subtask on Contextual Polarity Disambiguation (A), a single word or short phrase has to be classified in the context of the whole message.", "labels": [], "entities": [{"text": "Message Polarity Classification", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.6878676613171896}, {"text": "Contextual Polarity Disambiguation (A)", "start_pos": 195, "end_pos": 233, "type": "TASK", "confidence": 0.7643984456857046}]}, {"text": "The training data are the same as in SemEval-2013.", "labels": [], "entities": []}, {"text": "The test data from 2013 are used as a development set in order to select features and tune machine learning algorithms, but may not be included in the training data.", "labels": [], "entities": []}, {"text": "The 2014 test set comprises the development data, new Twitter messages, LiveJournal entries as out-of-domain data, and a small number of tweets containing sarcasm (see for further details).", "labels": [], "entities": [{"text": "2014 test set", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7632922132809957}]}, {"text": "For subtask B, there are 10,239 training items, 5,907 items in the development set, and 3,861 additional unseen items in the new test set.", "labels": [], "entities": []}, {"text": "For subtask A, there are 9,505 training items, 6,769 items in the development set, and 3,912 additional items in the test set.", "labels": [], "entities": []}, {"text": "Our team participated in the SemEval-2013 shared task with a relatively simple, but robust This work is licensed under a Creative Commons Attribution 4.0 International License.", "labels": [], "entities": [{"text": "SemEval-2013 shared task", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.8738496899604797}]}, {"text": "Page numbers and proceedings footer are added by the organisers.", "labels": [], "entities": []}, {"text": "Licence details: http: //creativecommons.org/licenses/by/4.0/ system (KLUE) based on a maximum entropy classifier and a small set of features (.", "labels": [], "entities": []}, {"text": "Despite its simplicity, KLUE performed very well in subtask B, ranking 5th out of 36 constrained systems on the Twitter data and 3rd out of 28 on the SMS data.", "labels": [], "entities": [{"text": "KLUE", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.688791036605835}, {"text": "Twitter data", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.7970654964447021}, {"text": "SMS data", "start_pos": 150, "end_pos": 158, "type": "DATASET", "confidence": 0.7169050127267838}]}, {"text": "Results for contextual polarity disambiguation (subtask A) were less encouraging, with rank 14 out of 21 constrained systems on the Twitter data and rank 12 out of 19 on the SMS data.", "labels": [], "entities": [{"text": "contextual polarity disambiguation", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.7317419250806173}, {"text": "Twitter data", "start_pos": 132, "end_pos": 144, "type": "DATASET", "confidence": 0.8428070247173309}, {"text": "SMS data", "start_pos": 174, "end_pos": 182, "type": "DATASET", "confidence": 0.856940895318985}]}, {"text": "This paper describes our efforts to bring the KLUE system up to date within a period of 48 hours.", "labels": [], "entities": [{"text": "KLUE system", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.7970075607299805}]}, {"text": "The results obtained by the new SentiKLUE system are summarised in, showing that the update was successful.", "labels": [], "entities": []}, {"text": "The ranking of the system has improved substantially in subtask A, making it one of the best-performing systems in the shared task.", "labels": [], "entities": []}, {"text": "Rankings in subtask B are similar to those of the previous year, showing that SentiKLUE has kept up with recent developments.", "labels": [], "entities": []}, {"text": "Moreover, differences to the best-performing systems are much smaller than in SemEval-2013.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to determine the importance of individual features, ablation experiments were carried out for both subtasks by deactivating one group of features at a time.", "labels": [], "entities": []}, {"text": "show the resulting changes in the official criterion F p/n separately for each subset of the development and test sets, as well as micro-averaged across the full development set (DEV) and test set (GOLD).", "labels": [], "entities": []}, {"text": "Rows are ordered by feature impact on the full gold standard.", "labels": [], "entities": []}, {"text": "Positive values indicate that a feature group has a negative impact on classification quality: results are improved by omitting the features (which is often the case for the Sarcasm subset).", "labels": [], "entities": []}, {"text": "The most important features are bag-of-words unigrams and bigrams, closely followed by sentiment lexica.", "labels": [], "entities": []}, {"text": "Training class weights had a strong positive impact in subtask B, but decreased performance in subtask A.", "labels": [], "entities": []}, {"text": "In our official submission, they were only used for subtask B.", "labels": [], "entities": []}, {"text": "Full-message polarity is the third most important feature in subtask A.", "labels": [], "entities": [{"text": "Full-message polarity", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8034783601760864}]}, {"text": "Other features contributed relatively small individual effects, but were necessary to achieve state-of-the-art performance in combination.", "labels": [], "entities": []}, {"text": "They are often specific to one of the subtasks or to a particular subset of the gold standard.", "labels": [], "entities": []}, {"text": "The bottom half of each table shows ablation results for individual sentiment lexica, with all other features active.", "labels": [], "entities": []}, {"text": "Key resources are the standard lexica (AFINN, Liu, MPQA) as well as Twitter-specific lexica (Sentiment140, NRC Hashtag).", "labels": [], "entities": [{"text": "AFINN", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.8443391919136047}, {"text": "NRC Hashtag)", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.8926764329274496}]}, {"text": "Noisy word lists (DSM extension, SNAP, SentiWords) have a small or even a negative effect.", "labels": [], "entities": []}, {"text": "Surprisingly, the standard lexica seem to give misleading cues on the Twitter 2014 subset", "labels": [], "entities": [{"text": "Twitter 2014 subset", "start_pos": 70, "end_pos": 89, "type": "DATASET", "confidence": 0.8104294538497925}]}], "tableCaptions": [{"text": " Table 1: SentiKLUE results in SemEval 2014  Task 9 (among constrained systems). See Rosen- thal et al. (2014) for further details and rankings  including the unconstrained systems.", "labels": [], "entities": [{"text": "SemEval 2014  Task 9", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.8188657015562057}]}, {"text": " Table 2: Performance of different machine learning algorithms on the training data (CV), development set  and test set (F all = weighted average F-score; F p/n = official score; best results highlighted in bold font).", "labels": [], "entities": [{"text": "F all = weighted average F-score", "start_pos": 121, "end_pos": 153, "type": "METRIC", "confidence": 0.8189276655515035}]}]}