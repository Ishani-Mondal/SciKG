{"title": [{"text": "Semantic Role Labelling with minimal resources: Experiments with French", "labels": [], "entities": [{"text": "Semantic Role Labelling", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8306330641110738}]}], "abstractContent": [{"text": "This paper describes a series of French semantic role labelling experiments which show that a small set of manually annotated training data is superior to a much larger set containing semantic role labels which have been projected from a source language via word alignment.", "labels": [], "entities": [{"text": "French semantic role labelling", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.5456785038113594}, {"text": "word alignment", "start_pos": 258, "end_pos": 272, "type": "TASK", "confidence": 0.7290055453777313}]}, {"text": "Using universal part-of-speech tags and dependencies makes little difference over the original fine-grained tagset and dependency scheme.", "labels": [], "entities": []}, {"text": "Moreover, there seems to be no improvement gained from projecting semantic roles between direct translations than between indirect translations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic role labelling (SRL) () is the task of identifying the predicates in a sentence, their semantic arguments and the roles these arguments take.", "labels": [], "entities": [{"text": "Semantic role labelling (SRL)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8366999924182892}]}, {"text": "The last decade has seen considerable attention paid to statistical SRL, thanks to the existence of two major hand-crafted resources for English, namely, FrameNet () and).", "labels": [], "entities": [{"text": "statistical SRL", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.5913659632205963}]}, {"text": "Apart from English, only a few languages have SRL resources and these resources tend to be of limited size compared to the English datasets.", "labels": [], "entities": [{"text": "SRL", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9402722716331482}]}, {"text": "French is one of those languages which suffer from a scarcity of hand-crafted SRL resources.", "labels": [], "entities": [{"text": "SRL", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9486925601959229}]}, {"text": "The only available gold-standard resource is a small set of 1000 sentences taken from Europarl () and manually annotated with Propbank verb predicates (van der.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.9858274459838867}, {"text": "Propbank", "start_pos": 126, "end_pos": 134, "type": "DATASET", "confidence": 0.9498672485351562}]}, {"text": "This dataset is then used by van der Plas et al. in French.", "labels": [], "entities": []}, {"text": "They additionally build a large, \"artificial\" or automatically labelled dataset of approximately 1M Europarl sentences by projecting the SRLs from English sentences to their French translations and use it for training an SRL system.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.95339435338974}, {"text": "SRL", "start_pos": 221, "end_pos": 224, "type": "TASK", "confidence": 0.9884109497070312}]}, {"text": "We build on the work of van derby answering the following questions: 1) How much artificial data is needed to train an SRL system?", "labels": [], "entities": [{"text": "SRL", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9692267179489136}]}, {"text": "2) Is it better to use direct translations than indirect translations, i.e. is it better to use for projection a source-target pair where the source represents the original sentence and the target represents its direct translation as opposed to a source-target pair where the source and target are both translations of an original sentence in a third language?", "labels": [], "entities": []}, {"text": "3) Is it better to use coarsegrained syntactic information (in the form of universal part-of-speech tags and universal syntactic dependencies) than to use fine-grained syntactic information?", "labels": [], "entities": []}, {"text": "We find that SRL performance levels off after only 5K training sentences obtained via projection and that direct translations are no more useful than indirect translations.", "labels": [], "entities": [{"text": "SRL", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9726487398147583}]}, {"text": "We also find that it makes very little difference to French SRL performance whether we use universal partof-speech tags and syntactic dependencies or more fine-grained tags and dependencies.", "labels": [], "entities": [{"text": "French SRL", "start_pos": 53, "end_pos": 63, "type": "TASK", "confidence": 0.5078553557395935}]}, {"text": "The surprising result that SRL performance levels off after just 5K training sentences leads us to directly compare the small hand-crafted set of 1K sentences to the larger artificial training set.", "labels": [], "entities": [{"text": "SRL", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9271332621574402}]}, {"text": "We use 5-fold cross-validation on the small dataset and find that the SRL performance is substantially higher (>10 F 1 in identification and classification) when the hand-crafted annotations are used.", "labels": [], "entities": [{"text": "SRL", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.5727156400680542}, {"text": "identification and classification", "start_pos": 122, "end_pos": 155, "type": "TASK", "confidence": 0.8344480991363525}]}], "datasetContent": [{"text": "We use the two datasets described in (van der Plas et al., 2011) and the delivery report of the Classic project (van der).", "labels": [], "entities": []}, {"text": "These are the gold standard set of 1K sentences which was annotated by manually identifying each verb predicate, finding its equivalent English frameset in PropBank and identifying and labelling its arguments based on the description of the frameset (henceforth known as Classic1K), and the synthetic dataset consisting of more than 980K sentences (henceforth known as Classic980K), which was created byword aligning an English-French parallel corpus (Europarl) using GIZA++ and projecting the French SRLs from the English SRLs via the word alignments.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 156, "end_pos": 164, "type": "DATASET", "confidence": 0.947482705116272}, {"text": "Classic980K", "start_pos": 369, "end_pos": 380, "type": "DATASET", "confidence": 0.9530194997787476}]}, {"text": "The joint syntactic-semantic parser described in () was used to produce the English SRLs and the dependency parses of the French side were produced using the ISBN parser described in.", "labels": [], "entities": [{"text": "English SRLs", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.728646844625473}]}, {"text": "We use LTH), a dependency-based SRL system, in all of our experiments.", "labels": [], "entities": []}, {"text": "This system was among the bestperforming systems in the CoNLL 2009 shared task and is straightforward to use.", "labels": [], "entities": [{"text": "CoNLL 2009 shared task", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.8625076413154602}]}, {"text": "It comes with a set of features tuned for each shared task language (English, German, Japanese, Spanish, Catalan, Czech, Chinese).", "labels": [], "entities": []}, {"text": "We compared the performance of the English and Spanish feature sets on French and chose the former due to its higher performance (by 1 F 1 point).", "labels": [], "entities": []}, {"text": "To evaluate SRL performance, we use the CoNLL 2009 shared task scoring script 1 , which assumes a semantic dependency between the argument and predicate and the predicate and a dummy root node and then calculates the precision (P), recall (R) and F 1 of identification of these dependencies and classification (labelling) of them.", "labels": [], "entities": [{"text": "SRL", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9923326969146729}, {"text": "CoNLL 2009 shared task scoring script 1", "start_pos": 40, "end_pos": 79, "type": "DATASET", "confidence": 0.9311138476644244}, {"text": "precision (P)", "start_pos": 217, "end_pos": 230, "type": "METRIC", "confidence": 0.9385141581296921}, {"text": "recall (R)", "start_pos": 232, "end_pos": 242, "type": "METRIC", "confidence": 0.9410685896873474}, {"text": "F 1", "start_pos": 247, "end_pos": 250, "type": "METRIC", "confidence": 0.9812277257442474}]}], "tableCaptions": [{"text": " Table 1: SRL performance using different syntactic parses with Classic 5K and 50K training sets", "labels": [], "entities": [{"text": "SRL", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9579277634620667}]}, {"text": " Table 2: Average scores of 5-fold cross-validation  with Classic 1K (1K), 5K (5K), 1K plus 5K  (1K+5K) and self-training with 1K seed and 5K  unlabeled data", "labels": [], "entities": []}, {"text": " Table 3: Average scores of 5-fold cross-validation  with Classic 1K (1K) and 5K (5K) using 200 sen- tences for training and 800 for testing at each fold", "labels": [], "entities": []}]}