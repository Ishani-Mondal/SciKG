{"title": [{"text": "CMUQ@Qatar:Using Rich Lexical Features for Sentiment Analysis on Twitter", "labels": [], "entities": [{"text": "CMUQ@Qatar", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8038757840792338}, {"text": "Sentiment Analysis", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.9490140974521637}]}], "abstractContent": [{"text": "In this paper, we describe our system for the Sentiment Analysis of Twitter shared task in SemEval 2014.", "labels": [], "entities": [{"text": "Sentiment Analysis of Twitter shared task", "start_pos": 46, "end_pos": 87, "type": "TASK", "confidence": 0.9324825008710226}, {"text": "SemEval 2014", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.5622554421424866}]}, {"text": "Our system uses an SVM classifier along with rich set of lexical features to detect the sentiment of a phrase within a tweet (Task-A) and also the sentiment of the whole tweet (Task-B).", "labels": [], "entities": []}, {"text": "We start from the lexical features that were used in the 2013 shared tasks, we enhance the underlying lexicon and also introduce new features.", "labels": [], "entities": []}, {"text": "We focus our feature engineering effort mainly on Task-A. Moreover, we adapt our initial framework and introduce new features for Task-B. Our system reaches weighted score of 87.11% in Task-A and 64.52% in Task-B. This places us in the 4th rank in the Task-A and 15th in the Task-B.", "labels": [], "entities": [{"text": "weighted score", "start_pos": 157, "end_pos": 171, "type": "METRIC", "confidence": 0.9238254427909851}]}], "introductionContent": [{"text": "With more than 500 million tweets sent per day, containing opinions and messages, Twitter 1 has become a gold-mine for organizations to monitor their brand reputation.", "labels": [], "entities": []}, {"text": "As more and more users post about products and services they use, Twitter becomes a valuable source of people's opinions and sentiments: what people can think about a product or a service, how positive they can be about it or what would people prefer the product to be like.", "labels": [], "entities": []}, {"text": "Such data can be efficiently used for marketing.", "labels": [], "entities": []}, {"text": "However, with the increasing amount of tweets posted on a daily basis, it is challenging and expensive to manually analyze them and locate the meaningful ones.", "labels": [], "entities": []}, {"text": "There has been a body of recent work to automatically learn the public sen- timents from tweets using natural language processing techniques).", "labels": [], "entities": []}, {"text": "However, the task of sentiment analysis of tweets in their free format is harder than that of any well-structured document.", "labels": [], "entities": [{"text": "sentiment analysis of tweets", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.9288113117218018}]}, {"text": "Tweet messages usually contain different kinds of orthographic errors such as the use of special and decorative characters, letter or word duplication, extra punctuation, as well as the use of special abbreviations.", "labels": [], "entities": [{"text": "letter or word duplication", "start_pos": 124, "end_pos": 150, "type": "TASK", "confidence": 0.6287253797054291}]}, {"text": "In this paper, we present our machine learning based system for sentiment analysis of Twitter shared task in SemEval 2014.", "labels": [], "entities": [{"text": "sentiment analysis of Twitter shared task", "start_pos": 64, "end_pos": 105, "type": "TASK", "confidence": 0.8623207410176595}]}, {"text": "Our system takes as input an arbitrary tweet and assigns it to one of the following classes that best reflects its sentiment: positive, negative or neutral.", "labels": [], "entities": []}, {"text": "While positive and negative tweets are subjective, neutral class encompasses not only objective tweets but also subjective tweets that does not contain any \"polar\" emotion.", "labels": [], "entities": []}, {"text": "Our classifier was developed as an undergrad course project but later pursued as a research topic.", "labels": [], "entities": []}, {"text": "Our training, development and testing experiments were performed on data sets published in.", "labels": [], "entities": []}, {"text": "Motivated with its performance, we participated in).", "labels": [], "entities": []}, {"text": "Our approach includes an extensive usage of offthe-shelf resources that have been developed for conducting NLP on social media text.", "labels": [], "entities": [{"text": "NLP on social media text", "start_pos": 107, "end_pos": 131, "type": "TASK", "confidence": 0.8129280924797058}]}, {"text": "Our original aim was enhancement of the task-A.", "labels": [], "entities": []}, {"text": "Moreover, we adapted our framework and introduced new features for task-B and participated in both shared tasks.", "labels": [], "entities": []}, {"text": "We reached an F-score of 83.3% in Task-A and an F-score of 65.57% in Task-B. That placed us in the 4th rank in the task-A and 15th rank in the task-B.", "labels": [], "entities": [{"text": "F-score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9992244243621826}, {"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9989678859710693}]}, {"text": "Our approach includes an extensive usage of off-the-shelf resources that have been developed for conducting NLP on social media text.", "labels": [], "entities": [{"text": "NLP on social media text", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.8073372602462768}]}, {"text": "That includes the Twitter Tokenizer and also the Twitter POS tagger, several sentiment analysis lexica and finally our own enhanced resources for special handling of Twitter-specific text.", "labels": [], "entities": [{"text": "sentiment analysis lexica", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.8815416296323141}]}, {"text": "Our original aim in introducing and evaluating many of the features was enhancement of the task-A.", "labels": [], "entities": []}, {"text": "Moreover, we adapted our framework and introduced new features for task-B and participated in both shared tasks.", "labels": [], "entities": []}, {"text": "We reached an F-score of 83.3% in Task-A and an F-score of 65.57% in Task-B. That placed us in the 4th rank in the task-A and 15th rank in the task-B.", "labels": [], "entities": [{"text": "F-score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9992244243621826}, {"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9989678859710693}]}], "datasetContent": [{"text": "In this section, we explain details of the data and the general settings for the different experiments we conducted.", "labels": [], "entities": []}, {"text": "We train and evaluate our classifier for both tasks with the training, development and testing datasets provided for the SemEval 2014 shared task.", "labels": [], "entities": [{"text": "SemEval 2014 shared task", "start_pos": 121, "end_pos": 145, "type": "TASK", "confidence": 0.7813096940517426}]}, {"text": "The size of the three datasets we use as well as their class distributions are illustrated in . It is important to note that the total dataset size for training and development set (10,586) is about the same as test set making the learning considerably challenging for correct predictions.", "labels": [], "entities": []}, {"text": "Positive instances covered more than half of each dataset for Task-A while Neutral were the most popular class for Task-B. The class distribution of training set is the same as the test set.: F-scores obtained on the test sets with the specific feature removed.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9933996200561523}]}, {"text": "The test dataset is composed of five different sets: Twitter2013 a set of tweets collected for the SemEval2013 test set, Twitter2014, tweets collected for this years version, LiveJournal2014 consisting of formal tweets, SMS2013, a collection of sms messages,TwitterSarcasm, a collection of sarcastic tweets.", "labels": [], "entities": [{"text": "SemEval2013 test set", "start_pos": 99, "end_pos": 119, "type": "DATASET", "confidence": 0.7804208993911743}]}, {"text": "The results of our system are shown in.", "labels": [], "entities": []}, {"text": "The top five rows shows the results by the SemEval scorer for all the data sets used by them.", "labels": [], "entities": []}, {"text": "This scorer took the average of F1-score of only positive and negative classes.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9995504021644592}]}, {"text": "The last row shows the weighted average score of all the scores for Task A and B from the different data sets.", "labels": [], "entities": [{"text": "weighted average score", "start_pos": 23, "end_pos": 45, "type": "METRIC", "confidence": 0.748116691907247}]}, {"text": "Our scores for Task-A and Task-B were 83.45 and 65.53 respectively for Twitter 2014.", "labels": [], "entities": []}, {"text": "Our system performed better on Twitter and SMS test sets from 2013.", "labels": [], "entities": []}, {"text": "This was reasonable since we tuned our system on these datasets.", "labels": [], "entities": []}, {"text": "On the other hand, the system performed worst on sarcasm test set.", "labels": [], "entities": [{"text": "sarcasm test set", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.8447575171788534}]}, {"text": "This drop is extremely evident in Task-B where the results were dropped by 25%.", "labels": [], "entities": []}, {"text": "To analyze the effects of each step of our system, we experimented with our system using different configurations.", "labels": [], "entities": []}, {"text": "The results are shown in Table 4 and our analysis is described in the following subsections.", "labels": [], "entities": []}, {"text": "The results were scored by SemEval 2014 scorer and we took the weighted average of all data sets to accurately reflect the performance of our system.", "labels": [], "entities": [{"text": "SemEval 2014 scorer", "start_pos": 27, "end_pos": 46, "type": "DATASET", "confidence": 0.785447915395101}]}, {"text": "We show the polarities values assigned to each token of a tweet by our classifier, in.: Polarity assigned using our classifier to each word of a Tweet message.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Class size distribution for all the three sets for both Task-A and Task-B.", "labels": [], "entities": []}, {"text": " Table 3: F1 measures and final results of the sys- tem for Task-A and Task-B for all the data sets  including the weighted average of the sets.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.998197615146637}]}, {"text": " Table 4: F-scores obtained on the test sets with the specific feature removed.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981204867362976}]}]}