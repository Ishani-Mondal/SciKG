{"title": [{"text": "UoW: Multi-task Learning Gaussian Process for Semantic Textual Similarity", "labels": [], "entities": [{"text": "UoW", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8240469098091125}, {"text": "Semantic Textual Similarity", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.6073490182558695}]}], "abstractContent": [{"text": "We report results obtained by the UoW method in SemEval-2014's Task 10-Multilingual Semantic Textual Similarity.", "labels": [], "entities": [{"text": "SemEval-2014's Task 10-Multilingual Semantic Textual Similarity", "start_pos": 48, "end_pos": 111, "type": "TASK", "confidence": 0.7727098039218357}]}, {"text": "We propose to model Semantic Textual Similarity in the context of Multi-task Learning in order to deal with inherent challenges of the task such as unbalanced performance across domains and the lack of training data for some domains (i.e. unknown domains).", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.7421475450197855}]}, {"text": "We show that the Multi-task Learning approach outperforms previous work on the 2012 dataset, achieves a robust performance on the 2013 dataset and competitive results on the 2014 dataset.", "labels": [], "entities": []}, {"text": "We highlight the importance of the challenge of unknown domains, as it affects overall performance substantially.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of Semantic Textual Similarity (STS) () is aimed at measuring the degree of semantic equivalence between a pair of texts.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.7935883303483328}]}, {"text": "Natural Language Processing (NLP) applications such as Question Answering), Text Summarisation ( and Information Retrieval () rely heavily on the ability to measure semantic similarity between pairs of texts.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8260180056095123}, {"text": "Text Summarisation", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7785980403423309}, {"text": "Information Retrieval", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.7172538936138153}]}, {"text": "The STS evaluation campaign provides datasets that consist of pairs of sentences from different NLP domains such as paraphrasing, video paraphrasing, and machine translation (MT) evaluation.", "labels": [], "entities": [{"text": "machine translation (MT) evaluation", "start_pos": 154, "end_pos": 189, "type": "TASK", "confidence": 0.8531373739242554}]}, {"text": "The participating systems are required to predict a graded similarity score from 0 to 5, where a score of 0 means that the two sentences are on different topics and This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": []}, {"text": "Page numbers and proceedings footer are added by the organisers.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by/4.0/ a score of 5 means that the two sentences have exactly the same meaning.", "labels": [], "entities": []}, {"text": "Methods for STS are commonly based on computing various types of similarity metrics between the pair of sentences, where the similarity scores are used as features to train regression algorithms.", "labels": [], "entities": [{"text": "STS", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.985527753829956}]}, {"text": "use similarity metrics of varying complexity.", "labels": [], "entities": []}, {"text": "The range of features goes from simple string similarity metrics to complex vector space models.", "labels": [], "entities": []}, {"text": "The method yielded the best average results based on the official evaluation metrics, despite not having achieved the best results in all individual domains.", "labels": [], "entities": []}, {"text": "\u02c7 Sari\u00b4c use a similar setup, extracting features from similarity metrics, where these features are based on wordoverlap and syntax similarity.", "labels": [], "entities": []}, {"text": "The method was among the best for domains related to paraphrasing.", "labels": [], "entities": []}, {"text": "It also achieved a high correlation between the training and test data.", "labels": [], "entities": []}, {"text": "In contrast, for the machine translation data the performance in the test set was lower than the one over the training data.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7755056619644165}]}, {"text": "A possible reason for the poor results on this domain is the difference in length between the training and test sentences, as in the test data the pairs tend to be short and share similar words.", "labels": [], "entities": []}, {"text": "\u02c7 Sari\u00b4c claim that these differences show that the MT training data is not representative of the test set given their choice of features.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.968475341796875}]}, {"text": "Most of the participating systems in the STS challenges achieve good results on certain domains (i.e. STS datasets), but poor results on others.", "labels": [], "entities": [{"text": "STS datasets", "start_pos": 102, "end_pos": 114, "type": "DATASET", "confidence": 0.8237894475460052}]}, {"text": "Even the most robust methods still show a big gap in performances for different datasets.", "labels": [], "entities": []}, {"text": "In the second evaluation campaign of STS anew challenge was proposed: domains for which no training sets are provided, but only test sets.", "labels": [], "entities": []}, {"text": "propose to incorporate domain adaptation techniques for STS to generalise models to new domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.6958923786878586}, {"text": "STS", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9735485315322876}]}, {"text": "They add new features into the model, where the feature set contains domain specific features plus general task features.", "labels": [], "entities": []}, {"text": "The machine learning algorithm infers the extra weights of each specific domain and of the general domain.", "labels": [], "entities": []}, {"text": "When an instance of a specific domain is to be predicted, only the copy of the features of that domain will be active; if the domain is unknown, the general features will be active.", "labels": [], "entities": []}, {"text": "propose to use meta-classification to cope with domain adaptation.", "labels": [], "entities": []}, {"text": "They merge each pair into a single text and extract meta-features such as bag-ofwords and syntactic similarity scores.", "labels": [], "entities": []}, {"text": "The metaclassification model predicts, for each instance, its most likely domain based on these features.", "labels": [], "entities": []}, {"text": "A possible solution to alleviate unbalanced performances on different domains is to model STS in the context of Multi-task Learning (MTL).", "labels": [], "entities": [{"text": "STS", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9433820843696594}, {"text": "Multi-task Learning (MTL)", "start_pos": 112, "end_pos": 137, "type": "TASK", "confidence": 0.6296571016311645}]}, {"text": "The motivation behind MTL is that by learning multiple related tasks simultaneously the model performance may improve compared to the case where the tasks are learnt separately.", "labels": [], "entities": [{"text": "MTL", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.974785327911377}]}, {"text": "MTL is based on the assumption that related tasks can be clustered and inter-task correlations between tasks within the same cluster can be transferred.", "labels": [], "entities": [{"text": "MTL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9248688817024231}]}, {"text": "We propose to model STS using MTL based on a state-of-the-art STS feature set ( \u02c7 Sari\u00b4c).", "labels": [], "entities": [{"text": "STS", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9698916077613831}]}, {"text": "As algorithm we use a non-parametric Bayesian approach, namely Gaussian Processes (GP)).", "labels": [], "entities": []}, {"text": "We show that the MTL model outperforms previous work on the 2012 datasets and leads to robust performance on the 2013 datasets.", "labels": [], "entities": [{"text": "MTL", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.6568853855133057}]}, {"text": "On the STS 2014 challenge, our method shows competitive results.", "labels": [], "entities": [{"text": "STS 2014 challenge", "start_pos": 7, "end_pos": 25, "type": "DATASET", "confidence": 0.8157483339309692}]}], "datasetContent": [{"text": "We apply MTL to cope with the challenge of unbalanced performances across domains and unknown domains present in the STS datasets.", "labels": [], "entities": [{"text": "MTL", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.864078938961029}, {"text": "STS datasets", "start_pos": 117, "end_pos": 129, "type": "DATASET", "confidence": 0.6688790619373322}]}], "tableCaptions": [{"text": " Table 1: Comparison with previous work on the STS 2012 test datasets.", "labels": [], "entities": [{"text": "STS 2012 test datasets", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.8996737003326416}]}, {"text": " Table 3.2, we show the comparison of the  MTL-GP and the sparse MTL-GP with the best  2014 system (DLSCU run2). For both MTL meth- ods we match the 2014 domains with the train- ing domain headlines. For the sparse MTL-GP,  we chose empirically a number m of 500 ran- domly induced points. For reference, the corre- lation of sparse MTL-GP with 50 points on deft- forum is r=0.4691 obtained in 0.23 hours, with  100 points, r=0.4895, with 500 points, r=0.4912,  and with 1000 points, r=0.4911. The sparse MTL-", "labels": [], "entities": []}, {"text": " Table 2: Matching of new 2013 domains with  2012 training data.", "labels": [], "entities": []}, {"text": " Table 3: Comparison between best matching MTL-GP (MSRvid) and previous work on the STS 2013  test datasets.", "labels": [], "entities": [{"text": "STS 2013  test datasets", "start_pos": 84, "end_pos": 107, "type": "DATASET", "confidence": 0.8223627656698227}]}, {"text": " Table 4: Official English STS 2014 results.", "labels": [], "entities": [{"text": "Official English STS 2014 results", "start_pos": 10, "end_pos": 43, "type": "DATASET", "confidence": 0.8723626017570496}]}, {"text": " Table 6: Official Spanish STS 2014 results.", "labels": [], "entities": [{"text": "Official Spanish STS 2014 results", "start_pos": 10, "end_pos": 43, "type": "DATASET", "confidence": 0.8028922200202941}]}, {"text": " Table 3.3 shows the comparison of the best  Spanish STS 2014 system (UMCC DLSI run2)  against two different sparse MTL-GP matched  with the Spanish trial with 500 inducing points.  Sparse MTL-GP run1 uses the sparse features de- scribed above, while run2 uses a modification of  the feature set consisting in specific features for  each type of domain. For the English domains  the simple features are set to 0, and for Spanish  the SoA are still set to 0. The difference between  sparse MTL-GP models is very small, where the  use of all the features on the English domains im- proves the results. However, the performance of  both models is still substantially lower than that of  the best system.", "labels": [], "entities": [{"text": "UMCC DLSI run2", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.9033008615175883}]}, {"text": " Table 5: Comparison between best matching MTL-GP (headlines), Sparse MTL-GP and best STS 2014  system.", "labels": [], "entities": []}]}