{"title": [{"text": "HulTech: A General Purpose System for Cross-Level Semantic Similarity based on Anchor Web Counts", "labels": [], "entities": [{"text": "Cross-Level Semantic Similarity", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.5866129299004873}, {"text": "Anchor Web", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.9161148965358734}]}], "abstractContent": [{"text": "This paper describes the HULTECH team participation in Task 3 of SemEval-2014.", "labels": [], "entities": []}, {"text": "Four different subtasks are provided to the participants , who are asked to determine the semantic similarity of cross-level test pairs: paragraph-to-sentence, sentence-to-phrase, phrase-to-word and word-to-sense.", "labels": [], "entities": []}, {"text": "Our system adopts a unified strategy (general purpose system) to calculate similarity across all subtasks based on word Web frequencies.", "labels": [], "entities": []}, {"text": "For that purpose, we define ClueWeb InfoSimba, a cross-level similarity corpus-based metric.", "labels": [], "entities": []}, {"text": "Results show that our strategy overcomes the proposed base-lines and achieves adequate to moderate results when compared to other systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Similarity between text documents is considered a challenging task.", "labels": [], "entities": [{"text": "Similarity between text documents", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8567428588867188}]}, {"text": "Recently, many works concentrate on the study of semantic similarity for multi-level text documents (), but skipping the crosslevel similarity task.", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.701375350356102}]}, {"text": "In the later, the underlying idea is that text similarity can be considered between pairs of text documents at different granularities levels: paragraph, sentence, phrase or word.", "labels": [], "entities": []}, {"text": "One obvious particularity of this task is that text pairs may not share the same characteristics of size, context or structure, i.e., the granularity level.", "labels": [], "entities": []}, {"text": "In task 3 of SemEval-2014, two different strategies have been proposed to solve this issue.", "labels": [], "entities": []}, {"text": "On the one hand, participants may propose a combination of individual systems, each one solving a particular subtask.", "labels": [], "entities": []}, {"text": "On the other hand, a general purpose system maybe proposed, which deals with all the subtasks following the exact same strategy.", "labels": [], "entities": []}, {"text": "In this paper, we describe a language-independent corpus-based general purpose system, which relies on a huge freely available Web collection called AnchorClueWeb12 ( tic similarity based on word-word frequencies.", "labels": [], "entities": []}, {"text": "Indeed, these frequencies are captured by the use of a collocation metric called SCP 2 (), which has similar properties as the well studied PMI-IR (Turney, 2001) but does not over-evaluate rare events.", "labels": [], "entities": []}, {"text": "Our system outputs a normalized (between 0 and 1) similarity value between two pieces of texts.", "labels": [], "entities": []}, {"text": "However, the subtasks proposed in task 3 of SemEval-2014 include a different scoring scale between 0 and 4.", "labels": [], "entities": []}, {"text": "To solve this issue, we applied linear, polynomial and exponential regressions as three different runs.", "labels": [], "entities": []}, {"text": "Results show that our strategy overcomes the proposed baselines and achieves adequate to moderate results when compared to other systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Anchor ClueWeb12 dataset contains 0.5 billion Web pages, which cover about 64% of the total number of Web pages in ClueWeb12.", "labels": [], "entities": [{"text": "Anchor ClueWeb12 dataset", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.9268653591473898}]}, {"text": "The particularity of Anchor ClueWeb12 is that each Web page is represented by the anchor texts of the links pointing to it in ClueWeb12.", "labels": [], "entities": []}, {"text": "Web pages are indexed not on their content but on their references.", "labels": [], "entities": []}, {"text": "As such, the size of the index is drastically reduced and the overall results are consistent with full text indexing as discussed in.", "labels": [], "entities": []}, {"text": "For development purposes, this dataset was indexed in Solr 4.4 on a desktop computer using a batch indexing script.", "labels": [], "entities": []}, {"text": "Particularly, each compressed part file of the Anchor ClueWeb12 was uncompressed, preprocessed and indexed in a sequential way using the features of incremental indexing offered by).", "labels": [], "entities": [{"text": "Anchor ClueWeb12", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.9174590110778809}]}, {"text": "For evaluation purposes, two metrics have been selected by the organizers: Pearson correlation (Pearson, 1895) and Spearman's rank correlation.", "labels": [], "entities": [{"text": "Pearson correlation (Pearson, 1895)", "start_pos": 75, "end_pos": 110, "type": "METRIC", "confidence": 0.8390101620129177}, {"text": "Spearman's rank correlation", "start_pos": 115, "end_pos": 142, "type": "METRIC", "confidence": 0.7424797713756561}]}, {"text": "Detailed information about the evaluation setup can be found in the task discussion paper ().", "labels": [], "entities": []}, {"text": "All results are given in for each run.", "labels": [], "entities": []}, {"text": "Note that the baseline metric is calculated for the longest common string (LCS) and that each regression has been tuned on the training dataset for each one of the four tasks.", "labels": [], "entities": [{"text": "longest common string (LCS)", "start_pos": 52, "end_pos": 79, "type": "METRIC", "confidence": 0.619293729464213}]}, {"text": "First, in almost all cases, the results outperform the baseline.", "labels": [], "entities": []}, {"text": "Second, performances show that with a certain amount of information (longer pieces of texts), interesting results can be obtained.", "labels": [], "entities": []}, {"text": "However, when the size decreases, the performance diminishes and extra information is certainly needed to better capture the semantics between two pieces of text.", "labels": [], "entities": []}, {"text": "Third, the polynomial regression provides better results for the Pearson correlation evaluation, while for the Rho test, linear and polynomial regressions get the lead.", "labels": [], "entities": []}, {"text": "Note that this situation depends on the data distribution and cannot be seen as a conclusive remark.", "labels": [], "entities": []}, {"text": "However, it is certainly an important subject of study for our unsupervised methodology.", "labels": [], "entities": []}, {"text": "Another key point is that training examples were used only for evaluation purposes . In the case of Spearman's rank correlation, the linear and exponen-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overall results for the Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 34, "end_pos": 53, "type": "METRIC", "confidence": 0.8216083943843842}]}, {"text": " Table 2: Overall results for the Spearman's rank correlation.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 34, "end_pos": 61, "type": "METRIC", "confidence": 0.5449085310101509}]}]}