{"title": [{"text": "tucSage: Grammar Rule Induction for Spoken Dialogue Systems via Probabilistic Candidate Selection", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe the grammar induction system for Spoken Dialogue Systems (SDS) submitted to SemEval'14: Task 2.", "labels": [], "entities": [{"text": "Spoken Dialogue Systems (SDS)", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.793065090974172}, {"text": "SemEval'14: Task 2", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.854921281337738}]}, {"text": "A statistical model is trained with a rich feature set and used for the selection of candidate rule fragments.", "labels": [], "entities": []}, {"text": "Posterior probabilities produced by the fragment selection model are fused with estimates of phrase-level similarity based on lexical and con-textual information.", "labels": [], "entities": []}, {"text": "Domain and language portability are among the advantages of the proposed system that was experimentally validated for three thematically different domains in two languages.", "labels": [], "entities": [{"text": "language portability", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.6866603344678879}]}], "introductionContent": [{"text": "A critical task for Spoken Dialogue Systems (SDS) is the understanding of the transcribed user input, that utilizes an underlying domain grammar.", "labels": [], "entities": [{"text": "Spoken Dialogue Systems (SDS)", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.8635254502296448}]}, {"text": "An obstacle to the rapid deployment of SDS to new domains and languages is the time-consuming development of grammars that require human expertise.", "labels": [], "entities": [{"text": "SDS", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9634572267532349}]}, {"text": "Machine-assisted grammar induction has been an open research area for decades (K. aiming to lower this barrier.", "labels": [], "entities": [{"text": "Machine-assisted grammar induction", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7461872100830078}]}, {"text": "Induction algorithms can be broadly distinguished into resource-based, e.g.,), and data-driven, e.g., (H. Meng and K.-C.).", "labels": [], "entities": []}, {"text": "The main drawback of the resource-based paradigm is the requirement of pre-existing knowledge bases.", "labels": [], "entities": []}, {"text": "This is addressed by the data-driven paradigm that relies (mostly) on plain corpora. are similar to gazetteers consisting of terminal entries, e.g., list of city names.", "labels": [], "entities": []}, {"text": "High-level rules can be lexicalized as textual fragments (or chunks), which are semantically defined on top of lowlevel rules, e.g., 'depart from <City>'.", "labels": [], "entities": []}, {"text": "The data-driven induction of low-level rules is a well-researched area enabled by various technologies including web harvesting for corpora creation (), term extraction (K. Frantzi and S. Ananiadou, 1997), word-level similarity computation () and clustering (E..", "labels": [], "entities": [{"text": "term extraction", "start_pos": 153, "end_pos": 168, "type": "TASK", "confidence": 0.7870731949806213}, {"text": "word-level similarity computation", "start_pos": 206, "end_pos": 239, "type": "TASK", "confidence": 0.7266887426376343}]}, {"text": "High-level rule induction is a less researched area that poses two main challenges: 1) the extraction and selection of salient candidate fragments from a corpus that convey semantics relevant to the domain of interests and 2) the organization of such fragments (e.g., via clustering) according to their semantic similarity.", "labels": [], "entities": [{"text": "High-level rule induction", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6123467683792114}]}, {"text": "Despite the recent interest on phrase (J. and sentence similarity, each respective problem remains open.", "labels": [], "entities": [{"text": "phrase (J. and sentence similarity", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.7183266878128052}]}, {"text": "Next, our submission 1 for the SemEval'14: Task2 is briefly described, which constitutes a data-driven approach for inducing high-level SDS grammar rules.", "labels": [], "entities": [{"text": "SemEval'14: Task2", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8525765736897787}, {"text": "SDS grammar rules", "start_pos": 136, "end_pos": 153, "type": "TASK", "confidence": 0.8989609877268473}]}, {"text": "At the system's core lies a statistical model for the selection of textual fragments based on a rich set of features.", "labels": [], "entities": []}, {"text": "This set includes various lexical features, augmented with statistics from n-gram language models, as well as with heuristic features.", "labels": [], "entities": []}, {"text": "The candidate selection model posterior is fused with a phrase-level semantic similarity metric.", "labels": [], "entities": []}, {"text": "Two different approaches are used for similarity computation relying on the overlap of character bigrams or context-based similarity according to the distributional hypothesis of meaning.", "labels": [], "entities": [{"text": "similarity computation", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7708882987499237}]}, {"text": "The domain and language portability of the proposed system is demonstrated by its successful application across three different domains and two languages.", "labels": [], "entities": []}, {"text": "All the four subtasks defined by the organizers were completed with very good performance that exceeds the baseline.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data was organized with respect to three different domains: 1) air travel (flight booking, car rental etc.), 2) tourism (information for city guide), and 3) finance (currency exchange).", "labels": [], "entities": []}, {"text": "In total, there are four separate datasets: two datasets for the air travel domain in English (EN) and Greek (GR), one dataset for the tourism domain in English, and one dataset for the finance domain in English.", "labels": [], "entities": []}, {"text": "The size of the used corpora are presented, while the process of corpus creation is detailed in ().", "labels": [], "entities": []}, {"text": "The classifiers used for the candidate selection module, described in Section 2.1 were random forests with 50 trees (L.).", "labels": [], "entities": []}, {"text": "The proposed model defined by (1) was evaluated in terms of weighted F-measure, (FM ).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9338172078132629}, {"text": "FM", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.8803595304489136}]}, {"text": "Initially, we run our system using the training and development set provided by the task organizers, in order to tune thew and K parameters.", "labels": [], "entities": []}, {"text": "The tuning was conducted on the Travel English domain, while the respective evaluation results are shown in in terms of FM . We observe that the best reWeight w 0 1 50 500 FM 0.68 0.72 0.70 0.72: Results for the tuning of w. sults are achieved for w = 1 and w = 500.", "labels": [], "entities": [{"text": "Travel English domain", "start_pos": 32, "end_pos": 53, "type": "DATASET", "confidence": 0.9559330344200134}, {"text": "FM", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.9899278283119202}, {"text": "reWeight", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9378623962402344}, {"text": "FM", "start_pos": 172, "end_pos": 174, "type": "METRIC", "confidence": 0.9315521717071533}]}, {"text": "In the case where w = 0 the rule mapping relies only on the similarity metric.", "labels": [], "entities": []}, {"text": "In addition, we experimented with various values the context window size K of the context-based similarity metric SK : K = 1, 3, 7.", "labels": [], "entities": []}, {"text": "For all values of K similar performance was obtained (0.70  tioned tuning the following values were selected for the official runs: w = 1, w = 500 and K = 1.", "labels": [], "entities": []}, {"text": "In total, three system runs were submitted: Run 1.", "labels": [], "entities": []}, {"text": "The character bigram similarity metric was used, while w was set to 1.", "labels": [], "entities": []}, {"text": "The context-based similarity metrics was used with K = 1, while w was set to 1.", "labels": [], "entities": []}, {"text": "The character bigram similarity metric was used, while w was set to 500.", "labels": [], "entities": [{"text": "character bigram similarity metric", "start_pos": 4, "end_pos": 38, "type": "METRIC", "confidence": 0.5154330134391785}]}, {"text": "The results for the aforementioned runs, along with the baseline performance are shown in Table 4.", "labels": [], "entities": []}, {"text": "An overview of the participating systems suggests that our submission achieved the highest performance for almost all domains and languages.", "labels": [], "entities": []}, {"text": "The weighted (WA) and unweighted (UA) average across the 4 datasets are also presented, where the weight depends on the number of rules in the dataset.", "labels": [], "entities": [{"text": "unweighted (UA) average", "start_pos": 22, "end_pos": 45, "type": "METRIC", "confidence": 0.7552022933959961}]}, {"text": "Using these measures, our main run (Run 1) obtained the best results.", "labels": [], "entities": []}, {"text": "We observe that the performance is consistently worse for Runs 2 and 3, with the exception of the Travel English dataset.", "labels": [], "entities": [{"text": "Travel English dataset", "start_pos": 98, "end_pos": 120, "type": "DATASET", "confidence": 0.9521719813346863}]}, {"text": "Comparing the performance of Runs 1 and 2, we observe that the character bigram metric consistently outperforms the context-based one.", "labels": [], "entities": []}, {"text": "For individual datasets, our system underperforms for the Finance (in Run 3) and the Tourism domain (in all Runs).", "labels": [], "entities": [{"text": "Finance", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.8509206175804138}]}, {"text": "For the case of the Finance domain this maybe attributed to the relatively limited training data.", "labels": [], "entities": [{"text": "Finance domain", "start_pos": 20, "end_pos": 34, "type": "DATASET", "confidence": 0.790155678987503}]}], "tableCaptions": [{"text": " Table 1: Number of rules and train/test fragments.", "labels": [], "entities": []}, {"text": " Table 2: Size of corpora used in S K metric.", "labels": [], "entities": []}]}