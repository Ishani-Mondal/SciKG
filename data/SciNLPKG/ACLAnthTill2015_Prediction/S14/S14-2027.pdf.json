{"title": [{"text": "CMU: Arc-Factored, Discriminative Semantic Dependency Parsing", "labels": [], "entities": [{"text": "CMU", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8832703828811646}, {"text": "Parsing", "start_pos": 54, "end_pos": 61, "type": "TASK", "confidence": 0.6373115181922913}]}], "abstractContent": [{"text": "We present an arc-factored statistical model for semantic dependency parsing, as defined by the SemEval 2014 Shared Task 8 on Broad-Coverage Semantic Dependency Parsing.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.7614602247873942}, {"text": "SemEval 2014 Shared Task 8 on Broad-Coverage Semantic Dependency Parsing", "start_pos": 96, "end_pos": 168, "type": "TASK", "confidence": 0.75644151866436}]}, {"text": "Our entry in the open track placed second in the competition.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of broad coverage semantic dependency parsing aims to provide a shallow semantic analysis of text not limited to a specific domain.", "labels": [], "entities": [{"text": "broad coverage semantic dependency parsing", "start_pos": 12, "end_pos": 54, "type": "TASK", "confidence": 0.7862123131752015}]}, {"text": "As distinct from deeper semantic analysis (e.g., parsing to a full lambda-calculus logical form), shallow semantic parsing captures relationships between pairs of words or concepts in a sentence, and has wide application for information extraction, knowledge base population, and question answering (among others).", "labels": [], "entities": [{"text": "shallow semantic parsing captures relationships between pairs of words or concepts in a sentence", "start_pos": 98, "end_pos": 194, "type": "TASK", "confidence": 0.7834686189889908}, {"text": "information extraction", "start_pos": 225, "end_pos": 247, "type": "TASK", "confidence": 0.8063541650772095}, {"text": "question answering", "start_pos": 280, "end_pos": 298, "type": "TASK", "confidence": 0.8774935603141785}]}, {"text": "We present here two systems that produce semantic dependency parses in the three formalisms of the SemEval 2014 Shared Task 8 on Broad-Coverage Semantic Dependency Parsing ().", "labels": [], "entities": [{"text": "semantic dependency parses", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.6350328226884207}, {"text": "SemEval 2014 Shared Task 8 on Broad-Coverage Semantic Dependency Parsing", "start_pos": 99, "end_pos": 171, "type": "TASK", "confidence": 0.8080767154693603}]}, {"text": "These systems generate parses by extracting features for each potential dependency arc and learning a statistical model to discriminate between good arcs and bad; the first treats each labeled edge decision as an independent multiclass logistic regression ( \u00a73.2.1), while the second predicts arcs as part of a graph-based structured support vector machine ( \u00a73.2.2).", "labels": [], "entities": []}, {"text": "Common to both models is a rich set of features on arcs, described in \u00a73.2.3.", "labels": [], "entities": []}, {"text": "We include a discussion of features found to have no discernable effect, or negative effect, during development ( \u00a74).", "labels": [], "entities": []}, {"text": "Our task (in which output from syntactic parsers and other outside resources can be used).", "labels": [], "entities": []}, {"text": "We present our results in \u00a75.", "labels": [], "entities": []}], "datasetContent": [{"text": "We participated in the Open Track, and used the syntactic dependency parses supplied by the organizers.", "labels": [], "entities": [{"text": "Open Track", "start_pos": 23, "end_pos": 33, "type": "DATASET", "confidence": 0.9055653214454651}]}, {"text": "Feature engineering was performed on a development set ( \u00a720), training on \u00a7 \u00a700-19.", "labels": [], "entities": [{"text": "Feature engineering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8534517586231232}]}, {"text": "We evaluate labeled precision (LP), labeled recall (LR), labeled F 1 (LF), and labeled whole-sentence match (LM) on the held-out test data using the evaluation script provided by the organizers.", "labels": [], "entities": [{"text": "labeled precision (LP)", "start_pos": 12, "end_pos": 34, "type": "METRIC", "confidence": 0.8052547097206115}, {"text": "recall (LR)", "start_pos": 44, "end_pos": 55, "type": "METRIC", "confidence": 0.9366518557071686}, {"text": "labeled F 1 (LF)", "start_pos": 57, "end_pos": 73, "type": "METRIC", "confidence": 0.8267910232146581}, {"text": "labeled whole-sentence match (LM)", "start_pos": 79, "end_pos": 112, "type": "METRIC", "confidence": 0.7380323509375254}]}, {"text": "LF was averaged over the formalisms to determine the winning system.", "labels": [], "entities": [{"text": "LF", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9852484464645386}]}], "tableCaptions": [{"text": " Table 2: Features and constraints giving negative results.", "labels": [], "entities": []}, {"text": " Table 3: Labeled precision (LP), recall (LR), F1 (LF), and  whole-sentence match (LM) on the held-out test data.", "labels": [], "entities": [{"text": "Labeled precision (LP)", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8769786596298218}, {"text": "recall (LR)", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9584688544273376}, {"text": "F1 (LF)", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9639902412891388}, {"text": "whole-sentence match (LM)", "start_pos": 61, "end_pos": 86, "type": "METRIC", "confidence": 0.7983166098594665}]}]}