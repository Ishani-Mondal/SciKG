{"title": [{"text": "Kea: Sentiment Analysis of Phrases Within Short Texts", "labels": [], "entities": [{"text": "Sentiment Analysis of Phrases Within Short Texts", "start_pos": 5, "end_pos": 53, "type": "TASK", "confidence": 0.9158097505569458}]}], "abstractContent": [{"text": "Sentiment Analysis has become an increasingly important research topic.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9528357088565826}]}, {"text": "This paper describes our approach to building a system for the Sentiment Analysis in Twit-ter task of the SemEval-2014 evaluation.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twit-ter task", "start_pos": 63, "end_pos": 98, "type": "TASK", "confidence": 0.8939499258995056}]}, {"text": "The goal is to classify a phrase within a short piece of text as positive, negative or neutral.", "labels": [], "entities": []}, {"text": "In the evaluation, classifiers trained on Twitter data are tested on data from other domains such as SMS, blogs as well as sarcasm.", "labels": [], "entities": []}, {"text": "The results indicate that apart from sarcasm, classifiers built for sentiment analysis of phrases from tweets can be generalized to other short text domains quite effectively.", "labels": [], "entities": [{"text": "sentiment analysis of phrases from tweets", "start_pos": 68, "end_pos": 109, "type": "TASK", "confidence": 0.9226844509442648}]}, {"text": "However, in cross-domain experiments, SMS data is found to generalize even better than Twitter data.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, new forms of communication such as microblogging and text messaging have become quite popular.", "labels": [], "entities": []}, {"text": "While there is no limit to the range of information conveyed by tweets and short texts, people often use these messages to share their sentiments.", "labels": [], "entities": []}, {"text": "Working with these informal text genres presents challenges for natural language processing beyond those typically encountered when working with more traditional text genres.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.6964390675226847}]}, {"text": "Tweets and short texts are shorter, the language is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genrespecific terminology such as, RT for \"re-tweet\" and #hashtags for tagging (.", "labels": [], "entities": [{"text": "RT", "start_pos": 184, "end_pos": 186, "type": "METRIC", "confidence": 0.6546138525009155}]}, {"text": "Although several systems have tackled the task of analyzing sentiment from entire tweets, the task of analyzing sentiments of phrases (a word or more) within a tweet has remained largely unexplored.", "labels": [], "entities": []}, {"text": "This paper describes the details of our system that participated in the subtask A of Semeval-2014 Task 9: Sentiment Analysis in Twitter ().", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter", "start_pos": 106, "end_pos": 135, "type": "TASK", "confidence": 0.8593762516975403}]}, {"text": "The goal of this task is to determine whether a phrase within a message is positive, negative or neutral in that context.", "labels": [], "entities": []}, {"text": "Here, a message indicates any short informal piece of text such as a tweet, SMS data, or a sentence from Live Journal blog, which is asocial networking service where Internet users keep an online diary.", "labels": [], "entities": [{"text": "Live Journal blog", "start_pos": 105, "end_pos": 122, "type": "DATASET", "confidence": 0.9139194885889689}]}, {"text": "A phrase could be a word or a few consecutive words within a message.", "labels": [], "entities": []}, {"text": "The novelty of this task lies in the fact that a model built using only Twitter data is used to classify instances from other short text domains such as SMS and Live Journal.", "labels": [], "entities": [{"text": "Live Journal", "start_pos": 161, "end_pos": 173, "type": "DATASET", "confidence": 0.806504637002945}]}, {"text": "Moreover, a short test corpus of sarcastic tweets is also used to test the performance of the sentiment classifier.", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 94, "end_pos": 114, "type": "TASK", "confidence": 0.7553445398807526}]}, {"text": "The main contributions of this paper include a) developing a sentiment analysis classifer for phrases; b) training on Twitter data and testing on other domains such as SMS and Live Journal data to see how well the classifier generalizes to different types of text, and c) testing on sarcastic tweets.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.8594324290752411}, {"text": "Live Journal data", "start_pos": 176, "end_pos": 193, "type": "DATASET", "confidence": 0.703472226858139}]}], "datasetContent": [{"text": "The task organizers made available a test data set composed of 10681 instances.", "labels": [], "entities": []}, {"text": "the breakdown of the various types of text, with example phrases that are to be classified.", "labels": [], "entities": []}, {"text": "As expected, Live Journal has a slightly more formal sentence structure with properly spelt words, whereas Twitter and SMS data include more creative spellings.", "labels": [], "entities": [{"text": "Live Journal", "start_pos": 13, "end_pos": 25, "type": "DATASET", "confidence": 0.9594275057315826}]}, {"text": "Clearly, the sarcasm category includes messages with two contradictory sentiments in close proximity.", "labels": [], "entities": []}, {"text": "The challenge of this task lies precisely in the fact that one classifier trained on Twitter data should be able to generalize reasonably well on different types of text.", "labels": [], "entities": []}, {"text": "In this section, we test how well the classifiers trained on one type of text classify other types of text., for example, the last row shows the results of a model trained on Journal data (1000 instances) and tested on Twitter, SMS and Sarcasm test sets, and 10-fold cross-validated on Journal data.", "labels": [], "entities": [{"text": "Journal data", "start_pos": 175, "end_pos": 187, "type": "DATASET", "confidence": 0.9440754055976868}, {"text": "Sarcasm test sets", "start_pos": 236, "end_pos": 253, "type": "DATASET", "confidence": 0.8175453245639801}, {"text": "Journal data", "start_pos": 286, "end_pos": 298, "type": "DATASET", "confidence": 0.9781323671340942}]}, {"text": "Since this experiment measures the generalizability of different data sets, we randomly selected 500 positive and 500 negative instances for each data set, in order to minimize the influence of the size of the training data set on the classification process.", "labels": [], "entities": []}, {"text": "Note that this experiment does not include the neutral class.", "labels": [], "entities": []}, {"text": "As expected, the best results on the test sets are obtained when using cross-validation (except on Twitter set).", "labels": [], "entities": [{"text": "Twitter set", "start_pos": 99, "end_pos": 110, "type": "DATASET", "confidence": 0.9106819331645966}]}, {"text": "However, the model built using SMS data has the best or the second-best result overall, which suggests that out of the three types of text, it is the SMS data that generalize the best.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Macro-average F1-scores. P, R and F represent precision, recall and F1-score, respectively.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9465469121932983}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9997063279151917}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9988849759101868}, {"text": "F1-score", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9976957440376282}]}, {"text": " Table 3: Ablation tests: Trained on Twitter only.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9924411773681641}]}, {"text": " Table 4: Cross-domain training and tests.", "labels": [], "entities": []}]}