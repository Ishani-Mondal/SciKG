{"title": [{"text": "SU-FMI: System Description for SemEval-2014 Task 9 on Sentiment Analysis in Twitter", "labels": [], "entities": [{"text": "SemEval-2014 Task 9", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8705953359603882}, {"text": "Sentiment Analysis", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.9377246499061584}]}], "abstractContent": [{"text": "We describe the submission of the team of the Sofia University to SemEval-2014 Task 9 on Sentiment Analysis in Twit-ter.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.866482675075531}, {"text": "Twit-ter", "start_pos": 111, "end_pos": 119, "type": "DATASET", "confidence": 0.8242373466491699}]}, {"text": "We participated in subtask B, where the participating systems had to predict whether a Twitter message expresses positive , negative, or neutral sentiment.", "labels": [], "entities": []}, {"text": "We trained an SVM classifier with a linear kernel using a variety of features.", "labels": [], "entities": []}, {"text": "We used publicly available resources only, and thus our results should be easily replicable.", "labels": [], "entities": []}, {"text": "Overall, our system is ranked 20th out of 50 submissions (by 44 teams) based on the average of the three 2014 evaluation data scores, with an F1-score of 63.62 on general tweets, 48.37 on sarcastic tweets, and 68.24 on LiveJournal messages.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9994552731513977}]}], "introductionContent": [{"text": "We describe the submission of the team of the Sofia University, Faculty of Mathematics and Informatics (SU-FMI) to SemEval-2014 Task 9 on Sentiment Analysis in Twitter ().", "labels": [], "entities": [{"text": "SemEval-2014 Task 9 on Sentiment Analysis in Twitter", "start_pos": 115, "end_pos": 167, "type": "TASK", "confidence": 0.7807035148143768}]}, {"text": "This SemEval challenge had two subtasks: \u2022 subtask A (term-level) asks to predict the sentiment of a phrase inside a tweet; \u2022 subtask B (message-level) asks to predict the overall sentiment of a tweet message.", "labels": [], "entities": []}, {"text": "In both subtasks, the sentiment can be positive, negative, or neutral.", "labels": [], "entities": []}, {"text": "Here are some examples: \u2022 positive: Gas by my house hit $3.39!!!!", "labels": [], "entities": []}, {"text": "I'm going to Chapel Hill on Sat.", "labels": [], "entities": [{"text": "Chapel Hill on Sat.", "start_pos": 13, "end_pos": 32, "type": "DATASET", "confidence": 0.9731676816940308}]}, {"text": ":) \u2022 neutral: New York Giants: Game-by-Game Predictions for the 2nd Half of the Season http://t.co/yK9VTjcs \u2022 negative: Why the hell does Selma have school tomorrow but Parlier clovis & others don't?", "labels": [], "entities": []}, {"text": "\u2022 negative (sarcastic): @MetroNorth wall to wall people on the platform at South Norwalk waiting for the 8:08.", "labels": [], "entities": [{"text": "South Norwalk waiting for the 8:08", "start_pos": 75, "end_pos": 109, "type": "DATASET", "confidence": 0.867791752020518}]}, {"text": "Great sense Below we first describe our preprocessing, features and classifier in Section 2.", "labels": [], "entities": []}, {"text": "Then, we discuss our experiments, results and analysis in Section 3.", "labels": [], "entities": []}, {"text": "Finally, we conclude with possible directions for future work in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "At development time, we trained on train-2013, tuned the C value of SVM on dev-2013, and evaluated on test-2013 ().", "labels": [], "entities": []}, {"text": "For our submission, we trained on train-2013+dev-2013, and we evaluated on the 2014 test dataset provided by the organizers.", "labels": [], "entities": []}, {"text": "This dataset contains two parts and a total of five datasets: (a) progress test (the Twitter and SMS test datasets for 2013), and (b) new test datasets (from Twitter, from Twitter with sarcasm, and from LiveJournal).", "labels": [], "entities": [{"text": "progress test", "start_pos": 66, "end_pos": 79, "type": "METRIC", "confidence": 0.9280970096588135}, {"text": "Twitter and SMS test datasets", "start_pos": 85, "end_pos": 114, "type": "DATASET", "confidence": 0.6771340727806091}]}, {"text": "We used C=0.012, which was best on development.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1.  The numbers after our score are the delta to the  best solution. We have also included a ranking  among 2014 participant systems on the 2013 data  sets, released by the organizers.", "labels": [], "entities": [{"text": "2013 data  sets", "start_pos": 147, "end_pos": 162, "type": "DATASET", "confidence": 0.8440267046292623}]}, {"text": " Table 2: Ablation experiments on the 2013 test sets.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9831320643424988}, {"text": "2013 test sets", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.8202792008717855}]}, {"text": " Table 3: Ablation experiments on the 2014 test sets.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9821969270706177}, {"text": "2014 test sets", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.8603453238805135}]}, {"text": " Table 4: Our post-submission results.", "labels": [], "entities": []}]}