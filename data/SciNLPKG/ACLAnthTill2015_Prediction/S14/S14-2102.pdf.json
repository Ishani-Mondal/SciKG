{"title": [{"text": "SSMT: A Machine Translation Evaluation View to Paragraph-to-Sentence Semantic Similarity", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 8, "end_pos": 38, "type": "TASK", "confidence": 0.7984609802563986}, {"text": "Paragraph-to-Sentence Semantic Similarity", "start_pos": 47, "end_pos": 88, "type": "TASK", "confidence": 0.7271906733512878}]}], "abstractContent": [{"text": "This paper presents the system SSMT measuring the semantic similarity between a paragraph and a sentence submitted to the SemEval 2014 task3: Cross-level Semantic Similarity.", "labels": [], "entities": [{"text": "SSMT", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.9716089963912964}, {"text": "SemEval 2014 task3: Cross-level Semantic Similarity", "start_pos": 122, "end_pos": 173, "type": "TASK", "confidence": 0.6755286157131195}]}, {"text": "The special difficulty of this task is the length disparity between the two semantic comparison texts.", "labels": [], "entities": []}, {"text": "We adapt several machine translation evaluation metrics for features to cope with this difficulty, then train a regression model for the semantic similarity prediction.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.7654552360375723}, {"text": "semantic similarity prediction", "start_pos": 137, "end_pos": 167, "type": "TASK", "confidence": 0.7717798749605814}]}, {"text": "This system is straightforward in intuition and easy in implementation.", "labels": [], "entities": []}, {"text": "Our best run gets 0.808 in Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 27, "end_pos": 46, "type": "METRIC", "confidence": 0.8145692646503448}]}, {"text": "METEOR-derived features are the most effective ones in our experiment.", "labels": [], "entities": [{"text": "METEOR-derived", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.5730113983154297}]}], "introductionContent": [{"text": "Cross level semantic similarity measures the similarity between different levels of text unit, for example, between a document and a paragraph, or between a phrase and a word.", "labels": [], "entities": [{"text": "Cross level semantic similarity", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6280377209186554}]}, {"text": "Paragraph and sentence are the natural language units to convey opinions or state events in daily life.", "labels": [], "entities": []}, {"text": "We can see posts on forums, questions and answers in Q&A communities and customer reviews on E-commerce websites, are mainly organised in these two units.", "labels": [], "entities": []}, {"text": "Better similarity measurement across them will be helpful in clustering similar answers or reviews.", "labels": [], "entities": [{"text": "similarity measurement", "start_pos": 7, "end_pos": 29, "type": "METRIC", "confidence": 0.9255782067775726}]}, {"text": "The paragraph-to-sentence semantic similarity subtask in SemEval2014 task3 () is the first semantic similarity competition across these two language levels.", "labels": [], "entities": []}, {"text": "The special difficulty of this task is the length disparity between the compared pair: a paragraph contains Semantic similarity on different levels, for example, on word level (, sentences level, document level (, have been well studied, yet methods on one level can hardly be applied to a different level, let alone be applied for the crosslevel tasks.", "labels": [], "entities": []}, {"text": "The work of was an exception.", "labels": [], "entities": []}, {"text": "They proposed a unified method for semantic comparison at multi-levels all the way from comparing word senses to comparing text documents Our work is inspired by automatic machine translation(MT) evaluation, in which different metrics are designed to compare the adequacy and fluency of a MT system's output, called hypothesis, against a gold standard translation, called reference.", "labels": [], "entities": [{"text": "semantic comparison", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7401871681213379}, {"text": "automatic machine translation(MT) evaluation", "start_pos": 162, "end_pos": 206, "type": "TASK", "confidence": 0.8405305402619498}]}, {"text": "As MT evaluation metrics measure sentence pair similarity, it is a natural idea to generalize them for paragraph-sentence pair.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.9077719449996948}]}, {"text": "In this paper, we follow the motivations of several MT evaluation metrics yet made adaption to cope with the length disparity difficulty of this task, and combine these features in a regression model.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9011923968791962}]}, {"text": "Our system SSMT (Semantic Similarity in view of Machine Translation evaluation) involves no extensive resource or strenuous computation, yet gives promising result with just a few simple features.", "labels": [], "entities": [{"text": "Machine Translation evaluation", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.856347660223643}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Effectiveness of Different Features.  \"-METEOR\" means the feature set excluding  METEOR-derived features.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9280418157577515}]}]}