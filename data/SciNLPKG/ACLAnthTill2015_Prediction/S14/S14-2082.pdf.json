{"title": [{"text": "Priberam: A Turbo Semantic Parser with Second Order Features", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents our contribution to the SemEval-2014 shared task on Broad-Coverage Semantic Dependency Parsing.", "labels": [], "entities": [{"text": "SemEval-2014 shared task", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.9017879764238993}, {"text": "Broad-Coverage Semantic Dependency Parsing", "start_pos": 72, "end_pos": 114, "type": "TASK", "confidence": 0.5603925138711929}]}, {"text": "We employ a feature-rich linear model, including scores for first and second-order dependencies (arcs, siblings, grandparents and co-parents).", "labels": [], "entities": []}, {"text": "Decoding is performed in a global manner by solving a linear relaxation with alternating directions dual decomposition (AD 3).", "labels": [], "entities": [{"text": "Decoding", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9424309730529785}]}, {"text": "Our system achieved the top score in the open challenge, and the second highest score in the closed track.", "labels": [], "entities": []}], "introductionContent": [{"text": "The last decade saw a considerable progress in statistical modeling for dependency syntactic parsing (.", "labels": [], "entities": [{"text": "dependency syntactic parsing", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.8400676250457764}]}, {"text": "Models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (;.", "labels": [], "entities": []}, {"text": "This paper applies the same rationale to semantic dependency parsing, in which the output variable is a semantic graph, rather than a syntactic tree.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.7477658192316691}]}, {"text": "We extend a recently proposed dependency parser,, to be able to perform semantic parsing using any of the three formalisms considered in this shared task (DM, PAS, and PCEDT).", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7723081111907959}, {"text": "semantic parsing", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.7365299165248871}]}, {"text": "The result is TurboSemanticParser, which we release as open-source software.", "labels": [], "entities": []}, {"text": "We describe here a second order model for semantic parsing ( \u00a72).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.8474664688110352}]}, {"text": "We follow prior work in semantic role labeling (; Jo-  hansson), by adding constraints and modeling interactions among arguments within the same frame; however, we go beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.66285440325737}]}, {"text": "We formulate parsing as a global optimization problem and solve a relaxation through AD 3 , a fast dual decomposition algorithm in which several simple local subproblems are solved iteratively ( \u00a73).", "labels": [], "entities": [{"text": "formulate parsing", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7302587032318115}]}, {"text": "Through a rich set of features ( \u00a74), we arrive at top accuracies at parsing speeds around 1,000 tokens per second, as described in the experimental section ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [{"text": "All models were trained by running 10 epochs of max-loss MIRA with C = 0.01 ().", "labels": [], "entities": [{"text": "max-loss", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9599096179008484}, {"text": "MIRA", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.896497905254364}, {"text": "C", "start_pos": 67, "end_pos": 68, "type": "METRIC", "confidence": 0.9579724073410034}]}, {"text": "The cost function takes into account mismatches between predicted and gold dependencies, with a cost c P on labeled arcs incorrectly predicted (false positives) and a cost c Ron gold labeled arcs that were missed (false negatives).", "labels": [], "entities": []}, {"text": "These values were set through cross-validation in the dev set, yielding c P = 0.4 and c R = 0.6 in all runs, except for the DM and PCEDT datasets in the closed track, for which c P = 0.3 and c R = 0.7.", "labels": [], "entities": [{"text": "PCEDT datasets", "start_pos": 131, "end_pos": 145, "type": "DATASET", "confidence": 0.8657968938350677}]}, {"text": "To speedup decoding, we discard arcs whose posterior probability is below 10 \u22124 , according to a probabilistic unlabeled first-order pruner.", "labels": [], "entities": []}, {"text": "shows a significant reduction of the search space with a very small drop in recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9985313415527344}]}, {"text": "shows our final results in the test set, fora model trained in the train and development partitions.", "labels": [], "entities": []}, {"text": "Our system achieved the best score in the open track (an LF score of 86.27%, averaged over DM, PAS, and PCEDT), and the second best in the closed track, after the Peking team.", "labels": [], "entities": [{"text": "LF score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9283880591392517}, {"text": "DM", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.909261167049408}, {"text": "PAS", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9656886458396912}, {"text": "PCEDT", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.5800575613975525}, {"text": "Peking team", "start_pos": 163, "end_pos": 174, "type": "DATASET", "confidence": 0.9281657934188843}]}, {"text": "Overall, we observe that the precision and recall in PCEDT are far below the other two formalisms, but this difference is much smaller when looking at unlabeled scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.999721109867096}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9993473887443542}, {"text": "PCEDT", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.6597289443016052}]}, {"text": "Comparing the results in the closed and open tracks, we observe a consistent improvement in the three formalisms of around 1% in F 1 from using syntactic information.", "labels": [], "entities": []}, {"text": "While this confirms previous findings that syntactic features are important in semantic role labeling, these improvements are less striking than expected.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.6953158378601074}]}, {"text": "We conjecture this is due to the fact that our model in the closed track already incorporates a variety of contextual features which are nearly as informative as those extracted from the dependency trees.", "labels": [], "entities": []}, {"text": "Finally, to assess the importance of the second order features, reports experiments in the dev-set that progressively add several groups of features, along with runtimes.", "labels": [], "entities": []}, {"text": "We can see that siblings, co-parents, and grandparents all provide valuable information that improves the final scores (with the exception of the PCEDT labeled scores, where the difference is negligible).", "labels": [], "entities": [{"text": "PCEDT labeled scores", "start_pos": 146, "end_pos": 166, "type": "DATASET", "confidence": 0.7255573272705078}]}, {"text": "This comes at only a small cost in terms of runtime, which is around 1,000 tokens per second for the full models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pruner statistics in the dev-set, for the  open track. Shown are oracle recall scores, consid- ering both unlabeled (UR) and labeled arcs (LR);  and the averaged number of unlabeled and la- beled arcs per token that remained after the prun- ing stage (# UA/tok and # LA/tok). In brackets,  we show the fraction of unlabeled/labeled arcs that  survived the pruning.", "labels": [], "entities": [{"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9404283761978149}]}, {"text": " Table 2: Submitted results for the closed and open  tracks. For comparison, the best-performing sys- tem in the closed track (Peking) obtained averaged  UF and LF scores of 91.03% and 85.91%, respec- tively.", "labels": [], "entities": [{"text": "UF", "start_pos": 154, "end_pos": 156, "type": "METRIC", "confidence": 0.9714967012405396}, {"text": "LF", "start_pos": 161, "end_pos": 163, "type": "METRIC", "confidence": 0.5240688920021057}]}, {"text": " Table 3: Results in the dev-set for the open track,  progressively adding several groups of features,  until the full model is obtained. We report un- labeled/labeled F 1 and parsing speeds in tokens  per second. Our speeds include the time necessary  for pruning, evaluating features, and decoding, as  measured on a Intel Core i7 processor @3.4 GHz.", "labels": [], "entities": []}]}