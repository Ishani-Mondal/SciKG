{"title": [{"text": "CECL: a New Baseline and a Non-Compositional Approach for the Sick Benchmark", "labels": [], "entities": [{"text": "CECL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9357882738113403}]}], "abstractContent": [{"text": "This paper describes the two procedures for determining the semantic similarities between sentences submitted for the Se-mEval 2014 Task 1.", "labels": [], "entities": [{"text": "determining the semantic similarities between sentences submitted for the Se-mEval 2014 Task 1", "start_pos": 44, "end_pos": 138, "type": "TASK", "confidence": 0.49179719044612}]}, {"text": "MeanMaxSim, an unsupervised procedure, is proposed as anew baseline to assess the efficiency gain provided by compositional models.", "labels": [], "entities": []}, {"text": "It out-performs a number of other baselines by a wide margin.", "labels": [], "entities": []}, {"text": "Compared to the word-overlap baseline, it has the advantage of taking into account the distributional similarity between words that are also involved in compositional models.", "labels": [], "entities": []}, {"text": "The second procedure aims at building a predictive model using as predictors MeanMaxSim and (transformed) lexical features describing the differences between each sentence of a pair.", "labels": [], "entities": []}, {"text": "It finished sixth out of 17 teams in the textual similarity sub-task and sixth out of 19 in the textual entailment sub-task.", "labels": [], "entities": []}], "introductionContent": [{"text": "The) was designed to allow a rigorous evaluation of compositional distributional semantic models.", "labels": [], "entities": []}, {"text": "CDSMs aim to represent the meaning of phrases and sentences by composing the distributional representations of the words they contain (; they are thus an extension of Distributional Semantic Models (DSMs), which approximate the meaning of words with vectors summarizing their patterns of co-occurrence in a corpus (Baroni and Lenci,).", "labels": [], "entities": []}, {"text": "The dataset for this task, called SICK (Sentences Involving Compositional Knowledge), consists of almost 10,000 English sentence pairs annotated for relatedness in meaning and entailment relation by ten annotators).", "labels": [], "entities": [{"text": "Sentences Involving Compositional Knowledge)", "start_pos": 40, "end_pos": 84, "type": "TASK", "confidence": 0.6779801547527313}]}, {"text": "The rationale behind this dataset is that \"understanding when two sentences have close meanings or entail each other crucially requires a compositional semantics step\"), and thus that annotators judge the similarity between the two sentences of a pair by first building a mental representation of the meaning of each sentence and then comparing these two representations.", "labels": [], "entities": []}, {"text": "However, another option was available to the annotators.", "labels": [], "entities": []}, {"text": "They could have paid attention only to the differences between the sentences, and assessed the significance of these differences.", "labels": [], "entities": []}, {"text": "Such an approach could have been favored by the dataset built on the basis of a thousand sentences modified by a limited number of (often) very specific transformations, producing sentence pairs that might seem quite repetitive.", "labels": [], "entities": []}, {"text": "An analysis conducted during the training phase of the challenge brought some support for this hypothesis.", "labels": [], "entities": []}, {"text": "The analysis focused on pairs of sentences in which the only difference between the two sentences was the replacement of one content word by another, as in A man is singing to a girl vs. A man is singing to a woman, but also in A man is sitting in afield vs. A man is running in afield.", "labels": [], "entities": []}, {"text": "The material was divided into two parts, 3500 sentence pairs in the training set and the remaining 1500 in the test set.", "labels": [], "entities": []}, {"text": "First, the average similarity score for each pair of interchanged words was calculated on the training set (e.g., in this sample, there were 16 sentence pairs in which woman and man were interchanged, and their mean similarity score was 3.6).", "labels": [], "entities": [{"text": "similarity score", "start_pos": 19, "end_pos": 35, "type": "METRIC", "confidence": 0.8962467610836029}, {"text": "mean similarity score", "start_pos": 211, "end_pos": 232, "type": "METRIC", "confidence": 0.6716559926668803}]}, {"text": "Then, these mean scores were used as the similarity scores of the sentence pairs of the test sample in which the same words were interchanged.", "labels": [], "entities": []}, {"text": "The correlation between the actual scores and the predicted score was 0.83 (N=92), a value that can be considered as very high, given the restrictions on the range in which the predicted similarity scores vary (min=3.5 and max=5.0;.", "labels": [], "entities": []}, {"text": "It is important to note that this observation does not prove that the participants have not built a compositional representation, especially as it only deals with a very specific type of transformation.", "labels": [], "entities": []}, {"text": "It nevertheless suggests that analyzing only the differences between the sentences of a pair could allow the similarity between them to be effectively estimated.", "labels": [], "entities": []}, {"text": "Following these observations, I opted to try to determine the degree of efficacy that can be achieved by two non-compositional approaches.", "labels": [], "entities": []}, {"text": "The first approach, totally unsupervised, is proposed as anew baseline to evaluate the efficacy gains brought by compositional systems.", "labels": [], "entities": []}, {"text": "The second, a supervised approach, aims to capitalize on the properties of the SICK benchmark.", "labels": [], "entities": [{"text": "SICK benchmark", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.7129510343074799}]}, {"text": "While these approaches have been developed specifically for the semantic relatedness sub-task, the second has also been applied to the textual entailment subtask.", "labels": [], "entities": []}, {"text": "This paper describes the two proposed approaches, their implementation in the context of SemEval 2014 Task 1, and the results obtained.", "labels": [], "entities": [{"text": "SemEval 2014 Task 1", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.678866982460022}]}], "datasetContent": [{"text": "All sentences were tokenized and lemmatized by the Stanford Parser (de Marneffe et al., 2006; Toutanova et al., 2003).", "labels": [], "entities": [{"text": "Stanford Parser", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.8207553625106812}]}], "tableCaptions": [{"text": " Table 1: Pearson's correlation for MeanMaxSim  and several other baselines on the test set.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8392077883084615}, {"text": "MeanMaxSim", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.7961965203285217}]}]}