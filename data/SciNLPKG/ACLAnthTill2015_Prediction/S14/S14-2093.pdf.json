{"title": [{"text": "SemantiKLUE: Robust Semantic Similarity at Multiple Levels Using Maximum Weight Matching", "labels": [], "entities": []}], "abstractContent": [{"text": "Being able to quantify the semantic similarity between two texts is important for many practical applications.", "labels": [], "entities": []}, {"text": "SemantiKLUE combines unsupervised and supervised techniques into a robust system for measuring semantic similarity.", "labels": [], "entities": []}, {"text": "At the core of the system is a word-to-word alignment of two texts using a maximum weight matching algorithm.", "labels": [], "entities": []}, {"text": "The system participated in three SemEval-2014 shared tasks and the competitive results are evidence for its usability in that broad field of application.", "labels": [], "entities": [{"text": "SemEval-2014 shared tasks", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.807976226011912}]}], "introductionContent": [{"text": "Semantic similarity measures the semantic equivalence between two texts ranging from total difference to complete semantic equivalence and is usually encoded as a number in a closed interval, e. g..", "labels": [], "entities": []}, {"text": "Here is an example for interpreting the numeric similarity scores taken from 0.", "labels": [], "entities": [{"text": "numeric similarity scores", "start_pos": 40, "end_pos": 65, "type": "METRIC", "confidence": 0.6525221665700277}]}, {"text": "The two sentences are on different topics.", "labels": [], "entities": []}, {"text": "1. The two sentences are not equivalent, but are on the same topic.", "labels": [], "entities": []}, {"text": "2. The two sentences are not equivalent, but share some details.", "labels": [], "entities": []}, {"text": "3. The two sentences are roughly equivalent, but some important information differs/missing.", "labels": [], "entities": []}, {"text": "4. The two sentences are mostly equivalent, but some unimportant details differ.", "labels": [], "entities": []}, {"text": "5. The two sentences are completely equivalent, as they mean the same thing.", "labels": [], "entities": []}, {"text": "Systems capable of reliably predicting the semantic similarity between two texts can be beneficial fora broad range of NLP applications, e. g. paraphrasing, MT evaluation, information extraction, question answering and summarization.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 157, "end_pos": 170, "type": "TASK", "confidence": 0.9880469739437103}, {"text": "information extraction", "start_pos": 172, "end_pos": 194, "type": "TASK", "confidence": 0.8753290772438049}, {"text": "question answering", "start_pos": 196, "end_pos": 214, "type": "TASK", "confidence": 0.9269069731235504}, {"text": "summarization", "start_pos": 219, "end_pos": 232, "type": "TASK", "confidence": 0.9890989065170288}]}, {"text": "A general system for semantic similarity aiming at being applicable in such abroad scope has to be able to adapt to the use case at hand, because different use cases might, for example, require different similarity scales: For one application, two texts dealing roughly with the same topic should get a high similarity score, whereas for another application being able to distinguish between subtle differences in meaning might be important.", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7981319725513458}, {"text": "similarity score", "start_pos": 308, "end_pos": 324, "type": "METRIC", "confidence": 0.95219686627388}]}, {"text": "The three SemEval-2014 shared tasks focussing on semantic similarity (cf. Sections 3, 4 and 5 for more detailed task descriptions) provide a rich testbed for such a general system, as the individual tasks and subtasks have slightly different objectives.", "labels": [], "entities": [{"text": "SemEval-2014 shared tasks focussing on semantic similarity", "start_pos": 10, "end_pos": 68, "type": "TASK", "confidence": 0.7269507646560669}]}, {"text": "In the remainder of this paper, we describe SemantiKLUE, a general system for measuring semantic similarity between texts that we built based on our experience from participating in the *SEM 2013 shared task on \"Semantic Textual Similarity\" ().", "labels": [], "entities": [{"text": "SEM 2013 shared task on \"Semantic Textual Similarity\"", "start_pos": 187, "end_pos": 240, "type": "TASK", "confidence": 0.7916237235069274}]}], "datasetContent": [{"text": "As an experiment, we included features from a commercial text clustering software that is currently being developed by our team (Greiner and Evert, in preparation).", "labels": [], "entities": [{"text": "text clustering", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.7393335700035095}, {"text": "Evert", "start_pos": 141, "end_pos": 146, "type": "METRIC", "confidence": 0.9320208430290222}]}, {"text": "We used this tool -which combines ideas from Latent Semantic Indexing and distributional semantics with multiple clustering steps -as a black box.", "labels": [], "entities": []}, {"text": "We loaded all training, development and test items fora given task into the system and applied the clustering algorithm.", "labels": [], "entities": []}, {"text": "However, we did not make use of the resulting topic clusters.", "labels": [], "entities": []}, {"text": "Instead, we computed cosine similarities for each pair (s 1 , s 2 ) of sentences (or other textual units) based on the internal representation.", "labels": [], "entities": []}, {"text": "In addition, we computed the average neighbour rank of the two sentences, based on the rank of s 2 among the nearest neighbours of s 1 and vice versa.", "labels": [], "entities": []}, {"text": "Since these features are generated from the task data themselves, they should adapt automatically to the range of meaning differences present in a given data set.", "labels": [], "entities": []}, {"text": "From participating in the *SEM 2013 shared task on semantic textual similarity ( we already know that the composition of the training data is one of the strongest influences on system performance in this task.", "labels": [], "entities": [{"text": "SEM 2013 shared task", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.5678203925490379}]}, {"text": "As the individual data sets are not very similar to each other, we tried to come up with a good subset of the available training data for each data set.", "labels": [], "entities": []}, {"text": "In doing so, we were moderately successful as the results in show.", "labels": [], "entities": []}, {"text": "Running the complete system with all of the available training data on all test data sets results in a lower weighted mean than our submitted run.", "labels": [], "entities": []}, {"text": "If we stick to using the same training data for all test data sets and optimize the subset of the training data we use, we achieve a slightly better result than our submitted run (the optimal subset consists of the FNWN, headlines, MSRpar, MSRvid and OnWN data sets).", "labels": [], "entities": [{"text": "FNWN", "start_pos": 215, "end_pos": 219, "type": "DATASET", "confidence": 0.9357224702835083}, {"text": "OnWN data sets", "start_pos": 251, "end_pos": 265, "type": "DATASET", "confidence": 0.9135336875915527}]}, {"text": "Using that optimal subset of the training data and adding the experimental features to the complete system has a minor positive effect on the weighted mean, with the biggest impact on the headlines and OnWN data sets.", "labels": [], "entities": [{"text": "headlines", "start_pos": 188, "end_pos": 197, "type": "DATASET", "confidence": 0.9607282280921936}, {"text": "OnWN data sets", "start_pos": 202, "end_pos": 216, "type": "DATASET", "confidence": 0.9520927866299947}]}, {"text": "Using the complete system without the dependency-based features gives roughly the same results but omitting all resource-heavy features has clearly a negative impact on the results.", "labels": [], "entities": []}, {"text": "In another experiment we try to optimize our strategy of finding the best subset of the training data for each test data set.", "labels": [], "entities": []}, {"text": "Doing that gives us a considerably higher weighted mean than using the same training data for every test data set, putting our system on the eighth place.", "labels": [], "entities": []}, {"text": "Using the complete system, we find that the best training data subsets for the individual test data sets are those shown in  If we add the experimental features to the complete system and still optimize the training data subsets, we get a small boost to the results.", "labels": [], "entities": []}, {"text": "Leaving out the dependency-based features does not really hurt performance but also omitting the WordNet-based features has a negative impact on the results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for task 1 (Pearson's r, Spearman's  \u03c1, mean squared error and accuracy).", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.8767561713854471}, {"text": "Spearman's  \u03c1", "start_pos": 43, "end_pos": 56, "type": "METRIC", "confidence": 0.7941709558169047}, {"text": "mean squared error", "start_pos": 58, "end_pos": 76, "type": "METRIC", "confidence": 0.955519954363505}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9983628392219543}]}, {"text": " Table 2: Results for task 3 (Pearson's r and Spearman's \u03c1).", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.8383464614550272}, {"text": "Spearman's \u03c1", "start_pos": 46, "end_pos": 58, "type": "METRIC", "confidence": 0.5785653988520304}]}, {"text": " Table 3: Results for task 10.", "labels": [], "entities": []}, {"text": " Table 5: Overview of results.", "labels": [], "entities": [{"text": "Overview", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8987998962402344}]}]}