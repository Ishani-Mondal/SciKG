{"title": [{"text": "IHS R&D Belarus: Cross-domain Extraction of Product Features using Conditional Random Fields", "labels": [], "entities": [{"text": "IHS R&D", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.696195587515831}, {"text": "Cross-domain Extraction of Product", "start_pos": 17, "end_pos": 51, "type": "TASK", "confidence": 0.7422173768281937}]}], "abstractContent": [{"text": "This paper describes the aspect extraction system submitted by IHS R&D Belarus team at the SemEval-2014 shared task related to Aspect-Based Sentiment Analysis.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7919659614562988}, {"text": "IHS R&D Belarus team", "start_pos": 63, "end_pos": 83, "type": "DATASET", "confidence": 0.7878936529159546}, {"text": "SemEval-2014 shared task", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.4822660783926646}, {"text": "Aspect-Based Sentiment Analysis", "start_pos": 127, "end_pos": 158, "type": "TASK", "confidence": 0.7733952601750692}]}, {"text": "Our system is based on IHS Goldfire linguistic processor and uses a rich set of lexical, syntactic and statistical features in CRF model.", "labels": [], "entities": [{"text": "IHS Goldfire linguistic processor", "start_pos": 23, "end_pos": 56, "type": "DATASET", "confidence": 0.948834702372551}]}, {"text": "We participated in two domain-specific tasks-restaurants and laptops-with the same system trained on a mixed corpus of reviews.", "labels": [], "entities": []}, {"text": "Among submissions of constrained systems from 28 teams, our submission was ranked first in laptop domain and fourth in restaurant domain for the subtask A devoted to aspect extraction.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.8080873191356659}]}], "introductionContent": [{"text": "With a rapid growth of the blogs, forums, review sites and social networks, more and more people express their personal views about products on the Internet inform of reviews, ratings, or recommendations.", "labels": [], "entities": []}, {"text": "This is a great source of data used by many researchers and commercial applications that are focused on the sentiment analysis to determine customer opinions.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.9165217578411102}]}, {"text": "Sentiment analysis can be done on document, sentence, and phrase level (Jagtap, V. S.,.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.955150842666626}]}, {"text": "Earlier works were focused mainly on the document; Pang,) and the sentence level ().", "labels": [], "entities": []}, {"text": "However, this information can be insufficient for customers \uf020 who are seeking opinions on specific product features (aspects) such as design, battery life, or screen.", "labels": [], "entities": [{"text": "screen", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9734973311424255}]}, {"text": "This fine-grained classification is a topic of as- pect-based sentiment analysis.", "labels": [], "entities": [{"text": "as- pect-based sentiment analysis", "start_pos": 47, "end_pos": 80, "type": "TASK", "confidence": 0.6736494541168213}]}, {"text": "Traditional approaches to aspect extraction are based on frequently used nouns and noun phrases (, exploiting opinions (), and supervised learning.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.9126766920089722}]}, {"text": "In this paper, we describe a system (IHS_RD_Belarus in official results) developed to participate in the international shared task organized by the Conference on Semantic Evaluation Exercises ( and focused on the phrase-level sentiment classification, namely aspect extraction ().", "labels": [], "entities": [{"text": "IHS_RD_Belarus", "start_pos": 37, "end_pos": 51, "type": "METRIC", "confidence": 0.5630772054195404}, {"text": "phrase-level sentiment classification", "start_pos": 213, "end_pos": 250, "type": "TASK", "confidence": 0.7463998198509216}, {"text": "aspect extraction", "start_pos": 259, "end_pos": 276, "type": "TASK", "confidence": 0.7239860147237778}]}, {"text": "An aspect term means particular feature of a product or service used in opinion-bearing sentences (My phone has amazing screen), as well as in neutral sentences (The screen brightness automatically adjusts).", "labels": [], "entities": []}, {"text": "The organizers of SemEval-2014 task have provided a dataset of customer reviews with annotated aspects of the target entities from two domains: restaurants (3041 sentences) and laptops (3045 sentences).", "labels": [], "entities": []}, {"text": "The results were evaluated separately in each domain.", "labels": [], "entities": []}, {"text": "Many studies showed that sentiment analysis is very sensitive to the source domain (training corpus domain) and performs poorly on data from other domain.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9681189060211182}]}, {"text": "This restriction limits the applicability of in-domain models to a wide domain diversity of reviews.", "labels": [], "entities": []}, {"text": "One of the common approaches to develop a cross-domain system is training on a mixture of labeled data from different domains (Aue and).", "labels": [], "entities": [{"text": "Aue", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.8834121823310852}]}, {"text": "Cross-domain approach has the advantage of better portability, but it suffers from lower accuracy compared to in-domain aspect extraction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.998969554901123}, {"text": "aspect extraction", "start_pos": 120, "end_pos": 137, "type": "TASK", "confidence": 0.7857388854026794}]}, {"text": "Our cross-domain system is trained on mixed training data, and the same model was used unchanged for classification of both domain-specific test datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our CRF model was trained on the mixed set of 6086 sentences with annotated aspect terms (3045 from the laptop domain and 3041 from the restaurant domain).", "labels": [], "entities": []}, {"text": "The same model was applied unchanged to the test dataset from laptop domain (800 sentences) and restaurant domain (800 sentences).", "labels": [], "entities": []}, {"text": "We evaluated our system using 5-fold cross-validation: in each of the five iterations of the cross-validation, we used 80% of the provided training data for learning, and 20% for testing..", "labels": [], "entities": []}, {"text": "Performance on different datasets (F 1-score).", "labels": [], "entities": [{"text": "F 1-score", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9837632775306702}]}, {"text": "The shows the model performance (F 1-score) obtained on the training set (using 5-fold cross validation), on the development set (we used apart of the training set as development set), on the final test set and the baseline provided by the task organizers.", "labels": [], "entities": [{"text": "F 1-score)", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9645329912503561}]}, {"text": "To evaluate the individual contribution of different feature sets, we performed ablation experiment, presented in.", "labels": [], "entities": []}, {"text": "This test involves removing one of the following feature sets at a time: current token and its POS tag (TOK), combinations with two previous and two next tokens and their POS tags (CONT), named entity (NE), semantic category (SC), semantic orientation (SO), word frequency (WF), opinion target (OT), noun phrase related features (NP_F), and SAO pattern and semantic label (SAO_F).", "labels": [], "entities": [{"text": "SAO pattern and semantic label (SAO_F)", "start_pos": 341, "end_pos": 379, "type": "METRIC", "confidence": 0.6978786021471024}]}, {"text": "Some features complement each other, so that despite small individual contribution, a cumulative improvement is generally achieved by using them in a set..", "labels": [], "entities": []}, {"text": "Ablation experiment (F 1 -score).", "labels": [], "entities": [{"text": "Ablation", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9772192239761353}, {"text": "F 1 -score", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.979493111371994}]}, {"text": "The importance of a feature set is measured by F 1 -score on development and testing datasets for both domains separately.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9855293333530426}]}, {"text": "Feature sets are listed in descending order of their impact on overall performance.", "labels": [], "entities": []}, {"text": "The analysis shows that the most important feature set is the combination of Token and POS features.", "labels": [], "entities": []}, {"text": "Other features contribute to the performance to a smaller degree.", "labels": [], "entities": []}, {"text": "As can be seen, the relative influence of features on F 1 -score is similar on test and development sets, showing that our model effectively overcomes the overfitting problem.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9713763892650604}]}, {"text": "We conducted several experiments on the training data to prove the domain portability of our CRF model.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Results of classification with different training datasets (F 1 -score).", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9818904101848602}]}], "tableCaptions": [{"text": " Table 1. Distribution of the provided data.", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9562941789627075}]}, {"text": " Table 2. Performance on different datasets (F 1- score).", "labels": [], "entities": [{"text": "F 1- score", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9810848832130432}]}, {"text": " Table 3. Ablation experiment (F 1 -score).", "labels": [], "entities": [{"text": "Ablation experiment", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.971027672290802}, {"text": "F 1 -score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9680589586496353}]}, {"text": " Table 4. Results of classification with different  training datasets (F 1 -score).", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9810610115528107}]}, {"text": " Table 5. Error types distribution.", "labels": [], "entities": []}]}