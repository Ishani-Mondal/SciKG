{"title": [{"text": "Columbia NLP: Sentiment Detection of Sentences and Subjective Phrases in Social Media", "labels": [], "entities": [{"text": "Columbia NLP", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9581219851970673}, {"text": "Sentiment Detection of Sentences and Subjective Phrases in Social Media", "start_pos": 14, "end_pos": 85, "type": "TASK", "confidence": 0.8844893276691437}]}], "abstractContent": [{"text": "We present two supervised sentiment detection systems which were used to compete in SemEval-2014 Task 9: Sentiment Analysis in Twitter.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7670193016529083}, {"text": "SemEval-2014 Task 9", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.8816370964050293}, {"text": "Sentiment Analysis", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.9335976541042328}]}, {"text": "The first system (Rosenthal and McKeown, 2013) classifies the polarity of subjective phrases as positive, negative, or neutral.", "labels": [], "entities": []}, {"text": "It is tailored towards online genres, specifically Twitter, through the inclusion of dictionaries developed to capture vocabulary used in online conversations (e.g., slang and emoticons) as well as stylistic features common to social media.", "labels": [], "entities": []}, {"text": "The second system (Agarwal et al., 2011) classifies entire tweets as positive, negative, or neutral.", "labels": [], "entities": []}, {"text": "It too includes dictionaries and stylistic features developed for social media, several of which are distinctive from those in the first system.", "labels": [], "entities": []}, {"text": "We use both systems to participate in Subtasks A and B of SemEval-2014 Task 9: Sentiment Analysis in Twit-ter.", "labels": [], "entities": [{"text": "SemEval-2014 Task 9", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.8035995960235596}, {"text": "Sentiment Analysis", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.9435372650623322}]}, {"text": "We participated for the first time in Subtask B: Message-Level Sentiment Detection by combining the two systems to achieve improved results compared to either system alone.", "labels": [], "entities": [{"text": "Message-Level Sentiment Detection", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.7104850014050802}]}], "introductionContent": [{"text": "In this paper we describe two prior sentiment detection algorithms for social media.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8065685629844666}]}, {"text": "Both systems () classify the polarity of sentence phrases and tweets as positive, negative, or neutral.", "labels": [], "entities": []}, {"text": "These algorithms were used to participate in the the expression level task (Subtask A) and message level task (Subtask B) of the SemEval-2014 Task 9: Sentiment Analysis in Twitter () which one of the authors helped organize.", "labels": [], "entities": [{"text": "SemEval-2014 Task 9: Sentiment Analysis in Twitter", "start_pos": 129, "end_pos": 179, "type": "TASK", "confidence": 0.7821828722953796}]}, {"text": "We first show improved results compared to our participation in the prior year in the expressionlevel task (Subtask A) by incorporating anew dictionary and new features into the system.", "labels": [], "entities": []}, {"text": "Our focus this year was on Subtask B which we participated in for the first time.", "labels": [], "entities": [{"text": "Subtask B", "start_pos": 27, "end_pos": 36, "type": "TASK", "confidence": 0.9422644078731537}]}, {"text": "We integrated two systems to achieve improved results compared to either system alone.", "labels": [], "entities": []}, {"text": "Our analysis shows that the first system performs better on recall while the second system performs better on precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9989591836929321}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9988105297088623}]}, {"text": "We used confidence metrics outputted by the systems to determine which answer should be used.", "labels": [], "entities": []}, {"text": "This resulted in a slight improvement in the Twitter dataset compared to either system alone.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.9056833684444427}]}, {"text": "In this rest of this paper, we discuss related work, the methods for each system, and experiments and results for each subtask using the data provided by Semeval-2014 Task 9: Sentiment Analysis in Twitter ().", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter", "start_pos": 175, "end_pos": 204, "type": "TASK", "confidence": 0.8884625285863876}]}], "datasetContent": [{"text": "This task was evaluated on the Twitter dataset provided by Semeval-2013 Task 2, Subtask B. All results are shown using the average F-measure of the positive and negative class.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9938825368881226}]}, {"text": "The full results in the participation of SemEval 2014: Sentiment Analysis in Twitter, Subtask B, are shown in.", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.7049038708209991}, {"text": "Sentiment Analysis", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.9176285266876221}]}, {"text": "All the results outperform the majority baseline of the more prominent positive polarity class significantly.", "labels": [], "entities": []}, {"text": "The combined system outperforms the individual systems for the Twitter development and test set.", "labels": [], "entities": []}, {"text": "It does not outperform the sarcasm test set, but this maybe due to the small size; it contains only 100 tweets.", "labels": [], "entities": [{"text": "sarcasm test set", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.7302788297335306}]}, {"text": "The Tweet-Level system outperforms the phrase-based and combined system for the LiveJournal and SMS test sets.", "labels": [], "entities": [{"text": "LiveJournal and SMS test sets", "start_pos": 80, "end_pos": 109, "type": "DATASET", "confidence": 0.7226498901844025}]}, {"text": "A closer look at the results indicated that the phrase-based system has particular difficulty with the short sentences which are more common in SMS and LiveJournal.", "labels": [], "entities": []}, {"text": "For example, the average number of characters in a tweet is 120 whereas it is 95.6 in SMS messages ().", "labels": [], "entities": []}, {"text": "Short sentences are harder because there are fewer polarity words which causes the phrase-based system to incorrectly pick neutral.", "labels": [], "entities": []}, {"text": "In addition, short sentences are harder because the BOW feature space, which is huge and already sparse, becomes sparser and individual features start to over-fit.", "labels": [], "entities": []}, {"text": "Part of this problem is handled by using Senti-features so the space will be less sparse.", "labels": [], "entities": []}, {"text": "Our ranking in the Twitter 2013 and SMS 2013 development data is 18/50 and 20/50 respectively.", "labels": [], "entities": [{"text": "Twitter 2013 and SMS 2013 development data", "start_pos": 19, "end_pos": 61, "type": "DATASET", "confidence": 0.7905043704169137}]}, {"text": "Our rank in the Twitter 2014 test set is 15/50 and our rank in the LiveJournal test set is 19/50.", "labels": [], "entities": [{"text": "Twitter 2014 test set", "start_pos": 16, "end_pos": 37, "type": "DATASET", "confidence": 0.8920721262693405}, {"text": "LiveJournal test set", "start_pos": 67, "end_pos": 87, "type": "DATASET", "confidence": 0.9778463244438171}]}, {"text": "Based on our rankings it is clear that our systems are geared more towards Twitter than other social media.", "labels": [], "entities": []}, {"text": "Finally our ranking in the Sarcasm test set is 41/50.", "labels": [], "entities": [{"text": "Sarcasm test set", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.8479218284289042}]}, {"text": "Although this ranking is quite low, it is in fact encouraging.", "labels": [], "entities": []}, {"text": "It indicates that the sarcasm has switched the polarity of the tweet.", "labels": [], "entities": []}, {"text": "In the future we would like to include a system (e.g. () that can detect whether the tweet is sarcastic.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A comparison between the 2013 and 2014  results for Subtask A using the SemEval Twitter  training corpus. All results exceed the majority  baseline of the positive class significantly.", "labels": [], "entities": [{"text": "SemEval Twitter  training corpus", "start_pos": 82, "end_pos": 114, "type": "DATASET", "confidence": 0.6494637206196785}]}, {"text": " Table 2: A comparison between the different systems using the Twitter training corpus provided by the  SemEval task for Subtask B. All results exceed the majority baseline of the positive class significantly.", "labels": [], "entities": []}]}