{"title": [{"text": "Bielefeld SC: Orthonormal Topic Modelling for Grammar Induction", "labels": [], "entities": [{"text": "Grammar Induction", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7292886525392532}]}], "abstractContent": [{"text": "In this paper, we consider the application of topic modelling to the task of inducting grammar rules.", "labels": [], "entities": [{"text": "topic modelling", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7496311366558075}]}, {"text": "In particular, we look at the use of a recently developed method called orthonormal explicit topic analysis, which combines explicit and latent models of semantics.", "labels": [], "entities": [{"text": "orthonormal explicit topic analysis", "start_pos": 72, "end_pos": 107, "type": "TASK", "confidence": 0.6587562635540962}]}, {"text": "Although, it remains unclear how topic model maybe applied to the case of grammar induction, we show that it is not impossible and that this may allow the capture of subtle semantic distinctions that are not captured by other methods.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7316466122865677}]}], "introductionContent": [{"text": "Grammar induction is the task of inducing highlevel rules for application of grammars in spoken dialogue systems.", "labels": [], "entities": [{"text": "Grammar induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7839335501194}]}, {"text": "In practice, we can extract relevant rules and the task of grammar induction reduces to finding similar rules between two strings.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7222223728895187}]}, {"text": "As these strings are not necessarily similar in surface form, what we really wish to calculate is the semantic similarity between these strings.", "labels": [], "entities": []}, {"text": "As such, we could think of applying a semantic analysis method.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7716065049171448}]}, {"text": "As such we attempt to apply topic modelling, that is methods such as Latent Dirichlet Allocation (, Latent Semantic Analysis) or Explicit Semantic Analysis ().", "labels": [], "entities": [{"text": "Explicit Semantic Analysis", "start_pos": 129, "end_pos": 155, "type": "TASK", "confidence": 0.5553978582223257}]}, {"text": "In particular we build on the recent work to unify latent and explicit methods by means of orthonormal explicit topics.", "labels": [], "entities": []}, {"text": "In topic modelling the key choice is the document space that will act as the corpus and hence topic space.", "labels": [], "entities": [{"text": "topic modelling", "start_pos": 3, "end_pos": 18, "type": "TASK", "confidence": 0.8875371813774109}]}, {"text": "The standard choice is to regard all articles from a background document collection -Wikipedia articles area typical choice -as the This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": []}, {"text": "Page numbers and proceedings footer are added by the organisers.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by/4.0/ topic space.", "labels": [], "entities": []}, {"text": "However, it is crucial to ensure that these topics cover the semantic space evenly and completely.", "labels": [], "entities": []}, {"text": "Following) we remap the semantic space defined by the topics in such a manner that it is orthonormal.", "labels": [], "entities": []}, {"text": "In this way, each document is mapped to a topic that is distinct from all other topics.", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows: we describe our method in three parts, first the method in section 2, followed by approximation method in section 3, the normalization methods in section 4 and finally the application to grammar induction in section 5, we finish with some conclusions in section 6. 2 Orthonormal explicit topic analysis, Orthonormal explicit topic analysis) follows Explicit Semantic Analysis in the sense that it assumes the availability of a background document collection B = {b 1 , b 2 , ..., b N } consisting of textual representations.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 229, "end_pos": 246, "type": "TASK", "confidence": 0.707182839512825}, {"text": "Orthonormal explicit topic analysis", "start_pos": 309, "end_pos": 344, "type": "TASK", "confidence": 0.6346769407391548}, {"text": "Orthonormal explicit topic analysis", "start_pos": 346, "end_pos": 381, "type": "TASK", "confidence": 0.591184951364994}]}, {"text": "The mapping into the explicit topic space is defined by a language-specific function \u03a6 that maps documents into RN such that the j th value in the vector is given by some association measure \u03c6 j (d) for each background document b j . Typical choices for this association measure \u03c6 are the sum of the TF-IDF scores or an information retrieval relevance scoring function such as BM-25.", "labels": [], "entities": [{"text": "BM-25", "start_pos": 377, "end_pos": 382, "type": "DATASET", "confidence": 0.9222710728645325}]}, {"text": "For the case of TF-IDF, the value of the j-th element of the topic vector is given by: Thus, the mapping function can be represented as the product of a TF-IDF vector of document d multiplied by a word-by-document (W \u00d7 N ) TF-IDF matrix, which we denote as a X: 1 1T denotes the matrix transpose as usual For simplicity, we shall assume from this point on that all vectors are already converted to a TF-IDF or similar numeric vector form.", "labels": [], "entities": []}, {"text": "In order to compute the similarity between two documents d i and d j , typically the cosine-function (or the normalized dot product) between the vectors \u03a6(d i ) and \u03a6(d j ) is computed as follows: The key challenge with topic modelling is choosing a good background document collection B = {b 1 , ..., b N }.", "labels": [], "entities": [{"text": "topic modelling", "start_pos": 220, "end_pos": 235, "type": "TASK", "confidence": 0.7827917337417603}]}, {"text": "A simple minimal criterion fora good background document collection is that each document in this collection should be maximally similar to itself and less similar to any other document: As shown in, this property is satisfied by the following projection: And hence the similarity between two documents can be calculated as:", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}