{"title": [{"text": "AT&T: The Tag&Parse Approach to Semantic Parsing of Robot Spatial Commands", "labels": [], "entities": [{"text": "AT&T", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9416149258613586}, {"text": "Semantic Parsing of Robot Spatial Commands", "start_pos": 32, "end_pos": 74, "type": "TASK", "confidence": 0.8036636511484782}]}], "abstractContent": [{"text": "The Tag&Parse approach to semantic parsing first assigns semantic tags to each word in a sentence and then parses the tag sequence into a semantic tree.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7251191884279251}]}, {"text": "We use statistical approach for tagging, parsing , and reference resolution stages.", "labels": [], "entities": [{"text": "tagging", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9798831343650818}, {"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9192292094230652}, {"text": "reference resolution", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.8096361458301544}]}, {"text": "Each stage produces multiple hypotheses which are re-ranked using spatial validation.", "labels": [], "entities": []}, {"text": "We evaluate the Tag&Parse approach on a corpus of Robotic Spatial Commands as part of the SemEval Task6 exercise.", "labels": [], "entities": [{"text": "SemEval Task6 exercise", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.7455566922823588}]}, {"text": "Our system accuracy is 87.35% and 60.84% with and without spatial validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.998078465461731}]}], "introductionContent": [{"text": "In this paper we describe a system participating in the SemEval2014 Task-6 on Supervised Semantic Parsing of Robotic Spatial Commands.", "labels": [], "entities": [{"text": "SemEval2014 Task-6 on Supervised Semantic Parsing of Robotic Spatial Commands", "start_pos": 56, "end_pos": 133, "type": "TASK", "confidence": 0.7742220818996429}]}, {"text": "It produces a semantic parse of natural language commands addressed to a robot arm designed to move objects on a grid surface.", "labels": [], "entities": []}, {"text": "Each command directs a robot to change position of an object given a current configuration.", "labels": [], "entities": []}, {"text": "A command uniquely identifies an object and its destination, for example \"Move the turquoise pyramid above the yellow cube\".", "labels": [], "entities": []}, {"text": "System output is a Robot Control Language (RCL) parse (see) which is processed by the robot arm simulator.", "labels": [], "entities": [{"text": "Robot Control Language (RCL) parse", "start_pos": 19, "end_pos": 53, "type": "TASK", "confidence": 0.5817067452839443}]}, {"text": "The Robot Spatial Commands dataset is used for training and testing.", "labels": [], "entities": [{"text": "Robot Spatial Commands dataset", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.7381512522697449}]}, {"text": "Our system uses a Tag&Parse approach which separates semantic tagging and semantic parsing stages.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.6989374309778214}]}, {"text": "It has four components: 1) semantic tagging, 2) parsing, 3) reference resolution, and 4) spatial validation.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7415877878665924}, {"text": "reference resolution", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.6954308450222015}]}, {"text": "The first three are trained using LLAMA), a supervised machine learning toolkit, on the RCL-parsed sentences.", "labels": [], "entities": []}, {"text": "For semantic tagging, we train a maximum entropy sequence tagger for assigning a semantic label and value to each word in a sentence, such as type cube or color blue.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7609144151210785}]}, {"text": "For parsing, we train a constituency parser on non-lexical RCL semantic trees.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9730321168899536}]}, {"text": "For reference resolution, we train a maximum entropy model that identifies entities for reference tags found by previous components.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.9393006563186646}]}, {"text": "All of these components can generate multiple hypotheses.", "labels": [], "entities": []}, {"text": "Spatial validation re-ranks these hypotheses by validating them against the input spatial configuration.", "labels": [], "entities": [{"text": "Spatial validation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8563862442970276}]}, {"text": "The top hypothesis after re-ranking is returned by the system.", "labels": [], "entities": []}, {"text": "Separating tagging and parsing stages has several advantages.", "labels": [], "entities": [{"text": "Separating tagging", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.957486480474472}, {"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.8397714495658875}]}, {"text": "A tagging stage allows the system flexibility to abstract from possible grammatical or spelling errors in a command.", "labels": [], "entities": []}, {"text": "It assigns a semantic category to each word in a sentence.", "labels": [], "entities": []}, {"text": "Words not contributing to the semantic meaning are assigned 'O' label by the tagger and are ignored in the further processing.", "labels": [], "entities": [{"text": "O' label", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9175504247347513}]}, {"text": "Words that are misspelled can potentially receive a correct tag when a word similarity feature is used in building a tagging model.", "labels": [], "entities": []}, {"text": "This will be especially important when processing output of spoken commands that may contain recognition errors.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized thusly.", "labels": [], "entities": []}, {"text": "In Section 2 we describe each of the components used in our system.", "labels": [], "entities": []}, {"text": "In Section 3 we describe the results reported for SemEval2014 and evaluation of each system component.", "labels": [], "entities": [{"text": "SemEval2014", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.809562087059021}]}, {"text": "We summarize our findings and present future work in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "Avg # hyp Accuracy 1 TEST2500 1-best 1 86.0% 2 TEST2500 max-5 3.34 95.2% 3 TEST500 1-best 1 67.9% 4 TEST500 max-5 4.25 83.8% 5 DEV2500 1-best 1 90.8% 6 DEV2500 max-5 2.9 98.0%: Tagger accuracy for 1-best and maximum of 5-best hypotheses (max-5).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9409587979316711}, {"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.990856409072876}]}, {"text": "sentences using a random data split.", "labels": [], "entities": []}, {"text": "We observe that sentence length and standard deviation of test sentences in the TEST2500 data set is higher than on the training sentences while in the DEV2500 data set training and test sentence length and standard deviation are comparable.", "labels": [], "entities": [{"text": "TEST2500 data set", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.9853256543477377}, {"text": "DEV2500 data set", "start_pos": 152, "end_pos": 168, "type": "DATASET", "confidence": 0.9915977915128072}]}, {"text": "presents sentence accuracy of the semantic tagging stage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9939641356468201}, {"text": "semantic tagging", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7106787115335464}]}, {"text": "Tagging accuracy is evaluated on 1-best and on max-5 best tagger outputs.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9398674964904785}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9683589339256287}]}, {"text": "In the max-5 setting the number of hypotheses generated by the tagger varies for each input with the average numbers reported in.", "labels": [], "entities": []}, {"text": "Tagging accuracy on TEST2500 using 1-best is 86.0%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.987923800945282}, {"text": "TEST2500", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.9232585430145264}, {"text": "1-best", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.8637141585350037}]}, {"text": "Considering max-5 best tagging sequences, the accuracy is 95.2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9997977614402771}]}, {"text": "On the TEST500 data set tagging accuracy is 67.9% and 83.8% on 1-best and max-5 best sequences respectively, approximately 8% points lower than on TEST2500 data set.", "labels": [], "entities": [{"text": "TEST500 data set", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.9825766483942667}, {"text": "tagging", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.5661553144454956}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9765243530273438}, {"text": "TEST2500 data set", "start_pos": 147, "end_pos": 164, "type": "DATASET", "confidence": 0.9776011109352112}]}, {"text": "On the DEV2500 data set tagging accuracy is 90.8% and 98.0% on 1-best and max-5 best sequences, 4.8% and 2.8% points higher than on the TEST2500 data set.", "labels": [], "entities": [{"text": "DEV2500 data set", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.9870695471763611}, {"text": "tagging", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.5755494832992554}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9792322516441345}, {"text": "TEST2500 data set", "start_pos": 136, "end_pos": 153, "type": "DATASET", "confidence": 0.9591363469759623}]}, {"text": "The higher performance on DEV2500 in comparison to the TEST2500 can be explained by the higher complexity of the test sentences in comparison to the training sentences in the TEST2500 data set.", "labels": [], "entities": [{"text": "DEV2500", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9602640867233276}, {"text": "TEST2500", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.8968971967697144}, {"text": "TEST2500 data set", "start_pos": 175, "end_pos": 192, "type": "DATASET", "confidence": 0.9593069752057394}]}], "tableCaptions": [{"text": " Table 2: Number of sentences, average length and standard deviation of the data sets.", "labels": [], "entities": [{"text": "standard", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9583488702774048}]}, {"text": " Table 4: System accuracy with and without spatial  validation using automatically assigned tags and  oracle tags (OT).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9960904717445374}]}]}