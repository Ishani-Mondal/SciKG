{"title": [{"text": "ECNU: One Stone Two Birds: Ensemble of Heterogenous Measures for Semantic Relatedness and Textual Entailment", "labels": [], "entities": [{"text": "ECNU: One Stone Two Birds", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.8951819042364756}, {"text": "Semantic Relatedness", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.7865047454833984}, {"text": "Textual Entailment", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.6964372247457504}]}], "abstractContent": [{"text": "This paper presents our approach to semantic relatedness and textual entailment subtasks organized as task 1 in SemEval 2014.", "labels": [], "entities": [{"text": "semantic relatedness", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7460634410381317}, {"text": "textual entailment subtasks", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.7379973828792572}]}, {"text": "Specifically, we address two questions: (1) Can we solve these two sub-tasks together?", "labels": [], "entities": []}, {"text": "(2) Are features proposed for textual entailment task still effective for semantic relatedness task?", "labels": [], "entities": [{"text": "textual entailment task", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.7953567504882812}]}, {"text": "To address them, we extracted seven types of features including text difference measures proposed in entailment judgement subtask, as well as common text similarity measures used in both subtasks.", "labels": [], "entities": []}, {"text": "Then we exploited the same feature set to solve the both sub-tasks by considering them as a regression and a classification task respectively and performed a study of influence of different features.", "labels": [], "entities": []}, {"text": "We achieved the first and the second rank for relatedness and entailment task respectively.", "labels": [], "entities": [{"text": "relatedness", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9599961638450623}]}], "introductionContent": [{"text": "Distributional Semantic Models (DSMs)(surveyed in () exploit the co-occurrences of other words with the word being modeled to compute the semantic meaning of the word under the distributional hypothesis: \"similar words share similar contexts\".", "labels": [], "entities": []}, {"text": "Despite their success, DSMs are severely limited to model the semantic of long phrases or sentences since they ignore grammatical structures and logical words.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.9402087330818176}]}, {"text": "Compositional Distributional Semantic Models (CDSMs)(  al., 2012) extend DSMs to sentence level to capture the compositionality in the semantic vector space, which has seen a rapidly growing interest in recent years.", "labels": [], "entities": []}, {"text": "Although several CDSMs have been proposed, benchmarks are lagging behind.", "labels": [], "entities": []}, {"text": "Previous work () performed experiments on their own datasets or on the same datasets which are limited to a few hundred instances of very short sentences with a fixed structure.", "labels": [], "entities": []}, {"text": "To provide a benchmark so as to compare different CDSMs, the sentences involving compositional knowledge task in) develops a large dataset which is full of lexical, syntactic and semantic phenomena.", "labels": [], "entities": []}, {"text": "It consists of two subtasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 (very related); and textual entailment (TE) task, which determines whether one of the following three relationships holds between two given sentences A and B: (1) entailment: the meaning of B can be inferred from A; (2) contradiction: A contradicts B; (3) neutral: the truth of B cannot be inferred on the basis of A.", "labels": [], "entities": []}, {"text": "Semantic textual similarity (STS) (Lintean and Rus, 2012) and semantic relatedness are closely related and interchangeably used in many literatures except that the concept of semantic similarity is more specific than semantic relatedness and the latter includes concepts as antonymy and meronymy.", "labels": [], "entities": [{"text": "Semantic textual similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6975755741198858}]}, {"text": "In this paper we regard the semantic relatedness task as a STS task.", "labels": [], "entities": []}, {"text": "Besides, regardless of the original intention of this task, we adopted the mainstream machine learning methods instead of CDSMs to solve these two tasks by extracting heterogenous features.", "labels": [], "entities": []}, {"text": "Like semantic relatedness, TE task (surveyed in) is also closely related to STS task since in TE task lots of similarity measures at different levels are exploited to boost classification.", "labels": [], "entities": [{"text": "STS", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9361547231674194}]}, {"text": "For example,) used ten string similarity measures such as cosine similarity at the word and the character level.", "labels": [], "entities": []}, {"text": "Therefore, the first fundamental question arises, i.e., \"Can we solve both of these two tasks together?\"", "labels": [], "entities": []}, {"text": "At the same time, since high similarity does not mean entailment holds, the TE task also utilizes other features besides similarity measures.", "labels": [], "entities": [{"text": "TE task", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.873410165309906}]}, {"text": "For example, in our previous work) text difference features were proposed and proved to be effective.", "labels": [], "entities": []}, {"text": "Therefore, the second question surfaces here, i.e., \"Are features proposed for TE task still effective for STS task?\"", "labels": [], "entities": []}, {"text": "To answer the first question, we extracted seven types of features including text similarity and text difference and then fed them to classifiers and regressors to solve TE and STS task respectively.", "labels": [], "entities": [{"text": "TE", "start_pos": 170, "end_pos": 172, "type": "METRIC", "confidence": 0.8687105774879456}, {"text": "STS", "start_pos": 177, "end_pos": 180, "type": "METRIC", "confidence": 0.9171400666236877}]}, {"text": "Regarding the second question, we conducted a series of experiments to study the performance of different features for these two tasks.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly describes the related work on STS and TE tasks.", "labels": [], "entities": [{"text": "STS and TE tasks", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7674852162599564}]}, {"text": "Section 3 presents our systems including features, learning methods, etc.", "labels": [], "entities": []}, {"text": "Section 4 shows the experimental results on training data and Section 5 reports the results of our submitted systems on test data and gives a detailed analysis.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes this paper with future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the performance of different algorithms, we adopted the official evaluation measures, i.e., Pearson correlation coefficient for STS task and accuracy for TE task.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 113, "end_pos": 144, "type": "METRIC", "confidence": 0.964298407236735}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.999355137348175}]}, {"text": "To make a reasonable comparison between different algorithms, we performed 5-fold cross validation on training data with 5000 sentence pairs.", "labels": [], "entities": []}, {"text": "The parameters tuned in different algorithms are listed below: the trade-off parameter c in SVM, the number of trees n in RF, the number of boosting stages n in GB, the number of nearest neighbors kin kNN and the number of passes over the training data n in SGD.", "labels": [], "entities": []}, {"text": "The rest parameters are set to be default.: The 5-fold cross validation results on training data with mean and standard deviation for each algorithm.", "labels": [], "entities": []}, {"text": "reports the experimental results of 5-fold cross validation with mean and standard deviation and the optimal parameters on training data.", "labels": [], "entities": []}, {"text": "The results of semi-supervised learning methods are not listed because only a few parameters are tried due to the limit of time.", "labels": [], "entities": []}, {"text": "From this table we see that SVM, RF and GB perform comparable results to each other.", "labels": [], "entities": [{"text": "GB", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.8231135010719299}]}, {"text": "To answer the second question and explore the influences of different feature types, we performed a series of experiments under the best system setting.", "labels": [], "entities": []}, {"text": "shows the results of different feature combinations where for each time we selected and added one best feature type.", "labels": [], "entities": []}, {"text": "From this table, we find that for STS the most effective feature is cps and for TE task is td.", "labels": [], "entities": [{"text": "TE", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.9866225123405457}]}, {"text": "Almost all feature types have positive effects on performance.", "labels": [], "entities": []}, {"text": "Specifically, td alone achieves 81.063% in TE task which is quite close to the best performance (84.128%) and cps alone achieves 0.7544 in STS task.", "labels": [], "entities": [{"text": "TE", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9093440771102905}]}, {"text": "Moreover, the td feature proposed for TE task is quite effective in STS task as well, which suggests that text semantic difference measures are also crucial when measuring sentence similarity.", "labels": [], "entities": [{"text": "TE task", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.7478646337985992}]}, {"text": "Therefore the answer to the second question is yes.", "labels": [], "entities": []}, {"text": "It is clear that the features proposed for TE are also effective for STS and heterogenous features yield better performance than a single feature type.", "labels": [], "entities": [{"text": "TE", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9511677622795105}, {"text": "STS", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9771490693092346}]}], "tableCaptions": [{"text": " Table 2: The 5-fold cross validation results on  training data with mean and standard deviation for  each algorithm.", "labels": [], "entities": []}, {"text": " Table 4: The results of our five systems for two  tasks and the officially top-ranked systems.", "labels": [], "entities": []}]}