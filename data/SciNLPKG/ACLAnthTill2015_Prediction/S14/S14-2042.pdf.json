{"title": [{"text": "ECNU: Expression-and Message-level Sentiment Orientation Classification in Twitter Using Multiple Effective Features", "labels": [], "entities": [{"text": "ECNU", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8509411811828613}, {"text": "Sentiment Orientation Classification", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.8629311720530192}]}], "abstractContent": [{"text": "Microblogging websites (such as Twitter, Facebook) are rich sources of data for opinion mining and sentiment analysis.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.8296401798725128}, {"text": "sentiment analysis", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.9439412355422974}]}, {"text": "In this paper, we describe our approaches used for sentiment analysis in twitter (task 9) organized in SemEval 2014.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.962929755449295}, {"text": "twitter (task 9) organized in SemEval 2014", "start_pos": 73, "end_pos": 115, "type": "DATASET", "confidence": 0.6026657952202691}]}, {"text": "This task tries to determine whether the sentiment orientations conveyed by the whole tweets or pieces of tweets are positive, negative or neutral.", "labels": [], "entities": []}, {"text": "To solve this problem, we extracted several simple and basic features considering the following aspects: surface text, syntax, sentiment score and twitter characteristic.", "labels": [], "entities": []}, {"text": "Then we exploited these features to build a classifier using SVM algorithm.", "labels": [], "entities": []}, {"text": "Despite the simplicity of features , our systems rank above the average.", "labels": [], "entities": []}], "introductionContent": [{"text": "Microblogging services such as Twitter 1 , Facebook 2 today play an important role in expressing opinions on a variety of topics, discussing current issues or sharing one's feelings about different objects in our daily life.", "labels": [], "entities": []}, {"text": "Therefore, Twitter (and other platforms) has become a valuable source of users' sentiments and opinions and with the continuous and rapid growth of the number of tweets, analyzing the sentiments expressed in twitter has attracted more and more researchers and communities, for example, the sentiment analysis task in twitter was held in).", "labels": [], "entities": [{"text": "sentiment analysis task", "start_pos": 290, "end_pos": 313, "type": "TASK", "confidence": 0.8514910141626993}]}, {"text": "It will benefit lots of real applications such as simultaneously businesses, media outlets, and help investors to discover product trends, identify customer preferences and categorize users by analyzing these tweets ().", "labels": [], "entities": []}, {"text": "The task of sentiment analysis in twitter in SemEval) aims to classify whether a tweet's sentiment is positive, negative or neutral at expression level or message level.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8168359398841858}]}, {"text": "The expression-level subtask (i.e., subtask A) is to determine the sentiment of a marked instance of a word or phrase in the context of a given message, while the message-level subtask (i.e., subtask B) aims to determine the sentiment of a whole message.", "labels": [], "entities": []}, {"text": "Previous work ( showed that message-level sentiment classification is more difficult than that of expression-level (i.e., 0.690 vs 0.889 in terms of F-measure) since a message maybe composed of inconsistent sentiments.", "labels": [], "entities": [{"text": "message-level sentiment classification", "start_pos": 28, "end_pos": 66, "type": "TASK", "confidence": 0.729772667090098}, {"text": "F-measure", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9928948283195496}]}, {"text": "To date, lots of approaches have been proposed for conventional blogging sentiment analysis and a very broad overview is presented in.", "labels": [], "entities": [{"text": "blogging sentiment analysis", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.7237605353196462}]}, {"text": "Inspired by that, many features used in microblogging mining are adopted from traditional blogging sentiment analysis task.", "labels": [], "entities": [{"text": "microblogging mining", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7572358548641205}, {"text": "blogging sentiment analysis task", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.7377723827958107}]}, {"text": "For example, n-grams at the character or word level, part-of-speech tags, negations, sentiment lexicons were used inmost of current work.", "labels": [], "entities": []}, {"text": "They found that n-grams are still effective in spite of the short length nature of microblogging and the distributions of different POS tags in tweets with different polarities are highly different.", "labels": [], "entities": []}, {"text": "Compared with formal blog texts, tweets often contain many informal writings including slangs, emoticons, cre-ative spellings, abbreviations and special marks (i.e., mentions @ and hashtags #), and thus many twitter-specific features are proposed to characterize this phenomena.", "labels": [], "entities": []}, {"text": "For example, features record the number of emoticons, elongated words and hashtags were used in (.", "labels": [], "entities": []}, {"text": "In this work, we adopted many features from previous work and then these features were fed to SVM to perform classification.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our systems including preprocessing, feature representations, data sets, etc.", "labels": [], "entities": []}, {"text": "Results of two subtasks and discussions are reported in Section 3.", "labels": [], "entities": []}, {"text": "Finally, we conclude this paper in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "The organizers provide tweet ids and a script for all participants to collect data.", "labels": [], "entities": []}, {"text": "shows the statistics of the data set used in our experiments.", "labels": [], "entities": []}, {"text": "To examine the generalization of models trained on tweets, the test data provided by the organizers consists of instances from different domains for both subtasks.", "labels": [], "entities": []}, {"text": "Specifically, five corpora are included: LiveJournal(2014) is a collection of comments from LiveJournal blogs, SMS2013 is a SMS data set directly from last year, Twitter2013 is atwitter data set directly from last year, Twitter2014 is anew twitter data set and Twitter2014Sarcasm is a collection of tweets that contain sarcasm.", "labels": [], "entities": []}, {"text": "Notice that the data set SMS2013 and Twitter2013 were also used as our development set.", "labels": [], "entities": [{"text": "SMS2013", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.8904944062232971}, {"text": "Twitter2013", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.8622719049453735}]}, {"text": "Form Table 1, we find that (1) the class distributions of test data sets almost agree with training data sets for both subtasks, (2) the percentages of class neutral in two subtasks are significantly different (4.7% vs 45.5%), which reflects that a sentence which is composed of different sentiment expressions may act neutrality, (3) Twitter2014Sarcasm data set is very small.", "labels": [], "entities": [{"text": "Twitter2014Sarcasm data set", "start_pos": 335, "end_pos": 362, "type": "DATASET", "confidence": 0.9933554927508036}]}, {"text": "According to the guideline, we did not use any development data for training in the evaluation period.", "labels": [], "entities": []}, {"text": "We used macro-averaged F-measure of positive and negative classes (without neutral since it is margin in training data) to evaluate the performance of our systems and the averaged F-measure of five corpora was used to rank the final results.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.8811900615692139}, {"text": "F-measure", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9718685150146484}]}, {"text": "To explore the effectiveness of different feature types, we conducted a series of feature combination experiments using the constrained setting as shown in for both subtasks.", "labels": [], "entities": []}, {"text": "For each time we repeated to add one feature type to current feature set and then selected the best one until all the feature types were processed.", "labels": [], "entities": []}, {"text": "shows the results of different feature combinations and the best results are shown in bold font.", "labels": [], "entities": []}, {"text": "From, we find that (1) MPQA, n-gram and Word cluster are the most effective feature types to identify the polarities; (2) The POS tags make margin contribution to improve the performance since Stanford parser is designed for formal texts and in the future we may use specific parser instead; (3) The lexicon IMDB extracted from movie reviews has negative effects to classify twitter data, which indicates that there exist differences in the way of expressing sentiments between these two domains; (4) Twitter-specific features, i.e., hashtag and emoticon, are not as effective as expected.", "labels": [], "entities": []}, {"text": "This is because they are sparse in the data sets.", "labels": [], "entities": []}, {"text": "In subtask A with 16578 instances, only 292 instances (1.76%) have hashtags and 419 instances (2.52%) have emoticons.", "labels": [], "entities": []}, {"text": "In subtask B with 17458 messages, more instances have hashtags (16.72%) and emoticons (26.70%).", "labels": [], "entities": []}, {"text": "(5) For subtask A MPQA, n-gram, NRC and punctuation features achieve the best performance and for subtask B the best performance is achieved by using almost all features.", "labels": [], "entities": []}, {"text": "In summary, we find that n-gram and some lexicons such as MPQA are the most effective while twitter-specific features (i.e., hashtag and emoticon) are not as discriminating as expected and the main reason for this is that they are sparse in the data sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of data sets in training (train),  development (dev), test (test) set. Twitter2014S  stands for Twitter2014Sarcasm.", "labels": [], "entities": []}, {"text": " Table 3: Performance of our systems and the top-ranked systems (marked with asterisk).", "labels": [], "entities": []}]}