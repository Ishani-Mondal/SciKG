{"title": [{"text": "Swiss-Chocolate: Sentiment Detection using Sparse SVMs and Part-Of-Speech n-Grams", "labels": [], "entities": [{"text": "Swiss-Chocolate", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9292109608650208}, {"text": "Sentiment Detection", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.9757349491119385}]}], "abstractContent": [{"text": "We describe a classifier to predict the message-level sentiment of English micro-blog messages from Twitter.", "labels": [], "entities": []}, {"text": "This paper describes the classifier submitted to the SemEval-2014 competition (Task 9B).", "labels": [], "entities": [{"text": "SemEval-2014 competition", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.8141230344772339}]}, {"text": "Our approach was to buildup on the system of the last year's winning approach by NRC Canada 2013 (Mohammad et al., 2013), with some modifications and additions of features, and additional sentiment lexicons.", "labels": [], "entities": [{"text": "NRC Canada 2013 (Mohammad et al., 2013)", "start_pos": 81, "end_pos": 120, "type": "DATASET", "confidence": 0.9293651342391968}]}, {"text": "Furthermore, we used a sparse (1-regularized) SVM, instead of the more commonly used 2-regularization, resulting in a very sparse linear classifier.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the immense growth of user generated text online, the interest in automatic sentiment analysis of text has greatly increased recently in both academia and industry.", "labels": [], "entities": [{"text": "automatic sentiment analysis of text", "start_pos": 71, "end_pos": 107, "type": "TASK", "confidence": 0.7768947422504425}]}, {"text": "In this paper, we describe our approach fora modified SVM based classifier for short text as in Twitter messages.", "labels": [], "entities": []}, {"text": "Our system has participated in the SemEval-2014 Task 9 competition, \"Sentiment Analysis in Twitter, Subtask-B Message Polarity Classification\" ().", "labels": [], "entities": [{"text": "SemEval-2014 Task 9", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7883142828941345}, {"text": "Sentiment Analysis", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.9274144768714905}, {"text": "Subtask-B Message Polarity Classification", "start_pos": 100, "end_pos": 141, "type": "TASK", "confidence": 0.6320350468158722}]}, {"text": "The goal is to classify a tweet (on the full message level) into the three classes positive, negative, and neutral.", "labels": [], "entities": []}, {"text": "An almost identical competition was already run in 2013.", "labels": [], "entities": []}, {"text": "Our Results in the Competition.", "labels": [], "entities": []}, {"text": "Our approach was ranked on the 8th place out of the 50 participating submissions, with an F1-score of 67.54 on the Twitter-2014 test set.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9992092251777649}, {"text": "Twitter-2014 test set", "start_pos": 115, "end_pos": 136, "type": "DATASET", "confidence": 0.8265155951182047}]}, {"text": "The 2014 winning team obtained an average F1-score of 70.96.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9990447163581848}]}, {"text": "(The more detailed rankings of our approach were 4th rank on the LiveJournal data, 5th on the SMS data (2013), 18th on Twitter-2013, and 16th on Twitter Sarcasm, see for full details and all results).", "labels": [], "entities": [{"text": "LiveJournal data", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.9852415323257446}, {"text": "SMS data", "start_pos": 94, "end_pos": 102, "type": "DATASET", "confidence": 0.8045876324176788}]}, {"text": "In the competition, the tweets for training and development were only provided as tweet IDs.", "labels": [], "entities": []}, {"text": "A fraction (10-15%) of the tweets were no longer available on twitter, which makes the results of the competition not fully comparable.", "labels": [], "entities": []}, {"text": "For testing, in addition to last years data (tweets and SMS), new tweets and data from a surprise domain were provided.", "labels": [], "entities": []}, {"text": "An overview of the data, which we were able to download, is shown in.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Overview of the data we found available  for training, development and testing.", "labels": [], "entities": []}]}