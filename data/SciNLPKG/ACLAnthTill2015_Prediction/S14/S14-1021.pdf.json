{"title": [{"text": "Dead parrots make bad pets: Exploring modifier effects in noun phrases", "labels": [], "entities": []}], "abstractContent": [{"text": "Sometimes modifiers have a strong effect on core aspects of the meaning of the nouns they are attached to: A parrot is a desirable pet, but a dead parrot is, at the very least, a rather unusual household companion.", "labels": [], "entities": []}, {"text": "In order to stimulate computational research into the impact of modification on phrase meaning, we collected and made available a large dataset containing subject ratings fora variety of noun phrases and the categories they might belong to.", "labels": [], "entities": []}, {"text": "We propose to use compositional distributional semantics to model these data, experimenting with numerous distri-butional semantic spaces, phrase composition methods and asymmetric similarity measures.", "labels": [], "entities": [{"text": "phrase composition", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.7342667281627655}]}, {"text": "Our models capture a statistically significant portion of the data, although much work is still needed before we achieve a full computational account of modification effects.", "labels": [], "entities": []}], "introductionContent": [{"text": "Not all modifiers are created equal.", "labels": [], "entities": []}, {"text": "Green parrots have all essential qualities of parrots, but dead parrots don't.", "labels": [], "entities": []}, {"text": "For example, as vocally argued by the disgruntled costumer in Monty Python's famous Dead Parrot Sketch, 1 dead parrots make rather poor pet birds.", "labels": [], "entities": []}, {"text": "In modifier-head constructions (that, for the purpose of this article, we restrict to right-headed adjective-noun and noun-noun constructions), modifiers are not simply picking a subset of the denotation of the head they modify, but they are often distorting the properties of the head in a radical manner.", "labels": [], "entities": []}, {"text": "These modifier effects on phrase meaning have been studied extensively by theoretical linguists, who have focused primarily on the extreme case of intensional modifiers such as fake, alleged and toy, where the phrase denotes something that is no longer (or is not necessarily) ahead (a toy gun is not a gun).", "labels": [], "entities": []}, {"text": "See McNally (2013) fora recent review of the linguistic literature.", "labels": [], "entities": []}, {"text": "Cognitive scientists have looked at modification phenomena within the general study of conceptual combination (see Chapter 12 of for an extensive review).", "labels": [], "entities": []}, {"text": "The cognitive tradition has focused on how modification affects prototypicality: a guppy is the prototypical pet fish, but it is neither atypical pet nor atypical fish.", "labels": [], "entities": []}, {"text": "This line of research has highlighted how strong modification effects might be the rule, rather than the exception: reports that, when subjects were asked to provide the meaning for more than 200 novel modifierhead constructions, \"70% [of the answers] involved the construal of a noun's referent as something other than the typical category named by the noun.\"", "labels": [], "entities": []}, {"text": "Indeed, recent research suggests that even the most stereotypical modifiers affect prototypicality, so that subjects are less willing to attribute to quacking ducks such obvious duck properties as having webbed feet.", "labels": [], "entities": []}, {"text": "The impact of modification on phrase meaning is not only very interesting from a linguistic and cognitive perspective, but also important from a practical point of view, as it might affect expected entailment patterns: If parrot entails pet, then lively parrot also entails pet.", "labels": [], "entities": []}, {"text": "However, as we saw above, dead parrot doesn't necessarily entail pet (at least not from the point of view of a disgruntled costumer who was just sold the corpse).", "labels": [], "entities": []}, {"text": "Being able to track the impact that modifiers have on heads should thus have a positive effect on important tasks such as recognizing textual entailment, paraphrasing and anaphora resolution.", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 122, "end_pos": 152, "type": "TASK", "confidence": 0.7993480364481608}, {"text": "anaphora resolution", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.7065581232309341}]}, {"text": "Despite their theoretical and practical import, modification effects have been largely overlooked in computational linguistics, with the notable exception of, who only focused on the extreme case of intensional adjectives, studied a limited number of modifiers, and did not attempt to capture the graded nature of modification (a dead parrot is not a prototypical animal, but a toy parrot is not an animal at all).", "labels": [], "entities": []}, {"text": "This paper aims to stimulate computational research into modifier effects on phrase meaning in two ways.", "labels": [], "entities": []}, {"text": "First, we introduce anew, large, publicly available data set of modifier-head phrases annotated with four kinds of modification-related subject ratings: whether the concept denoted by the phrase is an instance of the concept denoted by its head (is a dead parrot still a parrot?), to what extent it is a member of one of the larger categories the head belongs to (is it still a pet?), and typicality ratings for the same questions (how typical is a dead parrot as a parrot? and as a pet?).", "labels": [], "entities": [{"text": "typicality", "start_pos": 389, "end_pos": 399, "type": "METRIC", "confidence": 0.9448567032814026}]}, {"text": "Second, we present a first attempt to model the collected judgments computationally.", "labels": [], "entities": []}, {"text": "We choose distributional semantics as our frame of reference, as it produces continuous similarity scores, inline with the graded nature of the modification effects we are investigating.", "labels": [], "entities": []}, {"text": "In particular, we look at the compositional extension of distributional semantics, because we need representations not only for words, but also phrases, and we adopt the asymmetric similarity measures developed in the literature on lexical entailment), because we are interested in an asymmetric relation (to what extent the concept denoted by the phrase is a good instance of the target class, and not vice versa).", "labels": [], "entities": []}, {"text": "As far as we know, this is the first time these asymmetric measures are applied to composed representations ( experimented with entailment measures applied to phrase representations directly harvested from corpora, and not derived compositionally).", "labels": [], "entities": []}, {"text": "We are thus also providing a novel evaluation of compositional models and asymmetric measures on a challenging task where they could potentially be very useful.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given the methods described above, the main question we want to answer is: Which combination of compositional model and asymmetric similarity measure yields a better fit for the data in the NBP dataset?", "labels": [], "entities": [{"text": "NBP dataset", "start_pos": 190, "end_pos": 201, "type": "DATASET", "confidence": 0.9717219173908234}]}, {"text": "We start however with a sanity check on the ability of the measures to capture the direction of the instance-class membership relation.", "labels": [], "entities": []}, {"text": "Even a measure that is good at capturing degrees of membership/typicality won't be of much practical use clarkede weedsprec balapinc cosweeds invcl: Number of spaces (over totals of 16 lowrank and 4 full-rank spaces) in which each measure was able to predict class membership direction significantly above chance.", "labels": [], "entities": []}, {"text": "if it is notable to tell us which item in a pair is the instance and which is the class.", "labels": [], "entities": []}, {"text": "Detecting membership direction As described in Section 2 above, NBP also contains singleword h \u2192 c pairs (parrot\u2192 pet).", "labels": [], "entities": [{"text": "NBP", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.8717893362045288}]}, {"text": "We extracted the subset of those that all judges considered to be in the category membership relation, and we checked them manually to make sure that the direction was one-way only.", "labels": [], "entities": []}, {"text": "This resulted in a set of 639 pairs where the membership relation holds unidirectionally.", "labels": [], "entities": []}, {"text": "We tested all combination of semantic spaces (Section 3.3) and asymmetric similarity measures (Section 3.2) on the task of assigning a higher score to the pairs in the h \u2192 c (vs. c \u2192 h) direction (e.g., (score(parrot \u2192 pet) > score(pet \u2192 parrot)).", "labels": [], "entities": []}, {"text": "reports, for each measure, the number of spaces in which the measure was able to predict membership direction significantly better than chance (binomial test, p < 0.05).", "labels": [], "entities": []}, {"text": "We report results on full-and low-rank (SVD, NMF) spaces separately since, as discussed above, for most composition models we can only use the latter.", "labels": [], "entities": []}, {"text": "We observe that all measures are able to significantly detect directionality in at least some spaces.", "labels": [], "entities": []}, {"text": "For all the analyses below, we exclude from further testing the space-measure combinations that failed to pass this sanity check, since they are clearly failing to capture properties pertaining to the instance-class relation (if a combination is notable to tell that it is a parrot that is a pet, and not vice versa, there is no point in asking if the same combination is able to model how typical a dead parrot is as a pet).", "labels": [], "entities": []}, {"text": "Modeling typicality ratings of mh \u2192 c pairs Next, for each of the remaining spaces, we first performed composition as described in Section 3.1 above to build the representations for the nominal phrases in the NBP dataset, and then computed asymmetric similarity scores for pairs made of a phrase and the corresponding potential class.", "labels": [], "entities": [{"text": "NBP dataset", "start_pos": 209, "end_pos": 220, "type": "DATASET", "confidence": 0.9823067784309387}]}, {"text": "We computed the correlations between mean human membership or typicality ratings and the scores produced with each combination of composition model, similarity measure and space.", "labels": [], "entities": []}, {"text": "The resulting performance profiles for membership and typicality are very highly correlated (r = .99), and we thus report only the latter.", "labels": [], "entities": [{"text": "membership", "start_pos": 39, "end_pos": 49, "type": "TASK", "confidence": 0.8365700840950012}, {"text": "typicality", "start_pos": 54, "end_pos": 64, "type": "TASK", "confidence": 0.8637175559997559}]}, {"text": "We leave it to further work to devise measures that are more specifically tuned to capture membership or typicality.", "labels": [], "entities": []}, {"text": "reports the top correlation coefficients between typicality judgments and scores of each mh \u2192 c pair (dead parrot\u2192 pet) across spaces, organized by measures and composition methods.", "labels": [], "entities": [{"text": "typicality judgments", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.9105581641197205}]}, {"text": "The best correlation is achieved with the weedsprec measure using the mult composition model in a full-rank space (precisely that of context window size 2 and ppmi weighting).", "labels": [], "entities": [{"text": "weedsprec measure", "start_pos": 42, "end_pos": 59, "type": "METRIC", "confidence": 0.8040217757225037}]}, {"text": "Recall that mult returns the component-wise product of the vectors it combines.", "labels": [], "entities": []}, {"text": "Thus, modification under mult is carried out by picking only those features of the head that are also present in the modifier, and enhancing them by a factor given by the modifier's feature value.", "labels": [], "entities": []}, {"text": "The weedsprec measure is then given by the weighted proportion of active features in mh that are also active inc.", "labels": [], "entities": []}, {"text": "Therefore, the more the modifier shares features with the parent category, the higher weedsprec will be.", "labels": [], "entities": []}, {"text": "This might explain why weedsprec is a good fit for the mult model in measuring degrees of category typicality.", "labels": [], "entities": [{"text": "weedsprec", "start_pos": 23, "end_pos": 32, "type": "DATASET", "confidence": 0.9750661253929138}]}, {"text": "Looking at composition methods, there is no evidence that the more complex, matrix-based fulladd and lexfunc approaches are performing any better than the simple multiplicative and additive methods.", "labels": [], "entities": []}, {"text": "Indeed, mult shows the most consistent overall performance, confirming the conclusion of Blacoe and Lapata (2012) that, at the present time, when it comes to composition, \"simpler is better\".", "labels": [], "entities": []}, {"text": "A related point emerges from the comparison of the low-and full-rank results for mult and wadd.", "labels": [], "entities": []}, {"text": "The smoothing process due to dimensionality reduction is quite disruptive for the current asymmetric measures, that are based on feature inclusion.", "labels": [], "entities": []}, {"text": "This is a further reason to stick to simpler composition methods, that can be applied directly in the full-rank spaces.", "labels": [], "entities": []}, {"text": "Regarding the measures themselves, we see that cosweeds, that balances weedsprec with the classic cosine score, is the most robust, returning good  results across all composition methods.", "labels": [], "entities": [{"text": "weedsprec", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.9499635100364685}]}, {"text": "On the other hand, the related clarkede and invcl measures turnout to be quite brittle.", "labels": [], "entities": []}, {"text": "The highly significant correlations show that the measures do capture to some extent the patterns of variance in the data.", "labels": [], "entities": []}, {"text": "However, when considering potential practical applications, even the highest reported correlation (.39) is certainly not impressive, indicating that there is plenty of room for further research into developing better composition methods and/or membership/typicality measures.", "labels": [], "entities": [{"text": "correlation", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.8655232191085815}]}, {"text": "Focusing on the modifier effect for mh \u2192 c pairs The typicality judgment for dead parrot as a pet is influenced by two factors: how typical parrots are as pets, and how much more or less typical dead parrots are as pets, as opposed to parrots in general.", "labels": [], "entities": []}, {"text": "A good model must be able to capture both factors (and this is what we tested above).", "labels": [], "entities": []}, {"text": "However, we are also interested in assessing to what extent the models are capturing the modification effectproper, as opposed to the overall degree of typicality of the h concept as member of the c category.", "labels": [], "entities": []}, {"text": "To focus on the modification factor, we partialed out the h \u2192 c (parrot\u2192 pet) ratings from the mh\u2192c (dead parrot\u2192pet) ratings and from the corresponding model scores (that is, we correlated the residuals of mh\u2192c ratings and model-produced scores after regressing the h\u2192c ratings on both).", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Correlations are lower overall, but the general picture from the previous analysis still holds, confirming that the computational models are (also) capturing modifier effects.", "labels": [], "entities": []}, {"text": "Interestingly, wadd, dil and fulladd generally undergo larger performance drops than mult and lexfunc.", "labels": [], "entities": []}, {"text": "Evidently, models like the latter, in which the modifier selects the relevant features from the head, are better suited to explain modification than the former, in which  Modeling typicality ratings of mh \u2192 h pairs We repeated the first analysis for pairs of the type mh \u2192 h (dead parrot\u2192 parrot).", "labels": [], "entities": []}, {"text": "The results, shown in, are lower than in the previous analysis.", "labels": [], "entities": []}, {"text": "This is probably due to the fact that, as discussed in Section 2, when the very same concept is used as phrase head and category, judgments are subject to a strong ceiling effect, and none of our measures is designed to flatten out above a certain threshold.", "labels": [], "entities": []}, {"text": "Indeed, if we measure the skewness of the typicality ratings, we obtain that, while for h \u2192 c and mh \u2192 c the skewness is of \u22121.9 and \u22121.5, respectively, for mh \u2192 hit gets to \u22123.9.", "labels": [], "entities": []}, {"text": "In any case, the results confirm the brittleness of the clarkede and invcl measures.", "labels": [], "entities": []}, {"text": "The linguistically motivated lexfunc model emerges here as a competitive alternative to the simpler models.", "labels": [], "entities": []}, {"text": "Still, the best results are obtained with mult and cosweeds (on the full-rank, context window size 20, ppmi weighted space).", "labels": [], "entities": []}, {"text": "Notably, weedsprec applied to a pair of the type mh \u2192 h, where the phrase is constructed using the mult model, results in a constant value of 1, whatever the modifier and the head noun is.", "labels": [], "entities": []}, {"text": "This is due to the fact that the features of a phrase composed using mult area subset of the features of the head, and in this case the head is the same as the category.", "labels": [], "entities": []}, {"text": "Therefore, by definition, weedsprec yields a score of 1 for every pair, the variance is null and hence the correlation is unde-: Percentage Pearson r between asymmetric similarity measures and mh \u2192 h typicality ratings.", "labels": [], "entities": [{"text": "weedsprec", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.885663628578186}, {"text": "Percentage Pearson r", "start_pos": 129, "end_pos": 149, "type": "METRIC", "confidence": 0.7848344445228577}]}, {"text": "*p < 0.001, +p < 0.05 fined.", "labels": [], "entities": []}, {"text": "As a consequence, in this case cosweeds, which is the geometric mean between weedsprec and cosine, reduces to cosine similarity!", "labels": [], "entities": []}, {"text": "The latter might be effective in capturing the degree of similarity between the phrase and its potential category but, as asymmetric measure, it cannot, alone, provide a full account of category typicality effects.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: NBP summary statistics: Mean average  ratings and their standard deviations across pairs,  itemized by instance-class type and in total. Mem- bership values range from 0 to 1, typicality values  from 1 to 7.", "labels": [], "entities": [{"text": "NBP", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.7794387340545654}, {"text": "Mean average  ratings", "start_pos": 34, "end_pos": 55, "type": "METRIC", "confidence": 0.922557532787323}, {"text": "Mem- bership", "start_pos": 147, "end_pos": 159, "type": "METRIC", "confidence": 0.929534912109375}]}, {"text": " Table 3: Number of spaces (over totals of 16 low- rank and 4 full-rank spaces) in which each mea- sure was able to predict class membership direc- tion significantly above chance.", "labels": [], "entities": []}, {"text": " Table 5: Percentage Pearson r between asymmet- ric similarity measures and mh \u2192 c typicality rat- ings where h \u2192 c scores have been partialed out.  *p < 0.001, +p < 0.05", "labels": [], "entities": [{"text": "Percentage Pearson r", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8423899014790853}, {"text": "typicality rat- ings", "start_pos": 83, "end_pos": 103, "type": "METRIC", "confidence": 0.7428258657455444}]}, {"text": " Table 6: Percentage Pearson r between asymmet- ric similarity measures and mh \u2192 h typicality rat- ings. *p < 0.001, +p < 0.05", "labels": [], "entities": [{"text": "Percentage Pearson r", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.838623027006785}, {"text": "typicality rat- ings", "start_pos": 83, "end_pos": 103, "type": "METRIC", "confidence": 0.8325231373310089}]}]}