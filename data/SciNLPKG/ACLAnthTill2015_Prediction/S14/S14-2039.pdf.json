{"title": [{"text": "DLS@CU: Sentence Similarity from Word Alignment", "labels": [], "entities": [{"text": "Sentence Similarity from Word Alignment", "start_pos": 8, "end_pos": 47, "type": "TASK", "confidence": 0.8383170962333679}]}], "abstractContent": [{"text": "We present an algorithm for computing the semantic similarity between two sentences.", "labels": [], "entities": []}, {"text": "It adopts the hypothesis that semantic similarity is a monotonically increasing function of the degree to which (1) the two sentences contain similar semantic units, and (2) such units occur in similar semantic contexts.", "labels": [], "entities": []}, {"text": "With a simplis-tic operationalization of the notion of semantic units with individual words, we experimentally show that this hypothesis can lead to state-of-the-art results for sentence-level semantic similarity.", "labels": [], "entities": [{"text": "sentence-level semantic similarity", "start_pos": 178, "end_pos": 212, "type": "TASK", "confidence": 0.6181517640749613}]}, {"text": "At the Sem-Eval 2014 STS task (task 10), our system demonstrated the best performance (mea-sured by correlation with human annotations) among 38 system runs.", "labels": [], "entities": [{"text": "Sem-Eval 2014 STS task (task 10)", "start_pos": 7, "end_pos": 39, "type": "TASK", "confidence": 0.6330409944057465}]}], "introductionContent": [{"text": "Semantic textual similarity (STS), in the context of short text fragments, has drawn considerable attention in recent times.", "labels": [], "entities": [{"text": "Semantic textual similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8535866936047872}]}, {"text": "Its application spans a multitude of areas, including natural language processing, information retrieval and digital learning.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.6637564798196157}, {"text": "information retrieval", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.8243172764778137}]}, {"text": "Examples of tasks that benefit from STS include text summarization, machine translation, question answering, short answer scoring, and soon.", "labels": [], "entities": [{"text": "STS", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9352051615715027}, {"text": "text summarization", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7478287220001221}, {"text": "machine translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7994997799396515}, {"text": "question answering", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8572608232498169}, {"text": "short answer scoring", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.5614491204420725}, {"text": "soon", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9655494689941406}]}, {"text": "The annual series of SemEval STS tasks () is an important platform where STS systems are evaluated on common data and evaluation criteria.", "labels": [], "entities": [{"text": "SemEval STS tasks", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.9087152282396952}]}, {"text": "In this article, we describe an STS system which participated and outperformed all other systems at SemEval 2014.", "labels": [], "entities": []}, {"text": "The algorithm is a straightforward application of the monolingual word aligner presented in (Sul-).", "labels": [], "entities": []}, {"text": "This aligner aligns related words in two sentences based on the following properties of the words: 1.", "labels": [], "entities": []}, {"text": "2. They occur in similar semantic contexts in the respective sentences.", "labels": [], "entities": []}, {"text": "The output of the word aligner fora sentence pair can be used to predict the pair's semantic similarity by taking the proportion of their aligned content words.", "labels": [], "entities": []}, {"text": "Intuitively, the more semantic components in the sentences we can meaningfully align, the higher their semantic similarity should be.", "labels": [], "entities": []}, {"text": "In experiments on STS 2013 data reported by, this approach was found highly effective.", "labels": [], "entities": [{"text": "STS 2013 data", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.7356096903483073}]}, {"text": "We also adopt this hypothesis of semantic compositionality for STS 2014.", "labels": [], "entities": [{"text": "semantic compositionality", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.7060750126838684}, {"text": "STS 2014", "start_pos": 63, "end_pos": 71, "type": "TASK", "confidence": 0.6441670656204224}]}, {"text": "We implement an STS algorithm that is only slightly different from the algorithm in).", "labels": [], "entities": []}, {"text": "The approach remains equally successful on STS 2014 data.", "labels": [], "entities": [{"text": "STS 2014 data", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.7785871624946594}]}], "datasetContent": [{"text": "We submitted the results of two system runs at SemEval 2014 based on the idea presented in Section 3.", "labels": [], "entities": []}, {"text": "The two runs were identical, except for the fact that for the OnWN test set, we specified the following words as additional stop words during run 2 (but not during run 1): something, someone, somebody, act, activity, some, state.", "labels": [], "entities": [{"text": "OnWN test set", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.9799645145734152}]}, {"text": "For both  runs, the tweet-news sentences were preprocessed by separating the hashtag from the word for each hashtagged word.", "labels": [], "entities": []}, {"text": "shows the performance of each run.", "labels": [], "entities": []}, {"text": "Rows 1 through 6 show the Pearson correlation coefficients between the system scores and human annotations for all test sets.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 26, "end_pos": 45, "type": "METRIC", "confidence": 0.9406608939170837}]}, {"text": "The last row shows the value of the final evaluation metric, which is a weighted sum of all correlations in rows 1-6.", "labels": [], "entities": []}, {"text": "The weight assigned to a data set is proportional to its number of pairs.", "labels": [], "entities": []}, {"text": "Our run 1 ranked 7th and run 2 ranked 1st among 38 submitted system runs.", "labels": [], "entities": []}, {"text": "An important implication of these results is the fact that knowledge of domain-specific stop words can be beneficial for an STS system.", "labels": [], "entities": []}, {"text": "Even though we imparted this knowledge to our system during run 2 via a manually constructed set of additional stop words, simple measures like TF-IDF can be used to automate the process.", "labels": [], "entities": [{"text": "TF-IDF", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.904971718788147}]}], "tableCaptions": [{"text": " Table 1: Test sets for SemEval STS 2014.", "labels": [], "entities": [{"text": "SemEval STS 2014", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.8411900003751119}]}, {"text": " Table 2: Results of evaluation on SemEval STS  2014 data. Each value on columns 2 and 3 is the  correlation between system output and human an- notations for the corresponding data set. The last  row shows the value of the final evaluation metric.", "labels": [], "entities": [{"text": "SemEval STS  2014 data", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.7096689492464066}]}, {"text": " Table 3: Results of evaluation on *SEM STS 2013  data.", "labels": [], "entities": [{"text": "SEM STS 2013  data", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.7405015826225281}]}, {"text": " Table 4: Results of evaluation on SemEval STS  2012 data.", "labels": [], "entities": [{"text": "SemEval STS  2012 data", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.7771557122468948}]}]}