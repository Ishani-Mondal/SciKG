{"title": [{"text": "AI-KU: Using Co-Occurrence Modeling for Semantic Similarity", "labels": [], "entities": [{"text": "Similarity", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.5792904496192932}]}], "abstractContent": [{"text": "In this paper, we describe our unsupervised method submitted to the Cross-Level Semantic Similarity task in Semeval 2014 that computes semantic similarity between two different sized text fragments.", "labels": [], "entities": [{"text": "Cross-Level Semantic Similarity task in Semeval 2014", "start_pos": 68, "end_pos": 120, "type": "TASK", "confidence": 0.8080622638974871}]}, {"text": "Our method models each text fragment by using the co-occurrence statistics of either occurred words or their substitutes.", "labels": [], "entities": []}, {"text": "The co-occurrence mod-eling step provides dense, low-dimensional embedding for each fragment which allows us to calculate semantic similarity using various similarity metrics.", "labels": [], "entities": []}, {"text": "Although our current model avoids the syntactic information , we achieved promising results and outperformed all baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic similarity is a measure that specifies the similarity of one text's meaning to another's.", "labels": [], "entities": []}, {"text": "Semantic similarity plays an important role in various Natural Language Processing (NLP) tasks such as textual entailment, summarization (, question answering (), text classification), word sense disambiguation and information retrieval.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7183002829551697}, {"text": "summarization", "start_pos": 123, "end_pos": 136, "type": "TASK", "confidence": 0.9752520322799683}, {"text": "question answering", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.7173400372266769}, {"text": "text classification", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.7719566822052002}, {"text": "word sense disambiguation", "start_pos": 185, "end_pos": 210, "type": "TASK", "confidence": 0.7102382381757101}, {"text": "information retrieval", "start_pos": 215, "end_pos": 236, "type": "TASK", "confidence": 0.8133552968502045}]}, {"text": "There are three main approaches to computing the semantic similarity between two text fragments.", "labels": [], "entities": [{"text": "computing the semantic similarity between two text fragments", "start_pos": 35, "end_pos": 95, "type": "TASK", "confidence": 0.7149162068963051}]}, {"text": "The first approach uses Vector Space Models (see for an overview) where each text is represented as a bag-of-word model.", "labels": [], "entities": []}, {"text": "The similarity between two text fragments can then be computed with various metrics such as cosine similarity.", "labels": [], "entities": []}, {"text": "Sparseness in the input nature is the key problem for these models.", "labels": [], "entities": [{"text": "Sparseness", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9551345705986023}]}, {"text": "Therefore, later works such as Latent Semantic Indexing (?) and This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": [{"text": "Latent Semantic Indexing", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.6578223407268524}]}, {"text": "Page numbers and proceedings footer are added by the organisers.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by/4.0/ Topic Models () overcome sparsity problems via reducing the dimensionality of the model by introducing latent variables.", "labels": [], "entities": []}, {"text": "The second approach blends various lexical and syntactic features and attacks the problem through machine learning models.", "labels": [], "entities": []}, {"text": "The third approach is based on word-to-word similarity alignment (.", "labels": [], "entities": [{"text": "word-to-word similarity alignment", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.6715236902236938}]}, {"text": "The Cross-Level Semantic Similarity (CLSS) task in SemEval 2014) provides an evaluation framework to assess similarity methods for texts in different volumes (i.e., lexical levels).", "labels": [], "entities": [{"text": "Cross-Level Semantic Similarity (CLSS) task", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.7988882064819336}]}, {"text": "Unlike previous SemEval and *SEM tasks that were interested in comparing texts with similar volume, this task consists of four subtasks (paragraph2sentence, sentence2phrase, phrase2word and word2sense) that investigate the performance of systems based on pairs of texts of different sizes.", "labels": [], "entities": []}, {"text": "A system should report the similarity score of a given pair, ranging from 4 (two items have very similar meanings and the most important ideas, concepts, or actions in the larger text are represented in the smaller text) to 0 (two items do not mean the same thing and are not on the same topic).", "labels": [], "entities": [{"text": "similarity score", "start_pos": 27, "end_pos": 43, "type": "METRIC", "confidence": 0.9405940771102905}]}, {"text": "In this paper, we describe our two unsupervised systems that are based on co-occurrence statistics of words.", "labels": [], "entities": []}, {"text": "The only difference between the systems is the input they use.", "labels": [], "entities": []}, {"text": "The first system uses the words directly (after lemmatization, stop-word removal and excluding the non-alphanumeric characters) in text while the second system utilizes the most likely substitutes consulted by a 4-gram language model for each observed word position (i.e., context).", "labels": [], "entities": []}, {"text": "Note that we participated two subtasks which are paragraph2sentence and sentence2phrase.", "labels": [], "entities": []}, {"text": "The remainder of the paper proceeds as follows.", "labels": [], "entities": []}, {"text": "Section 2 explains the preprocessing part, the difference between the systems, co-occurrence modeling, and how we calculate the similarity between: Instance id-word pairs fora given sentence.", "labels": [], "entities": []}, {"text": "two texts after co-occurrence modeling has been done.", "labels": [], "entities": []}, {"text": "Section 3 discusses the results of our systems and compares them to other participants'.", "labels": [], "entities": []}, {"text": "Section 4 discusses the findings and concludes with plans for future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Paragraph-2-Sentence subtask scores for  the training data. Subscripts in AI-KU systems  specify the run number.", "labels": [], "entities": []}, {"text": " Table 4: Sentence2phrase subtask scores for the  training data.", "labels": [], "entities": []}, {"text": " Table 5: Paragraph-2-Sentence subtask scores for  the test data. Best indicates the best correlation  score for the subtask. LCS stands for Normalized  Longest Common Substring. Subscripts in AI-KU  systems specify the run number.", "labels": [], "entities": []}, {"text": " Table 6: Sentence2phrase subtask scores for the  test data.", "labels": [], "entities": []}]}