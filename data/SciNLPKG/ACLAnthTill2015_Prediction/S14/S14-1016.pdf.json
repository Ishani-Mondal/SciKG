{"title": [{"text": "Extracting Latent Attributes from Video Scenes Using Text as Background Knowledge", "labels": [], "entities": [{"text": "Extracting Latent Attributes from Video Scenes", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8942640721797943}]}], "abstractContent": [{"text": "We explore the novel task of identifying latent attributes in video scenes, such as the mental states of actors, using only large text collections as background knowledge and minimal information about the videos, such as activity and actor types.", "labels": [], "entities": []}, {"text": "We formalize the task and a measure of merit that accounts for the semantic re-latedness of mental state terms.", "labels": [], "entities": []}, {"text": "We develop and test several largely unsupervised information extraction models that identify the mental states of human participants in video scenes.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.7572208344936371}]}, {"text": "We show that these models produce complementary information and their combination significantly outperforms the individual models as well as other baseline methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "\"Labeling a narrowly avoided vehicular manslaughter as approach(car, person) is missing something.\"", "labels": [], "entities": []}, {"text": "The recognition of activities, participants, and objects in videos has advanced considerably in recent years ().", "labels": [], "entities": []}, {"text": "However, identifying latent attributes of scenes, such as the mental states of human participants, has not been addressed.", "labels": [], "entities": []}, {"text": "Latent attributes matter: If a video surveillance system detects one person chasing another, the response from law enforcement should be radically different if the people are happy (e.g., children playing) or afraid and angry (e.g., a person running from an assailant).", "labels": [], "entities": []}, {"text": "Attributes that are latent in visual representations are often explicit in textual representations.", "labels": [], "entities": []}, {"text": "This suggests a novel method for inferring latent attributes: Use explicit features of videos to query text corpora, and from the resulting texts extract attributes that are latent in the videos, such as mental states.", "labels": [], "entities": []}, {"text": "The contributions of this work are: 1: We formalize the novel task of latent attribute identification from video scenes, focusing on the identification of actors' mental states.", "labels": [], "entities": [{"text": "latent attribute identification from video scenes", "start_pos": 70, "end_pos": 119, "type": "TASK", "confidence": 0.7885645627975464}, {"text": "identification of actors' mental states", "start_pos": 137, "end_pos": 176, "type": "TASK", "confidence": 0.8383292436599732}]}, {"text": "The input for the task is contextual information about the scene, such as detections about the activity (e.g., chase) and actor types (e.g., policeman or child), and the output is a distribution over mental state labels.", "labels": [], "entities": []}, {"text": "We show that gold standard annotations for this task can be reliably generated using crowd sourcing.", "labels": [], "entities": []}, {"text": "We define a novel evaluation measure, called constrained weighted similarity-aligned F 1 score, that accounts for both the differences between mental state distributions and the semantic relatedness of mental state terms (e.g., partial credit is given for irate when the target is angry).", "labels": [], "entities": [{"text": "constrained weighted similarity-aligned F 1 score", "start_pos": 45, "end_pos": 94, "type": "METRIC", "confidence": 0.6780854910612106}]}], "datasetContent": [{"text": "Let R denote the response distribution over mental state labels produced fora single video by one of the models described in the previous section, and let G denote the gold standard distribution produced for the same video by MTurk workers.", "labels": [], "entities": []}, {"text": "If R is similar to G then our models produce similar mental state terms as the workers.", "labels": [], "entities": []}, {"text": "There are many ways to compare distributions (e.g., KL distance, chi-square statistics) but these give bad results when distributions are sparse.", "labels": [], "entities": []}, {"text": "More importantly, for our purposes, the measures that compare the shapes of distributions do not allow semantic comparisons at the level of distribution elements.", "labels": [], "entities": []}, {"text": "Suppose R assigns high scores to angry and mad, only, while G assigns a high score to happy, only.", "labels": [], "entities": []}, {"text": "Clearly, R is wrong.", "labels": [], "entities": [{"text": "R", "start_pos": 9, "end_pos": 10, "type": "METRIC", "confidence": 0.9277216792106628}]}, {"text": "But if instead G had assigned a high score to irate, only, then R would be more right than wrong because, at the level of the individual elements, angry and mad are similar to irate but not similar to happy.", "labels": [], "entities": [{"text": "R", "start_pos": 64, "end_pos": 65, "type": "METRIC", "confidence": 0.9422261118888855}]}, {"text": "We describe a series of measures, starting with the familiar F 1 score, and discuss their applicability.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9625997940699259}]}, {"text": "To illustrate the effectiveness of each measure, we will use the examples shown in.", "labels": [], "entities": []}, {"text": "As described in Section 3, MTurk workers annotated 26 videos by identifying the actor types and mental state labels for each video.", "labels": [], "entities": []}, {"text": "The actor types become query tuples of the form (activity, actor) and the mental state labels are compiled into one probability distribution over labels for each video, designated G.", "labels": [], "entities": []}, {"text": "The query tuples were provided to our neighborhood models (Sec.", "labels": [], "entities": []}, {"text": "4), which returned a response distribution over mental state labels for each video, designated R.", "labels": [], "entities": []}, {"text": "We selected four videos of the 26 to calibrate the prune parameters \u03b3 and the interpolation parameters \u03bb (Sec. 4).", "labels": [], "entities": []}, {"text": "One of these videos contains children, one has police involvement, and two contain adults.", "labels": [], "entities": []}, {"text": "We asked additional MTurk workers to annotate these videos, yielding an independent set of annotations to be used solely for calibration.", "labels": [], "entities": []}, {"text": "The experimental question is, how well does G match R for each video?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: The average evaluation performance  across 26 different chase videos are shown against  2 different baselines for all proposed models. Bold  font indicates the best score in a given column.", "labels": [], "entities": []}, {"text": " Table 5: The average CWSA-F 1 scores for the  win-n model with different window parameters are  shown in comparison to the coref model. The  coref model outperformed all tested configura- tions, though the difference is not significant for  n = 1. The p-value based on the average differ- ences were obtained using one-tailed nonparamet- ric bootstrap resampling with 10, 000 iterations.", "labels": [], "entities": []}, {"text": " Table 6: The average CWSA-F 1 scores for the en- semble model are shown in comparison to the uni- form baseline method, categorize by video types.", "labels": [], "entities": []}]}