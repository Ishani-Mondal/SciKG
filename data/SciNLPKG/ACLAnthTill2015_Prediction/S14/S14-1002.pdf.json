{"title": [{"text": "Generating a Word-Emotion Lexicon from #Emotional Tweets", "labels": [], "entities": []}], "abstractContent": [{"text": "Research in emotion analysis of text suggest that emotion lexicon based features are superior to corpus based n-gram features.", "labels": [], "entities": []}, {"text": "However the static nature of the general purpose emotion lexicons make them less suited to social media analysis, where the need to adopt to changes in vocabulary usage and context is crucial.", "labels": [], "entities": [{"text": "social media analysis", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.7426733175913492}]}, {"text": "In this paper we propose a set of methods to extract a word-emotion lexicon automatically from an emotion labelled corpus of tweets.", "labels": [], "entities": []}, {"text": "Our results confirm that the features derived from these lexicons outper-form the standard Bag-of-words features when applied to an emotion classification task.", "labels": [], "entities": [{"text": "emotion classification task", "start_pos": 132, "end_pos": 159, "type": "TASK", "confidence": 0.7913384536902109}]}, {"text": "Furthermore, a comparative analysis with both manually crafted lexicons and a state-of-the-art lexicon generated using Point-Wise Mutual Information, show that the lexicons generated from the proposed methods lead to significantly better classification performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Emotion mining or affect sensing is the computational study of natural language expressions in order to quantify their associations with different emotions (e.g. anger, fear, joy, sadness and surprise).", "labels": [], "entities": [{"text": "Emotion mining or affect sensing", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7933059692382812}]}, {"text": "It has a number of applications for the industry, commerce and government organisations, but uptake has arguably been slow.", "labels": [], "entities": []}, {"text": "This in part is due to the challenges involved with modelling subjectivity and complexity of the emotive content.", "labels": [], "entities": []}, {"text": "However, use of qualitative metrics to capture emotive strength and extraction of features from these metrics has in recent years shown promise.", "labels": [], "entities": []}, {"text": "A general-purpose emotion lexicon (GPEL) is a commonly used resource that allows qualitative assessment of apiece of emotive text.", "labels": [], "entities": [{"text": "general-purpose emotion lexicon (GPEL)", "start_pos": 2, "end_pos": 40, "type": "TASK", "confidence": 0.7129567563533783}]}, {"text": "Given a word and an emotion, the lexicon provides a score to quantify the strength of emotion expressed by that word.", "labels": [], "entities": []}, {"text": "Such lexicons are carefully crafted and are utilised by both supervised and unsupervised algorithms to directly aggregate an overall emotion score or indirectly derive features for emotion classification tasks),.", "labels": [], "entities": [{"text": "emotion classification tasks", "start_pos": 181, "end_pos": 209, "type": "TASK", "confidence": 0.7788429856300354}]}, {"text": "Socio-linguistics suggest that social media is a popular means for people to converse with individuals, groups and the world in general (.", "labels": [], "entities": []}, {"text": "These conversations often involve usage of non-standard natural language expressions which consistently evolve.", "labels": [], "entities": []}, {"text": "Twitter and Facebook were credited for providing momentum for the 2011 Arab Spring and Occupy Wall street movements,.", "labels": [], "entities": [{"text": "Occupy Wall street", "start_pos": 87, "end_pos": 105, "type": "DATASET", "confidence": 0.8824330766995748}]}, {"text": "Therefore efforts to model social conversations would provide valuable insights into how people influence each other through emotional expressions.", "labels": [], "entities": []}, {"text": "Emotion analysis in such domains calls for automated discovery of lexicons.", "labels": [], "entities": [{"text": "Emotion analysis", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9100971221923828}]}, {"text": "This is so since learnt lexicons can intuitively capture the evolving nature of vocabulary in such domains better than GPELs.", "labels": [], "entities": [{"text": "GPELs", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.8725671768188477}]}, {"text": "In this work we show how an emotion labelled corpus can be leveraged to generate a wordemotion lexicon automatically.", "labels": [], "entities": []}, {"text": "Key to this is the availability of a labelled corpus which maybe obtained using a distance-supervised approach to labelling (.", "labels": [], "entities": []}, {"text": "In this paper we propose three lexicon generation methods and evaluate the quality of these by deploying them in an emotion classification task.", "labels": [], "entities": [{"text": "emotion classification task", "start_pos": 116, "end_pos": 143, "type": "TASK", "confidence": 0.7807262341181437}]}, {"text": "We show through our experiments that the word-emotion lexicon generated using the proposed methods in this paper significantly outperforms GPELs such as WordnetAffect, NRC word-emotion association lexicon and a leaxicon learnt using Point-wise Mutual Information (PMI).", "labels": [], "entities": [{"text": "WordnetAffect", "start_pos": 153, "end_pos": 166, "type": "DATASET", "confidence": 0.9696855545043945}, {"text": "NRC word-emotion association lexicon", "start_pos": 168, "end_pos": 204, "type": "DATASET", "confidence": 0.744649201631546}]}, {"text": "Additionally, our lexicons also outperform the traditional Bag-of-Words representation.", "labels": [], "entities": []}, {"text": "The rest of the paper is organised as follows: In Section 2 we present the related work.", "labels": [], "entities": []}, {"text": "In Section 3 we outline the problem.", "labels": [], "entities": []}, {"text": "In Section 4 we formulate the different methods proposed to generate the word-emotion lexicons.", "labels": [], "entities": []}, {"text": "In Section 5 we discuss experimental results followed by conclusions and future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we detail our experimental evaluation.", "labels": [], "entities": []}, {"text": "We begin with the details about the Twitter data used in our experiments.", "labels": [], "entities": []}, {"text": "We then discuss how we created the folds fora cross validation experiment.", "labels": [], "entities": [{"text": "cross validation", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.6848324835300446}]}, {"text": "Thereafter we detail the classifi-cation task used to evaluate the word-emotion lexicon.", "labels": [], "entities": []}, {"text": "Finally we discuss the performance of our proposed methods for lexicon generation in comparison with other manually crafted lexicons, PMI based method for lexicon generation and the standard BoW in an emotion classification task.", "labels": [], "entities": [{"text": "lexicon generation", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7249645888805389}, {"text": "lexicon generation", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.7611054182052612}, {"text": "BoW", "start_pos": 191, "end_pos": 194, "type": "DATASET", "confidence": 0.8907033801078796}, {"text": "emotion classification task", "start_pos": 201, "end_pos": 228, "type": "TASK", "confidence": 0.7680761913458506}]}, {"text": "The data set used in our experiments was a corpus of emotion labelled tweets harnessed by ().", "labels": [], "entities": []}, {"text": "The data set was available in the form of tweet ID's and the corresponding emotion label.", "labels": [], "entities": []}, {"text": "The emotion labels comprised namely : anger, fear, joy, sadness, surprise, love and thankfulness.", "labels": [], "entities": []}, {"text": "We used the Twitter search API 1 to obtain the tweets by searching with the corresponding tweet ID.", "labels": [], "entities": []}, {"text": "After that we decided to consider only tweets that belong to the primary set of emotions defined by Parrott).", "labels": [], "entities": []}, {"text": "The emotion classes in our case included anger, fear, joy, sadness, surprise and love.", "labels": [], "entities": []}, {"text": "We had a collection of 0.28 million tweets which we used to carryout a 10 fold cross-validation experiment.", "labels": [], "entities": []}, {"text": "We decided to generate the folds manually,in order to compare the performance of the different algorithms used in our experiments.", "labels": [], "entities": []}, {"text": "We split the collection of 0.28 million tweets into 10 equal size sets to generate 10 folds with different training and test sets in each fold.", "labels": [], "entities": []}, {"text": "Also all the folds in our experiments were obtained by stratified sampling, ensuring that we had documents representing all the classes in both the training and test sets.", "labels": [], "entities": []}, {"text": "We used the training data in each fold to generate the word-emotion lexicon and measured the performance of it on the test data in an emotion classification task.", "labels": [], "entities": [{"text": "emotion classification task", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.7942505478858948}]}, {"text": "shows the average distribution of the different classes namely: anger, fear, joy, sadness, surprise and love over the 10 folds.", "labels": [], "entities": [{"text": "surprise", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9649415016174316}]}, {"text": "Observe that emotions such as joy and sadness had a very high number of representative documents . Emotions such as anger,love and fear were the next most represented emotions.", "labels": [], "entities": []}, {"text": "The emotion surprise had very few representative documents compared to that of the other emotions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average distribution of emotions across  the folds  Emotion  Training  Test  Anger  58410  6496  Fear  13692  1548  Joy  74108  8235  Sadness  63711  7069  Surprise  2533  282  Love  31127  3464  Total  243855  27095", "labels": [], "entities": [{"text": "folds  Emotion  Training  Test  Anger  58410  6496  Fear  13692  1548  Joy  74108  8235  Sadness  63711  7069  Surprise  2533  282  Love  31127  3464  Total  243855  27095", "start_pos": 55, "end_pos": 226, "type": "DATASET", "confidence": 0.8067365872859955}]}, {"text": " Table 3: Emotion classification results  Method  Average F-Score  Anger  Fear  Joy  Sadness  Surprise Love  Baselines  WNA-lex  25.82%  6.61%  12.94%  8.76%  0.76%  2.67%  NRC-lex  21.37%  3.97%  16.04%  8.87%  1.54%  7.22%  Bow  56.5%  13.56%  63.34%  50.57%  21.65%  20.52%  PMI-lex  56.42%  2.39%  63.4%  50.57%  0.69%  22.31%  Our Learnt Lexicons  TF-lex  55.85%  19.03%  62.01%  50.54%  11.29%  37.69%  EMallclass-lex  56.64%  14.53%  61.89%  50.48%  12.33%  38.13%  EMclass-corpus-lex  57.35%  16.1%  62.74%  51.14%  12.05%  39.19%", "labels": [], "entities": [{"text": "Emotion classification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.882860541343689}, {"text": "F-Score  Anger  Fear  Joy  Sadness  Surprise Love  Baselines  WNA-lex", "start_pos": 58, "end_pos": 127, "type": "TASK", "confidence": 0.472257802883784}, {"text": "NRC-lex", "start_pos": 173, "end_pos": 180, "type": "DATASET", "confidence": 0.9146496653556824}]}, {"text": " Table 4: Overall F-scores  Method  Avg Overall F- score  Baselines  WNA-lex  13.17%  NRC-lex  13.50%  Bow  49.30%  PMI-lex  48.99%  Our automatic lexicons  TF-lex  51.45%  EMallclass-lex  51.38%  EMclass-corpus-lex  52.20%", "labels": [], "entities": [{"text": "F-scores", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9682011604309082}, {"text": "Avg", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.8180317282676697}, {"text": "F- score  Baselines  WNA-lex", "start_pos": 48, "end_pos": 76, "type": "METRIC", "confidence": 0.8367445707321167}]}]}