{"title": [{"text": "IIT Patna: Supervised Approach for Sentiment Analysis in Twitter", "labels": [], "entities": [{"text": "IIT Patna", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.46864959597587585}, {"text": "Sentiment Analysis", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.9690340161323547}]}], "abstractContent": [{"text": "In this paper we report our works for SemEval-2014 Sentiment Analysis in Twitter evaluation challenge.", "labels": [], "entities": [{"text": "SemEval-2014 Sentiment Analysis", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.862354596455892}]}, {"text": "This is the first time we attempt for this task, and our submissions are based on supervised machine learning algorithm.", "labels": [], "entities": []}, {"text": "We use Support Vector Machine for both the tasks, viz.", "labels": [], "entities": []}, {"text": "contextual polarity disambiguation and message polarity classification.", "labels": [], "entities": [{"text": "contextual polarity disambiguation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7570637067159017}, {"text": "message polarity classification", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.7983243862787882}]}, {"text": "We identify and implement a small set of features for each the tasks, and did not make use of any external resources and/or tools.", "labels": [], "entities": []}, {"text": "The systems are tuned on the development sets and finally blind evaluation is performed on the respective test set, which consists of the datasets of five different domains.", "labels": [], "entities": []}, {"text": "Our submission for the first task shows the F-score values of 76.3%, 77.04%, 70.91%, 72.25% and 66.32% for LiveJournal2014, SMS2013, Twitter2013, Twitter2014 and Twitter2014Sarcasm datasets, respectively.", "labels": [], "entities": [{"text": "F-score", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9991914629936218}, {"text": "Twitter2013", "start_pos": 133, "end_pos": 144, "type": "DATASET", "confidence": 0.876483678817749}, {"text": "Twitter2014", "start_pos": 146, "end_pos": 157, "type": "DATASET", "confidence": 0.911849319934845}, {"text": "Twitter2014Sarcasm datasets", "start_pos": 162, "end_pos": 189, "type": "DATASET", "confidence": 0.939588189125061}]}, {"text": "The system developed for the second task yields the F-score values of 54.68%, 40.56%, 50.32%, 48.22% and 36.73%, respectively for the five different test datasets.", "labels": [], "entities": [{"text": "F-score", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.9992945194244385}]}], "introductionContent": [{"text": "During the past few years, the communications in the forms of microblogging and text messaging have emerged and become ubiquitous.", "labels": [], "entities": []}, {"text": "Opinions and sentiments about the surrounding worlds are widely expressed through the mediums of Twitter messages (Tweets) and Cell phone messages (SMS).", "labels": [], "entities": []}, {"text": "The availability of social content generated on sites such as Twitter creates new opportuThis work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": []}, {"text": "Page numbers and proceedings footer are added by the organisers.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by/4.0/ nities to automatically study public opinion.", "labels": [], "entities": []}, {"text": "Dealing with these informal text genres presents new challenges for data mining and language processing techniques beyond those encountered when working with more traditional text genres such as newswire.", "labels": [], "entities": [{"text": "data mining", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.8180963099002838}]}, {"text": "Tweets and SMS messages are short in length, usually a sentence or a headline rather than a document.", "labels": [], "entities": []}, {"text": "These texts are very informal in nature and contains creative spellings and punctuation symbols.", "labels": [], "entities": []}, {"text": "Text also contains lots of misspellings, slang, out-of-vocabulary words, URLs, and genre-specific terminology and abbreviations, e.g., RT for reTweet and #hash-tags.", "labels": [], "entities": []}, {"text": "The kind of these specific features pose great challenges for building various lexical and syntactic resources and/or tools, which are required for efficient processing of texts.", "labels": [], "entities": []}, {"text": "These aspects also introduce complexities to build the state-of-theart data mining systems.", "labels": [], "entities": [{"text": "data mining", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.7401141822338104}]}, {"text": "In recent times, there has been a huge interest to mine and understand the opinions and sentiments that people are communicating in social media ().", "labels": [], "entities": []}, {"text": "Recent studies show the interests in sentiment analysis of Tweets across a variety of domains such as commerce), health) and disaster management (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.9559392333030701}, {"text": "disaster management", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.7351641207933426}]}, {"text": "Another aspect of social media data, such as twitter messages, is that they include rich information about the individuals involved in the communication.", "labels": [], "entities": []}, {"text": "For e.g., twitter maintains information about who follows whom.", "labels": [], "entities": []}, {"text": "ReTweets (reshares of a Tweet) and tags inside of Tweets provide discourse information (.", "labels": [], "entities": []}, {"text": "Efficient modelling of such information is crucial in the sense that it provides a mean to empirically study the social interactions where opinion is conveyed.", "labels": [], "entities": []}, {"text": "Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA corpus ( of newswire text; i-sieve ( and TASS corpus2 for Twitter sentiment.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 106, "end_pos": 117, "type": "DATASET", "confidence": 0.9708208441734314}]}, {"text": "These resources were either in non-social media or they were small and proprietary.", "labels": [], "entities": []}, {"text": "They further focused on message-level sentiment.", "labels": [], "entities": [{"text": "message-level sentiment", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.7710679173469543}]}, {"text": "The SemEval-2013 shared task () on sentiment analysis in Twitter releases SemEval Tweet corpus, which contains Tweets and SMS messages with sentiment expressions annotated with contextual phrase-level polarity as well as an overall message-level polarity.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.8880159258842468}]}, {"text": "Among the 44 submissions, the highest-performing system () made use of Support Vector Machine (SVM) classifier.", "labels": [], "entities": []}, {"text": "It obtained the F-scores of 69.02% in the message-level task and 88.93% in the term-level task.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9994781613349915}]}, {"text": "Variety of features were implemented based on surface-forms, semantics, and sentiment features.", "labels": [], "entities": []}, {"text": "They generated two large wordsentiment association lexicons, one from Tweets with sentiment-word hashtags, and one from Tweets with emoticons.", "labels": [], "entities": []}, {"text": "They showed that in message-level task, the lexicon-based features gained 5 F-score points overall the others.", "labels": [], "entities": [{"text": "F-score", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.998950183391571}]}, {"text": "SemEval-14 shared task 1 on sentiment analysis in Twitter is a continuing effort to promote the research in this direction.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.9046533107757568}]}, {"text": "Similar to the previous year's evaluation campaigns two primary tasks were addressed in this year challenge.", "labels": [], "entities": []}, {"text": "The first task (i.e. Subtask A) deals with contextual polarity disambiguation and the second task (i.e. Subtask B) was about message polarity classification.", "labels": [], "entities": [{"text": "contextual polarity disambiguation", "start_pos": 43, "end_pos": 77, "type": "TASK", "confidence": 0.6419473985830942}, {"text": "message polarity classification", "start_pos": 125, "end_pos": 156, "type": "TASK", "confidence": 0.7929316560427347}]}, {"text": "For Subtask A, fora given message containing a marked instance of a word or phrase, the goal is to determine whether that instance is positive, negative or neutral in that context.", "labels": [], "entities": []}, {"text": "In Subtask B, fora given message, the task is to classify whether the message is of positive, negative, or neutral sentiment.", "labels": [], "entities": []}, {"text": "For messages that convey both positive and negative sentiments, the stronger one should be chosen.", "labels": [], "entities": []}, {"text": "In this paper we report on our submissions as part of our first-time participation in this kind of task (i.e. sentiment classification).", "labels": [], "entities": [{"text": "sentiment classification)", "start_pos": 110, "end_pos": 135, "type": "TASK", "confidence": 0.941821833451589}]}, {"text": "We develop the systems based on supervised machine learning algorithm, namely Support Vector Machine (SVM).", "labels": [], "entities": []}, {"text": "We identify and implement a very small set of features that do not make use of any external resources and/or tools.", "labels": [], "entities": []}, {"text": "For each task the system is tuned on the devel-", "labels": [], "entities": []}], "datasetContent": [{"text": "The: Results on development set for Task-B (in %).", "labels": [], "entities": []}, {"text": "carried out experiments with the different classifiers.", "labels": [], "entities": []}, {"text": "However we report the results of SVM as it produced the highest accuracy with respect to this particular feature set.", "labels": [], "entities": [{"text": "SVM", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.7907024621963501}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.998954176902771}]}, {"text": "We use the default parameters of SVM as implemented in this toolkit.", "labels": [], "entities": []}, {"text": "We submitted two runs, one for each task.", "labels": [], "entities": []}, {"text": "Both of our submissions were constrained in nature, i.e. we did not make use of any additional resources and/or tools to build our systems.", "labels": [], "entities": []}, {"text": "We perform several experiments using the development set.", "labels": [], "entities": []}, {"text": "Best results are reported in and for Task-A and Task-B, respectively.", "labels": [], "entities": []}, {"text": "Evaluation shows that for message polarity disambiguation we obtain the average precision, recall and F-score values of 52.19%, 55.46% and 53.77%, respectively.", "labels": [], "entities": [{"text": "message polarity disambiguation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.8652024666468302}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9996294975280762}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9956458210945129}, {"text": "F-score", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9978821873664856}]}, {"text": "For message polarity classification we obtain the precision, recall and FScore values of 50.68%, 49.73% and 66.39%, respectively.", "labels": [], "entities": [{"text": "message polarity classification", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.902669628461202}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9997817873954773}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9973148703575134}, {"text": "FScore", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9991267323493958}]}, {"text": "It is evident from the evaluation that the first task suffers most due to the problems in classifying the tweets having neutral sentiments, whereas the second task faces difficulties in classifying the negative sentiments.", "labels": [], "entities": []}, {"text": "We report the confusion matrices in    and second development sets, respectively.", "labels": [], "entities": []}, {"text": "Error analysis suggests that most miss-classifications are because of the less number of neutral instances compared to the positive and negative instances in Task-A. For the Task-B training set, the number of instances of positive and neutral sentiments are very low compared to the negative sentiment.", "labels": [], "entities": [{"text": "Task-B training set", "start_pos": 174, "end_pos": 193, "type": "DATASET", "confidence": 0.6508722404638926}]}, {"text": "After tuning the systems on the development sets, we perform blind evaluation on the test datasets.", "labels": [], "entities": []}, {"text": "Evaluation results on the test sets are reported in for both the tasks.", "labels": [], "entities": []}, {"text": "The evaluation is carried out based on the evaluation scripts as provided by the organizers.", "labels": [], "entities": []}, {"text": "For message polarity disambiguation we obtain the highest F-score of 77.04% for the SMS datatype in Task-A. The system shows the F-scores of 76.03%, 70.91%, 72.25% and 66.35% for LiveJournal2014, Twitter2013, Twitter2014 and Twitter2014sarcasm, respectively.", "labels": [], "entities": [{"text": "message polarity disambiguation", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8262240886688232}, {"text": "F-score", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9987437129020691}, {"text": "SMS datatype", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.705068051815033}, {"text": "F-scores", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9976274371147156}, {"text": "Twitter2013", "start_pos": 196, "end_pos": 207, "type": "DATASET", "confidence": 0.9153117537498474}, {"text": "Twitter2014", "start_pos": 209, "end_pos": 220, "type": "DATASET", "confidence": 0.9335908889770508}]}, {"text": "For the second task the system attains the highest F-score value of 54.68% for the LiveJournal2014 dataset.", "labels": [], "entities": [{"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9991670846939087}, {"text": "LiveJournal2014 dataset", "start_pos": 83, "end_pos": 106, "type": "DATASET", "confidence": 0.988350123167038}]}, {"text": "For the other datasets, the system shows the F-scores of 40.56%, 50.32%, 48.22% and 36.73% for the SMS2013, Twitter2013 and Twitter2014Sarcasm, respectively.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9987227320671082}, {"text": "SMS2013", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.942611813545227}, {"text": "Twitter2013", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.8791472911834717}, {"text": "Twitter2014Sarcasm", "start_pos": 124, "end_pos": 142, "type": "DATASET", "confidence": 0.8861028552055359}]}, {"text": "We followed a simple approach that needs fine-tuning in many places.", "labels": [], "entities": []}, {"text": "Currently our systems lack behind the best reported systems by margins of approximately 11-18% F-scores for Task-A, and 19-30% F-scores for Task-B.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9980719089508057}, {"text": "F-scores", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9945452213287354}]}], "tableCaptions": [{"text": " Table 1: Results on development set for Task-A  (%).", "labels": [], "entities": []}, {"text": " Table 2: Results on development set for Task-B (in  %).", "labels": [], "entities": []}, {"text": " Table 3: Confusion matrix for A. Here, gs: Gold  standard; pred: Predicted class.", "labels": [], "entities": [{"text": "Gold  standard", "start_pos": 44, "end_pos": 58, "type": "METRIC", "confidence": 0.8497433066368103}, {"text": "Predicted", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9249639511108398}]}, {"text": " Table 4: Confusion matrix for B. Here, gs: Gold  standard; pred: Predicted class.", "labels": [], "entities": [{"text": "Gold  standard", "start_pos": 44, "end_pos": 58, "type": "METRIC", "confidence": 0.8644410967826843}, {"text": "Predicted", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9177804589271545}]}, {"text": " Table 5: Results on the test set.", "labels": [], "entities": []}]}