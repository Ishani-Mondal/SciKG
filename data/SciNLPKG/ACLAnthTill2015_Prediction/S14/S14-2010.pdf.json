{"title": [{"text": "SemEval-2014 Task 10: Multilingual Semantic Textual Similarity", "labels": [], "entities": [{"text": "SemEval-2014 Task 10", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8807954986890157}, {"text": "Multilingual Semantic Textual Similarity", "start_pos": 22, "end_pos": 62, "type": "TASK", "confidence": 0.5584009066224098}]}], "abstractContent": [{"text": "In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets.", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.7452762126922607}]}, {"text": "This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as anew language in which to assess semantic similarity.", "labels": [], "entities": []}, {"text": "For the English subtask, we exposed the systems to a diversity of testing scenarios , by preparing additional OntoNotes-WordNet sense mappings and news headlines , as well as introducing new gen-res, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline mappings.", "labels": [], "entities": [{"text": "DEFT newswire", "start_pos": 254, "end_pos": 267, "type": "DATASET", "confidence": 0.8734417855739594}]}, {"text": "For Spanish, since, to our knowledge, this is the first time that official evaluations are conducted, we used well-formed text, by featuring sentences extracted from ency-clopedic content and newswire.", "labels": [], "entities": []}, {"text": "The annotations for both tasks leveraged crowd-sourcing.", "labels": [], "entities": []}, {"text": "The Spanish subtask engaged 9 teams participating with 22 system runs, and the English subtask attracted 15 teams with 38 system runs.", "labels": [], "entities": [{"text": "English subtask", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.9752275943756104}]}], "introductionContent": [], "datasetContent": [{"text": "Evaluation of STS is still an open issue.", "labels": [], "entities": [{"text": "STS", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9191918969154358}]}, {"text": "STS experiments have traditionally used Pearson product-moment correlation between the system scores and the GS scores, or, alternatively, Spearman rank order correlation.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9226949214935303}, {"text": "Pearson product-moment correlation", "start_pos": 40, "end_pos": 74, "type": "METRIC", "confidence": 0.8972267111142477}, {"text": "Spearman rank order correlation", "start_pos": 139, "end_pos": 170, "type": "METRIC", "confidence": 0.5846347063779831}]}, {"text": "In addition, we also need a method to aggregate the results from each dataset into an overall score.", "labels": [], "entities": []}, {"text": "The analysis performed in shows that Pearson and averaging across datasets are the best suited combination in general.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9932286143302917}]}, {"text": "In particular, Pearson is more informative than Spearman, in that Spearman only takes the rank differences into account, while Pearson does account for value differences as well.", "labels": [], "entities": []}, {"text": "The study also showed that other lynx.browser.org alternatives need to be considered, depending on the requirements of the target application.", "labels": [], "entities": []}, {"text": "We leave application-dependent evaluations for future work, and focus on average Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 81, "end_pos": 100, "type": "METRIC", "confidence": 0.9105135500431061}]}, {"text": "When averaging, we weight each individual correlation by the size of the dataset.", "labels": [], "entities": []}, {"text": "In order to compute statistical significance among system results, we use a one-tailed parametric test based on Fisher's z-transformation (, equation 14.5.10).", "labels": [], "entities": []}, {"text": "In addition, English subtask participants could provide an optional confidence measure between 0 and 100 for each of their predictions.", "labels": [], "entities": []}, {"text": "Team RTM-DCU is the only one who has provided these, and the evaluation of their runs using weighted Pearson () is listed at the end of.", "labels": [], "entities": [{"text": "RTM-DCU", "start_pos": 5, "end_pos": 12, "type": "DATASET", "confidence": 0.9194952845573425}, {"text": "Pearson", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.8586071133613586}]}, {"text": "Participants could take part in the shared task with a maximum of 3 system runs per subtask.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: English evaluation results. Results at the top correspond to out-of-the-box systems. Results at  the bottom correspond to results using the confidence score.  Notes: \"-\" for not submitted, \" \u2020\" for post-deadline submission.", "labels": [], "entities": []}, {"text": " Table 4: Spanish evaluation results in terms of Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 49, "end_pos": 68, "type": "METRIC", "confidence": 0.8123229146003723}]}]}