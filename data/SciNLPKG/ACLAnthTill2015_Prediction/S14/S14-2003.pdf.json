{"title": [{"text": "SemEval-2014 Task 3: Cross-Level Semantic Similarity", "labels": [], "entities": [{"text": "SemEval-2014 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8785456120967865}, {"text": "Similarity", "start_pos": 42, "end_pos": 52, "type": "TASK", "confidence": 0.7649608850479126}]}], "abstractContent": [{"text": "This paper introduces anew SemEval task on Cross-Level Semantic Similarity (CLSS), which measures the degree to which the meaning of a larger linguistic item, such as a paragraph, is captured by a smaller item, such as a sentence.", "labels": [], "entities": [{"text": "SemEval", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9769689440727234}, {"text": "Cross-Level Semantic Similarity (CLSS)", "start_pos": 43, "end_pos": 81, "type": "TASK", "confidence": 0.6872249990701675}]}, {"text": "High-quality data sets were constructed for four comparison types using multi-stage annotation procedures with a graded scale of similarity.", "labels": [], "entities": []}, {"text": "Nineteen teams submitted 38 systems.", "labels": [], "entities": []}, {"text": "Most systems surpassed the baseline performance, with several attaining high performance for multiple comparison types.", "labels": [], "entities": []}, {"text": "Further, our results show that comparisons of semantic representation increase performance beyond what is possible with text alone.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given two linguistic items, semantic similarity measures the degree to which the two items have the same meaning.", "labels": [], "entities": []}, {"text": "Semantic similarity is an essential component of many applications in Natural Language Processing (NLP), and similarity measurements between all types of text as well as between word senses lend themselves to a variety of NLP tasks such as information retrieval) or paraphrasing.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 240, "end_pos": 261, "type": "TASK", "confidence": 0.7527838349342346}]}, {"text": "Semantic similarity evaluations have largely focused on comparing similar types of lexical items.", "labels": [], "entities": [{"text": "Semantic similarity evaluations", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8033281366030375}]}, {"text": "Most recently, tasks in SemEval () and *SEM () have introduced benchmarks for measuring Semantic Textual Similarity (STS) between similar-sized sentences and phrases.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) between similar-sized sentences and phrases", "start_pos": 88, "end_pos": 165, "type": "TASK", "confidence": 0.6364023414525118}]}, {"text": "Other data sets such as that of measure similarity between word pairs, while the data sets of and offer a binary similar-dissimilar distinction between senses.", "labels": [], "entities": []}, {"text": "Notably, all of these evaluations have focused on comparisons between a single type, in contrast to application-based evaluations such as summarization and compositionality which incorporate textual items of different sizes, e.g., measuring the quality of a paragraph's sentence summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 138, "end_pos": 151, "type": "TASK", "confidence": 0.988554060459137}]}, {"text": "Task 3 introduces anew evaluation where similarity is measured between items of different types: paragraphs, sentences, phrases, words and senses.", "labels": [], "entities": []}, {"text": "Given an item of the lexically-larger type, a system measures the degree to which the meaning of the larger item is captured in the smaller type, e.g., comparing a paragraph to a sentence.", "labels": [], "entities": []}, {"text": "We refer to this task as Cross-Level Semantic Similarity (CLSS).", "labels": [], "entities": [{"text": "Cross-Level Semantic Similarity (CLSS)", "start_pos": 25, "end_pos": 63, "type": "TASK", "confidence": 0.7140263020992279}]}, {"text": "A major motivation of this task is to produce semantic similarity systems that are able to compare all types of text, thereby freeing downstream NLP applications from needing to consider the type of text being compared.", "labels": [], "entities": []}, {"text": "Task 3 enables assessing the extent to which the meaning of the sentence \"do u know where i can watch free older movies online without download?\" is captured in the phrase \"streaming vintage movies for free\", or how similar is \"circumscribe\" to the phrase \"beating around the bush.\"", "labels": [], "entities": []}, {"text": "Furthermore, by incorporating comparisons of a variety of item sizes, Task 3 unifies in a single task multiple objectives from different areas of NLP such as paraphrasing, summarization, and compositionality.", "labels": [], "entities": [{"text": "summarization", "start_pos": 172, "end_pos": 185, "type": "TASK", "confidence": 0.9804782271385193}]}, {"text": "Because CLSS generalizes STS to items of different types, successful CLSS systems can directly be applied to all STS-based applications.", "labels": [], "entities": []}, {"text": "Furthermore, CLSS systems can be used in other similarity-based applications such as text simplification (, keyphrase identification (, lexical substitution, summariza-tion, gloss-to-sense mapping, and modeling the semantics of multi-word expressions) or polysemous words).", "labels": [], "entities": [{"text": "text simplification", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7609878182411194}, {"text": "keyphrase identification", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.7266485244035721}]}, {"text": "Task 3 was designed with three main objectives.", "labels": [], "entities": []}, {"text": "First, the task should include multiple types of comparison in order to assess each type's difficulty and whether specialized resources are needed for each.", "labels": [], "entities": []}, {"text": "Second, the task should incorporate text from multiple domains and writing styles to ensure that system performance is robust across text types.", "labels": [], "entities": []}, {"text": "Third, the similarity methods should be able to operate at the sense level, thereby potentially uniting text-and sense-based similarity methods within a single framework.", "labels": [], "entities": []}], "datasetContent": [{"text": "Participation The ultimate goal of Task 3 is to produce systems that can measure similarity for multiple types of items.", "labels": [], "entities": []}, {"text": "Therefore, we strongly encouraged participating teams to submit systems that were capable of generating similarity judgments for multiple comparison types.", "labels": [], "entities": []}, {"text": "However, to further the analysis, participants were also permitted to submit systems specialized to a single domain.", "labels": [], "entities": []}, {"text": "Teams were allowed at most three system submissions, regardless of the number of comparison types supported.", "labels": [], "entities": []}, {"text": "Scoring Systems were required to provide similarity values for all items within a comparison type.", "labels": [], "entities": []}, {"text": "Following prior STS evaluations, systems were scored for each comparison type using Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 84, "end_pos": 103, "type": "METRIC", "confidence": 0.8820606470108032}]}, {"text": "Additionally, we include a second score using Spearman's rank correlation, which is only affected by differences in the ranking of items by similarity, rather than differences in the similarity values.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 46, "end_pos": 73, "type": "METRIC", "confidence": 0.49189428985118866}]}, {"text": "Pearson correlation was chosen as the official evaluation metric since the goal of the task is to produce similar scores.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.7977815866470337}]}, {"text": "However, Spearman's rank correlation provides an important metric for assessing systems whose scores do not match human scores but whose rankings might, e.g., stringsimilarity measures.", "labels": [], "entities": []}, {"text": "Ultimately, a global ranking was produced by ordering systems by the sum of their Pearson correlation values for each of the four comparison levels.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 82, "end_pos": 101, "type": "METRIC", "confidence": 0.8527220785617828}]}, {"text": "Baselines The official baseline system was based on the Longest Common Substring (LCS), normalized by the length of items using the method of.", "labels": [], "entities": [{"text": "Longest Common Substring (LCS)", "start_pos": 56, "end_pos": 86, "type": "METRIC", "confidence": 0.688288614153862}]}, {"text": "Given a pair, the similarity is reported as the normalized length of the LCS.", "labels": [], "entities": [{"text": "similarity", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9892038702964783}]}, {"text": "In the case of word-to-sense, the LCS fora word-sense pair is measured between the sense's definition in WordNet and the definitions of each sense of the pair's word, reporting the maximal LCS.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.9600964784622192}]}, {"text": "Because OOV and slang words are not in WordNet, the baseline reports the average similarity value of non-OOV items.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9639396667480469}, {"text": "similarity", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9442265629768372}]}, {"text": "Baseline scores were made public after the evaluation period ended.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9489940404891968}]}, {"text": "Because LCS is a simple procedure, a second baseline based on Greedy String Tiling (GST) was added after the evaluation period concluded.", "labels": [], "entities": [{"text": "LCS", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9119009375572205}, {"text": "Greedy String Tiling (GST)", "start_pos": 62, "end_pos": 88, "type": "METRIC", "confidence": 0.654893308877945}]}, {"text": "Unlike LCS, GST better handles the transpositions of tokens across the two texts and can still report high similarity when encountering reordered text.", "labels": [], "entities": [{"text": "GST", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.5570801496505737}, {"text": "similarity", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.9807501435279846}]}, {"text": "The minimum match length for GST was set to 6.", "labels": [], "entities": [{"text": "GST", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8679836392402649}]}], "tableCaptions": [{"text": " Table 2: Example pairs and their ratings.", "labels": [], "entities": []}, {"text": " Table 3: Percentages of the training and test data per source corpus.", "labels": [], "entities": []}, {"text": " Table 4: IAA rates for the task data.", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9608582854270935}]}, {"text": " Table 5: Task results. Systems marked with a  \u2020 were submitted after the deadline but are positioned  where they would have ranked.", "labels": [], "entities": []}]}