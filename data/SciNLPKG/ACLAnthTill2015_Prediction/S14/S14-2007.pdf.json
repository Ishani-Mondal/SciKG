{"title": [{"text": "SemEval-2014 Task 7: Analysis of Clinical Text", "labels": [], "entities": [{"text": "SemEval-2014 Task 7", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8196922143300375}, {"text": "Analysis of Clinical Text", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.6532839089632034}]}], "abstractContent": [{"text": "This paper describes the SemEval-2014, Task 7 on the Analysis of Clinical Text and presents the evaluation results.", "labels": [], "entities": []}, {"text": "It fo-cused on two subtasks: (i) identification (Task A) and (ii) normalization (Task B", "labels": [], "entities": [{"text": "normalization", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.9682780504226685}]}], "introductionContent": [{"text": "A large amount of very useful information-both for medical researchers and patients-is present in the form of unstructured text within the clinical notes and discharge summaries that form a patient's medical history.", "labels": [], "entities": []}, {"text": "Adapting and extending natural language processing (NLP) techniques to mine this information can open doors to better, novel, clinical studies on one hand, and help patients understand the contents of their clinical records on the other.", "labels": [], "entities": []}, {"text": "Organization of this shared task helps establish state-of-the-art benchmarks and paves the way for further explorations.", "labels": [], "entities": []}, {"text": "It tackles two important sub-problems in NLPnamed entity recognition and word sense disambiguation.", "labels": [], "entities": [{"text": "NLPnamed entity recognition", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.8772374391555786}, {"text": "word sense disambiguation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.7653570572535197}]}, {"text": "Neither of these problems are new to NLP.", "labels": [], "entities": []}, {"text": "Research in general-domain NLP goes back to about two decades.", "labels": [], "entities": [{"text": "general-domain NLP", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.5239624679088593}]}, {"text": "For an overview of the development in the field through roughly 2009, we refer the refer to.", "labels": [], "entities": []}, {"text": "NLP has also penetrated the field of bimedical informatics and has been particularly focused on biomedical literature for over the past decade.", "labels": [], "entities": []}, {"text": "Advances in that sub-field has also been documented in surveys such as one by.", "labels": [], "entities": []}, {"text": "Word sense disambiguation also has along history in the general NLP domain).", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6331410904725393}]}, {"text": "In spite of word sense annotations in the biomedical literature, recent work by highlights the importance of annotating them in clinical notes.", "labels": [], "entities": []}, {"text": "This is true for many other clinical and linguistic phenomena as the various characteristics of the clinical narrative present a unique challenge to NLP.", "labels": [], "entities": []}, {"text": "Recently various initiatives have led to annotated corpora for clinical NLP research.", "labels": [], "entities": []}, {"text": "Probably the first comprehensive annotation performed on a clinical corpora was by, but unfortunately that corpus is not publicly available owing to privacy regulations.", "labels": [], "entities": []}, {"text": "The i2b2 initiative 4 challenges have focused on such topics as concept recognition), coreference resolution (), temporal relations ( and their datasets are available to the community.", "labels": [], "entities": [{"text": "concept recognition", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7637338042259216}, {"text": "coreference resolution", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.9485631287097931}]}, {"text": "More recently, the Shared Annotated Resources (ShARe) 1 project has created a corpus annotated with disease/disorder mentions in clinical notes as well as normalized them to a concept unique identifier (CUI) within the SNOMED-CT subset of the Unified Medical Language System 5: Distribution of data in terms of notes and disorder mentions across the training, development and test sets.", "labels": [], "entities": []}, {"text": "The disorders are further split according to two criteria -whether they map to a CUI or whether they are contiguous.", "labels": [], "entities": []}, {"text": "The task of normalization is a combination of word/phrase sense disambiguation and semantic similarity where a phrase is mapped to a unique concept in an ontology (based on the description of that concept in the ontology) after disambiguating potential ambiguous surface words, or phrases.", "labels": [], "entities": [{"text": "word/phrase sense disambiguation", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.6526959538459778}]}, {"text": "This is especially true with abbreviations and acronyms which are much more common in clinical text ().", "labels": [], "entities": []}, {"text": "The SemEval-2014 task 7 was one of nine shared tasks organized at the SemEval-2014.", "labels": [], "entities": [{"text": "SemEval-2014 task 7", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7274271647135416}]}, {"text": "It was designed as a followup to the shared tasks organized during the ShARe/CLEF eHealth 2013 evaluation ().", "labels": [], "entities": [{"text": "ShARe/CLEF eHealth 2013 evaluation", "start_pos": 71, "end_pos": 105, "type": "DATASET", "confidence": 0.7770814001560211}]}, {"text": "Like the previous shared task, we relied on the ShARe corpus, but with more data for training and anew test set.", "labels": [], "entities": [{"text": "ShARe corpus", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.8474479019641876}]}, {"text": "Furthermore, in this task, we provided the options to participants to utilize a large corpus of unlabeled clinical notes.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the characteristics of the data used in the task.", "labels": [], "entities": []}, {"text": "Section 3 describes the tasks in more detail.", "labels": [], "entities": []}, {"text": "Section 4 explains the evaluation criteria for the two tasks.", "labels": [], "entities": []}, {"text": "Section 5 lists the participants of the task.", "labels": [], "entities": []}, {"text": "Section 6 discusses the results on this task and also compares them with the ShARe/CLEF eHealth 2013 results, and Section 7 concludes.", "labels": [], "entities": [{"text": "ShARe/CLEF eHealth 2013 results", "start_pos": 77, "end_pos": 108, "type": "DATASET", "confidence": 0.8219136397043864}]}], "datasetContent": [{"text": "The following evaluation criteria were used: \u2022 Task A -The system performance was evaluated against the gold standard using the F 1 -score of the Precision and Recall values.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9859156459569931}, {"text": "Precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.986912727355957}, {"text": "Recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.7193102836608887}]}, {"text": "There were two variations: (i) Strict; and (ii) Relaxed.", "labels": [], "entities": [{"text": "Strict", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.812563419342041}, {"text": "Relaxed", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9694836735725403}]}, {"text": "The formulae for computing these metrics are mentioned below.", "labels": [], "entities": []}, {"text": "Where, D tp = Number of true positives disorder mentions; D f p = Number of false positives disorder mentions; D f n = Number of false negative disorder mentions.", "labels": [], "entities": []}, {"text": "In the strict case, a span was counted as correct if it was identical to the gold standard span, whereas   in the relaxed case, a span overlapping with the gold standard span was also considered correct.", "labels": [], "entities": []}, {"text": "\u2022 Task B -Accuracy was used as the performance measure for Task 1b.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.5970059633255005}]}, {"text": "It was defined as follows: Where, D tp = Number of true positive disorder mentions with identical spans as in the gold standard; N correct = Number of correctly normalized disorder mentions; and T g = Total number of disorder mentions in the gold standard.", "labels": [], "entities": []}, {"text": "For Task B, the systems were only evaluated on annotations they identified in Task A. Relaxed accuracy only measured the ability to normalize correct spans.", "labels": [], "entities": [{"text": "Relaxed", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9025058746337891}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.6337690353393555}]}, {"text": "Therefore, it was possible to obtain very high values for this measure by simply dropping any mention with a low confidence span.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of data in terms of notes and disorder mentions across the training, development  and test sets. The disorders are further split according to two criteria -whether they map to a CUI or  whether they are contiguous.", "labels": [], "entities": []}, {"text": " Table 2: Inter-annotator (A1 and A2) and gold  standard (GS) agreement as F 1 -score for the Dis- order mentions and their normalization to the  UMLS CUI.", "labels": [], "entities": [{"text": "gold  standard (GS) agreement", "start_pos": 42, "end_pos": 71, "type": "METRIC", "confidence": 0.8858603735764822}, {"text": "F 1 -score", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9835696518421173}, {"text": "UMLS CUI", "start_pos": 146, "end_pos": 154, "type": "DATASET", "confidence": 0.934365451335907}]}, {"text": " Table 4: Performance on test data for participating systems on Task A -Identification of disorder men- tions.", "labels": [], "entities": [{"text": "Identification of disorder men- tions", "start_pos": 72, "end_pos": 109, "type": "TASK", "confidence": 0.786758283774058}]}, {"text": " Table 5: Performance on development data for participating systems on Task A -Identification of disor- der mentions.", "labels": [], "entities": []}, {"text": " Table 6: Performance on test data for participat- ing systems on Task B -Normalization of disorder  mentions to UMLS (SNOMED-CT subset) CUIs.", "labels": [], "entities": []}, {"text": " Table 7: Performance on development data  for some participating systems on Task B - Normalization of disorder mentions to UMLS  (SNOMED-CT subset) CUIs.", "labels": [], "entities": []}]}