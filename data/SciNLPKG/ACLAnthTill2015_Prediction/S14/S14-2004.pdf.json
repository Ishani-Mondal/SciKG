{"title": [{"text": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis", "labels": [], "entities": [{"text": "SemEval-2014 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8676744103431702}, {"text": "Aspect Based Sentiment Analysis", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.76009301841259}]}], "abstractContent": [{"text": "Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9728310406208038}]}, {"text": "The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, irrespective of the entities mentioned (e.g., laptops) and their aspects (e.g., battery, screen).", "labels": [], "entities": []}, {"text": "SemEval-2014 Task 4 aimed to foster research in the field of aspect-based sentiment analysis, where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect.", "labels": [], "entities": [{"text": "aspect-based sentiment analysis", "start_pos": 61, "end_pos": 92, "type": "TASK", "confidence": 0.7193691531817118}]}, {"text": "The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure.", "labels": [], "entities": []}, {"text": "It attracted 163 submissions from 32 teams.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the proliferation of user-generated content on the web, interest in mining sentiment and opinions in text has grown rapidly, both in academia and business.", "labels": [], "entities": []}, {"text": "Early work in sentiment analysis mainly aimed to detect the overall polarity (e.g., positive or negative) of a given text or text span ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.9615388512611389}]}, {"text": "However, the need fora more fine-grained approach, such as aspect-based (or 'feature-based') sentiment analysis (ABSA), soon became apparent (.", "labels": [], "entities": [{"text": "aspect-based (or 'feature-based') sentiment analysis (ABSA)", "start_pos": 59, "end_pos": 118, "type": "TASK", "confidence": 0.7621237771077589}]}, {"text": "For example, laptop reviews not only express the overall sentiment about a specific model (e.g., \"This is a great laptop\"), but also sentiments relating to its specific aspects, such as the hardware, software, price, etc.", "labels": [], "entities": []}, {"text": "Subsequently, a review may convey opposing sentiments (e.g., \"Its performance is ideal, I wish I could say the same about the price\") or objective information (e.g., \"This one still has the CD slot\") for different aspects of an entity.", "labels": [], "entities": []}, {"text": "ABSA is critical in mining and summarizing opinions from on-line reviews (.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8229427933692932}, {"text": "summarizing", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.958570122718811}]}, {"text": "In this setting, ABSA aims to identify the aspects of the entities being reviewed and to determine the sentiment the reviewers express for each aspect.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.8846611380577087}]}, {"text": "Within the last decade, several ABSA systems of this kind have been developed for movie reviews, customer reviews of electronic products like digital cameras (Hu and Liu, 2004a) or netbook computers, services (, and restaurants (.", "labels": [], "entities": []}, {"text": "Previous publicly available ABSA benchmark datasets adopt different annotation schemes within different tasks.", "labels": [], "entities": [{"text": "ABSA benchmark datasets", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.7701714138189951}]}, {"text": "The restaurant reviews dataset of uses six coarse-grained aspects (e.g., FOOD, PRICE, SERVICE) and four overall sentence polarity labels (positive, negative, conflict, neutral).", "labels": [], "entities": [{"text": "FOOD", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.993897020816803}, {"text": "SERVICE", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9826436638832092}]}, {"text": "Each sentence is assigned one or more aspects together with a polarity label for each aspect; for example, \"The restaurant was expensive, but the menu was great.\" would be assigned the aspect PRICE with negative polarity and FOOD with positive polarity.", "labels": [], "entities": [{"text": "PRICE", "start_pos": 192, "end_pos": 197, "type": "METRIC", "confidence": 0.747627854347229}, {"text": "FOOD", "start_pos": 225, "end_pos": 229, "type": "METRIC", "confidence": 0.995371401309967}]}, {"text": "In the product reviews dataset of, aspect terms, i.e., terms naming aspects (e.g., 'radio', 'voice dialing') together with strength scores (e.g., 'radio': +2, 'voice dialing': \u22123) are pro-vided.", "labels": [], "entities": []}, {"text": "No predefined inventory of aspects is provided, unlike the dataset of Ganu et al.", "labels": [], "entities": []}, {"text": "The SemEval-2014 ABSA Task is based on laptop and restaurant reviews and consists of four subtasks (see.", "labels": [], "entities": [{"text": "SemEval-2014 ABSA Task", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.676818052927653}]}, {"text": "Participants were free to participate in a subset of subtasks and the domains (laptops or restaurants) of their choice.", "labels": [], "entities": []}], "datasetContent": [{"text": "The datasets of the ABSA task were provided in an XML format (see).", "labels": [], "entities": [{"text": "ABSA task", "start_pos": 20, "end_pos": 29, "type": "TASK", "confidence": 0.9130900502204895}]}, {"text": "They are available with anon commercial, no redistribution license through META-SHARE, a repository devoted to the sharing and dissemination of language resources (Piperidis, 2012).", "labels": [], "entities": []}, {"text": "The evaluation of the ABSA task ran in two phases.", "labels": [], "entities": [{"text": "ABSA task", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.8556294739246368}]}, {"text": "In Phase A, the participants were asked to return the aspect terms (SB1) and aspect categories (SB3) for the provided test datasets.", "labels": [], "entities": [{"text": "aspect terms (SB1) and aspect categories (SB3)", "start_pos": 54, "end_pos": 100, "type": "METRIC", "confidence": 0.7356152805415067}]}, {"text": "Subsequently, in Phase B, the participants were given the gold aspect terms and aspect categories (as in) for the sentences of Phase A and they were asked to return the polarities of the aspect terms (SB2) and the polarities of the aspect categories of each sentence (SB4).", "labels": [], "entities": []}, {"text": "6 Each participating team was allowed to submit up to two runs per subtask and domain (restaurants, laptops) in each phase; one constrained (C), where only the provided training data and other resources (e.g., publicly available lexica) excluding additional annotated sentences could be used, and one unconstrained (U), where additional data of any kind could be used for training.", "labels": [], "entities": []}, {"text": "In the latter case, the teams had to report the resources they used.", "labels": [], "entities": []}, {"text": "To evaluate aspect term extraction (SB1) and aspect category detection (SB3) in Phase A, we used The datasets can be downloaded from http:// metashare.ilsp.gr:8080/.", "labels": [], "entities": [{"text": "aspect term extraction", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.6324944198131561}, {"text": "aspect category detection", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.6379349728425344}]}, {"text": "META-SHARE (http: //www.meta-share.org/) was implemented in the framework of the META-NET Network of Excellence (http://www.meta-net.eu/).", "labels": [], "entities": [{"text": "META-SHARE", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9339349865913391}]}, {"text": "Phase A ran from 9:00 GMT, March 24 to 21:00 GMT, March 25, 2014.", "labels": [], "entities": []}, {"text": "Phase Bran from 9:00 GMT, March 27 to 17:00 GMT, the F 1 measure, defined as usually: where precision (P ) and recall (R) are defined as: Here S is the set of aspect term or aspect category annotations (in SB1 and SB3, respectively) that a system returned for all the test sentences (of a domain), and G is the set of the gold (correct) aspect term or aspect category annotations.", "labels": [], "entities": [{"text": "Bran", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9511871337890625}, {"text": "F 1 measure", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9360135793685913}, {"text": "precision (P )", "start_pos": 92, "end_pos": 106, "type": "METRIC", "confidence": 0.9334812760353088}, {"text": "recall (R)", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9575700461864471}]}, {"text": "To evaluate aspect term polarity (SB2) and aspect category polarity (SB4) detection in Phase B, we calculated the accuracy of each system, defined as the number of correctly predicted aspect term or aspect category polarity labels, respectively, divided by the total number of aspect term or aspect category annotations.", "labels": [], "entities": [{"text": "aspect category polarity (SB4) detection", "start_pos": 43, "end_pos": 83, "type": "TASK", "confidence": 0.606565420116697}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9993367791175842}]}, {"text": "Recall that we used the gold aspect term and category annotations in Phase B. We provided four baselines, one per subtask: 7 Aspect term extraction (SB1) baseline: A sequence of tokens is tagged as an aspect term in a test sentence (of a domain), if it is listed in a dictionary that contains all the aspect terms of the training sentences (of the same domain).", "labels": [], "entities": [{"text": "Aspect term extraction (SB1) baseline", "start_pos": 125, "end_pos": 162, "type": "METRIC", "confidence": 0.7625828300203595}]}, {"text": "Aspect term polarity (SB2) baseline: For each aspect term tin a test sentence s (of a particular domain), this baseline checks if t had been encountered in the training sentences (of the domain).", "labels": [], "entities": [{"text": "Aspect term polarity (SB2) baseline", "start_pos": 0, "end_pos": 35, "type": "METRIC", "confidence": 0.7490829314504351}]}, {"text": "If so, it retrieves the k most similar to s training sentences (of the domain), and assigns to the aspect term t the most frequent polarity it had in the k sentences.", "labels": [], "entities": []}, {"text": "Otherwise, if t had not been encountered in the training sentences, it is assigned the most frequent aspect term polarity label of the <sentence id=\"11351725#582163#9\"> <text>Our waiter was friendly and it is a shame that he didnt have a supportive staff to work with.</text> <aspectTerms> <aspectTerm term=\"waiter\" polarity=\"positive\" from=\"4\" to=\"10\"/> <aspectTerm term=\"staff\" polarity=\"negative\" from=\"74\" to=\"79\"/> </aspectTerms> <aspectCategories> <aspectCategory category=\"service\" polarity=\"conflict\"/> </aspectCategories> </sentence>: An XML snippet that corresponds to the annotated sentence of training set.", "labels": [], "entities": []}, {"text": "The similarity between two sentences is measured as the Dice coefficient of the sets of (distinct) words of the two sentences.", "labels": [], "entities": [{"text": "Dice coefficient", "start_pos": 56, "end_pos": 72, "type": "METRIC", "confidence": 0.9859268665313721}]}, {"text": "For example, the similarity between \"this is a demo\" and \"that is yet another demo\" is 2\u00b72 4+5 = 0.44.", "labels": [], "entities": []}, {"text": "Aspect category extraction (SB3) baseline: For every test sentence s, the k most similar to s training sentences are retrieved (as in the SB2 baseline).", "labels": [], "entities": [{"text": "Aspect category extraction", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6077981193860372}]}, {"text": "Then, sis assigned them most frequent aspect category labels of the k retrieved sentences; m is the most frequent number of aspect category labels per sentence among the k sentences.", "labels": [], "entities": []}, {"text": "Aspect category polarity (SB4): This baseline assigns to each aspect category c of a test sentence s the most frequent polarity label that chad in the k most similar to s training sentences (of the same domain), considering only training sentences that have the aspect category label c.", "labels": [], "entities": []}, {"text": "Sentence similarity is computed as in the SB2 baseline.", "labels": [], "entities": [{"text": "Sentence similarity", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.5994985699653625}, {"text": "SB2 baseline", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.8142282366752625}]}, {"text": "For subtasks SB2 and SB4, we also use a majority baseline that assigns the most frequent polarity (in the training data) to all the aspect terms and aspect categories.", "labels": [], "entities": []}, {"text": "The scores of all the baselines and systems are presented in Tables 4-6.", "labels": [], "entities": []}, {"text": "The ABSA task attracted 32 teams in total and 165 submissions (systems), 76 for phase A and 89 for phase B.", "labels": [], "entities": [{"text": "ABSA task", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.5884190201759338}]}, {"text": "Based on the human-annotation experience, the expectations were that systems would perform better in Phase B (SB3, SB4, involving aspect categories) than in Phase A (SB1, SB2, involving aspect terms).", "labels": [], "entities": []}, {"text": "The evaluation results confirmed our expectations).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sizes (sentences) of the datasets.", "labels": [], "entities": [{"text": "Sizes (sentences)", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8158570826053619}]}, {"text": " Table 2: Aspect terms and their polarities per do- main. LPT and RST indicate laptop and restau- rant reviews, respectively. TR and TE indicate the  training and test set.", "labels": [], "entities": [{"text": "Aspect", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9648110866546631}, {"text": "RST", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9526886343955994}, {"text": "TE", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.8928923010826111}]}, {"text": " Table 3: Aspect categories distribution per sentiment class.", "labels": [], "entities": []}, {"text": " Table 4: Results for aspect term extraction (SB1).  Stars indicate unconstrained systems. The  \u2020 indi- cates a constrained system that was not trained on  the in-domain training dataset (unlike the rest of  the constrained systems), but on the union of the  two training datasets (laptops, restaurants).", "labels": [], "entities": [{"text": "aspect term extraction", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.5777084728082021}]}, {"text": " Table 5: Results for aspect category detection  (SB3) and aspect category polarity (SB4). Stars  indicate unconstrained systems.", "labels": [], "entities": [{"text": "aspect category detection  (SB3", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6463129758834839}]}]}