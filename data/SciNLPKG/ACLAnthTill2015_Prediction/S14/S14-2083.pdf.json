{"title": [{"text": "RelAgent: Entity Detection and Normalization for Diseases in Clinical Records: a Linguistically Driven Approach", "labels": [], "entities": [{"text": "Entity Detection and Normalization", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.6931018084287643}]}], "abstractContent": [{"text": "We refined the performance of Co-coa/Peaberry, a linguistically motivated system, on extracting disease entities from clinical notes in the training and development sets for Task 7.", "labels": [], "entities": []}, {"text": "Entities were identified in noun chunks by use of dictionaries, and events ('The left atrium is dilated') through our own parser and predicate-argument structures.", "labels": [], "entities": []}, {"text": "We also developed a module to map the extracted entities to the SNOMED subset of UMLS.", "labels": [], "entities": []}, {"text": "The module is based on direct matching against UMLS entries through regular expressions derived from a small set of morphological transformations, along with priority rules when multiple UMLS entries were matched.", "labels": [], "entities": []}, {"text": "The performance on training and development sets was 81.0% and 83.3% respectively (Task A), and the UMLS matching scores were respectively 75.3% and 78.2% (Task B).", "labels": [], "entities": [{"text": "UMLS", "start_pos": 100, "end_pos": 104, "type": "DATASET", "confidence": 0.48600929975509644}, {"text": "matching", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.5508533716201782}]}, {"text": "However, the performance against the test set was low by comparison, 72.0% for Task A and 63.9% for Task B, even while the pure UMLS mapping score was reasonably high (relaxed score in Task B = 91.2%).", "labels": [], "entities": [{"text": "relaxed score", "start_pos": 168, "end_pos": 181, "type": "METRIC", "confidence": 0.9721409380435944}]}, {"text": "We speculate that our moderate performance on the test set derives primarily from chunking/parsing errors.", "labels": [], "entities": [{"text": "chunking/parsing", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.7823116381963094}]}], "introductionContent": [{"text": "The increasing use of electronic health records, both for satisfying mandatory requirements as well as for administrative reasons, has created a need for systems to automatically tag and normalize disease/sign/symptom mentions.", "labels": [], "entities": [{"text": "normalize disease/sign/symptom mentions", "start_pos": 187, "end_pos": 226, "type": "TASK", "confidence": 0.7313143653529031}]}, {"text": "Statistically significant correlations extracted from automated analysis of large databases of clinical records are felt to be useful in detecting phenotype-genotype correlations (reviewed in), phenotypephenotype correlations) as well as in continuous monitoring of events such as adverse reactions and even early detection of outbreaks of epidemics/infectious diseases (.", "labels": [], "entities": [{"text": "early detection of outbreaks of epidemics/infectious diseases", "start_pos": 308, "end_pos": 369, "type": "TASK", "confidence": 0.7221366696887546}]}, {"text": "In this context, Task 7 of SemEval 2014, which is a continuation of the ShARe/CLEF eHealth 2013 task (, provides a testbed to evaluate systems that automatically tag and normalize mentions of diseases, signs and symptoms in clinical records, which include discharge summaries and echo, radiology and ECG reports.", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.85113725066185}, {"text": "ShARe/CLEF eHealth 2013 task", "start_pos": 72, "end_pos": 100, "type": "DATASET", "confidence": 0.658644879857699}, {"text": "echo", "start_pos": 280, "end_pos": 284, "type": "METRIC", "confidence": 0.9023491740226746}]}, {"text": "Our system consists of (i) Cocoa, a chunkbased entity tagger and (ii) Peaberry, a parser, followed by a module for predicate-argument structure.", "labels": [], "entities": []}, {"text": "We have tested the system in a variety of tasks, such as detecting and normalizing mentions of chemicals, proteins/genes, diseases and action terms in the BioCreative 13 Chemdner and CTD tasks, as well as in detecting cellular and pathological events in the BioNLP cancer genetics task (Ramanan and Senthil Nathan, 2013c); we also participated in the eHealth 2013 task ().", "labels": [], "entities": [{"text": "detecting and normalizing mentions of chemicals, proteins/genes, diseases and action terms", "start_pos": 57, "end_pos": 147, "type": "TASK", "confidence": 0.839831268787384}, {"text": "BioNLP cancer genetics task", "start_pos": 258, "end_pos": 285, "type": "TASK", "confidence": 0.5489764511585236}, {"text": "eHealth 2013 task", "start_pos": 351, "end_pos": 368, "type": "DATASET", "confidence": 0.9035693605740865}]}, {"text": "Throughout, we have retained a common core platform for simultaneous detection of a multiplicity of entity types as well as for chunking and parsing; we restrict task-specific optimization primarily to post-processing modules.", "labels": [], "entities": [{"text": "chunking and parsing", "start_pos": 128, "end_pos": 148, "type": "TASK", "confidence": 0.601069430510203}]}, {"text": "While this strategy may not be optimal for any individual task, we feel that it is necessary for multi-document spanning tasks such as literature-based discovery, where connections are established across a variety of scales, e.g. from molecular events to patho-physiological phenotypes.", "labels": [], "entities": [{"text": "literature-based discovery", "start_pos": 135, "end_pos": 161, "type": "TASK", "confidence": 0.6830696910619736}]}, {"text": "Moreover, these linkages need to be made across a multiplicity of documents from various sources, which encompass a linguistic range from complex syntactical utterances in biomedical publications to free-form phrase-centered clinical notes.", "labels": [], "entities": []}, {"text": "We refined performance against the provided training and development sets, with reasonable performance in Task A (relaxed f = 0.94, strict f = 0.81 \u2212 0.83, strict recall 0.80 \u2212 0.82).", "labels": [], "entities": [{"text": "recall", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.6959947347640991}]}, {"text": "A module to match text from gold-annotated exact spans to UMLS codes also achieved reasonable performance for Task B (relaxed accuracy = 0.94\u221296).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.7567947506904602}]}, {"text": "However, the results against from the test set were quite low for Task A, (relaxed f = 0.87, strict f = 0.72, strict recall = 0.70) as well as for Task B (strict f = 0.64).", "labels": [], "entities": [{"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.8595852255821228}]}, {"text": "Comparatively, the module for UMLS normalization fared better (relaxed f = 0.91 in Task B).", "labels": [], "entities": [{"text": "UMLS normalization", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8831973671913147}]}, {"text": "We speculate that the test set contains entities that are rare in the training/development sets which were chunked incorrectly, and also that the parse errors in the test set arose from syntactic structures missing in the training sets.", "labels": [], "entities": []}, {"text": "It is possible that a post-processing statistical module trained on a combination of gold annotations as well as linguistic output maybe needed for improving the performance of our system on clinical notes.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}