{"title": [{"text": "Contrasting Syntagmatic and Paradigmatic Relations: Insights from Distributional Semantic Models", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a large-scale evaluation of bag-of-words distributional models on two datasets from priming experiments involving syntagmatic and paradigmatic relations.", "labels": [], "entities": []}, {"text": "We interpret the variation in performance achieved by different settings of the model parameters as an indication of which aspects of distributional patterns characterize these types of relations.", "labels": [], "entities": []}, {"text": "Contrary to what has been argued in the literature (Rapp, 2002; Sahlgren, 2006)-that bag-of-words models based on second-order statistics mainly capture paradig-matic relations and that syntagmatic relations need to be gathered from first-order models-we show that second-order models perform well on both paradigmatic and syntagmatic relations if their parameters are properly tuned.", "labels": [], "entities": []}, {"text": "In particular, our results show that size of the context window and dimensionality reduction play a key role in differentiating DSM performance on paradigmatic vs. syntagmatic relations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional takes on the representation and acquisition of word meaning rely on the assumption that words with similar meaning tend to occur in similar contexts: this assumption, known as distributional hypothesis, has been first proposed by.", "labels": [], "entities": [{"text": "representation and acquisition of word meaning", "start_pos": 28, "end_pos": 74, "type": "TASK", "confidence": 0.7242386986811956}]}, {"text": "Distributional Semantic Models (henceforth, DSMs) are computational models that operationalize the distributional hypothesis; they produce semantic representations for words in the form of distributional vectors recording patterns of co-occurrence in large samples of language data;.", "labels": [], "entities": []}, {"text": "Comparison between distributional vectors allows the identification of shared contexts as an empirical correlate of the semantic similarity between the target words.", "labels": [], "entities": []}, {"text": "As noted in, the notion of semantic similarity applied in distributional approaches to meaning is an easy target of criticism, as it is employed to capture a wide range of semantic relations, such as synonymy, antonymy, hypernymy, up to topical relatedness.", "labels": [], "entities": []}, {"text": "The study presented in this paper contributes to the debate concerning the nature of the semantic representations built by DSMs, and it does so by comparing the performance of several DSMs in a classification task conducted on priming data and involving paradigmatic and syntagmatic relations.", "labels": [], "entities": []}, {"text": "Paradigmatic relations hold between words that occur in similar contexts; they are also called relations in absentia) because paradigmatically related words do not co-occur.", "labels": [], "entities": []}, {"text": "Examples of paradigmatic relations are synonyms (e.g., frigid-cold) and antonyms (e.g., cold-hot).", "labels": [], "entities": []}, {"text": "Syntagmatic relations hold between words that cooccur (relations in praesentia) and therefore exhibit a similar distribution across contexts.", "labels": [], "entities": []}, {"text": "Typical examples of syntagmatic relations are phrasal associates (e.g., help-wanted) and syntactic collocations (e.g., dog-bark).", "labels": [], "entities": []}, {"text": "Distributional modeling has already tackled the issue of paradigmatic and syntagmatic relations.", "labels": [], "entities": [{"text": "Distributional modeling", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8396192491054535}]}, {"text": "Key contributions of the present work are the scope of its evaluation (in terms of semantic relations and model parameters) and the new perspective on paradigmatic vs. syntagmatic models provided by our results.", "labels": [], "entities": []}, {"text": "Concerning the scope of the evaluation, this is the first study in which the comparison involves such a wide range of semantic relations (paradigmatic: synonyms, antonyms and co-hyponyms; syntagmatic: syntactic collocations, backward and forward phrasal associates).", "labels": [], "entities": []}, {"text": "Moreover, our evaluation covers a large number of DSM parameters: source corpus, size and direction of the context window, criteria for feature selection, feature weighting, dimensionality reduction and index of distributional relatedness.", "labels": [], "entities": []}, {"text": "We consider the variation in performance achieved by different parameter settings as a cue towards characteristic aspects of specific relations (or groups of relations).", "labels": [], "entities": []}, {"text": "Our work also differs from previous studies) in its focus on second-order models.", "labels": [], "entities": []}, {"text": "We aim to show that they are able to capture both paradigmatic and syntagmatic relations with appropriate parameter settings.", "labels": [], "entities": []}, {"text": "In addition, this focus provides a uniform experimental design for the evaluation.", "labels": [], "entities": []}, {"text": "For example, parameters like window size and directionality apply to bag-of-words DSMs and collocation lists but not to term-context models; dimensionality reduction, whose effect has not yet been explored systematically in the context of syntagmatic and paradigmatic relations, is not applicable to collocation lists.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 summarizes previous work.", "labels": [], "entities": []}, {"text": "Section 3 describes the experimental setup, in terms of task, datasets and evaluated parameters.", "labels": [], "entities": []}, {"text": "Section 4 introduces our model selection methodology.", "labels": [], "entities": [{"text": "model selection", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7470676898956299}]}, {"text": "Section 5 presents the results of our evaluation study.", "labels": [], "entities": []}, {"text": "Section 6 summarizes main findings and sketches ongoing and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this study, bag-of-words DSMs are evaluated on two datasets containing experimental items from two priming studies.", "labels": [], "entities": []}, {"text": "Each item is a word triple (target, consistent prime, inconsistent prime) with a particular semantic relation between target and consistent prime.", "labels": [], "entities": []}, {"text": "Following previous work on modeling priming effects as a comparison between prime-target pairs (, we evaluate our models in a classification task.", "labels": [], "entities": []}, {"text": "The goal is to identify the consistent prime on the basis of its distributional relatedness to the target: if a particular DSM (i.e., a certain parameter combination) is sensitive to a specific relation (or group of relations), we expect the consistent primes to be closer to the target in semantic space than the inconsistent ones.", "labels": [], "entities": []}, {"text": "The first dataset is derived from the Semantic Priming Project (SPP) (.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, our study represents the first evaluation of bag-of-words DSMs on items from this dataset.", "labels": [], "entities": []}, {"text": "The original data consist of 1661 word triples (target, consistent prime, inconsistent prime) collected within a large-scale project aiming at characterizing English words in terms of a set of lexical and associative/semantic characteristics, along with behavioral data from visual lexical decision and naming studies 2 . We manually discarded all triples containing proper nouns, adverbs or inflected words.", "labels": [], "entities": []}, {"text": "We then selected five subsets involving different semantic relations, namely: synonyms (SYN), 436 triples (example of a consistent prime and target: frigid-cold); antonyms (ANT): 135 triples (e.g., hot-cold); cohyponyms (COH): 159 triples (e.g., table-chair); forward phrasal associates (FPA): 144 triples (e.g., help-wanted); back-ward phrasal associates (BPA): 89 triples (e.g., wanted-help).", "labels": [], "entities": [{"text": "back-ward phrasal associates (BPA)", "start_pos": 327, "end_pos": 361, "type": "METRIC", "confidence": 0.6140715877215067}]}, {"text": "The second priming dataset is the Generalized Event Knowledge dataset (henceforth GEK), already evaluated in: a collection of 402 triples (target, consistent prime, inconsistent prime) from three priming studies conducted to demonstrate that event knowledge is responsible for facilitation of the processing of words that denote events and their participants.", "labels": [], "entities": [{"text": "Generalized Event Knowledge dataset (henceforth GEK)", "start_pos": 34, "end_pos": 86, "type": "DATASET", "confidence": 0.7224154807627201}]}, {"text": "The first study was conducted by, who found that verbs facilitate the processing of nouns denoting prototypical participants in the depicted event and of adjectives denoting features of prototypical participants.", "labels": [], "entities": []}, {"text": "The study covered five thematic relations: agent (e.g., pay-customer), patient, feature of the patient, instrument, location.", "labels": [], "entities": []}, {"text": "The second study) focussed on priming from nouns to verbs.", "labels": [], "entities": []}, {"text": "It involved four relations: agent (e.g., reporter-interview), patient, instrument, location.", "labels": [], "entities": []}, {"text": "The third study) investigated priming from nouns to nouns, referring to participants of the same event or the event itself.", "labels": [], "entities": []}, {"text": "The dataset involves seven relations: event-people (e.g., trial-judge), eventthing, location-living, location-thing, peopleinstrument, instrument-people, instrument-thing.", "labels": [], "entities": []}, {"text": "In the presentation of our results we group synonyms with antonyms and cohyponyms from SPP as paradigmatic relations, and the entire GEK dataset with backward and forward phrasal associates from SPP as syntagmatic relations.", "labels": [], "entities": [{"text": "GEK dataset", "start_pos": 133, "end_pos": 144, "type": "DATASET", "confidence": 0.9413832426071167}]}], "tableCaptions": [{"text": " Table 1: Evaluation, Global R 2", "labels": [], "entities": [{"text": "Global R 2", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.878933052221934}]}, {"text": " Table 2: Distribution of Accuracy", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9469172358512878}]}, {"text": " Table 3: Best settings: datasets, parameter values,  accuracy (acc), accuracy of the best model (best)", "labels": [], "entities": [{"text": "accuracy (acc)", "start_pos": 54, "end_pos": 68, "type": "METRIC", "confidence": 0.928345263004303}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.999108612537384}]}, {"text": " Table 4: General best settings: parameter values", "labels": [], "entities": []}]}