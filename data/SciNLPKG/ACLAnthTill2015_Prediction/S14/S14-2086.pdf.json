{"title": [{"text": "RTRGO: Enhancing the GU-MLT-LT System for Sentiment Analysis of Short Messages", "labels": [], "entities": [{"text": "RTRGO", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.44755637645721436}, {"text": "Sentiment Analysis of Short Messages", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.9363879442214966}]}], "abstractContent": [{"text": "This paper describes the enhancements made to our GU-MLT-LT system (G\u00fcnther and Furrer, 2013) for the SemEval-2014 rerun of the SemEval-2013 shared task on sentiment analysis in Twitter.", "labels": [], "entities": [{"text": "SemEval-2013 shared task on sentiment analysis in Twitter", "start_pos": 128, "end_pos": 185, "type": "TASK", "confidence": 0.7648568823933601}]}, {"text": "The changes include the usage of a Twitter-specific to-kenizer, additional features and sentiment lexica, feature weighting and random sub-space learning.", "labels": [], "entities": []}, {"text": "The improvements result in an increase of 4.18 F-measure points on this year's Twitter test set, ranking 3rd.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9813913106918335}, {"text": "Twitter test set", "start_pos": 79, "end_pos": 95, "type": "DATASET", "confidence": 0.7915954987208048}]}], "introductionContent": [{"text": "Automatic analysis of sentiment expressed in text is an active research area in natural language processing with obvious commercial interest.", "labels": [], "entities": [{"text": "Automatic analysis of sentiment expressed in text", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8681989397321429}, {"text": "natural language processing", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.651242713133494}]}, {"text": "In the simplest formulation of the problem, sentiment analysis is framed as a categorization problem over documents, where the set of categories is typically a set of polarity values, such as positive, neutral, and negative.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9701288640499115}]}, {"text": "Many approaches to document-level sentiment classification have been proposed.", "labels": [], "entities": [{"text": "document-level sentiment classification", "start_pos": 19, "end_pos": 58, "type": "TASK", "confidence": 0.8231930136680603}]}, {"text": "For an overview see e.g..", "labels": [], "entities": []}, {"text": "Text in social media and in particular microblog messages area challenging text genre for sentiment classification, as they introduce additional problems such as short text length, spelling variation, special tokens, topic variation, language style and multilingual content.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.9543666541576385}]}, {"text": "Following, most sentiment analysis systems have been based on standard text categorization techniques, e.g. training a classifier using some sort of bag-of-words feature representation.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.9583671689033508}]}, {"text": "This is also true for sentiment.", "labels": [], "entities": [{"text": "sentiment", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.9643996953964233}]}, {"text": "In 2013, the International Workshop on Semantic Evaluation (SemEval) organized a shared task on sentiment analysis in Twitter ( to enable a better comparison of different approaches for sentiment analysis of microblogs.", "labels": [], "entities": [{"text": "International Workshop on Semantic Evaluation (SemEval)", "start_pos": 13, "end_pos": 68, "type": "TASK", "confidence": 0.6302859745919704}, {"text": "sentiment analysis", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8869271278381348}, {"text": "sentiment analysis of microblogs", "start_pos": 186, "end_pos": 218, "type": "TASK", "confidence": 0.8511951863765717}]}, {"text": "The shared task consisted of two subtasks: one on recognizing contextual polarity of a given subjective expression (Task A), and one on document-level sentiment classification (Task B).", "labels": [], "entities": [{"text": "document-level sentiment classification", "start_pos": 136, "end_pos": 175, "type": "TASK", "confidence": 0.7225973705450693}]}, {"text": "For both tasks, the training sets consisted of manually labeled Twitter messages, while the test sets consisted of a Twitter part and an SMS part in order to test domain sensitivity.", "labels": [], "entities": []}, {"text": "Among the best performing systems were, and, who all train linear models on a variety of task-specific features.", "labels": [], "entities": []}, {"text": "In this year the corpus resources were used fora re-run of the shared task (), introducing two new Twitter test sets, as well as LiveJournal data.", "labels": [], "entities": [{"text": "Twitter test sets", "start_pos": 99, "end_pos": 116, "type": "DATASET", "confidence": 0.7721997797489166}, {"text": "LiveJournal data", "start_pos": 129, "end_pos": 145, "type": "DATASET", "confidence": 0.9407534897327423}]}], "datasetContent": [{"text": "For the experiments and the training of the final model we used the joined training and development sets of subtask B.", "labels": [], "entities": []}, {"text": "We were able to retrieve 10368 Tweets, of which we merged all samples labeled as objective into the neutral class.", "labels": [], "entities": []}, {"text": "This resulted in a training set of 3855 positive, 4889 neutral and 1624 negative tweets.", "labels": [], "entities": []}, {"text": "The results of the experiments were obtained by performing 10-fold cross-validation, predicting positive, negative and neutral class.", "labels": [], "entities": []}, {"text": "Just as in the evaluation of the shared task the results are reported as average F-measure (F 1 ) between positive and negative class.", "labels": [], "entities": [{"text": "F-measure (F 1 )", "start_pos": 81, "end_pos": 97, "type": "METRIC", "confidence": 0.9215373516082763}]}, {"text": "To be able to evaluate the contribution of the different features groups to the final model we perform an ablation study.", "labels": [], "entities": []}, {"text": "By disabling one feature group at the time one can easily compare the performance of the model without a certain feature to the model using the complete feature set.", "labels": [], "entities": []}, {"text": "In Table 1 we present the results for the feature groups bigrams (2gr), stems (stem), word clusters (wc), sentiment lexica (lex), negation (neg), excluding names and user mentions (excl), feature weighting (wei) and random subspace learning (rssl).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Feature ablation study", "labels": [], "entities": []}, {"text": " Table 2: Final results of our submissions on the different test sets (Subtask B)", "labels": [], "entities": []}]}