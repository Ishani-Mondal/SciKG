{"title": [{"text": "ULisboa: Identification and Classification of Medical Concepts", "labels": [], "entities": [{"text": "ULisboa", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8374547958374023}, {"text": "Identification and Classification of Medical Concepts", "start_pos": 9, "end_pos": 62, "type": "TASK", "confidence": 0.7801462958256403}]}], "abstractContent": [{"text": "This paper describes our participation on Task 7 of SemEval 2014, which fo-cused on the recognition and disambigua-tion of medical concepts.", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.7802209854125977}, {"text": "recognition and disambigua-tion of medical concepts", "start_pos": 88, "end_pos": 139, "type": "TASK", "confidence": 0.7152904172738394}]}, {"text": "We used an adapted version of the Stanford NER system to train CRF models to recognize tex-tual spans denoting diseases and disorders , within clinical notes.", "labels": [], "entities": [{"text": "Stanford NER system", "start_pos": 34, "end_pos": 53, "type": "DATASET", "confidence": 0.7706396579742432}]}, {"text": "We considered an encoding that accounts with non-continuous entities, together with a rich set of features (i) based on domain specific lexicons like SNOMED CT, or (ii) leveraging Brown clusters inferred from a large collection of clinical texts.", "labels": [], "entities": []}, {"text": "Together with this recognition mechanism, we used a heuristic similarity search method, to assign an unambiguous identifier to each concept recognized in the text.", "labels": [], "entities": []}, {"text": "Our best run on Task A (i.e., in the recognition of medical concepts in the text) achieved an F-measure of 0.705 in the strict evaluation mode, and a promising F-measure of 0.862 in the relaxed mode, with a precision of 0.914.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9993593096733093}, {"text": "F-measure", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.9985272884368896}, {"text": "precision", "start_pos": 207, "end_pos": 216, "type": "METRIC", "confidence": 0.9910925030708313}]}, {"text": "For Task B (i.e., the disambiguation of the recognized concepts), we achieved less promising results, with an accuracy of 0.405 in the strict mode, and of 0.615 in the relaxed mode.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9992688298225403}]}], "introductionContent": [{"text": "Currently, many off-the-shelf named entity recognition solutions are available, and these can be used to recognize mentions in clinical notes denoting diseases and disorders.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7601732611656189}, {"text": "recognize mentions in clinical notes denoting diseases and disorders", "start_pos": 105, "end_pos": 173, "type": "TASK", "confidence": 0.7008183730973138}]}, {"text": "We decided to use the Stanford NER tool () to train CRF models based on annotated biomedical text.", "labels": [], "entities": [{"text": "Stanford NER tool", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.8930210073788961}]}, {"text": "The use of unsupervised methods for inferring word representations is nowadays also known to increase the accuracy of entity recognition models ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.999135434627533}, {"text": "entity recognition", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.7438682019710541}]}, {"text": "Thus, we also used Brown clusters) inferred from a large collection of non-annotated clinical texts, together with domain specific lexicons, to build features for our CRF models.", "labels": [], "entities": []}, {"text": "An important challenge in entity recognition relates to the recognition of overlapping and noncontinuous entities.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7956877946853638}]}, {"text": "In this paper, we describe how we modified the Stanford NER system to be able to recognize noncontinuous entities, through an adapted version of the SBIEO scheme ( . Besides the recognition of medical concepts, we also present the strategy used to map each of the recognized concepts into a SNOMED CT identifier (Cornet and de.", "labels": [], "entities": [{"text": "Stanford NER system", "start_pos": 47, "end_pos": 66, "type": "DATASET", "confidence": 0.8274889985720316}]}, {"text": "This task is particularly challenging, since there are many ambiguous cases.", "labels": [], "entities": []}, {"text": "We describe our general approach to address the aforementioned CUI mapping problem, based on similarity search and on the information content of SNOMED CT concept names.", "labels": [], "entities": [{"text": "CUI mapping problem", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8505929311116537}, {"text": "SNOMED CT concept names", "start_pos": 145, "end_pos": 168, "type": "DATASET", "confidence": 0.7184555232524872}]}], "datasetContent": [{"text": "Task 7 of SemEval 2014 actually consisted of two smaller tasks: recognition of mentions of medical concepts (Task A) and mapping each medical concept, recognized in clinical notes, to a unique UMLS CUI (Task B).", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.8399942219257355}, {"text": "recognition of mentions of medical concepts", "start_pos": 64, "end_pos": 107, "type": "TASK", "confidence": 0.901375542084376}]}, {"text": "In the first task, recognition of medical concepts, systems have to detect continuous and discontinuous medical concepts that belong to the UMLS semantic group disorders.", "labels": [], "entities": [{"text": "recognition of medical concepts", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.8764737397432327}, {"text": "UMLS semantic group disorders", "start_pos": 140, "end_pos": 169, "type": "DATASET", "confidence": 0.7977262139320374}]}, {"text": "The second task, concerning with normalization and mapping, is limited to UMLS CUIs of SNOMED CT codes (i.e., although the UMLS meta-thesaurus integrates several resources, we are only interested in SNOMED CT).", "labels": [], "entities": [{"text": "normalization", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9755958318710327}]}, {"text": "Each concept that was previously recognized can have a unique CUI associated to it, or none at all (CUI-LESS).", "labels": [], "entities": []}, {"text": "The goal here is to disambiguate the concepts and choose the right CUI for each case.", "labels": [], "entities": []}, {"text": "For supporting the recognition and CUI mapping of medical concepts, we retrieved the disorders subset of SNOMED CT directly from UMLS . The evaluation can be done in a strict or a in a relaxed way.", "labels": [], "entities": [{"text": "CUI mapping of medical concepts", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.8014545142650604}, {"text": "SNOMED CT", "start_pos": 105, "end_pos": 114, "type": "DATASET", "confidence": 0.7092968821525574}, {"text": "UMLS", "start_pos": 129, "end_pos": 133, "type": "DATASET", "confidence": 0.9514374732971191}]}, {"text": "For the case of strict evaluation, an exact match must be achieved in the recognition, by having correct start and end offsets, within the text, for the continuous concepts, and a correct set of start and end offsets for the discontinuous concepts.", "labels": [], "entities": []}, {"text": "In the relaxed evaluation, there is some space for errors in the offset values from the recognition task.", "labels": [], "entities": []}, {"text": "If there is some overlap between the concepts, then the result is considered a partial match, otherwise it is a recognition error.", "labels": [], "entities": []}, {"text": "A set of annotated biomedical texts was given to the participants, separated in three categories: trial, development and training.", "labels": [], "entities": []}, {"text": "We also received a final test set, and a large set of non-annotated texts.", "labels": [], "entities": []}, {"text": "All the provided texts were initially converted into a common tokenized format, to be used as input to the tools that we considered for developing our approach.", "labels": [], "entities": []}, {"text": "After processing, we converted the results back into the format used by SemEval 2014, this way generating the official runs.", "labels": [], "entities": []}, {"text": "We submitted three distinct runs to the SemEval competition.", "labels": [], "entities": [{"text": "SemEval competition", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8932793438434601}]}, {"text": "These runs were as follows: Run 1: A SBIOEN model was used to recognize non-continuous entities.", "labels": [], "entities": []}, {"text": "This model was trained using only the annotated texts from the provided training set.", "labels": [], "entities": []}, {"text": "We also used some domain specific lexicons like SNOMED CT, or lists with names for drugs and diseases retrieved from DBPedia.", "labels": [], "entities": [{"text": "SNOMED CT", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.7071331441402435}, {"text": "DBPedia", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9705950021743774}]}, {"text": "Finally, the recognition model also used Brown clusters generated from the non-annotated datasets provided in the competition.", "labels": [], "entities": []}, {"text": "For assigning a SNOMED CT identifier to each entity, we used the disambiguation technique supported by Lucene indexes.", "labels": [], "entities": []}, {"text": "In this specific run we used all the considered heuristics for similarity search.", "labels": [], "entities": [{"text": "similarity search", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.8904946148395538}]}, {"text": "Run 2: A simpler model based on the SBIOE scheme was used in this case, which can only recognize continuous entities.", "labels": [], "entities": []}, {"text": "The same features from Run 1 were used for training the recognition model.", "labels": [], "entities": []}, {"text": "For assigning the SNOMED CT identifier to each entity, we also used the same strategy that was presented for Run 1.", "labels": [], "entities": []}, {"text": "Run 3: A similar SBIOE model to that from Run 2 was used for the recognition.", "labels": [], "entities": [{"text": "recognition", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.9694797992706299}]}, {"text": "For assigning the corresponding SNOMED CT identifier to each entity, we in this case limited the heuristic rules that were used.", "labels": [], "entities": []}, {"text": "Instead of using the string similarity algorithms, we used only exact matches, together with the information content measure and the neighboring terms for disambiguation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Our official results for Tasks A and B of the SemEval challenge focusing on clinical text.", "labels": [], "entities": [{"text": "SemEval challenge", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.9343240857124329}]}]}