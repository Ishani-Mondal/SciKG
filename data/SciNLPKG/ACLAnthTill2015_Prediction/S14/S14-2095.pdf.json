{"title": [{"text": "Senti.ue: Tweet Overall Sentiment Classification Approach for SemEval-2014 Task 9", "labels": [], "entities": [{"text": "Sentiment Classification Approach", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.6422776778539022}, {"text": "SemEval-2014 Task 9", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.8719861507415771}]}], "abstractContent": [{"text": "This document describes the senti.ue system and how it was used for participation in SemEval-2014 Task 9 challenge.", "labels": [], "entities": [{"text": "SemEval-2014 Task 9 challenge", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.8479658514261246}]}, {"text": "Our system is an evolution of our prior work, also used in last year's edition of Sentiment Analysis in Twitter.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter", "start_pos": 82, "end_pos": 111, "type": "TASK", "confidence": 0.9094905406236649}]}, {"text": "This system maintains a supervised machine learning approach to classify the tweet overall sentiment, but with a change in the used features and the algorithm.", "labels": [], "entities": []}, {"text": "We use a restricted set of 47 features in subtask B and 31 features in subtask A.", "labels": [], "entities": []}, {"text": "In the constrained mode, and for the five data sources, senti.ue achieved a score between 78,72 and 84,05 in subtask A, and a score between 55,31 and 71,39 in sub-task B.", "labels": [], "entities": []}, {"text": "For the unconstrained mode, our score was slightly below, except for one casein subtask A.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the approach taken by a team of Universidade d\u00e9 Evora's Computer Science Department in SemEval-2014 Task 9: Sentiment Analysis in Twitter ().", "labels": [], "entities": [{"text": "SemEval-2014 Task 9", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.787135899066925}, {"text": "Sentiment Analysis in Twitter", "start_pos": 129, "end_pos": 158, "type": "TASK", "confidence": 0.8457393199205399}]}, {"text": "SemEval-2014 Task 9 has an expression-level (subtask A) and a message-level (subtask B) polarity classification challenges.", "labels": [], "entities": []}, {"text": "The first subtask aims to determine whether a word (or phrase) is positive, negative or neutral, within the textual context in which it appears.", "labels": [], "entities": []}, {"text": "The second subtask concerns the classification of the overall text polarity, which corresponds to automatically detecting the sentiment expressed in a Twitter message.", "labels": [], "entities": [{"text": "automatically detecting the sentiment expressed in a Twitter message", "start_pos": 98, "end_pos": 166, "type": "TASK", "confidence": 0.653624051147037}]}, {"text": "In both subtasks, systems can operate in constrained or unconstrained mode.", "labels": [], "entities": []}, {"text": "Constrained means that learn- ing is based only on provided training texts, with the possible aid of static resources such as lexicons.", "labels": [], "entities": [{"text": "learn- ing", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.7825556397438049}]}, {"text": "Extra tweets or additional annotated documents for training are permitted only in unconstrained mode.", "labels": [], "entities": []}, {"text": "The system we used to respond to this challenge is called senti.ue, and follows on from our previous work on Natural Language Processing (NLP) and Sentiment Analysis (SA).", "labels": [], "entities": [{"text": "Sentiment Analysis (SA)", "start_pos": 147, "end_pos": 170, "type": "TASK", "confidence": 0.8591752767562866}]}, {"text": "We developed work in automatic reputation assessment, using a Machine Learning (ML) based classifier for comments with impact on a particular target entity.", "labels": [], "entities": [{"text": "automatic reputation assessment", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6670708556969961}]}, {"text": "We also participated in the previous edition of SemEval SA task, where we have implemented the basis for the current system.", "labels": [], "entities": [{"text": "SemEval SA task", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.623138944307963}]}, {"text": "In last year's solution, we treated both subtasks using the same method (except the training set).", "labels": [], "entities": []}, {"text": "We have updated the method for subtask A, now considering also the text around the area to classify, by dedicating new features to those preceding and following tweet parts.", "labels": [], "entities": []}, {"text": "Text overall sentiment classification is the core objective of our system, and is performed, as before, with a supervised machine learning technique.", "labels": [], "entities": [{"text": "Text overall sentiment classification", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.822945162653923}]}, {"text": "For subtask B, we fixed some implementation issues in the previous version, and we went from 22 to 53 features, explained in Section 3.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: all systems: higher and average score", "labels": [], "entities": [{"text": "average score", "start_pos": 34, "end_pos": 47, "type": "METRIC", "confidence": 0.9383765757083893}]}, {"text": " Table 3: senti.ue precision in best mode", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9898870587348938}]}, {"text": " Table 4: senti.ue recall in best mode", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9906495213508606}]}]}