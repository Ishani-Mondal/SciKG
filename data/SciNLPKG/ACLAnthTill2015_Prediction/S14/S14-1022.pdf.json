{"title": [{"text": "Syntactic Transfer Patterns of German Particle Verbs and their Impact on Lexical Semantics", "labels": [], "entities": [{"text": "Syntactic Transfer Patterns of German Particle Verbs", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.8517167227608817}]}], "abstractContent": [{"text": "German particle verbs, like anblicken (to gaze at) combine abase verb (blicken) with a particle (an) to form a special kind of Multi Word Expression.", "labels": [], "entities": [{"text": "Multi Word Expression", "start_pos": 127, "end_pos": 148, "type": "TASK", "confidence": 0.682440996170044}]}, {"text": "Particle verbs may share the semantics of the base verb and the particle to a variable degree.", "labels": [], "entities": []}, {"text": "However, while syntactic subcate-gorization frames tend to be good predic-tor for the semantics of verbs in general (verbs that are similar in meaning also tend to have similar subcategorization frames and selectional preferences), there are regular changes in subcategorization frames by particle verbs with regard to the corresponding base verbs.", "labels": [], "entities": []}, {"text": "This paper demonstrates that the syntactic behavior of particle verbs and base verbs together (mod-eling regular changes in subcategorization frames by particle verbs and corresponding base verbs) and applying clustering techniques allows us to distinguish particle verb meaning and shows the tight connection between transfer patterns and the semantic classes of particle verbs.", "labels": [], "entities": []}], "introductionContent": [{"text": "In German, particle verbs (PVs), like anblicken in (1), area highly productive class.", "labels": [], "entities": []}, {"text": "PVs present challenges fora both theoretical analysis and their computational treatment.", "labels": [], "entities": []}, {"text": "One of the central problems is the prediction of their meaning from their constituent parts: the base verb (BV, e.g. blicken in (1)) and the particle (e.g. an).", "labels": [], "entities": [{"text": "prediction of their meaning", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.8652452081441879}]}, {"text": "Many PVs derive their meaning from the corresponding BVswith a varying degree of transparency.", "labels": [], "entities": []}, {"text": "It is often not clear, however, how to interpret the semantics of the particles and their contribution to the meaning of the PVs.", "labels": [], "entities": []}, {"text": "Since particles never occur isolated, without the context of the verb, it is difficult to assign them a lexical semantic entry on their own.", "labels": [], "entities": []}, {"text": "Even more, German particles area notoriously ambiguous word class.", "labels": [], "entities": []}, {"text": "( One way to approximate the meaning of particles is to group together the particle verbs which share the same particle into semantic groups (such as anblicken, anstarren, anschauen 'to stare/look at'), such that both the meaning of the PV and the meaning of the BV is similar in each group.", "labels": [], "entities": []}, {"text": "This allows us to make inferences like \"taking a BV from semantic group \u03b1 and particle \u03b2, we will derive a PV from semantic group \u03b4\".", "labels": [], "entities": []}, {"text": "Such groups can be established and they represent productive paradigms.", "labels": [], "entities": []}, {"text": "have shown in a generation experiment setup that subjects are able to associate a meaning to artificially created, previously unattested PVs and to construct example sentences for them.", "labels": [], "entities": []}, {"text": "Different subjects also agree to a large degree on the meaning they attribute to the newly formed lexical items.", "labels": [], "entities": []}, {"text": "But this approach also rises a series of questions, especially concerning the way in which such groups can be distinguished, both from a theoretical and a corpus-based perspective.", "labels": [], "entities": []}, {"text": "For example, which kinds of linguistic features allow us to discriminate such semantic classes?", "labels": [], "entities": []}, {"text": "In this paper we investigate the influence of syntax, which represents one of the possible feature sources.", "labels": [], "entities": []}, {"text": "Syn-tactic subcategorization frames tend to be good predictors for the semantics of verbs in general: verbs that are similar in meaning also tend to have similar subcategorization frames and selectional preferences.", "labels": [], "entities": []}, {"text": "But, as we will show below, PV-BV pairs tend to have a special behavior with respect to their subcategorization, even if their meanings are closely related.", "labels": [], "entities": []}, {"text": "Because we are interested in pairs of PVs and their BVs, we thus have to look at pairs of subcategorization preferences, and rely on the concept of syntactic transfer.", "labels": [], "entities": [{"text": "syntactic transfer", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.7258311212062836}]}, {"text": "We use syntactic transfer as a technical term here, which we define as regular changes in subcategorization frames by PVs and corresponding BVs, e.g., the incorporation or addition of complements of PVs in comparison to their).", "labels": [], "entities": [{"text": "syntactic transfer", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7512210607528687}]}, {"text": "We claim that the syntactic behavior of PVs and BVs together allows us to distinguish semantic classes.", "labels": [], "entities": []}, {"text": "A better understanding of the nature of the connection between syntactic transfer patterns and semantic classes maybe beneficial for both theoretical and computational linguistics.", "labels": [], "entities": []}, {"text": "On the theoretical side we can hope to find new arguments to guide and justify lexical semantic classifications.", "labels": [], "entities": [{"text": "lexical semantic classifications", "start_pos": 79, "end_pos": 111, "type": "TASK", "confidence": 0.6801522970199585}]}, {"text": "We may also shed light on what particles actually mean, a topic which is not trivial by itself.", "labels": [], "entities": []}, {"text": "In computational semantics, a better understanding of syntactic transfer patterns can potentially contribute to a better treatment of PVs in meaningrelated areas, such as machine translation and information retrieval.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.8163650333881378}, {"text": "information retrieval", "start_pos": 195, "end_pos": 216, "type": "TASK", "confidence": 0.7484853267669678}]}, {"text": "In sum, this paper makes the following contributions: \u2022 We show that the meaning of verb particles can be modeled as classes of pairs of PVs and their corresponding BVs, where both PVs and BVs in each class are closely related in meaning.", "labels": [], "entities": []}, {"text": "In addition, the PV-BV pairs in each class undergo the same syntactic transfers, i.e. the selectional preferences of PV-BV pairs within each class tend to be very similar, even if the subcategorization preferences maybe different between PVs and BVs.", "labels": [], "entities": []}, {"text": "\u2022 We show that automatic clustering can replicate a gold standard classification of PV-BV pairs to a large degree when clustering only relies on syntax and the gold standard reflects semantic regularities.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: In section 2 we describe the task and our goals.", "labels": [], "entities": []}, {"text": "Here we also define the term syntactic transfer pattern, which is central to our discussion.", "labels": [], "entities": [{"text": "syntactic transfer pattern", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.7756682634353638}]}, {"text": "Section 3 is dedicated to related work relevant for our study.", "labels": [], "entities": []}, {"text": "In section 4 we describe the experimental setup, while sections 5 and 6 present the experiment results and discuss them.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our feature vectors area combination of the feature vector for the BV and the feature vector for the PV of each PV-BV pair.", "labels": [], "entities": []}, {"text": "Since the length of each vector depends on the base frequency of each verb we need to apply a feature normalization: we simply reduce each feature to its unit vector of length 1.", "labels": [], "entities": []}, {"text": "Because the frequency ratio between BV and PV may vary strongly, we need to normalize PV vectors and BV vectors separately before they can be combined.", "labels": [], "entities": []}, {"text": "The vector combination for each PV-BV pair is done by simply adding the dimensions (and not the    dimension extensions) of the two vectors.", "labels": [], "entities": []}, {"text": "In this way, each subcategorization frame is represented separately for the BV and the PV.", "labels": [], "entities": []}, {"text": "For example, the vectors for the intransitive frame will be represented as BV:intransitive and PV:intransitive.", "labels": [], "entities": [{"text": "BV", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9510111808776855}]}, {"text": "We evaluated the clusterings in terms of Purity (, Rand Index and Adjusted Rand Index.", "labels": [], "entities": [{"text": "Purity", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9696568250656128}, {"text": "Rand Index", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.7358327805995941}, {"text": "Adjusted Rand Index", "start_pos": 66, "end_pos": 85, "type": "METRIC", "confidence": 0.9135162432988485}]}, {"text": "Purity is a measure with values between 0 and 1 which captures the purity of individual clusters in terms of the ratio between the number of elements of the majority class in each cluster and the total of elements in the cluster.", "labels": [], "entities": [{"text": "Purity", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.97975093126297}]}, {"text": "A perfect clustering will have a purity of 1.", "labels": [], "entities": []}, {"text": "What Purity does not capture is the amount of clusters over which each target class is distributed.", "labels": [], "entities": []}, {"text": "That means that also non-perfect clusters may achieve a Purity of 1 if there are more clusters than target classes.", "labels": [], "entities": [{"text": "Purity", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9989067316055298}]}, {"text": "As long as the number of clusters is constant, however, purity is a good and intuitive approximation to clustering evaluation.", "labels": [], "entities": [{"text": "purity", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9809735417366028}]}, {"text": "The Rand Index (RI) looks at pairs of elements and assesses whether they have been correctly placed in the same cluster (which is correct if they pertain to the same target class) or in different clusters (correct if they belong to different target classes).", "labels": [], "entities": [{"text": "Rand Index (RI)", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9737073183059692}]}, {"text": "RI is sensitive to the number of non-empty clusters and can capture both the quality of individual clusters and the amount to which elements of target categories have been grouped together.", "labels": [], "entities": [{"text": "RI", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.44097840785980225}]}, {"text": "RI looks as pair-wise decisions, which makes it also applicable to the human ratings described in section 4.1.", "labels": [], "entities": [{"text": "RI", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.6103719472885132}]}, {"text": "The Adjusted Rand Index (ARI) is aversion of RI which is corrected for chance.", "labels": [], "entities": [{"text": "Adjusted Rand Index (ARI)", "start_pos": 4, "end_pos": 29, "type": "METRIC", "confidence": 0.9063127934932709}, {"text": "RI", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.992387056350708}]}, {"text": "While RI has values between 0 and 1, ARI can have negative values; 1 still represents a perfect clustering.", "labels": [], "entities": [{"text": "ARI", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.5978972911834717}]}, {"text": "The Adjusted Rand Index (ARI) is aversion of RI which is corrected for chance.", "labels": [], "entities": [{"text": "Adjusted Rand Index (ARI)", "start_pos": 4, "end_pos": 29, "type": "METRIC", "confidence": 0.9063127934932709}, {"text": "RI", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.992387056350708}]}, {"text": "While RI has values between 0 and 1, ARI can have negative values; 1 still represents a perfect clustering.", "labels": [], "entities": [{"text": "ARI", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.5978972911834717}]}, {"text": "We evaluated the clustering of the verbs with the particles an and auf separately from each other, since we have to expect that there is a different set of semantic classes for each verb particle.", "labels": [], "entities": []}, {"text": "We also ran the same experiments for the gold standard as a whole (an+auf ), in order to test if we could find some tendencies across clusters.", "labels": [], "entities": []}, {"text": "We set the number of clusters equal to the number of target categories from the gold standard.", "labels": [], "entities": []}, {"text": "This gave us 5 clusters for both the an-set and the auf -set and 10 clusters for the classification of the whole gold standard.", "labels": [], "entities": []}, {"text": "Note that LSC is a soft clustering algorithm.", "labels": [], "entities": []}, {"text": "For the evaluation of LSC clusters with respect to purity and RI and ARI, a conversion to hard clustering must be done.", "labels": [], "entities": [{"text": "purity", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9854147434234619}, {"text": "RI", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.994754433631897}, {"text": "ARI", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9952717423439026}]}, {"text": "We did this conversion by simply applying a cutoff value for the output probabilities for cluster membership.", "labels": [], "entities": []}, {"text": "We tried out various cut-off levels and found that for the sets of an and auf PVs the value of 0.1 gave a good trade-off between coverage (the total number of elements retained in all clusters) and ARI (cf also below).", "labels": [], "entities": [{"text": "coverage", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9884557723999023}, {"text": "ARI", "start_pos": 198, "end_pos": 201, "type": "METRIC", "confidence": 0.9886983036994934}]}, {"text": "This value is also the one used in K\u00fchner and Schulte im.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement and comparison of the gold standard to the ratings of 6 human anno- tators (Fleiss' Kappa Scores).", "labels": [], "entities": [{"text": "Fleiss' Kappa Scores", "start_pos": 112, "end_pos": 132, "type": "DATASET", "confidence": 0.6290983160336813}]}, {"text": " Table 3: Comparison of the results from different clustering methods and feature configurations.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation with LSC using extracted selected features for different cutoff points (probabilities  of class membership) when creating hard clusters from soft clusters. (n classes refers to the number of  elements across target classes, n clust refers to the number of elements across hard clusters.)", "labels": [], "entities": []}]}