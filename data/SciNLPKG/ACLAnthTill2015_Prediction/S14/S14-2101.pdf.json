{"title": [{"text": "SNAP: A Multi-Stage XML-Pipeline for Aspect Based Sentiment Analysis", "labels": [], "entities": [{"text": "Aspect Based Sentiment Analysis", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.7288791835308075}]}], "abstractContent": [{"text": "This paper describes the SNAP system, which participated in Task 4 of SemEval-2014: Aspect Based Sentiment Analysis.", "labels": [], "entities": [{"text": "SNAP", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.964672863483429}, {"text": "Aspect Based Sentiment Analysis", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.6779564619064331}]}, {"text": "We use an XML-based pipeline that combines several independent components to perform each subtask.", "labels": [], "entities": []}, {"text": "Key resources used by the system are Bing Liu's sentiment lexicon, Stanford CoreNLP, RFTag-ger, several machine learning algorithms and WordNet.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.9320863485336304}, {"text": "RFTag-ger", "start_pos": 85, "end_pos": 94, "type": "DATASET", "confidence": 0.7722222208976746}, {"text": "WordNet", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.9627814888954163}]}, {"text": "SNAP achieved satisfactory results in the evaluation, placing in the top half of the field for most subtasks.", "labels": [], "entities": [{"text": "SNAP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7857615947723389}]}], "introductionContent": [{"text": "This paper describes the approach of the SemaNtic Analyis Project (SNAP) to Task 4 of SemEval-2014: Aspect Based Sentiment Analysis ().", "labels": [], "entities": [{"text": "Aspect Based Sentiment Analysis", "start_pos": 100, "end_pos": 131, "type": "TASK", "confidence": 0.6193152219057083}]}, {"text": "SNAP is a team of undergraduate students at the Corpus Linguistics Group, FAU Erlangen-N\u00fcrnberg, who carried out this work as part of a seminar in computational linguistics.", "labels": [], "entities": [{"text": "FAU Erlangen-N\u00fcrnberg", "start_pos": 74, "end_pos": 95, "type": "DATASET", "confidence": 0.9051597714424133}]}, {"text": "Task 4 was divided into the four subtasks Aspect term extraction (1), Aspect term polarity (2), Aspect category detection (3) and Aspect category polarity (4), which were evaluated in two phases (A: subtasks 1/3; B: subtasks 2/4).", "labels": [], "entities": [{"text": "Aspect term extraction", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.543996532758077}, {"text": "Aspect category detection", "start_pos": 96, "end_pos": 121, "type": "TASK", "confidence": 0.679838220278422}]}, {"text": "Subtasks 1 and 3 were carried out on two different datasets, one of laptop reviews and one of restaurant reviews.", "labels": [], "entities": []}, {"text": "Subtasks 2 and 4 only made use of the latter.: Ranking among constrained systems.", "labels": [], "entities": []}, {"text": "The developed system consists of one module per subtask, in addition to a general infrastructure and preprocessing module, All modules accept training and test data in the XML format specfied by the task organizers.", "labels": [], "entities": []}, {"text": "The modules can be combined into a pipeline, where each step adds new annotation corresponding to one of the four subtasks.", "labels": [], "entities": []}, {"text": "shows our ranking among all constrained systems (counting only the best run from each team), the score achieved by SNAP (accuracy or F-score, depending on subtask), and the score achieved by the best system in the respective subtask.", "labels": [], "entities": [{"text": "SNAP", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9772205948829651}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9932345747947693}, {"text": "F-score", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9873647093772888}]}, {"text": "Because of a preprocessing mistake that was only discovered after phase A of the evaluation had ended, results for subtasks 1 and 3 are significantly lower than the results achieved during development of the system.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we compare two approches to aspect term polarity detection.", "labels": [], "entities": [{"text": "aspect term polarity detection", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.6111774742603302}]}, {"text": "The first approach simply counts all positive and negative words in each sentence and then assigns a label based on which of the two counts is larger.", "labels": [], "entities": []}, {"text": "It does not make use of machine learning techniques and its accuracy is only about 54%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9997164607048035}]}, {"text": "Results improve significantly with supervised machine learning based on the feature sets described above.", "labels": [], "entities": []}, {"text": "We experimented with different classifiers (Maximum Entropy, Linear SVM and SVMs with RBF kernel) and various subsets of features.", "labels": [], "entities": []}, {"text": "By default, we worked on the level of single opinion words that express a positive or negative polarity (sg).", "labels": [], "entities": []}, {"text": "We added the following features in different combinations: an extended list of opinion words (ex) obtained from a distribution semantic model, based on nearest neighbours of known opinion words (; potential misspellings of know opinion words, within a maximal Levenshtein distance of 1 (lv); word combinations and fixed phrases (ml) containing up to 3 words (e.g., good mannered, put forth, tried and true, up and down); and the sums of positive and negative opinion words in the whole sentence (st).", "labels": [], "entities": []}, {"text": "The best results for the laptops data were achieved with a Maximum Entropy classifier, excluding misspellings (lv) and word combinations (ml); the corresponding line in is highlighted in bold font.", "labels": [], "entities": []}, {"text": "Even though MaxEnt achieved the best results during development, we decided to use SVM with a RBF kernel for the test set, assuming that it would be able to exploit interdependencies between features.", "labels": [], "entities": []}, {"text": "The accuracy achieved by the submitted system is highlighted in italics in the table.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999446451663971}]}, {"text": "The training test data provided for restaurants and laptops categories were split equally into two sets where the first set (first half) was used for training a model and the second set was used for the test and evaluation stages.", "labels": [], "entities": []}, {"text": "Experiments on the restaurants data produced similar results.: Results for laptops category on train set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy of different aspect term taggers.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9883909225463867}]}]}