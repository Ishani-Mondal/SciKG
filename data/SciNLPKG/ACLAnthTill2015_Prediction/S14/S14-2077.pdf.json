{"title": [{"text": "NRC-Canada-2014: Recent Improvements in the Sentiment Analysis of Tweets", "labels": [], "entities": [{"text": "NRC-Canada-2014", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9541189670562744}, {"text": "Sentiment Analysis of Tweets", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.9341939687728882}]}], "abstractContent": [{"text": "This paper describes state-of-the-art statistical systems for automatic sentiment analysis of tweets.", "labels": [], "entities": [{"text": "automatic sentiment analysis of tweets", "start_pos": 62, "end_pos": 100, "type": "TASK", "confidence": 0.7570834577083587}]}, {"text": "Ina Semeval-2014 shared task (Task 9), our submissions obtained highest scores in the term-level sentiment classification subtask on both the 2013 and 2014 tweets test sets.", "labels": [], "entities": [{"text": "term-level sentiment classification subtask", "start_pos": 86, "end_pos": 129, "type": "TASK", "confidence": 0.6645641624927521}]}, {"text": "In the message-level sentiment classification task, our submissions obtained highest scores on the Live-Journal blog posts test set, sarcastic tweets test set, and the 2013 SMS test set.", "labels": [], "entities": [{"text": "message-level sentiment classification task", "start_pos": 7, "end_pos": 50, "type": "TASK", "confidence": 0.8497669845819473}, {"text": "Live-Journal blog posts test set", "start_pos": 99, "end_pos": 131, "type": "DATASET", "confidence": 0.8949469685554504}, {"text": "2013 SMS test set", "start_pos": 168, "end_pos": 185, "type": "DATASET", "confidence": 0.8768907636404037}]}, {"text": "These systems build on our SemEval-2013 sentiment analysis systems (Mohammad et al., 2013) which ranked first in both the term-and message-level subtasks in 2013.", "labels": [], "entities": [{"text": "SemEval-2013 sentiment analysis", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.8076275189717611}]}, {"text": "Key improvements over the 2013 systems are in the handling of negation.", "labels": [], "entities": [{"text": "handling of negation", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6912105878194174}]}, {"text": "We create separate tweet-specific sentiment lexicons for terms in affirmative contexts and in negated contexts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatically detecting sentiment of tweets (and other microblog posts) has attracted extensive interest from both the academia and industry.", "labels": [], "entities": [{"text": "detecting sentiment of tweets", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.8650711476802826}]}, {"text": "The Conference on Semantic Evaluation Exercises (SemEval) organizes a shared task on the sentiment analysis of tweets with two subtasks.", "labels": [], "entities": [{"text": "sentiment analysis of tweets", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.9089434146881104}]}, {"text": "In the message-level task, the participating systems are to identify whether a tweet as a whole expresses positive, negative, or neutral sentiment.", "labels": [], "entities": []}, {"text": "In the term-level task, the objective is to determine the sentiment of a marked target term (a single word or a multi-word expression) within the tweet.", "labels": [], "entities": []}, {"text": "Our submissions stood first in both subtasks in 2013.", "labels": [], "entities": []}, {"text": "This paper describes improvements over that sys-  tem and the subsequent submissions to the 2014 shared task ().", "labels": [], "entities": []}, {"text": "The training data for the SemEval-2014 shared task is same as that of,000 tweets).", "labels": [], "entities": [{"text": "SemEval-2014 shared task", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.7897273302078247}]}, {"text": "The 2014 test set has five subcategories: a tweet set provided newly in 2014 (Twt14), the tweet set used for testing in the 2013 shared task (Twt13), a set of tweets that are sarcastic (Sarc14), a set of sentences from the blogging website LiveJournal (LvJn14), and the set of SMS messages used for testing in the 2013 shared task (SMS13).", "labels": [], "entities": []}, {"text": "Instances from these categories were interspersed in the provided test set.", "labels": [], "entities": []}, {"text": "The participants were not told about the source of the individual messages.", "labels": [], "entities": []}, {"text": "The objective was to determine how well a system trained on tweets generalizes to texts from other domains.", "labels": [], "entities": []}, {"text": "Our submissions to SemEval-2014 Task 9, ranked first in five out of the ten subtask-dataset combinations.", "labels": [], "entities": [{"text": "SemEval-2014 Task 9", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7799413601557413}]}, {"text": "In the other evaluation sets as well, our submissions performed competitively.", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "As we will show, automatically generated tweet-specific lexicons were especially helpful in all subtask-dataset combinations.", "labels": [], "entities": []}, {"text": "The results also show that even though our models are trained only on tweets, they generalize well to data from other domains.", "labels": [], "entities": []}, {"text": "Our systems are based on supervised SVMs and a number of surface-form, semantic, and sentiment features.", "labels": [], "entities": []}, {"text": "The major improvement in our 2014 system over the 2013 system is in the way it handles negation.", "labels": [], "entities": [{"text": "negation", "start_pos": 87, "end_pos": 95, "type": "TASK", "confidence": 0.9813339114189148}]}, {"text": "define negation to be \"a grammatical category that allows the changing of the truth value of a proposition\".", "labels": [], "entities": []}, {"text": "Negation is often expressed through the use of negative signals or negators, words such as isnt and never, and it can significantly affect the sentiment of its scope.", "labels": [], "entities": [{"text": "Negation", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9600886106491089}]}, {"text": "We create separate tweetspecific sentiment lexicons for terms in affirmative contexts and in negated contexts.", "labels": [], "entities": []}, {"text": "That is, we automatically determine the average sentiment of a term when occurring in an affirmative context, and separately the average sentiment of a term when occurring in a negated context.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Overall rank of NRC-Canada sentiment  analysis models in Semeval-2014 Task 9 under the  constrained condition. The rows are five evalua- tion datasets and the columns are the two subtasks.", "labels": [], "entities": [{"text": "NRC-Canada sentiment  analysis", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.7096392313639323}]}, {"text": " Table 2: Overall performance of the NRC-Canada  sentiment analysis systems.", "labels": [], "entities": [{"text": "NRC-Canada  sentiment analysis", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.6967115998268127}]}, {"text": " Table 3: Term-level Task: The macro-averaged F-scores obtained on the 5 test sets with one of the feature  groups removed.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9181471467018127}]}]}