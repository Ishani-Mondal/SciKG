{"title": [{"text": "JOINT_FORCES: Unite Competing Sentiment Classifiers with Random Forest", "labels": [], "entities": [{"text": "FORCES", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.6732673645019531}, {"text": "Unite Competing Sentiment Classifiers", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.7217444330453873}]}], "abstractContent": [{"text": "In this paper, we describe how we created a meta-classifier to detect the message level sentiment of tweets.", "labels": [], "entities": []}, {"text": "We participated in SemEval-2014 Task 9B by combining the results of several existing classifiers using a random forest.", "labels": [], "entities": [{"text": "SemEval-2014 Task 9B", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.8309857249259949}]}, {"text": "The results of 5 other teams from the competition as well as from 7 general-purpose commercial classifiers were used to train the algorithm.", "labels": [], "entities": []}, {"text": "This way, we were able to get a boost of up to 3.24 F 1 score points.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9188538988431295}]}], "introductionContent": [{"text": "The interest in sentiment analysis grows as publicly available text content grows.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.9590911567211151}]}, {"text": "As one of the most used social media platforms, Twitter provides its users a unique way of expressing themselves.", "labels": [], "entities": []}, {"text": "Thus, sentiment analysis of tweets has become a hot research topic among academia and industry.", "labels": [], "entities": [{"text": "sentiment analysis of tweets", "start_pos": 6, "end_pos": 34, "type": "TASK", "confidence": 0.9408843219280243}]}, {"text": "In this paper, we describe our approach of combining multiple sentiment classifiers into a metaclassifier.", "labels": [], "entities": []}, {"text": "The introduced system participated in SemEval-2014 Task 9: \"Sentiment Analysis in Twitter, Subtask-B Message Polarity Classification\" ().", "labels": [], "entities": [{"text": "SemEval-2014 Task 9", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8156935969988505}, {"text": "Sentiment Analysis", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.9429068267345428}, {"text": "Subtask-B Message Polarity Classification", "start_pos": 91, "end_pos": 132, "type": "TASK", "confidence": 0.6521321162581444}]}, {"text": "The goal was to classify a tweet on the message level using the three classes positive, negative, and neutral.", "labels": [], "entities": []}, {"text": "The performance is measured using the macroaveraged F 1 score of the positive and negative classes which is simply named \"F 1 score\" throughout the paper.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9421959320704142}, {"text": "F 1 score", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9767818649609884}]}, {"text": "An almost identical task was already run in).", "labels": [], "entities": []}, {"text": "The tweets for training and development were only provided as tweet ids.", "labels": [], "entities": []}, {"text": "A fraction (10-15%) of the tweets was no longer available on twitter, which makes the results of the competition not fully comparable.", "labels": [], "entities": []}, {"text": "For testing, in addition to last year's data (tweets and SMS) new tweets and data from a surprise domain (LiveJournal) were provided.", "labels": [], "entities": []}, {"text": "An overview of the provided data is shown in.", "labels": [], "entities": []}, {"text": "Using additional manually labelled data for training the algorithm was not allowed fora \"constrained\" submission.", "labels": [], "entities": []}, {"text": "Submissions using additional data for training were marked as \"unconstrained\".", "labels": [], "entities": []}, {"text": "Lymbix (c_lym), MLAnalyzer 1 (c_mla), Semantria (c_sem), Sentigem (c_snt), Syttle (c_sky), Text-Processing.com (c_txp), and Webknox (c_web).", "labels": [], "entities": []}, {"text": "Subsystems c_txp and c_web are machine learning-based, c_sky is rule-based, and m_mla is a mix (other tools unknown).", "labels": [], "entities": []}, {"text": "All subsystems were designed to handle tweets and further text types.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our submission included a subset of all classifiers including unconstrained ones, leading to an unconstrained submission.", "labels": [], "entities": []}, {"text": "The 2014 winning team obtained an F 1 score of 70.96 on the Twitter2014 test set.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9908698399861654}, {"text": "Twitter2014 test set", "start_pos": 60, "end_pos": 80, "type": "DATASET", "confidence": 0.9733794728914896}]}, {"text": "Our approach was ranked on the 12th place out of the 50 participating submissions, with an F 1 score of 66.79.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9909745256106058}]}, {"text": "Our further rankings were 12th on the LiveJournal data, 12th on the SMS data, 12th on Twitter-2013, and 26th on Twitter Sarcasm.", "labels": [], "entities": [{"text": "LiveJournal data", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.981652706861496}, {"text": "SMS data", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.7449983656406403}, {"text": "Twitter-2013", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.9463263750076294}, {"text": "Twitter Sarcasm", "start_pos": 112, "end_pos": 127, "type": "DATASET", "confidence": 0.9166818857192993}]}, {"text": "Although our meta-classifier did not reach atop position in the competition, we were able to beat even the best single subsystem it was based on for almost all test sets (except sarcasm).", "labels": [], "entities": []}, {"text": "In previous research we showed that same behaviour on different systems and data sets . This shows that also other systems from the competition, even best ones, probably can be improved using our approach.", "labels": [], "entities": []}, {"text": "There exist three obvious selections of subsystems for our meta-classifier: all subsystems, only scientific subsystems, and only commercial subsystems (called All_Subsystems, All_Scientific, and All_Commercial, respectively).", "labels": [], "entities": []}, {"text": "shows performance of these selections of subsystems on the data sets.", "labels": [], "entities": []}, {"text": "For comparison, the table shows also the performance of the overall best individual subsystem in the first row.", "labels": [], "entities": []}, {"text": "It turns out that All_Subsystems is almost always better than the best individual subsystem, while the other two meta-classifiers are inferior.", "labels": [], "entities": []}, {"text": "We performed a systematic evaluation on how the performance depends on the choice of a particular selection of individual subsystems.", "labels": [], "entities": []}, {"text": "This resembles feature selection, which is a common task in machine learning, and As a general trend we see that the performance increases with the number of classifiers; however, there exist certain subsets which perform better than using all available classifiers.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7129029631614685}]}, {"text": "In, we marked for each number of subsystems the highest OOB-F 1 -Score on the Dev set by a diamond.", "labels": [], "entities": [{"text": "OOB-F 1 -Score", "start_pos": 56, "end_pos": 70, "type": "METRIC", "confidence": 0.9795531183481216}]}, {"text": "In addition, the subset with the overall highest OOB-F 1 -Score, consisting of 7 classifiers, is displayed as a filled diamond.", "labels": [], "entities": [{"text": "OOB-F 1 -Score", "start_pos": 49, "end_pos": 63, "type": "METRIC", "confidence": 0.9736189097166061}]}, {"text": "We also evaluated the performance of these \"best\" subsets on other unseen test data.", "labels": [], "entities": []}, {"text": "In, we show the results of the test set Twitter2014.", "labels": [], "entities": [{"text": "Twitter2014", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.8579937815666199}]}, {"text": "The scores for the very subsets marked in are displayed in the same way here.", "labels": [], "entities": []}, {"text": "For comparison, we marked the performance of the system with all classifiers by a straight line.", "labels": [], "entities": []}, {"text": "We find that all subsets that are \"best\" on the Dev set perform very well on the Twitter2014 set.", "labels": [], "entities": [{"text": "Dev set", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9016869962215424}, {"text": "Twitter2014 set", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.9228838682174683}]}, {"text": "In fact, some even beat the system with all classifiers.", "labels": [], "entities": []}, {"text": "Similar behaviour can be observed for Twitter2013 and LiveJournal2014 (data not shown), while All_Subsets yields significantly superior results on SMS2013 (see).", "labels": [], "entities": [{"text": "Twitter2013", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.9452573657035828}, {"text": "SMS2013", "start_pos": 147, "end_pos": 154, "type": "DATASET", "confidence": 0.9309651851654053}]}, {"text": "No conclusive observation is possible for Sarcasm2014 (data not shown).", "labels": [], "entities": [{"text": "Sarcasm2014", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.6371141672134399}]}, {"text": "To elucidate on the question whether to use a subset with the highest OOB-F 1 on the Dev set (called Max_OOB_Subset) or to use all available classifiers, we show in: Performance (in F1 score) of meta-classifiers with different subsystems.", "labels": [], "entities": [{"text": "OOB-F 1", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9856202006340027}, {"text": "F1 score)", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9643142223358154}]}, {"text": "The subset used in our submission is composed of s_gez, s_jag, s_mar, s_fil, s_gun, c_sma, c_sky, c_snt.", "labels": [], "entities": []}, {"text": "\"Max_OOB_Subset\" is composed of s_jag, s_mar, s_gun, c_lym, c_sma, c_sky, c_txp.", "labels": [], "entities": [{"text": "OOB", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.7340185642242432}]}, {"text": "Bold shows best result per data set.", "labels": [], "entities": []}, {"text": "The first row shows results of the best individual subsystem. that All_Systems generalizes better to foreign types of data, while Max_OOB_Subset performs well on similar data (in this case, tweets).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of Documents we were able to  download for Training, Development and Test- ing.", "labels": [], "entities": [{"text": "Test- ing", "start_pos": 86, "end_pos": 95, "type": "TASK", "confidence": 0.6629104415575663}]}, {"text": " Table 2: F 1 scores of the individual systems.  Bold shows the best commercial or scientific  system per data set; grey cells indicates the over- all maximum.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9735698103904724}]}, {"text": " Table 3: Performance (in F1 score) of meta-classifiers with different subsystems. The subset used in  our submission is composed of s_gez, s_jag, s_mar, s_fil, s_gun, c_sma, c_sky, c_snt.  \"Max_OOB_Subset\" is composed of s_jag, s_mar, s_gun, c_lym, c_sma, c_sky, c_txp. Bold shows  best result per data set. The first row shows results of the best individual subsystem.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9773578643798828}]}]}