{"title": [{"text": "Sense and Similarity: A Study of Sense-level Similarity Measures", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we investigate the difference between word and sense similarity measures and present means to convert a state-of-the-art word similarity measure into a sense similarity measure.", "labels": [], "entities": []}, {"text": "In order to evaluate the new measure, we create a special sense similarity dataset and re-rate an existing word similarity dataset using two different sense inventories from WordNet and Wikipedia.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 174, "end_pos": 181, "type": "DATASET", "confidence": 0.9468749165534973}]}, {"text": "We discover that word-level measures were notable to differentiate between different senses of one word, while sense-level measures actually increase correlation when shifting to sense similarities.", "labels": [], "entities": []}, {"text": "Sense-level similarity measures improve when evaluated with a re-rated sense-aware gold standard, while correlation with word-level similarity measures decreases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Measuring similarity between words is a very important task within NLP with applications in tasks such as word sense disambiguation, information retrieval, and question answering.", "labels": [], "entities": [{"text": "Measuring similarity between words", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8974231630563736}, {"text": "word sense disambiguation", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.7173276940981547}, {"text": "information retrieval", "start_pos": 133, "end_pos": 154, "type": "TASK", "confidence": 0.8101159334182739}, {"text": "question answering", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.9131514132022858}]}, {"text": "However, most of the existing approaches compute similarity on the word-level instead of the sense-level.", "labels": [], "entities": []}, {"text": "Consequently, most evaluation datasets have so far been annotated on the word level, which is problematic as annotators might not know some infrequent senses and are influenced by the more probable senses.", "labels": [], "entities": []}, {"text": "In this paper, we provide evidence that this process heavily influences the annotation process.", "labels": [], "entities": []}, {"text": "For example, when people are presented the word pair jaguar -gamepad only few people know that Jaguar Gamepad Zoo .0070 .0016 .0000 Figure 1: Similarity between words.", "labels": [], "entities": [{"text": "Jaguar Gamepad Zoo", "start_pos": 95, "end_pos": 113, "type": "DATASET", "confidence": 0.9519550800323486}]}, {"text": "jaguar is also the name of an Atari game console.", "labels": [], "entities": []}, {"text": "People rather know the more common senses of jaguar, i.e. the car brand or the animal.", "labels": [], "entities": []}, {"text": "Thus, the word pair receives a low similarity score, while computational measures are not so easily fooled by popular senses.", "labels": [], "entities": []}, {"text": "It is thus likely that existing evaluation datasets give a wrong picture of the true performance of similarity measures.", "labels": [], "entities": []}, {"text": "Thus, in this paper we investigate whether similarity should be measured on the sense level.", "labels": [], "entities": [{"text": "similarity", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9764021635055542}]}, {"text": "We analyze state-of-the-art methods and describe how the word-based Explicit Semantic Analysis (ESA) measure ( can be transformed into a sense-level measure.", "labels": [], "entities": []}, {"text": "We create a sense similarity dataset, where senses are clearly defined and evaluate similarity measures with this novel dataset.", "labels": [], "entities": []}, {"text": "We also re-annotate an existing word-level dataset on the sense level in order to study the impact of sense-level computation of similarity.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we discuss how an evaluation dataset should be constructed in order to correctly asses the similarity of two senses.", "labels": [], "entities": []}, {"text": "Typically, evaluation datasets for word similarity are constructed by letting annotators rate the similarity between both words without specifying any senses for these words.", "labels": [], "entities": []}, {"text": "It is common understanding that annotators judge the similarity of the combination of senses with the highest similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9599660634994507}]}, {"text": "We investigate this hypothesis by constructing anew dataset consisting of 105 ambiguous word pairs.", "labels": [], "entities": []}, {"text": "Word pairs are constructed by adding one word with two clearly distinct senses and a second word, which has a high similarity to only one of the senses.", "labels": [], "entities": []}, {"text": "We first ask two annotators to rate the word pairs on a scale from 0 (not similar at all) to 4 (almost identical).", "labels": [], "entities": []}, {"text": "In the second round, we ask the same annotators to rate 277 sense 8 pairs for these word pairs using the same scale.", "labels": [], "entities": []}, {"text": "The final dataset thus consists of two levels: (i) word similarity ratings and (ii) sense similarity ratings.", "labels": [], "entities": [{"text": "sense similarity ratings", "start_pos": 84, "end_pos": 108, "type": "METRIC", "confidence": 0.772554079691569}]}, {"text": "The gold ratings are the averaged ratings of both annotators, resulting in an agreement 9 of .510 (Spearman: .598) for word ratings and .792 (Spearman: .806) for sense ratings.", "labels": [], "entities": []}, {"text": "shows ratings of both annotators for two word pairs and ratings for all sense combinations.", "labels": [], "entities": []}, {"text": "In the given example, the word bass has the senses of the fish, the instrument, and the sound.", "labels": [], "entities": []}, {"text": "Annotators compare the words and senses to the words Fish and Horn, which appear only in one sense (most frequent sense) in the dataset.", "labels": [], "entities": []}, {"text": "The annotators' rankings contradict the assumption that the word similarity equals the similarity of the highest sense.", "labels": [], "entities": []}, {"text": "Instead, the highest sense similarity rating is higher than the word similarity rating.", "labels": [], "entities": [{"text": "sense similarity rating", "start_pos": 21, "end_pos": 44, "type": "METRIC", "confidence": 0.7817284464836121}, {"text": "word similarity rating", "start_pos": 64, "end_pos": 86, "type": "METRIC", "confidence": 0.6830348571141561}]}, {"text": "This maybe caused-among others-by two effects: (i) the correct sense is not known or not recalled, or (ii) the annotators (unconsciously) adjust their ratings to the probability of the sense.", "labels": [], "entities": []}, {"text": "Although, the annotation manual stated that Wikipedia (the source of the senses) could be used to get informed about senses and that any sense for the words can be selected, we see both effects in the annotators' ratings.", "labels": [], "entities": []}, {"text": "Both annotators rated the similarity between Bass and Fish as very low (1 and 2).", "labels": [], "entities": [{"text": "similarity", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9883643388748169}]}, {"text": "However, when asked to rate the similarity between the sense Bass (Fish) and Fish, both annotators rated the similarity as high (4).", "labels": [], "entities": []}, {"text": "Accordingly, for the word pair Bass and Horn, word similarity is low (1) while the highest sense frequency is medium to high (3 and 4).", "labels": [], "entities": [{"text": "word similarity", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.5771077871322632}]}, {"text": "Ina second experiment, we evaluate how well sense-based measures can decide, which one of   two sense pairs for one word pair have a higher similarity.", "labels": [], "entities": []}, {"text": "We thus create for every word pair all possible sense pairs and count cases where one measure correctly decides, which is the sense pair with a higher similarity.", "labels": [], "entities": []}, {"text": "shows evaluation results based on a minimal difference between two sense pairs.", "labels": [], "entities": []}, {"text": "We removed all sense pairs with a lower difference of their gold similarity.", "labels": [], "entities": [{"text": "gold similarity", "start_pos": 60, "end_pos": 75, "type": "METRIC", "confidence": 0.7933921813964844}]}, {"text": "Column #pairs gives the number of remaining sense pairs.", "labels": [], "entities": []}, {"text": "If a measure classifies two sense pairs wrongly, it may either be because it rated the sense pairs with an equal similarity or because it reversed the order.", "labels": [], "entities": []}, {"text": "Results show that accuracy increases with increasing minimum difference between sense pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9995310306549072}]}, {"text": "Overall, accuracy for this task is high (between .70 and .83), which shows that all the measures can discriminate sense pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9996898174285889}]}, {"text": "WLM (out) performs best for most cases with a difference inaccuracy of up to .06.", "labels": [], "entities": [{"text": "WLM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9324009418487549}, {"text": "difference inaccuracy", "start_pos": 46, "end_pos": 67, "type": "METRIC", "confidence": 0.9497101008892059}]}, {"text": "When comparing these results to results from, we see that correlation does not imply accurate discrimination of sense pairs.", "labels": [], "entities": []}, {"text": "Although, ESA on senses has the highest correlation to human ratings, it is outperformed by WLM (out) on the task of discriminating two sense pairs.", "labels": [], "entities": [{"text": "ESA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.849541425704956}, {"text": "WLM", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.7629355788230896}]}, {"text": "We see that results are not stable across both evaluation For one word pair with two senses for one word, there are two possible sense pairs.", "labels": [], "entities": []}, {"text": "Three senses result in three sense pairs.", "labels": [], "entities": []}, {"text": "In previous annotation studies, human annotators could take sense weights into account when judging the similarity of word pairs.", "labels": [], "entities": []}, {"text": "Additionally, some senses might not be known by annotators and, thus receive a lower rating.", "labels": [], "entities": []}, {"text": "We minimize these effects by asking annotators to select the best sense fora word based on a short summary of the corresponding sense.", "labels": [], "entities": []}, {"text": "To mimic this process, we created an annotation tool (see, for which an annotator first selects senses for both words, which have the highest similarity.", "labels": [], "entities": []}, {"text": "Then the annotator ranks the similarity of these sense pairs based on the complete sense definition.", "labels": [], "entities": []}, {"text": "A single word without any context cannot be disambiguated properly.", "labels": [], "entities": []}, {"text": "However, when word pairs are given, annotators first select senses based on the second word, e.g. if the word pair is Jaguar and Zoo, an annotator will select the wild animal for Jaguar.", "labels": [], "entities": []}, {"text": "After disambiguating, an annotator assigns a similarity score based on both selected senses.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 45, "end_pos": 61, "type": "METRIC", "confidence": 0.9765595197677612}]}, {"text": "To facilitate this process, a definition of each possible sense is shown.", "labels": [], "entities": []}, {"text": "As in the previous experiment, similarity is annotated on a five-point-scale from 0 to 4.", "labels": [], "entities": [{"text": "similarity", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9896829128265381}]}, {"text": "Although, we ask annotators to select senses for word pairs, we retrieve only one similarity rating for each word pair, which is the sense combination with the highest similarity.", "labels": [], "entities": []}, {"text": "No sense inventory To compare our results with the original dataset from, we asked annotators to rate similarity of word pairs without any given sense repository, i.e. comparing words directly.", "labels": [], "entities": []}, {"text": "The annotators reached an agreement of .73.", "labels": [], "entities": []}, {"text": "The resulting gold standard has a high correlation with the original dataset.", "labels": [], "entities": []}, {"text": "This is inline with our expectations and previous work that similarity ratings are stable across time).", "labels": [], "entities": []}, {"text": "Wikipedia sense inventory We now use the full functionality of our annotation tool and ask annotators to first, select senses for each word and second, rate the similarity.", "labels": [], "entities": [{"text": "Wikipedia sense inventory", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.8384549419085184}]}, {"text": "Possible senses and definitions for these senses are extracted from Wikipedia.", "labels": [], "entities": []}, {"text": "The same three annotators reached an agreement of .66.", "labels": [], "entities": []}, {"text": "The correlation to the original dataset is lower than for the re-rating (.881 Spearman, .896 Pearson).", "labels": [], "entities": [{"text": "re-rating", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9848806858062744}, {"text": "Pearson", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.8586311936378479}]}, {"text": "This effect is due to many entities in Wikipedia, which annotators would typically not know.", "labels": [], "entities": []}, {"text": "Two annotators rated the word pair graveyard-madhouse with a rather high similarity because both are names of music bands (still no very high similarity because one is a rock and the other a jazz band).", "labels": [], "entities": []}, {"text": "WordNet sense inventory Similar to the previous experiment, we list possible senses for each word from a sense inventory.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9086848497390747}]}, {"text": "In this experiment, we use WordNet senses, thus, not using any named entity.", "labels": [], "entities": []}, {"text": "The annotators reached an agreement of .73 and the resulting gold standard has a high correlation with the original dataset (.917 Spearman and .928 Pearson).", "labels": [], "entities": []}, {"text": "shows average annotator ratings in comparison to similarity judgments in the original dataset.", "labels": [], "entities": []}, {"text": "All re-rating studies follow the general tendency of having higher annotator judgments for similar pairs.", "labels": [], "entities": []}, {"text": "However, there is a strong fluctuation in the mid-similarity area (1 to 3).", "labels": [], "entities": []}, {"text": "This is due to fewer word pairs with such a similarity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Term-document-matrix for frequencies in  a corpus if words are used as terms", "labels": [], "entities": []}, {"text": " Table 3: Examples of ratings for two word pairs and all sense combinations with the highest ratings  marked bold", "labels": [], "entities": []}, {"text": " Table 4: Correlation of similarity measures with a human gold standard of ambiguous word pairs.", "labels": [], "entities": []}, {"text": " Table 5: Pair-wise comparison of measures: Results for ESA on senses (language model) and ESA on  senses (cosine) do not differ", "labels": [], "entities": [{"text": "ESA", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.6910573840141296}, {"text": "ESA", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.8655006289482117}]}, {"text": " Table 6: Correlation of similarity measures with a human gold standard on the word pairs by", "labels": [], "entities": []}]}