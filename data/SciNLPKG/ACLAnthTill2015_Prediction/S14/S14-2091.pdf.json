{"title": [{"text": "SAP-RI: Twitter Sentiment Analysis in Two Days", "labels": [], "entities": [{"text": "Twitter Sentiment Analysis", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.6754060387611389}]}], "abstractContent": [{"text": "We describe the submission of the SAP Research & Innovation team to the Se-mEval 2014 Task 9: Sentiment Analysis in Twitter.", "labels": [], "entities": [{"text": "Se-mEval 2014 Task 9", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.5442237332463264}, {"text": "Sentiment Analysis", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.9169766306877136}]}, {"text": "We challenged ourselves to develop a competitive sentiment analysis system within a very limited time frame.", "labels": [], "entities": [{"text": "competitive sentiment analysis", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.6672220925490061}]}, {"text": "Our submission was developed in less than two days and achieved an F 1 score of 77.26% for contextual polarity disambiguation and 55.47% for message polarity classification, which shows that rapid prototyping of sentiment analysis systems with reasonable accuracy is possible.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9924959341684977}, {"text": "contextual polarity disambiguation", "start_pos": 91, "end_pos": 125, "type": "TASK", "confidence": 0.7418012718359629}, {"text": "message polarity classification", "start_pos": 141, "end_pos": 172, "type": "TASK", "confidence": 0.8391567468643188}, {"text": "sentiment analysis", "start_pos": 212, "end_pos": 230, "type": "TASK", "confidence": 0.882487028837204}, {"text": "accuracy", "start_pos": 255, "end_pos": 263, "type": "METRIC", "confidence": 0.9800935387611389}]}], "introductionContent": [{"text": "Microblogging platforms and social networks have become increasingly popular for expressing opinions on a wide range of topics, hence making them valuable and ever-growing logs of public sentiment.", "labels": [], "entities": []}, {"text": "This has motivated the development of automatic natural language processing (NLP) methods to analyse the sentiment expressed in these short, informal messages (.", "labels": [], "entities": []}, {"text": "In particular, the study of sentiment and opinions in messages from the Twitter microblogging platform has attracted a lot of interest).", "labels": [], "entities": []}, {"text": "However, comparative evaluations of sentiment analysis of Twitter messages have previously been hindered by the lack of a large benchmark data set.", "labels": [], "entities": [{"text": "sentiment analysis of Twitter messages", "start_pos": 36, "end_pos": 74, "type": "TASK", "confidence": 0.9201013684272766}]}, {"text": "The goal of the SemEval 2013 task 2: Sentiment Analysis in Twitter () * The work was done during an internship at SAP.", "labels": [], "entities": [{"text": "SemEval 2013 task", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.8890064557393392}, {"text": "Sentiment Analysis", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.930322915315628}, {"text": "SAP", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.909305214881897}]}, {"text": "This work is licenced under a Creative Commons Attribution 4.0 International License.", "labels": [], "entities": []}, {"text": "Page numbers and proceedings footer are added by the organizers.", "labels": [], "entities": []}, {"text": "License details: http: //creativecommons.org/licenses/by/4.0/ and this year's continuation in the SemEval 2014 task 9: Sentiment Analysis in Twitter () is to close this gap by hosting a shared task competition which provided a large corpus of Twitter messages which are annotated with sentiment polarity labels.", "labels": [], "entities": [{"text": "SemEval 2014 task 9", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.824304923415184}, {"text": "Sentiment Analysis", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.9049635231494904}]}, {"text": "The task consists of two subtasks: in subtask A contextual polarity disambiguation, participants need to predict the polarity of a given word or phrase in the context of a tweet message, in subtask B message polarity classification, participants need to predict the dominating sentiment of the complete message.", "labels": [], "entities": [{"text": "contextual polarity disambiguation", "start_pos": 48, "end_pos": 82, "type": "TASK", "confidence": 0.6989496151606241}, {"text": "message polarity classification", "start_pos": 200, "end_pos": 231, "type": "TASK", "confidence": 0.7068578600883484}]}, {"text": "Both tasks consider sentiment analysis to be a three-way classification problem between positive, negative, and neutral sentiment.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.9297530055046082}]}, {"text": "In this paper, we describe the submission of the SAP-RI team to the SemEval 2014 task 9.", "labels": [], "entities": [{"text": "SemEval 2014 task 9", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.660018801689148}]}, {"text": "We challenged ourselves to develop a competitive sentiment analysis system within a very limited time frame.", "labels": [], "entities": [{"text": "competitive sentiment analysis", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.6672220925490061}]}, {"text": "The complete system was implemented within only two days.", "labels": [], "entities": []}, {"text": "Our system is based on supervised classification with support vector machines with lexical and dictionary-based features.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.7119871079921722}]}, {"text": "Our system achieved an F 1 score of 77.26% for contextual polarity disambiguation and 55.47% for message polarity classification.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9918572703997294}, {"text": "contextual polarity disambiguation", "start_pos": 47, "end_pos": 81, "type": "TASK", "confidence": 0.7581104834874471}, {"text": "message polarity classification", "start_pos": 97, "end_pos": 128, "type": "TASK", "confidence": 0.8185686866442362}]}, {"text": "Although our scores are about 10-20% behind the top-scoring systems, we show that it is possible to develop sentiment analysis systems via rapid prototyping with reasonable accuracy in a very short amount of time.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.9654181599617004}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9952823519706726}]}], "datasetContent": [{"text": "In this section, we report experimental result for our method.", "labels": [], "entities": []}, {"text": "We used the scikit-learn Python machine learning library) to implement the feature extraction pipeline and the support vector machine classifier.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.6763206571340561}]}, {"text": "We use a linear kernel for the support vector machine and fixed the SVM hyper-parameter C to 1.0.", "labels": [], "entities": []}, {"text": "We found that scikit-learn allowed us to implement the system faster and resulted in much more compact code than other machine learning tools we have worked within the past.", "labels": [], "entities": []}, {"text": "We used the official training set provided for the SemEval 2014 task to train our system and evaluated on the test set of the SemEval 2013 task which served as development data for this year's task . Tweets in the training data that were not available anymore through the Twitter API were removed from the training set.", "labels": [], "entities": [{"text": "SemEval 2014 task", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.72024933497111}, {"text": "SemEval 2013 task", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.5087102353572845}]}, {"text": "An overview of the data sets is shown in.", "labels": [], "entities": []}, {"text": "For the evaluation, we compute precision, recall and F 1 measure for the positive, negative, and neutral sentiment tweets.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9996479749679565}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9997304081916809}, {"text": "F 1", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.992613822221756}]}, {"text": "Following the official evaluation metric, the overall precision, recall, and F 1 measure of the system is the average of the precision, recall, and F 1 measures for positive and negative sentiment, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9995325803756714}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9983218312263489}, {"text": "F 1", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.996584415435791}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9993391633033752}, {"text": "recall", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9977390766143799}, {"text": "F 1", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.9927879273891449}]}, {"text": "Here, we report a feature ablation study: we omitted each individual feature category from the complete feature set to determine its influence on the overall performance.: Experimental Results for feature ablation study.", "labels": [], "entities": []}, {"text": "Each row shows the precision, recall, and F 1 score for the positive, negative, and neutral class and the overall precision, recall, and F 1 score after removing the particular feature from the features set.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9994408488273621}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9918973445892334}, {"text": "F 1 score", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9919621745745341}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9989535808563232}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9949567914009094}, {"text": "F 1 score", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9921261668205261}]}, {"text": "The most effective features are word N-grams and the sentiment lexicons.", "labels": [], "entities": []}, {"text": "It is interesting that the performance for the neutral class is very low for subtask A and high for subtask B.", "labels": [], "entities": []}, {"text": "We can also see that for subtask B, our system clearly has a problem with recall for the positive and negative sentiment.", "labels": [], "entities": [{"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9993694424629211}]}, {"text": "For the performance of our system in the SemEval 2014 shared task, we report the official overall F 1 scores of our system as released by the organizers on the official test set in, and anew test set of sarcastic tweets.", "labels": [], "entities": [{"text": "SemEval 2014 shared task", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.5525301545858383}, {"text": "F 1 scores", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9783344070116679}]}, {"text": "We also include the F 1 score of the best participating system for each test set and the rank of our system among all participating systems.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9915366768836975}]}, {"text": "The results of our system were fairly robust across different domains, with the exception of messages containing sarcasm which shows understanding sarcasm requires a deeper and more subtle understanding of the text that is not captured well in a simple linear model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of the data sets.", "labels": [], "entities": []}, {"text": " Table 2: Experimental Results for feature ablation study. Each row shows the precision, recall, and F 1  score for the positive, negative, and neutral class and the overall precision, recall, and F 1 score after  removing the particular feature from the features set.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9994398951530457}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9893670678138733}, {"text": "F 1  score", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9897485971450806}, {"text": "precision", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9982157945632935}, {"text": "recall", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.9925561547279358}, {"text": "F 1 score", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9901610414187113}]}, {"text": " Table 3: Official results for Semeval 2014 test set.  Reported scores are overall F 1 scores.", "labels": [], "entities": [{"text": "Semeval 2014 test set", "start_pos": 31, "end_pos": 52, "type": "DATASET", "confidence": 0.7914398908615112}, {"text": "F 1 scores", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9725879430770874}]}]}