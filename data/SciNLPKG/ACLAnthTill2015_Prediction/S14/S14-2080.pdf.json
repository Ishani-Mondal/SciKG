{"title": [{"text": "Peking: Profiling Syntactic Tree Parsing Techniques for Semantic Graph Parsing", "labels": [], "entities": [{"text": "Semantic Graph Parsing", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.6623536845048269}]}], "abstractContent": [{"text": "Using the SemEval-2014 Task 8 data, we profile the syntactic tree parsing techniques for semantic graph parsing.", "labels": [], "entities": [{"text": "SemEval-2014 Task 8 data", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.6532271802425385}, {"text": "syntactic tree parsing", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7459993561108907}, {"text": "semantic graph parsing", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.7559188604354858}]}, {"text": "In particular , we implement different transition-based and graph-based models, as well as a parser ensembler, and evaluate their effectiveness for semantic dependency parsing.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 148, "end_pos": 175, "type": "TASK", "confidence": 0.7137565612792969}]}, {"text": "Evaluation gauges how successful data-driven dependency graph parsing can be by applying existing techniques.", "labels": [], "entities": [{"text": "dependency graph parsing", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.6638688246409098}]}], "introductionContent": [{"text": "Bi-lexical dependency representation is quite powerful and popular to encode syntactic or semantic information, and parsing techniques under the dependency formalism have been well studied and advanced in the last decade.", "labels": [], "entities": []}, {"text": "The major focus is limited to tree structures, which fortunately correspond to many computationally good properties.", "labels": [], "entities": []}, {"text": "On the other hand, some leading linguistic theories argue that more general graphs are needed to encode a wide variety of deep syntactic and semantic phenomena, e.g. topicalization, relative clauses, etc.", "labels": [], "entities": []}, {"text": "However, algorithms for statistical graph spanning have not been well explored before, and therefore it is not very clear how good data-driven parsing techniques developed for tree parsing can be for graph generating.", "labels": [], "entities": [{"text": "statistical graph spanning", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.796334445476532}, {"text": "tree parsing", "start_pos": 176, "end_pos": 188, "type": "TASK", "confidence": 0.7971883118152618}, {"text": "graph generating", "start_pos": 200, "end_pos": 216, "type": "TASK", "confidence": 0.7516346573829651}]}, {"text": "Following several well-established syntactic theories, proposes using graphs to represent semantics.", "labels": [], "entities": []}, {"text": "Considering that semantic dependency parsing is a quite new topic and there is little previous work, we think it worth appropriately profiling successful tree parsing techniques for graph parsing.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.7361912727355957}, {"text": "graph parsing", "start_pos": 182, "end_pos": 195, "type": "TASK", "confidence": 0.729761928319931}]}, {"text": "To this end, we build a hybrid system that combines several important data-driven parsing techniques and evaluate their impact with the given data.", "labels": [], "entities": []}, {"text": "In particular, we implement different transition-based and graph-based models, as well as a parser ensembler.", "labels": [], "entities": []}, {"text": "Our experiments highlight the following facts: \u2022 Graph-based models are more effective than transition-based models.", "labels": [], "entities": []}, {"text": "\u2022 Parser ensemble is very useful to boost the parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9740083813667297}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9318757057189941}]}], "datasetContent": [{"text": "There are 3 subtasks in the task, namely DM, PAS, and PCEDT.", "labels": [], "entities": [{"text": "PAS", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.972831130027771}, {"text": "PCEDT", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.8666144013404846}]}, {"text": "For subtask DM, we finally obtained 19 models, just as stated in previous sections.", "labels": [], "entities": [{"text": "subtask DM", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.5027415454387665}]}, {"text": "For subtask PAS and PCEDT, only 17 models are trained due to the tight schedule.", "labels": [], "entities": [{"text": "PAS", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.5608020424842834}, {"text": "PCEDT", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.8191760778427124}]}, {"text": "The tree approximation algorithms may cause some edge loss, and the statistics are shown in Table 1.", "labels": [], "entities": []}, {"text": "We can see that DFS does not cause edge loss, but edge losses of other two algorithm are not negligible.", "labels": [], "entities": []}, {"text": "This may result in a lower recall and higher precision, but we can tune the final results during model ensemble.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9993833303451538}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9993481040000916}]}, {"text": "Edge loss in subtask DM is less than those in subtask PAS and PCEDT.", "labels": [], "entities": [{"text": "Edge loss", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.940615713596344}, {"text": "PCEDT", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.9282239079475403}]}, {"text": "We present the performance of several representative models in.", "labels": [], "entities": []}, {"text": "We can see that the tree approximation models performs better than the transition-based models, which highlights the effective of arc-factored models for semantic dependency parsing.", "labels": [], "entities": [{"text": "semantic dependency parsing", "start_pos": 154, "end_pos": 181, "type": "TASK", "confidence": 0.7265921235084534}]}, {"text": "For model ensemble, besides the accuracy of each single model, it is also important that the models to be ensembled are very different.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.999201238155365}]}, {"text": "As shown in, the evaluation between some of our models indicates that our models do vary a lot.", "labels": [], "entities": []}, {"text": "Following the suggestion of the task organizers, we use section 20 of the train data as the development set.", "labels": [], "entities": []}, {"text": "With the help of development set, we tune the parameters of the models and ensem-: Evaluation between some of our models.", "labels": [], "entities": []}, {"text": "Labeled f-score on test set is shown.", "labels": [], "entities": [{"text": "f-score", "start_pos": 8, "end_pos": 15, "type": "METRIC", "confidence": 0.9547489881515503}]}, {"text": "Titov r stands for reversed Titov, DFS n for DFS+nearest, DFS l for DFS+left, and BFS n for BFS+nearest.", "labels": [], "entities": [{"text": "BFS", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9816319942474365}]}, {"text": "The upper part gives the performance, and the lower part gives the agreement between systems.: Final results of the ensembled model. bling.", "labels": [], "entities": []}, {"text": "We set the weight of each transition-based model 1, and tree approximation model 2 in run 1, 3 in run 2.", "labels": [], "entities": []}, {"text": "The threshold is set to a half of the total weight.", "labels": [], "entities": []}, {"text": "The final results given by the organizers are shown in.", "labels": [], "entities": []}, {"text": "Compared to demonstrates the effectiveness of parser ensemble.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Edge loss of transformation algorithms.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation between some of our models.  Labeled f-score on test set is shown. Titov r stands  for reversed Titov, DFS n for DFS+nearest, DFS l  for DFS+left, and BFS n for BFS+nearest. The up- per part gives the performance, and the lower part  gives the agreement between systems.", "labels": [], "entities": [{"text": "BFS", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.9695780277252197}]}, {"text": " Table 3: Final results of the ensembled model.", "labels": [], "entities": []}]}