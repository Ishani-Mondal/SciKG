{"title": [{"text": "FBK-TR: SVM for Semantic Relatedness and Corpus Patterns for RTE", "labels": [], "entities": [{"text": "FBK-TR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8776956796646118}, {"text": "Semantic Relatedness", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7270421385765076}, {"text": "RTE", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.8573328852653503}]}], "abstractContent": [{"text": "This paper reports the description and scores of our system, FBK-TR, which participated at the SemEval 2014 task #1 \"Evaluation of Compositional Distribu-tional Semantic Models on Full Sentences through Semantic Relatedness and Entail-ment\".", "labels": [], "entities": [{"text": "FBK-TR", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.8359997868537903}, {"text": "SemEval 2014 task #", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.900671973824501}]}, {"text": "The system consists of two parts: one for computing semantic relatedness, based on SVM, and the other for identifying the entailment values on the basis of both semantic relatedness scores and entailment patterns based on verb-specific semantic frames.", "labels": [], "entities": []}, {"text": "The system ranked 11 th on both tasks with competitive results.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the Natural Language Processing community, meaning related tasks have gained an increasing popularity.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.6249723931153616}]}, {"text": "These tasks focus, in general, on a couple of short pieces of text, like pair of sentences, and the systems are required to infer a certain meaning relationship that exists between these texts.", "labels": [], "entities": []}, {"text": "Two of the most popular meaning related tasks are the identification of Semantic Text Similarity (STS) and Recognizing Textual Entailment (RTE).", "labels": [], "entities": [{"text": "identification of Semantic Text Similarity (STS)", "start_pos": 54, "end_pos": 102, "type": "TASK", "confidence": 0.8669977709650993}, {"text": "Recognizing Textual Entailment (RTE)", "start_pos": 107, "end_pos": 143, "type": "TASK", "confidence": 0.6949064135551453}]}, {"text": "The STS tasks require to identify the degree of similarity (or relatedness) that exists between two text fragments (sentences, paragraphs, . .", "labels": [], "entities": []}, {"text": "), where similarity is abroad concept and its value is normally obtained by averaging the opinion of several annotators.", "labels": [], "entities": []}, {"text": "The RTE task requires the identification of a directional relation between a pair of text fragments, namely a text (T) and a hypothesis (H).", "labels": [], "entities": [{"text": "RTE task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.919145941734314}]}, {"text": "The relation (T \u2192 H) holds whenever the truth of H follows from T.) primarily aimed at evaluating Compositional Distributional Semantic Models (CDSMs) of meaning over two subtasks, namely semantic relatedness and textual entailment (ENTAILMENT, CONTRADIC-TION and NEUTRAL), over pairs of sentences).", "labels": [], "entities": []}, {"text": "Concerning the relatedness subtask, the system outputs are evaluated against gold standard ratings in two ways, using Pearson correlation and Spearman's rank correlation (rho).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 118, "end_pos": 137, "type": "METRIC", "confidence": 0.9722065031528473}, {"text": "Spearman's rank correlation (rho)", "start_pos": 142, "end_pos": 175, "type": "METRIC", "confidence": 0.7480722878660474}]}, {"text": "The Pearson correlation is used for evaluating and ranking the participating systems.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9405432641506195}]}, {"text": "Similarly, for the textual entailment subtask, system outputs are evaluated against a gold standard rating with respect to accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9970645308494568}]}, {"text": "Our team, FBK-TR, participated in both subtasks with five different runs.", "labels": [], "entities": [{"text": "FBK-TR", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.9187530875205994}]}, {"text": "In this paper, we present a comprehensive description of our system which obtained competitive results in both tasks and which is not based on CDSMs.", "labels": [], "entities": []}, {"text": "Our approach for the relatedness task is based on machine learning techniques to learn models from different lexical and semantic features from the train corpus and then to make prediction on the test corpus.", "labels": [], "entities": []}, {"text": "Particularly, we used support vector machine (SVM) (, regression model to solve this subtask.", "labels": [], "entities": []}, {"text": "On the other hand, the textual entailment task uses a methodology mainly based on corpus patterns automatically extracted from annotated text corpora.", "labels": [], "entities": [{"text": "textual entailment task", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.7927435437838236}]}, {"text": "The remainder of the paper is organized as follows: Section 2 presents the SVM system for semantic relatedness.", "labels": [], "entities": []}, {"text": "Section 3 describes the methodology used for extracting patterns and computing the textual entailment values.", "labels": [], "entities": []}, {"text": "Finally, Section 4 discusses about the evaluations and Section 5 presents conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1 illustrates the results for Pearson and  Spearman correlations for the relatedness subtask  on the test set. Table 2 reports the Accuracy values  for the entailment subtask on the test set.  Concerning the relatedness results our systems  ranked 11 th out of 17 participating systems. Best  score of our system is reported in Table 1. One  of the main reason for the relatively low results", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9995837807655334}]}, {"text": " Table 2: Results for entailment subtask.", "labels": [], "entities": []}]}