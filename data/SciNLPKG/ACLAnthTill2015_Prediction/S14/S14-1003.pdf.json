{"title": [{"text": "Improvement of a Naive Bayes Sentiment Classifier Using MRS-Based Features", "labels": [], "entities": [{"text": "Naive Bayes Sentiment Classifier", "start_pos": 17, "end_pos": 49, "type": "TASK", "confidence": 0.9260404109954834}]}], "abstractContent": [{"text": "This study explores the potential of using deep semantic features to improve binary sentiment classification of paragraph-length movie reviews from the IMBD website.", "labels": [], "entities": [{"text": "binary sentiment classification of paragraph-length movie reviews", "start_pos": 77, "end_pos": 142, "type": "TASK", "confidence": 0.8037478753498623}, {"text": "IMBD website", "start_pos": 152, "end_pos": 164, "type": "DATASET", "confidence": 0.9113723337650299}]}, {"text": "Using a Naive Bayes classifier as a baseline, we show that features extracted from Minimal Recursion Semantics representations in conjunction with back-off replacement of sentiment terms is effective in obtaining moderate increases inaccuracy over the baseline's n-gram features.", "labels": [], "entities": []}, {"text": "Although our results are mixed, our most successful feature combination achieves an accuracy of 89.09%, which represents an increase of 0.76% over the baseline performance and a 6.48% reduction in error.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9997044205665588}, {"text": "error", "start_pos": 197, "end_pos": 202, "type": "METRIC", "confidence": 0.9781258702278137}]}], "introductionContent": [{"text": "Text-based sentiment analysis offers valuable insight into the opinions of large communities of reviewers, commenters and customers.", "labels": [], "entities": [{"text": "Text-based sentiment analysis", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6648198167483012}]}, {"text": "In their survey of the field, highlight the importance of sentiment analysis across a range of industries, including review aggregation websites, business intelligence, and reputation management.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.9325852394104004}, {"text": "reputation management", "start_pos": 173, "end_pos": 194, "type": "TASK", "confidence": 0.8035593330860138}]}, {"text": "Detection and classification of sentiment can improve downstream performance in applications sensitive to user opinions, such as questionanswering, automatic product recommendations, and social network analysis (ibid., p. 12).", "labels": [], "entities": [{"text": "Detection and classification of sentiment", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7093509376049042}, {"text": "social network analysis", "start_pos": 187, "end_pos": 210, "type": "TASK", "confidence": 0.7137287259101868}]}, {"text": "While previous research in sentiment analysis has investigated the extraction of features from syntactic dependency trees, semantic representations appear to be underused as a resource for modeling opinion in text.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9492450952529907}]}, {"text": "Indeed, to our knowledge, there has been no research using semantic dependencies created by a precision grammar for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.9626064896583557}]}, {"text": "The goal of the present research is to address this gap by augmenting a baseline classifier with features based on Minimal Resursion Semantics (MRS;), a formal semantic representation provided by the English Resource Grammar).", "labels": [], "entities": [{"text": "English Resource Grammar", "start_pos": 200, "end_pos": 224, "type": "DATASET", "confidence": 0.863035519917806}]}, {"text": "An MRS is a connected graph in which semantic entities maybe linked directly through shared arguments or indirectly through handle or qeq constraints, which denote equality modulo quantifier insertion ().", "labels": [], "entities": []}, {"text": "This schema allows for underspecification of quantifier scope.", "labels": [], "entities": []}, {"text": "Using Naive Bayes sentiment classifier as a baseline, we test the effectiveness of eight feature types derived from MRS.", "labels": [], "entities": [{"text": "Naive Bayes sentiment classifier", "start_pos": 6, "end_pos": 38, "type": "TASK", "confidence": 0.6820298731327057}]}, {"text": "Our feature pipeline crawls various links in the MRS representations of sentences in our corpus of paragraph-length movie reviews and outputs simple, human-readable features based on various types of semantic relationships.", "labels": [], "entities": [{"text": "MRS representations of sentences", "start_pos": 49, "end_pos": 81, "type": "TASK", "confidence": 0.8289733529090881}]}, {"text": "This improved system achieves modest increases in binary sentiment classification accuracy for several of the feature combinations tested.", "labels": [], "entities": [{"text": "binary sentiment classification", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.6154826382795969}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9469664096832275}]}, {"text": "In the following sections, we summarize previous research in MRS feature extraction and sentiment classification, describe the baseline system and our modifications to it, and outline our approach to parsing our data, constructing features, and integrating them into the existing system.", "labels": [], "entities": [{"text": "MRS feature extraction", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.9067523876825968}, {"text": "sentiment classification", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.9087053835391998}]}, {"text": "Finally, we report our findings, examine in more detail where our improved system succeeded and failed in relation to the baseline, and suggest avenues for further research in sentiment analysis with MRS-based features.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.9555351138114929}]}], "datasetContent": [{"text": "We use a dataset of 50,000 movie reviews crawled from the IMDB website, originally developed by.", "labels": [], "entities": [{"text": "IMDB website", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9498912990093231}]}, {"text": "The dataset is split equally between training and test sets.", "labels": [], "entities": []}, {"text": "Both training and test sets contain equal numbers of positive and negative reviews, which are defined according to the number of stars assigned by the author on the IMBD website: one to four stars for negative reviews, and seven to ten stars for positive reivews.", "labels": [], "entities": [{"text": "IMBD website", "start_pos": 165, "end_pos": 177, "type": "DATASET", "confidence": 0.9459956586360931}]}, {"text": "The reviews vary in length but generally contain between five and fifteen sentences.", "labels": [], "entities": []}, {"text": "The Natural Language ToolKit's (NLTK;) sentence tokenizer distinguishes 616,995 sentences in the dataset.", "labels": [], "entities": []}, {"text": "Unlike previous research over this dataset, we divide the 25,000 reviews of the test set into two development sets and a final test set.", "labels": [], "entities": []}, {"text": "As such, our results are not directly comparable to those of.", "labels": [], "entities": []}, {"text": "To test our MRS features, we adapted our baseline to treat them much like the n-gram features.", "labels": [], "entities": [{"text": "MRS", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.8524767160415649}]}, {"text": "As with n-grams, each MRS feature is counted toward the probability of the class of its source document, and a negated version of that feature, with not prepended, is counted toward the opposite class.", "labels": [], "entities": []}, {"text": "We ran our feature filtering trials using the first development set, then obtained preliminary accuracy figures from our second development set.", "labels": [], "entities": [{"text": "feature filtering", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7817487716674805}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9984000325202942}]}, {"text": "We began with each feature type in isolation and used these results to inform later experiments using combinations of feature types.", "labels": [], "entities": []}, {"text": "The numbers reported here are the results over the final, heldout test set.", "labels": [], "entities": []}, {"text": "Our final test accuracies indicate that three feature types produce the best gains in accuracy: back-off PRPs with first-and second-predicate replacement (types 4 and 5), and PRPs with string predicates only (type 3).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9990723133087158}]}, {"text": "ranks the top seven feature combinations in ascending order by post-feature filtering accuracies.", "labels": [], "entities": []}, {"text": "The bolded feature types show that all of the best combination runs include one or more of the top three features mentioned above.", "labels": [], "entities": []}, {"text": "Notable also are the accuracies for MRS-based features alone, which fall very close to the baseline.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9770491719245911}, {"text": "MRS-based", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.8959852457046509}]}, {"text": "The best accuracies for preand post-feature filtering tests appear in bold.", "labels": [], "entities": []}, {"text": "The highest accuracy, achieved by running a feature-filtered combination of the baseline's ngram features and feature types 3, 4, and 5, resulted in a 0.80% increase over the baseline performance with feature filtering, and a 0.76% increase in the best baseline accuracy overall (obtained without feature filtering).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9993696808815002}, {"text": "accuracy", "start_pos": 262, "end_pos": 270, "type": "METRIC", "confidence": 0.9407001733779907}]}, {"text": "The experimental best run successfully categorizes 63 more of the 8333 test documents than the baseline best run.", "labels": [], "entities": []}, {"text": "Although these gains are small, they account fora 6.48% reduction in error.", "labels": [], "entities": [{"text": "error", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.8860195279121399}]}, {"text": "The test accuracies indicate that our back-off replacement method, in combination with the simple predicate-argument relationships captured in PRP triples, is the most successful aspect of feature design in this project.", "labels": [], "entities": []}, {"text": "However, as our error analysis indicates, back-off is the likely source of many of our system's errors (see \u00a75.2).", "labels": [], "entities": []}, {"text": "lists the 15 most informative MRS features from our best run based on mutual information score, all of which are of feature type 4 or 5.", "labels": [], "entities": [{"text": "MRS", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8894873857498169}]}, {"text": "Note that the not prepended to some features is a function of way our classifier reads in binary features (as described in \u00a72.2), not an indication of grammatical negation.", "labels": [], "entities": []}, {"text": "The success of these partial back-off features confirms our intuition that the semantic relationships between sentiment-laden terms and other entities in the sentence offer a reliable indicator of author sentiment.", "labels": [], "entities": []}, {"text": "When we performed backoff replacement directly on the surface strings and ran our classifier with n-grams only, we obtained accuracies of 87.29% pre-feature filtering and 87.50% post-feature filtering, a small decrease from the baseline performance (see).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.9683231711387634}]}, {"text": "This lends additional support to the idea that the combination of sentiment back-off and semantic dependencies is significant.", "labels": [], "entities": []}, {"text": "These results also fit with the findings of of, who determined that back-off triple features provide \"more generalizable and useful patterns\" in sentiment data than lexical dependency features alone (p. 316).", "labels": [], "entities": []}, {"text": "Despite these promising results, we found that the separate EP values (type 1), PRP triples without replacement (type 2), PRPs with double replacement (type 6) and SL features (types 7 and 8) have very little effect on accuracy by themselves.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 219, "end_pos": 227, "type": "METRIC", "confidence": 0.9987826943397522}]}, {"text": "For type 1, we suspect that EP values alone don't contribute enough information beyond basic n-gram features.", "labels": [], "entities": []}, {"text": "We had hypothesized that the lemmatization in these values might provide some helpful back-off.", "labels": [], "entities": []}, {"text": "However, this effect is likely drowned out by the lack of any scope of negation handling in the MRS features.", "labels": [], "entities": [{"text": "negation handling", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.9694702625274658}, {"text": "MRS", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9373232126235962}]}, {"text": "We attribute the failure of the SL features to the fact that they often capture EPs originating in adjacent tokens in the surface string, which does not improve on the n-gram features.", "labels": [], "entities": []}, {"text": "Lastly, we believe the relative sparsity of double back-off features was the primary reason they did not produce meaningful results.", "labels": [], "entities": []}, {"text": "These results also call into question the usefulness of the feature filtering trials in our baseline.", "labels": [], "entities": [{"text": "feature filtering", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8172214031219482}]}, {"text": "By design, these trials produce performance increases on the dataset on which they are run.", "labels": [], "entities": []}, {"text": "However, filtering produces small and inconsistent gains for the final held-out test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Individual MRS feature trial results", "labels": [], "entities": [{"text": "MRS", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.894059419631958}]}, {"text": " Table 4: Combination feature results", "labels": [], "entities": []}]}