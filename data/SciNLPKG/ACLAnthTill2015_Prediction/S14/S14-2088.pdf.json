{"title": [{"text": "SAIL-GRS: Grammar Induction for Spoken Dialogue Systems using CF-IRF Rule Similarity", "labels": [], "entities": []}], "abstractContent": [{"text": "The SAIL-GRS system is based on a widely used approach originating from information retrieval and document indexing , the T F-IDF measure.", "labels": [], "entities": [{"text": "document indexing", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.6411060094833374}, {"text": "T F-IDF measure", "start_pos": 122, "end_pos": 137, "type": "METRIC", "confidence": 0.8560827374458313}]}, {"text": "In this implementation for spoken dialogue system grammar induction, rule constituent frequency and inverse rule frequency measures are used for estimating lexical and semantic similarity of candidate grammar rules to a seed set of rule pattern instances.", "labels": [], "entities": [{"text": "spoken dialogue system grammar induction", "start_pos": 27, "end_pos": 67, "type": "TASK", "confidence": 0.7658673822879791}]}, {"text": "The performance of the system is evaluated for the English language in three different domains, travel, tourism and finance and in the travel domain, for Greek.", "labels": [], "entities": []}, {"text": "The simplicity of our approach makes it quite easy and fast to implement irrespective of language and domain.", "labels": [], "entities": []}, {"text": "The results show that the SAIL-GRS system performs quite well in all three domains and in both languages .", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken dialogue systems typically rely on grammars which define the semantic frames and respective fillers in dialogue scenarios.", "labels": [], "entities": [{"text": "Spoken dialogue", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8276143670082092}]}, {"text": "Such systems are tailored for specific domains for which the respective grammars are mostly manually developed.", "labels": [], "entities": []}, {"text": "In order to address this issue, numerous current approaches attempt to infer these grammar rules automatically (.", "labels": [], "entities": []}, {"text": "The acquisition of grammar rules for spoken language systems is defined as a task comprising of two subtasks, the acquisition of: (i) Low-level rules These are rules defining domain-specific entities, such as names of locations, hotels, airports, e.g. CountryName: \"USA\", Date: \"July 15th, 2014\", CardType: \"VISA\" and other common domain multi-word expressions, e.g. DoYouKnowQ: \"do you know\".", "labels": [], "entities": []}, {"text": "(ii) High-level rules These are larger, frame-like rule patterns which contain as semantic slot fillers multi-word entities identified by low-level rules.", "labels": [], "entities": []}, {"text": "For example: DirectionsQ: \"<DoYouKnowQ> <where> the <MuseumName> is located\", ExpressionCardProblem: \"my <CardType> has expired\".", "labels": [], "entities": []}, {"text": "The shared task of Grammar Induction for Spoken Dialogue Systems, where our system participated, focused on the induction of high-level grammar rules and in particular on the identification and semantic classification of new rule patterns based on their semantic similarity to known rule instances.", "labels": [], "entities": [{"text": "Grammar Induction for Spoken Dialogue Systems", "start_pos": 19, "end_pos": 64, "type": "TASK", "confidence": 0.7625691294670105}, {"text": "identification and semantic classification of new rule patterns", "start_pos": 175, "end_pos": 238, "type": "TASK", "confidence": 0.77620929479599}]}, {"text": "Within this research framework, the work described in this paper proposes a methodology for estimating rule semantic similarity using a variation of the well-known measure of T F -IDF as rule constituent frequency vs. inverse rule frequency, henceforth CF -IRF . In the remainder of this paper, we start in Section 2 by a detailed description of our system.", "labels": [], "entities": [{"text": "estimating rule semantic similarity", "start_pos": 92, "end_pos": 127, "type": "TASK", "confidence": 0.7112473845481873}, {"text": "T F -IDF", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.8370945602655411}]}, {"text": "Subsequently, in Section 3, we present the datasets used and the evaluation process, and in Section 4 we discuss our results.", "labels": [], "entities": []}, {"text": "We conclude in Section 5 with a summary of our observations and directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The overall objective in spoken dialogue system grammar induction is the fast and efficient development and portability of grammar resources.", "labels": [], "entities": [{"text": "spoken dialogue system grammar induction", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.7319916129112244}]}, {"text": "In the Grammar Induction for Spoken Dialogue Systems task, this challenge was addressed by providing datasets in three different domains, travel, tourism and finance, and by attempting to cover more than one language for the travel domain, namely English and Greek.", "labels": [], "entities": [{"text": "Grammar Induction for Spoken Dialogue Systems task", "start_pos": 7, "end_pos": 57, "type": "TASK", "confidence": 0.7201823038714272}]}, {"text": "As illustrated in, the travel domain data for the two languages are comparable, with 32 and 35 number of known rule categories, for English and Greek, comprising of 982 and 956 high-level rule pattern instances respectively.", "labels": [], "entities": []}, {"text": "The smallest dataset is the finance dataset, with 9 rule categories and 136 rule pattern instances, while the tourism dataset has a relatively low number of rule categories comprising of the highest number of rule pattern instances.", "labels": [], "entities": [{"text": "finance dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.7902877628803253}]}, {"text": "Interestingly, as indicated in the column depicting the percent of unknown n-grams in the test-set, i.e. the unigrams and the bigrams without a CF -IRF value in the training data, the tourism domain test-set appears also to be the one with the greatest overlap with the training data, with a mere 0.72% and 4.84% of unknown unigrams and bigrams respectively.", "labels": [], "entities": [{"text": "CF -IRF value", "start_pos": 144, "end_pos": 157, "type": "METRIC", "confidence": 0.8796991556882858}]}, {"text": "For the evaluation, the system performance is estimated in terms of precision (P ), recall (R) and F -score measures, for the correct classification of an unknown text fragment to a given rule category cluster of pattern instances.", "labels": [], "entities": [{"text": "precision (P )", "start_pos": 68, "end_pos": 82, "type": "METRIC", "confidence": 0.9506948590278625}, {"text": "recall (R)", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.963796928524971}, {"text": "F -score", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9866645336151123}]}, {"text": "In addition to these measures, the weighted average of the per rule scores is computed as follows: where N \u2212 1 is the total number of rule categories, P i and R i are the per rule i scores for precision and recall, c i the unknown patterns correctly assigned to rule i, and n i the total number of correct rule instance patterns for rule i indicated in the ground truth data.", "labels": [], "entities": [{"text": "precision", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.9981997013092041}, {"text": "recall", "start_pos": 207, "end_pos": 213, "type": "METRIC", "confidence": 0.9914132356643677}]}], "tableCaptions": [{"text": " Table 2: Characteristics of training and test datasets.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation results for SAIL-GRS system compared to the baseline in all four datasets in terms  of per rule Precision P , Recall R, and F-score F . In the grey column, P w , R w , and F w stand for the  weighted average of the per rule precision, recall and F-score respectively, as defined in Equ. 1 and 2.", "labels": [], "entities": [{"text": "Precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.8908179402351379}, {"text": "Recall R", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9790357947349548}, {"text": "F-score F", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9722296297550201}, {"text": "recall", "start_pos": 256, "end_pos": 262, "type": "METRIC", "confidence": 0.9976981282234192}, {"text": "F-score", "start_pos": 267, "end_pos": 274, "type": "METRIC", "confidence": 0.9808526635169983}, {"text": "Equ. 1", "start_pos": 303, "end_pos": 309, "type": "DATASET", "confidence": 0.9218159914016724}]}]}