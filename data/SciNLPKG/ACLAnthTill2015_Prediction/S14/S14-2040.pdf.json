{"title": [{"text": "Duluth : Measuring Cross-Level Semantic Similarity with First and Second-Order Dictionary Overlaps", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the Duluth systems that participated in the Cross-Level Semantic Similarity task of SemEval-2014.", "labels": [], "entities": [{"text": "Cross-Level Semantic Similarity task of SemEval-2014", "start_pos": 65, "end_pos": 117, "type": "TASK", "confidence": 0.7076956282059351}]}, {"text": "These three systems were all unsupervised and relied on a dictionary melded together from various sources, and used first-order (Lesk) and second-order (Vector) overlaps to measure similarity.", "labels": [], "entities": []}, {"text": "The first-order overlaps fared well according to Spear-man's correlation (top 5) but less so relative to Pearson's.", "labels": [], "entities": []}, {"text": "Most systems performed at comparable levels for both Spearman's and Pearson's measure, which suggests the Duluth approach is potentially unique among the participating systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cross-Level Semantic Similarity (CLSS) is a novel variation on the problem of semantic similarity.", "labels": [], "entities": [{"text": "Cross-Level Semantic Similarity (CLSS)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7681342115004858}]}, {"text": "As traditionally formulated, pairs of words, pairs of phrases, or pairs of sentences are scored for similarity.", "labels": [], "entities": []}, {"text": "However, the CLSS shared task () included 4 subtasks where pairs of different granularity were measured for semantic similarity.", "labels": [], "entities": []}, {"text": "These included : word-2-sense (w2s), phrase-2-word (p2w), sentence-2-phrase (s2p), and paragraph-2-sentence (g2s).", "labels": [], "entities": []}, {"text": "In addition to different levels of granularity, these pairs included slang, jargon and other examples of non-standard English.", "labels": [], "entities": []}, {"text": "We were drawn to this task because of our longstanding interest in semantic similarity.", "labels": [], "entities": []}, {"text": "We have pursued approaches ranging from those that rely on structured knowledge sources like WordNet (e.g., WordNet::Similarity) (  to those that use distributional information found This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.9732652306556702}]}, {"text": "Page numbers and proceedings footer are added by the organisers.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by/4.0/ in raw text (e.g., SenseClusters) ().", "labels": [], "entities": []}, {"text": "Our approach in this shared task is a bit of both, but relies on using definitions for each item in a pair so that similarity can be measured using first or second-order overlaps.", "labels": [], "entities": []}, {"text": "A first-order approach finds direct matches between the words in a pair of definitions.", "labels": [], "entities": []}, {"text": "Ina second-order approach each word in a definition is replaced by a vector of the words it co-occurs with, and then the vectors for all the words in a definition are averaged together to represent the definition.", "labels": [], "entities": []}, {"text": "Then, similarity can be measured by finding the cosine between pairs of these vectors.", "labels": [], "entities": [{"text": "similarity", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9870069026947021}]}, {"text": "We decided on a definition based approach since it had the potential to normalize the differences in granularity of the pairs.", "labels": [], "entities": []}, {"text": "The main difficulty in comparing definitions is that they can be very brief or may not even exist at all.", "labels": [], "entities": []}, {"text": "This is why we combined various different kinds of resources to arrive at our dictionary.", "labels": [], "entities": []}, {"text": "While we achieved near total coverage of words and senses, phrases were sparsely covered, and sentences and paragraphs had no coverage.", "labels": [], "entities": []}, {"text": "In those cases we used the text of the phrase, sentence or paragraph to serve as its own definition.", "labels": [], "entities": []}, {"text": "The Duluth systems were implemented using the UMLS::Similarity package () (version 1.35) 1 , which includes support for user-defined dictionaries, first-order Lesk methods, and second-order Vector methods.", "labels": [], "entities": []}, {"text": "As a result the Duluth systems required minimal implementation, so once a dictionary was ready experiments could begin immediately.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, the first-order Lesk and second-order Vector measures are described.", "labels": [], "entities": []}, {"text": "Then we discuss the details of the three Duluth systems that participated in this task.", "labels": [], "entities": []}, {"text": "Finally, we review the task results and consider future directions for this problem and our system.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Pearson's Results  rank  g2s s2p p2w w2s (of 38)  Top  .811 .742 .415 .355  1  Duluth2 .501 .450 .241 .224  23  Duluth1 .458 .440 .075 .076  30  Duluth3 .455 .426 .075 .080  31  Baseline .527 .562 .165 .110", "labels": [], "entities": []}]}