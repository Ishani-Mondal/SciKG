{"title": [{"text": "SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents the task on the evaluation of Compositional Distributional Semantics Models on full sentences organized for the first time within SemEval-2014.", "labels": [], "entities": []}, {"text": "Participation was open to systems based on any approach.", "labels": [], "entities": []}, {"text": "Systems were presented with pairs of sentences and were evaluated on their ability to predict human judgments on (i) semantic relatedness and (ii) entailment.", "labels": [], "entities": []}, {"text": "The task attracted 21 teams, most of which participated in both subtasks.", "labels": [], "entities": []}, {"text": "We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs).", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional Semantic Models (DSMs) approximate the meaning of words with vectors summarizing their patterns of co-occurrence in corpora.", "labels": [], "entities": []}, {"text": "Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (.", "labels": [], "entities": []}, {"text": "Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging.", "labels": [], "entities": []}, {"text": "Existing data sets, such as those introduced by and, are limited to a few hundred instances of very short sentences with a fixed structure.", "labels": [], "entities": []}, {"text": "In the last ten years, several large).", "labels": [], "entities": []}, {"text": "Working with such data sets, however, requires dealing with issues, such as identifying multiword expressions, recognizing named entities or accessing encyclopedic knowledge, which have little to do with compositionality per se.", "labels": [], "entities": []}, {"text": "CDSMs should instead be evaluated on data that are challenging for reasons due to semantic compositionality (e.g. context-cued synonymy resolution and other lexical variation phenomena, active/passive and other syntactic alternations, impact of negation at various levels, operator scope, and other effects linked to the functional lexicon).", "labels": [], "entities": []}, {"text": "These issues do not occur frequently in, e.g., the STS and RTE data sets.", "labels": [], "entities": [{"text": "STS and RTE data sets", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.6904408097267151}]}, {"text": "With these considerations in mind, we developed SICK (Sentences Involving Compositional Knowledge), a data set aimed at filling the void, including a large number of sentence pairs that are rich in the lexical, syntactic and semantic phenomena that CDSMs are expected to account for, but do not require dealing with other aspects of existing sentential data sets that are not within the scope of compositional distributional semantics.", "labels": [], "entities": [{"text": "Sentences Involving Compositional Knowledge)", "start_pos": 54, "end_pos": 98, "type": "TASK", "confidence": 0.6943929195404053}]}, {"text": "Moreover, we distinguished between generic semantic knowledge about general concept categories (such as knowledge that a couple is formed by a bride and a groom) and encyclopedic knowledge about specific instances of concepts (e.g., knowing the fact that the current president of the US is Barack Obama).", "labels": [], "entities": []}, {"text": "The SICK data set contains many examples of the former, but none of the latter.", "labels": [], "entities": [{"text": "SICK data set", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8843680024147034}]}], "datasetContent": [{"text": "Both subtasks were evaluated using standard metrics.", "labels": [], "entities": []}, {"text": "In particular, the results on entailment were evaluated using accuracy, whereas the outputs on relatedness were evaluated using Pearson correlation, Spearman correlation, and Mean Squared Error (MSE).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9995260238647461}, {"text": "Pearson correlation", "start_pos": 128, "end_pos": 147, "type": "METRIC", "confidence": 0.9406128525733948}, {"text": "Spearman correlation", "start_pos": 149, "end_pos": 169, "type": "METRIC", "confidence": 0.7408508658409119}, {"text": "Mean Squared Error (MSE)", "start_pos": 175, "end_pos": 199, "type": "METRIC", "confidence": 0.9438767532507578}]}, {"text": "Pearson correlation was chosen as the official measure to rank the participating systems.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.7582394778728485}]}, {"text": "presents the performance of 4 baselines.", "labels": [], "entities": []}, {"text": "The Majority baseline always assigns the most common label in the training data (NEUTRAL), whereas the Probability baseline assigns labels randomly according to their relative frequency in the training set.", "labels": [], "entities": [{"text": "NEUTRAL", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.8288537859916687}]}, {"text": "The Overlap baseline measures word overlap, again with parameters (number of stop words and EN-TAILMENT/NEUTRAL/CONTRADICTION thresholds) estimated on the training part of the data.", "labels": [], "entities": [{"text": "Overlap baseline", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.6221129894256592}, {"text": "EN-TAILMENT/NEUTRAL/CONTRADICTION thresholds)", "start_pos": 92, "end_pos": 137, "type": "METRIC", "confidence": 0.8117980786732265}]}], "tableCaptions": [{"text": " Table 4: Distribution of sentence pairs across the  Training and Test Sets.", "labels": [], "entities": [{"text": "Distribution of sentence pairs", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7976494282484055}]}, {"text": " Table 5: Performance of baselines. Figure of merit  is Pearson correlation for relatedness and accuracy  for entailment. NA = Not Applicable", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 56, "end_pos": 75, "type": "METRIC", "confidence": 0.9610262215137482}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9992443323135376}, {"text": "NA", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.8508344292640686}, {"text": "Not Applicable", "start_pos": 127, "end_pos": 141, "type": "METRIC", "confidence": 0.6322393119335175}]}, {"text": " Table 6: Primary run results for the entailment  subtask. The table also shows whether a sys- tem exploits composition information at either the  phrase (P) or sentence (S) level.", "labels": [], "entities": []}, {"text": " Table 7: Primary run results for the relatedness  subtask (r for Pearson and \u03c1 for Spearman corre- lation). The table also shows whether a system ex- ploits composition information at either the phrase  (P) or sentence (S) level.", "labels": [], "entities": []}]}