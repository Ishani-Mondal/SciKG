{"title": [{"text": "BioinformaticsUA: Concept Recognition in Clinical Narratives Using a Modular and Highly Efficient Text Processing Framework", "labels": [], "entities": [{"text": "Concept Recognition in Clinical Narratives", "start_pos": 18, "end_pos": 60, "type": "TASK", "confidence": 0.6777300298213959}]}], "abstractContent": [{"text": "Clinical texts, such as discharge summaries or test reports, contain a valuable amount of information that, if efficiently and effectively mined, could be used to infer new knowledge, possibly leading to better diagnosis and therapeutics.", "labels": [], "entities": []}, {"text": "With this in mind, the SemEval-2014 Analysis of Clinical Text task aimed at assessing and improving current methods for identification and normalization of concepts occurring in clinical narrative.", "labels": [], "entities": [{"text": "SemEval-2014 Analysis of Clinical Text task", "start_pos": 23, "end_pos": 66, "type": "TASK", "confidence": 0.7561645905176798}, {"text": "identification and normalization of concepts occurring in clinical narrative", "start_pos": 120, "end_pos": 196, "type": "TASK", "confidence": 0.8280775845050812}]}, {"text": "This paper describes our approach in this task, which was based on a fully modular architecture for text mining.", "labels": [], "entities": [{"text": "text mining", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.8785306215286255}]}, {"text": "We followed a pure dictionary-based approach, after performing error analysis to refine our dictionaries.", "labels": [], "entities": []}, {"text": "We obtained an F-measure of 69.4% in the entity recognition task, achieving the second best precision overall submitted runs (81.3%), with above average recall (60.5%).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9996176958084106}, {"text": "entity recognition task", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.8759957750638326}, {"text": "precision overall submitted runs", "start_pos": 92, "end_pos": 124, "type": "METRIC", "confidence": 0.8967406004667282}, {"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9981301426887512}]}, {"text": "In the normalization task, we achieved a strict accuracy of 53.1% and a relaxed accuracy of 87.0%.", "labels": [], "entities": [{"text": "normalization task", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.9154686629772186}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9478132724761963}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9199262857437134}]}], "introductionContent": [{"text": "Named entity recognition (NER) is an information extraction task where the aim is to identify mentions of specific types of entities in text.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7958957701921463}, {"text": "information extraction task", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.7919781506061554}]}, {"text": "This task has been one of the main focus in the biomedical text mining research field, specially when applied to the scientific literature.", "labels": [], "entities": [{"text": "biomedical text mining", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.7379342913627625}]}, {"text": "Such efforts have led to the development of various tools for the recognition of diverse entities, including species names, genes and proteins, chemicals and drugs, anatomical concepts and diseases.", "labels": [], "entities": []}, {"text": "These tools use methods based on dictionaries, rules, and machine learning, or a combination of those depending on the specificities and requirements of each concept type ().", "labels": [], "entities": []}, {"text": "After identifying entities occurring in texts, it is also relevant to disambiguate those entities and associate each occurrence to a specific concept, using an univocal identifier from a reference database such as Uniprot 1 for proteins, or OMIM 2 for genetic disorders.", "labels": [], "entities": []}, {"text": "This is usually performed by matching the identified entities against a knowledge-base, possibly evaluating the textual context in which the entity occurred to identify the best matching concept.", "labels": [], "entities": []}, {"text": "The SemEval-2014 Analysis of Clinical Text task aimed at the identification and normalization of concepts in clinical narrative.", "labels": [], "entities": [{"text": "SemEval-2014 Analysis of Clinical Text task", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.6838322877883911}, {"text": "identification and normalization of concepts in clinical narrative", "start_pos": 61, "end_pos": 127, "type": "TASK", "confidence": 0.750329501926899}]}, {"text": "Two subtasks were defined, where Task A was focused on the recognition of entities belonging to the 'disorders' semantic group of the Unified Medical Language System (UMLS), and Task B was focused on normalization of these entities to a specific UMLS Concept Unique Identifier (CUI).", "labels": [], "entities": []}, {"text": "Specifically, the task definition required that concepts should only be normalized to CUIs that could be mapped to the SNOMED CT 3 terminology.", "labels": [], "entities": [{"text": "SNOMED CT 3 terminology", "start_pos": 119, "end_pos": 142, "type": "DATASET", "confidence": 0.7415838167071342}]}, {"text": "In this paper, we present a dictionary-based approach for the recognition of these concepts, supported by a modular text analysis and annotation pipeline.", "labels": [], "entities": []}], "datasetContent": [{"text": "The common evaluation metrics were used to evaluate the entity recognition task, namely P recision = T P/(T P + F P ) and Recall = T P/(T P +F N ), where TP, FP and FN are respectively the number of true positive, false positive, and false negative annotations, and F measure = 2 \u00d7 P recision \u00d7 Recall/(P recision + Recall), the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "entity recognition task", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.795852263768514}, {"text": "Recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9955745935440063}, {"text": "FP", "start_pos": 158, "end_pos": 160, "type": "METRIC", "confidence": 0.9384071230888367}, {"text": "FN", "start_pos": 165, "end_pos": 167, "type": "METRIC", "confidence": 0.6654703617095947}, {"text": "F measure", "start_pos": 266, "end_pos": 275, "type": "METRIC", "confidence": 0.9811387658119202}, {"text": "precision", "start_pos": 346, "end_pos": 355, "type": "METRIC", "confidence": 0.9988941550254822}, {"text": "recall", "start_pos": 360, "end_pos": 366, "type": "METRIC", "confidence": 0.9931354522705078}]}, {"text": "Additionally, the performance was evaluated considering both strict and relaxed, or overlap, matching of the gold standard annotations.", "labels": [], "entities": []}, {"text": "For the normalization task, the metric used to evaluate performance was accuracy.", "labels": [], "entities": [{"text": "normalization task", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.9274586737155914}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9987540245056152}]}, {"text": "Again, two matching methods were considered: strict accuracy was defined as the ratio between the number of correct identifiers assigned to the predicted entities, and the total number of entities manually annotated in the corpus; while relaxed accuracy measured the ratio between the number of correct Task A: Official results on the test dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.725425124168396}, {"text": "accuracy", "start_pos": 245, "end_pos": 253, "type": "METRIC", "confidence": 0.9942424893379211}]}, {"text": "The best results for each task and matching strategy are identified in bold.", "labels": [], "entities": []}, {"text": "The best run from all participating teams as well as the overall average are shown for comparison.", "labels": [], "entities": []}, {"text": "identifiers and the number of entities correctly predicted by the system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Official results on the test dataset. The best results for each task and matching strategy are  identified in bold. The best run from all participating teams as well as the overall average are shown for  comparison.", "labels": [], "entities": []}]}