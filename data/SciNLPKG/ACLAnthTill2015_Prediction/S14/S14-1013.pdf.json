{"title": [{"text": "Compositional Distributional Semantics Models in Chunk-based Smoothed Tree Kernels", "labels": [], "entities": []}], "abstractContent": [{"text": "The field of compositional distributional semantics has proposed very interesting and reliable models for accounting the distributional meaning of simple phrases.", "labels": [], "entities": []}, {"text": "These models however tend to disregard the syntactic structures when they are applied to larger sentences.", "labels": [], "entities": []}, {"text": "In this paper we propose the chunk-based smoothed tree kernels (CSTKs) as away to exploit the syntactic structures as well as the reliability of these compositional models for simple phrases.", "labels": [], "entities": []}, {"text": "We experiment with the recognizing textual entailment datasets.", "labels": [], "entities": []}, {"text": "Our experiments show that our CSTKs perform better than basic compositional dis-tributional semantic models (CDSMs) re-cursively applied at the sentence level, and also better than syntactic tree kernels.", "labels": [], "entities": []}], "introductionContent": [{"text": "A clear interaction between syntactic and semantic interpretations for sentences is important for many high-level NLP tasks, such as question-answering, textual entailment recognition, and semantic textual similarity.", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 153, "end_pos": 183, "type": "TASK", "confidence": 0.7772991359233856}, {"text": "semantic textual similarity", "start_pos": 189, "end_pos": 216, "type": "TASK", "confidence": 0.6125935216744741}]}, {"text": "Systems and models for these tasks often use classifiers or regressors that exploit convolution kernels) to model both interpretations.", "labels": [], "entities": []}, {"text": "Convolution kernels are naturally defined on spaces where there exists a similarity function between terminal nodes.", "labels": [], "entities": []}, {"text": "This feature has been used to integrate distributional semantics within tree kernels.", "labels": [], "entities": []}, {"text": "This class of kernels is often referred to as smoothed tree kernels (), yet, these models only use distributional vectors for words.", "labels": [], "entities": []}, {"text": "Compositional distributional semantics models (CDSMs) on the other hand are functions mapping text fragments to vectors (or higher-order tensors) which then provide a distributional meaning for simple phrases or sentences.", "labels": [], "entities": []}, {"text": "Many CDSMs have been proposed for simple phrases like nonrecursive noun phrases or verbal phrases.", "labels": [], "entities": []}, {"text": "Non-recursive phrases are often referred to as chunks, and thus, CDSMs are good and reliable models for chunks.", "labels": [], "entities": []}, {"text": "In this paper, we present the chunk-based smoothed tree kernels (CSTK) as away to merge the two approaches: the smoothed tree kernels and the models for compositional distributional semantics.", "labels": [], "entities": []}, {"text": "Our approach overcomes the limitation of the smoothed tree kernels which only use vectors for words by exploiting reliable CDSMs over chunks.", "labels": [], "entities": []}, {"text": "CSTKs are defined over a chunk-based syntactic subtrees where terminal nodes are words or word sequences.", "labels": [], "entities": []}, {"text": "We experimented with CSTKs on data from the recognizing textual entailment challenge () and we compared our CSTKs with other standard tree kernels and standard recursive CDSMs.", "labels": [], "entities": [{"text": "recognizing textual entailment challenge", "start_pos": 44, "end_pos": 84, "type": "TASK", "confidence": 0.7425525709986687}]}, {"text": "Experiments show that our CSTKs perform better than basic compositional distributional semantic models (CDSMs) recursively applied at the sentence level and better than syntactic tree kernels.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the CSTKs.", "labels": [], "entities": []}, {"text": "Section 3 reports on the experimental setting and on the results.", "labels": [], "entities": []}, {"text": "Finally, Section 4 draws the conclusions and sketches the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with the Recognizing Textual Entailment datasets (RTE) ().", "labels": [], "entities": [{"text": "Recognizing Textual Entailment datasets (RTE)", "start_pos": 25, "end_pos": 70, "type": "DATASET", "confidence": 0.6728353457791465}]}, {"text": "RTE is the task of deciding whether along text T entails a shorter text, typically a single sentence, called hypothesis H.", "labels": [], "entities": [{"text": "RTE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8333890438079834}]}, {"text": "It has been often seen as a classification task (see).", "labels": [], "entities": [{"text": "classification task", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.914148360490799}]}, {"text": "We used four datasets: RTE1, RTE2, RTE3, and RTE5, with the standard split between training and testing.", "labels": [], "entities": [{"text": "RTE3", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.7104654312133789}, {"text": "RTE5", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.7150914669036865}]}, {"text": "The dev/test distribution for RTE1-3, and RTE5 is respectively 567/800, 800/800, 800/800, and 600/600 T-H pairs.", "labels": [], "entities": [{"text": "RTE1-3", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.9036642909049988}, {"text": "RTE5", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9303536415100098}]}, {"text": "Distributional vectors are derived with DISSECT () from a corpus obtained by the concatenation of ukWaC (wacky.sslmit.unibo.it), a mid-2009 dump of the English Wikipedia (en.wikipedia.org) and the British National Corpus (www.natcorp.ox.ac.uk), fora total of about 2.8 billion words.", "labels": [], "entities": [{"text": "DISSECT", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.8209584951400757}, {"text": "ukWaC", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.9648854732513428}, {"text": "British National Corpus", "start_pos": 197, "end_pos": 220, "type": "DATASET", "confidence": 0.9354028503100077}]}, {"text": "We collected a 35K-by-35K matrix by counting co-occurrence of the 30K most frequent content lemmas in the corpus (nouns, adjectives and verbs) and all the content lemmas occurring in the datasets within a 3 word window.", "labels": [], "entities": []}, {"text": "The raw count vectors were transformed into positive Pointwise Mutual Information scores and reduced to 300 dimensions by Singular Value Decomposition.", "labels": [], "entities": []}, {"text": "This setup was picked without tuning, as we found it effective in previous, unrelated experiments.", "labels": [], "entities": []}, {"text": "We built the matrices for the full additive models using the procedure described in.", "labels": [], "entities": []}, {"text": "We considered only two relations: the Adjective-Noun and Verb-Noun.", "labels": [], "entities": []}, {"text": "The full additive model falls back to the basic additional model when syntactic relations are different from these two.", "labels": [], "entities": []}, {"text": "To build the final kernel to learn the classifier, we followed standard approaches (, that is, we exploited two models: a model with only a rewrite rule feature space (RR) and a model with the previous space along with a token-level similarity feature (RRTWS).", "labels": [], "entities": [{"text": "token-level similarity feature (RRTWS)", "start_pos": 221, "end_pos": 259, "type": "METRIC", "confidence": 0.6554485758145651}]}, {"text": "The two models use our CSTKs and the standard TKs in the following way as kernel functions: where T W S is a weighted token similarity (as in).", "labels": [], "entities": []}, {"text": "shows the results of the experiments, the table is organised as follows: columns 2-6 report the accuracy of the RTE systems based on rewrite rules (RR) and columns 7-11 report the accuracies of RR systems along with token similarity (RRTS).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9989977478981018}, {"text": "token similarity (RRTS)", "start_pos": 216, "end_pos": 239, "type": "METRIC", "confidence": 0.8031927227973938}]}, {"text": "We compare five differente models: ADD is the Basic Additive model with parameters \u03b1 = \u03b2 = 1 (as defined in 2.3) applied to the words of the sentence (without considering its tree structure), the same is done for the Full Additive (FullADD), defined as in 2.3.", "labels": [], "entities": []}, {"text": "The Tree Kernel (TK) as defined in) are applied to the constituency-based tree representation of the tree, without the intervening collapsing step described in 2.2.", "labels": [], "entities": []}, {"text": "These three models are the baseline against which we compare the CSTK models where the collapsing procedure is done via Basic Additive (CSTK + BA, again with \u03b1 = \u03b2 = 1) and FullAdditive (CSTK + FA), as described in section 2.2, again, with the aforementioned restriction on the relation considered.", "labels": [], "entities": [{"text": "BA", "start_pos": 143, "end_pos": 145, "type": "METRIC", "confidence": 0.6112984418869019}, {"text": "FullAdditive (CSTK + FA)", "start_pos": 173, "end_pos": 197, "type": "METRIC", "confidence": 0.6005041003227234}]}, {"text": "For RR models we have that CSTK+BA and CSTK+FA both achieve higher accuracy than ADD and FullAdd, with a statistical significante greater than 93.7%, as computed with the sign test.", "labels": [], "entities": [{"text": "BA", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.7502326369285583}, {"text": "FA", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.8263801336288452}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9988065958023071}, {"text": "FullAdd", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.8332396149635315}]}, {"text": "Specifically we have that CSTK+BA has an average accuracy 7.94% higher than ADD and 5.89% higher than FullADD, while CSTK+FA improves on ADD and FullADD by 8.52% and 6.46%, respectively.", "labels": [], "entities": [{"text": "BA", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.7913401126861572}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.998439371585846}, {"text": "FullADD", "start_pos": 102, "end_pos": 109, "type": "DATASET", "confidence": 0.6771494150161743}, {"text": "FA", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.7100780010223389}, {"text": "FullADD", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.8024793267250061}]}, {"text": "The same trend is visible for the RRTS model, again both models are statistically better than ADD and FullADD, in this case we have that CSTK+BA is 8.63% more accurate then ADD and 2.11% more than FullADD, CSTK+FA is respectively 8.98% and 2.43% more accurate than ADD and FullADD.", "labels": [], "entities": [{"text": "RRTS", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.6363887190818787}, {"text": "FullADD", "start_pos": 102, "end_pos": 109, "type": "DATASET", "confidence": 0.7848885655403137}, {"text": "BA", "start_pos": 142, "end_pos": 144, "type": "METRIC", "confidence": 0.600997805595398}, {"text": "FullADD", "start_pos": 197, "end_pos": 204, "type": "DATASET", "confidence": 0.8000882863998413}, {"text": "FA", "start_pos": 211, "end_pos": 213, "type": "METRIC", "confidence": 0.6985720992088318}, {"text": "FullADD", "start_pos": 273, "end_pos": 280, "type": "DATASET", "confidence": 0.9489272236824036}]}, {"text": "As for the TK models we have that both CSTK models achieve again an higher average accuracy: for RR models CSTK+BA and CSTK+FA are respectively 2.01% and 0.15% better than TK, while for RRTS models the number are 2.54% and 0.47%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.998803973197937}, {"text": "BA", "start_pos": 112, "end_pos": 114, "type": "METRIC", "confidence": 0.9221715331077576}, {"text": "FA", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.8946373462677002}]}, {"text": "These results though are not statistically significant, as is the difference between the two CSTK models themselves.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Task-based analysis: Accuracy on Recognizing Textual Entailment ( \u2020 is different from both ADD and", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9956319332122803}, {"text": "Recognizing Textual Entailment", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.6668020288149515}]}]}