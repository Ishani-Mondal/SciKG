{"title": [{"text": "USF: Chunking for Aspect Term Identification & Polarity Classification", "labels": [], "entities": [{"text": "USF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8984050154685974}, {"text": "Chunking", "start_pos": 5, "end_pos": 13, "type": "DATASET", "confidence": 0.7508454918861389}, {"text": "Aspect Term Identification & Polarity Classification", "start_pos": 18, "end_pos": 70, "type": "TASK", "confidence": 0.7253991017738978}]}], "abstractContent": [{"text": "This paper describes the systems submitted by the University of San Francisco (USF) to Semeval-2014 Task 4, Aspect Based Sentiment Analysis (ABSA), which provides labeled data in two domains, lap-tops and restaurants.", "labels": [], "entities": [{"text": "Aspect Based Sentiment Analysis (ABSA)", "start_pos": 108, "end_pos": 146, "type": "TASK", "confidence": 0.7528294410024371}]}, {"text": "For the constrained condition of both the aspect term extraction and aspect term polarity tasks, we take a supervised machine learning approach using a combination of lexical, syntactic, and baseline sentiment features.", "labels": [], "entities": [{"text": "aspect term extraction", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.6035517454147339}]}, {"text": "Our extraction approach is inspired by a chunk-ing approach, based on its strong past results on related tasks.", "labels": [], "entities": []}, {"text": "Our system performed slightly below average compared to other submissions, possibly because we use a simpler classification model than prior work.", "labels": [], "entities": []}, {"text": "Our polarity labeling approach uses two baseline hand-built sentiment classifiers as features in addition to lexical and syntactic features, and performed in the top ten of other constrained systems on both domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "As stated in the call for participation for this Semeval task, sentiment analysis focusing on overall polarity of a document, sentence, or similar context has been well studied in recent years ().", "labels": [], "entities": [{"text": "Semeval task", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.896964430809021}, {"text": "sentiment analysis", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.9607053697109222}]}, {"text": "However, there is less prior work examining finer levels of granularity associated with individual entities and their characteristics or attributes, which the organizers for this task call aspects.", "labels": [], "entities": []}, {"text": "The aspect based sentiment analysis This work is licenced under a Creative Commons Attribution 4.0 International License.", "labels": [], "entities": [{"text": "aspect based sentiment analysis", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.6548430100083351}]}, {"text": "Page numbers and proceedings footer are added by the organizers.", "labels": [], "entities": []}, {"text": "License details: http://creativecommons.org/licenses/ by/4.0/ task (ABSA) has the goal of identifying aspects of stated or implied target entities and the sentiment expressed towards each aspect.", "labels": [], "entities": []}, {"text": "This problem has not been deeply studied in prior literature due to the lack, until now, of a large gold standard dataset.", "labels": [], "entities": [{"text": "gold standard dataset", "start_pos": 100, "end_pos": 121, "type": "DATASET", "confidence": 0.7158281405766805}]}, {"text": "This Semeval task has provided two such datasets, in the domains of laptops and restaurants.", "labels": [], "entities": []}, {"text": "A full description of the task and data is presented with this volume (.", "labels": [], "entities": []}, {"text": "In this paper, we discuss our approach to the first two subtasks of the Semeval ABSA Task, those of aspect term extraction and aspect term polarity.", "labels": [], "entities": [{"text": "Semeval ABSA Task", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.8766922156016032}, {"text": "aspect term extraction", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.5914610028266907}]}, {"text": "In aspect term extraction the domain (restaurants or laptops) is known and the goal is to identify terms in a sentence that are features commonly associated with that domain, such as service and staff in the case of restaurants or size and speed in the case of laptops.", "labels": [], "entities": [{"text": "aspect term extraction", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.6405334571997324}, {"text": "speed", "start_pos": 240, "end_pos": 245, "type": "METRIC", "confidence": 0.8489212989807129}]}, {"text": "In the polarity subtask, the aspect terms fora given sentence are already identified and the sentiment polarity (positive, negative, conflict, or neutral) must be assigned.", "labels": [], "entities": []}, {"text": "We approach both subtasks using supervised machine learning with background knowledge of sentiment lexicons and syntax included in our feature set.", "labels": [], "entities": []}, {"text": "Our goal was to investigate whether techniques that have been successful in similar tasks would perform well on this newly created data set.", "labels": [], "entities": []}, {"text": "We did not use additional corpus-based resources, so qualified for the constrained (versus unconstrained) version of the task.", "labels": [], "entities": []}, {"text": "The remainder of the paper details related work, our approach, and experiments and the results we obtained.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we report our results and some additional analysis for the ABSA subtasks 1 and 2.", "labels": [], "entities": [{"text": "ABSA subtasks", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.8829160332679749}]}, {"text": "Please refer to for details on the tasks, corpora, and evaluation criteria.", "labels": [], "entities": []}, {"text": "We chose the constrained condition, which allows the use of sentiment lexicons in addition to the training data provided, but no additional data such as other reviews.", "labels": [], "entities": []}, {"text": "Aspect term extraction is evaluated using Precision, Recall, and F-measure on an unseen set of sentences.", "labels": [], "entities": [{"text": "Aspect term extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.744084636370341}, {"text": "Precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9984157085418701}, {"text": "Recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.977952778339386}, {"text": "F-measure", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9982477426528931}]}, {"text": "shows our results 1 on both domains, the top results, 2 and the mean score of all constrained submissions (21 entries).", "labels": [], "entities": [{"text": "mean score", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9650028944015503}]}, {"text": "Note that for Restaurants, COMMIT-P1WP3 had the best Precision, at 0.909, but XRCE had the best F-measure, so we show their three scores.", "labels": [], "entities": [{"text": "COMMIT-P1WP3", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.6145427823066711}, {"text": "Precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9991926550865173}, {"text": "XRCE", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.5836926102638245}, {"text": "F-measure", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9975946545600891}]}, {"text": "Our results were close to the mean for both corpora and quite a  bit above the lowest scoring submissions and the baseline provided by the organizers; the latter is also shown in the After the submission deadline, we continued to experiment with alternative approaches using 5-fold cross validation on the training set, shown in.", "labels": [], "entities": []}, {"text": "We found that using the full vocabulary was better than our original approach of only using the top 50% occurring words, even with 28% unseen words in the restaurant test set and 21% in laptops.", "labels": [], "entities": []}, {"text": "We also found that leaving out the polarity feature while using all vocabulary words (FVNo-Snt) improved our F-measure score to 0.669 for laptops but reduced it slightly to 0.745 for restaurants.", "labels": [], "entities": [{"text": "FVNo-Snt", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.5939076542854309}, {"text": "F-measure score", "start_pos": 109, "end_pos": 124, "type": "METRIC", "confidence": 0.9864863753318787}]}, {"text": "Finally, using IO versus IOB tagging did not influence the F-measure significantly.", "labels": [], "entities": [{"text": "IO versus IOB tagging", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.5784257650375366}, {"text": "F-measure", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9894223809242249}]}, {"text": "About 25% of the aspect terms in the restaurant training set have length greater than one, and 37% of the laptop terms.", "labels": [], "entities": [{"text": "length", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.969505786895752}]}, {"text": "Aspect term polarity is evaluated on accuracy overall labels: positive, negative, neutral, or conflict.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9916490316390991}]}, {"text": "shows our results on both domains, the top results, the mean score of all constrained submissions (24 entries for laptops, 28 for restaurants), and the baseline accuracy.", "labels": [], "entities": [{"text": "mean score", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9491501152515411}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9875906705856323}]}, {"text": "In this case our scores are above average in all cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Aspect Term Extraction Results, Con- strained.", "labels": [], "entities": [{"text": "Aspect Term Extraction", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6329078872998556}]}, {"text": " Table 2: Aspect Term Extraction Cross-Validation  Results.", "labels": [], "entities": [{"text": "Aspect Term Extraction", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6868156294027964}]}]}