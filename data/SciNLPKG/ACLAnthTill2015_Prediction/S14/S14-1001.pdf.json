{"title": [{"text": "More or less supervised supersense tagging of Twitter", "labels": [], "entities": [{"text": "supersense tagging", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7619963884353638}]}], "abstractContent": [{"text": "We present two Twitter datasets annotated with coarse-grained word senses (super-senses), as well as a series of experiments with three learning scenarios for super-sense tagging: weakly supervised learning , as well as unsupervised and supervised domain adaptation.", "labels": [], "entities": [{"text": "super-sense tagging", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.6739462912082672}]}, {"text": "We show that (a) off-the-shelf tools perform poorly on Twitter, (b) models augmented with em-beddings learned from Twitter data perform much better, and (c) errors can be reduced using type-constrained inference with distant supervision from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 242, "end_pos": 249, "type": "DATASET", "confidence": 0.9690561890602112}]}], "introductionContent": [{"text": "Supersense tagging) is the task of assigning high-level ontological classes to open-class words (here, nouns and verbs).", "labels": [], "entities": [{"text": "Supersense tagging", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8035962879657745}]}, {"text": "It is thus a coarse-grained word sense disambiguation task.", "labels": [], "entities": [{"text": "word sense disambiguation task", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.7645982354879379}]}, {"text": "The labels are based on the lexicographer file names for Princeton WordNet).", "labels": [], "entities": [{"text": "Princeton WordNet", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.9086377322673798}]}, {"text": "They include 15 senses for verbs and 26 for nouns (see).", "labels": [], "entities": []}, {"text": "While WordNet also provides catch-all supersenses for adjectives and adverbs, these are grammatically, not semantically motivated, and do not provide any higherlevel abstraction (recently, however, proposed a semantic taxonomy for adjectives).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 6, "end_pos": 13, "type": "DATASET", "confidence": 0.9412216544151306}]}, {"text": "They will not be considered in this paper.", "labels": [], "entities": []}, {"text": "Coarse-grained categories such as supersenses are useful for downstream tasks such as questionanswering (QA) and open relation extraction (RE).", "labels": [], "entities": [{"text": "open relation extraction (RE)", "start_pos": 113, "end_pos": 142, "type": "TASK", "confidence": 0.7902156213919321}]}, {"text": "SST is different from NER in that it has a larger set of labels and in the absence of strong orthographic cues (capitalization, quotation marks, etc.).", "labels": [], "entities": [{"text": "SST", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9559465646743774}]}, {"text": "Moreover, supersenses can be applied to any of the lexical parts of speech and not only proper names.", "labels": [], "entities": []}, {"text": "Also, while high-coverage gazetteers can be found for named entity recognition, the lexical resources available for SST are very limited in coverage.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.6707005500793457}, {"text": "SST", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9833875894546509}]}, {"text": "Twitter is a popular micro-blogging service, which, among other things, is used for knowledge sharing among friends and peers.", "labels": [], "entities": []}, {"text": "Twitter posts (tweets) announce local events, say talks or concerts, present facts about pop stars or programming languages, or simply express the opinions of the author on some subject matter.", "labels": [], "entities": [{"text": "Twitter posts (tweets) announce local events, say talks or concerts, present facts about pop stars or programming languages, or simply express the opinions of the author on some subject matter", "start_pos": 0, "end_pos": 192, "type": "Description", "confidence": 0.7267482714993613}]}, {"text": "Supersense tagging is relevant for Twitter, because it can aid e.g. QA and open RE.", "labels": [], "entities": [{"text": "Supersense tagging", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7697239816188812}]}, {"text": "If someone posts a message saying that some LaTeX module now supports \"drawing trees\", it is important to know whether the post is about drawing natural objects such as oaks or pines, or about drawing tree-shaped data representations.", "labels": [], "entities": []}, {"text": "This paper is, to the best of our knowledge, the first work to address the problem of SST for Twitter.", "labels": [], "entities": [{"text": "SST", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9885390400886536}]}, {"text": "While there exist corpora of newswire and literary texts that are annotated with supersenses, e.g., SEMCOR, no data is available for microblogs or related domains.", "labels": [], "entities": [{"text": "SEMCOR", "start_pos": 100, "end_pos": 106, "type": "DATASET", "confidence": 0.8359224200248718}]}, {"text": "This paper introduces two new data sets.", "labels": [], "entities": []}, {"text": "Furthermore, most, if not all, of previous work on SST has relied on gold standard part-of-speech (POS) tags as input.", "labels": [], "entities": [{"text": "SST", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9906395673751831}]}, {"text": "However, in a domain such as Twitter, which has proven to be challenging for POS tagging, results obtained under the assumption of available perfect POS information are almost meaningless for any real-life application.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.9251939356327057}]}, {"text": "In this paper, we instead use predicted POS tags and investigate experimental settings in which one or more of the following resources are available to us: \u2022 a large corpus of unlabeled Twitter data; \u2022 Princeton WordNet); \u2022 SEMCOR (; and \u2022 a small corpus of Twitter data annotated with supersenses.", "labels": [], "entities": [{"text": "Princeton WordNet", "start_pos": 202, "end_pos": 219, "type": "DATASET", "confidence": 0.8664852380752563}, {"text": "SEMCOR", "start_pos": 224, "end_pos": 230, "type": "DATASET", "confidence": 0.6617984771728516}]}, {"text": "We approach SST of Twitter using various degrees of supervision for both learning and domain adaptation (here, from newswire to Twitter).", "labels": [], "entities": [{"text": "SST", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9733251929283142}, {"text": "domain adaptation", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7522709667682648}]}, {"text": "In 1 weakly supervised learning, only unlabeled data and the lexical resource WordNet are available to us.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9717879891395569}]}, {"text": "While the quality of lexical resources varies, this is the scenario for most languages.", "labels": [], "entities": []}, {"text": "We present an approach to weakly supervised SST based on type-constrained EM-trained second-order HMMs (HMM2s) with continuous word representations.", "labels": [], "entities": [{"text": "SST", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.830419659614563}]}, {"text": "In contrast, when using supervised learning, we can distinguish between two degrees of supervision for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.7595420479774475}]}, {"text": "For some languages, e.g., Basque, English, Swedish, sense-annotated resources exist, but these corpora are all limited to newswire or similar domains.", "labels": [], "entities": []}, {"text": "In such languages, unsupervised domain adaptation (DA) techniques can be used to exploit these resources.", "labels": [], "entities": [{"text": "unsupervised domain adaptation (DA)", "start_pos": 19, "end_pos": 54, "type": "TASK", "confidence": 0.8129256963729858}]}, {"text": "The setting does not presume labeled data from the target domain.", "labels": [], "entities": []}, {"text": "We use discriminative models for unsupervised domain adaptation, training on SEMCOR and testing on Twitter.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.758672684431076}, {"text": "SEMCOR", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.8723784685134888}]}, {"text": "Finally, we annotated data sets for Twitter, making supervised domain adaptation (SU) experiments possible.", "labels": [], "entities": [{"text": "supervised domain adaptation (SU)", "start_pos": 52, "end_pos": 85, "type": "TASK", "confidence": 0.7940054436524709}]}, {"text": "For supervised domain adaptation, we use the annotated training data sets from both the newswire and the Twitter domain, as well as WordNet.", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6916893919308981}, {"text": "WordNet", "start_pos": 132, "end_pos": 139, "type": "DATASET", "confidence": 0.9809657335281372}]}, {"text": "For both unsupervised domain adaptation and supervised domain adaptation, we use structured perceptron), i.e., a discriminative HMM model, and search-based structured prediction (SEARN) ().", "labels": [], "entities": []}, {"text": "We augment both the EM-trained HMM2, discriminative HMMs and SEARN with type constraints and continuous word representations.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9410927295684814}]}, {"text": "We also experimented with conditional random fields (), but obtained worse or similar results than with the other models.", "labels": [], "entities": []}, {"text": "Contributions In this paper, we present two Twitter data sets with manually annotated supersenses, as well as a series of experiments with these data sets.", "labels": [], "entities": [{"text": "Twitter data sets", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.8186868826548258}]}, {"text": "These experiments cover existing approaches to related tasks, as well as some new methods.", "labels": [], "entities": []}, {"text": "In particular, we present type-constrained extensions of discriminative HMMs and SEARN sequence models with continuous word representations that perform well.", "labels": [], "entities": []}, {"text": "We show that when no in-domain labeled data is available, type constraints improve model performance considerably.", "labels": [], "entities": []}, {"text": "Our best models achieve a weighted average F1 score of 57.1 over nouns and verbs on our main evaluation data set, i.e., a 20% error reduction over the most frequent sense baseline.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9807644486427307}, {"text": "error", "start_pos": 126, "end_pos": 131, "type": "METRIC", "confidence": 0.95329350233078}]}, {"text": "The two annotated Twitter data sets are publicly released for download at https://github.com/coastalcph/ supersense-data-twitter.", "labels": [], "entities": [{"text": "Twitter data sets", "start_pos": 18, "end_pos": 35, "type": "DATASET", "confidence": 0.8337480227152506}]}], "datasetContent": [{"text": "We experiment with weakly supervised learning, unsupervised domain adaptation, as well as supervised domain adaptation, i.e., where our models are induced from hand-annotated newswire and Twitter data.", "labels": [], "entities": []}, {"text": "Note that in all our experiments, we use predicted POS tags as input to the system, in order to produce a realistic estimate of SST performance.", "labels": [], "entities": [{"text": "SST", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.9804369807243347}]}, {"text": "We also experimented with the more coarsegrained classes proposed by.", "labels": [], "entities": []}, {"text": "Here our best model obtained an F 1 score for mental concepts (nouns) of 72.3%, and 62.6% for physical concepts, on RITTER-DEV.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9890881180763245}, {"text": "RITTER-DEV", "start_pos": 116, "end_pos": 126, "type": "DATASET", "confidence": 0.8502597212791443}]}, {"text": "The overall F 1 score for verbs is 85.6%.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9916963974634806}]}, {"text": "The overall F 1 is 75.5%.", "labels": [], "entities": [{"text": "F 1", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9829282462596893}]}, {"text": "Note that this result is not directly comparable to the figure (72.9%) reported in, since they use different data sets, exclude verbs and make different assumptions, e.g., relying on gold POS tags.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Properties of dataset.", "labels": [], "entities": []}, {"text": " Table 2: Weighted F1 average over 41 supersenses.", "labels": [], "entities": [{"text": "Weighted", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.931110143661499}, {"text": "F1 average", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9301944673061371}]}]}