{"title": [{"text": "NTNU: Measuring Semantic Similarity with Sublexical Feature Representations and Soft Cardinality", "labels": [], "entities": [{"text": "NTNU", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8798919916152954}]}], "abstractContent": [{"text": "The paper describes the approaches taken by the NTNU team to the SemEval 2014 Semantic Textual Similarity shared task.", "labels": [], "entities": [{"text": "SemEval 2014 Semantic Textual Similarity shared task", "start_pos": 65, "end_pos": 117, "type": "TASK", "confidence": 0.9110098992075238}]}, {"text": "The solutions combine measures based on lexical soft cardinality and character n-gram feature representations with lexical distance metrics from TakeLab's base-line system.", "labels": [], "entities": []}, {"text": "The final NTNU system is based on bagged support vector machine regression over the datasets from previous shared tasks and shows highly competitive performance, being the best system on three of the datasets and third best overall (on weighted mean overall six datasets).", "labels": [], "entities": [{"text": "NTNU", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.8534411787986755}]}], "introductionContent": [{"text": "The Semantic Textual Similarity (STS) shared task aims at providing a unified framework for evaluating textual semantic similarity, ranging from exact semantic equivalence to completely unrelated texts.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) shared task", "start_pos": 4, "end_pos": 49, "type": "TASK", "confidence": 0.805626168847084}]}, {"text": "This is represented by the prediction of a similarity score between two sentences, drawn from a particular category of text, which ranges from 0 (different topics) to 5 (exactly equivalent) through six grades of semantic similarity.", "labels": [], "entities": []}, {"text": "This paper describes the NTNU submission to the SemEval 2014 STS shared task (Task 10).", "labels": [], "entities": [{"text": "SemEval 2014 STS shared task", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.7041482567787171}]}, {"text": "The approach is based on the lexical and distributional features of the baseline TakeLab system from the 2012 shared task ( \u02c7 Sari\u00b4c), but improves on it in three ways: by adding two new categories of features and by using a bagging regression model to predict similarity scores.", "labels": [], "entities": [{"text": "TakeLab system from the 2012 shared task ( \u02c7 Sari\u00b4c)", "start_pos": 81, "end_pos": 133, "type": "DATASET", "confidence": 0.7200703241608359}]}, {"text": "The new feature categories added are based on soft cardinality and character n-grams, described in Section 2.", "labels": [], "entities": []}, {"text": "The parameters of the two categories are optimised over several corpora and the features are combined through support vector regression (Section 3) to create the actual systems (Section 4).", "labels": [], "entities": []}, {"text": "As Section 5 shows, the new measures give the baseline system a substantial boost, leading to very competitive results in the shared task evaluation.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Training-test set pairs.", "labels": [], "entities": []}, {"text": " Table 2. The character n-gram repre- sentation vectors were trained in an unsupervised  manner on two subsets of Wikipedia consisting,  respectively, of the first 12 million words (10 8  characters, hence referred to as Wiki8) and of 125  million words (10 9 characters; Wiki9).  First, however, the training data had to be pre- processed. Thus, before extracting the idf weights  and the soft cardinality features, all the texts  shown in", "labels": [], "entities": []}, {"text": " Table 4: Final evaluation results for the submitted systems.", "labels": [], "entities": []}]}