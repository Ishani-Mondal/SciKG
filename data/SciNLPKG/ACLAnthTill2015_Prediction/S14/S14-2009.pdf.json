{"title": [{"text": "SemEval-2014 Task 9: Sentiment Analysis in Twitter", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.9503933787345886}]}], "abstractContent": [{"text": "We describe the Sentiment Analysis in Twitter task, ran as part of SemEval-2014.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter task", "start_pos": 16, "end_pos": 50, "type": "TASK", "confidence": 0.9284235835075378}]}, {"text": "It is a continuation of the last year's task that ran successfully as part of SemEval-2013.", "labels": [], "entities": []}, {"text": "As in 2013, this was the most popular SemEval task; a total of 46 teams contributed 27 submissions for subtask A (21 teams) and 50 submissions for subtask B (44 teams).", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.9217853248119354}]}, {"text": "This year, we introduced three new test sets: (i) regular tweets, (ii) sarcastic tweets, and (iii) LiveJournal sentences.", "labels": [], "entities": []}, {"text": "We further tested on (iv) 2013 tweets, and (v) 2013 SMS messages.", "labels": [], "entities": []}, {"text": "The highest F1-score on (i) was achieved by NRC-Canada at 86.63 for subtask A and by TeamX at 70.96 for subtask B.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9989774227142334}, {"text": "NRC-Canada", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.9177323579788208}, {"text": "TeamX", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.9406708478927612}]}], "introductionContent": [{"text": "In the past decade, new forms of communication have emerged and have become ubiquitous through social media.", "labels": [], "entities": []}, {"text": "Microblogs (e.g., Twitter), Weblogs (e.g., LiveJournal) and cellphone messages (SMS) are often used to share opinions and sentiments about the surrounding world, and the availability of social content generated on sites such as Twitter creates new opportunities to automatically study public opinion.", "labels": [], "entities": []}, {"text": "Working with these informal text genres presents new challenges for natural language processing beyond those encountered when working with more traditional text genres such as newswire.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.7059403459231058}]}, {"text": "The language in social media is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genrespecific terminology and abbreviations, e.g., RT for re-tweet and #hashtags  Moreover, tweets and SMS messages are short: a sentence or a headline rather than a document.", "labels": [], "entities": []}, {"text": "How to handle such challenges so as to automatically mine and understand people's opinions and sentiments has only recently been the subject of research ().", "labels": [], "entities": []}, {"text": "Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA newswire corpus (), the movie reviews corpus (), or the restaurant and laptop reviews corpora that are part of this year's SemEval Task 4 ().", "labels": [], "entities": [{"text": "MPQA newswire corpus", "start_pos": 106, "end_pos": 126, "type": "DATASET", "confidence": 0.9682877659797668}, {"text": "SemEval Task 4", "start_pos": 234, "end_pos": 248, "type": "TASK", "confidence": 0.8643999695777893}]}, {"text": "These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they do not focus on tweets.", "labels": [], "entities": []}, {"text": "While some Twitter sentiment datasets were created prior to SemEval-2013, they were either small and proprietary, such as the isieve corpus) or focused solely on message-level sentiment.", "labels": [], "entities": []}, {"text": "Thus, the primary goal of our SemEval task is to promote research that will lead to better understanding of how sentiment is conveyed in Social Media.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.9058621227741241}]}, {"text": "Toward that goal, we created the SemEval Tweet corpus as part of our inaugural Sentiment Analysis in Twitter Task,.", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter Task", "start_pos": 79, "end_pos": 113, "type": "TASK", "confidence": 0.8470135450363159}]}, {"text": "It contains tweets and SMS messages with sentiment expressions annotated with contextual phrase-level and messagelevel polarity.", "labels": [], "entities": []}, {"text": "This year, we extended the corpus by adding new tweets and LiveJournal sentences.", "labels": [], "entities": []}, {"text": "Another interesting phenomenon that has been studied in Twitter is the use of the #sarcasm hashtag to indicate that a tweet should not betaken literally (.", "labels": [], "entities": []}, {"text": "In fact, sarcasm indicates that the message polarity should be flipped.", "labels": [], "entities": []}, {"text": "With this in mind, this year, we also evaluate on sarcastic tweets.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we first describe the task, the dataset creation process and the evaluation methodology.", "labels": [], "entities": []}, {"text": "We then summarize the characteristics of the approaches taken by the participating systems, and we discuss their scores.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the process of collecting and annotating the 2014 testing tweets, including the sarcastic ones, and LiveJournal sentences.", "labels": [], "entities": []}, {"text": "For training and development, we released the Twitter train/dev/test datasets from SemEval-2013 task 2, as well as the SMS test set, which uses messages from the NUS SMS corpus, which we annotated for sentiment in 2013.", "labels": [], "entities": [{"text": "Twitter train/dev/test datasets from SemEval-2013 task 2", "start_pos": 46, "end_pos": 102, "type": "DATASET", "confidence": 0.6889591948552565}, {"text": "SMS test set", "start_pos": 119, "end_pos": 131, "type": "DATASET", "confidence": 0.7082651058832804}, {"text": "NUS SMS corpus", "start_pos": 162, "end_pos": 176, "type": "DATASET", "confidence": 0.9092053572336832}]}, {"text": "We further added anew 2014 Twitter test set, as well as a small set of tweets that contained the #sarcasm hashtag to determine how sarcasm affects the tweet polarity.", "labels": [], "entities": [{"text": "2014 Twitter test set", "start_pos": 22, "end_pos": 43, "type": "DATASET", "confidence": 0.8600282371044159}]}, {"text": "Finally, we included sentences from LiveJournal in order to determine how systems trained on Twitter perform on other sources.", "labels": [], "entities": []}, {"text": "The statistics for each dataset and for each subtask are shown in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset statistics for Subtask A.", "labels": [], "entities": [{"text": "Subtask A", "start_pos": 33, "end_pos": 42, "type": "TASK", "confidence": 0.8790077567100525}]}, {"text": " Table 2: Dataset statistics for Subtask B.", "labels": [], "entities": [{"text": "Subtask B", "start_pos": 33, "end_pos": 42, "type": "TASK", "confidence": 0.9262873828411102}]}, {"text": " Table 4: Results for subtask A. The  *  indicates system resubmissions (because they initially trained on  Twitter2013-test), and the indicates a system that includes a task co-organizer as a team member. The  systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets  are indicated with a subscript. The last two columns show macro-and micro-averaged results across the  three 2014 test datasets.", "labels": [], "entities": [{"text": "Twitter2013-test", "start_pos": 108, "end_pos": 124, "type": "DATASET", "confidence": 0.9593098163604736}, {"text": "Twitter2014 test dataset", "start_pos": 252, "end_pos": 276, "type": "DATASET", "confidence": 0.9671867887179056}]}, {"text": " Table 5: Results for subtask B. The  *  indicates system resubmissions (because they initially trained on  Twitter2013-test), and the indicates a system that includes a task co-organizer as a team member. The  systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets  are indicated with a subscript. The last two columns show macro-and micro-averaged results across the  three 2014 test datasets.", "labels": [], "entities": [{"text": "Twitter2013-test", "start_pos": 108, "end_pos": 124, "type": "DATASET", "confidence": 0.9588587880134583}, {"text": "Twitter2014 test dataset", "start_pos": 252, "end_pos": 276, "type": "DATASET", "confidence": 0.9661623438199362}]}]}