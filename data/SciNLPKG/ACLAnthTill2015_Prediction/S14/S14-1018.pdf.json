{"title": [{"text": "Cognitive Compositional Semantics using Continuation Dependencies", "labels": [], "entities": [{"text": "Cognitive Compositional Semantics", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7734351555506388}]}], "abstractContent": [{"text": "This paper describes a graphical semantic representation based on bottom-up 'con-tinuation' dependencies which has the important property that its vertices define a usable set of discourse referents in working memory even in contexts involving conjunction in the scope of quantifiers.", "labels": [], "entities": []}, {"text": "An evaluation on an existing quantifier scope disambiguation task shows that non-local continuation dependencies can be as reliably learned from annotated data as representations used in a state-of-the-art quanti-fier scope resolver, suggesting that continuation dependencies may provide a natural representation for scope information.", "labels": [], "entities": [{"text": "quantifier scope disambiguation task", "start_pos": 29, "end_pos": 65, "type": "TASK", "confidence": 0.7205229252576828}]}], "introductionContent": [{"text": "It is now fairly well established that at least shallow semantic interpretation informs parsing decisions inhuman sentence processing), and recent evidence points to incremental processing of quantifier implicatures as well).", "labels": [], "entities": []}, {"text": "This may indicate that inferences about the meaning of quantifiers are processed directly in working memory.", "labels": [], "entities": []}, {"text": "Human working memory is widely assumed to store events (including linguistic events) as re-usable activation-based states, connected by a durable but rapidly mutable weight-based memory of cued associations).", "labels": [], "entities": []}, {"text": "Complex dependency structures can therefore be stored in this associative memory as graphs, with states as vertices and cued associations as directed edges (e.g..", "labels": [], "entities": []}, {"text": "This kind of representation is necessary to formulate and evaluate algorithmic claims about cued associations and working memory use inhuman sentence processing (e.g. van . But accounting for syntax and semantics in this way must be done carefully in order to preserve linguistically important distinctions.", "labels": [], "entities": []}, {"text": "For example, positing spurious local dependencies in filler-gap constructions can lead to missed integrations of dependency structure in incremental processing, resulting in weaker model fitting ).", "labels": [], "entities": []}, {"text": "Similar care maybe necessary in cases of dependencies arising from anaphoric coreference or quantifier scope.", "labels": [], "entities": []}, {"text": "Unfortunately, most existing theories of compositional semantics) are defined at the computational level, employing beta reduction over complete or underspecified lambda calculus expressions as a precise description of the language processing task to be modeled, not at the algorithmic level, as a model of human language processing itself.", "labels": [], "entities": []}, {"text": "The structured expressions these theories generate are not intended to represent re-usable referential states of the sort that could be modeled in current theories of associative memory.", "labels": [], "entities": []}, {"text": "As such, it should not be surprising that structural adaptations of lambda calculus expressions as referential states exhibit a number of apparent deficiencies: First, representations based on lambda calculus expressions lack topologically distinguishable referents for sets defined in the context of outscoping quantifiers.", "labels": [], "entities": []}, {"text": "For example, a structural adaptation of a lambda calculus expression for the sentence Every line contains two numbers, shown in Figure 1a), contains referents for the set of all document lines (s L ) and for the set of all numbers (s N ) which can be identified by cued associations to predicate constants like Number, but it is not clear how a referent for the set of numbers in document lines can be distinguished from a referent for the set of numbers a) Figure 1: Semantic dependency graph in a 'direct' (top-down) style, adapted from a disambiguated representation of, excluding quantifiers over eventualities.", "labels": [], "entities": []}, {"text": "The semantic dependency structure for the sentence Every line contains two numbers (a), with flat logical form (c), is not a subgraph of the semantic dependency structure for Every line begins with a space and contains two numbers (b), because the structure is interrupted by the explicit conjunction predicate 'And'.", "labels": [], "entities": []}, {"text": "in each document line (s N ) using local topological features of the dependency graph, as would be required to accurately recall assertions about total or average quantities of numbers in document lines.", "labels": [], "entities": []}, {"text": "Second, graphs based on traditional lambda calculus representations do not model conjuncts as subgraphs of conjunctions.", "labels": [], "entities": []}, {"text": "For example, the graphical representation of the sentence Every line This graph matching can be implemented in a vectorial model of associative memory by comparing the (e.g. cosine) similarity of superposed vectors resulting from cueing incoming and outgoing dependencies with all possible labels in increasingly longer paths from one or more constant vector states (e.g. vectors for predicate constants).", "labels": [], "entities": []}, {"text": "This graph matching does not necessarily preclude the introduction of monotonicity constraints from matched quantifiers.", "labels": [], "entities": []}, {"text": "For example, More than two perl scripts work, can entail More than two scripts work, using a subgraph in the first argument, but Fewer than two scripts work, can entail Fewer than two perl scripts work, using a supergraph in the first argument.", "labels": [], "entities": []}, {"text": "This consideration is similar to those observed in representations based on natural logic) which also uses low-level matching to perform some kinds of inference, but representations based on natural logic typically exclude other forms of inference, whereas the present model does not.", "labels": [], "entities": []}, {"text": "This matching also assumes properties of nuclear scope variables are inherited from associated restrictor variables, e.g. through a set of dependencies from nuclear scope sets to restrictor sets not shown in the figure.", "labels": [], "entities": []}, {"text": "This assumption will be revisited in Section 3.", "labels": [], "entities": []}, {"text": "begins with a space and contains two numbers shown in does not contain the graphical representation of the sentence Every line contains two numbers shown in as a connected subgraph.", "labels": [], "entities": []}, {"text": "Although one might expect a query about a conjunct to be directly answerable from a knowledge base containing the conjoined representation, the pattern of dependencies that makeup the conjunct in a graphical representation of a lambda calculus expression does not match those in the larger conjunction.", "labels": [], "entities": []}, {"text": "Finally, representations based on lambda calculus expressions contain vertices that do not seem to correspond to viable discourse referents.", "labels": [], "entities": []}, {"text": "For example, following the sentence Every line contains two numbers, using the lambda expression shown in, d L may serve as a referent of it in but it has only one underscore, s N may serve as a referent of they in but they are not negative, e C may serve as a referent of that in but that was before it was edited, and p L may serve as a referent of that in but the compiler doesn't enforce that, but it is not clear what if anything would naturally refer to the internal conjunction p A . Predications over such conjunctions (e.g. Kim believes that every line begins with a space and contains two numbers) are usually predicated at the outer proposition p L , and in any case do not have truth values that are independent of the same predication at each conjunct.", "labels": [], "entities": []}, {"text": "One of the goals of Minimal Recursion Semantics () was to eliminate similar kinds of superfluous conjunction structure.", "labels": [], "entities": [{"text": "Minimal Recursion Semantics", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.8953956961631775}]}, {"text": "Fortunately, lambda calculus expressions like those shown in are not the only way to represent compositional semantics of sentences.", "labels": [], "entities": []}, {"text": "This paper defines a graphical semantic dependency representation that can be translated into lambda calculus, but has the important property that its vertices define a usable set of discourse referents in working memory even in contexts involving conjunction in the scope of quantifiers.", "labels": [], "entities": []}, {"text": "It does this by reversing the direction of dependencies from parent-to-child subsumption in a lambda-calculus tree to a representation similar to the inside-out structure of function definitions in a continuation-passing style) 2 so that sets are defined in terms of their context, and explicit 'And' predicates are no longer required, leaving nothing to get in the way of an exact pattern match.", "labels": [], "entities": []}, {"text": "The learnability of the non-local continuation dependencies involved in this representation is then evaluated on an existing quantifier scope disambiguation task using a dependency-based statistical scope resolver, with results comparable to a state-of-the-art unrestricted graph-based quantifier scope resolver ().", "labels": [], "entities": [{"text": "quantifier scope disambiguation task", "start_pos": 125, "end_pos": 161, "type": "TASK", "confidence": 0.7728433609008789}]}], "datasetContent": [{"text": "This paper defines a graphical semantic representation with desirable properties for storing sentence meanings as cued associations in associative memory.", "labels": [], "entities": []}, {"text": "In order to determine whether this representation of continuation dependencies is reliably learnable, the set of test sentences from the QuanText corpus ) was automatically annotated with these continuation dependencies and evaluated against the associated set of gold-standard quantifier scopes.", "labels": [], "entities": [{"text": "QuanText corpus", "start_pos": 137, "end_pos": 152, "type": "DATASET", "confidence": 0.9444270431995392}]}, {"text": "The sentences in this corpus were collected as descriptions of text editing tasks using unix tools like sed and awk, collected from online tutorials and from graduate students asked to write and describe example scripts.", "labels": [], "entities": []}, {"text": "Gold-standard scoping relations in this corpus are specified over bracketed sequences of words in each sentence.", "labels": [], "entities": []}, {"text": "For example, the sentence Print every line that starts with a number might be annotated: scoping relations: 1 > 2 meaning that the quantifier over lines, referenced in constituent 1, outscopes the quantifier over numbers, referenced in constituent 2.", "labels": [], "entities": []}, {"text": "In order to isolate the learnablility of the continuation dependencies described in this paper, both training and test sentences of this corpus were annotated with hand-corrected GCG derivations which are then used to obtain semantic dependencies as described in Section 4.", "labels": [], "entities": []}, {"text": "Continuation dependencies are then inferred from these semantic dependencies using the algorithm described in Section 5.", "labels": [], "entities": []}, {"text": "Goldstandard scoping relations are considered successfully recalled if a restrictor (f 1 (f 1 i)) or nuclear scope (f 2 (f 1 i)) referent of any lexical item i within the outscoped span is connected by a sequence of continuation dependencies (in the appropriate direction) to any restrictor or nuclear scope referent of any lexical item within the outscoping span.", "labels": [], "entities": []}, {"text": "First, the algorithm was run without any lexicalization on the 94 non-duplicate sentences of the QuanText test set.", "labels": [], "entities": [{"text": "QuanText test set", "start_pos": 97, "end_pos": 114, "type": "DATASET", "confidence": 0.94034210840861}]}, {"text": "Results of this evaluation are shown in the third line of using the persentence complete recall accuracy ('AR') defined by.", "labels": [], "entities": [{"text": "persentence complete recall accuracy ('AR')", "start_pos": 68, "end_pos": 111, "type": "METRIC", "confidence": 0.8035008353846413}]}, {"text": "The algorithm was then run using bilexical weights based on the frequencies\u02dcFfrequencies\u02dc frequencies\u02dcF(h, h ) with which a word h occurs as ahead of a category outscoped by a category headed byword h in the 350-sentence training set of the QuanText corpus.", "labels": [], "entities": [{"text": "Ffrequencies", "start_pos": 76, "end_pos": 88, "type": "METRIC", "confidence": 0.9515851140022278}, {"text": "F", "start_pos": 102, "end_pos": 103, "type": "METRIC", "confidence": 0.9519932866096497}, {"text": "QuanText corpus", "start_pos": 241, "end_pos": 256, "type": "DATASET", "confidence": 0.9478424191474915}]}, {"text": "For example, since quantifiers over lines are often outscoped by quantifiers over files in the training data, the system learns to rank continuation dependencies to referents associated with the word lines ahead of continuation dependencies to referents associated with the word files in bottomup inference.", "labels": [], "entities": []}, {"text": "These lexical features maybe particularly helpful because continuation dependencies are generated only between directly adjacent sets.", "labels": [], "entities": []}, {"text": "Results for scope disambiguation using these rankings are shown in the fourth line of.", "labels": [], "entities": [{"text": "scope disambiguation", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.8472136855125427}]}, {"text": "This increase is statistically significant (p = 0.001 by two-tailed McNemar's test).", "labels": [], "entities": []}, {"text": "This significance for local head-word features on continuation dependencies shows that these dependencies can be reliably learned from training examples, and suggests that continuation dependencies maybe a natural representation for scope information.", "labels": [], "entities": []}, {"text": "Interestingly, effects of lexical features for quantifiers (the word each, or definite/indefinite distinctions) were not substantial or statistically significant, despite the relatively high frequencies System AR Manshadi and Allen (2011) baseline 63% 72% This system, w/o lexicalized model 61% This system, w. lexicalized model 72%: Per-sentence complete recall accuracy ('AR') of tree-based algorithm as compared to  and on explicit NP chunks in the QuanText test set, correcting for use of gold standard trees as described in footnote 19 of. of the words each and the in the test corpus (occurring in 16% and 68% of test sentences, respectively), which suggests that these words may often be redundant with syntactic and head-word constraints.", "labels": [], "entities": [{"text": "Per-sentence complete recall accuracy ('AR')", "start_pos": 334, "end_pos": 378, "type": "METRIC", "confidence": 0.7368267178535461}, {"text": "QuanText test set", "start_pos": 452, "end_pos": 469, "type": "DATASET", "confidence": 0.9673336744308472}]}, {"text": "Results using preferences that rank referents quantified by the word each after other referents achieve a numerical increase inaccuracy over a model with no preferences (up 5 points, to 66%), but it is not statistically significant (p = .13).", "labels": [], "entities": []}, {"text": "Results using preferences that rank referents quantified by the word the after other referents achieve a numerical increase inaccuracy over a model with no preferences (up 1 point, to 62%), but this is even less significant (p = 1).", "labels": [], "entities": []}, {"text": "Results are even weaker in combination with head-word features (up 1 point, to 73%, for each; down two points, to 70%, for the).", "labels": [], "entities": []}, {"text": "This suggests that world knowledge (in the form of head-word information) maybe more salient to quantifier scope disambiguation than many intuitive linguistic preferences.", "labels": [], "entities": [{"text": "quantifier scope disambiguation", "start_pos": 96, "end_pos": 127, "type": "TASK", "confidence": 0.8302149573961893}]}], "tableCaptions": []}