{"title": [{"text": "FBK-TR: Applying SVM with Multiple Linguistic Features for Cross-Level Semantic Similarity", "labels": [], "entities": [{"text": "FBK-TR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8255519866943359}]}], "abstractContent": [{"text": "Recently, the task of measuring semantic similarity between given texts has drawn much attention from the Natural Language Processing community.", "labels": [], "entities": [{"text": "measuring semantic similarity between given texts", "start_pos": 22, "end_pos": 71, "type": "TASK", "confidence": 0.7958589792251587}]}, {"text": "Especially , the task becomes more interesting when it comes to measuring the semantic similarity between different-sized texts, e.g paragraph-sentence, sentence-phrase, phrase-word, etc.", "labels": [], "entities": []}, {"text": "In this paper, we, the FBK-TR team, describe our system participating in Task 3 \"Cross-Level Semantic Similarity\", at SemEval 2014.", "labels": [], "entities": [{"text": "FBK-TR", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.9428752064704895}, {"text": "Cross-Level Semantic Similarity\"", "start_pos": 81, "end_pos": 113, "type": "TASK", "confidence": 0.7055185213685036}]}, {"text": "We also report the results obtained by our system, compared to the baseline and other participating systems in this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Measuring semantic text similarity has become a hot trend in NLP as it can be applied to other tasks, e.g. Information Retrieval, Paraphrasing, Machine Translation Evaluation, Text Summarization, Question and Answering, and others.", "labels": [], "entities": [{"text": "semantic text similarity", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6148009498914083}, {"text": "Information Retrieval", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.8308257460594177}, {"text": "Paraphrasing", "start_pos": 130, "end_pos": 142, "type": "TASK", "confidence": 0.9484029412269592}, {"text": "Machine Translation Evaluation", "start_pos": 144, "end_pos": 174, "type": "TASK", "confidence": 0.8694883386294047}, {"text": "Text Summarization", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.8060131072998047}, {"text": "Question and Answering", "start_pos": 196, "end_pos": 218, "type": "TASK", "confidence": 0.7318912943204244}]}, {"text": "Several approaches proposed to measure the semantic similarity between given texts.", "labels": [], "entities": []}, {"text": "The first approach is based on vector space models (VSMs).", "labels": [], "entities": []}, {"text": "A VSM transforms given texts into \"bagof-words\" and presents them as vectors.", "labels": [], "entities": []}, {"text": "Then, it deploys different distance metrics to compute the closeness between vectors, which will return as the distance or similarity between given texts.", "labels": [], "entities": []}, {"text": "The next well-known approach is using text-alignment.", "labels": [], "entities": []}, {"text": "By assuming that two given texts are semantically similar, they could be aligned on word or phrase levels.", "labels": [], "entities": []}, {"text": "The alignment quality can serve as a similarity measure.", "labels": [], "entities": [{"text": "similarity", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9731143712997437}]}, {"text": "\"It typically pairs words from the two texts by maximizing the summation of the word similarity of the resulting pairs\" ().", "labels": [], "entities": []}, {"text": "In contrast, the third approach uses machine learning techniques to learn models built from different lexical, semantic and syntactic features and then give predictions on degree of similarity between given texts).", "labels": [], "entities": []}, {"text": "At SemEval 2014, the Task 3 \"Cross-Level Semantic Similarity\" () is to evaluate the semantic similarity across different sizes of texts, in particular, a larger-sized text is compared to a smaller-sized one.", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 3, "end_pos": 15, "type": "TASK", "confidence": 0.7727370262145996}, {"text": "Cross-Level Semantic Similarity", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.6322554647922516}]}, {"text": "The task consists of four types of semantic similarity comparison: paragraph to sentence, sentence to phrase, phrase to word, and word to sense.", "labels": [], "entities": [{"text": "semantic similarity comparison", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.678385595480601}]}, {"text": "The degree of similarity ranges from 0 (different meanings) to 4 (similar meanings).", "labels": [], "entities": [{"text": "similarity", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9020130634307861}]}, {"text": "For evaluation, systems were evaluated, first, within comparison type and second, across all comparison types.", "labels": [], "entities": []}, {"text": "Two methods are used to evaluate between system outputs and gold standard (human annotation), which are Pearson correlation and Spearman's rank correlation (rho).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 104, "end_pos": 123, "type": "METRIC", "confidence": 0.9349389672279358}, {"text": "Spearman's rank correlation (rho)", "start_pos": 128, "end_pos": 161, "type": "METRIC", "confidence": 0.8189014366694859}]}, {"text": "The FBK-TR team participated in this task with three different runs.", "labels": [], "entities": [{"text": "FBK-TR", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.7750979661941528}]}, {"text": "In this paper, we present a clear and comprehensive description of our system which obtained competitive results.", "labels": [], "entities": []}, {"text": "Our main approach is using machine learning technique to learn models from different lexical and semantic features from train corpora to make prediction on the test corpora.", "labels": [], "entities": []}, {"text": "We used support vector machine (SVM) regression model to solve the task.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the system overview.", "labels": [], "entities": []}, {"text": "Sections 3, 4 and 5 describe the Semantic Word Similarity, String Similarity and other features, respectively.", "labels": [], "entities": [{"text": "Semantic Word Similarity", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.6544657647609711}, {"text": "String Similarity", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.6077200919389725}]}, {"text": "Section 6 discusses about SVM approach.", "labels": [], "entities": []}, {"text": "Section 7 presents the experiment settings for each subtask.", "labels": [], "entities": []}, {"text": "Finally, Sections 8 and 9 present the evaluation and conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "For subtasks paragraph-to-sentence and sentenceto-phrase, since the length between two units is completely different, we decided, first to apply topic model to identify if two given texts are expressing a same topic.", "labels": [], "entities": []}, {"text": "Furthermore, named entities play an important role in these subtasks.", "labels": [], "entities": []}, {"text": "However, as there are many named entities which are not English words and cannot be identified by the NER tool, we developed a program to detect and identify common words occurring in both given texts.", "labels": [], "entities": []}, {"text": "Then we continued to extract other lexical and semantic features to measure the similarity between the two texts.: Results for sentence-to-phrase.", "labels": [], "entities": []}, {"text": "For the subtask word-to-sense, we used the Semantic Word Similarity model which consists of three components: WordNet similarity, Wikipedia relatedness and LSA similarity (described in section 3).", "labels": [], "entities": [{"text": "Semantic Word Similarity", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.566819409529368}, {"text": "LSA similarity", "start_pos": 156, "end_pos": 170, "type": "METRIC", "confidence": 0.9247296750545502}]}, {"text": "For phrase-to-word, we extracted all glosses of the given word, then computed the similarity between the given phrase and each extracted gloss.", "labels": [], "entities": []}, {"text": "Finally, we selected the highest similarity score for result.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 33, "end_pos": 49, "type": "METRIC", "confidence": 0.9788815081119537}]}, {"text": "As a result, we report our performance in the four subtasks as follows.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for paragraph-to-sentence.", "labels": [], "entities": []}, {"text": " Table 2: Results for sentence-to-phrase.", "labels": [], "entities": []}, {"text": " Table 3: Overall result using Pearson.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.49643608927726746}]}, {"text": " Table 4: Overall result using Spearman.", "labels": [], "entities": []}]}