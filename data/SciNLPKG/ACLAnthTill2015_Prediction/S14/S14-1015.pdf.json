{"title": [{"text": "See No Evil, Say No Evil: Description Generation from Densely Labeled Images", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper studies generation of descriptive sentences from densely annotated images.", "labels": [], "entities": [{"text": "generation of descriptive sentences from densely annotated images", "start_pos": 19, "end_pos": 84, "type": "TASK", "confidence": 0.8600588738918304}]}, {"text": "Previous work studied generation from automatically detected visual information but produced a limited class of sentences , hindered by currently unreliable recognition of activities and attributes.", "labels": [], "entities": [{"text": "generation from automatically detected visual information", "start_pos": 22, "end_pos": 79, "type": "TASK", "confidence": 0.8295473357041677}]}, {"text": "Instead , we collect human annotations of objects , parts, attributes and activities in images.", "labels": [], "entities": []}, {"text": "These annotations allow us to build a significantly more comprehensive model of language generation and allow us to study what visual information is required to generate human-like descriptions.", "labels": [], "entities": [{"text": "language generation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7454505562782288}]}, {"text": "Experiments demonstrate high quality output and that activity annotations and relative spatial location of objects contribute most to producing high quality sentences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Image descriptions compactly summarize complex visual scenes.", "labels": [], "entities": []}, {"text": "For example, consider the descriptions of the image in, which vary in content but focus on the women and what they are doing.", "labels": [], "entities": []}, {"text": "Automatically generating such descriptions is challenging: a full system must understand the image, select the relevant visual content to present, and construct complete sentences.", "labels": [], "entities": []}, {"text": "Existing systems aim to address all of these challenges but use visual detectors for only a small vocabulary of words, typically nouns, associated with objects that can be reliably found.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collected a dataset of richly annotated images to approximate gold standard visual recognition.", "labels": [], "entities": [{"text": "gold standard visual recognition", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.6281088292598724}]}, {"text": "In collecting the data, we sought a visual annotation with sufficient coverage to support the generation of as many of the words in the original image descriptions as possible.", "labels": [], "entities": []}, {"text": "We also aimed to make it as visually exhaustive as possible-giving equal treatment to all visible objects.", "labels": [], "entities": []}, {"text": "This ensures less bias from annotators' perception about which objects are important, since one of the problems we would like to solve is content selection.", "labels": [], "entities": [{"text": "content selection", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.6825626641511917}]}, {"text": "This dataset will be available for future experiments.", "labels": [], "entities": []}, {"text": "We built on the dataset from (Rashtchian et al., 2010) which contained 8,000 Flickr images and associated descriptions gathered using Amazon Mechanical Turk (MTurk).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk)", "start_pos": 134, "end_pos": 164, "type": "DATASET", "confidence": 0.8818248411019644}]}, {"text": "Restricting ourselves to Creative Commons images, we sampled 500 images for annotation.", "labels": [], "entities": []}, {"text": "We collected annotations of images in three stages using MTurk, and assigned each annotation task to 3-5 workers to improve quality through redundancy).", "labels": [], "entities": [{"text": "MTurk", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.8574396967887878}]}, {"text": "Below we describe the process for annotating a single image.", "labels": [], "entities": []}, {"text": "Stage 1: We prompted five turkers to list all objects in an image, ignoring objects that are parts of larger objects (e.g., the arms of a person), which we collected later in Stage 3.", "labels": [], "entities": []}, {"text": "This list also included groups, such as crowds of people.", "labels": [], "entities": []}, {"text": "Stage 2: For each unique object label from Stage 1, we asked two turkers to draw a polygon around the object identified.", "labels": [], "entities": []}, {"text": "In cases where the object is a group, we also asked for the number of objects present (1-6 or many).", "labels": [], "entities": []}, {"text": "Finally, we created a list of all references to the object from the first stage, which we call the Object facet.", "labels": [], "entities": []}, {"text": "Stage 3: For each objector group, we prompted three turkers to provide descriptive phrases of: \u2022 Doing -actions the object participates in, e.g. \"jumping.\"", "labels": [], "entities": []}, {"text": "\u2022 Parts -physical parts e.g. \"legs\", or other items in the possession of the object e.g. \"shirt.\"", "labels": [], "entities": []}, {"text": "\u2022 Attributes -adjectives describing the object, e.g. \"red.\"", "labels": [], "entities": []}, {"text": "\u2022 Isa -alternative names fora object e.g. \"boy\", \"rider.\" shows more examples for objects We modified LabelMe ().", "labels": [], "entities": []}, {"text": "We refer to all of these annotations, including the merged Object labels, as facets.", "labels": [], "entities": []}, {"text": "These labels provide feature norms (), which have recently used as a visual proxy in distributional semantics) but have not been previous studied for generation.", "labels": [], "entities": []}, {"text": "This annotation of 500 images (2500 sentences) yielded over 4000 object instances and 100,000 textual labels.", "labels": [], "entities": []}, {"text": "Data We used 70% of the data for training (1750 sentences, 350 images), 15% for development, and 15% for testing (375 sentences, 75 images).", "labels": [], "entities": []}, {"text": "Parameters The regularization parameter was set on the held out data tor = 8.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9422426223754883}]}, {"text": "The reranker candidate list included the top 500 sentences for each sentence length up to 15 and weights were optimized with Z-MERT (Zaidan, 2009).", "labels": [], "entities": []}, {"text": "Metrics Our evaluation is based on BLEU-n (), which considers all ngrams up to length n.", "labels": [], "entities": [{"text": "BLEU-n", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.99819415807724}]}, {"text": "To assess human performance using BLEU, we score each of the five references against the four other ones and finally average the five BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9901666641235352}, {"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9988439083099365}]}, {"text": "In order to make these results comparable to BLEU scores for our model and baselines, we perform the same five-fold averaging when computing BLEU for each system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9992221593856812}, {"text": "BLEU", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9970099925994873}]}, {"text": "We also compute accuracy for different syntactic positions in the sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9992565512657166}]}, {"text": "We look at a number of categories: the main clause's components (S,V,O), prepositional phrase components, the preposition (Pp) and their objects (Po) and noun modifying words (N), including determiners.", "labels": [], "entities": []}, {"text": "Phrases match if they have an exact string match and share context identifiers as defined in the features sections.", "labels": [], "entities": []}, {"text": "Human Evaluation Annotators rated sentences output by our full model against either human or a baseline system generated descriptions.", "labels": [], "entities": []}, {"text": "Three criteria were evaluated: grammaticality, which sentence is more complete and well formed; truthfulness, which sentence is more accurately capturing something true in the image; and salience, which sentence is capturing important things in the image while still being concise.", "labels": [], "entities": []}, {"text": "Two annotators annotated all test pairs for all criteria fora given pair of systems.", "labels": [], "entities": []}, {"text": "Six annotators were used (none authors) and agreement was high (Cohen's kappa = 0.963, 0.823 and 0.703 for grammar, truth and salience).", "labels": [], "entities": [{"text": "agreement", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9988300204277039}]}, {"text": "Machine Translation Baseline The first baseline is designed to see if it is possible to generate good sentences from the facet string labels alone, with no visual information.", "labels": [], "entities": [{"text": "Machine Translation Baseline", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.781444787979126}]}, {"text": "We use an extension of phrase-based machine translation techniques ().", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.6518312195936838}]}, {"text": "We created a virtual bitext by pairing each image description (the target sentence) with a sequence 10 of visual identifiers (the source \"sentence\") listing strings from the facet labels.", "labels": [], "entities": []}, {"text": "Since phrases produced by turkers lack many of the functions words needed to create fluent sentences, we added one of 47 function words either at the start or the end of each output phrase.", "labels": [], "entities": []}, {"text": "The translation model included standard features such as language model score (using our caption language model described previously), word count, phrase count, linear distortion, and the count of deleted source words.", "labels": [], "entities": []}, {"text": "We also define three features that count the number of Object, Isa, and Doing phrases, to learn a preference for types of phrases.", "labels": [], "entities": []}, {"text": "The feature weights are tuned with MERT to maximize BLEU-4.", "labels": [], "entities": [{"text": "MERT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9953742623329163}, {"text": "BLEU-4", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9975149631500244}]}, {"text": "Midge Baseline As described in related work, the Midge system creates a set of sentences to describe everything in an input image.", "labels": [], "entities": []}, {"text": "These sen- We defined a consistent ordering of visual identifiers and set the distortion limit of the phrase-based decoder to infinity.", "labels": [], "entities": []}, {"text": "tences must all be true, but do not have to select the same content that a person would.", "labels": [], "entities": []}, {"text": "It can be adapted to our task by adding object selection and sentence ranking rules.", "labels": [], "entities": []}, {"text": "For object selection, we choose the three most frequently named objects in the scene according to a background corpus of image descriptions.", "labels": [], "entities": [{"text": "object selection", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8612522482872009}]}, {"text": "For sentence selection, we take all sentences within one word of the average length of a sentence in our corpus, 11, and select the one with best Midge generation score.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7881759703159332}, {"text": "Midge generation", "start_pos": 146, "end_pos": 162, "type": "TASK", "confidence": 0.5130394697189331}]}], "tableCaptions": [{"text": " Table 1: Results for the test set for the BLEU1-4 metrics.", "labels": [], "entities": [{"text": "BLEU1-4", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9902100563049316}]}, {"text": " Table 2: Human evaluation of our Full-Model in heads", "labels": [], "entities": []}, {"text": " Table 3: Ablation results on development data using BLEU1-4 and reporting match accuracy for sentence structures.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9892248511314392}, {"text": "BLEU1-4", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9714267253875732}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.8253081440925598}]}]}