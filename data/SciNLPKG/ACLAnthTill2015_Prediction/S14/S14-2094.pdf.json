{"title": [{"text": "Sensible: L2 Translation Assistance by Emulating the Manual Post-Editing Process", "labels": [], "entities": [{"text": "Translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.8807904124259949}]}], "abstractContent": [{"text": "This paper describes the Post-Editor Z system submitted to the L2 writing assistant task in SemEval-2014.", "labels": [], "entities": [{"text": "L2 writing assistant task in SemEval-2014", "start_pos": 63, "end_pos": 104, "type": "TASK", "confidence": 0.6931657940149307}]}, {"text": "The aim of task is to build a translation assistance system to translate untranslated sentence fragments.", "labels": [], "entities": [{"text": "translate untranslated sentence fragments", "start_pos": 63, "end_pos": 104, "type": "TASK", "confidence": 0.844092383980751}]}, {"text": "This is not unlike the task of post-editing where human translators improve machine-generated translations.", "labels": [], "entities": []}, {"text": "Post-Editor Z emulates the manual process of post-editing by (i) crawling and extracting parallel sentences that contain the untranslated fragments from a Web-based translation memory, (ii) extracting the possible translations of the fragments indexed by the translation memory and (iii) applying simple cosine-based sentence similarity to rank possible translations for the un-translated fragment.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we present a collaborative submission between Saarland University and Nanyang Technological University to the L2 Translation Assistant task in SemEval-2014.", "labels": [], "entities": [{"text": "Nanyang Technological University", "start_pos": 85, "end_pos": 117, "type": "DATASET", "confidence": 0.9239150087038676}, {"text": "L2 Translation Assistant task", "start_pos": 125, "end_pos": 154, "type": "TASK", "confidence": 0.7328122705221176}]}, {"text": "Our team name is Sensible and the participating system is PostEditor Z (PEZ).", "labels": [], "entities": []}, {"text": "The L2 Translation Assistant task concerns the translation of an untranslated fragment from a partially translated sentence.", "labels": [], "entities": [{"text": "L2 Translation Assistant", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6627507706483206}, {"text": "translation of an untranslated fragment from a partially translated sentence", "start_pos": 47, "end_pos": 123, "type": "TASK", "confidence": 0.8233721613883972}]}, {"text": "For instance, given a sentence, \"Ich konnte B\u00e4rbel noch on the border in einen letzten S-Bahn-Zug nach Westberlin setzen.\", the aim is to provide an appropriate translation for the underline phrase, i.e. an der Grenze.", "labels": [], "entities": []}, {"text": "The aim of the task is not unlike the task of post-editing where human translators correct errors provided by machine-generated translations.", "labels": [], "entities": []}, {"text": "The main difference is that in the context of postediting the source text is provided.", "labels": [], "entities": []}, {"text": "A translation workflow that incorporates post-editing begins with a source sentence, e.g. \"I could still sit on the border in the very last tram to West Berlin.\" and the human translator is provided with a machine-generated translation with untranslated fragments such as the previous example and sometimes \"fixing\" the translation would simply require substituting the appropriate translation for the untranslated fragment.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of the task is based on three metrics, viz.", "labels": [], "entities": []}, {"text": "absolute accuracy (acc), word-based accuracy (wac) and recall (rec).", "labels": [], "entities": [{"text": "accuracy (acc)", "start_pos": 9, "end_pos": 23, "type": "METRIC", "confidence": 0.902633473277092}, {"text": "accuracy (wac)", "start_pos": 36, "end_pos": 50, "type": "METRIC", "confidence": 0.9265441298484802}, {"text": "recall (rec)", "start_pos": 55, "end_pos": 67, "type": "METRIC", "confidence": 0.9649915993213654}]}, {"text": "Absolute accuracy measures the number of fragments that match the gold translation of the untranslated fragments.", "labels": [], "entities": [{"text": "Absolute", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9670157432556152}, {"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.8651660680770874}]}, {"text": "Word-based accuracy assigns a score according to the longest consecutive matching substring between output fragment and reference fragment; it is computed as such: wac = |longestmatch(output,ref erence)| max(|output|,|ref erence|) Recall accounts for the number of fragments for which output was given (regardless of whether it was correct).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9640781879425049}, {"text": "longestmatch(output,ref erence)| max", "start_pos": 171, "end_pos": 207, "type": "METRIC", "confidence": 0.7327619940042496}, {"text": "Recall", "start_pos": 231, "end_pos": 237, "type": "METRIC", "confidence": 0.538435161113739}]}, {"text": "presents the results for the best evaluation scores of the PEZ system runs for the English to German (en-de), English to Spanish (enes), French to English (fr-en) and Dutch to English (nl-en) evaluations.", "labels": [], "entities": [{"text": "PEZ system", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.694441631436348}]}, {"text": "presents the word accuracy of the system runs for both best and out-offive (oof) evaluation 2 .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9714300036430359}]}], "tableCaptions": [{"text": " Table 1: Results for Best Evaluation of the System Runs.", "labels": [], "entities": []}]}