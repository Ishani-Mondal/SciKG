{"title": [{"text": "One-Level Phonology: Autosegmental Representations and Rules as Finite Automata", "labels": [], "entities": []}], "abstractContent": [{"text": "When phonological rules are regarded as declarative descriptions, it is possible to construct a model of phonology in which rules and representations are no longer distinguished and such procedural devices as rule-ordering are absent.", "labels": [], "entities": []}, {"text": "In this paper we present a finite-state model of phonology in which automata are the descriptions and tapes (or strings) are the objects being described.", "labels": [], "entities": []}, {"text": "This provides the formal semantics for an autosegmental phonology without structure-changing rules.", "labels": [], "entities": []}, {"text": "Logical operations on the phonological domain-such as conjunction, disjunction, and negation-make sense since the phonological domain consists of descriptions rather than objects.", "labels": [], "entities": []}, {"text": "These operations as applied to automata are the straightforward operations of intersection, union, and complement.", "labels": [], "entities": []}, {"text": "If the arrow in a rewrite rule is viewed as logical implication, then a phonological rule can also be represented as an automaton, albeit a less restrictive automaton than would be required fora lexical representation.", "labels": [], "entities": []}, {"text": "The model is then compared with the transducer models for autosegmental phonology of Kay (1987), Kornai (1991), and Wiebe (1992).", "labels": [], "entities": []}, {"text": "We conclude that the declarative approach to phonology presents an attractive way of extending finite-state techniques to autosegmental phonology while remaining within the confines of regular grammar.", "labels": [], "entities": []}], "introductionContent": [{"text": "The decade since the publication of and since the development of the KIMMO system has witnessed a spectacular flurry of activity as the linguistic and computational consequences of this work have been fleshed out.", "labels": [], "entities": []}, {"text": "A considerable body of literature has grownup around TWO-LEVEL MOR-PHOLOGY, along with texts 1 and implementations.", "labels": [], "entities": [{"text": "MOR-PHOLOGY", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.6237000823020935}]}, {"text": "The existence of a rule compiler has made it possible for the linguist to work at a conveniently abstract level, and analyses of several languages now exemplify the approach.", "labels": [], "entities": []}, {"text": "Today, two-level morphology encompasses much of traditional segmental generative phonology of the SPE variety (.", "labels": [], "entities": []}, {"text": "Although further development and application of this model is set to continue for sometime, there is now a clear need to integrate it more closely with computational grammar frameworks on the one hand and modern nonlinear phonology on the other.", "labels": [], "entities": []}, {"text": "The primary goal of this article is to show how the central tenets of autosegmental phonology translate into an implemented finite state model.", "labels": [], "entities": []}, {"text": "The model is named ONE-LEVEL PHONOLOGY for two reasons.", "labels": [], "entities": [{"text": "ONE-LEVEL PHONOLOGY", "start_pos": 19, "end_pos": 38, "type": "METRIC", "confidence": 0.6818189024925232}]}, {"text": "First, the model is monostratal, in that there is only one level of linguistic description.", "labels": [], "entities": []}, {"text": "Second, the name is intended to contrast with models employing two levels (such as the FST model mentioned above) or three levels, or an unbounded number of levels (.", "labels": [], "entities": [{"text": "FST model", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.8943031430244446}]}, {"text": "The one-level model represents the outgrowth of three independent strands of research: (i) the finitestate modeling of phonology, (ii) the declarative approach to phonology, 4 and (iii) the automatic learning of phonological generalizations.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents an overview of autosegmental phonology and the temporal semantics of.", "labels": [], "entities": []}, {"text": "Then we define state-labeled automata (Section 3.1), show their equivalence to finite state automata (Section 3.2), define the operations of concatenation, union, intersection, and complement (Section 3.3), and further define state-labeled transducers (Section 3.4).", "labels": [], "entities": []}, {"text": "The central proposals of the paper are contained in Section 4.", "labels": [], "entities": []}, {"text": "We show how autosegmental association can be interpreted in terms of the synchronization of two automata, where each automaton specifies an autosegmental tier.", "labels": [], "entities": []}, {"text": "We now give a brief foretaste of this procedure.", "labels": [], "entities": [{"text": "foretaste", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9923143982887268}]}, {"text": "Suppose that we have the autosegmental diagram in (1), encoding high (hi) and round (rnd) autosegments.", "labels": [], "entities": []}, {"text": "+hi:l -hi:2 -rnd: 2 +rnd: 1 From this encoding, we can write down the following regular expression.", "labels": [], "entities": []}, {"text": "Although such expressions will be opaque at this early stage of the exposition, it suffices to note here that each line of the expression represents a tier and the tiers are combined using the intersection operation (m).", "labels": [], "entities": []}, {"text": "Moreover, the ls act as synchronization marks between the operands of the intersection operation.", "labels": [], "entities": []}, {"text": "(+hi, 0)* (+hi, 1) (+hi, 0)* (-hi, 0)* (-hi, 1)(-hi, 0)* (-hi, 1)(-hi, 0)* m (-rnd, 0)* (-rnd, 1) (-rnd, 0)* (-rnd, 1) (-rnd, 0)* (+rnd, 0)* (+rnd, 1) (+rnd, 0)* The final step is to compute the intersection and project the first element of each tuple (ignoring the ls and 0s).", "labels": [], "entities": []}, {"text": "This produces the expression: (+hi A -rnd) + (-hi N -rnd) + (-hi n +rnd) +.", "labels": [], "entities": []}, {"text": "Given plausible interpretations of the high and round features, this last expression simplifies to i+a+o +, which describes an automaton tape (or a string) divided into three nonempty intervals, the first containing, the second containing, and the third containing.", "labels": [], "entities": []}, {"text": "This, we shall claim, is the intended interpretation of (1).", "labels": [], "entities": []}, {"text": "After a detailed discussion of this procedure, the remainder of Section 4 is given over to generalizing the procedure to an arbitrary number of autosegmental charts, an evaluation of the encoding with respect to Kornai's desiderata (Section 4.5), and a presentation of the encoding of autosegmental rules (Section 4.6).", "labels": [], "entities": []}, {"text": "Finally, Section 5 compares our proposals with those of,, and.", "labels": [], "entities": []}, {"text": "While our model has regular grammar power and is fully implemented, these three models go beyond regular grammar power and to our knowledge have never been implemented.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}