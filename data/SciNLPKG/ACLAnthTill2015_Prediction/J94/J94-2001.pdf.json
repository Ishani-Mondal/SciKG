{"title": [{"text": "Tagging English Text with a Probabilistic Model", "labels": [], "entities": [{"text": "Tagging English Text", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9049988985061646}]}], "abstractContent": [{"text": "In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence.", "labels": [], "entities": []}, {"text": "The main novelty of these experiments is the use of untagged text in the training of the model.", "labels": [], "entities": []}, {"text": "We have used a simple triclass Marlcov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided.", "labels": [], "entities": []}, {"text": "Two approaches in particular are compared and combined: \u2022 using text that has been tagged by hand and computing relative frequency counts, \u2022 using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle.", "labels": [], "entities": []}, {"text": "Experiments show that the best training is obtained by using as much tagged text as possible.", "labels": [], "entities": []}, {"text": "They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9232367873191833}]}, {"text": "In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9993307590484619}]}], "introductionContent": [], "datasetContent": [{"text": "The main objective of this paper is to compare RF and ML training.", "labels": [], "entities": [{"text": "RF", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.8544071912765503}, {"text": "ML training", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.8987820148468018}]}, {"text": "This is done in Section 7.2.", "labels": [], "entities": []}, {"text": "We also take advantage of the environment that we have setup to perform other experiments, described in Section 7.3, that have some theoretical interest, but did We shall begin by describing the textual data that we are using, before presenting the different tagging experiments using these various training and tagging methods.", "labels": [], "entities": []}, {"text": "In this experiment, we extracted N tagged sentences from the training data.", "labels": [], "entities": []}, {"text": "We then computed the relative frequencies on these sentences and built a \"smoothed\" model using the procedure previously described.", "labels": [], "entities": []}, {"text": "This model was then used to tag the 2,000 test sentences.", "labels": [], "entities": []}, {"text": "We experimented with different values of N, for each of which we indicate the value of the interpolation coefficient and the number and percentage of correctly tagged words.", "labels": [], "entities": []}, {"text": "As expected, as the size of the training increases, the interpolation coefficient increases and the quality of the tagging improves.", "labels": [], "entities": [{"text": "interpolation coefficient", "start_pos": 56, "end_pos": 81, "type": "METRIC", "confidence": 0.9652320444583893}]}, {"text": "When N = 0, the model is made up of uniform distributions.", "labels": [], "entities": []}, {"text": "In this case, all alignments fora sentence are equally probable, so that the choice of the correct tag is just a choice at random.", "labels": [], "entities": []}, {"text": "However, the percentage of correct tags is relatively high (more than three out of four) because: \u2022 almost half of the words of the text have a single possible tag, so that no mistake can be made on these words \u2022 about a quarter of the words of the text have only two possible tags so that, on the average, a random choice is correct every other time.", "labels": [], "entities": []}, {"text": "Note that this behavior is obviously very dependent on the system of tags that is used.", "labels": [], "entities": []}, {"text": "It can be noted that reasonable results are obtained quite rapidly.", "labels": [], "entities": []}, {"text": "Using 2,000 tagged sentences (less than 50,000 words), the tagging error rate is already less than 5%.", "labels": [], "entities": [{"text": "tagging error rate", "start_pos": 59, "end_pos": 77, "type": "METRIC", "confidence": 0.8491864800453186}]}, {"text": "Using 10 times as much data (20,000 tagged sentences) provides an improvement of only 1.5%.", "labels": [], "entities": []}, {"text": "For this experiment we considered the initial model built by RF training over the whole training data and all the successive models created by the iterations of ML training.", "labels": [], "entities": [{"text": "ML training", "start_pos": 161, "end_pos": 172, "type": "TASK", "confidence": 0.8359599113464355}]}, {"text": "For each of these models we performed Viterbi tagging and ML tagging on the same test data, then evaluated and compared the number of tagging errors produced by these two methods.", "labels": [], "entities": [{"text": "Viterbi tagging", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.6849238574504852}, {"text": "ML tagging", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.8987398743629456}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The models obtained at different iterations are related, so one should not draw strong conclusions about the definite superiority of one tagging procedure.", "labels": [], "entities": []}, {"text": "However, the difference in error rate is very small, and shows that the choice of the tagging procedure is not as critical as the kind of training material.", "labels": [], "entities": [{"text": "error rate", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9845082759857178}, {"text": "tagging", "start_pos": 86, "end_pos": 93, "type": "TASK", "confidence": 0.9588101506233215}]}], "tableCaptions": [{"text": " Table 1  RF training on N sentences, Viterbi tagging.", "labels": [], "entities": [{"text": "RF", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9645216464996338}, {"text": "N sentences", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.8051308989524841}, {"text": "Viterbi tagging", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.629508912563324}]}, {"text": " Table 2  ML training from various initial points.", "labels": [], "entities": [{"text": "ML", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9928708672523499}]}, {"text": " Table 3  Viterbi vs. ML tagging.", "labels": [], "entities": [{"text": "ML tagging", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.7813408076763153}]}, {"text": " Table 4  Standard ML vs. tw-constrained ML training.", "labels": [], "entities": [{"text": "ML", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.7522563338279724}, {"text": "ML", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9416443109512329}]}]}