{"title": [{"text": "Weakly Supervised Approaches for Ontology Population", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a weakly supervised approach to automatic Ontology Population from text and compare it with other two unsu-pervised approaches.", "labels": [], "entities": [{"text": "Ontology Population", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.9280827939510345}]}, {"text": "In our experiments we populate apart of our ontology of Named Entities.", "labels": [], "entities": []}, {"text": "We considered two high level categories-geographical locations and person names and ten sub-classes for each category.", "labels": [], "entities": []}, {"text": "For each sub-class, from a list of training examples and a syntactically parsed corpus, we automatically learn a syntactic model-a set of weighted syntactic features, i.e. words which typically co-occur in certain syntactic positions with the members of that class.", "labels": [], "entities": []}, {"text": "The model is then used to classify the unknown Named Entities in the test set.", "labels": [], "entities": []}, {"text": "The method is weakly supervised, since no annotated corpus is used in the learning process.", "labels": [], "entities": []}, {"text": "We achieved promising results, i.e. 65% accuracy , outperforming significantly previous unsupervised approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9994858503341675}]}], "introductionContent": [{"text": "Automatic Ontology Population (OP) from texts has recently emerged as anew field of application for knowledge acquisition techniques (see, among others, ().", "labels": [], "entities": [{"text": "Automatic Ontology Population (OP)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.5333765546480814}, {"text": "knowledge acquisition", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.8095517754554749}]}, {"text": "Although there is no a univocally accepted definition for the OP task, a useful approximation has been suggested ( as Ontology Driven Information Extraction, where, in place of a template to be filled, the goal of the task is the extraction and classification of instances of concepts and relations defined in a Ontology.", "labels": [], "entities": [{"text": "OP task", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.8968048691749573}, {"text": "Ontology Driven Information Extraction", "start_pos": 118, "end_pos": 156, "type": "TASK", "confidence": 0.6907349973917007}, {"text": "extraction and classification of instances of concepts and relations defined in a Ontology", "start_pos": 230, "end_pos": 320, "type": "TASK", "confidence": 0.7463320791721344}]}, {"text": "The task has been approached in a variety of similar perspectives, including term clustering (e.g. ( and () and term categorization (e.g. ().", "labels": [], "entities": []}, {"text": "A rather different task is Ontology Learning (OL), where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (see, for instance, ().", "labels": [], "entities": [{"text": "Ontology Learning (OL)", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.7744652271270752}]}, {"text": "In this paper OP is defined in the following scenario.", "labels": [], "entities": [{"text": "OP", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.7063180208206177}]}, {"text": "Given a set of terms T = t 1 , t 2 , ..., tn , a document collection D, where terms in T are supposed to appear, and a set of predefined classes C = c 1 , c 2 , ..., cm denoting concepts in an Ontology, each term ti has to be assigned to the proper class in C.", "labels": [], "entities": []}, {"text": "For the purposes of the experiments presented in this paper we assume that (i) classes in C are mutually disjoint and (ii) each term is assigned to just one class.", "labels": [], "entities": []}, {"text": "As we have defined it, OP shows a strong similarity with Named Entity Recognition and Classification (NERC).", "labels": [], "entities": [{"text": "Named Entity Recognition and Classification (NERC)", "start_pos": 57, "end_pos": 107, "type": "TASK", "confidence": 0.8007285296916962}]}, {"text": "However, a major difference is that in NERC each occurrences of a recognized term has to be classified separately, while in OP it is the term, independently of the context in which it appears, that has to be classified.", "labels": [], "entities": [{"text": "NERC", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.6348122954368591}]}, {"text": "While Information Extraction, and NERC in particular, have been addressed prevalently by means of supervised approaches, Ontology Population is typically attacked in an unsupervised way.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.8166600465774536}]}, {"text": "As many authors have pointed out (e.g. (), the main motivation is the fact that in OP the set of classes is usually larger and more fine grained than in NERC (where the typical set includes Person, Location, Organization, GPE, and a Miscellanea class for all other kind of entities).", "labels": [], "entities": []}, {"text": "In addition, by definition, the set of classes in C changes as anew ontology is considered, making the creation of annotated data almost impossible practically.", "labels": [], "entities": []}, {"text": "According with the demand for weakly supervised approaches to OP, we propose a method, called Class \u2212 Example, which learns a classification model from a set of classified terms, exploiting lexico-syntactic features.", "labels": [], "entities": [{"text": "OP", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.96290522813797}]}, {"text": "Unlike most of the approaches which consider pairwise similarity between terms (;), the Class-Example method considers the similarity between a term ti and a set of training examples which represent a certain class.", "labels": [], "entities": []}, {"text": "This results in a great number of class features and opens the possibility to exploit more statistical data, such as the frequency of appearance of a class feature in different training terms.", "labels": [], "entities": []}, {"text": "In order to show the effectiveness of the ClassExample approach, it has been compared against two different approaches: (i) a Class-Pattern unsupervised approach, in the style of; (ii) an unsupervised approach that considers the word of the class as a pivot word for acquiring relevant contexts for the class (we refer to this method as Class\u2212W ord).", "labels": [], "entities": []}, {"text": "Results of the comparison show that the Class-Example method outperforms significantly the other two methods, making it appealing even considering the need of supervision.", "labels": [], "entities": []}, {"text": "Although the Class-Example method we propose is applicable in general, in this paper we show its usefulness when applied to terms denoting Named Entities.", "labels": [], "entities": []}, {"text": "The motivation behind this choice is the practical value of Named Entity classifications, as, for instance, in applications such as Questions Answering and Information Extraction.", "labels": [], "entities": [{"text": "Questions Answering", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.8334111571311951}, {"text": "Information Extraction", "start_pos": 156, "end_pos": 178, "type": "TASK", "confidence": 0.7568033933639526}]}, {"text": "Moreover, some Named Entity classes, including names of writers, athletes and organizations, dynamically changeover the time, which makes it impossible to capture them in a static Ontology.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the state-of-the-art methods in Ontology Population.", "labels": [], "entities": []}, {"text": "Section 3 presents the three approaches to the task we have compared.", "labels": [], "entities": []}, {"text": "Section 4 introduces Syntactic Network, a formalism used for the representation of syntactic information and exploited in both the Class-Word and the ClassExample approaches.", "labels": [], "entities": []}, {"text": "Section 5 reports on the experimental settings, results obtained, and discusses the three approaches.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper and suggests directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have evaluated all the three approaches described in Section 3.", "labels": [], "entities": []}, {"text": "The same evaluation settings were used for the three experiments.", "labels": [], "entities": []}, {"text": "The source of features was a news corpus of about half a gigabyte.", "labels": [], "entities": []}, {"text": "The corpus was parsed with MiniPar and a Syntactic Network representation was built from the dependency parse trees produced by the parser.", "labels": [], "entities": []}, {"text": "Syntactic features were extracted from this SyntNet.", "labels": [], "entities": []}, {"text": "We considered two high-level Named Entity categories: Locations and Persons.", "labels": [], "entities": []}, {"text": "For each of them five fine-grained sub-classes were taken into consideration.", "labels": [], "entities": []}, {"text": "For locations: mountain, lake, river, city, and country; for persons: statesman, writer, athlete, actor, and inventor.", "labels": [], "entities": []}, {"text": "For each class under consideration we created a test set of Named Entities using WordNet 2.0 and Internet sites like Wikipedia.", "labels": [], "entities": []}, {"text": "For the ClassExample approach we also provided training data using the same resources.", "labels": [], "entities": []}, {"text": "WordNet was the primary data source for training and test data.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9816717505455017}]}, {"text": "The examples from it were extracted automatically.", "labels": [], "entities": []}, {"text": "We: Performance of the Class-Example approach.", "labels": [], "entities": []}, {"text": "used Internet to get additional examples for some classes.", "labels": [], "entities": []}, {"text": "To do this, we created automatic text extraction scripts for Web pages and manually filtered their output when it was necessary.", "labels": [], "entities": [{"text": "text extraction", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.7317182719707489}]}, {"text": "Totally, the test data comprised 280 Named Entities which were not ambiguous and appeared at least twice in the corpus.", "labels": [], "entities": []}, {"text": "For the Class-Example approach we provided a training set of 1194 names.", "labels": [], "entities": []}, {"text": "The only requirement to the names in the training set was that they appear at least twice in the parsed corpus.", "labels": [], "entities": []}, {"text": "They were allowed to be ambiguous and no manual post-processing or filtering was carried out on this data.", "labels": [], "entities": []}, {"text": "For both context feature approaches (i.e. ClassWord and Class-Example), we used the same type of syntactic features and the same classification function, namely the one described in Section 3.3.", "labels": [], "entities": []}, {"text": "This was done in order to compare better the approaches.", "labels": [], "entities": []}, {"text": "Results from the comparative evaluation are shown in.", "labels": [], "entities": []}, {"text": "For each approach we measured macro average precision, macro average recall, macro average F-measure and micro average F; for Class-Word and Class-Example micro F is equal to the overall accuracy, i.e. the percent of the instances classified correctly.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.559207022190094}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.7821052670478821}, {"text": "macro average F-measure", "start_pos": 77, "end_pos": 100, "type": "METRIC", "confidence": 0.5745135645071665}, {"text": "micro average F", "start_pos": 105, "end_pos": 120, "type": "METRIC", "confidence": 0.9406018455823263}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9961950778961182}]}, {"text": "The first row shows  the results obtained with superficial patterns.", "labels": [], "entities": []}, {"text": "The second row presents the results from the ClassWord approach.", "labels": [], "entities": []}, {"text": "The third row shows the results of our Class-Example method.", "labels": [], "entities": []}, {"text": "The fourth line presents the results for the same approach but using second-order features for the person category.", "labels": [], "entities": []}, {"text": "The Class-Pattern approach showed low performance, similar to the random classification, for which macro and micro F=10%.", "labels": [], "entities": [{"text": "F", "start_pos": 115, "end_pos": 116, "type": "METRIC", "confidence": 0.5381930470466614}]}, {"text": "Patterns succeeded to classify correctly only instances of the classes \"river\" and \"city\".", "labels": [], "entities": []}, {"text": "For the class \"city\" the patterns reached precision of 100% and recall 65%; for the class \"river\" precision was high (i.e. 75%), but recall was 15%.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9995362758636475}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9995960593223572}, {"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9928128123283386}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9996021389961243}]}, {"text": "The Class-Word approach showed significantly better performance (macro F=33%, micro F=42%) than the Class-Pattern approach.", "labels": [], "entities": [{"text": "macro F", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.5178538858890533}, {"text": "micro F", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.6530555784702301}]}, {"text": "The performance of the Class-Example (62% macro F and 65%-68% micro F) is much higher than the performance of Class-Word (29% increase in macro F and 23% in micro F).", "labels": [], "entities": []}, {"text": "The last row of the table shows that second-order syntactic features augment further the performance of the Class-Example method in terms of micro average F (68% vs. 65%).", "labels": [], "entities": [{"text": "micro average F", "start_pos": 141, "end_pos": 156, "type": "METRIC", "confidence": 0.8133084376653036}]}, {"text": "A more detailed evaluation of the ClassExample approach is shown in show the performance of the approach without the second-order features.", "labels": [], "entities": []}, {"text": "Results vary between different classes: The highest F is measured for the class \"country\" -89% and the lowest is for the class \"inventor\" -18%.", "labels": [], "entities": [{"text": "F", "start_pos": 52, "end_pos": 53, "type": "METRIC", "confidence": 0.9981663823127747}]}, {"text": "However, the class \"inventor\" is an exception -for all the other classes the F measure is over 50%.", "labels": [], "entities": [{"text": "inventor", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9512163996696472}, {"text": "F measure", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9940910041332245}]}, {"text": "Another difference maybe observed between the Location and Person classes: Our approach performs significantly better for the locations (68% vs. 57% macro F and 78% vs. 57% micro F).", "labels": [], "entities": []}, {"text": "Although different classes had different number of training examples, we observed that the performance fora class does not depend on the size of its training set.", "labels": [], "entities": []}, {"text": "We think, that the variation in performance between categories is due to the different specificity of their textual contexts.", "labels": [], "entities": []}, {"text": "As a consequence, some classes tend to co-occur with more specific syntactic features, while for other classes this is not true.", "labels": [], "entities": []}, {"text": "Additionally, we measured the performance of our approach considering only the macrocategories \"Location\" and \"Person\".", "labels": [], "entities": []}, {"text": "For this purpose we did not run another experiment, we rather used the results from the fine-grained classification and grouped the already obtained classes.", "labels": [], "entities": []}, {"text": "Results are shown in the last two rows of table 1: It turns out that the Class-Example method makes very well the difference between \"location\" and \"person\" -90% of the test instances were classified correctly between these categories.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of different approaches.", "labels": [], "entities": []}]}