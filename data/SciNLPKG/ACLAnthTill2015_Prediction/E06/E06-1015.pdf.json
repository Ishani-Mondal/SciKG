{"title": [{"text": "Making Tree Kernels practical for Natural Language Learning", "labels": [], "entities": [{"text": "Natural Language Learning", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.6478971242904663}]}], "abstractContent": [{"text": "In recent years tree kernels have been proposed for the automatic learning of natural language applications.", "labels": [], "entities": []}, {"text": "Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9990591406822205}]}, {"text": "In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.", "labels": [], "entities": []}, {"text": "Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis .", "labels": [], "entities": [{"text": "predicate argument classification task", "start_pos": 48, "end_pos": 86, "type": "TASK", "confidence": 0.8914613127708435}]}], "introductionContent": [{"text": "In recent years tree kernels have been shown to be interesting approaches for the modeling of syntactic information in natural language tasks, e.g. syntactic parsing), relation extraction (, Named Entity recognition) and Semantic Parsing ().", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.8038110136985779}, {"text": "relation extraction", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.9221349954605103}, {"text": "Named Entity recognition", "start_pos": 191, "end_pos": 215, "type": "TASK", "confidence": 0.6057971715927124}, {"text": "Semantic Parsing", "start_pos": 221, "end_pos": 237, "type": "TASK", "confidence": 0.8582417666912079}]}, {"text": "The main tree kernel advantage is the possibility to generate a high number of syntactic features and let the learning algorithm to select those most relevant fora specific application.", "labels": [], "entities": []}, {"text": "In contrast, their major drawback are (a) the computational time complexity which is superlinear in the number of tree nodes and (b) the accuracy that they produce is often lower than the one provided by linear models on manually designed features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9989843964576721}]}, {"text": "To solve problem (a), a linear complexity algorithm for the subtree (ST) kernel computation, was designed in ().", "labels": [], "entities": []}, {"text": "Unfortunately, the ST set is rather poorer than the one generated by the subset tree (SST) kernel designed in).", "labels": [], "entities": []}, {"text": "Intuitively, an ST rooted in anode n of the target tree always contains all n's descendants until the leaves.", "labels": [], "entities": []}, {"text": "This does not hold for the SSTs whose leaves can be internal nodes.", "labels": [], "entities": [{"text": "SSTs", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.9616048336029053}]}, {"text": "To solve the problem (b), a study on different tree substructure spaces should be carried out to derive the tree kernel that provide the highest accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9959837198257446}]}, {"text": "On the one hand, SSTs provide learning algorithms with richer information which maybe critical to capture syntactic properties of parse trees as shown, for example, in ().", "labels": [], "entities": [{"text": "SSTs", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.9793187975883484}]}, {"text": "On the other hand, if the SST space contains too many irrelevant features, overfitting may occur and decrease the classification accuracy.", "labels": [], "entities": [{"text": "SST", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9491366147994995}, {"text": "classification", "start_pos": 114, "end_pos": 128, "type": "TASK", "confidence": 0.9393461346626282}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9513556361198425}]}, {"text": "As a consequence, the fewer features of the ST approach maybe more appropriate.", "labels": [], "entities": []}, {"text": "In this paper, we aim to solve the above problems.", "labels": [], "entities": []}, {"text": "We present (a) an algorithm for the evaluation of the ST and SST kernels which runs in linear average time and (b) a study of the impact of diverse tree kernels on the accuracy of Support Vector Machines (SVMs).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9980523586273193}]}, {"text": "Our fast algorithm computes the kernels between two syntactic parse trees in O(m + n) average time, where m and n are the number of nodes in the two trees.", "labels": [], "entities": []}, {"text": "This low complexity allows SVMs to carryout experiments on hundreds of thousands of training instances since it is not higher than the complexity of the polynomial ker-nel, widely used on large experimentation e.g. ().", "labels": [], "entities": [{"text": "SVMs", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.9533934593200684}]}, {"text": "To confirm such hypothesis, we measured the impact of the algorithm on the time required by SVMs for the learning of about 122,774 predicate argument examples annotated in PropBank () and 37,948 instances annotated in FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 218, "end_pos": 226, "type": "DATASET", "confidence": 0.9187803864479065}]}, {"text": "Regarding the classification properties, we studied the argument labeling accuracy of ST and SST kernels and their combinations with the standard features ().", "labels": [], "entities": []}, {"text": "The results show that, on both PropBank and FrameNet datasets, the SST-based kernel, i.e. the richest in terms of substructures, produces the highest SVM accuracy.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.9466080069541931}, {"text": "FrameNet datasets", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.8788807094097137}, {"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.910749614238739}]}, {"text": "When SSTs are combined with the manual designed features, we always obtain the best figure classifier.", "labels": [], "entities": [{"text": "SSTs", "start_pos": 5, "end_pos": 9, "type": "TASK", "confidence": 0.97453373670578}]}, {"text": "This suggests that the many fragments included in the SST space are relevant and, since their manual design maybe problematic (requiring a higher programming effort and deeper knowledge of the linguistic phenomenon), tree kernels provide a remarkable help in feature engineering.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 259, "end_pos": 278, "type": "TASK", "confidence": 0.8063549399375916}]}, {"text": "In the remainder of this paper, Section 2 describes the parse tree kernels and our fast algorithm.", "labels": [], "entities": []}, {"text": "Section 3 introduces the predicate argument classification problem and its solution.", "labels": [], "entities": [{"text": "predicate argument classification problem", "start_pos": 25, "end_pos": 66, "type": "TASK", "confidence": 0.9197859019041061}]}, {"text": "Section 4 shows the comparative performance in term of the execution time and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9989468455314636}]}, {"text": "Finally, Section 5 discusses the related work whereas Section 6 summarizes the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The aim of the experiments is twofold.", "labels": [], "entities": []}, {"text": "On the one hand, we show that the FTK running time is linear on the average case and is much faster than QTK.", "labels": [], "entities": [{"text": "FTK running time", "start_pos": 34, "end_pos": 50, "type": "METRIC", "confidence": 0.6589860121409098}, {"text": "QTK", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.8731476664543152}]}, {"text": "This is accomplished by measuring the learning time and the average kernel computation time.", "labels": [], "entities": []}, {"text": "On the other hand, we study the impact of the different tree based kernels on the predicate argument classification accuracy.", "labels": [], "entities": [{"text": "predicate argument classification", "start_pos": 82, "end_pos": 115, "type": "TASK", "confidence": 0.8213303287823995}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9082110524177551}]}, {"text": "We used two different corpora: PropBank (www.cis.upenn.edu/\u223cace) along with PennTree bank 2 (Marcus et al., 1993) and FrameNet.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.9091999530792236}, {"text": "PennTree bank 2 (Marcus et al., 1993)", "start_pos": 76, "end_pos": 113, "type": "DATASET", "confidence": 0.9235873579978943}]}, {"text": "PropBank contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches, e.g. ().", "labels": [], "entities": [{"text": "PropBank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9394305944442749}]}, {"text": "In this split, sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set.", "labels": [], "entities": []}, {"text": "We considered a total of 122,774 and 7,359 arguments (from ARG0 to ARG9, ARGA and ARGM) in training and testing, respectively.", "labels": [], "entities": [{"text": "ARG0", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9455662369728088}, {"text": "ARG9", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.8678315281867981}, {"text": "ARGA", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.778290867805481}, {"text": "ARGM", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.6151414513587952}]}, {"text": "Their tree structures were extracted from the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.9962836503982544}]}, {"text": "It should be noted that the main contribution to the global accuracy is given by ARG0, ARG1 and ARGM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9923445582389832}, {"text": "ARG0", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9177056550979614}, {"text": "ARG1", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.8838756084442139}, {"text": "ARGM", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.8809273838996887}]}, {"text": "From the FrameNet corpus (http://www.icsi .berkeley.edu/\u223cframenet), we extracted all 24,558 sentences of the 40 Frames selected for the Automatic Labeling of Semantic Roles task of Senseval 3 (www.senseval.org).", "labels": [], "entities": [{"text": "FrameNet corpus", "start_pos": 9, "end_pos": 24, "type": "DATASET", "confidence": 0.8075839579105377}, {"text": "Automatic Labeling of Semantic Roles task", "start_pos": 136, "end_pos": 177, "type": "TASK", "confidence": 0.7897856136163076}]}, {"text": "We mapped together the semantic roles having the same name and we considered only the 18 most frequent roles associated with verbal predicates, fora total of 37,948 arguments.", "labels": [], "entities": []}, {"text": "We randomly selected 30% of sentences for testing and 70% for training.", "labels": [], "entities": []}, {"text": "Additionally, 30% of training was used as a validationset.", "labels": [], "entities": []}, {"text": "Note that, since the FrameNet data does not include deep syntactic tree annotation, we processed the FrameNet data with, consequently, the experiments on FrameNet relate to automatic syntactic parse trees.", "labels": [], "entities": [{"text": "FrameNet data", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.9173127114772797}, {"text": "FrameNet data", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.9299766719341278}]}, {"text": "The classifier evaluations were carried outwith the SVM-light-TK software available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes ST and SST kernels in the SVMlight software.", "labels": [], "entities": []}, {"text": "We used the default linear (Linear) and polynomial (Poly) kernels for the evaluations with the standard features defined in ().", "labels": [], "entities": []}, {"text": "We adopted the default regularization parameter (i.e., the average of 1/|| x||) and we tried a few cost-factor values (i.e., j \u2208 {1, 3, 7, 10, 30, 100}) to adjust the rate between Precision and Recall on the validation-set.", "labels": [], "entities": [{"text": "Precision", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9704299569129944}, {"text": "Recall", "start_pos": 194, "end_pos": 200, "type": "METRIC", "confidence": 0.8995180726051331}]}, {"text": "For the ST and SST kernels, we derived that the best \u03bb (see Section 2.2) were 1 and 0.4, respectively.", "labels": [], "entities": []}, {"text": "The classification performance was evaluated using the F 1 measure 3 for the single arguments and the accuracy for the final multiclassifier.", "labels": [], "entities": [{"text": "F 1 measure 3", "start_pos": 55, "end_pos": 68, "type": "METRIC", "confidence": 0.9692460894584656}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9994937181472778}]}, {"text": "This latter choice allows us to compare our results with previous literature work, e.g. ().", "labels": [], "entities": []}, {"text": "In this section we compare our Fast Tree Kernel (FTK) approach with the Quadratic Tree Kernel (QTK) algorithm.", "labels": [], "entities": []}, {"text": "The latter refers to the naive evaluation of Eq.", "labels": [], "entities": []}, {"text": "1 as presented in).", "labels": [], "entities": []}, {"text": "shows the learning time 4 of the SVMs using QTK and FTK (over the SST structures) for the classification of one large argument (i.e. ARG0), according to different percentages of training data.", "labels": [], "entities": [{"text": "FTK", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9945551156997681}]}, {"text": "We note that, with 70% of the training data, FTK is about 10 times faster than QTK.", "labels": [], "entities": [{"text": "FTK", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9630679488182068}, {"text": "QTK", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.7869682908058167}]}, {"text": "With all the training data FTK terminated in 6 hours whereas QTK required more than 1 week.", "labels": [], "entities": [{"text": "FTK", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.5354166626930237}]}, {"text": "The above results are quite interesting because they show that (1) we can use tree kernels with SVMs on huge training sets, e.g. on 122,774 instances and (2) the time needed to converge is approximately the one required by SVMs when using polynomial kernel.", "labels": [], "entities": []}, {"text": "This latter shows the minimal complexity needed to work in the dual space.", "labels": [], "entities": []}, {"text": "To study the FTK running time, we extracted from PennTree bank the first 500 trees 5 containing exactly n nodes, then, we evaluated all 25,000 possible tree pairs.", "labels": [], "entities": [{"text": "FTK", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.6430267691612244}, {"text": "PennTree bank", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.9788861870765686}]}, {"text": "Each point of the shows the average computation time on all the tree pairs of a fixed size n.", "labels": [], "entities": []}, {"text": "In the figures, the trend lines which best interpolates the experimental values are also shown.", "labels": [], "entities": []}, {"text": "It clearly appears that the training time is quadratic as SVMs have quadratic learning time complexity (see) whereas the FTK running time has a linear behavior ().", "labels": [], "entities": [{"text": "FTK", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.7372342348098755}]}, {"text": "The QTK algorithm shows a quadratic running time complexity, as expected.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation of Kernels on PropBank.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of the Kernels on FrameNet semantic", "labels": [], "entities": []}, {"text": " Table 4: Multiclassifier accuracy using Kernel Combina-", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9007163047790527}]}]}