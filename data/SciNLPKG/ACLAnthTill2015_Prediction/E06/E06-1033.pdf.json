{"title": [{"text": "Adaptive Transformation-based Learning for Improving Dictionary Tagging", "labels": [], "entities": [{"text": "Improving Dictionary Tagging", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6705003380775452}]}], "abstractContent": [{"text": "We present an adaptive technique that enables users to produce a high quality dictionary parsed into its lexicographic components (headwords, pronunciations, parts of speech, translations, etc.) using an extremely small amount of user provided training data.", "labels": [], "entities": [{"text": "headwords, pronunciations, parts of speech, translations", "start_pos": 131, "end_pos": 187, "type": "TASK", "confidence": 0.655793160200119}]}, {"text": "We use transformation-based learning (TBL) as a postprocessor at two points in our system to improve performance.", "labels": [], "entities": []}, {"text": "The results using two dictionaries show that the tagging accuracy increases from 83% and 91% to 93% and 94% for individual words or \"tokens\", and from 64% and 83% to 90% and 93% for contiguous \"phrases\" such as definitions or examples of usage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9869226813316345}]}], "introductionContent": [{"text": "The availability and use of electronic resources such as electronic dictionaries has increased tremendously in recent years and their use in Natural Language Processing (NLP) systems is widespread.", "labels": [], "entities": []}, {"text": "For languages with limited electronic resources, i.e. low-density languages, however, we cannot use automated techniques based on parallel corpora (), comparable corpora, or multilingual thesauri).", "labels": [], "entities": []}, {"text": "Yet for these lowdensity languages, printed bilingual dictionaries often offer effective mapping from the low-density language to a high-density language, such as English.", "labels": [], "entities": []}, {"text": "Dictionaries can have different formats and can provide a variety of information.", "labels": [], "entities": []}, {"text": "However, they typically have a consistent layout of entries and a", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed our experiments on a CebuanoEnglish dictionary consisting of 1163 pages, 4 font styles, and 18 tags, and on an Iraqi Arabic-English dictionary consisting of 507 pages, 3 font styles, and 26 tags.", "labels": [], "entities": []}, {"text": "For our experiments, we used a publicly available implementation of TBL's fast version, fnTBL , described in Section 3.", "labels": [], "entities": []}, {"text": "We used eight randomly selected pages from the dictionaries to train TBL, and six additional randomly selected pages for testing.", "labels": [], "entities": [{"text": "TBL", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.652918815612793}]}, {"text": "The font style and tag of each token on these pages are manually corrected from an initial run.", "labels": [], "entities": []}, {"text": "Our goal is to measure the effect of TBL on font style and tagging that have the same noisy input.", "labels": [], "entities": []}, {"text": "For the Cebuano dictionary, the training data contains 156 entries, 8370 tokens, and 6691 non-punctuation tokens, and the test data contains 137 entries, 6251 tokens, and 4940 non-punctuation tokens.", "labels": [], "entities": []}, {"text": "For the Iraqi Arabic dictionary, the training data contains 232 entries, 6130 tokens, and 4621 non-punctuation tokens, and the test data contains 175 entries, 4708 tokens, 3467 non-punctuation tokens.", "labels": [], "entities": [{"text": "Iraqi Arabic dictionary", "start_pos": 8, "end_pos": 31, "type": "DATASET", "confidence": 0.8224982420603434}]}, {"text": "For evaluation, we used the percentage of accuracy for non-punctuation tokens, i.e., the number of correctly identified tags divided by total number of tokens/phrases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9976319074630737}]}, {"text": "The learning phase of TBL took less than one minute for each run, and application of learned transformations to the whole dictionary less than two minutes.", "labels": [], "entities": [{"text": "TBL", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.6483933925628662}]}, {"text": "We report how TBL affects accuracy of tagging 7 http://nlp.cs.jhu.edu/rflorian/fntbl when applied to font styles, tags, and font styles and tags together.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9989978671073914}]}, {"text": "To find the upper bound tagging results with correct font styles, we also ran rule-based entry tagger using manually corrected font styles, and applied TBL for tagging accuracy improvement to these results.", "labels": [], "entities": [{"text": "TBL", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.9847878217697144}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9762094616889954}]}, {"text": "We should note that feeding the correct font to the rule-based entry tagger does not necessarily mean the data is totally correct, it may still contain noise from document image analysis or ambiguity in the entry.", "labels": [], "entities": []}, {"text": "We conducted three sets of experiments to observe the effects of TBL (Section 5.1), the effects of different training data (Section 5.2), and the effects of training data size (Section 5.3).", "labels": [], "entities": [{"text": "TBL", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.8415908217430115}]}, {"text": "We report the accuracy of font styles on the test data before and after applying TBL to the font style of the non-punctuation tokens in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9991785883903503}, {"text": "TBL", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.7916869521141052}]}, {"text": "The initial font style accuracy of Cebuano dictionary was much less than the Iraqi Arabic dictionary, but applying TBL resulted in similar font style accuracy for both dictionaries (97% and 98%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.8916019797325134}, {"text": "TBL", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.730048656463623}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9106293320655823}]}, {"text": "The results of tagging accuracy experiments are presented in.", "labels": [], "entities": [{"text": "tagging", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.9434948563575745}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.7377468943595886}]}, {"text": "In the tables, RB is rule-based method, TBL(tag) is the TBL run on tags, TBL(font) is the TBL run on font style, and GT(font) is the ground truth font style.", "labels": [], "entities": [{"text": "RB", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9795486927032471}]}, {"text": "In each case, we begin with font style information provided by document image analysis.", "labels": [], "entities": []}, {"text": "We tabulate percentages of tagging accuracy of individual nonpunctuation tokens and phrases 8 . The results for token and phrase accuracy are presented for three different sets: The entry tagger using the font style (1) provided by document image analysis, (2) after TBL is applied to font style, and (3) corrected manually, i.e. the ground truth.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9019339680671692}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.8875802755355835}]}, {"text": "All results reported, except the token accuracies for two cases for the Iraqi Arabic dictionary, namely using TBL(font) vs. GT(font) and using TBL(font) and TBL(tag) together vs. using GT(font) and TBL(tag), are statistically significant within the 95% confidence interval with two-tailed paired ttests .", "labels": [], "entities": [{"text": "Iraqi Arabic dictionary", "start_pos": 72, "end_pos": 95, "type": "DATASET", "confidence": 0.8831851283709208}]}], "tableCaptions": [{"text": " Table 1: Font style accuracy results for non- punctuation tokens", "labels": [], "entities": [{"text": "Font style accuracy", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8317363460858663}]}, {"text": " Table 2: Tagging accuracy results for non-punctu- ation tokens and phrases for two dictionaries", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9503870010375977}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9765443801879883}]}, {"text": " Table 3: Average tagging accuracy results with standard deviation for ten runs using different eight pages  for training, and six pages for testing", "labels": [], "entities": [{"text": "tagging", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9370637536048889}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9808164238929749}, {"text": "standard", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9776923060417175}]}, {"text": " Table 5: Illustration of TBL application to the incorrect tags in the sample entries shown in", "labels": [], "entities": []}]}