{"title": [{"text": "Computing Term Translation Probabilities with Generalized Latent Semantic Analysis", "labels": [], "entities": [{"text": "Computing Term Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6273858745892843}, {"text": "Generalized Latent Semantic Analysis", "start_pos": 46, "end_pos": 82, "type": "TASK", "confidence": 0.5981724187731743}]}], "abstractContent": [{"text": "Term translation probabilities proved an effective method of semantic smoothing in the language modelling approach to information retrieval tasks.", "labels": [], "entities": [{"text": "Term translation probabilities", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8705090880393982}, {"text": "semantic smoothing", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7747626006603241}, {"text": "information retrieval tasks", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.838478684425354}]}, {"text": "In this paper, we use Generalized Latent Semantic Analysis to compute semantically motivated term and document vectors.", "labels": [], "entities": [{"text": "Generalized Latent Semantic Analysis", "start_pos": 22, "end_pos": 58, "type": "TASK", "confidence": 0.6843955591320992}]}, {"text": "The normalized cosine similarity between the term vectors is used as term translation probability in the language modelling framework.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that GLSA-based term translation probabilities capture semantic relations between terms and improve performance on document classification .", "labels": [], "entities": [{"text": "GLSA-based term translation probabilities capture semantic relations between terms", "start_pos": 33, "end_pos": 115, "type": "TASK", "confidence": 0.8116166260507371}, {"text": "document classification", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.7419093549251556}]}], "introductionContent": [{"text": "Many recent applications such as document summarization, passage retrieval and question answering require a detailed analysis of semantic relations between terms since often there is no large context that could disambiguate words's meaning.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.6727855801582336}, {"text": "passage retrieval", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.9377464354038239}, {"text": "question answering", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.8875890672206879}]}, {"text": "Many approaches model the semantic similarity between documents using the relations between semantic classes of words, such as representing dimensions of the document vectors with distributional term clusters ( and expanding the document and query vectors with synonyms and related terms as discussed in ( ).", "labels": [], "entities": []}, {"text": "They improve the performance on average, but also introduce some instability and thus increased variance ( ).", "labels": [], "entities": [{"text": "instability", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.9584378004074097}, {"text": "variance", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.995598316192627}]}, {"text": "The language modelling approach) proved very effective for the information retrieval task.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7765592932701111}, {"text": "information retrieval task", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.8735278248786926}]}, {"text": "Berger et. al) used translation probabilities between terms to account for synonymy and polysemy.", "labels": [], "entities": []}, {"text": "However, their model of such probabilities was computationally demanding.", "labels": [], "entities": []}, {"text": "Latent Semantic Analysis (LSA)) is one of the best known dimensionality reduction algorithms.", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA))", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7213177482287089}, {"text": "dimensionality reduction", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.736064076423645}]}, {"text": "Using a bag-of-words document vectors, it computes a dual representation for terms and documents in a lower dimensional space.", "labels": [], "entities": []}, {"text": "The resulting document vectors reside in the space of latent semantic concepts which can be expressed using different words.", "labels": [], "entities": []}, {"text": "The statistical analysis of the semantic relatedness between terms is performed implicitly, in the course of a matrix decomposition.", "labels": [], "entities": []}, {"text": "In this project, we propose to use a combination of dimensionality reduction and language modelling to compute the similarity between documents.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.669303834438324}]}, {"text": "We compute term vectors using the Generalized Latent Semantic Analysis ().", "labels": [], "entities": []}, {"text": "This method uses co-occurrence based measures of semantic similarity between terms to compute low dimensional term vectors in the space of latent semantic concepts.", "labels": [], "entities": []}, {"text": "The normalized cosine similarity between the term vectors is used as term translation probability.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of the experiments was to understand whether the GLSA term vectors can be used to model the term translation probabilities.", "labels": [], "entities": []}, {"text": "We used a simple k-NN classifier and a basic baseline to evalute the performance.", "labels": [], "entities": []}, {"text": "We used the GLSAbased term translation probabilities within the language modelling framework and GLSA document vectors.", "labels": [], "entities": []}, {"text": "We used the 20 news groups data set because previous studies showed that the classification performance on this document collection can noticeably benefit from additional semantic information (.", "labels": [], "entities": [{"text": "news groups data set", "start_pos": 15, "end_pos": 35, "type": "DATASET", "confidence": 0.8028014302253723}]}, {"text": "For the GLSA computations we used the terms that occurred in at least 15 documents, and had a vocabulary of 9732 terms.", "labels": [], "entities": []}, {"text": "We removed documents with fewer than 5 words.", "labels": [], "entities": []}, {"text": "Here we used 2 sets of 6 news groups.", "labels": [], "entities": []}, {"text": "Group d contained documents from dissimilar news groups 1 , with a total of 5300 documents.", "labels": [], "entities": []}, {"text": "Group s contained documents from more similar news groups 2 and had 4578 documents.", "labels": [], "entities": []}, {"text": "We ran the k-NN classifier with k=5 on ten random splits of training and test sets, with different numbers of training documents.", "labels": [], "entities": []}, {"text": "The baseline was to use the cosine similarity between the bag-ofwords document vectors weighted with term frequency.", "labels": [], "entities": []}, {"text": "Other weighting schemes such as maximum likelihood and Laplace smoothing did not improve results.", "labels": [], "entities": []}, {"text": "We computed the score between the training and test documents using two approaches: cosine similarity between the GLSA document vectors according to Equation 3 (denoted as GLSA), and the language modelling score which included the translation probabilities between the terms as in Equation 2 (denoted as LM ).", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 84, "end_pos": 101, "type": "METRIC", "confidence": 0.8727607429027557}]}, {"text": "We used the term frequency as an estimate for p(w|d).", "labels": [], "entities": [{"text": "frequency", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9555018544197083}]}, {"text": "To compute the matrix of translation probabilities P , where P [i][j] = t(t j |t i ) for the LM CLSA approach, we first obtained the matrix\u02c6P matrix\u02c6 matrix\u02c6P[j] = cos( \ud97b\udf59 ti , \ud97b\udf59 t j ).", "labels": [], "entities": []}, {"text": "We set the negative and zero entries in\u02c6Pin\u02c6 in\u02c6P to a small positive value.", "labels": [], "entities": []}, {"text": "Finally, we normalized the rows of\u02c6Pof\u02c6 of\u02c6P to sum up to one.", "labels": [], "entities": []}, {"text": "shows that for both settings GLSA and LM outperform the tf document vectors.", "labels": [], "entities": []}, {"text": "As expected, the classification task was more difficult for the similar news groups.", "labels": [], "entities": [{"text": "classification task", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.909575343132019}]}, {"text": "However, in this case both GLSA-based approaches outperform the baseline.", "labels": [], "entities": []}, {"text": "In both cases, the advantage is more significant with smaller sizes of the training set.", "labels": [], "entities": []}, {"text": "GLSA and LM performance usually peaked at around 300-500 dimensions which is inline with results for other SVD-based approaches.", "labels": [], "entities": [{"text": "dimensions", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.956379771232605}]}, {"text": "When the highest accuracy was achieved at higher dimensions, the increase after 500 dimensions was rather small, as illustrated in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9989834427833557}]}, {"text": "These results illustrate that the pair-wise similarities between the GLSA term vectors add important semantic information which helps to go beyond term matching and deal with synonymy and polysemy.", "labels": [], "entities": [{"text": "term matching", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.7075356692075729}]}], "tableCaptions": []}