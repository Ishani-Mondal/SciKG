{"title": [{"text": "Using Encyclopedic Knowledge for Named Entity Disambiguation", "labels": [], "entities": [{"text": "Named Entity Disambiguation", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.6478934983412424}]}], "abstractContent": [{"text": "We present anew method for detecting and disambiguating named entities in open domain text.", "labels": [], "entities": [{"text": "detecting and disambiguating named entities in open domain text", "start_pos": 27, "end_pos": 90, "type": "TASK", "confidence": 0.8155193527539571}]}, {"text": "A disambiguation SVM kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia.", "labels": [], "entities": []}, {"text": "The resulting model significantly outperforms a less informed baseline.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The taxonomy kernel was trained using the SVM \u00d0\u00d8 package (Joachims, 1999).", "labels": [], "entities": [{"text": "SVM \u00d0\u00d8 package (Joachims, 1999)", "start_pos": 42, "end_pos": 73, "type": "DATASET", "confidence": 0.8180672191083431}]}, {"text": "As described in Section 4, through its hyperlinks, Wikipedia provides a dataset of 1,783,868 ambiguous queries that can be used for training a named entity disambiguator.", "labels": [], "entities": []}, {"text": "The apparently high number of queries actually corresponds to a moderate size dataset, given that the space of parameters includes one parameter for each word-category combination.", "labels": [], "entities": []}, {"text": "However, assuming SVM \u00d0\u00d8 does not run out of memory, using the entire dataset for training and testing is extremely  . In order to make the task more realistic, all queries from the initial Wikipedia dataset are considered as follows.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 190, "end_pos": 207, "type": "DATASET", "confidence": 0.8810333907604218}]}, {"text": "For each query \u00d5, out of all matching entities that do not have a category under People by occupation, one is randomly selected as an out-of-Wikipedia entity.", "labels": [], "entities": []}, {"text": "Then, out of all queries for which the true answer is an out-of-Wikipedia entity, a subset is randomly selected such that, in the end, the number of queries with out-of-Wikipedia true answers is \u00bd\u00bc\u00b1 of the total number of queries.", "labels": [], "entities": []}, {"text": "In other words, the scenario assumes the task is to detect if a name denotes an entity belonging to the People by occupation taxonomy and, in the positive cases, to disambiguate between multiple entities under People by occupation that have the same name.", "labels": [], "entities": []}, {"text": "The dataset for each scenario is split into a training dataset and a testing dataset which are disjoint in terms of the query names used in their examples.", "labels": [], "entities": []}, {"text": "For instance, if a query for the name John Williams is included in the training dataset, then all other queries with this name are allocated for learning (and consequently excluded from testing).", "labels": [], "entities": [{"text": "John Williams", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.8824819028377533}]}, {"text": "Using a disjoint split is motivated by the fact that many Wikipedia queries that have the same true answer also have similar contexts, containing rare words that are highly correlated, almost exclusively, with the answer.", "labels": [], "entities": []}, {"text": "For example, query names that refer to singers often contain album or song names, query names that refer to writers often contain book names, etc.", "labels": [], "entities": []}, {"text": "The taxonomy kernel can easily \"memorize\" these associations, especially when the categories are very fine-grained.", "labels": [], "entities": []}, {"text": "In the current framework, the unsupervised method of context-article similarity does not utilize the correlations present in the training data.", "labels": [], "entities": []}, {"text": "Therefore, for the sake of comparison, we decided to prohibit the taxonomy kernel from using these correlations by training and testing on a disjoint split.", "labels": [], "entities": []}, {"text": "Section 6 describes how the training queries could be used in the computation of the context-article similarity, which has the potential of boosting the accuracy for both disambiguation methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9993676543235779}]}, {"text": "shows a number of relevant statistics for each scenario: #CAT represents the number of Wikipedia categories, #SV is the number of support vectors, TK(A) and Cos(A) are the accuracy of the Taxonomy Kernel and the Cosine similarity respectively.", "labels": [], "entities": [{"text": "CAT", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9607126116752625}, {"text": "SV", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.9763672947883606}, {"text": "Cos(A)", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.8410441130399704}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9994623064994812}]}, {"text": "The training and testing datasets are characterized in terms of the number of queries and query-answer pairs.", "labels": [], "entities": []}, {"text": "The number of ranking contraints (as specified in) is also included for the training data in column #CONSTR.", "labels": [], "entities": [{"text": "CONSTR", "start_pos": 101, "end_pos": 107, "type": "DATASET", "confidence": 0.8130476474761963}]}, {"text": "The size of the training data is limited so that learning in each scenario takes within three days on a Pentium 4 CPU at 2.6 GHz.", "labels": [], "entities": []}, {"text": "Furthermore, in \u00cb \ud97b\udf59 , the termination error criterion \u00af is changed from its default value of \u00bc\ud97b\udf59\u00bc\u00bc\u00bd to \u00bc\ud97b\udf59\u00bc\u00bd.", "labels": [], "entities": [{"text": "termination error criterion \u00af", "start_pos": 26, "end_pos": 55, "type": "METRIC", "confidence": 0.9328221678733826}]}, {"text": "Also, the threshold \ud97b\udf59 for detecting out-of-Wikipedia entities when ranking with cosine similarity is set to the value that gives highest accuracy on training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.998638927936554}]}, {"text": "As can be seen in the last two columns, the Taxonomy Kernel significantly outperforms the Cosine similarity in the first three scenarios, confirming our intuition that correlations between words from the query context and categories from Wikipedia taxonomy provide useful information for disambiguating named entities.", "labels": [], "entities": [{"text": "Cosine similarity", "start_pos": 90, "end_pos": 107, "type": "METRIC", "confidence": 0.9205632507801056}]}, {"text": "In the last scenario, which combines detection and disambiguation, the gain is not that substantial.", "labels": [], "entities": []}, {"text": "Most queries in the corresponding dataset have only two possible answers, one of them an out-of-Wikipedia answer, and for these cases the cosine is already doing well at disambiguation.", "labels": [], "entities": []}, {"text": "We conjecture that a more significant impact would be observed if the dataset queries were more ambiguous.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Scenario statistics and comparative evaluation.", "labels": [], "entities": [{"text": "comparative evaluation", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.9028950333595276}]}]}