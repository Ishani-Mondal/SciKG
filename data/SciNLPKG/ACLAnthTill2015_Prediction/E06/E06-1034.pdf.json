{"title": [{"text": "From detecting errors to automatically correcting them", "labels": [], "entities": []}], "abstractContent": [{"text": "Faced with the problem of annotation errors in part-of-speech (POS) annotated corpora, we develop a method for automatically correcting such errors.", "labels": [], "entities": []}, {"text": "Building on top of a successful error detection method, we first try correcting a corpus using two off-the-shelf POS taggers, based on the idea that they enforce consistency; with this, we find some improvement.", "labels": [], "entities": [{"text": "error detection", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.7112082988023758}]}, {"text": "After some discussion of the tagging process, we alter the tagging model to better account for problematic tagging distinctions.", "labels": [], "entities": [{"text": "tagging process", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.8921501338481903}]}, {"text": "This modification results in significantly improved performance, reducing the error rate of the corpus.", "labels": [], "entities": [{"text": "error rate", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9766821265220642}]}], "introductionContent": [{"text": "Annotated corpora serve as training material and as \"gold standard\" testing material for the development of tools in computational linguistics, and as a source of data for theoretical linguists searching for relevant language patterns.", "labels": [], "entities": []}, {"text": "However, they contain annotation errors, and such errors provide unreliable training and evaluation data, as has been previously shown (see ch. 1 of and references therein).", "labels": [], "entities": []}, {"text": "Improving the quality of linguistic annotation where possible is thus a key issue for the use of annotated corpora in computational and theoretical linguistics.", "labels": [], "entities": []}, {"text": "Research has gone into automatically detecting annotation errors for part-of-speech annotation), yet there has been virtually no work on automatically or semiautomatically correcting such annotation errors.", "labels": [], "entities": []}, {"text": "Automatic correction can speedup corpus improvement efforts and provide new data for NLP technology training on the corpus.", "labels": [], "entities": [{"text": "Automatic correction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7175237834453583}, {"text": "corpus improvement", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7777925431728363}]}, {"text": "Additionally, an investigation into automatic correction forces us to re-evaluate the technology using the corpus, providing new insights into such technology.", "labels": [], "entities": [{"text": "automatic correction", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.8054197728633881}]}, {"text": "We propose in this paper to automatically correct part-of-speech (POS) annotation errors in corpora, by adapting existing technology for POS disambiguation.", "labels": [], "entities": [{"text": "POS disambiguation", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.8342542946338654}]}, {"text": "We build the correction work on top of a POS error detection phase, described in section 2.", "labels": [], "entities": [{"text": "POS error detection", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.6945320169130961}]}, {"text": "In section 3 we discuss how to evaluate corpus correction work, given that we have no benchmark corpus to compare with.", "labels": [], "entities": [{"text": "corpus correction", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.8224209845066071}]}, {"text": "We turn to the actual work of correction in section 4, using two different POS taggers as automatic correctors and using the Wall Street Journal (WSJ) corpus as our data.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus", "start_pos": 125, "end_pos": 157, "type": "DATASET", "confidence": 0.9558976718357631}]}, {"text": "After more thoroughly investigating how problematic tagging distinctions affect the POS disambiguation task, in section 5 we modify the tagging model in order to better account for these distinctions, and we show this to significantly reduce the error rate of a corpus.", "labels": [], "entities": [{"text": "POS disambiguation task", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.8900349140167236}, {"text": "error rate", "start_pos": 246, "end_pos": 256, "type": "METRIC", "confidence": 0.9435102343559265}]}, {"text": "It might be objected that automatic correction of annotation errors will cause information to be lost or will make the corpus worse than it was, but the construction of a large corpus generally requires semi-automated methods of annotation, and automatic tools must be used sensibly at every stage in the corpus building process.", "labels": [], "entities": []}, {"text": "Automated annotation methods are not perfect, but humans also add errors, from biases and inconsistent judgments.", "labels": [], "entities": []}, {"text": "Thus, automatic corpus correction methods can be used semi-automatically, just as the original corpus creation methods were used.", "labels": [], "entities": [{"text": "corpus correction", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7410495281219482}]}, {"text": "then correct errors, but there is no general correction scheme.", "labels": [], "entities": [{"text": "correct errors", "start_pos": 5, "end_pos": 19, "type": "METRIC", "confidence": 0.9223877191543579}]}], "datasetContent": [], "tableCaptions": []}