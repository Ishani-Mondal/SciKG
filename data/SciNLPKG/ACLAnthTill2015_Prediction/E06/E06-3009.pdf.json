{"title": [{"text": "Example-Based Metonymy Recognition for Proper Nouns", "labels": [], "entities": []}], "abstractContent": [{"text": "Metonymy recognition is generally approached with complex algorithms that rely heavily on the manual annotation of training and test data.", "labels": [], "entities": [{"text": "Metonymy recognition", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.955369770526886}]}, {"text": "This paper will relieve this complexity in two ways.", "labels": [], "entities": []}, {"text": "First, it will show that the results of the current learning algorithms can be replicated by the 'lazy' algorithm of Memory-Based Learning.", "labels": [], "entities": []}, {"text": "This approach simply stores all training instances to its memory and classifies a test instance by comparing it to all training examples.", "labels": [], "entities": []}, {"text": "Second, this paper will argue that the number of labelled training examples that is currently used in the literature can be reduced drastically.", "labels": [], "entities": []}, {"text": "This finding can help relieve the knowledge acquisition bottleneck in metonymy recognition , and allow the algorithms to be applied on a wider scale.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.7391416132450104}, {"text": "metonymy recognition", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.9457760155200958}]}], "introductionContent": [{"text": "Metonymy is a figure of speech that uses \"one entity to refer to another that is related to it\" (.", "labels": [], "entities": []}, {"text": "In example (1), for instance, China and Taiwan stand for the governments of the respective countries: (1) China has always threatened to use force if Taiwan declared independence.", "labels": [], "entities": []}, {"text": "(BNC) Metonymy resolution is the task of automatically recognizing these words and determining their referent.", "labels": [], "entities": [{"text": "Metonymy resolution", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.7141940146684647}]}, {"text": "It is therefore generally split up into two phases: metonymy recognition and metonymy interpretation.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.9047753810882568}, {"text": "metonymy interpretation", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.876973032951355}]}, {"text": "The earliest approaches to metonymy recognition identify a word as metonymical when it violates selectional restrictions.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.9567423462867737}]}, {"text": "Indeed, in example (1), China and Taiwan both violate the restriction that threaten and declare require an animate subject, and thus have to be interpreted metonymically.", "labels": [], "entities": []}, {"text": "However, it is clear that many metonymies escape this characterization.", "labels": [], "entities": []}, {"text": "Nixon in example (2) does not violate the selectional restrictions of the verb to bomb, and yet, it metonymically refers to the army under Nixon's command.", "labels": [], "entities": [{"text": "Nixon", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.940297544002533}]}], "datasetContent": [{"text": "In order to see if Memory-Based Learning is able to replicate results, I used their corpora fora number of experiments.", "labels": [], "entities": []}, {"text": "These corpora consist of one set with about 1000 mixed country names, another with 1000 occurrences of Hungary, and a final set with about 1000 mixed organization names.", "labels": [], "entities": []}, {"text": "Evaluation was performed with ten-fold cross-validation.", "labels": [], "entities": []}, {"text": "The first round of experiments used only grammatical information.", "labels": [], "entities": []}, {"text": "The experiments for the location data were similar to, and took the following features into account: \u2022 the grammatical function of the word (subj, obj, iobj, pp, gen, premod, passive subj, other); \u2022 its head; \u2022 the presence of a second head; \u2022 the second head (if present).", "labels": [], "entities": []}, {"text": "The experiments for the organization names used the same features as: \u2022 the grammatical function of the word; \u2022 its head; \u2022 its type of determiner (if present) (def, indef, bare, demonst, other); \u2022 its grammatical number (sing, plural); \u2022 its number of grammatical roles (if present).", "labels": [], "entities": []}, {"text": "The number of words in the organization name, which Nissim and Markert used as a sixth and final feature, led to slightly worse results in my experiments and was therefore dropped.", "labels": [], "entities": []}, {"text": "The results of these first experiments clearly beat the baselines of 79.7% (countries) and 63.4% (organizations).", "labels": [], "entities": []}, {"text": "Moreover, despite its extremely Acc.", "labels": [], "entities": [{"text": "Acc", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.998062789440155}]}, {"text": "As table 1 shows, accuracy for the mixed country data is almost identical to Nissim and Markert's figure, and precision, recall and F-score for the metonymical class lie only slightly lower.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9996033310890198}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9997634291648865}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9997263550758362}, {"text": "F-score", "start_pos": 132, "end_pos": 139, "type": "METRIC", "confidence": 0.9988875985145569}]}, {"text": "TiMBL's results for the Hungary data were similar, and equally comparable to Markert and Nissim's (Katja Markert, personal communication).", "labels": [], "entities": [{"text": "Hungary data", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9263795912265778}]}, {"text": "Note, moreover, that these results were reached with grammatical information only, whereas Nissim and Markert's (2003) algorithm relied on semantics as well.", "labels": [], "entities": []}, {"text": "Next, table 2 indicates that TiMBL's accuracy for the mixed organization data lies about 1.5% below This result should be treated with caution, however.", "labels": [], "entities": [{"text": "TiMBL", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.760482907295227}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9930934906005859}]}, {"text": "First, Nissim and Markert's available organization data had not yet been annotated for grammatical features, and my annotation may slightly differ from theirs.", "labels": [], "entities": []}, {"text": "Second, Nissim and Markert used several feature vectors for instances with more than one grammatical role and filtered all mixed instances from the training set.", "labels": [], "entities": []}, {"text": "A test instance was treated as mixed only when its several feature vectors were classified differently.", "labels": [], "entities": []}, {"text": "My experiments, in contrast, were similar to those for the location data, in that each instance corresponded to one vector.", "labels": [], "entities": []}, {"text": "Hence, the slightly lower performance of TiMBL is probably due to differences between the two experiments.", "labels": [], "entities": []}, {"text": "These first experiments thus demonstrate that Memory-Based Learning can give state-of-the-art performance in metonymy recognition.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.934594988822937}]}, {"text": "In this respect, it is important to stress that the results for the country data were reached without any semantic information, whereas Nissim and Markert's (2003) algorithm used Dekang Lin's (1998) clusters of semantically similar words in order to deal with data sparseness.", "labels": [], "entities": []}, {"text": "This fact, together Precision, recall and F-score are given for the metonymical class only, since this is the category that metonymy recognition is concerned with.", "labels": [], "entities": [{"text": "Precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.99886155128479}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9982629418373108}, {"text": "F-score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9983175992965698}, {"text": "metonymy recognition", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.8731763064861298}]}, {"text": "It is still intuitively true, however, that the interpretation of a possibly metonymical word depends mainly on the semantics of its head.", "labels": [], "entities": []}, {"text": "The question is if this information is still able to improve the classifier's performance.", "labels": [], "entities": []}, {"text": "I therefore performed a second round of experiments with the location data, in which I also made use of semantic information.", "labels": [], "entities": []}, {"text": "In this round, I extracted the hypernym synsets of the head's first sense from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.9705438017845154}]}, {"text": "WordNet's hierarchy of synsets makes it possible to quantify the semantic relatedness of two words: the more hypernyms two words share, the more closely related they are.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9253743290901184}]}, {"text": "I therefore used the ten highest hypernyms of the first head as features 5 to 14.", "labels": [], "entities": []}, {"text": "For those heads with fewer than ten hypernyms, a copy of their lowest hypernym filled the 'empty' features.", "labels": [], "entities": []}, {"text": "As a result, TiMBL would first look for training instances with ten identical hypernyms, then with nine, etc.", "labels": [], "entities": []}, {"text": "It would thus compare the test example to the semantically most similar training examples.", "labels": [], "entities": []}, {"text": "However, TiMBL did not perform better with this semantic information.", "labels": [], "entities": []}, {"text": "Although F-scores for the metonymical category went up slightly, the system's accuracy hardly changed.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9887841939926147}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9994895458221436}]}, {"text": "This result was not due to the automatic selection of the first (most frequent) WordNet sense.", "labels": [], "entities": [{"text": "WordNet sense", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9329071640968323}]}, {"text": "By manually disambiguating all the heads in the training and test set of the country data, I observed that this first sense was indeed often incorrect, but that choosing the correct sense did not lead to a more robust system.", "labels": [], "entities": []}, {"text": "Clearly, the classifier did not benefit from WordNet information as did from Lin's (1998) thesaurus.", "labels": [], "entities": [{"text": "WordNet information", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.9055999219417572}]}, {"text": "The learning curves for the country set allow us to compare the two types of feature vectors in more detail.", "labels": [], "entities": []}, {"text": "As indicates, with respect to overall accuracy, semantic features have a negative influence: the learning curve with both features climbs much more slowly than that with only grammatical features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9984216690063477}]}, {"text": "Hence, contrary to my expectations, grammatical features seem to allow a better generalization from a limited number of training instances.", "labels": [], "entities": []}, {"text": "With respect to the F-score on the metonymical category in, the differences are much less outspoken.", "labels": [], "entities": [{"text": "F-score", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9984716773033142}]}, {"text": "Both features give similar learning curves, but semantic features lead to a higher final F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9908604621887207}]}, {"text": "In particular, the use of semantic features results in a lower precision figure, but a higher recall score.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9989057779312134}, {"text": "recall score", "start_pos": 94, "end_pos": 106, "type": "METRIC", "confidence": 0.9846695959568024}]}, {"text": "Semantic features thus cause the classifier to slightly overgeneralize from the metonymic training examples.", "labels": [], "entities": []}, {"text": "There are two possible reasons for this inability of semantic information to improve the classifier's performance.", "labels": [], "entities": []}, {"text": "First, WordNet's synsets do not always map well to one of our semantic labels: many are rather broad and allow for several readings of the target word, while others are too specific to make generalization possible.", "labels": [], "entities": []}, {"text": "Second, there is the predominance of prepositional phrases in our data.", "labels": [], "entities": []}, {"text": "With their closed set of heads, the number of examples that benefits from semantic information about its head is actually rather small.", "labels": [], "entities": []}, {"text": "Nevertheless, my first round of experiments has indicated that Memory-Based Learning is a simple but robust approach to metonymy recognition.", "labels": [], "entities": [{"text": "metonymy recognition", "start_pos": 120, "end_pos": 140, "type": "TASK", "confidence": 0.9664162397384644}]}, {"text": "It is able to replace current approaches that need smoothing or iterative searches through a thesaurus, with a simple, distance-based algorithm.", "labels": [], "entities": []}, {"text": "Moreover, in contrast to some other successful classifiers, it incorporates a plausible hypothesis of human learning.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the mixed country data.  TiMBL: my TiMBL results  N&M: Nissim and Markert's (2003) results", "labels": [], "entities": []}]}