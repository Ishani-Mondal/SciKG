{"title": [{"text": "Using Reinforcement Learning to Build a Better Model of Dialogue State", "labels": [], "entities": []}], "abstractContent": [{"text": "Given the growing complexity of tasks that spoken dialogue systems are trying to handle, Reinforcement Learning (RL) has been increasingly used as away of automatically learning the best policy fora system to make.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.5828162729740143}]}, {"text": "While most work has focused on generating better policies fora dialogue manager, very little work has been done in using RL to construct a better dialogue state.", "labels": [], "entities": []}, {"text": "This paper presents a RL approach for determining what dialogue features are important to a spoken dialogue tutoring system.", "labels": [], "entities": [{"text": "RL", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9314010739326477}]}, {"text": "Our experiments show that incorporating dialogue factors such as dialogue acts, emotion, repeated concepts and performance play a significant role in tutoring and should betaken into account when designing dialogue systems .", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents initial research toward the long-term goal of designing a tutoring system that can effectively adapt to the student.", "labels": [], "entities": []}, {"text": "While most work in Markov Decision Processes (MDPs) and spoken dialogue have focused on building better policies), to date very little empirical work has tested the utility of adding specialized features to construct a better dialogue state.", "labels": [], "entities": []}, {"text": "We wish to show that adding more complex factors to a representation of student state is a worthwhile pursuit, since it alters what action the tutor should make.", "labels": [], "entities": []}, {"text": "The five dialogue factors we explore are dialogue acts, certainty level, frustration level, concept repetition, and student performance.", "labels": [], "entities": [{"text": "concept repetition", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7069243788719177}]}, {"text": "All five are factors that are not just unique to the tutoring domain but are important to dialogue systems in general.", "labels": [], "entities": []}, {"text": "Our results show that using these features, combined with the common baseline of student correctness, leads to a significant change in the policies produced, and thus should betaken into account when designing a system.", "labels": [], "entities": []}], "datasetContent": [{"text": "With the infrastructure created and the MDP parameters set, we can then move onto the goal of this experiment -to see what sources of information impact a tutoring dialogue system.", "labels": [], "entities": []}, {"text": "First, we need to develop a baseline to compare the effects of adding more information.", "labels": [], "entities": []}, {"text": "Second, we generate anew policy by adding the new information source to the baseline state.", "labels": [], "entities": []}, {"text": "However, since we are currently not running any new experiments to test our policy, or evaluating over user simulations, we evaluate the reliability of our policies by looking at how well they converge overtime, that is, if you incrementally add more data (ie. a student's 5 dialogues) does the policy generated tend to stabilize over time?", "labels": [], "entities": []}, {"text": "And also, do the V-values for each state stabilize overtime as well?", "labels": [], "entities": []}, {"text": "The intuition is that if both the policies and V-values tend to converge then we can be sure that the policy generated is reasonable.", "labels": [], "entities": []}, {"text": "The first step in our experiment is to determine a baseline.", "labels": [], "entities": []}, {"text": "We use feedback as our system action in our MDP.", "labels": [], "entities": []}, {"text": "The action size is 3 (tutor can give feedback (Feed), give feedback with another tutor act (Mix), or give no feedback at all (NonFeed).", "labels": [], "entities": []}, {"text": "Examples from our corpus can be seen in.", "labels": [], "entities": []}, {"text": "It should be noted that \"NonFeed\" does not mean that the student's answer is not acknowledged, it: Tutor Action Examples means that something more complex than a simple positive or negative phrase is given (such as a Hint or Restatement).", "labels": [], "entities": []}, {"text": "Currently, the system's response to a student depends only on whether or not the student answered the last question correctly, so we use correctness as the sole feature in our dialogue state.", "labels": [], "entities": []}, {"text": "Recall that a student can either be correct, partially correct, or incorrect.", "labels": [], "entities": []}, {"text": "Since partially correct occurs infrequently compared to the other two, we reduced the state size to two by combining Incorrect and Partially Correct into one state (IPC) and keeping correct (C).", "labels": [], "entities": [{"text": "Incorrect", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9363772869110107}]}, {"text": "The third column of has the resulting learned MDP policy as well as the frequencies of both states in the data.", "labels": [], "entities": []}, {"text": "So for both states, the best action for the tutor to make is to give feedback, without knowing anything else about the student state.", "labels": [], "entities": []}, {"text": "The second step in our experiment is to test whether the policies generated are indeed reliable.", "labels": [], "entities": []}, {"text": "Normally, the best way to verify a policy is by conducting experiments and seeing if the new policy leads to a higher reward for the new dialogues.", "labels": [], "entities": []}, {"text": "In our context, this would entail running more subjects with the augmented dialogue manager and checking if the students had a higher learning gain with the new policies.", "labels": [], "entities": []}, {"text": "However, collecting data in this fashion can take months.", "labels": [], "entities": []}, {"text": "So, we take a different tact of checking if the polices and values for each state are indeed converging as we add data to our MDP model.", "labels": [], "entities": []}, {"text": "The intuition here is that if both of those parameters were varying between a corpus of 19 students to 20 students, then we can't assume that our policy is stable, and hence not reliable.", "labels": [], "entities": []}, {"text": "However, if these parameters converged as more data was added, this would indicate that the MDP is reliable.", "labels": [], "entities": []}, {"text": "To test this out, we conducted a 20-fold crossaveraging test over our corpus of 20 students.", "labels": [], "entities": []}, {"text": "Specifically, we made 20 random orderings of our students to prevent anyone ordering from giving a false convergence.", "labels": [], "entities": []}, {"text": "Each ordering was then chunked into 20 cuts ranging from a size of 1 student, to the entire corpus of 20 students.", "labels": [], "entities": []}, {"text": "We then passed each cut to our MDP infrastructure such that we started with a corpus of just the first student of the ordering and then determined a MDP policy for that cut, then added another student to that original corpus and reran our MDP system.", "labels": [], "entities": []}, {"text": "We continue this incremental addition of a student (5 dialogues) until we completed all 20 students.", "labels": [], "entities": []}, {"text": "So at the end, we have 20 random orderings with 20 cuts each, so 400 MDP trials were run.", "labels": [], "entities": []}, {"text": "Finally, we average the V-values of same size cuts together to produce an average V-value for that cut size.", "labels": [], "entities": []}, {"text": "The left-hand graph in shows a plot of the average Vvalues for each state against a cut.", "labels": [], "entities": []}, {"text": "The state with the plusses is the positive final state, and the one at the bottom is the negative final state.", "labels": [], "entities": []}, {"text": "However, we are most concerned with how the non-final states converge, which are the states in the middle.", "labels": [], "entities": []}, {"text": "The plot shows that for early cuts, there is a lot of instability but then each state tends to stabilize after cut 10.", "labels": [], "entities": []}, {"text": "So this tells us that the V-values are fairly stable and thus reliable when we derive policies from the entire corpus of 20 students.", "labels": [], "entities": []}, {"text": "As a further test, we also check that the policies generated for each cut tend to stabilize overtime.", "labels": [], "entities": []}, {"text": "That is, the differences between a policy at a smaller cut and the final cut converge to zero as more data is added.", "labels": [], "entities": []}, {"text": "This \"diffs\" testis discussed in more detail in Section 6.", "labels": [], "entities": []}], "tableCaptions": []}