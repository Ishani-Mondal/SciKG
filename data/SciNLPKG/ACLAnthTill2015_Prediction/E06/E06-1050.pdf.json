{"title": [], "abstractContent": [{"text": "All questions are implicitly associated with an expected answer type.", "labels": [], "entities": []}, {"text": "Unlike previous approaches that require a prede-fined set of question types, we present a method for dynamically constructing a probability-based answer type model for each different question.", "labels": [], "entities": []}, {"text": "Our model evaluates the appropriateness of a potential answer by the probability that it fits into the question contexts.", "labels": [], "entities": []}, {"text": "Evaluation is performed against manual and semi-automatic methods using a fixed set of answer labels.", "labels": [], "entities": []}, {"text": "Results show our approach to be superior for those questions classified as having a miscellaneous answer type.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given a question, people are usually able to form an expectation about the type of the answer, even if they do not know the actual answer.", "labels": [], "entities": []}, {"text": "An accurate expectation of the answer type makes it much easier to select the answer from a sentence that contains the query words.", "labels": [], "entities": []}, {"text": "Consider the question \"What is the capital of Norway?\"", "labels": [], "entities": []}, {"text": "We would expect the answer to be a city and could filter out most of the words in the following sentence: The landed aristocracy was virtually crushed by Hakon V, who reigned from 1299 to 1319, and Oslo became the capital of Norway, replacing Bergen as the principal city of the kingdom.", "labels": [], "entities": []}, {"text": "The goal of answer typing is to determine whether a word's semantic type is appropriate as an answer fora question.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.7980620563030243}]}, {"text": "Many previous approaches to answer typing, e.g.,;), employ a predefined set of answer types and use supervised learning or manually constructed rules to classify a question according to expected answer type.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9152949154376984}]}, {"text": "A disadvantage of this approach is that there will always be questions whose answers do not belong to any of the predefined types.", "labels": [], "entities": []}, {"text": "Consider the question: \"What are tourist attractions in Reims?\"", "labels": [], "entities": []}, {"text": "The answer maybe many things: a church, a historic residence, a park, a famous intersection, a statue, etc.", "labels": [], "entities": []}, {"text": "A common method to deal with this problem is to define a catch-all class.", "labels": [], "entities": []}, {"text": "This class, however, tends not to be as effective as other answer types.", "labels": [], "entities": []}, {"text": "Another disadvantage of predefined answer types is with regard to granularity.", "labels": [], "entities": []}, {"text": "If the types are too specific, they are more difficult to tag.", "labels": [], "entities": []}, {"text": "If they are too general, too many candidates maybe identified as having the appropriate type.", "labels": [], "entities": []}, {"text": "In contrast to previous approaches that use a supervised classifier to categorize questions into a predefined set of types, we propose an unsupervised method to dynamically construct a probabilistic answer type model for each question.", "labels": [], "entities": []}, {"text": "Such a model can be used to evaluate whether or not a word fits into the question context.", "labels": [], "entities": []}, {"text": "For example, given the question \"What are tourist attractions in Reims?\", we would expect the appropriate answers to fit into the context \"X is a tourist attraction.\"", "labels": [], "entities": []}, {"text": "From a corpus, we can find the words that appeared in this context, such as: Using the frequency counts of these words in the context, we construct a probabilistic model to compute P (in(w, \u0393)|w), the probability fora word w to occur in a set of contexts \u0393, given an occurrence of w.", "labels": [], "entities": []}, {"text": "The parameters in this model are obtained from a large, automatically parsed, unlabeled corpus.", "labels": [], "entities": []}, {"text": "By asking whether a word would occur in a particular context extracted from a ques-tion, we avoid explicitly specifying a list of possible answer types.", "labels": [], "entities": []}, {"text": "This has the added benefit of being easily adapted to different domains and corpora in which a list of explicit possible answer types maybe difficult to enumerate and/or identify within the text.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses the work related to answer typing.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9409033954143524}]}, {"text": "Section 3 discusses some of the key concepts employed by our probabilistic model, including word clusters and the contexts of a question and a word.", "labels": [], "entities": []}, {"text": "Section 4 presents our probabilistic model for answer typing.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9410236477851868}]}, {"text": "Section 5 compares the performance of our model with that of an oracle and a semi-automatic system performing the same task.", "labels": [], "entities": []}, {"text": "Finally, the concluding remarks in are made in Section 6.", "labels": [], "entities": []}, {"text": "performed an analysis of the effect of multiple answer type occurrences in a sentence.", "labels": [], "entities": []}, {"text": "When multiple words of the same type appear in a sentence, answer typing with fixed types must assign each the same score.", "labels": [], "entities": []}, {"text": "found that even with perfect answer sentence identification, question typing, and semantic tagging, a system could only achieve 59% accuracy over the TREC-9 questions when using their set of 24 non-overlapping answer types.", "labels": [], "entities": [{"text": "answer sentence identification", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.6556996405124664}, {"text": "semantic tagging", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.7327938377857208}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9987506866455078}]}, {"text": "By computing the probability of an answer candidate occurring in the question contexts directly, we avoid having multiple candidates with the same level of appropriateness as answers.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our answer typing system by using it to filter the contents of documents retrieved by the information retrieval portion of a question answering system.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.7890230715274811}, {"text": "question answering", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.6846549808979034}]}, {"text": "Each answer candidate in the set of documents is scored by the answer typing system and the list is sorted in descending order of score.", "labels": [], "entities": []}, {"text": "We treat the system as a filter and observe the proportion of candidates that must be accepted by the filter so that at least one correct answer is accepted.", "labels": [], "entities": []}, {"text": "A model that allows a low percentage of candidates to pass while still allowing at least one correct answer through is favorable to a model in which a high number of candidates must pass.", "labels": [], "entities": []}, {"text": "This represents an intrinsic rather than extrinsic evaluation) that we believe illustrates the usefulness of our model.", "labels": [], "entities": []}, {"text": "The evaluation data consist of 154 questions from the TREC-2003 QA Track satisfying the following criteria, along with the top 10 documents returned for each question as identified by NIST using the PRISE 1 search engine.", "labels": [], "entities": [{"text": "TREC-2003 QA Track", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.8896961212158203}, {"text": "NIST", "start_pos": 184, "end_pos": 188, "type": "DATASET", "confidence": 0.9416646957397461}, {"text": "PRISE 1 search engine", "start_pos": 199, "end_pos": 220, "type": "DATASET", "confidence": 0.8889790624380112}]}, {"text": "\u2022 the question begins with What, Which, or Who.", "labels": [], "entities": []}, {"text": "We restricted the evaluation such questions because our system is designed to deal with questions whose answer types are often semantically open-ended noun phrases.", "labels": [], "entities": []}, {"text": "\u2022 There exists entry for the question in the answer patterns provided by Ken Litkowski 2 . \u2022 One of the top-10 documents returned by PRISE contains a correct answer.", "labels": [], "entities": [{"text": "PRISE", "start_pos": 133, "end_pos": 138, "type": "DATASET", "confidence": 0.8663092851638794}]}, {"text": "We compare the performance of our probabilistic model with that of two other systems.", "labels": [], "entities": []}, {"text": "Both comparison systems make use of a small, predefined set of manually-assigned MUC-7 named-entity types (location, person, organization, cardinal, percent, date, time, duration, measure, money) augmented with thing-name (proper 1 www.itl.nist.gov/iad/894.02/works/papers/zp2/zp2.html 2 trec.nist.gov/data/qa/2003 qadata/03QA.tasks/t12.pats.txt names of inanimate objects) and miscellaneous (a catch-all answer type of all other candidates).", "labels": [], "entities": []}, {"text": "Some examples of thing-name are Guinness Book of World Records, Thriller, Mars Pathfinder, and Grey Cup.", "labels": [], "entities": [{"text": "Guinness Book of World Records", "start_pos": 32, "end_pos": 62, "type": "DATASET", "confidence": 0.9244091510772705}, {"text": "Thriller", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.8541014790534973}, {"text": "Mars Pathfinder", "start_pos": 74, "end_pos": 89, "type": "DATASET", "confidence": 0.824099987745285}, {"text": "Grey Cup", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.9202180802822113}]}, {"text": "Examples of miscellaneous answers are copper, oil, red, and iris.", "labels": [], "entities": []}, {"text": "The differences in the comparison systems is with respect to how entity types are assigned to the words in the candidate documents.", "labels": [], "entities": []}, {"text": "We make use of the ANNIE () named entity recognition system, along with a manual assigned \"oracle\" strategy, to assign types to candidate answers.", "labels": [], "entities": [{"text": "ANNIE () named entity recognition", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.46956567764282225}]}, {"text": "In each case, the score fora candidate is either 1 if it is tagged as the same type as the question or 0 otherwise.", "labels": [], "entities": []}, {"text": "With this scoring scheme producing a sorted list we can compute the probability of the first correct answer appearing at rank R = k as follows: where t is the number of unique candidate answers that are of the appropriate type and c is the number of unique candidate answers that are correct.", "labels": [], "entities": []}, {"text": "Using the probabilities in equation, we compute the expected rank, E(R), of the first correct answer of a given question in the system as: Answer candidates are the set of ANNIEidentified tokens with stop words and punctuation removed.", "labels": [], "entities": []}, {"text": "This yields between 900 and 8000 candidates for each question, depending on the top 10 documents returned by PRISE.", "labels": [], "entities": [{"text": "PRISE", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.9406901597976685}]}, {"text": "The oracle system represents an upper bound on using the predefined set of answer types.", "labels": [], "entities": []}, {"text": "The ANNIE system represents a more realistic expectation of performance.", "labels": [], "entities": [{"text": "ANNIE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.7801650166511536}]}, {"text": "The median percentage of candidates that are accepted by a filter over the questions of our evaluation data provides one measure of performance and is preferred to the average because of the effect of large values on the average.", "labels": [], "entities": []}, {"text": "In QA, a system accepting 60% of the candidates is not significantly better or worse than one accepting 100%,   The overall results of our comparison can be found in.", "labels": [], "entities": []}, {"text": "We have added the results of a system that scores candidates based on their frequency within the document as a comparison with a simple, yet effective, strategy.", "labels": [], "entities": []}, {"text": "The second column is the median percentage of where the highest scored correct answer appears in the sorted candidate list.", "labels": [], "entities": []}, {"text": "Low percentage values mean the answer is usually found high in the sorted list.", "labels": [], "entities": []}, {"text": "The remaining columns list the number of questions that have a correct answer somewhere in the top N % of their sorted lists.", "labels": [], "entities": []}, {"text": "This is meant to show the effects of imposing a strict cutoff prior to running the answer type model.", "labels": [], "entities": []}, {"text": "The oracle system performs best, as it benefits from both manual question classification and manual entity tagging.", "labels": [], "entities": [{"text": "question classification", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.684240072965622}, {"text": "entity tagging", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.7280563116073608}]}, {"text": "If entity assignment is performed by an automatic system (as it is for ANNIE), the performance drops noticeably.", "labels": [], "entities": [{"text": "entity assignment", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7323012053966522}]}, {"text": "Our probabilistic model performs better than ANNIE and achieves approximately 2/3 of the performance of the oracle system.", "labels": [], "entities": [{"text": "ANNIE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.5674405694007874}]}, {"text": "Table 2 also shows that the use of candidate contexts increases the performance of our answer type model.", "labels": [], "entities": []}, {"text": "shows the performance of the oracle system, our model, and the ANNIE system broken down by manually-assigned answer types.", "labels": [], "entities": [{"text": "ANNIE", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.7256861925125122}]}, {"text": "Due to insufficient numbers of questions, the cardinal, percent, time, duration, measure, and money types are combined into an \"Other\" category.", "labels": [], "entities": [{"text": "duration", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9355893135070801}]}, {"text": "When compared with the oracle system, our model performs worse overall for questions of all types except for those seeking miscellaneous answers.", "labels": [], "entities": []}, {"text": "For miscellaneous questions, the oracle identifies all tokens that do not belong to one of the other known categories as possible answers.", "labels": [], "entities": []}, {"text": "For all questions of non-miscellaneous type, only a small subset of the candidates are marked appropriate.", "labels": [], "entities": []}, {"text": "In particular, our model performs worse than the oracle for questions seeking persons and thingnames.", "labels": [], "entities": []}, {"text": "Person questions often seek rare person names, which occur in few contexts and are difficult to reliably cluster.", "labels": [], "entities": []}, {"text": "Thing-name questions are easy fora human to identify but difficult for automatic system to identify.", "labels": [], "entities": []}, {"text": "Thing-names area diverse category and are not strongly associated with any identifying contexts.", "labels": [], "entities": []}, {"text": "Our model outperforms the ANNIE system in general, and for questions seeking organizations, thing-names, and miscellaneous targets in particular.", "labels": [], "entities": []}, {"text": "ANNIE may have low coverage on organization names, resulting in reduced performance.", "labels": [], "entities": [{"text": "ANNIE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.636580765247345}]}, {"text": "Like the oracle, ANNIE treats all candidates not assigned one of the categories as appropriate for miscellaneous questions.", "labels": [], "entities": [{"text": "ANNIE", "start_pos": 17, "end_pos": 22, "type": "TASK", "confidence": 0.43396785855293274}]}, {"text": "Because ANNIE cannot identify thing-names, they are treated as miscellaneous.", "labels": [], "entities": [{"text": "ANNIE", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.6066976189613342}]}, {"text": "ANNIE shows low performance on thingnames because words incorrectly assigned types are sorted to the bottom of the list for miscellaneous and thing-name questions.", "labels": [], "entities": [{"text": "ANNIE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5608974099159241}]}, {"text": "If a correct answer is incorrectly assigned a type it will be sorted near the bottom, resulting in a poor score.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Summary of Results", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.5276196599006653}]}]}