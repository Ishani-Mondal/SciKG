{"title": [{"text": "What's There to Talk About? A Multi-Modal Model of Referring Behavior in the Presence of Shared Visual Information", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the development of a rule-based computational model that describes how a feature-based representation of shared visual information combines with linguistic cues to enable effective reference resolution.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 202, "end_pos": 222, "type": "TASK", "confidence": 0.7642614841461182}]}, {"text": "This work explores a language-only model, a visual-only model, and an integrated model of reference resolution and applies them to a corpus of transcribed task-oriented spoken dialogues.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.7256380468606949}]}, {"text": "Preliminary results from a corpus-based analysis suggest that integrating information from a shared visual environment can improve the performance and quality of existing discourse-based models of reference resolution.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 197, "end_pos": 217, "type": "TASK", "confidence": 0.7628066539764404}]}], "introductionContent": [{"text": "In this paper, we present work in progress towards the development of a rule-based computational model to describe how various forms of shared visual information combine with linguistic cues to enable effective reference resolution during task-oriented collaboration.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 211, "end_pos": 231, "type": "TASK", "confidence": 0.7252849787473679}]}, {"text": "A number of recent studies have demonstrated that linguistic patterns shift depending on the speaker's situational context.", "labels": [], "entities": []}, {"text": "Patterns of proximity markers (e.g., this/here vs. that/there) change according to whether speakers perceive themselves to be physically co-present or remote from their partner.", "labels": [], "entities": []}, {"text": "The use of particular forms of definite referring expressions (e.g., personal pronouns vs. demonstrative pronouns vs. demonstrative descriptions) varies depending on the local visual context in which they are constructed).", "labels": [], "entities": []}, {"text": "And people are found to use shorter and syntactically simpler language and different surface realizations) when gestures accompany their spoken language.", "labels": [], "entities": []}, {"text": "More specifically, work examining dialogue patterns in collaborative environments has demonstrated that pairs adapt their linguistic patterns based on what they believe their partner can see.", "labels": [], "entities": []}, {"text": "For example, when a speaker knows their partner can see their actions but will incur a small delay before doing so, they increase the proportion of full NPs used ().", "labels": [], "entities": []}, {"text": "Similar work by demonstrates that the forms of referring expressions vary according to a partner's proximity to visual objects of interest.", "labels": [], "entities": []}, {"text": "Together this work suggests that the interlocutors' shared visual context has a major impact on their patterns of referring behavior.", "labels": [], "entities": []}, {"text": "Yet, a number of discourse-based models of reference primarily rely on linguistic information without regard to the surrounding visual environment (e.g., see).", "labels": [], "entities": []}, {"text": "Recently, multi-modal models have emerged that integrate visual information into the resolution process.", "labels": [], "entities": []}, {"text": "However, many of these models are restricted by their simplifying assumption of communication via a command language.", "labels": [], "entities": []}, {"text": "Thus, their approaches apply to explicit interaction techniques but do not necessarily support more general communication in the presence of shared visual information (e.g., see.", "labels": [], "entities": []}, {"text": "It is the goal of the work presented in this paper to explore the performance of languagebased models of reference resolution in contexts where speakers share a common visual space.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.7502536773681641}]}, {"text": "In particular, we examine three basic hypotheses regarding the likely impact of linguistic and visual salience on referring behavior.", "labels": [], "entities": []}, {"text": "The first hypothesis suggests that visual information is disregarded and that linguistic context provides sufficient information to describe referring behavior.", "labels": [], "entities": []}, {"text": "The second hypothesis suggests that visual salience overrides any linguistic salience in governing referring behavior.", "labels": [], "entities": []}, {"text": "Finally, the third hypothesis posits that a balance of linguistic and visual salience is needed in order to account for patterns of referring behavior.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we begin by presenting a brief discussion of the motivation for this work.", "labels": [], "entities": []}, {"text": "We then describe three computational models of referring behavior used to explore the hypotheses described above, and the corpus on which they have been evaluated.", "labels": [], "entities": []}, {"text": "We conclude by presenting preliminary results and discussing future modeling plans.", "labels": [], "entities": []}], "datasetContent": [{"text": "Together, the models described above allow us to test three basic hypotheses regarding the likely impact of linguistic and visual salience: Purely linguistic context.", "labels": [], "entities": []}, {"text": "One hypothesis is that the visual information is completely disregarded and the entities are salient purely based on linguistic information.", "labels": [], "entities": []}, {"text": "While our prior work has suggested this should not be the case, several existing computational models function only at this level.", "labels": [], "entities": []}, {"text": "A second possibility is that the visual information completely overrides linguistic salience.", "labels": [], "entities": []}, {"text": "Thus, visual information dominates the discourse structure when it is available and relegates linguistic information to a subordinate role.", "labels": [], "entities": []}, {"text": "This too should be unlikely given the fact that not all discourse deals with external elements from the surrounding world.", "labels": [], "entities": []}, {"text": "A balance of syntactic and visual context.", "labels": [], "entities": []}, {"text": "A third hypothesis is that both linguistic entities and visual entities are required in order to accurately and perspicuously account for patterns of observed referring behavior.", "labels": [], "entities": []}, {"text": "Salient discourse entities result from some balance of linguistic salience and visual salience.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Overview of the data used.", "labels": [], "entities": [{"text": "Overview", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9512978196144104}]}]}