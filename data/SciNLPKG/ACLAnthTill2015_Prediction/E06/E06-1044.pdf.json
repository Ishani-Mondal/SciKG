{"title": [{"text": "Modelling Semantic Role Plausibility in Human Sentence Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "We present the psycholinguistically motivated task of predicting human plausibility judgements for verb-role-argument triples and introduce a probabilistic model that solves it.", "labels": [], "entities": [{"text": "predicting human plausibility judgements", "start_pos": 54, "end_pos": 94, "type": "TASK", "confidence": 0.8718928843736649}]}, {"text": "We also evaluate our model on the related role-labelling task, and compare it with a standard role labeller.", "labels": [], "entities": []}, {"text": "For both tasks, our model benefits from class-based smoothing, which allows it to make correct argument-specific predictions despite a severe sparse data problem.", "labels": [], "entities": []}, {"text": "The standard labeller suffers from sparse data and a strong reliance on syntactic cues, especially in the prediction task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computational psycholinguistics is concerned with modelling human language processing.", "labels": [], "entities": [{"text": "modelling human language processing", "start_pos": 50, "end_pos": 85, "type": "TASK", "confidence": 0.6349877491593361}]}, {"text": "Much work has gone into the exploration of sentence comprehension.", "labels": [], "entities": [{"text": "sentence comprehension", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.6736961752176285}]}, {"text": "Syntactic preferences that unfold during the course of the sentence have been successfully modelled using incremental probabilistic context-free parsing models (e.g.,).", "labels": [], "entities": []}, {"text": "These models assume that humans prefer the most likely structural alternative at each point in the sentence.", "labels": [], "entities": []}, {"text": "If the preferred structure changes during processing, such models correctly predict processing difficulty fora range of experimentally investigated constructions.", "labels": [], "entities": []}, {"text": "They do not, however, incorporate an explicit notion of semantic processing, while there are many phenomena inhuman sentence processing that demonstrate a non-trivial interaction of syntactic preferences and semantic plausibility.", "labels": [], "entities": [{"text": "semantic processing", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7114315778017044}]}, {"text": "Consider, for example, the well-studied case of reduced relative clause constructions.", "labels": [], "entities": []}, {"text": "When incrementally processing the sentence The deer shot by the hunter was used as atrophy, there is a local ambiguity at shot between continuation as a main clause (as in The deer shot the hunter) or as a reduced relative clause modifying deer (equivalent to The deer which was shot . .", "labels": [], "entities": []}, {"text": "). The main clause continuation is syntactically more likely.", "labels": [], "entities": []}, {"text": "However, there is a second, semantic clue provided by the high plausibility of deer being shot and the low plausibility of them shooting.", "labels": [], "entities": []}, {"text": "This influences readers to choose the syntactically dispreferred reduced relative reading which interprets the deer as an object of shot (.", "labels": [], "entities": []}, {"text": "Plausibility has overridden the syntactic default.", "labels": [], "entities": []}, {"text": "On the other hand, fora sentence like The hunter shot by the teenager was only 30 years old, semantic plausibility initially reinforces the syntactic main clause preference and readers show difficulty accommodating the subsequent disambiguation towards the reduced relative.", "labels": [], "entities": []}, {"text": "In order to model effects like these, we need to extend existing models of sentence processing by introducing a semantic dimension.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7138171195983887}]}, {"text": "Possible ways of integrating different sources of information have been presented e.g. by and.", "labels": [], "entities": []}, {"text": "Our aim is to formulate a model that reliably predicts human plausibility judgements from corpus resources, in parallel to the standard practice of basing the syntax component of psycholinguistic models on corpus probabilities or even probabilistic treebank grammars.", "labels": [], "entities": [{"text": "predicts human plausibility judgements", "start_pos": 46, "end_pos": 84, "type": "TASK", "confidence": 0.7238171696662903}]}, {"text": "We can then use both the syntactic likelihood and the semantic plausibility score to predict the preferred syntactic alternative, thus accounting for the effects shown e.g. by.", "labels": [], "entities": []}, {"text": "Independent of a syntactic model, we want any semantic model we define to satisfy two criteria: First, it needs to be able to make predictions in-crementally, in parallel with the syntactic model.", "labels": [], "entities": []}, {"text": "This entails dealing with incomplete or unspecified (syntactic) information.", "labels": [], "entities": [{"text": "dealing with incomplete or unspecified (syntactic) information", "start_pos": 13, "end_pos": 75, "type": "TASK", "confidence": 0.6782978044615852}]}, {"text": "Second, we want to extend to semantics the assumption made in syntactic models that the most probable alternative is the one preferred by humans.", "labels": [], "entities": []}, {"text": "The model therefore must be probabilistic.", "labels": [], "entities": []}, {"text": "We present such a probabilistic model that can assign roles incrementally as soon as a predicateargument pair is seen.", "labels": [], "entities": []}, {"text": "It uses the likelihood of thematic role assignments to model human interpretation of verb-argument relations.", "labels": [], "entities": []}, {"text": "Thematic roles area description of the link between verb and argument at the interface between syntax and semantics.", "labels": [], "entities": []}, {"text": "Thus, they provide a shallow level of sentence semantics which can be learnt from annotated corpora.", "labels": [], "entities": []}, {"text": "We evaluate our model by verifying that it indeed correctly predicts human judgements, and by comparing its performance with that of a standard role labeller in terms of both judgement prediction and role assignment.", "labels": [], "entities": [{"text": "judgement prediction", "start_pos": 175, "end_pos": 195, "type": "TASK", "confidence": 0.7151854038238525}, {"text": "role assignment", "start_pos": 200, "end_pos": 215, "type": "TASK", "confidence": 0.7655424475669861}]}, {"text": "Our model has two advantages over the standard labeller: It does not rely on syntactic features (which can be hard to come by in an incremental task) and our smoothing approach allows it to make argument-specific role predictions in spite of extremely sparse training data.", "labels": [], "entities": []}, {"text": "We conclude that (a) our model solves the task we set, and (b) our model is better equipped for our task than a standard role labeller.", "labels": [], "entities": []}, {"text": "The outline of the paper is as follows: After defining the prediction task more concretely (Section 2), we present our simple probabilistic model that is tailoured to the task (Section 3).", "labels": [], "entities": []}, {"text": "We introduce our test and training data in Section 4.", "labels": [], "entities": []}, {"text": "It becomes evident immediately that we face a severe sparse data problem, which we tackle on two levels: By smoothing the distribution and by acquiring additional counts for sparse cases.", "labels": [], "entities": []}, {"text": "The smoothed model succeeds on the prediction task (Section 5).", "labels": [], "entities": [{"text": "prediction", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.9443492293357849}]}, {"text": "Finally, in Section 6, we compare our model to a standard role labeller.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now turn to evaluating our model.", "labels": [], "entities": []}, {"text": "It is immediately clear that we have a severe sparse data problem.", "labels": [], "entities": []}, {"text": "Even if all the verbs are seen, the combinations of verbs and arguments are still mostly unseen in training for all data sets.", "labels": [], "entities": []}, {"text": "We describe two complementary approaches to smoothing sparse training data.", "labels": [], "entities": [{"text": "smoothing sparse training", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.8644517262776693}]}, {"text": "One, GoodTuring smoothing, approaches the problem of unseen data points by assigning them a small probability.", "labels": [], "entities": [{"text": "GoodTuring smoothing", "start_pos": 5, "end_pos": 25, "type": "TASK", "confidence": 0.7517985999584198}]}, {"text": "The other, class-based smoothing, attempts to arrive at semantic generalisations for words.", "labels": [], "entities": []}, {"text": "These serve to identify equivalent verb-argument pairs that furnish additional counts for the estimation of P(a|v, c, g f , r).", "labels": [], "entities": []}, {"text": "We have shown that our model performs well on its intended task of predicting plausibility judgements, once we have proper smoothing methods in place.", "labels": [], "entities": [{"text": "predicting plausibility judgements", "start_pos": 67, "end_pos": 101, "type": "TASK", "confidence": 0.9043249090512594}]}, {"text": "But since this task has some similarity to role labelling, we can also compare the model to a standard role labeller on both the prediction and role labelling tasks.", "labels": [], "entities": []}, {"text": "The questions are: How well do we do labelling, and does a standard role labeller also predict human judgements?", "labels": [], "entities": []}, {"text": "Beginning with work by, there has been a large interest in semantic role labelling, as evidenced by its adoption as a shared task in the Senseval-III competition (FrameNet data,) and at the).", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.7028201421101888}, {"text": "FrameNet data", "start_pos": 163, "end_pos": 176, "type": "DATASET", "confidence": 0.8548522889614105}]}, {"text": "As our model currently focuses on noun phrase arguments only, we do not adopt these test sets but continue to use ours, defining the correct role label to be the one with the higher probability judgement.", "labels": [], "entities": []}, {"text": "We evaluate the model on the McRae test set (recall that the other sets only contain good patients/themes and are therefore susceptible to labeller biases).", "labels": [], "entities": [{"text": "McRae test set", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.957380215326945}]}, {"text": "We formulate frequency baselines for our training data.", "labels": [], "entities": []}, {"text": "For PropBank, always assigning Arg1 results in F = 45.7 (43.8 on the full test set).", "labels": [], "entities": [{"text": "PropBank", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.911648154258728}, {"text": "Arg1", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9908043146133423}, {"text": "F", "start_pos": 47, "end_pos": 48, "type": "METRIC", "confidence": 0.9984063506126404}]}, {"text": "For FrameNet, we assign the most frequent role given the verb, so the baseline is F = 34.4.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.8197797536849976}, {"text": "F", "start_pos": 82, "end_pos": 83, "type": "METRIC", "confidence": 0.9980161190032959}]}, {"text": "We base our standard role labelling system on the SVM labeller described in, although without integrating information from PropBank and VerbNet for FrameNet classification as presented in their paper.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 123, "end_pos": 131, "type": "DATASET", "confidence": 0.9666416645050049}, {"text": "VerbNet", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.8988018035888672}, {"text": "FrameNet classification", "start_pos": 148, "end_pos": 171, "type": "TASK", "confidence": 0.736235499382019}]}, {"text": "Thus, we are left with a set of fairly standard features, such as phrase type, voice, governing category or path through parse tree from predicate.", "labels": [], "entities": []}, {"text": "These are used to train two classifiers, one which decides which phrases should be considered arguments and one which assigns role labels to these arguments.", "labels": [], "entities": []}, {"text": "The SVM labeller's F score on an unseen test set is F = 80.5 for FrameNet data when using gold argument boundaries.", "labels": [], "entities": [{"text": "F score", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9826394319534302}, {"text": "F", "start_pos": 52, "end_pos": 53, "type": "METRIC", "confidence": 0.9825657606124878}, {"text": "FrameNet data", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.8070341050624847}]}, {"text": "We also trained the labeller on the PropBank data, resulting in an F score of F = 98.6 on Section 23, again on gold boundaries.", "labels": [], "entities": [{"text": "PropBank data", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9636569023132324}, {"text": "F score", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9887585043907166}, {"text": "F", "start_pos": 78, "end_pos": 79, "type": "METRIC", "confidence": 0.9842624664306641}, {"text": "Section 23", "start_pos": 90, "end_pos": 100, "type": "DATASET", "confidence": 0.9009411334991455}]}, {"text": "We also evaluate the SVM labeller on the correlation task by normalising the scores that the labeller assigns to each role and then correlating the normalised scores to the human ratings.", "labels": [], "entities": []}, {"text": "In order to extract features for the SVM labeller, we had to present the verb-noun pairs in full sen-  The hunter shot the ...) and present the results for comparison.", "labels": [], "entities": []}, {"text": "For our model, we have previously not specified the grammatical function of the argument, but in order to put both models on a level playing field, we now supply the grammatical function of Ext (external argument), which applies for both formulations of the items.", "labels": [], "entities": []}, {"text": "shows that for the labelling task, our model outperforms the labelling baseline and the SVM labeller on the FrameNet data by at least 16 points F score while the correlation with human data remains significant.", "labels": [], "entities": [{"text": "labelling task", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.8964990079402924}, {"text": "FrameNet data", "start_pos": 108, "end_pos": 121, "type": "DATASET", "confidence": 0.9442136883735657}, {"text": "F", "start_pos": 144, "end_pos": 145, "type": "METRIC", "confidence": 0.9919353723526001}]}, {"text": "For the PropBank data, labelling performance is on baseline level, below the better of the two SVM labeller conditions.", "labels": [], "entities": [{"text": "PropBank data", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9601682126522064}]}, {"text": "This result underscores the usefulness of argument-specific plausibility estimates furnished by class-based smoothing for the FrameNet data.", "labels": [], "entities": [{"text": "FrameNet data", "start_pos": 126, "end_pos": 139, "type": "DATASET", "confidence": 0.9142770767211914}]}, {"text": "For the PropBank data, our model essentially assigns the most frequent role for the verb.", "labels": [], "entities": [{"text": "PropBank data", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9707049131393433}]}, {"text": "The performance of the SVM labeller suggests a strong influence of syntactic features: On the PropBank data set, it always assigns the Arg0 label if the argument was presented as a subject (this is correct in 50% of cases) and mostly the appropriate ArgN label if the argument was presented as an object.", "labels": [], "entities": [{"text": "PropBank data set", "start_pos": 94, "end_pos": 111, "type": "DATASET", "confidence": 0.9807073275248209}, {"text": "Arg0 label", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.9745489954948425}, {"text": "ArgN label", "start_pos": 250, "end_pos": 260, "type": "METRIC", "confidence": 0.9728679358959198}]}, {"text": "On FrameNet, performance again is above baseline only for the subject condition, where there is also a clear trend for assigning agent-style roles.", "labels": [], "entities": []}, {"text": "(The object condition is less clear-cut.)", "labels": [], "entities": []}, {"text": "This strong reliance on syntactic cues, which maybe misleading for our data, makes the labeller perform much worse than on the standard test sets.", "labels": [], "entities": []}, {"text": "For both training corpora, it does not take word-specific plausibility into account due to data sparseness and usually assigns the same role to both arguments of a verb.", "labels": [], "entities": []}, {"text": "This precludes a significant correlation with the human ratings.", "labels": [], "entities": []}, {"text": "Comparing the training corpora, we find that both models perform better on the FrameNet data even though there are many more role labels in FrameNet, and the SVM labeller does not profit from the greater smoothing power of FrameNet verb clusters.", "labels": [], "entities": [{"text": "FrameNet data", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9462665319442749}, {"text": "FrameNet", "start_pos": 140, "end_pos": 148, "type": "DATASET", "confidence": 0.8942354321479797}]}, {"text": "Overall, FrameNet has proven more useful to us, despite its smaller size.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.8278971910476685}]}, {"text": "In sum, our model does about as well (PB data) or better (FN data) on the labelling task as the SVM labeller, while the labeller does not solve the prediction task.", "labels": [], "entities": []}, {"text": "The success of our model, especially on the prediction task, stems partly from the absence of global syntactic features that bias the standard labeller strongly.", "labels": [], "entities": [{"text": "prediction task", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.8955815732479095}]}, {"text": "This also makes our model suited for an incremental task.", "labels": [], "entities": []}, {"text": "Instead of  syntactic cues, we successfully rely on argumentspecific plausibility estimates furnished by classbased smoothing.", "labels": [], "entities": []}, {"text": "Our joint probability model has the further advantage of being conceptually much simpler than the SVM labeller, which relies on a sophisticated machine learning paradigm.", "labels": [], "entities": []}, {"text": "Also, we need to compute only about one-fifth of the number of SVM features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test items: Verb-noun pairs with ratings  on a 7 point scale from McRae et al. (1998).", "labels": [], "entities": []}, {"text": " Table 2: Test sets: Total number of ratings and size of revised test sets containing only ratings for seen  verbs (% of total ratings). -: Coverage too low (26.7%).", "labels": [], "entities": [{"text": "Coverage", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9645493030548096}]}, {"text": " Table 4: Experiment 1: Combining the smoothing methods. Coverage on seen verbs (and on all items)  and correlation strength (Spearman's \u03c1) for PB and FN data. WN synsets as noun classes. Verb classes:  IB/ID: smoothing algorithm, followed by number of clusters. ns: not significant, *: p<0.05, **: p<0.01", "labels": [], "entities": [{"text": "correlation strength (Spearman's \u03c1)", "start_pos": 104, "end_pos": 139, "type": "METRIC", "confidence": 0.7925682323319572}]}]}