{"title": [], "abstractContent": [{"text": "We present and compare two approaches to the task of summarizing evaluative arguments.", "labels": [], "entities": [{"text": "summarizing evaluative arguments", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.9091934164365133}]}, {"text": "The first is a sentence extraction-based approach while the second is a language generation-based approach.", "labels": [], "entities": [{"text": "sentence extraction-based", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.7763960659503937}]}, {"text": "We evaluate these approaches in a user study and find that they quantitatively perform equally well.", "labels": [], "entities": []}, {"text": "Qualitatively, however, we find that they perform well for different but complementary reasons.", "labels": [], "entities": []}, {"text": "We conclude that an effective method for summarizing eval-uative arguments must effectively synthesize the two approaches.", "labels": [], "entities": [{"text": "summarizing eval-uative arguments", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.8732906579971313}]}], "introductionContent": [{"text": "Many organizations are faced with the challenge of summarizing large corpora of text data.", "labels": [], "entities": [{"text": "summarizing large corpora of text", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.8919976949691772}]}, {"text": "One important application is evaluative text, i.e. any document expressing an evaluation of an entity as either positive or negative.", "labels": [], "entities": []}, {"text": "For example, many websites collect large quantities of online customer reviews of consumer electronics.", "labels": [], "entities": []}, {"text": "Summaries of this literature could be of great strategic value to product designers, planners and manufacturers.", "labels": [], "entities": []}, {"text": "There are other equally important commercial applications, such as the summarization of travel logs, and non-commercial applications, such as the summarization of candidate reviews.", "labels": [], "entities": [{"text": "summarization of travel logs", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.8794606328010559}, {"text": "summarization of candidate reviews", "start_pos": 146, "end_pos": 180, "type": "TASK", "confidence": 0.8942016214132309}]}, {"text": "The general problem we consider in this paper is how to effectively summarize a large corpora of evaluative text about a single entity (e.g., a product).", "labels": [], "entities": [{"text": "summarize a large corpora of evaluative text", "start_pos": 68, "end_pos": 112, "type": "TASK", "confidence": 0.7809059194156102}]}, {"text": "In contrast, most previous work on multidocument summarization has focused on factual text (e.g., news ), biographies ().", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.7513263523578644}]}, {"text": "For factual documents, the goal of a summarizer is to select the most important facts and present them in a sensible ordering while avoiding repetition.", "labels": [], "entities": []}, {"text": "Previous work has shown that this can be effectively achieved by carefully extracting and ordering the most informative sentences from the original documents in a domain-independent way.", "labels": [], "entities": []}, {"text": "Notice however that when the source documents are assumed to contain inconsistent information (e.g., conflicting reports of a natural disaster ()), a different approach is needed.", "labels": [], "entities": []}, {"text": "The summarizer needs first to extract the information from the documents, then process such information to identify overlaps and inconsistencies between the different sources and finally produce a summary that points out and explain those inconsistencies.", "labels": [], "entities": []}, {"text": "A corpus of evaluative text typically contains a large number of possibly inconsistent 'facts' (i.e. opinions), as opinions on the same entity feature maybe uniform or varied.", "labels": [], "entities": []}, {"text": "Thus, summarizing a corpus of evaluative text is much more similar to summarizing conflicting reports than a consistent set of factual documents.", "labels": [], "entities": [{"text": "summarizing a corpus of evaluative text", "start_pos": 6, "end_pos": 45, "type": "TASK", "confidence": 0.8513296643892924}, {"text": "summarizing conflicting reports", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.90656578540802}]}, {"text": "When there are diverse opinions on the same issue, the different perspectives need to be included in the summary.", "labels": [], "entities": []}, {"text": "Based on this observation, we argue that any strategy to effectively summarize evaluative text about a single entity should rely on a preliminary phase of information extraction from the target corpus.", "labels": [], "entities": [{"text": "summarize evaluative text about a single entity", "start_pos": 69, "end_pos": 116, "type": "TASK", "confidence": 0.8660414048603603}]}, {"text": "In particular, the summarizer should at least know for each document: what features of the entity were evaluated, the polarity of the evaluations and their strengths.", "labels": [], "entities": []}, {"text": "In this paper, we explore this hypothesis by considering two alternative approaches.", "labels": [], "entities": []}, {"text": "First, we developed a sentence-extraction based summarizer that uses the information extracted from the corpus to select and rank sentences from the corpus.", "labels": [], "entities": []}, {"text": "We implemented this system, called MEAD*, by adapting MEAD (, an opensource framework for multi-document summarization.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.6409834623336792}]}, {"text": "Second, we developed a summarizer that produces summaries primarily by generating language from the information extracted from the corpus.", "labels": [], "entities": [{"text": "summarizer", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.9632940292358398}]}, {"text": "We implemented this system, called the Summarizer of Evaluative Arguments (SEA), by adapting the Generator of Evaluative Arguments (GEA)) a framework for generating user tailored evaluative arguments.", "labels": [], "entities": [{"text": "Summarizer of Evaluative Arguments (SEA)", "start_pos": 39, "end_pos": 79, "type": "TASK", "confidence": 0.7067976423672268}]}, {"text": "We have performed an empirical formative evaluation of MEAD* and SEA in a user study.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.833438515663147}, {"text": "SEA", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.873755156993866}]}, {"text": "In this evaluation, we also tested the effectiveness of human generated summaries (HGS) as a topline and of summaries generated by MEAD without access to the extracted information as a baseline.", "labels": [], "entities": [{"text": "summaries generated", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.8946241438388824}]}, {"text": "The results indicate that SEA and MEAD* quantitatively perform equally well above MEAD and below HGS.", "labels": [], "entities": [{"text": "SEA", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.8827928900718689}, {"text": "MEAD", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9006738662719727}, {"text": "MEAD", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.7360249757766724}, {"text": "HGS", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.7055709362030029}]}, {"text": "Qualitatively, we find that they perform well for different but complementary reasons.", "labels": [], "entities": []}, {"text": "While SEA appears to provide a more general overview of the source text, MEAD* seems to provide a more varied language and detail about customer opinions.", "labels": [], "entities": [{"text": "SEA", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.7858511805534363}, {"text": "MEAD", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.8066425919532776}]}], "datasetContent": [{"text": "We evaluated our two summarizers by performing a user study in which four treatments were considered: SEA, MEAD*, human-written summaries as a topline and summaries generated by MEAD (with all options set to default) as a baseline.", "labels": [], "entities": [{"text": "SEA", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9899319410324097}, {"text": "MEAD", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.8937090635299683}, {"text": "summaries generated by MEAD", "start_pos": 155, "end_pos": 182, "type": "TASK", "confidence": 0.7391355633735657}]}, {"text": "Twenty-eight undergraduate students participated in our experiment, seven for each treatment.", "labels": [], "entities": []}, {"text": "Each participant was given a set of 20 customer reviews randomly selected from a corpus of reviews.", "labels": [], "entities": []}, {"text": "In each treatment three participants received reviews from a corpus of 46 reviews of the Canon G3 digital camera and four received them from a corpus of 101 reviews of the Apex 2600 Progressive Scan DVD player, both obtained from Hu and Liu (2004b).", "labels": [], "entities": [{"text": "Canon G3 digital camera", "start_pos": 89, "end_pos": 112, "type": "DATASET", "confidence": 0.9147176593542099}, {"text": "Apex 2600 Progressive Scan DVD player", "start_pos": 172, "end_pos": 209, "type": "DATASET", "confidence": 0.9316458702087402}]}, {"text": "The reviews from these corpora which serve as input to our systems have been manually annotated with crude features, strength, and polarity.", "labels": [], "entities": []}, {"text": "We used this 'gold standard' for crude feature, strength, and polarity extraction because we wanted our experiments to focus on our summary and not be confounded by errors in the knowledge extraction phase.", "labels": [], "entities": [{"text": "polarity extraction", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7508362233638763}, {"text": "knowledge extraction phase", "start_pos": 179, "end_pos": 205, "type": "TASK", "confidence": 0.7687542339166006}]}, {"text": "The participant was told to pretend that they work for the manufacturer of the product (either Canon or Apex).", "labels": [], "entities": [{"text": "Canon or Apex", "start_pos": 95, "end_pos": 108, "type": "DATASET", "confidence": 0.7173731724421183}]}, {"text": "They were told that they would have to provide a 100 word summary of the reviews to the quality assurance department.", "labels": [], "entities": []}, {"text": "The purpose of these instructions was to prime the user to the task of looking for information worthy of summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.9812036156654358}]}, {"text": "They were then given 20 minutes to explore the set of reviews.", "labels": [], "entities": []}, {"text": "After 20 minutes, the participant was asked to stop.", "labels": [], "entities": []}, {"text": "The participant was then given a set of in-structions which explained that the company was testing a computer-based system for automatically generating a summary of the reviews s/he has been reading.", "labels": [], "entities": []}, {"text": "S/he was then shown a 100 word summary of the 20 reviews generated either by MEAD, MEAD*, SEA, or written by a human . shows four summaries of the same 20 reviews, one of each type.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.8584451675415039}, {"text": "MEAD", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.8805883526802063}, {"text": "SEA", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9553671479225159}]}, {"text": "In order to facilitate their analysis, summaries were displayed in a web browser.", "labels": [], "entities": [{"text": "summaries", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.9580484628677368}]}, {"text": "The upper portion of the browser contained the text of the summary with 'footnotes' linking to reviews on which the summary was based.", "labels": [], "entities": []}, {"text": "For MEAD and MEAD*, for each sentence the footnote pointed to the review from which the sentence had been extracted.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8860300183296204}, {"text": "MEAD", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9355831742286682}]}, {"text": "For SEA and human-generated summaries, for each aggregate evaluation the footnote pointed to the review containing a sample sentence on which that evaluation was based.", "labels": [], "entities": [{"text": "SEA", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9057326912879944}]}, {"text": "In all summaries, clicking on one of the footnotes caused the corresponding review to be displayed in which the appropriate sentence was highlighted.", "labels": [], "entities": []}, {"text": "Once finished, the participant was asked to fill out a questionnaire assessing the summary along several dimensions related to its effectiveness.", "labels": [], "entities": []}, {"text": "The participant could still access the summary while s/he worked on the questionnaire.", "labels": [], "entities": []}, {"text": "Our questionnaire consisted of nine questions.", "labels": [], "entities": []}, {"text": "The first five questions were the SEE linguistic well-formedness questions used at the 2005 Document Understanding Conference (DUC).", "labels": [], "entities": [{"text": "SEE linguistic well-formedness", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.7958759069442749}, {"text": "Document Understanding Conference (DUC)", "start_pos": 92, "end_pos": 131, "type": "TASK", "confidence": 0.621461903055509}]}, {"text": "The next three questions were designed to assess the content of the summary.", "labels": [], "entities": []}, {"text": "We based our questions on the Responsive evaluation at DUC 2005; however, we were interested in a more specific evaluation of the content that one overall rank.", "labels": [], "entities": [{"text": "Responsive evaluation at DUC 2005", "start_pos": 30, "end_pos": 63, "type": "DATASET", "confidence": 0.7644352376461029}]}, {"text": "As such, we split the content into the following three separate questions:   The final question in the questionnaire asked the participant to rank the overall quality of the summary holistically.", "labels": [], "entities": []}, {"text": "The first top half focuses on linguistic questions while the second bottom half focuses on content issues.", "labels": [], "entities": []}, {"text": "We performed a two-way ANOVA test with summary type as rows and the question sets as columns.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.714415967464447}]}, {"text": "Overall, it is easy to conclude that MEAD* and SEA performed at a roughly equal level, while the baseline MEAD performed significantly lower and the Human summarizer significantly higher (p \u00a1 \u00a3 001).", "labels": [], "entities": [{"text": "MEAD", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9680337905883789}, {"text": "SEA", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9924677014350891}, {"text": "MEAD", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9795497059822083}]}, {"text": "When individual questions/categories are considered, there are few questions that differentiate between MEAD* and SEA with a p-value below 0.05.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.8415220379829407}, {"text": "SEA", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9631098508834839}]}, {"text": "The primary reason is our small sample size.", "labels": [], "entities": []}, {"text": "Nonetheless, if we relax the p-value threshold, we can make the following observations/hypotheses.", "labels": [], "entities": []}, {"text": "To validate some of these hypotheses, we would conduct a larger user study in future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Quantative results of user responses to our questionnaire on a scale from 1 (Strongly Disagree)  to 5 (Strongly Agree).", "labels": [], "entities": []}]}