{"title": [{"text": "Exploiting Shallow Linguistic Information for Relation Extraction from Biomedical Literature", "labels": [], "entities": [{"text": "Relation Extraction from Biomedical Literature", "start_pos": 46, "end_pos": 92, "type": "TASK", "confidence": 0.9070294022560119}]}], "abstractContent": [{"text": "We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information.", "labels": [], "entities": [{"text": "extracting relations between entities from biomedical literature", "start_pos": 27, "end_pos": 91, "type": "TASK", "confidence": 0.8739242383411953}]}, {"text": "We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities.", "labels": [], "entities": []}, {"text": "We performed experiments on extracting gene and protein interactions from two different data sets.", "labels": [], "entities": [{"text": "extracting gene and protein interactions", "start_pos": 28, "end_pos": 68, "type": "TASK", "confidence": 0.8131129741668701}]}, {"text": "The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information Extraction (IE) is the process of finding relevant entities and their relationships within textual documents.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8437248110771179}]}, {"text": "Applications of IE range from Semantic Web to Bioinformatics.", "labels": [], "entities": [{"text": "IE", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9816147685050964}]}, {"text": "For example, there is an increasing interest in automatically extracting relevant information from biomedical literature.", "labels": [], "entities": [{"text": "automatically extracting relevant information from biomedical literature", "start_pos": 48, "end_pos": 120, "type": "TASK", "confidence": 0.8338586560317448}]}, {"text": "Recent evaluation campaigns on bio-entity recognition, such as BioCreAtIvE and JNLPBA 2004 shared task, have shown that several systems are able to achieve good performance (even if it is a bit worse than that reported on news articles).", "labels": [], "entities": [{"text": "bio-entity recognition", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7330500930547714}, {"text": "JNLPBA 2004 shared task", "start_pos": 79, "end_pos": 102, "type": "DATASET", "confidence": 0.7939464896917343}]}, {"text": "However, relation identification is more useful from an applicative perspective but it is still a considerable challenge for automatic tools.", "labels": [], "entities": [{"text": "relation identification", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.9713359177112579}]}, {"text": "In this work, we propose a supervised machine learning approach to relation extraction which is applicable even when (deep) linguistic processing is not available or reliable.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.9603399336338043}]}, {"text": "In particular, we explore a kernel-based approach based solely on shallow linguistic processing, such as tokenization, sentence splitting, Part-of-Speech (PoS) tagging and lemmatization.", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7238098233938217}, {"text": "Part-of-Speech (PoS) tagging", "start_pos": 139, "end_pos": 167, "type": "TASK", "confidence": 0.6638655543327332}]}, {"text": "Kernel methods show their full potential when an explicit computation of the feature map becomes computationally infeasible, due to the high or even infinite dimension of the feature space.", "labels": [], "entities": []}, {"text": "For this reason, kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information, in which the examples preserve their original representations (i.e. parse trees) and are compared by the kernel function (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.8836712837219238}]}, {"text": "Despite the positive results obtained exploiting syntactic information, we claim that there is still room for improvement relying exclusively on shallow linguistic information for two main reasons.", "labels": [], "entities": []}, {"text": "First of all, previous comparative evaluations put more stress on the deep linguistic approaches and did not put as much effort on developing effective methods based on shallow linguistic information.", "labels": [], "entities": []}, {"text": "A second reason concerns the fact that syntactic parsing is not always robust enough to deal with real-world sentences.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7855316698551178}]}, {"text": "This may prevent approaches based on syntactic features from producing any result.", "labels": [], "entities": []}, {"text": "Another related issue concerns the fact that parsers are available only for few languages and may not produce reliable results when used on domain specific texts (as is the case of the biomedical literature).", "labels": [], "entities": []}, {"text": "For example, most of the participants at the Learning Language in Logic (LLL) challenge on Genic Interaction Extraction (see Section 4.2) were unable to successfully exploit linguistic information provided by parsers.", "labels": [], "entities": [{"text": "Learning Language in Logic (LLL) challenge on Genic Interaction Extraction", "start_pos": 45, "end_pos": 119, "type": "TASK", "confidence": 0.7325508172313372}]}, {"text": "It is still an open issue whether the use of domainspecific treebanks (such as the Genia treebank 1 ) can be successfully exploited to overcome this problem.", "labels": [], "entities": [{"text": "Genia treebank 1", "start_pos": 83, "end_pos": 99, "type": "DATASET", "confidence": 0.9055018027623495}]}, {"text": "Therefore it is essential to better investigate the potential of approaches based exclusively on simple linguistic features.", "labels": [], "entities": []}, {"text": "In our approach we use a combination of kernel functions to represent two distinct information sources: the global context where entities appear and their local contexts.", "labels": [], "entities": []}, {"text": "The whole sentence where the entities appear (global context) is used to discover the presence of a relation between two entities, similarly to what was done by.", "labels": [], "entities": []}, {"text": "Windows of limited size around the entities (local contexts) provide useful clues to identify the roles of the entities within a relation.", "labels": [], "entities": []}, {"text": "The approach has some resemblance with what was proposed by.", "labels": [], "entities": []}, {"text": "The main difference is that we perform the extraction task in a single step via a combined kernel, while they used two separate classifiers to identify entities and relations and their output is later combined with a probabilistic global inference.", "labels": [], "entities": []}, {"text": "We evaluated our relation extraction algorithm on two biomedical data sets (i.e. the AImed corpus and the LLL challenge data set; see Section 4).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.8217109143733978}, {"text": "AImed corpus", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.8132719099521637}, {"text": "LLL challenge data set", "start_pos": 106, "end_pos": 128, "type": "DATASET", "confidence": 0.8241271525621414}]}, {"text": "The motivations for using these benchmarks derive from the increasing applicative interest in tools able to extract relations between relevant entities in biomedical texts and, consequently, from the growing availability of annotated data sets.", "labels": [], "entities": []}, {"text": "The experiments show clearly that our approach consistently improves previous results.", "labels": [], "entities": []}, {"text": "Surprisingly, it outperforms most of the systems based on syntactic or semantic information, even when this information is manually annotated (i.e. the LLL challenge).", "labels": [], "entities": []}], "datasetContent": [{"text": "Before describing the results of the experiments, a note concerning the evaluation methodology.", "labels": [], "entities": []}, {"text": "There are different ways of evaluating performance in extracting information, as noted in () for the extraction of slot fillers in the Seminar Announcement and the Job Posting data sets.", "labels": [], "entities": [{"text": "extraction of slot fillers", "start_pos": 101, "end_pos": 127, "type": "TASK", "confidence": 0.6818850338459015}, {"text": "Job Posting data sets", "start_pos": 164, "end_pos": 185, "type": "DATASET", "confidence": 0.7861945629119873}]}, {"text": "Adapting the proposed classification to relation extraction, the following two cases can be identified: \u2022 One Answer per Occurrence in the Document -OAOD (each individual occurrence of a protein interaction has to be extracted from the document); \u2022 One Answer per Relation in a given Document -OARD (where two occurrences of the same protein interaction are considered one correct answer).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8358822464942932}]}, {"text": "shows a fragment of tagged text drawn from the AImed corpus.", "labels": [], "entities": [{"text": "AImed corpus", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.8601170778274536}]}, {"text": "It contains three different interactions between pairs of proteins, fora total of seven occurrences of interactions.", "labels": [], "entities": []}, {"text": "For example, there are three occurrences of the interaction between IGF-IR and p52Shc (i.e. number 1, 3 and 7).", "labels": [], "entities": []}, {"text": "If we adopt the OAOD methodology, all the seven occurrences have to be extracted to achieve the maximum score.", "labels": [], "entities": [{"text": "OAOD", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.5396140217781067}]}, {"text": "On the other hand, if we use the OARD methodology, only one occurrence for each interaction has to be extracted to maximize the score.", "labels": [], "entities": [{"text": "OARD methodology", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.657863050699234}]}, {"text": "On the AImed data set both evaluations were performed, while on the LLL challenge only the OAOD evaluation methodology was performed because this is the only one provided by the evaluation server of the challenge.", "labels": [], "entities": [{"text": "AImed data set", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.8292694886525472}, {"text": "OAOD", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9298165440559387}]}], "tableCaptions": [{"text": " Table 1: Performance on the AImed data set us- ing the two evaluation methodologies, OAOD and  OARD.", "labels": [], "entities": [{"text": "AImed data set", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.8463275035222372}, {"text": "OAOD", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.7552843689918518}, {"text": "OARD", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.8022388219833374}]}, {"text": " Table 2: K SL performance on the LLL challenge  test set using only the basic linguistic information.", "labels": [], "entities": [{"text": "LLL challenge  test set", "start_pos": 34, "end_pos": 57, "type": "DATASET", "confidence": 0.8509152829647064}]}, {"text": " Table 3: Best performance on basic and enriched  test sets obtained by participants in the official  competition at the LLL challenge.", "labels": [], "entities": [{"text": "LLL challenge", "start_pos": 121, "end_pos": 134, "type": "TASK", "confidence": 0.7459868788719177}]}, {"text": " Table 4: Comparison of the performance of kernel  combination on the LLL challenge using 10-fold  cross validation.", "labels": [], "entities": []}]}