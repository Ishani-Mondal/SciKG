{"title": [{"text": "Adaptivity in Question Answering with User Modelling and a Dialogue Interface", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7692307531833649}]}], "abstractContent": [{"text": "Most question answering (QA) and information retrieval (IR) systems are insensitive to different users' needs and preferences , and also to the existence of multiple , complex or controversial answers.", "labels": [], "entities": [{"text": "question answering (QA) and information retrieval (IR)", "start_pos": 5, "end_pos": 59, "type": "TASK", "confidence": 0.8021508021788164}]}, {"text": "We introduce adaptivity in QA and IR by creating a hybrid system based on a dialogue interface and a user model.", "labels": [], "entities": [{"text": "IR", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.8987445831298828}]}], "introductionContent": [{"text": "While standard information retrieval (IR) systems present the results of a query in the form of a ranked list of relevant documents, question answering (QA) systems attempt to return them in the form of sentences (or paragraphs, or phrases), responding more precisely to the user's request.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.8505902647972107}, {"text": "question answering (QA)", "start_pos": 133, "end_pos": 156, "type": "TASK", "confidence": 0.8107672333717346}]}, {"text": "However, inmost state-of-the-art QA systems the output remains independent of the questioner's characteristics, goals and needs.", "labels": [], "entities": []}, {"text": "In other words, there is alack of user modelling: a 10-year-old and a University History student would get the same answer to the question: \"When did the Middle Ages begin?\".", "labels": [], "entities": []}, {"text": "Secondly, most of the effort of current QA is on factoid questions, i.e. questions concerning people, dates, etc., which can generally be answered by a short sentence or phrase ().", "labels": [], "entities": []}, {"text": "The main QA evaluation campaign, TREC-QA 1 , has long focused on this type of questions, for which the simplifying assumption is that there exists only one correct answer.", "labels": [], "entities": [{"text": "TREC-QA 1", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.5609830915927887}]}, {"text": "Even recent TREC campaigns) do not move sufficiently beyond the factoid approach.", "labels": [], "entities": []}, {"text": "They account for two types of nonfactoid questions -list and definitional-but not for non-factoid answers.", "labels": [], "entities": []}, {"text": "In fact, a) TREC defines list questions as questions requiring multiple factoid 1 http://trec.nist.gov answers, b) it is clear that a definition question maybe answered by spotting definitional passages (what is not clear is how to spot them).", "labels": [], "entities": []}, {"text": "However, accounting for the fact that some simple questions may have complex or controversial answers (e.g. \"What were the causes of World War II?\") remains an unsolved problem.", "labels": [], "entities": []}, {"text": "We argue that in such situations returning a short paragraph or text snippet is more appropriate than exact answer spotting.", "labels": [], "entities": [{"text": "exact answer spotting", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.5794555346171061}]}, {"text": "Finally, QA systems rarely interact with the user: the typical session involves the user submitting a query and the system returning a result; the session is then concluded.", "labels": [], "entities": []}, {"text": "To respond to these deficiencies of existing QA systems, we propose an adaptive system where a QA module interacts with a user model and a dialogue interface (see).", "labels": [], "entities": []}, {"text": "The dialogue interface provides the query terms to the QA module, and the user model (UM) provides criteria to adapt query results to the user's needs.", "labels": [], "entities": []}, {"text": "Given such information, the goal of the QA module is to be able to discriminate between simple/factoid answers and more complex answers, presenting them in a TREC-style manner in the first case and more appropriately in the second.", "labels": [], "entities": []}, {"text": "Related work To our knowledge, our system is among the first to address the need fora different approach to non-factoid (complex/controversial) answers.", "labels": [], "entities": []}, {"text": "Although the three-tiered structure of our QA module reflects that of atypical webbased QA system, e.g. MULDER (), a significant aspect of novelty in our architecture is that the QA component is supported by the user model.", "labels": [], "entities": []}, {"text": "Additionally, we drastically reduce the amount of linguistic processing applied during question processing and answer generation, while giving more relief to the post-retrieval phase and to the role of the UM.", "labels": [], "entities": [{"text": "question processing", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7897355556488037}, {"text": "answer generation", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.8061607778072357}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Sample queries and accuracy values", "labels": [], "entities": [{"text": "Sample queries", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8407429158687592}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9993088245391846}]}]}