{"title": [{"text": "Towards Robust Context-Sensitive Sentence Alignment for Monolingual Corpora", "labels": [], "entities": [{"text": "Robust Context-Sensitive Sentence Alignment", "start_pos": 8, "end_pos": 51, "type": "TASK", "confidence": 0.783243715763092}]}], "abstractContent": [{"text": "Aligning sentences belonging to comparable monolingual corpora has been suggested as a first step towards training text rewriting algorithms, for tasks such as summarization or paraphrasing.", "labels": [], "entities": [{"text": "text rewriting", "start_pos": 115, "end_pos": 129, "type": "TASK", "confidence": 0.718223437666893}, {"text": "summarization", "start_pos": 160, "end_pos": 173, "type": "TASK", "confidence": 0.9866357445716858}]}, {"text": "We present here anew monolingual sentence alignment algorithm, combining a sentence-based TF*IDF score, turned into a probability distribution using logistic regression , with a global alignment dynamic programming algorithm.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7169777601957321}, {"text": "TF*IDF score", "start_pos": 90, "end_pos": 102, "type": "METRIC", "confidence": 0.8459192961454391}]}, {"text": "Our approach provides a simpler and more robust solution achieving a substantial improvement inaccuracy over existing systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence-aligned bilingual corpora area crucial resource for training statistical machine translation systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6302669644355774}]}, {"text": "Several authors have suggested that large-scale aligned monolingual corpora could be similarly used to advance the performance of monolingual text-to-text rewriting systems, for tasks including summarization) and paraphrasing ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 194, "end_pos": 207, "type": "TASK", "confidence": 0.9872006177902222}]}, {"text": "Unlike bilingual corpora, such as the Canadian Hansard corpus, which are relatively rare, it is now fairly easy to amass corpora of related monolingual documents.", "labels": [], "entities": [{"text": "Canadian Hansard corpus", "start_pos": 38, "end_pos": 61, "type": "DATASET", "confidence": 0.8106490174929301}]}, {"text": "For instance, with the advent of news aggregator services such as \"Google News\", one can readily collect multiple news stories covering the same news item ( ).", "labels": [], "entities": []}, {"text": "Utilizing such a resource requires aligning related documents at a finer level of resolution, identifying which sentences from one document align with which sentences from the other.", "labels": [], "entities": [{"text": "resolution", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9702310562133789}]}, {"text": "Previous work has shown that aligning related monolingual documents is quite different from the well-studied multi-lingual alignment task.", "labels": [], "entities": []}, {"text": "Whereas documents in a bilingual corpus are typically very closely aligned, monolingual corpora exhibit a much looser level of alignment, with similar content expressed using widely divergent wording, grammatical form, and sentence order.", "labels": [], "entities": []}, {"text": "Consequently, many of the simple surface-based methods that have proven to be so successful in bilingual sentence alignment, such as correlation of sentence length, linearity of alignment, and a predominance of one-to-one sentence mapping, are much less likely to be effective for monolingual sentence alignment.", "labels": [], "entities": [{"text": "bilingual sentence alignment", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.6127726833025614}, {"text": "monolingual sentence alignment", "start_pos": 281, "end_pos": 311, "type": "TASK", "confidence": 0.6364312271277109}]}, {"text": "suggested that these disadvantages could beat least partially offset by the recurrence of the same lexical items in document pairs.", "labels": [], "entities": []}, {"text": "Indeed, they showed that a simple cosine word-overlap score is a good baseline for the task, outperforming much more sophisticated methods.", "labels": [], "entities": []}, {"text": "They also observed that context is a powerful factor in determining alignment.", "labels": [], "entities": []}, {"text": "They illustrated this on a corpus of Encyclopedia Britannica entries describing world cities, where each entry comes in two flavors, the comprehensive encyclopedia entry, and a shorter and simpler elementary version.", "labels": [], "entities": []}, {"text": "Barzilay and Elhadad used context in two different forms.", "labels": [], "entities": []}, {"text": "First, using interdocument context, they took advantage of commonalities in the topical structure of the encyclopedia entries to identify paragraphs that are likely to be about the same topic.", "labels": [], "entities": []}, {"text": "They then took advantage of intra-document context by using dynamic programming to locally align sequences of sentences belonging to paragraphs about the same topic, yielding improved accuracy on the corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9987572431564331}]}, {"text": "While powerful, such commonalities in document structure appear to be a special feature of the Britannica corpus, and therefore cannot be relied upon for other corpora.", "labels": [], "entities": [{"text": "Britannica corpus", "start_pos": 95, "end_pos": 112, "type": "DATASET", "confidence": 0.9838633239269257}]}, {"text": "In this paper we present a novel algorithm for sentence alignment in monolingual corpora.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7405317723751068}]}, {"text": "At the core of the algorithm is a classical similar-ity score based on differentially weighting words according to their Term Frequency-Inverse Document Frequency (TF*IDF).", "labels": [], "entities": [{"text": "Term Frequency-Inverse Document Frequency (TF*IDF)", "start_pos": 121, "end_pos": 171, "type": "METRIC", "confidence": 0.8262925876511468}]}, {"text": "We treat sentences as documents, and the collection of sentences in the two documents being compared as the document collection, and use this score to estimate the probability that two sentences are aligned using logistic regression.", "labels": [], "entities": []}, {"text": "Surprisingly, this approach by itself yields competitive accuracy, yielding the same level of accuracy as Barzilay and Elhadad's algorithm, and higher than all previous approaches on the Britannica corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9974672794342041}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.998910665512085}, {"text": "Britannica corpus", "start_pos": 187, "end_pos": 204, "type": "DATASET", "confidence": 0.980353593826294}]}, {"text": "Such matching, however, is still noisy.", "labels": [], "entities": []}, {"text": "We further improve accuracy by using a global alignment dynamic programming algorithm, which prunes many spurious matches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9981858134269714}, {"text": "global alignment dynamic programming", "start_pos": 39, "end_pos": 75, "type": "TASK", "confidence": 0.7682149112224579}]}, {"text": "Our approach validates Barzilay and Elhadad's observation regarding the utility of incorporating context.", "labels": [], "entities": []}, {"text": "In fact, we are able to extract more information out of the intra-document context.", "labels": [], "entities": []}, {"text": "First, by using TF*IDF at the level of sentences, we weigh words in a sentence with respect to other sentences of the document.", "labels": [], "entities": [{"text": "TF*IDF", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.8448134660720825}]}, {"text": "Second, global alignment takes advantage of (noisy) linear order of sentences.", "labels": [], "entities": [{"text": "global alignment", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.8238732814788818}]}, {"text": "We make no use of inter-document context, and in particular make no assumptions about common topical structure that are unique to the Britannica corpus, thus ensuring the scalability of the approach.", "labels": [], "entities": [{"text": "Britannica corpus", "start_pos": 134, "end_pos": 151, "type": "DATASET", "confidence": 0.9702141582965851}]}, {"text": "Indeed, we successfully apply our algorithm to a very different corpus, the three Synoptic gospels of the New Testament: Matthew, Mark, and Luke.", "labels": [], "entities": []}, {"text": "Putting aside any religious or theological significance of these texts, they offer an excellent data source for studying alignment, since they contain many parallels, which have been conveniently annotated by bible scholars.", "labels": [], "entities": [{"text": "alignment", "start_pos": 121, "end_pos": 130, "type": "TASK", "confidence": 0.958179771900177}]}, {"text": "Our algorithm achieves a significant improvement over the baseline for this corpus as well, demonstrating the general applicability of our approach.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Precision at 55.8% Recall", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8309946060180664}, {"text": "Recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9872817397117615}]}]}