{"title": [{"text": "A Comparison of Syntactically Motivated Word Alignment Spaces", "labels": [], "entities": [{"text": "Syntactically Motivated Word Alignment Spaces", "start_pos": 16, "end_pos": 61, "type": "TASK", "confidence": 0.6847116947174072}]}], "abstractContent": [{"text": "This work is concerned with the space of alignments searched byword alignment systems.", "labels": [], "entities": [{"text": "alignments searched byword alignment", "start_pos": 41, "end_pos": 77, "type": "TASK", "confidence": 0.7164872884750366}]}, {"text": "We focus on situations where word reordering is limited by syntax.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.721838116645813}]}, {"text": "We present two new alignment spaces that limit an ITG according to a given dependency parse.", "labels": [], "entities": []}, {"text": "We provide D-ITG grammars to search these spaces completely and without redundancy.", "labels": [], "entities": []}, {"text": "We conduct a careful comparison of five alignment spaces, and show that limiting search with an ITG reduces error rate by 10%, while a D-ITG produces a 31% reduction.", "labels": [], "entities": [{"text": "error rate", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9906814992427826}]}], "introductionContent": [{"text": "Bilingual word alignment finds word-level correspondences between parallel sentences.", "labels": [], "entities": [{"text": "Bilingual word alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7593132853507996}]}, {"text": "The task originally emerged as an intermediate result of training the IBM translation models).", "labels": [], "entities": [{"text": "IBM translation", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.4232408404350281}]}, {"text": "These models use minimal linguistic intuitions; they essentially treat sentences as flat strings.", "labels": [], "entities": []}, {"text": "They remain the dominant method for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.8052438199520111}]}, {"text": "There have been several proposals to introduce syntax into word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.774630218744278}]}, {"text": "Some work within the framework of synchronous grammars, while others create a generative story that includes a parse tree provided for one of the sentences).", "labels": [], "entities": []}, {"text": "There are three primary reasons to add syntax to word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.7521717250347137}]}, {"text": "First, one can incorporate syntactic features, such as grammar productions, into the models that guide the alignment search.", "labels": [], "entities": []}, {"text": "Second, movement can be modeled more naturally; when a three-word noun phrase moves during translation, it can be modeled as one movement operation instead of three.", "labels": [], "entities": []}, {"text": "Finally, one can restrict the type of movement that is considered, shrinking the number of alignments that are attempted.", "labels": [], "entities": []}, {"text": "We investigate this last advantage of syntactic alignment.", "labels": [], "entities": [{"text": "syntactic alignment", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7257642447948456}]}, {"text": "We fix an alignment scoring model that works equally well on flat strings as on parse trees, but we vary the space of alignments evaluated with that model.", "labels": [], "entities": []}, {"text": "These spaces become smaller as more linguistic guidance is added.", "labels": [], "entities": []}, {"text": "We measure the benefits and detriments of these constrained searches.", "labels": [], "entities": []}, {"text": "Several of the spaces we investigate draw guidance from a dependency tree for one of the sentences.", "labels": [], "entities": []}, {"text": "We will refer to the parsed language as English and the other as Foreign.", "labels": [], "entities": []}, {"text": "have shown that adding a dependency-based cohesion constraint to an alignment search can improve alignment quality.", "labels": [], "entities": []}, {"text": "Unfortunately, the usefulness of their beam search solution is limited: potential alignments are constructed explicitly, which prevents a perfect search of alignment space and the use of algorithms like EM.", "labels": [], "entities": [{"text": "beam search", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.8529836237430573}]}, {"text": "However, the cohesion constraint is based on a tree, which should make it amenable to dynamic programming solutions.", "labels": [], "entities": []}, {"text": "To enable such techniques, we bring the cohesion constraint inside the ITG framework ().", "labels": [], "entities": [{"text": "ITG framework", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9023869037628174}]}, {"text": "tree-to-string alignment model to ITGs.", "labels": [], "entities": [{"text": "tree-to-string alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5847341418266296}]}, {"text": "They concluded that methods like ITGs, which create a tree during alignment, perform better than methods with a fixed tree established before alignment begins.", "labels": [], "entities": []}, {"text": "However, the use of a fixed tree is not the only difference between) and ITGs; the probability models are also very different.", "labels": [], "entities": []}, {"text": "By using a fixed dependency tree inside an ITG, we can revisit the question of whether using a fixed tree is harmful, but in a controlled environment.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the alignment spaces described in this paper under two criteria.", "labels": [], "entities": []}, {"text": "First we test the guidance provided by a space, or its capacity to stop an aligner from selecting bad alignments.", "labels": [], "entities": []}, {"text": "We also test expressiveness, or how often a space allows an aligner to select the best alignment.", "labels": [], "entities": []}, {"text": "In all cases, we report our results in terms of alignment quality, using the standard word alignment error metrics: precision, recall, F-measure and alignment error rate.", "labels": [], "entities": [{"text": "word alignment error metrics", "start_pos": 86, "end_pos": 114, "type": "METRIC", "confidence": 0.6729927808046341}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9944877028465271}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.995971143245697}, {"text": "F-measure", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9933057427406311}, {"text": "alignment error rate", "start_pos": 149, "end_pos": 169, "type": "METRIC", "confidence": 0.9657765030860901}]}, {"text": "Our test set is the 500 manually aligned sentence pairs created by Franz Och and Hermann . These English-French pairs are drawn from the Canadian Hansards.", "labels": [], "entities": [{"text": "Canadian Hansards", "start_pos": 137, "end_pos": 154, "type": "DATASET", "confidence": 0.8268835246562958}]}, {"text": "English dependency trees are supplied by Minipar (Lin, 1994).", "labels": [], "entities": [{"text": "Minipar (Lin, 1994)", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.9155759712060293}]}], "tableCaptions": [{"text": " Table 1: Results with the learned link score.  Method Prec Rec  F  AER  Greedy  78.1 81.4 79.5 20.47  Beam  79.1 82.7 80.7 19.32  Match  79.3 82.7 80.8 19.24  ITG  81.8 83.7 82.6 17.36  Dep  88.8 84.0 86.6 13.40  D-ITG  88.8 84.2 86.7 13.32  HD-ITG 89.2 84.0 86.9 13.15", "labels": [], "entities": [{"text": "AER  Greedy  78.1 81.4 79.5 20.47  Beam  79.1 82.7 80.7 19.32  Match  79.3 82.7 80.8 19.24  ITG  81.8 83.7 82.6 17.36  Dep  88.8 84.0 86.6 13.40  D-ITG  88.8 84.2 86.7 13.32  HD-ITG 89.2 84.0 86.9 13.15", "start_pos": 68, "end_pos": 270, "type": "DATASET", "confidence": 0.8181855264637206}]}, {"text": " Table 2: Results with the perfect link score.", "labels": [], "entities": [{"text": "perfect link score", "start_pos": 27, "end_pos": 45, "type": "METRIC", "confidence": 0.892957329750061}]}]}