{"title": [{"text": "Generating statistical language models from interpretation grammars in dialogue systems", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we explore statistical language modelling fora speech-enabled MP3 player application by generating a corpus from the interpretation grammar written for the application with the Grammatical Framework (GF) (Ranta, 2004).", "labels": [], "entities": [{"text": "statistical language modelling", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.7091177105903625}]}, {"text": "We create a statistical language model (SLM) directly from our interpretation grammar and compare recognition performance of this model against a speech recognition grammar compiled from the same GF interpretation grammar.", "labels": [], "entities": []}, {"text": "The results show a relative Word Error Rate (WER) reduction of 37% for the SLM derived from the interpretation grammar while maintaining a low in-grammar WER comparable to that associated with the speech recognition grammar.", "labels": [], "entities": [{"text": "Word Error Rate (WER) reduction", "start_pos": 28, "end_pos": 59, "type": "METRIC", "confidence": 0.9208894882883344}, {"text": "WER", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.8292298913002014}, {"text": "speech recognition grammar", "start_pos": 197, "end_pos": 223, "type": "TASK", "confidence": 0.7596975664297739}]}, {"text": "From this starting point we try to improve our artificially generated model by interpolating it with different corpora achieving great reduction in perplexity and 8% relative recognition improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ideally when building spoken dialogue systems, we would like to use a corpus of transcribed dialogues corresponding to the specific task of the dialogue system, in order to build a statistical language model (SLM).", "labels": [], "entities": []}, {"text": "However, it is rarely the case that such a corpus exists in the early stage of the development of a dialogue system.", "labels": [], "entities": []}, {"text": "Collecting such a corpus and transcribing it is very timeconsuming and delays the building of the actual dialogue system.", "labels": [], "entities": []}, {"text": "An approach taken both in dialogue systems and dictation applications is to first write an interpretation grammar and from that generate an artificial corpus which is used as training corpus for the SLM).", "labels": [], "entities": [{"text": "SLM", "start_pos": 199, "end_pos": 202, "type": "TASK", "confidence": 0.913994312286377}]}, {"text": "These models obtained from grammars are not as good as the ones built from real data as the estimates are artificial, lacking areal distribution.", "labels": [], "entities": []}, {"text": "However, it is a quick way to get a dialogue system working with an SLM.", "labels": [], "entities": []}, {"text": "When the system is up and running it is possible to collect real data that can be used to improve the model.", "labels": [], "entities": []}, {"text": "We will explore this idea by generating a corpus from an interpretation grammar from one of our applications.", "labels": [], "entities": []}, {"text": "A different approach is to compile the interpretation grammar into a speech recognition grammar as the Gemini and REGULUS compilers do.", "labels": [], "entities": []}, {"text": "In this way it is assured that the linguistic coverage of the speech recognition and interpretation are kept in sync.", "labels": [], "entities": [{"text": "speech recognition and interpretation", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.7194385752081871}]}, {"text": "Such an approach enables us to interpret all that we can recognize and the other way round.", "labels": [], "entities": []}, {"text": "In the European-funded project TALK the Grammatical Framework () has been extended with such a facility that compiles GF grammars into speech recognition grammars in Nuance GSL format (www.nuance.com).", "labels": [], "entities": []}, {"text": "Speech recognition for commercial dialogue systems has focused on grammar-based approaches despite the fact that statistical language models seem to have a better overall performance).", "labels": [], "entities": [{"text": "Speech recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.793230414390564}]}, {"text": "This probably depends on the time-consuming work of collecting corpora for training SLMs compared with the more rapid and straightforward development of speech recognition grammars.", "labels": [], "entities": [{"text": "speech recognition grammars", "start_pos": 153, "end_pos": 180, "type": "TASK", "confidence": 0.765600303808848}]}, {"text": "However, SLMs are more robust, can handle out-of-coverage output, perform better in difficult conditions and seem to work bet-ter for naive users (see)) while speech recognition grammars are limited in their coverage depending on how well grammar writers succeed in predicting what users may say).", "labels": [], "entities": []}, {"text": "Nevertheless, as grammars only output phrases that can be interpreted their output makes the following interpretation task easier than with the unpredictable output from an SLM (especially if the speech recognition grammar has been compiled from the interpretation grammar and these are both in sync).", "labels": [], "entities": []}, {"text": "In addition, the grammar-based approach in the experiments reported in) outperforms the SLM approach on semantic error rate on in-coverage data.", "labels": [], "entities": []}, {"text": "This has lead to the idea of trying to combine both approaches, as shown in . This is also something that we are aiming for.", "labels": [], "entities": []}, {"text": "Domain adaptation of SLMs is another issue in dialogue system recognition which involves reusing a successful language model by adapting it to anew domain i.e. anew application).", "labels": [], "entities": [{"text": "Domain adaptation of SLMs", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7797384411096573}, {"text": "dialogue system recognition", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.7231970429420471}]}, {"text": "If a large corpus is not available for the specific domain but there is a corpus fora collection of topics we could use this corpus and adapt the resulting SLM to the domain.", "labels": [], "entities": []}, {"text": "One may assume that the resulting SLM based on a large corpus with a good mixture of topics should be able to capture at least apart of general language use that does not vary from one domain to another.", "labels": [], "entities": []}, {"text": "We will explore this idea by using the Gothenburg Spoken Language Corpus (GSLC)) and a newspaper corpus to adapt these to our MP3 domain.", "labels": [], "entities": [{"text": "Gothenburg Spoken Language Corpus (GSLC))", "start_pos": 39, "end_pos": 80, "type": "DATASET", "confidence": 0.8917097875050136}, {"text": "newspaper corpus", "start_pos": 87, "end_pos": 103, "type": "DATASET", "confidence": 0.844944179058075}]}, {"text": "We will consider several different SLMs based on the corpus generated from the GF interpretation grammar and compare their recognition performance with the baseline: a Speech Recognition Grammar in Nuance format compiled from the same interpretation grammar.", "labels": [], "entities": [{"text": "GF interpretation grammar", "start_pos": 79, "end_pos": 104, "type": "DATASET", "confidence": 0.7506134708722433}]}, {"text": "Hence, what we could expect from our experiment, by looking at earlier research, is very low word error rate for our speech recognition grammar on in-grammar coverage but a lot worse performance on out-ofgrammar coverage.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 93, "end_pos": 108, "type": "METRIC", "confidence": 0.7212731838226318}, {"text": "speech recognition grammar", "start_pos": 117, "end_pos": 143, "type": "TASK", "confidence": 0.7738149364789327}]}, {"text": "The SLMs we are considering should tackle out-of-grammar utterances better and it will be interesting to see how well these models built from the grammar will perform on in-grammar utterances.", "labels": [], "entities": []}, {"text": "This study is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the domain for which we are doing language modelling and the corpora we have at our disposal.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7500693500041962}]}, {"text": "Section 3 will describe the different SLMs we have generated.", "labels": [], "entities": []}, {"text": "Section 4 describes the evaluation of these and the results.", "labels": [], "entities": []}, {"text": "Finally, we review the main conclusions of the work and discuss future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Perplexity for the different SLMs.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.967923641204834}]}]}