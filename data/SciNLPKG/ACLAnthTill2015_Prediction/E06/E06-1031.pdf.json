{"title": [{"text": "CDER: Efficient MT Evaluation Using Block Movements", "labels": [], "entities": [{"text": "MT Evaluation", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9095610976219177}]}], "abstractContent": [{"text": "Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.8114312291145325}]}, {"text": "In many cases though such movements still result incorrect or almost correct sentences.", "labels": [], "entities": []}, {"text": "In this paper, we will present anew evaluation measure which explicitly models block reordering as an edit operation.", "labels": [], "entities": []}, {"text": "Our measure can be exactly calculated in quadratic time.", "labels": [], "entities": []}, {"text": "Furthermore, we will show how some evaluation measures can be improved by the introduction of word-dependent substitution costs.", "labels": [], "entities": []}, {"text": "The correlation of the new measure with human judgment has been investigated systematically on two different language pairs.", "labels": [], "entities": []}, {"text": "The experimental results will show that it significantly outperforms state-of-the-art approaches in sentence-level correlation.", "labels": [], "entities": []}, {"text": "Results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between automatic evaluation measures and human judgment.", "labels": [], "entities": [{"text": "word dependent substitution", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.6666492422421774}]}], "introductionContent": [{"text": "Research in machine translation (MT) depends heavily on the evaluation of its results.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.8703041076660156}]}, {"text": "Especially for the development of an MT system, an evaluation measure is needed which reliably assesses the quality of MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.990641176700592}, {"text": "MT", "start_pos": 119, "end_pos": 121, "type": "TASK", "confidence": 0.9833963513374329}]}, {"text": "Such a measure will help analyze the strengths and weaknesses of different translation systems or different versions of the same system by comparing output at the sentence level.", "labels": [], "entities": []}, {"text": "In most applications of MT, understandability for humans in terms of readability as well as semantical correctness should be the evaluation criterion.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9922343492507935}]}, {"text": "But as human evaluation is tedious and cost-intensive, automatic evaluation measures are used inmost MT research tasks.", "labels": [], "entities": [{"text": "MT research", "start_pos": 101, "end_pos": 112, "type": "TASK", "confidence": 0.9276556670665741}]}, {"text": "A high correlation between these automatic evaluation measures and human evaluation is thus desirable.", "labels": [], "entities": []}, {"text": "State-of-the-art measures such as BLEU) or NIST aim at measuring the translation quality rather on the document level 1 than on the level of single sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9961894154548645}, {"text": "NIST", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8908609747886658}]}, {"text": "They are thus not well-suited for sentence-level evaluation.", "labels": [], "entities": []}, {"text": "The introduction of smoothing () solves this problem only partially.", "labels": [], "entities": []}, {"text": "In this paper, we will present anew automatic error measure for MT -the CDER -which is designed for assessing MT quality on the sentence level.", "labels": [], "entities": [{"text": "automatic error measure", "start_pos": 36, "end_pos": 59, "type": "METRIC", "confidence": 0.7662904063860575}, {"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9841012358665466}, {"text": "MT quality", "start_pos": 110, "end_pos": 120, "type": "TASK", "confidence": 0.8243062794208527}]}, {"text": "It is based on edit distance -such as the well-known word error rate (WER) -but allows for reordering of blocks.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 53, "end_pos": 74, "type": "METRIC", "confidence": 0.8491446723540624}]}, {"text": "Nevertheless, by defining reordering costs, the ordering of the words in a sentence is still relevant for the measure.", "labels": [], "entities": []}, {"text": "In this, the new measure differs significantly from the position independent error rate (PER) by ().", "labels": [], "entities": [{"text": "position independent error rate (PER)", "start_pos": 56, "end_pos": 93, "type": "METRIC", "confidence": 0.8793834447860718}]}, {"text": "Generally, finding an optimal solution for such a reordering problem is NP hard, as is shown in.", "labels": [], "entities": []}, {"text": "In previous work, researchers have tried to reduce the complexity, for example by restricting the possible permutations on the block-level, or by approximation or heuristics during the calculation.", "labels": [], "entities": []}, {"text": "Nevertheless, most of the resulting algorithms still have high run times and are hardly applied in practice, or give only a rough approximation.", "labels": [], "entities": []}, {"text": "An overview of some better-known measures can be found in Section 3.1.", "labels": [], "entities": []}, {"text": "In contrast to this, our new measure can be calculated very efficiently.", "labels": [], "entities": []}, {"text": "This is achieved by requiring complete and disjoint coverage of the blocks only for the reference sentence, and not for the candidate translation.", "labels": [], "entities": []}, {"text": "We will present an algorithm which computes the new error measure in quadratic time.", "labels": [], "entities": []}, {"text": "The new evaluation measure will be investigated and compared to state-of-the-art methods on two translation tasks.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.9053589701652527}]}, {"text": "The correlation with human assessment will be measured for several different statistical MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9407855868339539}]}, {"text": "We will see that the new measure significantly outperforms the existing approaches.", "labels": [], "entities": []}, {"text": "As a further improvement, we will introduce word dependent substitution costs.", "labels": [], "entities": [{"text": "word dependent substitution", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.5945010284582773}]}, {"text": "This method will be applicable to the new measure as well as to established measures like WER and PER.", "labels": [], "entities": [{"text": "WER", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.8690891265869141}, {"text": "PER", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9776554107666016}]}, {"text": "Starting from the observation that the substitution of a word with a similar one is likely to affect translation quality less than the substitution with a completely different word, we will show how the similarity of words can be accounted for in automatic evaluation measures.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: In Section 2, we will present the state of the art in MT evaluation and discuss the problem of block reordering.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.9689425528049469}, {"text": "block reordering", "start_pos": 131, "end_pos": 147, "type": "TASK", "confidence": 0.7595232725143433}]}, {"text": "Section 3 will introduce the new error measure CDER and will show how it can be calculated efficiently.", "labels": [], "entities": [{"text": "error measure CDER", "start_pos": 33, "end_pos": 51, "type": "METRIC", "confidence": 0.9382826487223307}]}, {"text": "The concept of worddependent substitution costs will be explained in Section 4.", "labels": [], "entities": [{"text": "worddependent substitution", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.6247221082448959}]}, {"text": "In Section 5, experimental results on the correlation of human judgment with the CDER and other well-known evaluation measures will be presented.", "labels": [], "entities": [{"text": "CDER", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.8802420496940613}]}, {"text": "Section 6 will conclude the paper and give an outlook on possible future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The different evaluation measures were assessed experimentally on data from the Chinese-English and the Arabic-English task of the NIST 2004 evaluation workshop.", "labels": [], "entities": [{"text": "NIST 2004 evaluation workshop", "start_pos": 131, "end_pos": 160, "type": "DATASET", "confidence": 0.9089026749134064}]}, {"text": "In this evaluation campaign, 4460 and 1735 candidate translations, respectively, generated by different research MT systems were evaluated by human judges with regard to fluency and adequacy.", "labels": [], "entities": []}, {"text": "Four reference translations are provided for each candidate translation.", "labels": [], "entities": []}, {"text": "Detailed corpus statistics are listed in.", "labels": [], "entities": []}, {"text": "For the experiments in this study, the candidate translations from these tasks were evaluated using different automatic evaluation measures.", "labels": [], "entities": []}, {"text": "Pearson's correlation coefficient r between automatic evaluation and the sum of fluency and adequacy was calculated.", "labels": [], "entities": [{"text": "correlation coefficient r", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.9561147689819336}]}, {"text": "As it could be arguable whether Pearson's r is meaningful for categorical data like human MT evaluation, we have also calculated Kendall's correlation coefficient \u03c4 . Because of the high number of samples (= sentences, 4460) versus the low number of categories (= outcomes of adequacy+fluency, 9), we calculated \u03c4 separately for each source sentence.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.9428257644176483}, {"text": "Kendall's correlation coefficient \u03c4", "start_pos": 129, "end_pos": 164, "type": "METRIC", "confidence": 0.754185950756073}]}, {"text": "These experiments showed that Kendall's \u03c4 reflects the same tendencies as Pearson's r regarding the ranking of the evaluation measures.", "labels": [], "entities": [{"text": "Kendall's \u03c4", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.7118035952250162}, {"text": "Pearson's r", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.6698715090751648}]}, {"text": "But only the latter allows for an efficient calculation of confidence intervals.", "labels": [], "entities": []}, {"text": "Consequently, figures of \u03c4 are omitted in this paper.", "labels": [], "entities": []}, {"text": "Due to the small number of samples for evaluation on system level (10 and 5, respectively), all correlation coefficients between automatic and human evaluation on system level are very close to 1.", "labels": [], "entities": []}, {"text": "Therefore, they do not show any significant differences for the different evaluation for the sake of clarity they are not included here.", "labels": [], "entities": []}, {"text": "All correlation coefficients presented here were calculated for sentence level evaluation.", "labels": [], "entities": [{"text": "sentence level evaluation", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.7528865138689677}]}, {"text": "For comparison with state-of-the-art evaluation measures, we have also calculated the correlation between human evaluation and WER and BLEU, which were both measures of choice in several international MT evaluation campaigns.", "labels": [], "entities": [{"text": "WER", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.9830591678619385}, {"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9901008009910583}, {"text": "MT evaluation", "start_pos": 201, "end_pos": 214, "type": "TASK", "confidence": 0.919450432062149}]}, {"text": "Furthermore, we included TER () as a recent heuristic block movement measure in some of our experiments for comparison with our measure.", "labels": [], "entities": [{"text": "TER", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9987731575965881}]}, {"text": "As the BLEU score is unsuitable for sentence level evaluation in its original definition, BLEU-S smoothing as described by) is performed.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.9708778858184814}, {"text": "sentence level evaluation", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.6454967757066091}, {"text": "BLEU-S", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9928355813026428}]}, {"text": "Additionally, we added sentence boundary symbols for BLEU, and a different reference length calculation scheme for TER, because these changes improved the correlation between human evaluation and the two automatic measures.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9974498152732849}, {"text": "TER", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9909849166870117}]}, {"text": "Details on this have been described in ().", "labels": [], "entities": []}, {"text": "presents the correlation of BLEU, WER, and CDER with human assessment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9995183944702148}, {"text": "WER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9950847029685974}, {"text": "CDER", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9483078718185425}]}, {"text": "It can be seen that CDER shows better correlation than BLEU and WER on both corpora.", "labels": [], "entities": [{"text": "correlation", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9738238453865051}, {"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9991710186004639}, {"text": "WER", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9937273263931274}]}, {"text": "On the Chinese-English task, the smoothed BLEU score has a higher sentence-level correlation than WER.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9684544801712036}, {"text": "WER", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9904070496559143}]}, {"text": "However, this is not the case for the Arabic-  English task.", "labels": [], "entities": []}, {"text": "So none of these two measures is superior to the other one, but they are both outperformed by CDER.", "labels": [], "entities": [{"text": "CDER", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.7733398675918579}]}, {"text": "If the direction of CDER is reversed (i.e, the CD constraints are required for the candidate instead of the reference, such that the measure has precision instead of recall characteristics), the correlation with human evaluation is much lower.", "labels": [], "entities": [{"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9984947443008423}, {"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.9950729012489319}]}], "tableCaptions": [{"text": " Table 1: Example of word-dependent substitution costs.  Levenshtein  prefix  e  \u02dc  e  distance substitution cost similarity substitution cost", "labels": [], "entities": []}, {"text": " Table 2: Corpus statistics.  TIDES corpora,  NIST 2004 evaluation.  Source language  Chinese Arabic  Target language  English English  Sentences  446  347  Running words  13 016 10 892  Ref. translations  4  4  Avg. ref. length  29.2  31.4  Candidate systems  10  5", "labels": [], "entities": [{"text": "NIST 2004 evaluation", "start_pos": 46, "end_pos": 66, "type": "DATASET", "confidence": 0.9284464120864868}]}, {"text": " Table 3: Correlation (r) between human evalua- tion (adequacy + fluency) and automatic evalu- ation with BLEU, WER, and CDER (NIST 2004  evaluation; sentence level).", "labels": [], "entities": [{"text": "Correlation (r)", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9261371493339539}, {"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9985654950141907}, {"text": "WER", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9656670689582825}, {"text": "NIST 2004  evaluation", "start_pos": 127, "end_pos": 148, "type": "DATASET", "confidence": 0.8979487419128418}]}, {"text": " Table 4: Correlation (r) between human evalua- tion (adequacy + fluency) and automatic evalua- tion for CDER with different penalties.", "labels": [], "entities": [{"text": "Correlation (r)", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9066008925437927}]}, {"text": " Table 5: Correlation (r) between human evalua- tion (adequacy + fluency) and automatic evalu- ation for WER and CDER with word-dependent  substitution costs.  Measure Subst. costs Chin.-E. Arab.-E.  WER  const (1)  0.559  0.589  prefix  0.571  0.605  Levenshtein  0.580  0.611  CDER  const (1)  0.625  0.623  prefix  0.637  0.634  Levenshtein  0.638  0.637", "labels": [], "entities": []}, {"text": " Table 6: Correlation (r) between human evalua- tion (adequacy + fluency) and automatic evalua- tion for different automatic evaluation measures.  Automatic measure  Chin.-E. Arab.-E.  BLEU  0.615  0.603  TER  0.548  0.582  WER  0.559  0.589  WER + Lev. subst.  0.580  0.611  CDER  0.625  0.623  CDER +prefix subst.  0.637  0.634  CDER +prefix+PER  0.649  0.635  95%-confidence  \u00b10.018  \u00b10.028", "labels": [], "entities": [{"text": "BLEU  0.615  0.603  TER  0.548  0.582  WER", "start_pos": 185, "end_pos": 227, "type": "METRIC", "confidence": 0.869338870048523}, {"text": "PER  0.649  0.635  95%-confidence  \u00b10.018  \u00b10.028", "start_pos": 344, "end_pos": 393, "type": "METRIC", "confidence": 0.9022986650466919}]}]}