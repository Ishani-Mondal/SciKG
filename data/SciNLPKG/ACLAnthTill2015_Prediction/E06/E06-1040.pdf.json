{"title": [{"text": "Comparing Automatic and Human Evaluation of NLG Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "We consider the evaluation problem in Natural Language Generation (NLG) and present results for evaluating several NLG systems with similar functionality, including a knowledge-based generator and several statistical systems.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 38, "end_pos": 71, "type": "TASK", "confidence": 0.8225692510604858}]}, {"text": "We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, including NIST, BLEU, and ROUGE.", "labels": [], "entities": [{"text": "NIST", "start_pos": 144, "end_pos": 148, "type": "DATASET", "confidence": 0.8729397654533386}, {"text": "BLEU", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9980033040046692}, {"text": "ROUGE", "start_pos": 160, "end_pos": 165, "type": "METRIC", "confidence": 0.9669299125671387}]}, {"text": "We find that NIST scores correlate best (> 0.8) with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone.", "labels": [], "entities": []}, {"text": "We conclude that automatic evaluation of NLG systems has considerable potential, in particular where high-quality reference texts and only a small number of human evalua-tors are available.", "labels": [], "entities": []}, {"text": "However, in general it is probably best for automatic evaluations to be supported by human-based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluation is becoming an increasingly important topic in Natural Language Generation (NLG), as in other fields of computational linguistics.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.8172594308853149}]}, {"text": "Some NLG researchers are impressed by the success of the BLEU evaluation metric () in Machine Translation (MT), which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9964955449104309}, {"text": "Machine Translation (MT)", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.858488929271698}, {"text": "MT", "start_pos": 138, "end_pos": 140, "type": "TASK", "confidence": 0.9777883887290955}]}, {"text": "BLEU and related metrics work by comparing the output of an MT system to a set of reference ('gold standard') translations, and in principle this kind of evaluation could be done with NLG systems as well.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9702778458595276}, {"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9396110773086548}]}, {"text": "Indeed NLG researchers are already starting to use BLEU) in their evaluations, as this is much cheaper and easier to organise than the human evaluations that have traditionally been used to evaluate NLG systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9984766840934753}]}, {"text": "However, the use of such corpus-based evaluation metrics is only sensible if they are known to be correlated with the results of human-based evaluations.", "labels": [], "entities": []}, {"text": "While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments (), we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgments; correlation studies have been made of individual components (), but not of systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9726022481918335}, {"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9546911120414734}]}, {"text": "In this paper we present an empirical study of how well various corpus-based metrics agree with human judgments, when evaluating several NLG systems that generate sentences which describe changes in the wind (for weather forecasts).", "labels": [], "entities": []}, {"text": "These systems do not perform content determination (they are limited to microplanning and realisation), so our study does not address corpus-based evaluation of content determination.", "labels": [], "entities": [{"text": "content determination", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7853329181671143}, {"text": "content determination", "start_pos": 161, "end_pos": 182, "type": "TASK", "confidence": 0.6619055718183517}]}], "datasetContent": [{"text": "NLG systems have traditionally been evaluated using human subjects.", "labels": [], "entities": []}, {"text": "NLG evaluations have tended to be of the intrinsic type, involving subjects reading and rating texts; usually subjects are shown both NLG and human-written texts, and the NLG system is evaluated by comparing the ratings of its texts and human texts.", "labels": [], "entities": [{"text": "NLG evaluations", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7532027661800385}]}, {"text": "In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison.", "labels": [], "entities": []}, {"text": "This methodology was first used in NLG in the mid-1990s by and, and continues to be popular today.", "labels": [], "entities": []}, {"text": "Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance, measuring how much experts postedit generated texts ( ), and measuring how quickly people read generated texts).", "labels": [], "entities": []}, {"text": "In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts.", "labels": [], "entities": []}, {"text": "As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable.", "labels": [], "entities": []}, {"text": "Corpus-based evaluation was first used in NLG by, who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts.", "labels": [], "entities": []}, {"text": "Similar evaluations have been used e.g. by.", "labels": [], "entities": []}, {"text": "Such corpus-based evaluations have sometimes been criticised in the NLG community, for example by.", "labels": [], "entities": []}, {"text": "Grounds for criticism include the fact that regenerating a parsed text is not a realistic NLG task; that texts can be very different from a corpus text but still effectively meet the system's communicative goal; and that corpus texts are often not of high enough quality to form a realistic test.", "labels": [], "entities": []}, {"text": "The MT and document summarisation communities have developed evaluation metrics based on comparing output texts to a corpus of human texts, and have shown that some of these metrics are highly correlated with human judgments.", "labels": [], "entities": [{"text": "MT and document summarisation", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.6595645844936371}]}, {"text": "The BLEU metric () in MT has been particularly successful; for example MT-05, the 2005 NIST MT evaluation exercise, used as the only method of evaluation.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.978332132101059}, {"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9903312921524048}, {"text": "MT-05", "start_pos": 71, "end_pos": 76, "type": "TASK", "confidence": 0.5921849012374878}, {"text": "NIST MT evaluation exercise", "start_pos": 87, "end_pos": 114, "type": "TASK", "confidence": 0.4920765906572342}]}, {"text": "BLEU is a precision metric that assesses the quality of a translation in terms of the proportion of its word ngrams (n = 4 has become standard) that it shares with one or more high-quality reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9796584844589233}, {"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9961716532707214}]}, {"text": "BLEU scores range from 0 to 1, 1 being the highest which can only be achieved by a translation if all its substrings can be found in one of the reference texts (hence a reference text will always score 1).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9660816788673401}]}, {"text": "BLEU should be calculated on a large test set with several reference translations (four appears to be standard in MT).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9882510304450989}, {"text": "MT", "start_pos": 114, "end_pos": 116, "type": "TASK", "confidence": 0.919475257396698}]}, {"text": "Properly calculated BLEU scores have been shown to correlate reliably with human judgments ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9981594681739807}]}, {"text": "The NIST MT evaluation metric) is an adaptation of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more importance to less frequent (hence more informative) n-grams.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.4448253810405731}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9957000017166138}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9967837333679199}, {"text": "NIST", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.8685964941978455}]}, {"text": "BLEU's ability to detect subtle but important differences in translation quality has been questioned, some research showing NIST to be more sensitive).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9797612428665161}]}, {"text": "The ROUGE metric (Lin and Hovy, 2003) was conceived as document summarisation's answer to BLEU, but it does not appear to have met with the same degree of enthusiasm.", "labels": [], "entities": [{"text": "ROUGE metric", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9598857760429382}, {"text": "document summarisation", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.497882142663002}, {"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.994895875453949}]}, {"text": "There are several different ROUGE metrics.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.716954231262207}]}, {"text": "The simplest is ROUGE-N, which computes the highest proportion in any reference summary of n-grams that are matched by the system-generated summary.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9838870167732239}]}, {"text": "A procedure is applied that averages the score across leave-oneout subsets of the set of reference texts.", "labels": [], "entities": []}, {"text": "ROUGE-N is an almost straightforward n-gram recall metric between two texts, and has several counterintuitive properties, including that even a text composed entirely of sentences from reference texts cannot score 1 (unless there is only one reference text).", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.970680832862854}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9023422002792358}]}, {"text": "There are several other variants of the ROUGE metric, and ROUGE-2, along with ROUGE-SU (based on skip bigrams and unigrams), were among the official scores for the DUC 2005 summarisation task.", "labels": [], "entities": [{"text": "ROUGE metric", "start_pos": 40, "end_pos": 52, "type": "METRIC", "confidence": 0.9267794191837311}, {"text": "ROUGE-2", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9240683317184448}, {"text": "ROUGE-SU", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9673014879226685}, {"text": "DUC 2005 summarisation task", "start_pos": 164, "end_pos": 191, "type": "DATASET", "confidence": 0.7133059948682785}]}, {"text": "The main goal of our experiments was to determine how well a variety of automatic evaluation metrics correlated with human judgments of text quality in NLG.", "labels": [], "entities": []}, {"text": "A secondary goal was to determine if there were types of NLG systems for which the correlation of automatic and human evaluation was particularly good or bad.", "labels": [], "entities": []}, {"text": "Data: We extracted from each forecast in the SUMTIME corpus the first description of wind (at 10m height) from every morning forecast (the text shown in is atypical example), which resulted in a set of about 500 wind forecasts.", "labels": [], "entities": [{"text": "SUMTIME corpus", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.8184983730316162}]}, {"text": "We excluded several forecasts for which we had no input data (numerical weather predictions) or an incomplete set of system outputs; this left 465 texts, which we used in our evaluation.", "labels": [], "entities": []}, {"text": "The inputs to the generators were tuples composed of an index, timestamp, wind direction, wind speed range, and gust speed range (see examples at top of.", "labels": [], "entities": [{"text": "gust speed range", "start_pos": 112, "end_pos": 128, "type": "METRIC", "confidence": 0.7805531819661459}]}, {"text": "We randomly selected a subset of 21 forecast dates for use inhuman evaluations.", "labels": [], "entities": []}, {"text": "For these 21 forecast dates, we also asked two meteorologists who had not contributed to the original SUMTIME corpus to write new forecasts texts; we used these as reference texts for the automatic metrics.", "labels": [], "entities": [{"text": "SUMTIME corpus", "start_pos": 102, "end_pos": 116, "type": "DATASET", "confidence": 0.7419022917747498}]}, {"text": "The forecasters created these texts by rewriting the corpus texts, as this was a more natural task for them than writing texts based on tuples.", "labels": [], "entities": []}, {"text": "500 wind descriptions may seem like a small corpus, but in fact provides very good coverage as Input,,  the domain language is extremely simple, involving only about 90 word forms (not counting numbers and wind directions) and a small handful of different syntactic structures.", "labels": [], "entities": [{"text": "Input", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.6938402652740479}]}, {"text": "Systems and texts evaluated: We evaluated four pCRU generators and the SUMTIME system, operating in Hybrid mode (Section 2.3) for better comparability because the pCRU generators do not perform content determination.", "labels": [], "entities": [{"text": "content determination", "start_pos": 194, "end_pos": 215, "type": "TASK", "confidence": 0.7369034290313721}]}, {"text": "A base pCRU generator was created semiautomatically by running a chunker over the corpus, extracting generation rules and adding some higher-level rules taking care of aggregation, elision etc.", "labels": [], "entities": []}, {"text": "This base generator was then trained on 9/10 of the corpus (the training data).", "labels": [], "entities": []}, {"text": "5 different random divisions of the corpus into training and testing data were used (i.e. all results were validated by 5-fold hold-out cross-validation).", "labels": [], "entities": []}, {"text": "Additionally, a back-off 2-gram model with GoodTuring discounting and no lexical classes was built from the same training data, using the SRILM toolkit).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 138, "end_pos": 151, "type": "DATASET", "confidence": 0.8995188474655151}]}, {"text": "Forecasts were then generated for all corpus inputs, in all four generation modes (Section 2.4).", "labels": [], "entities": []}, {"text": "shows an example of an input to the systems, along with the three human texts (Corpus, Human1, Human2) and the texts produced by all five NLG systems from this data. with substitution at cost 2, and deletion and insertion at cost 1, and normalised to range 0 to 1 (perfect match).", "labels": [], "entities": []}, {"text": "When multiple reference texts are used, the SE score fora generator forecast is the average of its scores against the reference texts; the SE score fora set of generator forecasts is the average of scores for individual forecasts.", "labels": [], "entities": [{"text": "SE score", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9746904373168945}, {"text": "SE score", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9742874503135681}]}, {"text": "Human evaluations: We recruited 9 experts (people with experience reading forecasts for offshore oil rigs) and 21 non-experts (people with no such experience).", "labels": [], "entities": []}, {"text": "Subjects did not have a background in NLP, and were native speakers of English.", "labels": [], "entities": []}, {"text": "They were shown forecast texts from all the generators and from the corpus, and asked to score them on a scale of 0 to 5, for readability, clarity and general appropriateness.", "labels": [], "entities": [{"text": "clarity", "start_pos": 139, "end_pos": 146, "type": "METRIC", "confidence": 0.9964233040809631}]}, {"text": "Experts were additionally shown the numerical weather data that the forecast text was based on.", "labels": [], "entities": []}, {"text": "At the start, subjects were shown two practice examples.", "labels": [], "entities": []}, {"text": "The experiments were carried out over the web.", "labels": [], "entities": []}, {"text": "Subjects completed the experiment unsupervised, at a time and place of their choosing.", "labels": [], "entities": []}, {"text": "Expert subjects were shown a randomly selected forecast for 18 of the dates.", "labels": [], "entities": []}, {"text": "The non-experts were shown 21 forecast texts, in a repeated Latin squares (non-repeating column and row entries) experimental design where each combination of date and system is assigned one evaluation.: Pearson correlation coefficients between all scores for systems in.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 204, "end_pos": 223, "type": "METRIC", "confidence": 0.8644033372402191}]}], "tableCaptions": [{"text": " Table 2: Evaluation scores against 2 reference texts, for set of 18 forecasts used in expert evaluation.", "labels": [], "entities": []}, {"text": " Table 3: Pearson correlation coefficients between all scores for systems in", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8620545864105225}]}, {"text": " Table 4: Evaluation scores against the SUMTIME corpus, on 5 test sets from pCRU validation.", "labels": [], "entities": [{"text": "SUMTIME corpus", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.7648293972015381}]}]}