{"title": [], "abstractContent": [{"text": "In this paper, we investigate the problem of automatically predicting segment boundaries in spoken multiparty dialogue.", "labels": [], "entities": [{"text": "predicting segment boundaries in spoken multiparty dialogue", "start_pos": 59, "end_pos": 118, "type": "TASK", "confidence": 0.7028904982975551}]}, {"text": "We extend prior work in two ways.", "labels": [], "entities": []}, {"text": "We first apply approaches that have been proposed for predicting top-level topic shifts to the problem of identifying subtopic boundaries.", "labels": [], "entities": [{"text": "predicting top-level topic shifts", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.7787684798240662}]}, {"text": "We then explore the impact on performance of using ASR output as opposed to human transcription.", "labels": [], "entities": [{"text": "ASR", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9864819049835205}]}, {"text": "Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks: (1) for predicting subtopic boundaries, the lexical cohesion-based approach alone can achieve competitive results, (2) for predicting top-level boundaries, the machine learning approach that combines lexical-cohesion and conversational features performs best, and (3) conversational cues, such as cue phrases and overlapping speech, are better indicators for the top-level prediction task.", "labels": [], "entities": []}, {"text": "We also find that the transcription errors inevitable in ASR output have a negative impact on models that combine lexical-cohesion and conversational features, but do not change the general preference of approach for the two tasks.", "labels": [], "entities": [{"text": "ASR output", "start_pos": 57, "end_pos": 67, "type": "TASK", "confidence": 0.8914693593978882}]}], "introductionContent": [{"text": "Text segmentation, i.e., determining the points at which the topic changes in a stream of text, plays an important role in applications such as topic detection and tracking, summarization, automatic genre detection and information retrieval and extraction ().", "labels": [], "entities": [{"text": "Text segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6803575456142426}, {"text": "topic detection and tracking", "start_pos": 144, "end_pos": 172, "type": "TASK", "confidence": 0.8596441596746445}, {"text": "summarization", "start_pos": 174, "end_pos": 187, "type": "TASK", "confidence": 0.9903839826583862}, {"text": "automatic genre detection", "start_pos": 189, "end_pos": 214, "type": "TASK", "confidence": 0.6006719172000885}, {"text": "information retrieval and extraction", "start_pos": 219, "end_pos": 255, "type": "TASK", "confidence": 0.7571725696325302}]}, {"text": "In recent work, researchers have applied these techniques to corpora such as newswire feeds, transcripts of radio broadcasts, and spoken dialogues, in order to facilitate browsing, information retrieval, and topic detection (.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 181, "end_pos": 202, "type": "TASK", "confidence": 0.7790971100330353}, {"text": "topic detection", "start_pos": 208, "end_pos": 223, "type": "TASK", "confidence": 0.8787931501865387}]}, {"text": "In this paper, we focus on segmentation of multiparty dialogues, in particular recordings of small group meetings.", "labels": [], "entities": []}, {"text": "We compare models based solely on lexical information, which are common in approaches to automatic segmentation of text, with models that combine lexical and conversational features.", "labels": [], "entities": [{"text": "automatic segmentation of text", "start_pos": 89, "end_pos": 119, "type": "TASK", "confidence": 0.8281885981559753}]}, {"text": "Because tasks as diverse as browsing, on the one hand, and summarization, on the other, require different levels of granularity of segmentation, we explore the performance of our models for two tasks: hypothesizing where major topic changes occur and hypothesizing where more subtle nested topic shifts occur.", "labels": [], "entities": [{"text": "summarization", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.977920651435852}]}, {"text": "In addition, because we do not wish to make the assumption that high quality transcripts of meeting records, such as those produced by human transcribers, will be commonly available, we require algorithms that operate directly on automatic speech recognition (ASR) output.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR) output", "start_pos": 230, "end_pos": 271, "type": "TASK", "confidence": 0.7854826024600438}]}], "datasetContent": [{"text": "To compare to prior work, we perform a 25-fold leave-one-out cross validation on the set of 25 ICSI meetings that were used in.", "labels": [], "entities": [{"text": "ICSI meetings", "start_pos": 95, "end_pos": 108, "type": "DATASET", "confidence": 0.6680662631988525}]}, {"text": "We repeated the procedure to evaluate the accuracy using the lexical cohesion and combined models on both human and ASR transcriptions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9996795654296875}, {"text": "ASR transcriptions", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.8512317538261414}]}, {"text": "In each evaluation, we trained the automatic segmentation models for two tasks: predicting subtopic boundaries (SUB) and predicting only top-level boundaries (TOP).", "labels": [], "entities": []}, {"text": "In order to be able to compare our results directly with previous work, we first report our results using the standard error rate metrics of Pk and Wd.) is the probability that two utterances drawn randomly from a document (in our case, a meeting transcript) are incorrectly identified as belonging to the same topic segment.", "labels": [], "entities": []}, {"text": "WindowDiff (Wd)) calculates the error rate by moving a sliding window across the meeting transcript counting the number of times the hypothesized and reference segment boundaries are different.", "labels": [], "entities": [{"text": "error rate", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9698372185230255}]}, {"text": "The meetings in the ICSI corpus last approximately 1 hour and have an average of 8-10 toplevel topic segments.", "labels": [], "entities": [{"text": "ICSI corpus", "start_pos": 20, "end_pos": 31, "type": "DATASET", "confidence": 0.835546225309372}]}, {"text": "In order to facilitate meeting browsing and question-answering, we believe it is useful to include subtopic boundaries in order to narrow in more accurately on the portion of the meeting that contains the information the user needs.", "labels": [], "entities": []}, {"text": "Therefore, we performed experiments aimed at analysing how the LM and CM segmentation models behave in predicting segment boundaries at the two different levels of granularity.", "labels": [], "entities": []}, {"text": "All of the results are reported on the test set.", "labels": [], "entities": []}, {"text": "shows the performance of the lexical cohesion model (LM) and the combined model (CM) integrating the lexical cohesion and conversational features discussed in Section 3.3.2. 3 For the task of predicting top-level topic boundaries from human transcripts, CM outperforms LM.", "labels": [], "entities": [{"text": "predicting top-level topic boundaries from human transcripts", "start_pos": 192, "end_pos": 252, "type": "TASK", "confidence": 0.8511610286576408}]}, {"text": "LM tends to over-predict on the top-level, resulting in a higher false alarm rate.", "labels": [], "entities": [{"text": "LM", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8592719435691833}, {"text": "false alarm rate", "start_pos": 65, "end_pos": 81, "type": "METRIC", "confidence": 0.8106770714124044}]}, {"text": "However, for the task of predicting subtopic shifts, LM alone is considerably better than CM.: Performance comparison of probabilistic segmentation models.", "labels": [], "entities": [{"text": "predicting subtopic shifts", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.8816351691881815}]}, {"text": "In prior work, empirically identified cue phrases that are indicators of segment boundaries, and then eliminated all cues that had not previously been identified as cue phrases in the literature.", "labels": [], "entities": []}, {"text": "Here, we conduct an experiment to explore how different ways of identifying cue phrases can help identify useful new features for the two boundary prediction tasks.", "labels": [], "entities": [{"text": "boundary prediction tasks", "start_pos": 138, "end_pos": 163, "type": "TASK", "confidence": 0.8057411710421244}]}, {"text": "In each fold of the 25-fold leave-one-out cross validation, we use a modified 5 Chi-square test to In order to satisfy the mathematical assumptions undercalculate statistics for each word (unigram) and word pair (bi-gram) that occurred in the 24 training meetings.", "labels": [], "entities": []}, {"text": "We then rank unigrams and bigrams according to their Chi-square scores, filtering out those with values under 6.64, the threshold for the Chi-square statistic at the 0.01 significance level.", "labels": [], "entities": []}, {"text": "The unigrams and bigrams in this ranked list are the learned cue phrases.", "labels": [], "entities": []}, {"text": "We then use the occurrence counts of cue phrases in an analysis window around each potential topic boundary in the test meeting as a feature.", "labels": [], "entities": []}, {"text": "shows the performance of models that use statistically learned cue phrases in their feature sets compared with models using no cue phrase features and Galley's model, which only uses cue phrases that correspond to those identified in the literature (Col-cue).", "labels": [], "entities": []}, {"text": "We see that for predicting subtopics, models using the cue word features (1gram) and the combination of cue words and bigrams (1+2gram) yield a 15% and 8.24% improvement over models using no cue features (NOCUE) (p < 0.01) respectively, while models using only cue phrases found in the literature (Col-cue) improve performance by just 3.18%.", "labels": [], "entities": []}, {"text": "In contrast, for predicting top-level topics, the model using cue phrases from the literature (Col-cue) achieves a 4.2% improvement, and this is the only model that produces statistically significantly better results than the model using no cue phrases (NOCUE).", "labels": [], "entities": [{"text": "predicting top-level topics", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.8360471725463867}]}, {"text": "The superior performance of models using statistically learned cue phrases as features for predicting subtopic boundaries suggests there may exist a different set of cue phrases that serve as segmentation cues for subtopic boundaries.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance comparison of probabilistic  segmentation models.", "labels": [], "entities": []}, {"text": " Table 2: Effect of different feature combinations  for predicting topic boundaries from human tran- scripts. MC-B is the randomly generated baseline.", "labels": [], "entities": [{"text": "predicting topic boundaries", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.8808778524398804}]}, {"text": " Table 3: Effect of different feature combinations  for predicting topic boundaries from ASR output.", "labels": [], "entities": [{"text": "predicting topic boundaries", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.8935203154881796}, {"text": "ASR", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.8737264275550842}]}, {"text": " Table 4: Performance of models trained with cue phrases from the literature (Col-cue) and cue phrases  learned from statistical tests, including cue words (1gram), cue word pairs (2gram), and cue phrases  composed of both words and word pairs (1+2gram). NOCUE is the model using no cue phrase features.  The Topline is the agreement of human annotators on top-level segments.", "labels": [], "entities": []}]}