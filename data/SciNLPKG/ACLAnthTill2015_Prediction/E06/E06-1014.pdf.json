{"title": [{"text": "Improving Probabilistic Latent Semantic Analysis with Principal Component Analysis", "labels": [], "entities": [{"text": "Improving Probabilistic Latent Semantic Analysis", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8372116804122924}]}], "abstractContent": [{"text": "Probabilistic Latent Semantic Analysis (PLSA) models have been shown to provide a better model for capturing poly-semy and synonymy than Latent Semantic Analysis (LSA).", "labels": [], "entities": []}, {"text": "However, the parameters of a PLSA model are trained using the Expectation Maximization (EM) algorithm , and as a result, the trained model is dependent on the initialization values so that performance can be highly variable.", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.7459335446357727}]}, {"text": "In this paper we present a method for using LSA analysis to initialize a PLSA model.", "labels": [], "entities": [{"text": "LSA analysis", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.7985329627990723}]}, {"text": "We also investigated the performance of our method for the tasks of text segmenta-tion and retrieval on personal-size corpora, and present results demonstrating the efficacy of our proposed approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "In modeling a collection of documents for information access applications, the documents are often represented as a \"bag of words\", i.e., as term vectors composed of the terms and corresponding counts for each document.", "labels": [], "entities": []}, {"text": "The term vectors fora document collection can be organized into a term by document co-occurrence matrix.", "labels": [], "entities": []}, {"text": "When directly using these representations, synonyms and polysemous terms, that is, terms with multiple senses or meanings, are not handled well.", "labels": [], "entities": []}, {"text": "Methods for smoothing the term distributions through the use of latent classes have been shown to improve the performance of a number of information access tasks, including retrieval over smaller collections), text segmentation (), and text classification ().", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 210, "end_pos": 227, "type": "TASK", "confidence": 0.7958818674087524}, {"text": "text classification", "start_pos": 236, "end_pos": 255, "type": "TASK", "confidence": 0.8132296204566956}]}, {"text": "The Probabilistic Latent Semantic Analysis model (PLSA)) provides a probabilistic framework that attempts to capture polysemy and synonymy in text for applications such as retrieval and segmentation.", "labels": [], "entities": [{"text": "Probabilistic Latent Semantic Analysis", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.568856954574585}]}, {"text": "It uses a mixture decomposition to model the co-occurrence data, and the probabilities of words and documents are obtained by a convex combination of the aspects.", "labels": [], "entities": []}, {"text": "The mixture approximation has a well defined probability distribution and the factors have a clear probabilistic meaning in terms of the mixture component distributions.", "labels": [], "entities": []}, {"text": "The PLSA model computes the relevant probability distributions by selecting the model parameter values that maximize the probability of the observed data, i.e., the likelihood function.", "labels": [], "entities": []}, {"text": "The standard method for maximum likelihood estimation is the Expectation Maximization (EM) algorithm.", "labels": [], "entities": [{"text": "maximum likelihood estimation", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.6249081393082937}, {"text": "Expectation Maximization (EM)", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.8184368014335632}]}, {"text": "For a given initialization, the likelihood function increases with EM iterations until a local maximum is reached, rather than a global maximum, so that the quality of the solution depends on the initialization of the model.", "labels": [], "entities": []}, {"text": "Additionally, the likelihood values across different initializations are not comparable, as we will show.", "labels": [], "entities": []}, {"text": "Thus, the likelihood function computed over the training data cannot be used as a predictor of model performance across different models.", "labels": [], "entities": []}, {"text": "Rather than trying to predict the best performing model from a set of models, in this paper we focus on finding a good way to initialize the PLSA model.", "labels": [], "entities": []}, {"text": "We will present a framework for using Latent Semantic Analysis (LSA)) to better initialize the parameters of a corresponding PLSA model.", "labels": [], "entities": []}, {"text": "The EM algorithm is then used to further refine the initial estimate.", "labels": [], "entities": [{"text": "EM", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.675497829914093}]}, {"text": "This combination of LSA and PLSA leverages the advantages of both.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: in section 2, we review related work in the area.", "labels": [], "entities": []}, {"text": "In section 3, we summarize related work on LSA and its probabilistic interpretation.", "labels": [], "entities": [{"text": "LSA", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.7751350998878479}]}, {"text": "In section 4 we review the PLSA model and in section 5 we present our method for initializing a PLSA model using LSA model parameters.", "labels": [], "entities": []}, {"text": "In section 6, we evaluate the performance of our framework on a text segmentation task and several smaller information retrieval tasks.", "labels": [], "entities": [{"text": "text segmentation task", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.8051937818527222}, {"text": "information retrieval", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.7060001939535141}]}, {"text": "And in section 7, we summarize our results and give directions for future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Correlation between the negative log- likelihood and Average or BreakEven Precision", "labels": [], "entities": [{"text": "negative log- likelihood", "start_pos": 34, "end_pos": 58, "type": "METRIC", "confidence": 0.7614793851971626}, {"text": "Average", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9778741002082825}, {"text": "BreakEven Precision", "start_pos": 74, "end_pos": 93, "type": "METRIC", "confidence": 0.8669890463352203}]}, {"text": " Table 2: Retrieval Evaluation with Single Models.  Best performing model for each dataset/metric is  in bold.", "labels": [], "entities": [{"text": "Retrieval Evaluation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9357157051563263}]}, {"text": " Table 3: Retrieval Evaluation with Multiple Mod- els. Best performing model for each dataset and  metric are in bold. L-PLSA corresponds to LSA- PLSA", "labels": [], "entities": []}, {"text": " Table 4: Single Model Segmentation Word and  Sentence Error Rates (%). PLSA error rate at the  optimal number of classes in terms of", "labels": [], "entities": [{"text": "Single Model Segmentation Word and  Sentence Error Rates", "start_pos": 10, "end_pos": 66, "type": "TASK", "confidence": 0.7589976638555527}, {"text": "PLSA error rate", "start_pos": 72, "end_pos": 87, "type": "METRIC", "confidence": 0.6744734048843384}]}, {"text": " Table 5: Multiple Model Segmentation Word and  Sentence Error Rates (%). Performance at the op- timal number of classes in terms of", "labels": [], "entities": [{"text": "Multiple Model Segmentation Word and  Sentence Error Rates", "start_pos": 10, "end_pos": 68, "type": "TASK", "confidence": 0.7458815537393093}]}]}