{"title": [{"text": "Automatic Detection of Nonreferential It in Spoken Multi-Party Dialog", "labels": [], "entities": [{"text": "Automatic Detection of Nonreferential It in Spoken Multi-Party Dialog", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.7909697790940603}]}], "abstractContent": [{"text": "We present an implemented machine learning system for the automatic detection of nonreferential it in spoken dialog.", "labels": [], "entities": [{"text": "automatic detection of nonreferential it in spoken dialog", "start_pos": 58, "end_pos": 115, "type": "TASK", "confidence": 0.7715973816812038}]}, {"text": "The system builds on shallow features extracted from dialog transcripts.", "labels": [], "entities": []}, {"text": "Our experiments indicate a level of performance that makes the system usable as a preprocess-ing filter fora coreference resolution system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.9066695272922516}]}, {"text": "We also report results of an annotation study dealing with the classification of it by naive subjects.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes an implemented system for the detection of nonreferential it in spoken multiparty dialog.", "labels": [], "entities": [{"text": "detection of nonreferential it in spoken multiparty dialog", "start_pos": 51, "end_pos": 109, "type": "TASK", "confidence": 0.7331394031643867}]}, {"text": "The system has been developed on the basis of meeting transcriptions from the ICSI Meeting Corpus (, and it is intended as a preprocessing component fora coreference resolution system in the DIANA-Summ dialog summarization project.", "labels": [], "entities": [{"text": "ICSI Meeting Corpus", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.9728214939435323}, {"text": "coreference resolution", "start_pos": 154, "end_pos": 176, "type": "TASK", "confidence": 0.9187619388103485}, {"text": "DIANA-Summ dialog summarization project", "start_pos": 191, "end_pos": 230, "type": "DATASET", "confidence": 0.8375490307807922}]}, {"text": "Consider the following utterance: MN059: Yeah.", "labels": [], "entities": [{"text": "MN059", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.825736403465271}]}, {"text": "I'm sure I could learn a lot about um, yeah, just how to -how to come up with these structures, cuz it's -it's very easy to whip up something quickly, but it maybe then makes sense toto me, but not to anybody else, and -and if we want to share and integrate things, they must -well, they must be well designed really.", "labels": [], "entities": []}, {"text": "In this example, only one of the three instances of it is a referential pronoun: The first it appears in the reparandum part of a speech repair).", "labels": [], "entities": []}, {"text": "It is replaced by a subsequent alteration and is thus not part of the final utterance.", "labels": [], "entities": []}, {"text": "The second it is the subject of an extraposition construction and serves as the placeholder for the postposed infinitive phrase to whip up something quickly.", "labels": [], "entities": []}, {"text": "Only the third it is a referential pronoun which anaphorically refers to something.", "labels": [], "entities": []}, {"text": "The task of the system described in the following is to identify and filter out nonreferential instances of it, like the first and second one in the example.", "labels": [], "entities": []}, {"text": "By preventing these instances from triggering the search for an antecedent, the precision of a coreference resolution system is improved.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9994575381278992}, {"text": "coreference resolution", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.9443532228469849}]}, {"text": "Up to the present, coreference resolution has mostly been done on written text.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.9853620827198029}]}, {"text": "In this domain, the detection of nonreferential it has by now become a standard preprocessing step (e.g.).", "labels": [], "entities": []}, {"text": "In the few works that exist on coreference resolution in spoken language, on the other hand, the problem could be ignored, because almost none of these aimed at developing a system that could handle unrestricted input.", "labels": [], "entities": [{"text": "coreference resolution in spoken language", "start_pos": 31, "end_pos": 72, "type": "TASK", "confidence": 0.9076958775520325}]}, {"text": "focus on an unimplemented algorithm for determining the type of antecedent (mostly NP vs. non-NP), given an anaphorical pronoun or demonstrative.", "labels": [], "entities": []}, {"text": "The system of Byron (2002) is implemented, but deals mainly with how referents for already identified discourse-deictic anaphors can be created.", "labels": [], "entities": []}, {"text": "Finally, describe an implemented system for resolving 3rd person pronouns in spoken dialog, but they also exclude nonreferential it from consideration.", "labels": [], "entities": []}, {"text": "In contrast, the present work is part of a project to develop a coreference resolution system that, in its final implementation, can handle unrestricted multi-party dialog.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.8964883685112}]}, {"text": "In such a system, no a priori knowledge is available about whether an instance of it is referential or not.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows: Section 2 describes the current state of the art for the detection of nonreferential it in written text.", "labels": [], "entities": [{"text": "detection of nonreferential it in written text", "start_pos": 111, "end_pos": 157, "type": "TASK", "confidence": 0.8147900530270168}]}, {"text": "Section 3 describes our corpus of transcribed spoken dialog.", "labels": [], "entities": []}, {"text": "It also reports on the annotation that we performed in order to collect training and test data for our machine learning experiments.", "labels": [], "entities": []}, {"text": "The annotation also offered interesting insights into how reliably humans can identify nonreferential it in spoken language, a question that, to our knowledge, has not been adressed before.", "labels": [], "entities": [{"text": "identify nonreferential it in spoken language", "start_pos": 78, "end_pos": 123, "type": "TASK", "confidence": 0.7666928023099899}]}, {"text": "Section 4 describes the setup and results of our machine learning experiments, Section 5 contains conclusion and future work.", "labels": [], "entities": [{"text": "conclusion", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.8857918381690979}]}], "datasetContent": [{"text": "We then applied machine learning in order to build an automatic classifier for detecting nonreferential instances of it, given a vector of features as described above.", "labels": [], "entities": []}, {"text": "We used JRip, the WEKA 4 reimplementation of.", "labels": [], "entities": [{"text": "JRip", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.7809333801269531}, {"text": "WEKA", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.8406102061271667}]}, {"text": "All following figures were obtained by means of ten-fold cross-validation.", "labels": [], "entities": []}, {"text": "contains all results discussed in what follows.", "labels": [], "entities": []}, {"text": "Ina first experiment, we did not use either of the two options described above, so that no information about interruption points or sentence boundaries was available during training or testing.", "labels": [], "entities": []}, {"text": "With this setting, the classifier achieved a recall of 55.1%, a precision of 71.9% and a resulting F-measure of 62.4% for the detection of the class nonreferential.", "labels": [], "entities": [{"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9996390342712402}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.992158055305481}, {"text": "F-measure", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9990646243095398}]}, {"text": "The overall classification accuracy was 75.1%.", "labels": [], "entities": [{"text": "classification", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.8642705082893372}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9850870370864868}]}, {"text": "The advantage of using a machine learning sys-4 http://www.cs.waikato.ac.nz/ ml/ tem that produces human-readable models is that it allows direct introspection of which of the features were used, and to which effect.", "labels": [], "entities": []}, {"text": "It turned out that the discarded feature is very successful.", "labels": [], "entities": []}, {"text": "The model produced a rule that used this feature and correctly identified 83 instances of nonreferential it, while it produced no false positives.", "labels": [], "entities": []}, {"text": "Similarly, the seem list feature alone was able to correctly identify 22 instances, producing nine false positives.", "labels": [], "entities": []}, {"text": "The following is an example of a more complex rule involving distance features, which is also very successful The fact that these rules with these conditions were learned show that the features found to be most important for the detection of nonreferential it in written text (cf. Section 2) are also highly relevant for performing that task for spoken language.", "labels": [], "entities": [{"text": "detection of nonreferential it in written text", "start_pos": 229, "end_pos": 275, "type": "TASK", "confidence": 0.8129756110055106}]}, {"text": "We then ran a second experiment in which we used sentence boundary information to restrict the scope of both the pattern matching features and the distance-related features.", "labels": [], "entities": []}, {"text": "We expected this to improve the performance of the model, as patterns should apply less generously (and thus more accurately), which could be expected to result in an increase in precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.9983964562416077}]}, {"text": "However, the second experiment yielded a recall of 57.7%, a precision of only 70.1% and an F-measure of 63.3% for the detection of this class.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.999756395816803}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9979580640792847}, {"text": "F-measure", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9995526671409607}]}, {"text": "The overall accuracy was 74.9%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9998400211334229}]}, {"text": "The system produced a mere five rules (compared to seven before).", "labels": [], "entities": []}, {"text": "The model produced the identical rule using the discarded-feature.", "labels": [], "entities": []}, {"text": "The same applies to the seem list feature, with the difference that both precision and recall of this rule were altered: The rule now produced 23 true positives and six false positives.", "labels": [], "entities": [{"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9997206330299377}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9995636343955994}]}, {"text": "The slightly higher recall of the model using the sentence boundary information is mainly due to a better coverage of the rule using the features encoding the distance to the next toinfinitive and the next adjective: it now produced  We then wanted to compare the contribution of the sentence breaks to that of the interruption points.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9994814991950989}]}, {"text": "We ran another experiment, using only the latter and leaving everything else unaltered.", "labels": [], "entities": []}, {"text": "This time, the overall performance of the classifier improved considerably: recall was 60.9%, precision 80.0%, F-measure 69.2%, and the overall accuracy was 79.6%.", "labels": [], "entities": [{"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.999848484992981}, {"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9995303153991699}, {"text": "F-measure", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9879869818687439}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9991668462753296}]}, {"text": "The resulting model was rather complicated, including seven complex rules.", "labels": [], "entities": []}, {"text": "The increase in recall is mainly due to the following rule, which is not easily interpreted: 5 it_s = match and dist_to_next_nominal >=21 and dist_to_next_adj >=500 and subj_verb = null ==> nonref (116.0/31.0) The considerable improvement (in particular in precision) brought about by the interruption points, and the comparatively small impact of sentence boundary information, might be explainable in several ways.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9991214871406555}, {"text": "precision", "start_pos": 257, "end_pos": 266, "type": "METRIC", "confidence": 0.9987379908561707}]}, {"text": "For instance, although sentence boundary information allows to limit both the search space for distance features and the scope of pattern matching, due to the shallow nature of preprocessing, what is between two sentence breaks is by no means a well-formed sentence.", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 130, "end_pos": 146, "type": "TASK", "confidence": 0.7351908087730408}]}, {"text": "In that respect, it seems plausible to assume that smaller The value 500 is used as a MAX VALUE to indicate that no match was found.", "labels": [], "entities": [{"text": "MAX", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.7894119024276733}, {"text": "VALUE", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.814877986907959}]}, {"text": "units (as delimited by interruption points) maybe beneficial for precision as they give rise to fewer spurious matches.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9791080355644226}]}, {"text": "It must also be noted that interruption points do not mark arbitrary breaks in the flow of speech, but that they can signal important information (cf.).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classification of it by two annotators in a corpus subset.", "labels": [], "entities": []}]}