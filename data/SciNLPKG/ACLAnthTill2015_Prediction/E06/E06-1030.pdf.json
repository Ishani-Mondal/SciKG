{"title": [{"text": "Web Text Corpus for Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "Web text has been successfully used as training data for many NLP applications.", "labels": [], "entities": []}, {"text": "While most previous work accesses web text through search engine hit counts, we created a Web Corpus by downloading web pages to create a topic-diverse collection of 10 billion words of English.", "labels": [], "entities": []}, {"text": "We show that for context-sensitive spelling correction the Web Corpus results are better than using a search engine.", "labels": [], "entities": [{"text": "context-sensitive spelling correction", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.6642148792743683}, {"text": "Web Corpus", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.8529089391231537}]}, {"text": "For thesaurus extraction, it achieved similar overall results to a corpus of newspaper text.", "labels": [], "entities": [{"text": "thesaurus extraction", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8118074834346771}]}, {"text": "With many more words available on the web, better results can be obtained by collecting much larger web corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditional written corpora for linguistics research are created primarily from printed text, such as newspaper articles and books.", "labels": [], "entities": []}, {"text": "With the growth of the World Wide Web as an information resource, it is increasingly being used as training data in Natural Language Processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "There are many advantages to creating a corpus from web data rather than printed text.", "labels": [], "entities": []}, {"text": "All web data is already in electronic form and therefore readable by computers, whereas not all printed data is available electronically.", "labels": [], "entities": []}, {"text": "The vast amount of text available on the web is a major advantage, with estimating that over 98 billion words were indexed by Google in 2003.", "labels": [], "entities": []}, {"text": "The performance of NLP systems tends to improve with increasing amount of training data.", "labels": [], "entities": []}, {"text": "showed that for contextsensitive spelling correction, increasing the training data size increases the accuracy, for up to 1 billion words in their experiments.", "labels": [], "entities": [{"text": "contextsensitive spelling correction", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.6455998122692108}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9995594620704651}]}, {"text": "To date, most NLP tasks that have utilised web data have accessed it through search engines, using only the hit counts or examining a limited number of results pages.", "labels": [], "entities": []}, {"text": "The tasks are reduced to determining n-gram probabilities which are then estimated by hit counts from search engine queries.", "labels": [], "entities": []}, {"text": "This method only gathers information from the hit counts but does not require the computationally expensive downloading of actual text for analysis.", "labels": [], "entities": []}, {"text": "Unfortunately search engines were not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations).", "labels": [], "entities": []}, {"text": "proposed a linguistic search engine to extract word relationships more accurately.", "labels": [], "entities": []}, {"text": "We created a 10 billion word topic-diverse Web Corpus by spidering websites from a set of seed URLs.", "labels": [], "entities": []}, {"text": "The seed set is selected from the Open Directory to ensure that a diverse range of topics is included in the corpus.", "labels": [], "entities": [{"text": "Open Directory", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.8920518755912781}]}, {"text": "A process of text cleaning transforms the HTML text into a form useable by most NLP systems -tokenised words, one sentence per line.", "labels": [], "entities": [{"text": "text cleaning", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.7676483988761902}]}, {"text": "Text filtering removes unwanted text from the corpus, such as non-English sentences and most lines of text that are not grammatical sentences.", "labels": [], "entities": [{"text": "Text filtering", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7606136798858643}]}, {"text": "We compare the vocabulary of the Web Corpus with newswire.", "labels": [], "entities": [{"text": "Web Corpus", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.816902369260788}]}, {"text": "Our Web Corpus is evaluated on two NLP tasks.", "labels": [], "entities": [{"text": "Our Web Corpus", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.66008789340655}]}, {"text": "Context-sensitive spelling correction is a disambiguation problem, where the correction word in a confusion set (e.g. {their, they're}) needs to be selected fora given context.", "labels": [], "entities": [{"text": "Context-sensitive spelling correction", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.808574914932251}]}, {"text": "Thesaurus extraction is a similarity task, where synonyms of a target word are extracted from a corpus of unlabelled text.", "labels": [], "entities": [{"text": "Thesaurus extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8281763792037964}]}, {"text": "Our evaluation demonstrates that web text can be used for the same tasks as search engine hit counts and newspaper text.", "labels": [], "entities": []}, {"text": "However, there is a much larger quantity of freely available web text to exploit.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of a list of synonyms of a target word is subject to human judgement.", "labels": [], "entities": [{"text": "evaluation of a list of synonyms of a target word", "start_pos": 4, "end_pos": 53, "type": "TASK", "confidence": 0.8274958193302154}]}, {"text": "We use the evaluation method of Curran   house apartment building run office resident residence headquarters victory native place mansion room trip mile family night hometown town win neighborhood life suburb school restaurant hotel store city street season area road homer day car shop hospital friend game farm facility center north child land weekend community loss return hour . .", "labels": [], "entities": []}, {"text": "Web Corpus (18 matches out of 200) page loan contact house us owner search finance mortgage office map links building faq equity news center estate privacy community info business car site web improvement extention heating rate directory room apartment family service rental credit shop life city school property place location job online vacation store facility library free . .", "labels": [], "entities": [{"text": "Web Corpus", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8944064378738403}, {"text": "faq equity news center estate privacy community info business car site web improvement extention heating", "start_pos": 118, "end_pos": 222, "type": "TASK", "confidence": 0.5916815976301829}]}], "tableCaptions": [{"text": " Table 1: Classification of corpus token by type", "labels": [], "entities": []}, {"text": " Table 2: Misspellings of receive", "labels": [], "entities": []}, {"text": " Table 3: Context-sensitive spelling correction  (* denotes also using 60% WSJ, 5% corrupted)", "labels": [], "entities": [{"text": "Context-sensitive spelling correction", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.6149737139542898}, {"text": "WSJ", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9282419681549072}]}, {"text": " Table 4: Memory-based learner results", "labels": [], "entities": []}, {"text": " Table 5: Average INVR for 300 headwords", "labels": [], "entities": [{"text": "INVR", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9748008847236633}]}, {"text": " Table 6: InvR scores ranked by difference, Giga- word to Web Corpus", "labels": [], "entities": []}]}