{"title": [{"text": "Determining Term Subjectivity and Term Orientation for Opinion Mining", "labels": [], "entities": [{"text": "Term Orientation", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7429687976837158}]}], "abstractContent": [{"text": "Opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses.", "labels": [], "entities": [{"text": "Opinion mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7965972423553467}]}, {"text": "To aid the extraction of opinions from text, recent work has tackled the issue of determining the orientation of \"subjec-tive\" terms contained in text, i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation.", "labels": [], "entities": []}, {"text": "This is believed to be of key importance for identifying the orientation of documents, i.e. determining whether a document expresses a positive or negative opinion about its subject matter.", "labels": [], "entities": [{"text": "identifying the orientation of documents", "start_pos": 45, "end_pos": 85, "type": "TASK", "confidence": 0.8743046522140503}]}, {"text": "We contend that the plain determination of the orientation of terms is not a realistic problem, since it starts from the non-realistic assumption that we already know whether a term is subjective or not; this would imply that a linguistic resource that marks terms as \"subjective\" or \"objective\" is available, which is usually not the case.", "labels": [], "entities": [{"text": "plain determination of the orientation of terms", "start_pos": 20, "end_pos": 67, "type": "TASK", "confidence": 0.7904042516435895}]}, {"text": "In this paper we confront the task of deciding whether a given term has a positive connotation, or a negative connotation, or has no subjective connotation at all; this problem thus subsumes the problem of determining subjectivity and the problem of determining orientation.", "labels": [], "entities": []}, {"text": "We tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection.", "labels": [], "entities": [{"text": "orientation detection", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.9593616127967834}]}, {"text": "Our results show that determining subjectivity and orientation is a much harder problem than determining orientation alone.", "labels": [], "entities": []}], "introductionContent": [{"text": "Opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses.", "labels": [], "entities": [{"text": "Opinion mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7965972423553467}]}, {"text": "Opinion-driven content management has several important applications, such as determining critics' opinions about a given product by classifying online product reviews, or tracking the shifting attitudes of the general public toward apolitical candidate by mining online forums.", "labels": [], "entities": [{"text": "Opinion-driven content management", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7448713382085165}]}, {"text": "Within opinion mining, several subtasks can be identified, all of them having to do with tagging a given document according to expressed opinion: 1.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.7827498912811279}]}, {"text": "determining document subjectivity, as in deciding whether a given text has a factual nature (i.e. describes a given situation or event, without expressing a positive or a negative opinion on it) or expresses an opinion on its subject matter.", "labels": [], "entities": []}, {"text": "This amounts to performing binary text categorization under categories Objective and Subjective (; Yu and Hatzivassiloglou, 2003); 2.", "labels": [], "entities": []}, {"text": "determining document orientation (or polarity), as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (); 3.", "labels": [], "entities": []}, {"text": "determining the strength of document orientation, as in deciding e.g. whether the Positive opinion expressed by a text on its subject matter is Weakly Positive, Mildly Positive, or Strongly Positive ().", "labels": [], "entities": []}, {"text": "To aid these tasks, recent work (; has tackled the issue of identifying the orientation of subjective terms contained in text, i.e. determining whether a term that carries opinionated content has a positive or a negative connotation (e.g. deciding that -using examples -honest and intrepid have a positive connotation while disturbing and superfluous have a negative connotation).", "labels": [], "entities": []}, {"text": "This is believed to be of key importance for identifying the orientation of documents, since it is by considering the combined contribution of these terms that one may hope to solve Tasks 1, 2 and 3 above.", "labels": [], "entities": [{"text": "identifying the orientation of documents", "start_pos": 45, "end_pos": 85, "type": "TASK", "confidence": 0.8920490741729736}]}, {"text": "The conceptually simplest approach to this latter problem is probably, who has obtained interesting results on Task 2 by considering the algebraic sum of the orientations of terms as representative of the orientation of the document they belong to; but more sophisticated approaches are also possible).", "labels": [], "entities": []}, {"text": "Implicit inmost works dealing with term orientation is the assumption that, for many languages for which one would like to perform opinion mining, there is no available lexical resource where terms are tagged as having either a Positive or a Negative connotation, and that in the absence of such a resource the only available route is to generate such a resource automatically.", "labels": [], "entities": [{"text": "term orientation", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7298970967531204}, {"text": "opinion mining", "start_pos": 131, "end_pos": 145, "type": "TASK", "confidence": 0.7448264360427856}]}, {"text": "However, we think this approach lacks realism, since it is also true that, for the very same languages, there is no available lexical resource where terms are tagged as having either a Subjective or an Objective connotation.", "labels": [], "entities": []}, {"text": "Thus, the availability of an algorithm that tags Subjective terms as being either Positive or Negative is of little help, since determining if a term is Subjective is itself non-trivial.", "labels": [], "entities": []}, {"text": "In this paper we confront the task of determining whether a given term has a Positive connotation (e.g. honest, intrepid), or a Negative connotation (e.g. disturbing, superfluous), or has instead no Subjective connotation at all (e.g. white, triangular); this problem thus subsumes the problem of deciding between Subjective and Objective and the problem of deciding between Positive and Negative.", "labels": [], "entities": []}, {"text": "We tackle this problem by testing three different variants of the semi-supervised method for orientation detection proposed in).", "labels": [], "entities": [{"text": "orientation detection", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.9675040543079376}]}, {"text": "Our results show that determining subjectivity and orientation is a much harder problem than determining orientation alone.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper we extend the method of) to the determination of term subjectivity and term orientation altogether.", "labels": [], "entities": [{"text": "term orientation", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.6611765772104263}]}, {"text": "We experiment with three \"philosophically\" different learning approaches to the problem of distinguishing between Positive, Negative, and Objective terms.", "labels": [], "entities": []}, {"text": "Approach I is a two-stage method which consists in learning two binary classifiers: the first classifier places terms into either Subjective or Objective, while the second classifier places terms that have been classified as Subjective by the first classifier into either Positive or Negative.", "labels": [], "entities": [{"text": "Approach I", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.7165036797523499}]}, {"text": "In the training phase, the terms in T r K p \u222a T r K n are used as training examples of category Subjective.", "labels": [], "entities": []}, {"text": "Approach II is again based on learning two binary classifiers.", "labels": [], "entities": []}, {"text": "Here, one of them must discriminate between terms that belong to the Positive category and ones that belong to its complement (not Positive), while the other must discriminate between terms that belong to the Negative category and ones that belong to its complement (not Negative).", "labels": [], "entities": []}, {"text": "Terms that have been classified both into Positive by the former classifier and into (not Negative) by the latter are deemed to be positive, and terms that have been classified both into (not Positive) by the former classifier and into Negative by the latter are deemed to be negative.", "labels": [], "entities": []}, {"text": "The terms that have been classified (i) into both (not Positive) and (not Negative), or (ii) into both Positive and Negative, are taken to be Objective.", "labels": [], "entities": []}, {"text": "In the training phase of Approach II, the terms in T r K n \u222a T r K o are used as training examples of category (not Positive), and the terms in T r K p \u222a T r K o are used as training examples of category (not Negative).", "labels": [], "entities": [{"text": "Approach II", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.7708855867385864}]}, {"text": "Approach III consists instead in viewing Positive, Negative, and Objective as three categories with equal status, and in learning a ternary classifier that classifies each term into exactly one among the three categories.", "labels": [], "entities": [{"text": "Approach", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8563557267189026}]}, {"text": "There are several differences among these three approaches.", "labels": [], "entities": []}, {"text": "A first difference, of a conceptual nature, is that only Approaches I and III view Objective as a category, or concept, in its own right, while Approach II views objectivity as a nonexistent entity, i.e. as the \"absence of subjectivity\" (in fact, in Approach II the training examples of Objective are only used as training examples of the complements of Positive and Negative).", "labels": [], "entities": []}, {"text": "A second difference is that Approaches I and II are based on standard binary classification technology, while Approach III requires \"multiclass\" (i.e. 1-of-m) classification.", "labels": [], "entities": []}, {"text": "As a consequence, while for the former we use well-known learners for binary classification (the naive Bayesian learner using the multinomial model, support vector machines using linear kernels, the Rocchio learner, and its PrTFIDF probabilistic version (Joachims, 1997)), for Approach III we use their multiclass versions . Before running our learners we make a pass of feature selection, with the intent of retaining only those features that are good at discriminating our categories, while discarding those which are not.", "labels": [], "entities": []}, {"text": "Feature selection is implemented by scoring each feature f k (i.e. each term that occurs in the glosses of at least one training term) by means of the mutual information (MI) function, defined as and discarding the x% features f k that minimize it.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.778024435043335}]}, {"text": "We will call x% the reduction factor.", "labels": [], "entities": [{"text": "reduction", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9740059971809387}]}, {"text": "Note that the set {c 1 , . .", "labels": [], "entities": []}, {"text": ", cm } from Equation 1 is interpreted differently in Approaches Ito III, and always consistently with who the categories at stake are.", "labels": [], "entities": [{"text": "Approaches Ito III", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.7724432945251465}]}, {"text": "Since the task we aim to solve is manifold, we will evaluate our classifiers according to two evaluation measures: \u2022 SO-accuracy, i.e. the accuracy of a classifier in separating Subjective from Objective, i.e. in deciding term subjectivity alone; \u2022 PNO-accuracy, the accuracy of a classifier in discriminating among Positive, Negative, The naive Bayesian, Rocchio, and PrTFIDF learners we have used are from Andrew McCallum's Bow package (http://www-2.cs.cmu.edu/\u02dcmccallum/bow/), while the SVMs learner we have used is Thorsten Joachims' SV M light (http://svmlight.joachims.org/), version 6.01.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9891022443771362}, {"text": "accuracy", "start_pos": 267, "end_pos": 275, "type": "METRIC", "confidence": 0.9891697764396667}]}, {"text": "Both packages allow the respective learners to be run in \"multiclass\" fashion. and Objective, i.e. in deciding both term orientation and subjectivity.", "labels": [], "entities": [{"text": "Objective", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.7939587235450745}]}], "tableCaptions": [{"text": " Table 1: Average and best accuracy values over  the four dimensions analysed in the experiments.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9702063798904419}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9984715580940247}]}, {"text": " Table 2: Human inter-coder agreement values re- ported by", "labels": [], "entities": []}]}