{"title": [], "abstractContent": [{"text": "The Arabic language is a collection of spoken dialects with important phonolog-ical, morphological, lexical, and syntactic differences, along with a standard written language, Modern Standard Arabic (MSA).", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 176, "end_pos": 204, "type": "DATASET", "confidence": 0.8336891432603201}]}, {"text": "Since the spoken dialects are not officially written, it is very costly to obtain adequate corpora to use for training dialect NLP tools such as parsers.", "labels": [], "entities": []}, {"text": "In this paper, we address the problem of parsing transcribed spoken Levantine Arabic (LA).", "labels": [], "entities": [{"text": "parsing transcribed spoken Levantine Arabic (LA)", "start_pos": 41, "end_pos": 89, "type": "TASK", "confidence": 0.9034286066889763}]}, {"text": "We do not assume the existence of any annotated LA corpus (except for development and testing), nor of a parallel corpus LA-MSA.", "labels": [], "entities": []}, {"text": "Instead, we use explicit knowledge about the relation between LA and MSA.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We first use DEV to determine which of the transformations are useful.", "labels": [], "entities": [{"text": "DEV", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.4786921441555023}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The baseline is the same as in the previous two approaches.", "labels": [], "entities": [{"text": "baseline", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9708503484725952}]}, {"text": "We see that important improvements are obtained using lexicon SLXUN.", "labels": [], "entities": []}, {"text": "Adding the SVO transformation does not improve the results, but the NEG and BD transformations help slightly, and their effect is (partly) cumulative.", "labels": [], "entities": [{"text": "NEG", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.8589425086975098}, {"text": "BD", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.8833397626876831}]}, {"text": "(We did not perform these tuning experiments on input with no POS tags.)", "labels": [], "entities": []}, {"text": "We also experimented with the SLXEM and BLXEM lexicons.", "labels": [], "entities": [{"text": "BLXEM", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.9499017596244812}]}, {"text": "There was no consistent improvement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sentence transduction results on DEV (la- beled precision/recall/F-measure)", "labels": [], "entities": [{"text": "Sentence transduction", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9610592424869537}, {"text": "DEV", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.5622910857200623}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9022374749183655}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.7684352993965149}, {"text": "F-measure", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.8428465127944946}]}, {"text": " Table 2: Sentence transduction results on TEST  (labeled F-measure)", "labels": [], "entities": [{"text": "Sentence transduction", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9803846776485443}, {"text": "TEST", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.8884201645851135}, {"text": "F-measure", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9239670634269714}]}, {"text": " Table 3: Treebank transduction results on  DEV(labeled precision/recall/F-measure)", "labels": [], "entities": [{"text": "DEV", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.7135043740272522}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9661303162574768}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.7836219668388367}, {"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.8698592185974121}]}, {"text": " Table 4: Treebank transduction results on TEST  (labeled F-measure)", "labels": [], "entities": [{"text": "TEST", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.7928813695907593}]}, {"text": " Table 5: Grammar transduction results on  development corpus (labeled precision/recall/F- measure)", "labels": [], "entities": [{"text": "Grammar transduction", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7786618173122406}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.987661600112915}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.7051487565040588}, {"text": "F- measure", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9426320989926656}]}, {"text": " Table 6: Grammar transduction results on TEST  (labeled F-measure)", "labels": [], "entities": [{"text": "Grammar transduction", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7719132006168365}, {"text": "TEST", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.852349042892456}]}]}