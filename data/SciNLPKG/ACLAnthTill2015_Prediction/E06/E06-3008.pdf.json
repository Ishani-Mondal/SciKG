{"title": [{"text": "Towards Robust Animacy Classification Using Morphosyntactic Distributional Features", "labels": [], "entities": [{"text": "Robust Animacy Classification", "start_pos": 8, "end_pos": 37, "type": "TASK", "confidence": 0.7693033218383789}]}], "abstractContent": [{"text": "This paper presents results from experiments in automatic classification of animacy for Norwegian nouns using decision-tree classifiers.", "labels": [], "entities": [{"text": "automatic classification of animacy", "start_pos": 48, "end_pos": 83, "type": "TASK", "confidence": 0.7515821605920792}]}, {"text": "The method makes use of relative frequency measures for linguistically motivated morphosyn-tactic features extracted from an automatically annotated corpus of Norwegian.", "labels": [], "entities": []}, {"text": "The classifiers are evaluated using leave-one-out training and testing and the initial results are promising (approaching 90% accuracy) for high frequency nouns, however deteriorate gradually as lower frequency nouns are classified.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9984843134880066}]}, {"text": "Experiments attempting to empirically locate a frequency threshold for the classification method indicate that a subset of the chosen mor-phosyntactic features exhibit a notable resilience to data sparseness.", "labels": [], "entities": []}, {"text": "Results will be presented which show that the classification accuracy obtained for high frequency nouns (with absolute frequencies >1000) can be maintained for nouns with considerably lower frequencies (\u223c50) by backing off to a smaller set of features at classification .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9066531658172607}]}], "introductionContent": [{"text": "Animacy is a an inherent property of the referents of nouns which has been claimed to figure as an influencing factor in a range of different grammatical phenomena in various languages and it is correlated with central linguistic concepts such as agentivity and discourse salience.", "labels": [], "entities": []}, {"text": "Knowledge about the animacy of a noun is therefore relevant for several different kinds of NLP problems ranging from coreference resolution to parsing and generation.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.9377160668373108}, {"text": "parsing", "start_pos": 143, "end_pos": 150, "type": "TASK", "confidence": 0.9502621293067932}]}, {"text": "In recent years a range of linguistic studies have examined the influence of argument animacy in grammatical phenomena such as differential object marking, the passive construction, the dative alternation), etc.", "labels": [], "entities": [{"text": "differential object marking", "start_pos": 127, "end_pos": 154, "type": "TASK", "confidence": 0.6743876139322916}]}, {"text": "A variety of languages are sensitive to the dimension of animacy in the expression and interpretation of core syntactic arguments.", "labels": [], "entities": []}, {"text": "A key generalisation or tendency observed there is that prominent grammatical features tend to attract other prominent features; 1 subjects, for instance, will tend to be animate and agentive, whereas objects prototypically are inanimate and themes/patients.", "labels": [], "entities": []}, {"text": "Exceptions to this generalisation express a more marked structure, a property which has consequences, for instance, for the distributional properties of the structure in question.", "labels": [], "entities": []}, {"text": "Even though knowledge about the animacy of a noun clearly has some interesting implications, little work has been done within the field of lexical acquisition in order to automatically acquire such knowledge.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.7880201935768127}]}, {"text": "Or\u02d8 asan and Evans (2001) make use of hyponym-relations taken from the Word Net resource) in order to classify animate referents.", "labels": [], "entities": [{"text": "Word Net resource", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.9660057028134664}]}, {"text": "However, such a method is clearly restricted to languages for which large scale lexical resources, such as the Word Net, are available.", "labels": [], "entities": [{"text": "Word Net", "start_pos": 111, "end_pos": 119, "type": "DATASET", "confidence": 0.9711803495883942}]}, {"text": "present a method for verb classification which relies only on distributional statistics taken from corpora in order to train a decision tree classifier to distinguish between three groups of intransitive verbs.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7330257445573807}]}, {"text": "This paper presents experiments in automatic classification of the animacy of unseen Norwegian common nouns, inspired by the method for verb classification presented in.", "labels": [], "entities": [{"text": "automatic classification of the animacy of unseen Norwegian common nouns", "start_pos": 35, "end_pos": 107, "type": "TASK", "confidence": 0.7312319308519364}, {"text": "verb classification", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.7289559990167618}]}, {"text": "The learning task is, fora given common noun, to classify it as either belonging to the class animate or inanimate.", "labels": [], "entities": []}, {"text": "Based on correlations between animacy and other linguistic dimensions, a set of morphosyntactic features is presented and shown to differentiate common nouns along the binary dimension of animacy with promising results.", "labels": [], "entities": []}, {"text": "The method relies on aggregated relative frequencies for common noun lemmas, hence might be expected to seriously suffer from data sparseness.", "labels": [], "entities": []}, {"text": "Experiments attempting to empirically locate a frequency threshold for the classification method will therefore be presented.", "labels": [], "entities": []}, {"text": "It turns out that a subset of the chosen morphosyntactic approximators of animacy show a resilience to data sparseness which can be exploited in classification.", "labels": [], "entities": []}, {"text": "By backing off to this smaller set of features, we show that we can maintain the same classification accuracy also for lower frequency nouns.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9641661643981934}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 identifies and motivates the set of chosen features for the classification task and describes how these features are approximated through feature extraction from an automatically annotated corpus of Norwegian.", "labels": [], "entities": []}, {"text": "In section 3, a group of experiments testing the viability of the method and chosen features is presented.", "labels": [], "entities": []}, {"text": "Section 4 goes onto investigate the effect of sparse data on the classification performance and present experiments which address possible remedies for the sparse data problem.", "labels": [], "entities": []}, {"text": "Section 5 sums up the main findings of the previous sections and outlines a few suggestions for further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Based on the data collected on seven different features for our 40 nouns, a set of feature vectors are constructed for each noun.", "labels": [], "entities": []}, {"text": "They contain the relative frequencies for each feature along with the name of the noun and its class (animate or inanimate).", "labels": [], "entities": []}, {"text": "Note that the vectors do not contain the mean values presented in above, but rather the individual relative frequencies for each noun.", "labels": [], "entities": []}, {"text": "The experimental methodology chosen for the classification experiments is similar to the one described in for verb classification.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.737777590751648}]}, {"text": "We also make use of leave-oneout training and testing of the classifiers and the same software package for decision tree learning, C5.0, is employed.", "labels": [], "entities": [{"text": "decision tree learning", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.7673797011375427}]}, {"text": "In addition, all our classifiers employ the boosting option for constructing classifiers.", "labels": [], "entities": []}, {"text": "For calculation of the statistical significance of differences in the performance of classifiers tested on the same data set, McNemar's testis employed.", "labels": [], "entities": [{"text": "McNemar's testis", "start_pos": 126, "end_pos": 142, "type": "DATASET", "confidence": 0.9035230278968811}]}, {"text": "shows the performance of each individual feature in the classification of animacy.", "labels": [], "entities": [{"text": "classification", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.9739043116569519}]}, {"text": "As we can see, the performance of the features differ quite a bit, ranging from mere baseline performance (ANAIN) to a 70% improvement of the baseline (SUBJ).", "labels": [], "entities": [{"text": "baseline performance (ANAIN)", "start_pos": 85, "end_pos": 113, "type": "METRIC", "confidence": 0.7821204543113709}, {"text": "baseline (SUBJ)", "start_pos": 142, "end_pos": 157, "type": "METRIC", "confidence": 0.6937549114227295}]}, {"text": "The first line of shows the performance using all the seven features collectively where we achieve an accuracy of 87.5%, a 75% improvement of the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9996786117553711}]}, {"text": "The SUBJ, GEN and REFL features employed individually are the best performing individual features and their classification performance do not differ significantly from the performance of the combined classifier, whereas the rest of the individual features do (at the p<.05 level).", "labels": [], "entities": [{"text": "GEN", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.6323405504226685}, {"text": "REFL", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.8677772283554077}]}, {"text": "The subsequent lines (2-8) of show the accuracy results for classification using all features except one at a time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9997162222862244}, {"text": "classification", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.976179838180542}]}, {"text": "This provides an indication of the contribution of each feature to the classification task.", "labels": [], "entities": [{"text": "classification task", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.904608815908432}]}, {"text": "In general, the removal of a feature causes a 0% -12.5% deterioration of results, however, only the difference in performance caused by the removal of the REFL feature is significant (at the p<0.05 level).", "labels": [], "entities": []}, {"text": "Since this feature is one of the best performing features individually, it is not surprising that its removal causes a notable difference in performance.", "labels": [], "entities": []}, {"text": "The removal of the ANAIN feature, on the other hand, does not have any effect on accuracy whatsoever.", "labels": [], "entities": [{"text": "ANAIN", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.8792320489883423}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9989911913871765}]}, {"text": "This feature was the poorest performing feature with a baseline, or mere chance, performance.", "labels": [], "entities": []}, {"text": "We also see, however, that the behaviour of the features in combination is not strictly predictable from their individual performance, as presented in table 2.", "labels": [], "entities": []}, {"text": "The SUBJ, GEN and REFL features were the strongest features individually with a performance that did not differ significantly from that of the combined classifier.", "labels": [], "entities": [{"text": "GEN", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.5316678881645203}, {"text": "REFL", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9629494547843933}]}, {"text": "However, as line 9 in shows, the classifier as a whole is not solely reliant on these three features.", "labels": [], "entities": []}, {"text": "When they are removed from the feature pool, the performance (77.5% accuracy) does not differ significantly (p<.05) from that of the classifier employing all features collectively.", "labels": [], "entities": [{"text": "accuracy)", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9782662391662598}]}, {"text": "In order to establish how much of the generalizing power of the old classifier is lost when the frequency of the nouns is lowered, an experiment was conducted which tested the performance of the old classifier, i.e. a classifier trained on all the more frequent nouns, on the three groups of less frequent nouns.", "labels": [], "entities": []}, {"text": "As we can see from the first column in, this resulted in a clear deterioration of results, from our earlier accuracy of 87.5% to new accuracies ranging from 70% to 52.5%, barely above the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9948758482933044}]}, {"text": "Not surprisingly, the results decline steadily as the absolute frequency of the classified noun is lowered.", "labels": [], "entities": [{"text": "absolute frequency", "start_pos": 54, "end_pos": 72, "type": "METRIC", "confidence": 0.8538594841957092}]}, {"text": "Accuracy results provide an indication that the classification is problematic.", "labels": [], "entities": [{"text": "classification", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.9721243977546692}]}, {"text": "However, it does not indicate what the damage is to each class as such.", "labels": [], "entities": []}, {"text": "A confusion matrix is in this respect more informative.", "labels": [], "entities": []}, {"text": "Confusion matrices for the classification of the three groups of nouns, \u223c100, \u223c50 and \u223c10, are provided in table 5.", "labels": [], "entities": []}, {"text": "These clearly indicate that it is the animate class which suffers when data becomes more sparse.", "labels": [], "entities": []}, {"text": "The percentage of misclassified animate nouns drop drastically from 50% at \u223c100 to 80% at \u223c50 and finally 95% at \u223c10.", "labels": [], "entities": []}, {"text": "The classification of the inanimate class remains pretty stable throughout.", "labels": [], "entities": []}, {"text": "The fact that a majority of our features (SUBJ, GEN, PASS, ANAAN and REFL) target animacy, in the sense that a higher proportion of animate than inanimate nouns exhibit the feature, gives a possible explanation for this.", "labels": [], "entities": [{"text": "GEN", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.7704700827598572}, {"text": "PASS", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9806733131408691}, {"text": "ANAAN", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9887266755104065}, {"text": "REFL", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9900947213172913}]}, {"text": "As data gets more limited, this differentiation becomes harder to make, and the animate feature profiles come to resemble the inanimate more and more.", "labels": [], "entities": []}, {"text": "Because the inanimate nouns are expected to have low proportions (compared to the animate) for all these features, the data sparseness is not as damaging.", "labels": [], "entities": []}, {"text": "In order to examine the effect on each individual feature of the lowering of the frequency threshold, we also ran classifiers trained on the high frequency nouns with only individual features on the three groups of new nouns.", "labels": [], "entities": []}, {"text": "These results are depicted in.", "labels": [], "entities": []}, {"text": "In our earlier experiment, the performance of a majority of the individual features (OBJ, PASS, ANAAN, ANAIN) was significantly worse (at the p<0.05 level) than the performance of the classifier including all the features.", "labels": [], "entities": [{"text": "OBJ", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.5133780837059021}, {"text": "PASS", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9422926902770996}, {"text": "ANAAN", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9634615182876587}, {"text": "ANAIN", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9839037656784058}]}, {"text": "Three of the individual features (SUBJ, GEN, REFL) had a performance which did not differ significantly from that of the classifier employing all the features in combination.", "labels": [], "entities": [{"text": "GEN", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.6091792583465576}, {"text": "REFL", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9779893755912781}]}, {"text": "As the frequency threshold is lowered, however, the performance of the classifiers employing all features and those trained only on individual features become more similar.", "labels": [], "entities": []}, {"text": "For the \u223c100 nouns, only the two anaphoric features ANAAN and the reflexive feature REFL, have a performance that differs significantly (p<0.05) from the classifier employing all features.", "labels": [], "entities": [{"text": "ANAAN", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9865214824676514}, {"text": "REFL", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.934857189655304}]}, {"text": "For the \u223c50 and \u223c10 nouns, there are no significant differences between the classifiers employing individual fea-: Confusion matrices for classification of lower frequency nouns with old classifier tures only and the classifiers trained on the feature set as a whole.", "labels": [], "entities": []}, {"text": "This indicates that the combined classifiers no longer exhibit properties that are not predictable from the individual features alone and they do not generalize over the data based on the combinations of features.", "labels": [], "entities": []}, {"text": "In terms of accuracy, a few of the individual features even outperform the collective result.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9990624785423279}]}, {"text": "On average, the three most frequent features, the SUBJ, OBJ and GEN features, improve the performance by 9.5% for the \u223c100 nouns and 24.6% for the \u223c50 nouns.", "labels": [], "entities": [{"text": "OBJ", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.704903244972229}]}, {"text": "For the lowest frequency nouns (\u223c10) we see that the object feature alone improves the result by almost 24%, from 52.5% to 65 % accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9979173541069031}]}, {"text": "In fact, the object feature seems to be the most stable feature of all the features.", "labels": [], "entities": []}, {"text": "When examining the means of the results extracted for the different features, the object feature is the feature which maintains the largest difference between the two classes as the frequency threshold is lowered.", "labels": [], "entities": []}, {"text": "The second most stable feature in this respect is the subject feature.", "labels": [], "entities": []}, {"text": "The group of experiments reported above shows that the lowering of the frequency threshold for the classified nouns causes a clear deterioration of results in general, and most gravely when all the features are employed together.", "labels": [], "entities": []}, {"text": "The three most frequent features, the SUBJ, OBJ and GEN features, were the most stable in the two experiments reported above and had a performance which did not differ significantly from the combined classifiers throughout.", "labels": [], "entities": [{"text": "OBJ", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.7565696835517883}]}, {"text": "In light of this we ran some experiments where all possible combinations of these more frequent features were employed.", "labels": [], "entities": []}, {"text": "The results for each of the three groups of nouns is presented in.", "labels": [], "entities": []}, {"text": "The exclusion of the less frequent features has a clear positive effect on the accuracy results, as we can see in table 6.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.999594509601593}]}, {"text": "For the \u223c100 and \u223c50 nouns, the performance has improved compared to the classifier trained both on all the features and on the individual features.", "labels": [], "entities": []}, {"text": "The classification performance for these nouns is now identical or only slightly worse than the performance for the high-frequency nouns in experiment 1.", "labels": [], "entities": []}, {"text": "For the \u223c10 group of nouns, the performance is, at best, the same as for all the features and at worse fluctuating around baseline.", "labels": [], "entities": []}, {"text": "In general, the best performing feature combinations are SUBJ&OBJ&GEN and SUBJ&OBJ . These two differ significantly (at the p<.05 level) from the results obtained by employing all the features collectively for both the \u223c100 and the \u223c50 nouns, hence indicate a clear improvement.", "labels": [], "entities": []}, {"text": "The feature combinations both contain the two most stable features -one feature which targets the animate class (SUBJ) and another which target the inanimate class (OBJ), a property which facilitates differentiation even as the marginals between the two decrease.", "labels": [], "entities": []}, {"text": "It seems, then, that backing off to the most frequent features might constitute a partial remedy for the problems induced by data sparseness in the classification.", "labels": [], "entities": []}, {"text": "The feature combinations SUBJ&OBJ&GEN and SUBJ&OBJ both significantly improve the classification performance and actually enable us to maintain the same accuracy for both the \u223c100 and \u223c50 nouns as for the higher frequency nouns, as reported in experiment 1.: Accuracy obtained when employing the old classifier on new lower-frequency nouns: combinations of the most frequent features  Another option, besides a back-off to more frequent features in classification, is to back off to another classifier, i.e. a classifier trained on nouns with a similar frequency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9988406300544739}, {"text": "Accuracy", "start_pos": 259, "end_pos": 267, "type": "METRIC", "confidence": 0.9878498911857605}]}, {"text": "An approach of this kind will attempt to exploit any group similarities that these nouns may have in contrast to the mores frequent ones, hopefully resulting in a better classification.", "labels": [], "entities": []}, {"text": "In this experiment classifiers were trained and tested using leave-one-out cross-validation on the three groups of lower frequency nouns and employing individual, as well as various other feature combinations.", "labels": [], "entities": []}, {"text": "The results for all features as well as individual features are summarized in Table 7.", "labels": [], "entities": []}, {"text": "As we can see, the result for the classifier employing all the features has improved somewhat compared to the corresponding classifiers in experiment 3 (as reported above in) for all our three groups of nouns.", "labels": [], "entities": []}, {"text": "This indicates that there is a certain group similarity for the nouns of similar frequency that is captured in the combination of the seven features.", "labels": [], "entities": []}, {"text": "However, backing off to a classifier trained on nouns that are more similar frequency-wise does not cause an improvement in classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9155917763710022}]}, {"text": "Apart from the SUBJ feature for the \u223c100 nouns, none of the other classifiers trained on individual or all features for the three different groups differ significantly (p<.05) from their counterparts in experiment 3.", "labels": [], "entities": []}, {"text": "As before, combinations of the most frequent features were employed in the new classifiers trained and tested on each of the three frequencyordered groups of nouns.", "labels": [], "entities": []}, {"text": "In the terminology employed above, this amounts to a backing off both classifier-and feature-wise.", "labels": [], "entities": []}, {"text": "The accuracy measures obtained for these experiments are summarized in table 8.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9992784857749939}]}, {"text": "For these classifiers, the backed off feature combinations do not differ significantly (at the p<.05 level) from their counterparts in experiment 3, where the classifiers were trained on the more frequent nouns with feature back-off.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Mean relative frequencies and standard deviation for each class (A(nimate) vs. I(nanimate))  from feature extraction (SUBJ=Transitive Subject, OBJ=Object, GEN=Genitive -s, PASS=Passive by- phrase, ANAAN=Anaphoric reference by animate pronoun, ANAIN=Anaphoric reference by inanimate  pronoun, REFL=Anaphoric reference by reflexive pronoun).", "labels": [], "entities": [{"text": "PASS", "start_pos": 182, "end_pos": 186, "type": "METRIC", "confidence": 0.94646817445755}, {"text": "ANAAN", "start_pos": 207, "end_pos": 212, "type": "METRIC", "confidence": 0.9261656999588013}, {"text": "ANAIN", "start_pos": 253, "end_pos": 258, "type": "METRIC", "confidence": 0.9158902168273926}, {"text": "REFL", "start_pos": 302, "end_pos": 306, "type": "METRIC", "confidence": 0.9951991438865662}]}, {"text": " Table 2: Accuracy for the in- dividual features using leave- one-out training and testing", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985559582710266}]}, {"text": " Table 3: Accuracy for all features and 'all minus one' using leave-one-out  training and testing", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988309741020203}]}, {"text": " Table 4: Accuracy obtained when employing the old classifier on new lower-frequency nouns with leave- one-out training and testing: all and individual features", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9979585409164429}]}, {"text": " Table 5: Confusion matrices for classification of lower frequency nouns with old classifier", "labels": [], "entities": []}, {"text": " Table 6: Accuracy obtained when employing the old classifier on new lower-frequency nouns: combina- tions of the most frequent features", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9971147775650024}]}, {"text": " Table 8: Accuracy obtained when employing a new classifier on new lower-frequency nouns: combina- tions of the most frequent features", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9973899722099304}]}]}