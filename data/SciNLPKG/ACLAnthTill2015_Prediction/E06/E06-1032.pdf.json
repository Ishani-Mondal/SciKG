{"title": [{"text": "Re-evaluating the Role of BLEU in Machine Translation Research", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9557408094406128}, {"text": "Machine Translation Research", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.912465512752533}]}], "abstractContent": [{"text": "We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric.", "labels": [], "entities": [{"text": "machine translation community", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.8175327579180399}, {"text": "machine translation evaluation", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.6399023532867432}]}, {"text": "We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality , and give two significant counterexamples to Bleu's correlation with human judgments of quality.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.8019533157348633}]}, {"text": "This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.", "labels": [], "entities": [{"text": "Bleu scores", "start_pos": 111, "end_pos": 122, "type": "METRIC", "confidence": 0.9729507863521576}]}], "introductionContent": [{"text": "Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7936363518238068}, {"text": "summarization", "start_pos": 135, "end_pos": 148, "type": "TASK", "confidence": 0.9665983319282532}]}, {"text": "The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.659027506907781}, {"text": "Bleu", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.8766741156578064}]}, {"text": "Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations.", "labels": [], "entities": [{"text": "Bleu scores", "start_pos": 92, "end_pos": 103, "type": "METRIC", "confidence": 0.9328219592571259}]}, {"text": "Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation.", "labels": [], "entities": [{"text": "Bleu scores", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9096423089504242}]}, {"text": "All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality, which has been shown to hold in many cases.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.8764612674713135}]}, {"text": "However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee genuine translation improvements.", "labels": [], "entities": []}, {"text": "If Bleu's correlation with human judgments has been overestimated, then the field needs to ask itself whether it should continue to be driven by Bleu to the extent that it currently is.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.77125483751297}]}, {"text": "In this paper we give a number of counterexamples for Bleu's correlation with human judgments.", "labels": [], "entities": []}, {"text": "We show that under some circumstances an improvement in Bleu is not sufficient to reflect a genuine improvement in translation quality, and in other circumstances that it is not necessary to improve Bleu in order to achieve a noticeable improvement in translation quality.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.5358635783195496}, {"text": "Bleu", "start_pos": 199, "end_pos": 203, "type": "DATASET", "confidence": 0.6221175193786621}]}, {"text": "We argue that Bleu is insufficient by showing that Bleu admits a huge amount of variation for identically scored hypotheses.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.7915403246879578}]}, {"text": "Typically there are millions of variations on a hypothesis translation that receive the same Bleu score.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.9810922741889954}]}, {"text": "Because not all these variations are equally grammatically or semantically plausible there are translations which have the same Bleu score but a worse human evaluation.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9786398708820343}]}, {"text": "We further illustrate that in practice a higher Bleu score is not necessarily indicative of better translation quality by giving two substantial examples of Bleu vastly underestimating the translation quality of systems.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9551412463188171}]}, {"text": "Finally, we discuss appropriate uses for Bleu and suggest that for some research projects it maybe preferable to use a focused, manual evaluation instead.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.4876299202442169}]}], "datasetContent": [], "tableCaptions": []}