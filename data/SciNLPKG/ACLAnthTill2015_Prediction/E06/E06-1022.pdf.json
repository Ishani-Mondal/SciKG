{"title": [], "abstractContent": [{"text": "We present results on addressee identification in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers.", "labels": [], "entities": [{"text": "addressee identification", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.6960442662239075}]}, {"text": "First, we investigate how well the addressee of a dialogue act can be predicted based on gaze, utterance and conversational context features.", "labels": [], "entities": []}, {"text": "Then, we explore whether information about meeting context can aid classi-fiers' performances.", "labels": [], "entities": []}, {"text": "Both classifiers perform the best when conversational context and utterance features are combined with speaker's gaze information.", "labels": [], "entities": []}, {"text": "The classifiers show little gain from information about meeting context.", "labels": [], "entities": []}], "introductionContent": [{"text": "Addressing is an aspect of every form of communication.", "labels": [], "entities": []}, {"text": "It represents a form of orientation and directionality of the act the current actor performs toward the particular other(s) who are involved in an interaction.", "labels": [], "entities": []}, {"text": "In conversational communication involving two participants, the hearer is always the addressee of the speech act that the speaker performs.", "labels": [], "entities": []}, {"text": "Addressing, however, becomes areal issue in multi-party conversation.", "labels": [], "entities": []}, {"text": "The concept of addressee as well as a variety of mechanisms that people use in addressing their speech have been extensively investigated by conversational analysts and social psychologists.", "labels": [], "entities": []}, {"text": "Recently, addressing has received considerable attention in modeling multi-party interaction in various domains.", "labels": [], "entities": [{"text": "addressing", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9667899012565613}]}, {"text": "Research on automatic addressee identification has been conducted in the context of mixed human-human and human-computer interaction (), human-humanrobot interaction), and mixed human-agents and multi-agents interaction).", "labels": [], "entities": [{"text": "automatic addressee identification", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.6002638737360636}]}, {"text": "In the context of automatic analysis of multi-party face-to-face conversation, proposed a framework for automating inference of conversational structure that is defined in terms of conversational roles: speaker, addressee and unaddressed participants.", "labels": [], "entities": []}, {"text": "In this paper, we focus on addressee identification in a special type of communication, namely, face-to-face meetings.", "labels": [], "entities": [{"text": "addressee identification", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.7087218016386032}]}, {"text": "Moreover, we restrict our analysis to small group meetings with four participants.", "labels": [], "entities": []}, {"text": "Automatic analysis of recorded meetings has become an emerging domain fora range of research focusing on different aspects of interactions among meeting participants.", "labels": [], "entities": [{"text": "Automatic analysis of recorded meetings", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8412883043289184}]}, {"text": "The outcomes of this research should be combined in a targeted application that would provide users with useful information about meetings.", "labels": [], "entities": []}, {"text": "For answering questions such as \"Who was asked to prepare a presentation for the next meeting?\" or \"Were there any arguments between participants A and B?\", some sort of understanding of dialogue structure is required.", "labels": [], "entities": []}, {"text": "In addition to identification of dialogue acts that participants perform in multi-party dialogues, identification of addressees of those acts is also important for inferring dialogue structure.", "labels": [], "entities": []}, {"text": "There are many applications related to meeting research that could benefit from studying addressing in human-human interactions.", "labels": [], "entities": []}, {"text": "The results can be used by those who develop communicative agents in interactive intelligent environments and remote meeting assistants.", "labels": [], "entities": []}, {"text": "These agents need to recognize when they are being addressed and how they should address people in the environment.", "labels": [], "entities": []}, {"text": "This paper presents results on addressee identi-fication in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers.", "labels": [], "entities": []}, {"text": "The goals in the current paper are (1) to find relevant features for addressee classification in meeting conversations using information obtained from multi-modal resources -gaze, speech and conversational context, (2) to explore to what extent the performances of classifiers can be improved by combining different types of features obtained from these resources, (3) to investigate whether the information about meeting context can aid the performances of classifiers, and (4) to compare performances of the Bayesian Network and Naive Bayes classifiers for the task of addressee prediction over various feature sets.", "labels": [], "entities": [{"text": "addressee classification", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.7301572114229202}, {"text": "addressee prediction", "start_pos": 571, "end_pos": 591, "type": "TASK", "confidence": 0.7330819666385651}]}], "datasetContent": [{"text": "The performances of the classifiers are measured using different feature sets.", "labels": [], "entities": []}, {"text": "First, we measured the performances of classifiers using utterance features, gaze features and contextual features separately.", "labels": [], "entities": []}, {"text": "Then, we conducted experiments with all possible combinations of different types of features.", "labels": [], "entities": []}, {"text": "For each classifier, we performed 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "summarizes the accuracies of the classifiers (with 95% confidence interval) for different feature sets (1) using gaze information of all meeting participants and (2) using only information about speaker gaze direction.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9687538146972656}]}, {"text": "The results show that the Bayesian Network classifier outperforms the Naive Bayes classifier for all feature sets, although the difference is significant only for the feature sets that include contextual features.", "labels": [], "entities": []}, {"text": "For the feature set that contains only information about gaze behavior combined with information about the speaker (Gaze+SP), both classifiers perform significantly better when exploiting gaze information of all meeting participants.", "labels": [], "entities": []}, {"text": "In other words, when using solely focus of visual attention to identify the addressee of a dialogue act, listeners' focus of attention provides valuable information for addressee prediction.", "labels": [], "entities": [{"text": "addressee prediction", "start_pos": 169, "end_pos": 189, "type": "TASK", "confidence": 0.7005249857902527}]}, {"text": "The same conclusion can be drawn when adding information about utterance duration to the gaze feature set (Gaze+SP+Short), although for the Bayesian Network classifier the difference is not significant.", "labels": [], "entities": []}, {"text": "For all other feature sets, the classifiers do not perform significantly different when including or excluding the listeners gaze information.", "labels": [], "entities": []}, {"text": "Even more, both classifiers perform better using only speaker gaze information in all cases except when combined utterance and gaze features are exploited (Utterance+Gaze+SP).", "labels": [], "entities": []}, {"text": "The Bayesian network and Naive Bayes classifiers show the same changes in the performances over different feature sets.", "labels": [], "entities": []}, {"text": "The results indicate that the selected utterance features are less informative for addressee prediction (BN:52.62%, NB:52.50%) compared to contextual features (BN:73.11%; NB:68.12%) or features of gaze behavior (BN:66.45%, NB:64.53%).", "labels": [], "entities": [{"text": "addressee prediction", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7227720022201538}, {"text": "BN", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.843411922454834}]}, {"text": "The results also show that adding the information about the utterance duration to the gaze features, slightly increases the accuracies of the classifiers (BN:67.73%, NB:65.94%), which confirms findings presented in (.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.9961537718772888}, {"text": "BN", "start_pos": 155, "end_pos": 157, "type": "METRIC", "confidence": 0.9955990314483643}, {"text": "NB", "start_pos": 166, "end_pos": 168, "type": "METRIC", "confidence": 0.8383448719978333}]}, {"text": "Combining the information from the gaze and speech channels significantly improves the performances of the classifiers (BN:70.68%; NB:69.78%) in comparison to performances obtained from each channel separately.", "labels": [], "entities": [{"text": "BN", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.9946205615997314}, {"text": "NB", "start_pos": 131, "end_pos": 133, "type": "METRIC", "confidence": 0.46565183997154236}]}, {"text": "Furthermore, higher accuracies are gained when adding contextual features to the utterance features (BN:76.82%; NB:72.21%) and even more to the features of gaze behavior (BN:80.03%, NB:77.59%).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9931530952453613}, {"text": "BN", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.8407852649688721}]}, {"text": "As it is expected, the best performances are achieved by combining all three types of features (BN:82.59%, NB:78.49%), although not significantly better compared to combined contextual and gaze features.", "labels": [], "entities": [{"text": "BN", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.9978470802307129}, {"text": "NB:78.49%)", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.8925811350345612}]}, {"text": "We also explored how well the addressee can be predicted excluding information about the related utterance (i.e. AP information).", "labels": [], "entities": [{"text": "AP", "start_pos": 113, "end_pos": 115, "type": "METRIC", "confidence": 0.9248219728469849}]}, {"text": "The best performances are achieved combining speaker gaze information with contextual and utterance features (BN:79.39%; NB:76.06%).", "labels": [], "entities": [{"text": "BN:79.39%", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.936604306101799}, {"text": "NB:76.06%)", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.5982257202267647}]}, {"text": "A small decrease in the classification accuracies when excluding AP information (about 3%) indicates that remaining contextual, utterance and gaze features capture most of the useful information provided by AP.: Classification results for Bayesian Network and Naive Bayes classifiers using gaze information of all meeting participants (Gaze All) and using speaker gaze information (Gaze SP) Error analysis Further analysis of confusion matrixes for the best performed BN and NB classifiers, show that most misclassifications were between addressing types (individual vs. group): each P x was more confused with ALLP than with P y . A similar type of confusion is observed between human annotators regarding addressee annotation ().", "labels": [], "entities": []}, {"text": "Out of all misclassified cases for each classifier, individual types of addressing (P x ) were, in average, misclassified with addressing the group (ALLP) in 73% cases for NB, and 68% cases for BN.", "labels": [], "entities": [{"text": "BN", "start_pos": 194, "end_pos": 196, "type": "DATASET", "confidence": 0.7259283661842346}]}, {"text": "We examined whether meeting context information can aid the classifiers' performances.", "labels": [], "entities": []}, {"text": "First, we conducted experiments using the six values set for the MA-TYPE feature.", "labels": [], "entities": []}, {"text": "Then, we experimented with employing the reduced set of four types of meeting actions (see Section 5.2).", "labels": [], "entities": []}, {"text": "The accuracies obtained by combining the MA-TYPE feature with contextual, utterance and gaze features are presented in.", "labels": [], "entities": []}, {"text": "The results indicate that adding meeting context information to the initial feature set improves slightly, but not significantly, the classifiers' performances.", "labels": [], "entities": []}, {"text": "The highest accuracy (83.74%) is achieved using the Bayesian Network classifier by combining the four-values MA-TYPE feature with contextual, utterance and the speaker's gaze features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9993270635604858}, {"text": "MA-TYPE", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.8852673768997192}]}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement on DA and ad- dressee annotation: N-number of agreed segments", "labels": [], "entities": []}, {"text": " Table 4: Classification results combining MA- TYPE with the initial feature set", "labels": [], "entities": [{"text": "MA- TYPE", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.785901665687561}]}]}