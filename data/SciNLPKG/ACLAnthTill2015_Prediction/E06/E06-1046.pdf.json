{"title": [{"text": "Edit Machines for Robust Multimodal Language Processing", "labels": [], "entities": [{"text": "Robust Multimodal Language Processing", "start_pos": 18, "end_pos": 55, "type": "TASK", "confidence": 0.6009401381015778}]}], "abstractContent": [{"text": "Multimodal grammars provide an expressive formalism for multimodal integration and understanding.", "labels": [], "entities": [{"text": "multimodal integration", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7990517020225525}]}, {"text": "However, hand-crafted multimodal grammars can be brittle with respect to unexpected, erroneous, or disfluent inputs.", "labels": [], "entities": []}, {"text": "Spoken language (speech-only) understanding systems have addressed this issue of lack of robustness of hand-crafted grammars by exploiting classification techniques to extract fillers of a frame representation.", "labels": [], "entities": [{"text": "Spoken language (speech-only) understanding", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.5633083780606588}]}, {"text": "In this paper, we illustrate the limitations of such classification approaches for multimodal integration and understanding and present an approach based on edit machines that combine the expressiveness of multimodal grammars with the robustness of stochas-tic language models of speech recognition.", "labels": [], "entities": [{"text": "multimodal integration and understanding", "start_pos": 83, "end_pos": 123, "type": "TASK", "confidence": 0.6885373145341873}, {"text": "speech recognition", "start_pos": 280, "end_pos": 298, "type": "TASK", "confidence": 0.7309080809354782}]}, {"text": "We also present an approach where the edit operations are trained from data using a noisy channel model paradigm.", "labels": [], "entities": []}, {"text": "We evaluate and compare the performance of the hand-crafted and learned edit machines in the context of a multimodal conversational system (MATCH).", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the years, there have been several multimodal systems that allow input and/or output to be conveyed over multiple channels such as speech, graphics, and gesture, for example, put that there,,,), Match ( ).", "labels": [], "entities": [{"text": "Match", "start_pos": 200, "end_pos": 205, "type": "METRIC", "confidence": 0.9965135455131531}]}, {"text": "Multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars).", "labels": [], "entities": [{"text": "Multimodal integration", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.797810286283493}]}, {"text": "These grammars support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation.", "labels": [], "entities": []}, {"text": "In), we have shown that such grammars can be compiled into finite-state transducers enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities.", "labels": [], "entities": [{"text": "gesture recognition", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.6933629363775253}]}, {"text": "However, like other approaches based on handcrafted grammars, multimodal grammars can be brittle with respect to extra-grammatical, erroneous and disfluent input.", "labels": [], "entities": []}, {"text": "For speech recognition, a corpus-driven stochastic language model (SLM) with smoothing or a combination of grammarbased and -gram model) can be builtin order to overcome the brittleness of a grammar-based language model.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7678300142288208}]}, {"text": "Although the corpus-driven language model might recognize a user's utterance correctly, the recognized utterance may not be assigned a semantic representation by the multimodal grammar if the utterance is not part of the grammar.", "labels": [], "entities": []}, {"text": "There have been two main approaches to improving robustness of the understanding component in the spoken language understanding literature.", "labels": [], "entities": []}, {"text": "First, a parsing-based approach attempts to recover partial parses from the parse chart when the input cannot be parsed in its entirety due to noise, in order to construct a (partial) semantic representation (.", "labels": [], "entities": []}, {"text": "Second, a classification-based approach views the problem of understanding as extracting certain bits of information from the input.", "labels": [], "entities": []}, {"text": "It attempts to classify the utterance and identifies substrings of the input as slot-filler values to construct a frame-like semantic representation.", "labels": [], "entities": []}, {"text": "Although in the first approach, the grammar can encode richer semantic representations, the method for combining the fragmented parses is quite ad hoc.", "labels": [], "entities": []}, {"text": "In the second approach, the robustness is derived from training classifiers on annotated data, this data is very expensive to collect and annotate, and the semantic representation is fairly limited.", "labels": [], "entities": []}, {"text": "Furthermore, it is not clear how to extend this approach to apply on lattice input -an important requirement for multimodal processing.", "labels": [], "entities": []}, {"text": "An alternative to these approaches is to edit the recognized string to match the closest string that can be accepted by the grammar.", "labels": [], "entities": []}, {"text": "Essentially the idea is that, if the recognized string cannot be parsed, then we determine which in-grammar string it is most like.", "labels": [], "entities": []}, {"text": "For example, in, the recognized string is mapped to the closest string in the grammar by deletion of the words restaurants and in.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the approach, we collected a corpus of multimodal utterances for the MATCH domain in a laboratory setting from a set of sixteen first time users (8 male, 8 female).", "labels": [], "entities": []}, {"text": "A total of 833 user interactions (218 multimodal / 491 speech-only / 124 pen-only) resulting from six sample task scenarios were collected and annotated for speech transcription, gesture, and meaning ).", "labels": [], "entities": [{"text": "speech transcription", "start_pos": 157, "end_pos": 177, "type": "TASK", "confidence": 0.7069791257381439}]}, {"text": "These scenarios involved finding restaurants of various types and getting their names, phone numbers, addresses, or reviews, and getting subway directions between locations.", "labels": [], "entities": []}, {"text": "The data collected was conversational speech where the users gestured and spoke freely.", "labels": [], "entities": []}, {"text": "Since we are concerned herewith editing errors out of disfluent, misrecognized or unexpected speech, we report results on the 709 inputs that involve speech (491 unimodal speech and 218 multimodal).", "labels": [], "entities": []}, {"text": "Since there are only a small number of scenarios performed by all users, we partitioned the data six ways by scenario.", "labels": [], "entities": []}, {"text": "This ensures that the specific tasks in the test data for each partition are not also found in the training data for that partition.", "labels": [], "entities": []}, {"text": "For each scenario we built a class-based trigram language model using the other five scenarios as training data.", "labels": [], "entities": []}, {"text": "Averaging over the six partitions, ASR sentence accuracy was 49% and word accuracy was 73.4%.", "labels": [], "entities": [{"text": "ASR sentence", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.6736021637916565}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.8958905935287476}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.7511297464370728}]}, {"text": "In order to evaluate the understanding performance of the different edit machines, for each partition of the data we first composed the output from speech recognition with the edit machine and the multimodal grammar, flattened the meaning representation (as described in Section 3.1), and computed the exact string match accuracy between the flattened meaning representation and the reference meaning representation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.7363745868206024}, {"text": "exact string match accuracy", "start_pos": 302, "end_pos": 329, "type": "METRIC", "confidence": 0.5909254178404808}]}, {"text": "We then averaged this concept sentence accuracy measure overall six partitions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9524037837982178}]}, {"text": "The results are tabulated in.", "labels": [], "entities": []}, {"text": "The columns show the concept sentence accuracy (ConSentAcc) and the relative improvement over the the baseline of no edits.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9560298919677734}, {"text": "ConSentAcc)", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9147717654705048}]}, {"text": "Compared to the baseline of 38.9% concept sentence accuracy without edits (No Edits), Basic Edit gave a relative improvement of 32%, yielding 51.5% concept sentence accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9352161288261414}, {"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9147352576255798}]}, {"text": "4-edit further improved concept sentence accuracy (53%) compared to Basic Edit.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9865264296531677}]}, {"text": "The heuristics in Smart Edit brought the concept sentence accuracy to 60.2%, a 55% improvement over the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9963510036468506}]}, {"text": "Applying Smart edit to lattice input improved performance from 60.2% to 63.2%.", "labels": [], "entities": []}], "tableCaptions": []}