{"title": [{"text": "Semantic Role Labeling for Coreference Resolution", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7469743490219116}, {"text": "Coreference Resolution", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.9692398011684418}]}], "abstractContent": [{"text": "Extending a machine learning based coref-erence resolution system with a feature capturing automatically generated information about semantic roles improves its performance.", "labels": [], "entities": [{"text": "coref-erence resolution", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7731870710849762}]}], "introductionContent": [{"text": "The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (, inter alia).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.7346591055393219}]}, {"text": "Similarly, many researchers have explored techniques for robust, broad coverage semantic parsing in terms of semantic role labeling.", "labels": [], "entities": [{"text": "broad coverage semantic parsing", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.6066489219665527}, {"text": "semantic role labeling", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.6565853655338287}]}, {"text": "This paper explores whether coreference resolution can benefit from SRL, more specifically, which phenomena are affected by such information.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.9824460744857788}, {"text": "SRL", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9802339673042297}]}, {"text": "The motivation comes from the fact that current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.9413646161556244}, {"text": "string matching", "start_pos": 183, "end_pos": 198, "type": "TASK", "confidence": 0.7139560133218765}]}, {"text": "On the other hand, the literature emphasizes since the very beginning the relevance of world knowledge and inference.", "labels": [], "entities": []}, {"text": "As an example, consider a sentence from the Automatic Content Extraction (ACE) 2003 data.", "labels": [], "entities": [{"text": "Automatic Content Extraction (ACE) 2003 data", "start_pos": 44, "end_pos": 88, "type": "DATASET", "confidence": 0.7142521180212498}]}, {"text": "(1) A state commission of inquiry into the sinking of the Kursk will convene in Moscow on Wednesday, the Interfax news agency reported.", "labels": [], "entities": [{"text": "Interfax news agency", "start_pos": 105, "end_pos": 125, "type": "DATASET", "confidence": 0.9828058481216431}]}, {"text": "It said that the diving operation will be completed by the end of next week.", "labels": [], "entities": [{"text": "diving", "start_pos": 17, "end_pos": 23, "type": "TASK", "confidence": 0.9635804891586304}]}, {"text": "It seems that in this example, knowing that the Interfax news agency is the AGENT of the report predicate, and It being the AGENT of say, could trigger the (semantic parallelism based) inference required to correctly link the two expressions, in contrast to anchoring the pronoun to Moscow.", "labels": [], "entities": [{"text": "Interfax news agency", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.9728615880012512}, {"text": "AGENT", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.971259355545044}]}, {"text": "SRL provides the semantic relationships that constituents have with predicates, thus allowing us to include document-level event descriptive information into the relations holding between referring expressions (REs).", "labels": [], "entities": []}, {"text": "This layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than predicate argument statistics () and Latent Semantic Analysis used as a model of world knowledge).", "labels": [], "entities": []}, {"text": "In this respect, the present work is closer in spirit to, who explore the employment of the ACE 2004 relation ontology as a semantic filter.", "labels": [], "entities": [{"text": "ACE 2004 relation ontology", "start_pos": 92, "end_pos": 118, "type": "DATASET", "confidence": 0.884122297167778}]}], "datasetContent": [{"text": "We investigated the contribution of the different features in the learning process.", "labels": [], "entities": []}, {"text": "shows the chi-square statistic (normalized in the interval) for each feature occurring in the training data of the MERGED dataset.", "labels": [], "entities": [{"text": "MERGED dataset", "start_pos": 115, "end_pos": 129, "type": "DATASET", "confidence": 0.9587269127368927}]}, {"text": "SRL features show a high \u03c7 2 value, ranking immediately after string matching and alias, which indicates a high correlation of these features to the decision classes.", "labels": [], "entities": []}, {"text": "The importance of SRL is also indicated by the analysis of the contribution of individual features to the overall performance.", "labels": [], "entities": [{"text": "SRL", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9754562973976135}]}, {"text": "shows the performance variations obtained by leaving out each feature in turn.", "labels": [], "entities": []}, {"text": "Again, it can be seen that removing both I and J SEMROLE induces a relatively high performance degradation when compared to other features.", "labels": [], "entities": [{"text": "SEMROLE", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.4556228816509247}]}, {"text": "Their removal ranks 5th out of 12, following only essential features such as string matching, alias, pronoun and number.", "labels": [], "entities": [{"text": "string matching", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.7084924280643463}]}], "tableCaptions": [{"text": " Table 1: Partitions of the ACE 2003 training data corpus", "labels": [], "entities": [{"text": "ACE 2003 training data", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.9567951709032059}]}, {"text": " Table 2: Results on MUC", "labels": [], "entities": [{"text": "MUC", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.6934194564819336}]}, {"text": " Table 4: Results ACE (merged BNEWS/NWIRE)", "labels": [], "entities": [{"text": "ACE", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.6123960018157959}, {"text": "BNEWS/NWIRE)", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.7906146198511124}]}, {"text": " Table 5: \u03c7 2 statistic for each feature", "labels": [], "entities": []}, {"text": " Table 3: Results on the ACE 2003 data (BNEWS and NWIRE sections)", "labels": [], "entities": [{"text": "ACE 2003 data", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9543046156565348}, {"text": "BNEWS", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.8936336636543274}, {"text": "NWIRE", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.68892902135849}]}]}