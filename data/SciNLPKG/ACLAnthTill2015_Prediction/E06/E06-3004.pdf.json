{"title": [{"text": "Bootstrapping Named Entity Recognition with Automatically Generated Gazetteer Lists", "labels": [], "entities": [{"text": "Bootstrapping Named Entity Recognition", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.5657313615083694}]}], "abstractContent": [{"text": "Current Named Entity Recognition systems suffer from the lack of hand-tagged data as well as degradation when moving to other domain.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.6971129377683004}]}, {"text": "This paper explores two aspects: the automatic generation of gazetteer lists from unlabeled data; and the building of a Named Entity Recognition system with labeled and unlabeled data.", "labels": [], "entities": [{"text": "automatic generation of gazetteer lists", "start_pos": 37, "end_pos": 76, "type": "TASK", "confidence": 0.749530017375946}]}], "introductionContent": [{"text": "Automatic information extraction and information retrieval concerning particular person, location, organization, title of movie or book, juxtaposes to the Named Entity Recognition (NER) task.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7281998097896576}, {"text": "information retrieval concerning particular person, location, organization, title of movie or book", "start_pos": 37, "end_pos": 135, "type": "TASK", "confidence": 0.6708981851736705}, {"text": "Named Entity Recognition (NER)", "start_pos": 155, "end_pos": 185, "type": "TASK", "confidence": 0.8147527476151785}]}, {"text": "NER consists in detecting the most silent and informative elements in a text such as names of people, company names, location, monetary currencies, dates.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7893713712692261}]}, {"text": "Early NER systems (, etc., participating in Message Understanding Conferences (MUC), used linguistic tools and gazetteer lists.", "labels": [], "entities": [{"text": "Message Understanding Conferences (MUC)", "start_pos": 44, "end_pos": 83, "type": "TASK", "confidence": 0.7308853169282278}]}, {"text": "However these are difficult to develop and domain sensitive.", "labels": [], "entities": []}, {"text": "To surmount these obstacles, application of machine learning approaches to NER became a research subject.", "labels": [], "entities": [{"text": "NER", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9844582080841064}]}, {"text": "Various state-of-the-art machine learning algorithms such as Maximum Entropy), AdaBoost(), Hidden Markov Models (, Memory-based Based learning), have been used 1 . (,, (, () among others, combined several classifiers to obtain better named entity coverage rate.", "labels": [], "entities": []}, {"text": "Nevertheless all these machine learning algorithms rely on previously hand-labeled training data.", "labels": [], "entities": []}, {"text": "Obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding.", "labels": [], "entities": []}, {"text": "Resource limitation, directed NER research,, () toward the usage of semi-supervised techniques.", "labels": [], "entities": [{"text": "NER", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9568862318992615}]}, {"text": "These techniques are needed, as we live in a multilingual society and access to information from various language sources is reality.", "labels": [], "entities": []}, {"text": "The development of NER systems for languages other than English commenced.", "labels": [], "entities": [{"text": "NER", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.93647700548172}]}, {"text": "This paper presents the development of a Spanish Named Recognition system based on machine learning approach.", "labels": [], "entities": [{"text": "Spanish Named Recognition", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.6704931656519572}]}, {"text": "For it no morphologic or syntactic information was used.", "labels": [], "entities": []}, {"text": "However, we propose and incorporate a very simple method for automatic gazetteer 2 construction.", "labels": [], "entities": [{"text": "automatic gazetteer 2 construction", "start_pos": 61, "end_pos": 95, "type": "TASK", "confidence": 0.6193745136260986}]}, {"text": "Such method can be easily adapted to other languages and it is low-costly obtained as it relies on n-gram extraction from unlabeled data.", "labels": [], "entities": []}, {"text": "We compare the performance of our NER system when labeled and unlabeled training data is present.", "labels": [], "entities": [{"text": "NER", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9130454659461975}]}, {"text": "The paper is organized in the following way: brief explanation about NER process is represented in Section 2.", "labels": [], "entities": [{"text": "NER", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9723996520042419}]}, {"text": "In Section 3 follows feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8285323977470398}]}, {"text": "The experimental evaluation for the Named Entity detection and classification tasks with and without labeled data are in Sections 4 and 5.", "labels": [], "entities": [{"text": "Named Entity detection and classification", "start_pos": 36, "end_pos": 77, "type": "TASK", "confidence": 0.788681423664093}]}, {"text": "We conclude in Section 6.", "labels": [], "entities": []}, {"text": "The NER how to A Named Entity Recognition task can be described as composition of two subtasks, entity de-tection and entity classification.", "labels": [], "entities": [{"text": "NER how to A Named Entity Recognition task", "start_pos": 4, "end_pos": 46, "type": "TASK", "confidence": 0.8750407174229622}, {"text": "entity classification", "start_pos": 118, "end_pos": 139, "type": "TASK", "confidence": 0.7214857488870621}]}, {"text": "Entity delimitation consist in determining the boundaries of the entity (e.g. the place from where it starts and the place it finishes).", "labels": [], "entities": [{"text": "Entity delimitation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8320564925670624}]}, {"text": "This is important for tracing entities composed of two or more words such as \"Presidente de los Estados Unidos \" 3 , \"Universidad Politecnica de Catalu\u00f1a\" 4 . For this purpose, the BIO scheme was incorporated.", "labels": [], "entities": [{"text": "BIO", "start_pos": 181, "end_pos": 184, "type": "METRIC", "confidence": 0.5925957560539246}]}, {"text": "In this scheme, tag B denotes the start of an entity, tag I continues the entity and tag O marks words that do not form part of an entity.", "labels": [], "entities": []}, {"text": "This scheme was initially introduced in and) NER competitions, and we decided to adapt it for our experimental work.", "labels": [], "entities": [{"text": "NER competitions", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.5617499351501465}]}, {"text": "Once all entities in the text are detected, they are passed for classification in a predefined set of categories such as location, person, organization or miscellaneous 5 names.", "labels": [], "entities": []}, {"text": "This task is known as entity classification.", "labels": [], "entities": [{"text": "entity classification", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.7922129034996033}]}, {"text": "The final NER performance is measured considering the entity detection and classification tasks together.", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9768152236938477}, {"text": "entity detection and classification", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.6766739338636398}]}, {"text": "Our NER approach is based on machine learning.", "labels": [], "entities": [{"text": "NER", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9517201781272888}]}, {"text": "The two algorithms we used for the experiments were instance-based and decision trees, implemented by.", "labels": [], "entities": []}, {"text": "They were used with their default parameter settings.", "labels": [], "entities": []}, {"text": "We selected the instance-based model, because it is known to be useful when the amount of training data is not sufficient.", "labels": [], "entities": []}, {"text": "Important part in the NE process takes the location and person gazetteer lists which were automatically extracted from unlabeled data.", "labels": [], "entities": [{"text": "NE", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9802882671356201}]}, {"text": "More detailed explanation about their generation can be found in Section 3.", "labels": [], "entities": []}, {"text": "To explore the effect of labeled and unlabeled training data to our NER, two types of experiments were conducted.", "labels": [], "entities": [{"text": "NER", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.652539849281311}]}, {"text": "For the supervised approach, the labels in the training data were previously known.", "labels": [], "entities": []}, {"text": "For the semi-supervised approach, the labels in the training data were hidden.", "labels": [], "entities": []}, {"text": "We used bootstrapping) which refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier.", "labels": [], "entities": []}, {"text": "\u2022 Goals: -utilize a minimal amount of supervised examples; 3 \"President of the United States\" 4 \"Technical University of Catalu\u00f1a\" 5 book titles, sport events, etc.", "labels": [], "entities": []}, {"text": "-obtain learning from many unlabeled examples; \u2022 General scheme: -initial supervision seed examples for training an initial model; -corpus classification with seed model; -add most confident classifications to training data and iterate.", "labels": [], "entities": []}, {"text": "In our bootstrapping, a newly labeled example was added into the training data L, if the two classifiers C 1 and C 2 agreed on the class of that example.", "labels": [], "entities": []}, {"text": "The number n of iterations for our experiments is setup to 25 and when this bound is reached the bootstrapping stops.", "labels": [], "entities": []}, {"text": "The scheme we follow is described below.", "labels": [], "entities": []}, {"text": "1. for iteration = 0 . .", "labels": [], "entities": []}, {"text": "pool 1000 examples from unlabeled data; 3.", "labels": [], "entities": []}, {"text": "annotate all 1000 examples with classifier C 1 and C 2 ; 4.", "labels": [], "entities": []}, {"text": "for each of the 1000 examples compare classes of C 1 and C 2 ; 5.", "labels": [], "entities": []}, {"text": "add example into L only if classes of C 1 and C 2 agree; 6.", "labels": [], "entities": []}, {"text": "train model with L;) classified NEs through co-training, () used self-training and cotraining to detect and classify named entities in news domain,) conducted experiments with multi-criteria-based active learning for biomedical NER.", "labels": [], "entities": [{"text": "NEs", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.7251275777816772}, {"text": "biomedical NER", "start_pos": 217, "end_pos": 231, "type": "TASK", "confidence": 0.4937639534473419}]}, {"text": "The experimental data we work with is taken from the CoNLL-2002 competition.", "labels": [], "entities": [{"text": "CoNLL-2002 competition", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.9164749383926392}]}, {"text": "The Spanish corpus 6 comes from news domain and was previously manually annotated.", "labels": [], "entities": [{"text": "Spanish corpus 6", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.865278939406077}]}, {"text": "The train data set contains 264715 words of which 18798 are entities and the test set has 51533 words of which 3558 are entities.", "labels": [], "entities": [{"text": "train data set", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8691261013348898}]}, {"text": "We decided to work with available NE annotated corpora in order to conduct an exhaustive and comparative NER study when labeled and unlabeld data is present.", "labels": [], "entities": []}, {"text": "For our bootstrapping experiment, we simply ignored the presence of the labels in the training data.", "labels": [], "entities": []}, {"text": "Of course this approach can be applied to other domain or language, the only need is labeled test data to conduct correct evaluation.", "labels": [], "entities": []}, {"text": "The evaluation is computed per NE class by the help of conlleval 7 script.", "labels": [], "entities": []}, {"text": "The evaluation measures are: P recision = number of correct answers found by the system number of answers given by the system (1) Recall = number of correct answers found by the system number of correct answers in the test corpus (2)", "labels": [], "entities": [{"text": "P recision", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.8813934326171875}, {"text": "Recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9927431344985962}]}], "datasetContent": [{"text": "In this section we describe the conducted experiments for named entity detection.", "labels": [], "entities": [{"text": "named entity detection", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.636444479227066}]}, {"text": "Previously () demonstrated that in supervised learning only superficial features as context and ortografics are sufficient to identify the boundaries of a Named Entity.", "labels": [], "entities": []}, {"text": "In our experiment the superficial features f 1 \u00f7 f 10 were used by the supervised and semi-supervised classifiers.", "labels": [], "entities": []}, {"text": "shows the obtained results for Begin and Inside tags, which actually detect the entities and the total BIO tag performance.", "labels": [], "entities": []}, {"text": "On the first row are the results of the supervised method and on the second row are the highest results of the bootstrapping achieved in its seventeenth iteration.", "labels": [], "entities": []}, {"text": "For the supervised learning 91.88% of the entity boundaries were correctly identified and for the bootstrapping 81.62% were correctly detected.", "labels": [], "entities": []}, {"text": "The lower performance of bootstrapping is due to the noise introduced during the learning.", "labels": [], "entities": []}, {"text": "Some examples were learned with the wrong class and others didn't introduce new information in the training data.", "labels": [], "entities": []}, {"text": "presents the learning curve of the bootstrapping processes for 25 iterations.", "labels": [], "entities": []}, {"text": "On each iteration 1000 examples were tagged, but only the examples having classes that coincide by the two classifiers were later included in the training data.", "labels": [], "entities": []}, {"text": "We should note that for each iteration the same amount of B, I and O classes was included.", "labels": [], "entities": [{"text": "B", "start_pos": 58, "end_pos": 59, "type": "METRIC", "confidence": 0.9948411583900452}]}, {"text": "Thus the balance among the three different classes in the training data is maintained.", "labels": [], "entities": []}, {"text": "According to z \ud97b\udf59 statistics, the highest score reached by bootstrapping cannot outperform the supervised method, however if both methods were evaluated on small amount of data the results were similar.", "labels": [], "entities": []}, {"text": "Ina Named Entity classification process, to the previously detected Named Entities a predefined category of interest such as name of person, organization, location or miscellaneous names should be assigned.", "labels": [], "entities": [{"text": "Named Entity classification", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.694843610127767}]}, {"text": "To obtain a better idea of the performance of the classification methods, several experiments were conducted.", "labels": [], "entities": []}, {"text": "The influence of the automatically extracted gazetteers was studied, and a comparison of the supervised and semi-supervised methods was done.: F-score of classified entities.", "labels": [], "entities": [{"text": "F-score", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.9941257238388062}]}, {"text": "shows the obtained results for each one of the experimental settings.", "labels": [], "entities": []}, {"text": "The first row indicates the performance of the supervised classifier when no gazetteer information is present.", "labels": [], "entities": []}, {"text": "The classifier used f 1 , f 2 , f 3 , f 4 , f 5 , f 6 , f 7 , f 8 , f 18 , f 19 , f 20 , f 21 attributes.", "labels": [], "entities": []}, {"text": "The performance of the second row concerns the same classifier, but including the gazetteer information by adding f 22 , f 23 , f 24 and f 25 attributes.", "labels": [], "entities": []}, {"text": "The third row relates to the bootstrapping process.", "labels": [], "entities": []}, {"text": "The attributes used for the supervised and semi-supervised learning were the same.", "labels": [], "entities": []}, {"text": "Results show that among all classes, miscellaneous is the one with the lowest performance.", "labels": [], "entities": []}, {"text": "This is related to the heterogeneous information of the category.", "labels": [], "entities": []}, {"text": "The other three categories performed above 70%.", "labels": [], "entities": []}, {"text": "As expected gazetteer information contributed for better distinction of person and location names.", "labels": [], "entities": []}, {"text": "Organization names benefitted from the contextual information, the organization trigger words and the attribute validating if an entity is not a person or location then is treated as an organization.", "labels": [], "entities": []}, {"text": "Bootstrapping performance was not high, due to the previously 81% correctly detected named entity boundaries and from another side to the training examples which were incorrectly classified and included into the training data.", "labels": [], "entities": [{"text": "Bootstrapping", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.7632871270179749}]}, {"text": "In our experiment, unlabeled data was used to construct in an easy and effective way person and location gazetteer lists.", "labels": [], "entities": []}, {"text": "By their help supervised and semi-supervised classifiers improved performance.", "labels": [], "entities": []}, {"text": "Although one semi-supervised method cannot reach the performance of a supervised classifier, we can say that results are promising.", "labels": [], "entities": []}, {"text": "We call them promising in the aspect of constructing NE recognizer for languages with no resources or even adapting the present Spanish Named Entity system to other domain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: F-score of detected entities.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9982461929321289}]}, {"text": " Table 3: F-score of classified entities.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9983530044555664}]}]}