{"title": [{"text": "A Two-Stage Approach to Retrieving Answers for How-To Questions", "labels": [], "entities": [{"text": "Retrieving Answers for How-To Questions", "start_pos": 24, "end_pos": 63, "type": "TASK", "confidence": 0.8757819175720215}]}], "abstractContent": [{"text": "This paper addresses the problem of automatically retrieving answers for how-to questions, focusing on those that inquire about the procedure for achieving a specific goal.", "labels": [], "entities": []}, {"text": "For such questions, typical information retrieval methods, based on keyword matching, are better suited to detecting the content of the goal (e.g., 'installing a Windows XP server') than the general nature of the desired information (i.e., procedural, a series of steps for achieving this goal).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.7257792055606842}]}, {"text": "We suggest dividing the process of retrieving answers for such questions into two stages, with each stage focusing on modeling one aspect of a how-to question.", "labels": [], "entities": []}, {"text": "We compare the two-stage approach with two alternative approaches: a baseline approach that only uses the content of the goal to retrieve relevant documents and another approach that explores the potential of automatic query expansion.", "labels": [], "entities": [{"text": "automatic query expansion", "start_pos": 209, "end_pos": 234, "type": "TASK", "confidence": 0.6376895407835642}]}, {"text": "The result of the experiment shows that the two-stage approach significantly outperforms the baseline but achieves similar result with the systems using automatic query expansion techniques.", "labels": [], "entities": []}, {"text": "We analyze the reason and also present some future work.", "labels": [], "entities": []}], "introductionContent": [{"text": "How-To questions constitute a large proportion of questions on the Web.", "labels": [], "entities": []}, {"text": "Many how-to questions inquire about the procedure for achieving a specific goal.", "labels": [], "entities": []}, {"text": "For such questions, typical information retrieval (IR) methods, based on keyword matching, are better suited to detecting the content of the goal (e.g., installing a Windows XP server) than the general nature of the desired information (i.e., procedural, a series of steps for achieving this goal).", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.865759551525116}]}, {"text": "The reasons are given as below.", "labels": [], "entities": []}, {"text": "First, documents that describe a procedure often do not contain the word 'procedure' itself, but we are able to abstract the concept 'procedure' from cues such as 'first', 'next' and 'then', all of which indicate sequential relationships between actions.", "labels": [], "entities": []}, {"text": "Secondly, We expect that the word 'procedure' or the phrase 'how to' will occur in a much broader context than the words in the goal.", "labels": [], "entities": []}, {"text": "In other words, a document that contains the words in the goal is more likely to be relevant than a document that contains the word 'procedure' or the phrase 'how to'.", "labels": [], "entities": []}, {"text": "Without noticing this difference, treating the two parts equally in the retrieving process will get many noisy documents.", "labels": [], "entities": []}, {"text": "Many information requests seem to show such a structure, with one part identifying a specific topic and another part constraining the kind of information required about this topic).", "labels": [], "entities": []}, {"text": "The second part is often omitted when selecting retrieval terms from the request to construct an effective query for an IR system, such as in.", "labels": [], "entities": []}, {"text": "The first point given above suggests that using cues such as 'first' and 'next' to expand the initial query may help in retrieving more relevant documents.", "labels": [], "entities": []}, {"text": "Expansion terms can be generated automatically by query expansion techniques.", "labels": [], "entities": []}, {"text": "The typical process is: (1) use the initial query to retrieve documents (referred to as the first round of retrieval); (2) consider a few top ranked documents as relevant and the rest irrelevant; (3) compare the relevant set with the irrelevant set to extract a list of most distinctive terms; (4) use the extracted terms to retrieve documents (referred to as the second round of retrieval).", "labels": [], "entities": []}, {"text": "However, query expansion may not constitute a good solution, because its effectiveness largely depends on the quality of the few top ranked documents retrieved in the first round when the aforementioned two problems are not yet tackled.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.8440283834934235}]}, {"text": "Our solution is to divide the process of retrieving answers for such questions into two stages: (1) use typical IR approaches for retrieving documents that are relevant to the specific goal; (2) use a text categorization approach to re-rank the retrieved documents according to the proportion of procedural text they contain.", "labels": [], "entities": []}, {"text": "By 'procedural text' we refer to ordered lists of steps, which are very common in some instructional genres such as online manuals.", "labels": [], "entities": []}, {"text": "In this report, we will briefly introduce the text categorization approach (details are presented in) ) and will explain in more concrete terms how it is integrated into the two-stage architecture proposed above.", "labels": [], "entities": []}, {"text": "We will compare the performance of our two-stage architecture with a baseline system that uses only the content of the goal to retrieve relevant documents (equivalent to the first stage in the two-stage architecture).", "labels": [], "entities": []}, {"text": "We will also compare the two-stage approach with systems that applies automatic query expansion techniques.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces some relevant work in IR and question answering (QA).", "labels": [], "entities": [{"text": "IR", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9970955848693848}, {"text": "question answering (QA)", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.8695207238197327}]}, {"text": "Section 3 talks about the text categorization approach for ranking procedural documents, covering issues such as the features used, the training corpus, the design of a classification model as well as some experiments for evaluation.", "labels": [], "entities": []}, {"text": "Section 4 talks about integrating the text categorizer into the two-stage architecture and presents some experiments on retrieving relevant documents for how-to questions.", "labels": [], "entities": []}, {"text": "Section 5 provides a short summary and presents some future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are two sources from which we compiled the training and testing corpora: the Pagewise collection and the SPIRIT collection.", "labels": [], "entities": [{"text": "Pagewise collection", "start_pos": 83, "end_pos": 102, "type": "DATASET", "confidence": 0.9632468521595001}, {"text": "SPIRIT collection", "start_pos": 111, "end_pos": 128, "type": "DATASET", "confidence": 0.8372064232826233}]}, {"text": "The SPIRIT collection contains a terabyte of HTML that are crawled from the web starting from an initial seed set of a few thousands universities and other educational organizations ().", "labels": [], "entities": [{"text": "SPIRIT collection", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8269194066524506}]}, {"text": "Our test set contained 103 documents, including the 53 documents that were sampled previously and then separated from the initial training corpus, another 30 documents randomly chosen from the Pagewise collection and 20 documents chosen from the SPIRIT collection.", "labels": [], "entities": [{"text": "Pagewise collection", "start_pos": 193, "end_pos": 212, "type": "DATASET", "confidence": 0.9621421694755554}, {"text": "SPIRIT collection", "start_pos": 246, "end_pos": 263, "type": "DATASET", "confidence": 0.8487279117107391}]}, {"text": "We asked two human subjects to score the procedurality for these documents, following the same instruction described in section 3.2.", "labels": [], "entities": []}, {"text": "The correlation coefficient (Kendall tau-b) between the two rankings was 0.725, which is the upper bound of the performance of the classifiers.", "labels": [], "entities": [{"text": "correlation coefficient (Kendall tau-b)", "start_pos": 4, "end_pos": 43, "type": "METRIC", "confidence": 0.7911838044722875}]}, {"text": "We first used the initial training corpus to bootstrap a larger training set (378 procedural documents and 608 non-procedural documents), which was then used to select distinctive feature co-occurrence patterns and to train different classifiers.", "labels": [], "entities": []}, {"text": "We compared the Adapted Naive Bayes classifier with the Naive Bayes classifier and three other classifiers, including Maximum Entropy (ME) 4 , Alternating Decision Tree (ADTree)) and Linear Regression)..", "labels": [], "entities": [{"text": "Maximum Entropy (ME) 4", "start_pos": 118, "end_pos": 140, "type": "METRIC", "confidence": 0.8884443044662476}]}, {"text": "Ranking results using individual features: 1 refers to Adapted Naive Bayes, 2 refers to Naive Bayes, 3 refers to ME, 4 refers to ADTree and 5 refers to Linear Regression..", "labels": [], "entities": [{"text": "ME", "start_pos": 113, "end_pos": 115, "type": "METRIC", "confidence": 0.9852031469345093}]}, {"text": "Ranking results using feature cooccurrence patterns.", "labels": [], "entities": []}, {"text": "show the Kendall tau-b coefficient between human subjects' ranking results and the trained classifiers' ranking results of the test set when using individual features (112); and table 3 show the Kendall tau-b coefficient when using feature cooccurrence patterns (813).", "labels": [], "entities": []}, {"text": "We randomly chose 60 how-to questions from the query logs of the FAQ finder system ().", "labels": [], "entities": [{"text": "FAQ finder system", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.7181964119275411}]}, {"text": "Three judges went through these questions and agreed on 10 procedural questions . We searched Google and downloaded 40 top ranked documents for each question, which were then mixed with 1000 web documents from the SPIRIT collection to compile a test set.", "labels": [], "entities": [{"text": "SPIRIT collection", "start_pos": 214, "end_pos": 231, "type": "DATASET", "confidence": 0.8389880359172821}]}, {"text": "The twostage architecture is as shown in.", "labels": [], "entities": []}, {"text": "In the first stage, we sent only the content of the goal to a state-of-the-art IR model to retrieve 30 documents from the test set, which were reranked in the second stage according to the degree of procedurality by a trained document classifier.", "labels": [], "entities": []}, {"text": "We also tried to test how well query expansion could help in retrieving procedural documents, following a process as shown in.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.7402676641941071}]}, {"text": "First, key words in the content of goal were used to query an IR model to retrieve an initial set of relevant documents, those of which that do not contain the phrase 'how to' were then removed.", "labels": [], "entities": []}, {"text": "The remaining top ten documents were used to generate 40 searching terms, which were applied in the second round to retrieve documents.", "labels": [], "entities": []}, {"text": "Finally the 30 top ranked documents were returned as relevant documents..", "labels": [], "entities": []}, {"text": "An alternative architecture using query expansion.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.6954072862863541}]}], "tableCaptions": [{"text": " Table 2. Ranking results using individual  features.", "labels": [], "entities": []}, {"text": " Table 3. Ranking results using feature co- occurrence patterns.", "labels": [], "entities": []}, {"text": " Table 4. Results of different systems.", "labels": [], "entities": []}]}