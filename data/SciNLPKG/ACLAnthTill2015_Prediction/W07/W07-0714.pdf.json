{"title": [{"text": "Labelled Dependencies in Machine Translation Evaluation", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.844242513179779}]}], "abstractContent": [{"text": "We present a method for evaluating the quality of Machine Translation (MT) output, using labelled dependencies produced by a Lexical-Functional Grammar (LFG) parser.", "labels": [], "entities": [{"text": "Machine Translation (MT) output", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.8730849822362264}]}, {"text": "Our dependency-based method, in contrast to most popular string-based evaluation metrics, does not unfairly penalize perfectly valid syntactic variations in the translation, and the addition of WordNet provides away to accommodate lexical variation.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 194, "end_pos": 201, "type": "DATASET", "confidence": 0.9697885513305664}]}, {"text": "In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since the creation of BLEU () and NIST, the subject of automatic evaluation metrics for MT has been given quite a lot of attention.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9943056702613831}, {"text": "NIST", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.9488976001739502}, {"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9919289350509644}]}, {"text": "Although widely popular thanks to their speed and efficiency, both BLEU and NIST have been criticized for inadequate accuracy of evaluation at the segment level).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9411400556564331}, {"text": "NIST", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.9267651438713074}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9981698989868164}]}, {"text": "As string based-metrics, they are limited to superficial comparison of word sequences between a translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the translation, beyond what can be found in the multiple references.", "labels": [], "entities": []}, {"text": "A natural next step in the field of evaluation was to introduce metrics that would better reflect our human judgement by accepting synonyms in the translated sentence or evaluating the translation on the basis of what syntactic features it shares with the reference.", "labels": [], "entities": []}, {"text": "Our method follows and substantially extends the earlier work of, who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement.", "labels": [], "entities": [{"text": "MT", "start_pos": 133, "end_pos": 135, "type": "TASK", "confidence": 0.9920549392700195}, {"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9978622794151306}]}, {"text": "Dependencies abstract away from the particulars of the surface string (and syntactic tree) realization and provide a \"normalized\" representation of (some) syntactic variants of a given sentence.", "labels": [], "entities": []}, {"text": "While calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a LexicalFunctional Grammar (LFG) parser.", "labels": [], "entities": []}, {"text": "These dependencies differ from those used by, in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc.", "labels": [], "entities": []}, {"text": "The presence of grammatical relation labels adds another layer of important linguistic information into the comparison and allows us to account for partial matches, for example when a lexical item finds itself in a correct relation but with an incorrect partner.", "labels": [], "entities": []}, {"text": "Moreover, we use a number of best parses for the translation and the reference, which serves to decrease the amount of noise that can be introduced by the process of parsing and extracting dependency information.", "labels": [], "entities": [{"text": "parsing and extracting dependency information", "start_pos": 166, "end_pos": 211, "type": "TASK", "confidence": 0.6647778749465942}]}, {"text": "The translation and reference files are analyzed by a treebank-based, probabilistic LFG parser (), which produces a set of dependency triples for each input.", "labels": [], "entities": []}, {"text": "The translation set is compared to the reference set, and the number of matches is calculated, giving the precision, recall, and f-score for each particular translation.", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9997413754463196}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9984992742538452}, {"text": "f-score", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.9956507086753845}]}, {"text": "In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score.", "labels": [], "entities": []}, {"text": "In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium's (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (, Translation Error Rate (TER)) 1 , and METEOR (), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment.", "labels": [], "entities": [{"text": "Multiple Translation", "start_pos": 150, "end_pos": 170, "type": "TASK", "confidence": 0.6933134943246841}, {"text": "BLEU", "start_pos": 255, "end_pos": 259, "type": "METRIC", "confidence": 0.9964606165885925}, {"text": "Translation Error Rate (TER)) 1", "start_pos": 297, "end_pos": 328, "type": "METRIC", "confidence": 0.7664240258080619}, {"text": "METEOR", "start_pos": 335, "end_pos": 341, "type": "METRIC", "confidence": 0.9439839124679565}]}, {"text": "Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Translation data; Section 5 discusses ongoing work; Section 6 concludes.", "labels": [], "entities": [{"text": "LFG", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.7692291140556335}, {"text": "Multiple Translation", "start_pos": 206, "end_pos": 226, "type": "TASK", "confidence": 0.7668136656284332}]}], "datasetContent": [{"text": "LFG-based automatic MT evaluation reflects the same process that underlies the evaluation of parser-produced f-structure quality against a gold standard: we parse the translation and the reference, and then, for each sentence, we check the set of labelled translation dependencies against the set of labelled reference dependencies, counting the number of matches.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.8313190639019012}]}, {"text": "As a result, we obtain the precision and recall scores for the translation, and we calculate the f-score for the given pair.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9997162222862244}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9971734285354614}]}, {"text": "To evaluate the correlation with human assessment, we used the data from the Linguistic Data Consortium Multiple Translation Chinese (MTC) Parts 2 and 4, which consists of multiple translations of Chinese newswire text, four humanproduced references, and segment-level human scores fora subset of the translation-reference pairs.", "labels": [], "entities": [{"text": "Linguistic Data Consortium Multiple Translation Chinese (MTC) Parts 2", "start_pos": 77, "end_pos": 146, "type": "DATASET", "confidence": 0.8424459858374163}]}, {"text": "Although a single translated segment was always evaluated by more than one judge, the judges used a different reference every time, which is why we treated each translation-referencehuman score triple as a separate segment.", "labels": [], "entities": []}, {"text": "In effect, the test set created from this data contained 16,800 segments.", "labels": [], "entities": []}, {"text": "As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and our labelled dependencybased method.", "labels": [], "entities": [{"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9737879633903503}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9987506866455078}, {"text": "NIST", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.8387541174888611}, {"text": "TER", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9957701563835144}, {"text": "METEOR", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9893916845321655}]}], "tableCaptions": [{"text": " Table 1. Scores for sentences with reordered adjuncts", "labels": [], "entities": []}, {"text": " Table 2. Dependency f-scores for sentences with reordered  adjuncts with n-best parses available", "labels": [], "entities": []}, {"text": " Table 3. Pearson's correlation between human scores and  evaluation metrics. Legend: d = dependency f-score, _pr =  predicate-only f-score, 2, 10, 50 = n-best parses; var =  partial-match version; M = METEOR, WN = WordNet 6", "labels": [], "entities": [{"text": "METEOR", "start_pos": 202, "end_pos": 208, "type": "METRIC", "confidence": 0.9515101909637451}, {"text": "WordNet", "start_pos": 215, "end_pos": 222, "type": "DATASET", "confidence": 0.9467753767967224}]}]}