{"title": [{"text": "Evaluating Impact of Re-training a Lexical Disambiguation Model on Domain Adaptation of an HPSG Parser", "labels": [], "entities": [{"text": "Lexical Disambiguation Model", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.8532984455426534}, {"text": "Domain Adaptation", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7241206467151642}]}], "abstractContent": [{"text": "This paper describes an effective approach to adapting an HPSG parser trained on the Penn Treebank to a biomedical domain.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.8732009828090668}]}, {"text": "In this approach, we train probabilities of lexical entry assignments to words in a target domain and then incorporate them into the original parser.", "labels": [], "entities": []}, {"text": "Experimental results show that this method can obtain higher parsing accuracy than previous work on domain adaptation for parsing the same data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.9674776792526245}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9145296216011047}, {"text": "domain adaptation", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.7121493518352509}, {"text": "parsing", "start_pos": 122, "end_pos": 129, "type": "TASK", "confidence": 0.9597503542900085}]}, {"text": "Moreover, the results show that the combination of the proposed method and the existing method achieves parsing accuracy that is as high as that of an HPSG parser retrained from scratch, but with much lower training cost.", "labels": [], "entities": [{"text": "parsing", "start_pos": 104, "end_pos": 111, "type": "TASK", "confidence": 0.9790865778923035}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9847393035888672}]}, {"text": "We also evaluated our method in the Brown corpus to show the portability of our approach in another domain.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9482008218765259}]}], "introductionContent": [{"text": "Domain portability is an important aspect of the applicability of NLP tools to practical tasks.", "labels": [], "entities": [{"text": "Domain portability", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7629603743553162}]}, {"text": "Therefore, domain adaptation methods have recently been proposed in several NLP areas, e.g., word sense disambiguation (), statistical parsing (), and lexicalized-grammar parsing).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.751330554485321}, {"text": "word sense disambiguation", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.7142513990402222}, {"text": "statistical parsing", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.83701291680336}, {"text": "lexicalized-grammar parsing", "start_pos": 151, "end_pos": 178, "type": "TASK", "confidence": 0.7688131034374237}]}, {"text": "Their aim was to re-train a probabilistic model fora new domain at low cost, and more or less successfully improved the accuracy for the domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9992691874504089}]}, {"text": "In this paper, we propose a method for adapting an HPSG parser () trained on the WSJ section of the Penn Treebank () to a biomedical domain.", "labels": [], "entities": [{"text": "WSJ section of the Penn Treebank", "start_pos": 81, "end_pos": 113, "type": "DATASET", "confidence": 0.9321423768997192}]}, {"text": "Our method re-trains a probabilistic model of lexical entry assignments to words in a target domain, and incorporates it into the original parser.", "labels": [], "entities": []}, {"text": "The model of lexical entry assignments is a loglinear model re-trained with machine learning features only of word n-grams.", "labels": [], "entities": [{"text": "lexical entry assignments", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.6722060441970825}]}, {"text": "Hence, the cost for the re-training is much lower than the cost of training the entire disambiguation model from scratch.", "labels": [], "entities": []}, {"text": "In the experiments, we used an HPSG parser originally trained with the Penn Treebank, and evaluated a disambiguation model re-trained with the GENIA treebank (, which consists of abstracts of biomedical papers.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9965913593769073}, {"text": "GENIA treebank", "start_pos": 143, "end_pos": 157, "type": "DATASET", "confidence": 0.977825939655304}]}, {"text": "We varied the size of a training corpus, and measured the transition of the parsing accuracy and the cost required for parameter estimation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9634038805961609}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.7498213648796082}]}, {"text": "For comparison, we also examined other possible approaches to adapting the same parser.", "labels": [], "entities": []}, {"text": "In addition, we applied our approach to the Brown corpus () in order to examine portability of our approach.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.9577006697654724}]}, {"text": "The experimental results revealed that by simply re-training the probabilistic model of lexical entry assignments we achieve higher parsing accuracy than with a previously proposed adaptation method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9544869661331177}]}, {"text": "In addition, combined with the existing adaptation method, our approach achieves accuracy as high as that obtained by re-training the original parser from scratch, but with much lower training cost.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9992825388908386}]}, {"text": "In this paper, we report these experimental results in detail, and discuss how disambiguation models of lexical entry assignments contribute to domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.724914088845253}]}, {"text": "In recent years, it has been shown that lexical in-formation plays a very important role for high accuracy of lexicalized grammar parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.997715950012207}, {"text": "lexicalized grammar parsing", "start_pos": 110, "end_pos": 137, "type": "TASK", "confidence": 0.5928174257278442}]}, {"text": "indicated that, correct disambiguation with supertagging, i.e., assignment of lexical entries before parsing, enabled effective LTAG (Lexicalized Tree-Adjoining Grammar) parsing.", "labels": [], "entities": [{"text": "Lexicalized Tree-Adjoining Grammar) parsing", "start_pos": 134, "end_pos": 177, "type": "TASK", "confidence": 0.5423847436904907}]}, {"text": "showed that supertagging reduced cost for training and execution of a CCG (Combinatory Categorial Grammar) parser while keeping accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9981639981269836}]}, {"text": "showed that a CCG parser trained on data derived from lexical category sequences alone was only slightly less accurate than one trained on complete dependency structures.", "labels": [], "entities": []}, {"text": "also succeeded in significantly improving speed and accuracy of HPSG parsing by using supertagging probabilities.", "labels": [], "entities": [{"text": "speed", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9870585799217224}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9987483024597168}, {"text": "HPSG parsing", "start_pos": 64, "end_pos": 76, "type": "TASK", "confidence": 0.7382810711860657}]}, {"text": "These results indicate that the probability of lexical entry assignments is essential for parse disambiguation.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.9360490143299103}]}, {"text": "Such usefulness of lexical information has also been shown for domain adaptation methods.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7461731433868408}]}, {"text": "showed how existing domainspecific lexical resources on a target domain maybe leveraged to augment PTB-training: part-of-speech tags, dictionary collocations, and named-entities.", "labels": [], "entities": []}, {"text": "Our findings basically follow the above results.", "labels": [], "entities": []}, {"text": "The contribution of this paper is to provide empirical results of the relationships among domain variation, probability of lexical entry assignment, training data size, and training cost.", "labels": [], "entities": [{"text": "lexical entry assignment", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.6464007596174876}]}, {"text": "In particular, this paper empirically shows how much in-domain corpus is required for satisfiable performance.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce an HPSG parser and describe an existing method for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.7673157751560211}]}, {"text": "In Section 3, we show our methods of re-training a lexical disambiguation model and incorporating it into the original model.", "labels": [], "entities": []}, {"text": "In Section 4, we examine our method through experiments on the GENIA treebank.", "labels": [], "entities": [{"text": "GENIA treebank", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.9861467778682709}]}, {"text": "In Section 5, we examine the portability of our method through experiments on the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.9441061913967133}]}, {"text": "In Section 6, we showed several recent researches related to domain adaptation.) is a syntactic theory based on lexicalized grammar formalism.", "labels": [], "entities": [{"text": "domain adaptation.", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7271099835634232}]}, {"text": "In HPSG, a small number of grammar rules describe general construction rules, and a large number of lexical entries express word-specific characteristics.", "labels": [], "entities": []}, {"text": "The structures of sentences are explained using combinations of grammar rules and lexical entries.", "labels": [], "entities": []}, {"text": "shows an example of HPSG parsing of the sentence \"John has come.\"", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.80173459649086}]}, {"text": "First, as shown at the top of the figure, an HPSG parser assigns a lexical entry to each word in this sentence.", "labels": [], "entities": []}, {"text": "Next, a grammar rule is assigned and applied to lexical entries.", "labels": [], "entities": []}, {"text": "At the middle of this figure, the grammar rule is applied to the lexical entries for \"has\" and \"come.\"", "labels": [], "entities": []}, {"text": "We then obtain the structure represented at the bottom of the After that, the application of grammar rules is done iteratively, and then we can finally obtain the parse tree as is shown in.", "labels": [], "entities": []}, {"text": "In practice, since two or more parse candidates can be given for one sentence, a disambiguation model gives probabilities to these candidates, and a candidate given the highest probability is then chosen as a correct parse.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implemented the models shown in Section 3, and then evaluated the performance of them.", "labels": [], "entities": []}, {"text": "The original parser, Enju, was developed on Section 02-21 of the Penn Treebank (39,832 sentences) ().", "labels": [], "entities": [{"text": "Section 02-21 of the Penn Treebank (39,832 sentences)", "start_pos": 44, "end_pos": 97, "type": "DATASET", "confidence": 0.8459387123584747}]}, {"text": "For training those models, we used the GENIA treebank (, which consisted of 1,200 abstracts (10,848 sentences) extracted from MED-LINE.", "labels": [], "entities": [{"text": "GENIA treebank", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.9333878457546234}]}, {"text": "We divided it into three sets of 900, 150, and 150 abstracts, and 1,360 sentences), and these sets were used respectively as training, development, and final evaluation data.", "labels": [], "entities": []}, {"text": "The method of Gaussian MAP estimation) was used for smoothing.", "labels": [], "entities": [{"text": "MAP estimation", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.6854340136051178}, {"text": "smoothing", "start_pos": 52, "end_pos": 61, "type": "TASK", "confidence": 0.9714971780776978}]}, {"text": "The meta parameter \ud97b\udf59 of the Gaussian distribution was determined so as to maximize the accuracy on the development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9993841648101807}]}, {"text": "When we focus on the \"ALL\" domain, the approaches other than the baseline succeeded to give higher parsing accuracy than the baseline.", "labels": [], "entities": [{"text": "parsing", "start_pos": 99, "end_pos": 106, "type": "TASK", "confidence": 0.9409307837486267}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9201475381851196}]}, {"text": "This would show that these approaches were effective not only for the GENIA corpus but also for the Brown corpus.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.96562060713768}, {"text": "Brown corpus", "start_pos": 100, "end_pos": 112, "type": "DATASET", "confidence": 0.9741163551807404}]}, {"text": "The \"Mixture\" method gave the highest accuracy which was 3.41 point higher than the baseline.", "labels": [], "entities": [{"text": "Mixture", "start_pos": 5, "end_pos": 12, "type": "TASK", "confidence": 0.9155796766281128}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9996885061264038}]}, {"text": "The \"Our method + HMT05\" approach also gave the accuracy as high as the \"Mixture\" method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9996737241744995}, {"text": "Mixture\"", "start_pos": 73, "end_pos": 81, "type": "TASK", "confidence": 0.9062036573886871}]}, {"text": "In addition, as is the case with the GENIA corpus, the approach could be trained with much less time than the \"Mixture\" method.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.9496901631355286}, {"text": "Mixture\"", "start_pos": 111, "end_pos": 119, "type": "TASK", "confidence": 0.8957503736019135}]}, {"text": "Not only for these two  methods, the experimental results for the \"All\" domain showed the tendency similar to the GENIA corpus as a whole, except for the less improvement with the \"HMT05\" method.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 114, "end_pos": 126, "type": "DATASET", "confidence": 0.9545086622238159}]}, {"text": "When we focus on the individual domains, our method could successfully obtain higher parsing accuracy than the baseline for all the domains.", "labels": [], "entities": [{"text": "parsing", "start_pos": 85, "end_pos": 92, "type": "TASK", "confidence": 0.9433799386024475}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.938250720500946}]}, {"text": "Moreover, for the \"CP\" domain, our method could give the highest parsing accuracy among the methods.", "labels": [], "entities": [{"text": "parsing", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.9436048269271851}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9414337873458862}]}, {"text": "These results would support the portability of retraining the model for lexical entry assignment.", "labels": [], "entities": [{"text": "lexical entry assignment", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.6691354910532633}]}, {"text": "The \"Our method + HMT05\" approach, which gave the highest performance for the GENIA corpus, also gave accuracy improvement for the all domains while it did not give so much impact for the \"CL\" domain.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.9554893374443054}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9990164041519165}]}, {"text": "The \"Mixture\" approach, which utilized the same lexical entry assignment model, could obtain 0.94 point higher parsing accuracy than the \"Our method + HMT05\" approach., which shows the lexical coverage with each domains, does not seem to indicate the noteworthy difference in lexical entry coverage between the \"CL\" and the other domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.971889078617096}]}, {"text": "As mentioned in the error analysis in Section 4, the model of tree construction might affect the performance in someway.", "labels": [], "entities": [{"text": "tree construction", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7365663051605225}]}, {"text": "In our future work, we must clarify the mechanism of this result and would like to further improve the performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Parsing accuracy and time for various methods", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.7610439658164978}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9777801632881165}, {"text": "time", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9860591292381287}]}, {"text": " Table 3: Training cost of various methods", "labels": [], "entities": []}, {"text": " Table 5: Errors in various methods", "labels": [], "entities": [{"text": "Errors", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9767103791236877}]}, {"text": " Table 6: Types of disambiguation errors", "labels": [], "entities": []}, {"text": " Table 8: Parsing accuracy for the Brown corpus", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9875150918960571}, {"text": "Brown corpus", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.9644500315189362}]}, {"text": " Table 9: Consumed time for various methods for the Brown corpus", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9405820667743683}]}]}