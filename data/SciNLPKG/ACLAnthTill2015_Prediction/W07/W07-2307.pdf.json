{"title": [{"text": "Evaluating algorithms for the Generation of Referring Expressions using a balanced corpus", "labels": [], "entities": [{"text": "Generation of Referring Expressions", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.8992019295692444}]}], "abstractContent": [{"text": "Despite being the focus of intensive research, evaluation of algorithms that generate referring expressions is still in its infancy.", "labels": [], "entities": []}, {"text": "We describe a corpus-based evaluation methodology, applied to a number of classic algorithms in this area.", "labels": [], "entities": []}, {"text": "The methodology focuses on balance and semantic transparency to enable comparison of human and algorithmic output.", "labels": [], "entities": []}, {"text": "Although the Incremental Algorithm emerges as the best match, we found that its dependency on manually-set parameters makes its performance difficult to predict.", "labels": [], "entities": []}], "introductionContent": [{"text": "The current state of the art in the Generation of Referring Expressions (GRE) is dominated by versions of the Incremental Algorithm (IA) of.", "labels": [], "entities": []}, {"text": "Focusing on the generation of \"first-mention\" definite descriptions, Dale and Reiter compared the IA to a number of its predecessors, including a Full Brevity (FB) algorithm, which generates descriptions of minimal length, and a Greedy algorithm (GR), which approximates Full Brevity.", "labels": [], "entities": [{"text": "IA", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.8758119940757751}, {"text": "Full Brevity (FB", "start_pos": 146, "end_pos": 162, "type": "METRIC", "confidence": 0.7179113030433655}]}, {"text": "In doing so, the authors focused on Content Determination (CD, which is the purely semantic part of GRE), and on a description's ability to identify a referent fora hearer.", "labels": [], "entities": []}, {"text": "Under this problem definition, GRE algorithms take as input a Knowledge Base (KB), which lists domain entities and their properties (often represented as attribute-value pairs), together with a set of intended referents, R. The output of CD is a distinguishing description of R, that is, a logical form which distinguishes this set from its distractors.", "labels": [], "entities": [{"text": "GRE", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9619241952896118}]}, {"text": "Dale and Reiter argued that the IA was a superior model, and predicted that it would be the better match to human referential behaviour.", "labels": [], "entities": [{"text": "IA", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.775341272354126}]}, {"text": "1 This was due in part to the way the IA searches fora distinguishing description by performing gradient descent along a predetermined list of domain attributes, called the preference order, whose ranking reflects general or domain-specific preferences (see \u00a74.1).", "labels": [], "entities": []}, {"text": "The Incremental Algorithm has served as a starting point for later models), and has also served as a yardstick against which to compare other approaches).", "labels": [], "entities": []}, {"text": "Despite its influence, few empirical evaluations have focused on the IA.", "labels": [], "entities": [{"text": "IA", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.918775200843811}]}, {"text": "Evaluation is even more desirable given the dependency of the algorithm on a preference order, which can radically change its behaviour, so that in a domain with n attributes, there are in principle n!", "labels": [], "entities": []}, {"text": "This paper is concerned with applying a corpusbased methodology to evaluate content determination for GRE, comparing the three classic algorithms that formed the basis for contribution, adapted to also deal with pluralities and gradable properties.", "labels": [], "entities": [{"text": "content determination", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.6919400840997696}, {"text": "GRE", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.6769700050354004}]}], "datasetContent": [{"text": "One of the problems with evaluating GRE is that it interfaces with several other sub-tasks of NLG including, among others, realisation and discourse coherence (especially where anaphoric reference is concerned).", "labels": [], "entities": [{"text": "GRE", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.7960243225097656}]}, {"text": "On the other hand, a large amount of work in the area has focused on the semantic heart of the problem.", "labels": [], "entities": []}, {"text": "Given identification as the over-arching goal of such algorithms, a crucial question concerns the extent to which their choice of content from the available attributes fora referent matches that produced by a speaker in a comparable situation.", "labels": [], "entities": []}, {"text": "This is the main focus of this paper, whose evaluation methodology therefore targets content determination, abstracting away from issues of lexical choice and realisation.", "labels": [], "entities": [{"text": "content determination", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.8011763095855713}]}, {"text": "A corpusbased evaluation of a content determination GRE algorithm requires a resource that satisfies the following desiderata.", "labels": [], "entities": []}, {"text": "Semantic transparency: The human 'gold standard' descriptions in the corpus need to be paired with a domain representation so that, as far as possible, an algorithm is exposed to the same domain as an author.", "labels": [], "entities": [{"text": "Semantic transparency", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7624302208423615}]}, {"text": "To evaluate content determination, descriptions need to be semantically annotated, abstacting away from variations in syntax and lexicalisation.", "labels": [], "entities": [{"text": "content determination", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7359342873096466}]}, {"text": "For example, the right-facing sofa and the settee which is oriented towards the right are, from the point of view of a content determination procedure, semantically equivalent.", "labels": [], "entities": [{"text": "content determination procedure", "start_pos": 119, "end_pos": 150, "type": "TASK", "confidence": 0.796205202738444}]}, {"text": "Pragmatic transparency: Ideally, the communicative intention underlying corpus descriptions should match those for which an algorithm was designed.", "labels": [], "entities": []}, {"text": "If an algorithm is primarily aimed at identification, then human gold-standards should, as far as possible, be restricted to this intention.", "labels": [], "entities": []}, {"text": "Balance: To assess the extent to which an algorithm matches human performance, the corpus should contain an equal number of instances where each attribute is required.", "labels": [], "entities": [{"text": "Balance", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9205157160758972}]}, {"text": "Only in this way would the claim that algorithm X matches humans on content y% of the time be reliable 2 . These desiderata suggest that the way forward in evaluation in this area is to design controlled studies for corpus construction.", "labels": [], "entities": [{"text": "corpus construction", "start_pos": 216, "end_pos": 235, "type": "TASK", "confidence": 0.811301589012146}]}, {"text": "The rest of this paper describes the construction of such a corpus, and the results of an evaluation that addressed the differences between IA and its predecessors against human descriptions in domains of varying complexity, containing both singular and plural descriptions.", "labels": [], "entities": []}, {"text": "The study also aimed to contribute to a growing debate in the NLG community, on the evaluation of NLG systems, arguing in favour of the careful construction of balanced and transparent corpora to serve as resources for NLG.", "labels": [], "entities": []}, {"text": "The reliability of the annotation scheme was evaluated in a study involving two independent annotators (hereafter A and B), both postgraduate students with an interest in NLG, who used the same annotation manual (van der ).", "labels": [], "entities": [{"text": "NLG", "start_pos": 171, "end_pos": 174, "type": "DATASET", "confidence": 0.8204171061515808}]}, {"text": "They were given a stratified random sample of 270 descriptions, 2 from each Cardinality/Similarity condition, from each author in the corpus.", "labels": [], "entities": [{"text": "Similarity", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.8310707807540894}]}, {"text": "To estimate inter-annotator agreement, we compared their annotations against the consensus labelling made by the present authors, using aversion of the Dice coefficient.", "labels": [], "entities": []}, {"text": "Let D 1 and D 2 be two descriptions, and att(D) be the attributes in any description D.", "labels": [], "entities": []}, {"text": "The coefficient, which ranges between 0 (no agreement) and 1 (perfect agreement) is calculated as in (1).", "labels": [], "entities": [{"text": "perfect agreement)", "start_pos": 62, "end_pos": 80, "type": "METRIC", "confidence": 0.920707643032074}]}, {"text": "Because descriptions could contain more than one instance of an attribute (e.g.(b) contains two instances of SIZE), the sets of attributes for this comparison were represented as multisets.", "labels": [], "entities": []}, {"text": "In the present context, Dice is more appropriate than agreement measures (such as the \u03ba statistic) which rely on predefined categories in which discrete events can be classified.", "labels": [], "entities": [{"text": "Dice", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.9334354400634766}]}, {"text": "The 'events' in the corpus are NL expressions, each of which is 'classified' in several ways (depending on how many attributes a description expresses), and it was up to an annotator's judgment, given the instructions, to select those segments and mark them up.", "labels": [], "entities": []}, {"text": "Both annotators showed a high mean agreement with the authors, as indicated by their mean and modal (most frequent) scores (A:: mean = 0.93, mode = 1 (74.4%); B: mean = 0.92; mode = 1 (73%)).", "labels": [], "entities": [{"text": "mode", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9663315415382385}]}, {"text": "They also evinced substantial agreement among themselves (mean = 0.89, mode = 1 (71.1%)).", "labels": [], "entities": [{"text": "mean", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9913524389266968}, {"text": "mode", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9979192614555359}]}, {"text": "These results suggest that the annotation scheme used is replicable to a high degree, and that independent annotators are likely to produce very similar semantic markup.", "labels": [], "entities": []}, {"text": "In the evaluation study reported below, we use the same measure to compare algorithm and human output, because an optimally informative comparison should take into account the number of attributes that an algorithm omits in relation to the human gold standard, and the number of attributes that it includes.", "labels": [], "entities": []}, {"text": "We also evaluated the validity of the experimental setup.", "labels": [], "entities": [{"text": "validity", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.967674195766449}]}, {"text": "Since communicating with a machine may have biased participants, they were asked, during a debriefing phase, to assess the performance of their virtual interlocutor, by indicating agreement to the statement The system performed well on this task.", "labels": [], "entities": []}, {"text": "Of the 5 response categories, ranging from 1 strongly disagree to 5 (strongly agree), 34 individuals selected agree or strongly agree while no one selected strongly disagree.", "labels": [], "entities": []}, {"text": "The mean score was 3.9.", "labels": [], "entities": [{"text": "mean score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9688840210437775}]}], "tableCaptions": [{"text": " Table 2: Comparison to the Random Baseline (  *  p < .05)", "labels": [], "entities": []}, {"text": " Table 3: Effect of Cardinality/Similarity  *  p < .001", "labels": [], "entities": [{"text": "Similarity", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.695052444934845}]}]}