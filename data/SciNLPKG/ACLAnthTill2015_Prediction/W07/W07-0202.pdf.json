{"title": [{"text": "Multi-level Association Graphs - A New Graph-Based Model for Information Retrieval", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.769417941570282}]}], "abstractContent": [{"text": "This paper introduces multi-level association graphs (MLAGs), anew graph-based framework for information retrieval (IR).", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.8566664695739746}]}, {"text": "The goal of that framework is twofold: First, it is meant to be a meta model of IR, i.e. it subsumes various IR models under one common representation.", "labels": [], "entities": []}, {"text": "Second , it allows to model different forms of search, such as feedback, associative retrieval and browsing at the same time.", "labels": [], "entities": []}, {"text": "It is shown how the new integrated model gives insights and stimulates new ideas for IR algorithms.", "labels": [], "entities": [{"text": "IR algorithms", "start_pos": 85, "end_pos": 98, "type": "TASK", "confidence": 0.9397121667861938}]}, {"text": "One of these new ideas is presented and evaluated, yielding promising experimental results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Developing formal models for information retrieval has along history.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7873688042163849}]}, {"text": "A model of information retrieval \"predicts and explains what a user will find relevant given the user query\".", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.7071532160043716}]}, {"text": "Most IR models are firmly grounded in mathematics and thus provide a formalisation of ideas that facilitates discussion and makes sure that the ideas can be implemented.", "labels": [], "entities": [{"text": "IR", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.9744208455085754}]}, {"text": "More specifically, most IR models provide a so-called retrieval function f (q, d) , which returns -for given representations of a document d and of a user information need q -a so-called retrieval status value by which documents can be ranked according to their presumed relevance w.r.t. to the query q.", "labels": [], "entities": []}, {"text": "In order to understand the commonalities and differences among IR models, this paper introduces the notion of meta modeling.", "labels": [], "entities": [{"text": "IR", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9574891924858093}]}, {"text": "Since the word \"meta model\" is perhaps not standard terminology in IR, it should be explained what is meant by it: a meta model is a model or framework that subsumes other IR models, such that they are derived by specifying certain parameters of the meta model.", "labels": [], "entities": []}, {"text": "In terms of IR theory, such a framework conveys what is common to all IR models by subsuming them.", "labels": [], "entities": [{"text": "IR theory", "start_pos": 12, "end_pos": 21, "type": "TASK", "confidence": 0.972878485918045}]}, {"text": "At the same time, the differences between models are highlighted in a conceptually simple way by the different values of parameters that have to beset in order to arrive at this subsumption.", "labels": [], "entities": []}, {"text": "It will be shown that a graph-based representation of IR data is very well suited to this problem.", "labels": [], "entities": [{"text": "IR", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9340258836746216}]}, {"text": "IR models concentrate on the matching process, i.e. on measuring the degree of overlap between a query q and a document representation d.", "labels": [], "entities": [{"text": "IR", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9508036971092224}]}, {"text": "On the other hand, there are the problems of finding suitable representations for documents (indexing) and for users' information needs (query formulation).", "labels": [], "entities": [{"text": "query formulation", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.7422999739646912}]}, {"text": "Since users are often notable to adequately state their information need, some interactive and associative procedures have been developed by IR researchers that help to overcome this problem: \u2022 Associative retrieval, i.e. retrieving information which is associated to objects known or suspected to be relevant to the user -e.g. query terms or documents that have been retrieved already.", "labels": [], "entities": [{"text": "Associative retrieval", "start_pos": 194, "end_pos": 215, "type": "TASK", "confidence": 0.8696486949920654}, {"text": "retrieving information which is associated to objects known or suspected to be relevant to the user -e.g. query terms or documents that have been retrieved already", "start_pos": 222, "end_pos": 385, "type": "Description", "confidence": 0.7337903501810851}]}, {"text": "\u2022 Feedback, another method for boosting recall, either relies on relevance information given by the user (relevance feedback) or assumes top-ranked documents to be relevant (pseudo feedback) and learns better query formulations from this information.", "labels": [], "entities": [{"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9886606931686401}]}, {"text": "\u2022 Browsing, i.e. exploring a document collection interactively by following links between objects such as documents, terms or concepts.", "labels": [], "entities": []}, {"text": "Again, it will be shown that -using a graph-based representation -these forms of search can be subsumed easily.", "labels": [], "entities": []}], "datasetContent": [{"text": "Combining them with the idea of \"absence penalties\" works as indicated above, i.e. weights are accumulated for each document using the tf.idf -like retrieval functions.", "labels": [], "entities": []}, {"text": "Then, from each score, the contributions that one occurrence of each missing term would have earned is subtracted.", "labels": [], "entities": []}, {"text": "More precisely, what is subtracted consists of the usual tf.idf weight for the missing term, where tf = 1 is substituted in the tf part of the formula.", "labels": [], "entities": []}, {"text": "Experiments were run with queries from TREC-7 and TREC-8.", "labels": [], "entities": [{"text": "TREC-7", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.9474470615386963}, {"text": "TREC-8", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.9584819078445435}]}, {"text": "In order to study the effect of query length, very short queries (using only the title field of TREC queries), medium ones (using title and description fields) and long ones (using all fields) were used.", "labels": [], "entities": []}, {"text": "shows an example TREC query.", "labels": [], "entities": []}, {"text": "< top> < num> Number: 441 < title> Lyme disease < desc> Description: How do you prevent and treat Lyme disease?", "labels": [], "entities": []}, {"text": "< narr> Narrative: Documents that discuss current prevention and treatment techniques for Lyme disease are relevant [...]", "labels": [], "entities": [{"text": "Lyme disease", "start_pos": 90, "end_pos": 102, "type": "TASK", "confidence": 0.6400003731250763}]}, {"text": "< /top>: A sample TREC query shows that both weighting schemes can be significantly improved by using penalties, especially for short queries, reaching and sometimes surpassing the performance of retrieval with language models.", "labels": [], "entities": []}, {"text": "This holds even when the parameter \u03b1 is not tuned and confirms that interesting insights are gained from a common representation of IR models in a graph-based environment.", "labels": [], "entities": []}, {"text": "3: Mean average precision of BM25 and Lnu.ltn and their corresponding penalty schemes (+ P) for TREC-7 and TREC-8.", "labels": [], "entities": [{"text": "Mean average precision", "start_pos": 3, "end_pos": 25, "type": "METRIC", "confidence": 0.8760325113932291}, {"text": "BM25", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.7425994873046875}, {"text": "TREC-8", "start_pos": 107, "end_pos": 113, "type": "DATASET", "confidence": 0.7533645629882812}]}, {"text": "Asterisks indicate statistically significant deviations (using a paired Wilcoxon test on a 95% confidence level) from each baseline, whereas the best run for each query length is marked with bold font.", "labels": [], "entities": []}, {"text": "Performance of language models (LM) is given for reference, where the value of the smoothing parameter \u00b5 was set to the average document length.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Mean average precision of BM25 and Lnu.ltn and their corresponding penalty schemes (+ P) for  TREC-7 and TREC-8. Asterisks indicate statistically significant deviations (using a paired Wilcoxon test  on a 95% confidence level) from each baseline, whereas the best run for each query length is marked with  bold font. Performance of language models (LM) is given for reference, where the value of the smoothing  parameter \u00b5 was set to the average document length.", "labels": [], "entities": [{"text": "Mean average precision", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8750789960225424}, {"text": "BM25", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8142110705375671}, {"text": "TREC-7", "start_pos": 104, "end_pos": 110, "type": "DATASET", "confidence": 0.6995331645011902}, {"text": "TREC-8", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.8445246815681458}]}]}