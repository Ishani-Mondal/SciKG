{"title": [], "abstractContent": [{"text": "We investigate two methods for enhancing variation in the output of a stochastic surface realiser: choosing from among the highest-scoring realisa-tion candidates instead of taking the single highest-scoring result (\u03b5-best sampling), and penalising the words from earlier sentences in a discourse when generating later ones (anti-repetition scoring).", "labels": [], "entities": []}, {"text": "Ina human evaluation study, subjects were asked to compare texts generated with and without the variation enhancements.", "labels": [], "entities": []}, {"text": "Strikingly, subjects judged the texts generated using these two methods to be better written and less repetitive than the texts generated with optimal n-gram scoring; at the same time, no significant difference in understandability was found between the two versions.", "labels": [], "entities": []}, {"text": "In analysing the two methods, we show that the simpler \u03b5-best sampling method is considerably more prone to introducing dispreferred variants into the output, indicating that best results can be obtained using anti-repetition scoring with strict or no \u03b5-best sampling.", "labels": [], "entities": []}], "introductionContent": [{"text": "A classic rule of writing, found in many style guides, is to avoid repetition in order to keep text interesting and make it more lively.", "labels": [], "entities": []}, {"text": "When designing systems to automatically generate text, it is often taken for granted that this stylistic goal should be met as well: for example, van incorporated random choice into a language generation system \"to maximise the variety of sentences produced\" (emphasis original).", "labels": [], "entities": []}, {"text": "Repetitiveness may take several forms: using the same words or syntactic structures, repeatedly giving the same facts, or even repeating entire turns (for example, error-handling turns in dialogue systems).", "labels": [], "entities": [{"text": "Repetitiveness", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9739516973495483}]}, {"text": "At the level of word choice and phrasing, recent advances in stochastic text generation have made it possible to implement corpus-based approaches to varying output.", "labels": [], "entities": [{"text": "word choice", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.7105167359113693}, {"text": "stochastic text generation", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.6863264441490173}]}, {"text": "However, as note, there is an inherent conflict between producing output that is optimally similar to the corpus and incorporating variability: varying output requires choosing less frequent options, which inevitably reduces scores on corpus similarity measures.", "labels": [], "entities": []}, {"text": "To the extent that corpus-based measures (such as n-gram scores) are used to avoid overgeneration and select preferred paraphrases, it is not obvious how to enhance variation without reducing output quality.", "labels": [], "entities": []}, {"text": "With this question in mind, we investigate in this paper the impact of two different methods for enhancing variation in the output generated by the COMIC multimodal dialogue system.", "labels": [], "entities": [{"text": "COMIC multimodal dialogue", "start_pos": 148, "end_pos": 173, "type": "TASK", "confidence": 0.592471698919932}]}, {"text": "1 Both methods take advantage of the periphrastic ability of the OpenCCG surface realiser).", "labels": [], "entities": [{"text": "OpenCCG surface realiser", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.7642364501953125}]}, {"text": "In the usual OpenCCG realisation process, when a logical form is transformed into output text, n-gram models are used to steer the realiser towards the single highest-scoring option for the sentence.", "labels": [], "entities": []}, {"text": "This process tends to select the same syntactic structure for every sentence describing the same feature: for example, in the COMIC domain (describing and comparing bathroom tiles), the structure The colours are would be used every time the colours of a tile design are to be presented, even though alternative paraphrases are available.", "labels": [], "entities": []}, {"text": "The first (and simplest) means of avoiding such repetition using OpenCCG, \u03b5-best sampling, is to perform n-best realisation and then to select randomly from among those options whose score is within a threshold \u03b5 of the top score.", "labels": [], "entities": [{"text": "OpenCCG", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9535093307495117}]}, {"text": "The second means of adding variation, anti-repetition scoring, is to store the words from recently generated sentences and to penalise a proposed realisation based on the number of words that it shares with these sentences.", "labels": [], "entities": []}, {"text": "OpenCCG provides a built-in facility for implementing such anti-repetition scorers and integrating them with the normal n-gram-based scoring algorithm.", "labels": [], "entities": [{"text": "OpenCCG", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9605527520179749}]}, {"text": "To verify that it can be beneficial fora natural language generation system to strive to avoid repetition, we first conducted a human evaluation study in which subjects were asked to compare texts generated with and without the two variation-enhancing methods.", "labels": [], "entities": []}, {"text": "Strikingly, subjects judged the versions generated using \u03b5-best sampling and anti-repetition scoring to be both better written and less repetitive than the versions generated with optimal n-gram scoring.", "labels": [], "entities": []}, {"text": "To our knowledge, this study is the first to show a clear benefit for enhancing variation; while other recent studies (e.g.,) have shown that automatic evaluation metrics do not always correlate well with human judgments of high quality generated texts with periphrastic variations, these studies examined sentences out of context, and thus could not take into account the benefit of avoiding repetition as a discourse progresses.", "labels": [], "entities": []}, {"text": "Following the human evaluation study, we varied the main parameters used in \u03b5-best sampling and anti-repetition scoring and analysed the resulting impact on the amount of periphrastic variation and the number of dispreferred paraphrases in the generated outputs.", "labels": [], "entities": []}, {"text": "The analysis revealed that the simpler \u03b5-best sampling method is considerably more prone to introducing dispreferred variants into the output.", "labels": [], "entities": []}, {"text": "It also showed that essentially the same amount of variation can be achieved using anti-repetition scoring on its own, or just with strict \u03b5-best sampling, as using both methods together.", "labels": [], "entities": []}, {"text": "This suggests away of resolving the conflict between enhancing variation and maximising corpus similarity.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe previous work on generating paraphrases.", "labels": [], "entities": []}, {"text": "In Section 3, we next summarise the realisation process of the OpenCCG surface realiser, concentrating on its use of n-gram models in the generation process and its support for disjunctive logical forms.", "labels": [], "entities": [{"text": "OpenCCG surface realiser", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.7218594352404276}]}, {"text": "In Section 4, we then give details of how the two anti-repetition methods were integrated into this realisation algorithm.", "labels": [], "entities": []}, {"text": "Section 5 next presents the result of the human evaluation study.", "labels": [], "entities": []}, {"text": "In Section 6, we then explore the impact of the two anti-repetition methods on the variability and quality of the generated text, using a range of parameter settings.", "labels": [], "entities": []}, {"text": "In Section 7, we discuss the results of both studies and compare them with related work.", "labels": [], "entities": []}, {"text": "Finally, in Section 8, we give some conclusions and outline possible extensions to this work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}