{"title": [{"text": "Rule-based Translation With Statistical Phrase-based Post-editing", "labels": [], "entities": [{"text": "Rule-based Translation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5746508687734604}]}], "abstractContent": [{"text": "This article describes a machine translation system based on an automatic post-editing strategy: initially translate the input text into the target-language using a rule-based MT system, then automatically post-edit the output using a statistical phrase-based system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7618619501590729}]}, {"text": "An implementation of this approach based on the SYSTRAN and PORTAGE MT systems was used in the shared task of the Second Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 133, "end_pos": 164, "type": "TASK", "confidence": 0.7224297523498535}]}, {"text": "Experimental results on the test data of the previous campaign are presented.", "labels": [], "entities": []}], "introductionContent": [{"text": "have recently shown how a statistical phrase-based machine translation system can be used as an automatic post-editing (APE) layer, on top of a rule-based machine translation system.", "labels": [], "entities": [{"text": "statistical phrase-based machine translation", "start_pos": 26, "end_pos": 70, "type": "TASK", "confidence": 0.5689783170819283}, {"text": "rule-based machine translation", "start_pos": 144, "end_pos": 174, "type": "TASK", "confidence": 0.7115999460220337}]}, {"text": "The motivation for their work is the repetitive nature of the errors typically made by rule-based systems.", "labels": [], "entities": []}, {"text": "Given appropriate training material, a statistical MT system can be trained to correct these systematic errors, therefore reducing the post-editing effort.", "labels": [], "entities": [{"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9506978392601013}]}, {"text": "The statistical system views the output of the rule-based system as the source language, and reference human translations as the target language.", "labels": [], "entities": []}, {"text": "Because the training material for the APE layer will typically be domain-specific, this process can be viewed as away of automatically adapting a rule-based system to a specific application domain.", "labels": [], "entities": [{"text": "APE layer", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.890125572681427}]}, {"text": "This approach has been shown experimentally to produce large improvements in performance not only over the baseline rule-based system that it corrects, but also over a similar statistical phrase-based MT system used in standalone mode, i.e. translating the \"real\" source text directly: Simard et al. report a reduction in post-editing effort of up to a third when compared to the input rule-based translation, and as much as 5 BLEU points improvement over the direct SMT approach.", "labels": [], "entities": [{"text": "MT", "start_pos": 201, "end_pos": 203, "type": "TASK", "confidence": 0.9155510067939758}, {"text": "BLEU", "start_pos": 427, "end_pos": 431, "type": "METRIC", "confidence": 0.9994565844535828}, {"text": "SMT", "start_pos": 467, "end_pos": 470, "type": "TASK", "confidence": 0.9589648246765137}]}, {"text": "These impressive results, however, were obtained in a very specific and somewhat unusual context: the training and test corpora were extracted from a collection of manually post-edited machine translations.", "labels": [], "entities": []}, {"text": "The two corpora (one English-to-French, one French-to-English) each contained three parallel \"views\" of the same data: 1) the source language text, 2) a machine translation of that text into the target language, as produced by a commercial rulebased MT system, and 3) the final target-language version of the text, produced by manually postediting the machine translation.", "labels": [], "entities": []}, {"text": "Furthermore, the corpus was very small, at least by SMT standards: 500K words of source-language data in the Frenchto-English direction, 350K words in the English-toFrench.", "labels": [], "entities": [{"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9401842951774597}]}, {"text": "Because of this, the authors were left with two important questions: 1) how would the results scale up to much larger quantities of training data? and 2) are the results related to the dependent nature of the translations, i.e. is the automatic post-editing approach still effective when the machine and human translations are produced independently of one another?", "labels": [], "entities": []}, {"text": "With these two questions in mind, we participated in the shared task of the Second Workshop on Statistical Machine Translation with an automatic post-editing strategy: initially translate the input text into the target-language using a rule-based system, namely SYSTRAN, and automatically postedit the output using a statistical phrase-based system, namely PORTAGE.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 95, "end_pos": 126, "type": "TASK", "confidence": 0.6886228819688162}]}, {"text": "We describe our system in more detail in Section 2, and present some experimental results in Section 3.", "labels": [], "entities": []}], "datasetContent": [{"text": "We computed BLEU scores for all four systems on the 2006 test data (test2006 for the Europarl domain and nc-devtest2007 for the News Commentary).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9989223480224609}, {"text": "2006 test data", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.8829394181569418}, {"text": "Europarl domain", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.9869536459445953}]}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "As points of comparison, we also give the scores obtained by the SYSTRAN systems on their own (i.e. without a post-editing layer), and by the PORTAGE MT systems on their own (i.e. translating directly source into target).", "labels": [], "entities": [{"text": "PORTAGE MT", "start_pos": 142, "end_pos": 152, "type": "TASK", "confidence": 0.4463895857334137}]}, {"text": "The first observation is that, as was the casein the Simard et al. study, post-editing (SYS-TRAN+PORTAGE lines) very significantly increases the BLEU scores of the rule-based system (SYSTRAN lines).", "labels": [], "entities": [{"text": "PORTAGE", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9510194659233093}, {"text": "BLEU scores", "start_pos": 145, "end_pos": 156, "type": "METRIC", "confidence": 0.9755052030086517}]}, {"text": "This increase is more spectacular in the Europarl domain and when translating into English, but it is visible for all four systems.", "labels": [], "entities": [{"text": "Europarl domain", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.9840888977050781}]}, {"text": "For the News Commentary domain, the APE strategy (SYSTRAN+PORTAGE lines) clearly outperforms the direct SMT strategy (PORTAGE lines): translating into English, the gain exceeds 1.5 BLEU points, while for French, it is close to 3 BLEU points.", "labels": [], "entities": [{"text": "APE", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9380173087120056}, {"text": "SMT", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9797662496566772}, {"text": "BLEU", "start_pos": 181, "end_pos": 185, "type": "METRIC", "confidence": 0.9984990358352661}, {"text": "BLEU", "start_pos": 229, "end_pos": 233, "type": "METRIC", "confidence": 0.9988629817962646}]}, {"text": "In contrast, for the Europarl domain, both approaches display similar performances.", "labels": [], "entities": [{"text": "Europarl domain", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.9846099317073822}]}, {"text": "Let us recall that the News Commentary corpus contains less than 50K sentence pairs, totalling a little over one million words in each language.", "labels": [], "entities": [{"text": "News Commentary corpus", "start_pos": 23, "end_pos": 45, "type": "DATASET", "confidence": 0.8099811871846517}]}, {"text": "With close to 1.3 million sentence pairs, the Europarl corpus is almost 30 times larger.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.9934625029563904}]}, {"text": "Our results therefore appear to confirm one of the conjectures of the Simard et al. study: that APE is better suited for domains with limited quantities of available training data.", "labels": [], "entities": [{"text": "APE", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.5255387425422668}]}, {"text": "To better understand this behavior, we trained series of APE and SMT systems on the Europarl data, using increasing amounts of training data.", "labels": [], "entities": [{"text": "APE", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.6696983575820923}, {"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9006291031837463}, {"text": "Europarl data", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.9970047771930695}]}, {"text": "The resulting learning curves are presented in.", "labels": [], "entities": []}, {"text": "As observed in the Simard et al. study, while both the SMT and APE systems improve quite steadily with more data (note the logarithmic scale), SMT appears to improve more rapidly than APE.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9814341068267822}, {"text": "SMT", "start_pos": 143, "end_pos": 146, "type": "TASK", "confidence": 0.9881958961486816}, {"text": "APE", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.523868978023529}]}, {"text": "However, there doesn't seem to be a clear \"crossover\" point, as initially conjectured by Simard et al.", "labels": [], "entities": []}, {"text": "Instead, SMT eventually catches up with APE (anywhere between 100K and 1M sentence pairs), beyond which point both approaches appear to be more or less equivalent.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9871746301651001}, {"text": "APE", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9373173713684082}]}, {"text": "Again, one impressive feature of the APE strategy is how little data is actually required to improve upon the rule-based system upon which it is built: around 5000 sentence pairs for English-to-French, and 2000 for French-to-English.", "labels": [], "entities": [{"text": "APE", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.8631011843681335}]}], "tableCaptions": [{"text": " Table 1: System performances on WMT-06 test. All  figures are single-reference BLEU scores, computed  on truecased, detokenized translations.", "labels": [], "entities": [{"text": "WMT-06 test", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.9427296221256256}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9862516522407532}]}]}