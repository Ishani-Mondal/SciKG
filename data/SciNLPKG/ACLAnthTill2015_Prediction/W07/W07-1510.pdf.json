{"title": [{"text": "Querying multimodal annotation: A concordancer for GeM", "labels": [], "entities": [{"text": "GeM", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.5446362495422363}]}], "abstractContent": [{"text": "This paper presents a multimodal corpus of comparable pack messages and the concor-dancer that has been built to query it.", "labels": [], "entities": []}, {"text": "The design of the corpus and its annotation is introduced.", "labels": [], "entities": []}, {"text": "This is followed by a description of the concordancer's interface, implementation and concordance display.", "labels": [], "entities": []}, {"text": "Finally, some ideas for future work are outlined.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper introduces a multimodal concordancer 1 that has been developed to investigate variation between messages on fast-moving consumer goods packaging from China, Taiwan and the UK.", "labels": [], "entities": []}, {"text": "The need to develop such a concordancer arises from the fact that these pack messages are themselves multimodal.", "labels": [], "entities": []}, {"text": "While they communicate through what calls the visual channel, messages are realized using a combination of three modes (verbal, schematic, pictorial).", "labels": [], "entities": []}, {"text": "Moreover, the verbal components of visual messages are modulated and segmented through typography.", "labels": [], "entities": []}, {"text": "It is assumed that this multimodality will have complex implications for cross-linguistic variation within the genre of pack messages.", "labels": [], "entities": []}, {"text": "The specific nature of these implications is not yet known, but variation in the construal of textual meaning and cohesion would seem to offer a good starting point for investigation.", "labels": [], "entities": []}, {"text": "However, using purely linguistic annotation and a monomodal concordancer to analyze such material could reveal only part of the picture.", "labels": [], "entities": []}, {"text": "1 http://corpus.leeds.ac.uk/ \u223c martin/ An existing annotation scheme, developed by the Genre and Multimodality (GeM) project 2 , is wellsuited to my needs.", "labels": [], "entities": []}, {"text": "In addition to information about their verbal and visual realization, the scheme provides a mechanism for encoding the rhetorical relations between message components.", "labels": [], "entities": []}, {"text": "However, existing tools for multimodal analysis do not support simultaneous investigation of verbal, visual and rhetorical phenomena.", "labels": [], "entities": [{"text": "multimodal analysis", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7492799758911133}]}, {"text": "While Baldry's (2004) multimodal concordancer supports multilayered analysis of video data, his approach does not support the segmentation of still visual layouts, let alone consideration of specific typographical realizations.", "labels": [], "entities": []}, {"text": "From an altogether different perspective, the database developed as part of the Typographic Design for Children 3 project does allow access to such typographic information, but does not relate this directly to the linguistic realization of messages.", "labels": [], "entities": []}, {"text": "Their multimodal realization makes pack messages a rich testing ground for the new concordancer and Chinese and English offer great potential for looking at multimodal cross-linguistic variation.", "labels": [], "entities": []}, {"text": "Typographic resources are constrained by the writing system of a given language: Chinese offers variety in reading directions and a consistent footprint for each character; English offers a range of case distinctions and a predictable reading direction.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}