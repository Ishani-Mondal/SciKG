{"title": [{"text": "Adding semantic role annotation to a corpus of written Dutch", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an approach to automatic semantic role labeling (SRL) carried out in the context of the Dutch Language Corpus Initiative (D-Coi) project.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.7954531411329905}, {"text": "Dutch Language Corpus Initiative (D-Coi) project", "start_pos": 99, "end_pos": 147, "type": "DATASET", "confidence": 0.8039580956101418}]}, {"text": "Adapting earlier research which has mainly focused on English to the Dutch situation poses an interesting challenge especially because there is no semantically annotated Dutch corpus available that can be used as training data.", "labels": [], "entities": [{"text": "Adapting", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9813171625137329}]}, {"text": "Our automatic SRL approach consists of three steps: bootstrapping from a syntactically annotated corpus by means of a rule-based tagger developed for this purpose, manual correction on the basis of the Prop-Bank guidelines which have been adapted to Dutch and training a machine learning system on the manually corrected data.", "labels": [], "entities": [{"text": "SRL", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9590457081794739}]}], "introductionContent": [{"text": "The creation of semantically annotated corpora has lagged dramatically behind.", "labels": [], "entities": []}, {"text": "As a result, the need for such resources has now become urgent.", "labels": [], "entities": []}, {"text": "Several initiatives have been launched at the international level in the last years, however, they have focused almost entirely on English and not much attention has been dedicated to the creation of semantically annotated Dutch corpora.", "labels": [], "entities": []}, {"text": "Within the Dutch Language Corpus Initiative (DCoi) , a recently completed Dutch project, guidelines have been developed for the annotation of a Dutch written corpus.", "labels": [], "entities": [{"text": "Dutch Language Corpus Initiative (DCoi)", "start_pos": 11, "end_pos": 50, "type": "TASK", "confidence": 0.5950763395854405}]}, {"text": "In particular, a pilot corpus has been compiled, parts of which have been enriched with (verified) linguistic annotations.", "labels": [], "entities": []}, {"text": "One of the innovative aspects of the D-Coi project with respect to previous initiatives, such as the Spoken Dutch Corpus (CGN -Corpus Gesproken Nederlands), was the development of a protocol fora semantic annotation layer.", "labels": [], "entities": [{"text": "Spoken Dutch Corpus (CGN -Corpus Gesproken Nederlands)", "start_pos": 101, "end_pos": 155, "type": "DATASET", "confidence": 0.8455791026353836}]}, {"text": "In particular, two types of semantic annotation have been addressed, that is semantic role assignment and temporal and spatial semantics).", "labels": [], "entities": [{"text": "semantic role assignment", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.6630004644393921}]}, {"text": "The reason for this choice lies in the fact that semantic role assignment (i.e. the semantic relationships identified between items in the text such as the agents or patients of particular actions), is one of the most attested and feasible types of semantic annotation within corpora.", "labels": [], "entities": [{"text": "semantic role assignment", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.640857050816218}]}, {"text": "On the other hand, temporal and spatial annotation was chosen because there is a clear need for such a layer of annotation in applications like information retrieval or question answering.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 144, "end_pos": 165, "type": "TASK", "confidence": 0.773111879825592}, {"text": "question answering", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.8884684443473816}]}, {"text": "The focus of this paper is on semantic role annotation.", "labels": [], "entities": [{"text": "semantic role annotation", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.8095291455586752}]}, {"text": "We analyze the choices we have made in selecting an appropriate annotation protocol by taking into consideration existing initiatives such as FrameNet () and PropBank () (cf. also the Chinese and Arabic PropBank).", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 142, "end_pos": 150, "type": "DATASET", "confidence": 0.8681747317314148}]}, {"text": "We motivate our choice for the PropBank annotation scheme on the basis of the promising results with respect to automatic semantic role labeling (SRL) which have been obtained for English.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 122, "end_pos": 150, "type": "TASK", "confidence": 0.7685476442178091}]}, {"text": "Furthermore, we discuss how the SRL research could be adapted to the Dutch situation given that no semantically annotated corpus was available that could be used as training data.", "labels": [], "entities": [{"text": "SRL", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.956748366355896}]}], "datasetContent": [{"text": "The best performing system that participated in CoNLL 2005 reached an F 1 of 80.", "labels": [], "entities": [{"text": "CoNLL 2005", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.7868956029415131}, {"text": "F 1", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.981472373008728}]}, {"text": "There were seven systems with an F 1 performance in the 75-78 range, seven more with performances in the 70-75 range and five with a performance between 65 and 70.", "labels": [], "entities": [{"text": "F 1", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9426492750644684}]}, {"text": "A system that did not participate in the CoNLL task, but still provides interesting material for comparison since it is also based on dependency structures, is the system by.", "labels": [], "entities": []}, {"text": "This system scored 85,6% precision, 83,6% recall and 84,6 F 1 on the CoNLL data set, which is even higher than the best results published so far on the PropBank data sets): 84% precision, 75% recall and 79 F 1 . These results support our claim that dependency structures can be very useful in the SRL task.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.999632716178894}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9993104934692383}, {"text": "F 1", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9469542801380157}, {"text": "CoNLL data set", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.9738006790479025}, {"text": "PropBank data sets", "start_pos": 152, "end_pos": 170, "type": "DATASET", "confidence": 0.9803990920384725}, {"text": "precision", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.9986054301261902}, {"text": "recall", "start_pos": 192, "end_pos": 198, "type": "METRIC", "confidence": 0.998656153678894}, {"text": "F 1", "start_pos": 206, "end_pos": 209, "type": "METRIC", "confidence": 0.9750357270240784}, {"text": "SRL task", "start_pos": 297, "end_pos": 305, "type": "TASK", "confidence": 0.943309098482132}]}, {"text": "As one would expect, the overall precision and recall scores of the classifier are higher than those of the XARA rule-based system.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9995673298835754}, {"text": "recall scores", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.9788760840892792}]}, {"text": "Yet, we expected a better performance of the classifier on the lower numbered arguments (ARG0 and ARG1).", "labels": [], "entities": [{"text": "ARG0", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.9256553649902344}, {"text": "ARG1", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.8612518906593323}]}, {"text": "Our hypothesis is that performance on these arguments can be improved by by adding semantic features to our feature set.", "labels": [], "entities": []}, {"text": "Examples of such features are the subcategorization frame of the predicate and the semantic category (e.g. WordNet synset) of the candidate argument.", "labels": [], "entities": []}, {"text": "We expect that such semantic features will improve the performance of the classifier for certain types of verbs and arguments, especially the lower numbered arguments ARG0 and ARG1 and temporal and spatial modifiers.", "labels": [], "entities": [{"text": "ARG0", "start_pos": 167, "end_pos": 171, "type": "DATASET", "confidence": 0.8144561648368835}, {"text": "ARG1", "start_pos": 176, "end_pos": 180, "type": "DATASET", "confidence": 0.789365291595459}]}, {"text": "For example, the Dutch preposition over can either head a phrase indicating a location or a time-span.", "labels": [], "entities": []}, {"text": "The semantic category of the neighboring noun phrase might be helpful in such cases to choose the right PropBank label.", "labels": [], "entities": []}, {"text": "Thanks to new lexical resources, such as Cornetto), and clustering techniques based on dependency structures (van de Cruys, 2005), we might be able to add lexical semantic information about noun phrases in future research.", "labels": [], "entities": []}, {"text": "Performance of the classifier can also be improved by automatically optimizing the feature set.", "labels": [], "entities": []}, {"text": "The optimal set of features fora classifier can be found by employing bi-directional hill climbing (van den ).", "labels": [], "entities": []}, {"text": "There is a wrapper script (Paramsearch) available that can be used with TiMBL and several other learning systems that implements this approach . In addition, iterative deepening (ID) can be used as a heuristic way of finding the optimal algorithm parameters for TiMBL.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of SRL with XARA  Label  Precision  Recall F \u03b2=1  Overall  65,11% 45,83% 53,80  Arg0  98.97% 94.95% 96.92  Arg1  70.08% 64.83% 67.35  Arg2  47.41% 36.07% 40.97  Arg3  13.89%  6.85%  9.17  Arg4  1.56%  1.35%  1.45  ArgM-LOC  83.49% 13.75% 23.61  ArgM-NEG  72.79% 58.79% 65.05  ArgM-PNC  91.94% 39.31% 55.07  ArgM-PRD  63.64% 26.25% 37.17  ArgM-REC  85.19% 69.70% 76.67", "labels": [], "entities": [{"text": "SRL", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9735809564590454}, {"text": "XARA  Label  Precision  Recall F \u03b2", "start_pos": 30, "end_pos": 64, "type": "METRIC", "confidence": 0.9159990151723226}, {"text": "Arg0", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9921665191650391}, {"text": "Arg4", "start_pos": 206, "end_pos": 210, "type": "METRIC", "confidence": 0.9502257108688354}, {"text": "ArgM-LOC", "start_pos": 232, "end_pos": 240, "type": "METRIC", "confidence": 0.9649498462677002}, {"text": "ArgM-NEG", "start_pos": 263, "end_pos": 271, "type": "METRIC", "confidence": 0.9855254292488098}, {"text": "ArgM-PNC", "start_pos": 294, "end_pos": 302, "type": "METRIC", "confidence": 0.9071978330612183}, {"text": "ArgM-PRD", "start_pos": 325, "end_pos": 333, "type": "METRIC", "confidence": 0.7625896334648132}, {"text": "ArgM-REC", "start_pos": 356, "end_pos": 364, "type": "METRIC", "confidence": 0.9154267311096191}]}]}