{"title": [{"text": "ISA meets Lara: An incremental word space model for cognitively plausible simulations of semantic learning", "labels": [], "entities": [{"text": "ISA meets Lara", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.867738942305247}, {"text": "semantic learning", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7640303671360016}]}], "abstractContent": [{"text": "We introduce Incremental Semantic Analysis , a fully incremental word space model, and we test it on longitudinal child-directed speech data.", "labels": [], "entities": [{"text": "Incremental Semantic Analysis", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.7360203266143799}]}, {"text": "On this task, ISA outperforms the related Random Indexing algorithm, as well as a SVD-based technique.", "labels": [], "entities": [{"text": "Random Indexing", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.5835958868265152}]}, {"text": "In addition , the model has interesting properties that might also be characteristic of the semantic space of children.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word space models induce a semantic space from raw textual input by keeping track of patterns of co-occurrence of words with other words through a vectorial representation.", "labels": [], "entities": []}, {"text": "Proponents of word space models such as HAL and LSA have argued that such models can capture a variety of facts about human semantic learning, processing, and representation.", "labels": [], "entities": [{"text": "semantic learning, processing, and representation", "start_pos": 124, "end_pos": 173, "type": "TASK", "confidence": 0.6197291740349361}]}, {"text": "As such, word space methods are not only increasingly useful as engineering applications, but they are also potentially promising for modeling cognitive processes of lexical semantics.", "labels": [], "entities": []}, {"text": "However, to the extent that current word space models are largely non-incremental, they can hardly accommodate how young children develop a semantic space by moving from virtually no knowledge of the language to reach an adult-like state.", "labels": [], "entities": []}, {"text": "The family of models based on singular value decomposition (SVD) and similar dimensionality reduction techniques (e.g., LSA) first construct a full cooccurrence matrix based on statistics extracted from the whole input corpus, and then build a model at once via matrix algebra operations.", "labels": [], "entities": [{"text": "singular value decomposition (SVD)", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.7777242759863535}]}, {"text": "Admittedly, this is hardly a plausible simulation of how children learn word meanings incrementally by being exposed to short sentences containing a relatively small number of different words.", "labels": [], "entities": []}, {"text": "The lack of incrementality of several models appears conspicuous especially given their explicit claim to solve old theoretical issues about the acquisition of language (e.g.,).", "labels": [], "entities": []}, {"text": "Other extant models display some degree if incrementality.", "labels": [], "entities": []}, {"text": "For instance, HAL and Random Indexing) can generate well-formed vector representations at intermediate stages of learning.", "labels": [], "entities": []}, {"text": "However, they lack incrementality when they make use of stop word lists or weigthing techniques that are based on whole corpus statistics.", "labels": [], "entities": []}, {"text": "For instance, consistently with the HAL approach, first build a word co-occurrence matrix, and then compute the variance of each column to reduce the vector dimensions by discarding those with the least contextual diversity. and propose an incremental version of HAL by using a a recurrent neural network trained with Hebbian learning.", "labels": [], "entities": []}, {"text": "The networks incrementally build distributional vectors that are then used to induce word semantic clusters with a Self-Organizing Map.", "labels": [], "entities": []}, {"text": "does not contain any evaluation of the structure of the semantic categories emerged in the SOM.", "labels": [], "entities": []}, {"text": "A more precise evaluation is instead performed by, revealing the model's ability to simulate interesting aspects of early vocabulary dynamics.", "labels": [], "entities": []}, {"text": "However, this is achieved by using hybrid word 49 representations, in which the distributional vectors are enriched with semantic features derived from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 152, "end_pos": 159, "type": "DATASET", "confidence": 0.9563296437263489}]}, {"text": "also model word learning in a fairly incremental fashion, by using the hidden layer vectors of a Simple Recurrent Network as word representations.", "labels": [], "entities": [{"text": "word learning", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.7860384583473206}]}, {"text": "The network is probed at different training epochs and its internal representations are evaluated against a gold standard ontology of semantic categories to monitor the progress in word learning.'s claim that their model simulates relevant aspects of child word learning should probably be moderated by the fact that they used a simplified set of artificial sentences as training corpus.", "labels": [], "entities": []}, {"text": "From their simulations it is thus difficult to evaluate whether the model would scale up to large naturalistic samples of language.", "labels": [], "entities": []}, {"text": "In this paper, we introduce Incremental Semantic Indexing (ISA), a model that strives to be more developmentally plausible by achieving full incrementality.", "labels": [], "entities": [{"text": "Incremental Semantic Indexing (ISA)", "start_pos": 28, "end_pos": 63, "type": "TASK", "confidence": 0.7306157847245535}]}, {"text": "We test the model and some of its less incremental rivals on Lara, a longitudinal corpus of childdirected speech based on samples of child-adult linguistic interactions collected regularly from 1 to 3 years of age of a single English child.", "labels": [], "entities": [{"text": "Lara", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.9380854964256287}]}, {"text": "ISA achieves the best performance on these data, and it learns a semantic space that has interesting properties for our understanding of how children learn and structure word meaning.", "labels": [], "entities": []}, {"text": "Thus, the desirability of incrementality increases as the model promises to capture specific developmental trajectories in semantic learning.", "labels": [], "entities": [{"text": "semantic learning", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.8498649895191193}]}, {"text": "The plan of the paper is as follows.", "labels": [], "entities": []}, {"text": "First, we introduce ISA together with its main predecessor, Random Indexing.", "labels": [], "entities": [{"text": "ISA", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9637627005577087}, {"text": "Random Indexing", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.6680479645729065}]}, {"text": "Then, we present the learning experiments in which several versions of ISA and other models are trained to induce and organize lexical semantic information from child-directed speech transcripts.", "labels": [], "entities": []}, {"text": "Lastly, we discuss further work in developmental computational modeling using word space models.", "labels": [], "entities": [{"text": "developmental computational modeling", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.6290844182173411}]}], "datasetContent": [{"text": "The input for our experiments is provided by the Child-Directed-Speech (CDS) section of the Lara corpus (), a longitudinal corpus of natural conversation transcripts of a single child, Lara, between the ages of 1;9 and 3;3.", "labels": [], "entities": [{"text": "Lara corpus", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.7250126153230667}]}, {"text": "Lara was the firstborn monolingual English daughter of two White university graduates and was born and brought up in Nottinghamshire, England.", "labels": [], "entities": []}, {"text": "The corpus consists of transcripts from 122 separate recording sessions in which the child interacted with adult caretakers in spontaneous conversations.", "labels": [], "entities": []}, {"text": "The total recording time of the corpus is of about 120 hours, representing one of the densest longitudinal corpora available.", "labels": [], "entities": []}, {"text": "The adult CDS section we used contains about 400K tokens and about 6K types.", "labels": [], "entities": [{"text": "adult CDS section", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.7552828788757324}]}, {"text": "We are aware that the use of a single-child corpus may have a negative impact on the generalizations on semantic development that we can draw from the experiments.", "labels": [], "entities": [{"text": "semantic development", "start_pos": 104, "end_pos": 124, "type": "TASK", "confidence": 0.8837730586528778}]}, {"text": "On the other hand, this choice has the important advantage of providing a fairly homogeneous data environment for our computational simulations.", "labels": [], "entities": []}, {"text": "In fact, we can abstract from the intrinsic variability characterizing any multi-child corpus, 51 and stemming from differences in the conversation settings, in the adults' grammar and lexicon, etc.", "labels": [], "entities": []}, {"text": "Moreover, whereas we can take our experiments to constitute a (very rough) simulation of how a particular child acquires semantic representations from her specific linguistic input, it is not clear what simulations based on an \"averages\" of different linguistic experiences would represent.", "labels": [], "entities": []}, {"text": "The corpus was part-of-speech-tagged and lemmatized using the CLAN toolkit).", "labels": [], "entities": []}, {"text": "The automated output was subsequently checked and disambiguated manually, resulting in very accurate annotation.", "labels": [], "entities": []}, {"text": "In our experiments, we use lemma-POS pairs as input to the word space models (e.g., go-v rather than going, goes, etc.)", "labels": [], "entities": []}, {"text": "Thus, we make the unrealistic assumptions that the learner already solved the problem of syntactic categorization and figured out the inflectional morphology of her language.", "labels": [], "entities": []}, {"text": "While a multi-level bootstrapping process in which the morphosyntactic and lexical properties of words are learned in parallel is probably cognitively more likely, it seems reasonable at the current stage of experimentation to fix morphosyntax and focus on semantic learning.", "labels": [], "entities": []}, {"text": "The test set was composed of 100 nouns and 70 verbs (henceforth, Ns and Vs), selected from the most frequent words in Lara's CDS section (word frequency ranges from 684 to 33 for Ns, and from 52 3501 to 89 for Vs).", "labels": [], "entities": [{"text": "Lara's CDS section", "start_pos": 118, "end_pos": 136, "type": "DATASET", "confidence": 0.8369659185409546}]}, {"text": "This asymmetry in the test set mirrors the different number of V and N types that occur in the input (2828 Ns vs. 944 Vs).", "labels": [], "entities": []}, {"text": "As a further constraint, we verified that all the words in the test set also appeared among the child's productions in the corpus.", "labels": [], "entities": []}, {"text": "The test words were unambiguously assigned to semantic categories previously used to model early lexical development and represent plausible early semantic groupings.", "labels": [], "entities": []}, {"text": "Semantic categories for nouns and verbs were derived by combining two methods.", "labels": [], "entities": []}, {"text": "For nouns, we used the ontologies from the Macarthur-Bates Communicative Development Inventories (CDI).", "labels": [], "entities": [{"text": "Macarthur-Bates Communicative Development Inventories (CDI)", "start_pos": 43, "end_pos": 102, "type": "DATASET", "confidence": 0.7500436902046204}]}, {"text": "All the Ns in the test set also appear in the Toddler's List in CDI.", "labels": [], "entities": [{"text": "Toddler's List in CDI", "start_pos": 46, "end_pos": 67, "type": "DATASET", "confidence": 0.9518999099731446}]}, {"text": "It is worth emphasizing that this experimental setting is much more challenging than those that are usually adopted by state-of-the-art computational simulations of word learning, as the ones reported above.", "labels": [], "entities": []}, {"text": "For instance, the number of words in our test set is larger than the one in, and so is the number of semantic categories, both for Ns and for Vs. Conversely, the Lara corpus is much smaller than the data-sets normally used to train word space models.", "labels": [], "entities": [{"text": "Lara corpus", "start_pos": 162, "end_pos": 173, "type": "DATASET", "confidence": 0.7795757353305817}]}, {"text": "For instance, the best results reported by are obtained with an input corpus which is 10 times bigger than ours.", "labels": [], "entities": []}, {"text": "As an evaluation measure of the model performance in the word learning task, we adopted Aver-age Precision (AP), recently used by.", "labels": [], "entities": [{"text": "word learning task", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.8357662955919901}, {"text": "Aver-age Precision (AP)", "start_pos": 88, "end_pos": 111, "type": "METRIC", "confidence": 0.9454917073249817}]}, {"text": "AP evaluates how close all members of a certain category are to each other in the semantic space built by the model.", "labels": [], "entities": [{"text": "AP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.712602436542511}]}, {"text": "To calculate AP, for each w i in the test set we first extracted the corresponding distributional vector vi produced by the model.", "labels": [], "entities": [{"text": "AP", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9509137272834778}]}, {"text": "Vectors were used to calculate the pair-wise cosine between each test word, as a measure of their distance in the semantic space.", "labels": [], "entities": []}, {"text": "AP ranges from 0 to 1: AP (w i ) = 1 would correspond to the ideal casein which all the closest words tow i in r i belonged to the same category as w i ; conversely, if all the words belonging to categories other than C w i were closer tow i than the words in C w i , AP (w i ) would approach 0.", "labels": [], "entities": [{"text": "AP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9853944778442383}, {"text": "AP", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9662461280822754}, {"text": "AP", "start_pos": 268, "end_pos": 270, "type": "METRIC", "confidence": 0.9640865921974182}]}, {"text": "We also defined the Class AP fora certain semantic category by simply averaging over the Word AP (w i ) for each word in that category: We adopted AP as a measure of the purity and cohesiveness of the semantic representations produced by the model.", "labels": [], "entities": [{"text": "AP", "start_pos": 147, "end_pos": 149, "type": "METRIC", "confidence": 0.952357828617096}]}, {"text": "Words and categories for which the model is able to converge on well-formed representations should therefore have higher AP values.", "labels": [], "entities": [{"text": "AP", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.9968619346618652}]}, {"text": "If we define Recall as the number of words inn w j belonging to C w i divided by the total number of words in C w i , then all the AP scores reported in our experiments correspond to 100% Recall, since the neighbourhood we used to compute AP (w i ) always included all the words in C w i . This represents a very 53", "labels": [], "entities": [{"text": "Recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9835020899772644}, {"text": "AP", "start_pos": 131, "end_pos": 133, "type": "METRIC", "confidence": 0.9573881030082703}, {"text": "Recall", "start_pos": 188, "end_pos": 194, "type": "METRIC", "confidence": 0.9977689981460571}]}], "tableCaptions": [{"text": " Table 1: Word AP scores for Nouns (top) and Verbs  (bottom). For ISA and RI, scores are averaged  across 10 iterations", "labels": [], "entities": [{"text": "Word AP scores", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.6252690553665161}]}, {"text": " Table 2: Class AP scores for Nouns. For ISA and  RI, scores are averaged across 10 iterations", "labels": [], "entities": []}]}