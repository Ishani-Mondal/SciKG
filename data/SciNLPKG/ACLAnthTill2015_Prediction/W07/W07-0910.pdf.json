{"title": [{"text": "Deriving a Domain Specific Test Collection from a Query Log", "labels": [], "entities": []}], "abstractContent": [{"text": "Cultural heritage, and other special domains, pose a particular problem for information retrieval: evaluation requires a dedicated test collection that takes the particular documents and information requests into account , but building such a test collection requires substantial human effort.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.7864814698696136}]}, {"text": "This paper investigates methods of generating a document retrieval test collection from a search engine's transaction log, based on submitted queries and user-click data.", "labels": [], "entities": [{"text": "document retrieval test collection from a search engine's transaction", "start_pos": 48, "end_pos": 117, "type": "TASK", "confidence": 0.77819564640522}]}, {"text": "We test our methods on a museum's search log file, and compare the quality of the generated test collections against a collection with manually generated and judged known-item topics.", "labels": [], "entities": []}, {"text": "Our main findings are the following.", "labels": [], "entities": []}, {"text": "First, the test collection derived from a transaction log corresponds well to the actual search experience of real users.", "labels": [], "entities": []}, {"text": "Second, the ranking of systems based on the derived judgments corresponds well to the ranking based on the manual topics.", "labels": [], "entities": []}, {"text": "Third, deriving pseudo-relevance judgments from a transaction log file is an attractive option in domains where dedicated test collections are not readily available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cultural heritage, and other special domains, pose a particular problem for information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.8500290811061859}]}, {"text": "Progress in information retrieval depends heavily on the availability of suitable test collections consisting of a set of documents; a set of search topics; and (human) relevance judgments.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7915141582489014}]}, {"text": "Standard benchmarks, such as those developed at TREC, have been developed using newspaper and newswire data.", "labels": [], "entities": [{"text": "TREC", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.8551638126373291}]}, {"text": "Whilst these test collections are immensely useful to evaluate generic properties of retrieval systems, such as fundamental ranking principles, they do not capture the specific context of particular domains).", "labels": [], "entities": []}, {"text": "To take cultural heritage as an example, the documents are cultural heritage descriptions which are different in character from newspaper articles, and also the search requests and relevance judgments about art are more subjective than factual queries about news (.", "labels": [], "entities": []}, {"text": "As a result, special domains like cultural heritage require a dedicated test collection that takes the particular documents and information requests into account, but building such a test collection requires substantial human effort.", "labels": [], "entities": []}, {"text": "We opt fora different approach.", "labels": [], "entities": []}, {"text": "Search engines commonly store the actions of users in transaction logs, which allow an unobtrusive way of studying user behaviour.", "labels": [], "entities": []}, {"text": "Logs contain valuable information such as what searchers are looking for, what results they find interesting enough to click on, etc.", "labels": [], "entities": []}, {"text": "In this paper, we investigate methods of extracting queries and user-clicks (on the search result items) from transaction logs in order to create a quality test collection for Document Retrieval.", "labels": [], "entities": [{"text": "Document Retrieval", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.8772421777248383}]}, {"text": "A quality test collection for Document Retrieval is traditionally considered as a set of queries on a document collection with complete and reliable relevance judgements.", "labels": [], "entities": [{"text": "Document Retrieval", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8816398084163666}]}, {"text": "Complete in the sense that all documents are judged for relevance against all queries, and reliable in the sense that judgements are sta-ble across a majority of human assessors.", "labels": [], "entities": []}, {"text": "Nevertheless, considering the fact that a test collection is used \"as a mechanism for comparing system performance\"), the requirements for completeness and reliability maybe relaxed somewhat.", "labels": [], "entities": [{"text": "reliability", "start_pos": 156, "end_pos": 167, "type": "METRIC", "confidence": 0.9631304144859314}]}, {"text": "The Text REtrial Conference (TREC) has traditionally used incomplete judgements for comparing system effectiveness via the \"pooling\" method, and it is also well-known that human assessor agreement is relatively low).", "labels": [], "entities": [{"text": "Text REtrial Conference (TREC)", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.6678121437629064}, {"text": "agreement", "start_pos": 187, "end_pos": 196, "type": "METRIC", "confidence": 0.8082481026649475}]}, {"text": "Consequently, test collections which preserve the effectiveness ranking of several systems can be considered of equivalent quality in the context of comparing system effectiveness.", "labels": [], "entities": []}, {"text": "In order to evaluate the quality of test collections extracted in various ways from a transaction log, it would be sufficient to compare their ability to rank several retrieval systems against a reference system ranking produced by an already known good test collection not produced from the log.", "labels": [], "entities": []}, {"text": "One can think of several ways of extracting queries and clicks from a transaction log and turning them into a set of queries with relevance judgments.", "labels": [], "entities": []}, {"text": "A simple (and naive) way would be to treat every query typed by a user as a topic, and every result that the user clicked on as a positive relevance judgment.", "labels": [], "entities": []}, {"text": "However, such an approach may not lead to a good test set.", "labels": [], "entities": []}, {"text": "Previous research on user click behaviour has shown that clicks on search engine results do not directly correspond to explicit, absolute relevance judgments, but can be considered as relative relevance judgments (), i.e., if a user skips result a and clicks on result b, than the user preference reflects rank(b) > rank(a).", "labels": [], "entities": []}, {"text": "Moreover, the occurrence frequencies of queries and the numbers of retrieved items vary significantly across queries which may lead to wide variation in effectiveness.", "labels": [], "entities": []}, {"text": "The challenge we take up has several dimensions which can be summarized in the following questions: \u2022 How can we derive topics and pseudorelevance judgments from a transaction log file, and how does this impact the quality of the generated test collection?", "labels": [], "entities": []}, {"text": "\u2022 How does system effectiveness on the automatically generated test collection compare to the effectiveness on a set of manually constructed known-item topics?", "labels": [], "entities": []}, {"text": "If automatic methods of building test collections are indeed feasible, this opens up a whole new dimension of possibilities for Information Retrieval evaluation: there is an enormous lengths of transaction logs generated daily at numerous web-sites and at on-line search engines.", "labels": [], "entities": [{"text": "Information Retrieval evaluation", "start_pos": 128, "end_pos": 160, "type": "TASK", "confidence": 0.8988115191459656}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Next, in Section 2 we discuss transaction logs in general, and the specific transaction log from a museum that we'll use in the case study of this paper.", "labels": [], "entities": []}, {"text": "Section 3 details how we have extracted topics and pseudo-relevance judgments from a museum's log file, and their evaluation.", "labels": [], "entities": []}, {"text": "Then, in Section 4, we evaluate the merits of the derived test collection in comparison to human generated and judged topics.", "labels": [], "entities": []}, {"text": "We end with Section 5 in which we summarize our findings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have obtained the log files covering a period of one and a half years, between From the transaction log, we extracted the queries and the object identifiers from the database query, and turned them into Qrels, i.e., the object is relevant for the query.", "labels": [], "entities": []}, {"text": "We use the following terminology: \u2022 User: the client side of the transaction, identified by ip-address.", "labels": [], "entities": []}, {"text": "\u2022 Transaction: any exchange between client (user) and server (system), corresponding to a line in the transaction log.", "labels": [], "entities": []}, {"text": "\u2022 Session: A sequence of transactions by the same user, where the maximum interval between transaction n and n + 1 is 1 hour.: Statistics on the extracted topic sets.", "labels": [], "entities": []}, {"text": "More than 1 hour of inactivity signals a session boundary.", "labels": [], "entities": []}, {"text": "\u2022 Query: the string typed by the user as it appears in the transaction log.", "labels": [], "entities": []}, {"text": "\u2022 Result: the identifier of the museum object, used to retrieve the object data from the object database.", "labels": [], "entities": [{"text": "Result", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9533072113990784}]}, {"text": "In our experiments we will emulate a set of different retrieval systems by using arbitrary parameter settings for smoothing (\u03bb) and length prior (\u03b2).", "labels": [], "entities": [{"text": "length prior (\u03b2)", "start_pos": 132, "end_pos": 148, "type": "METRIC", "confidence": 0.9601576209068299}]}, {"text": "This will result in a range of different rankings of documents, and we can compare their retrieval effectiveness on our various topic sets.", "labels": [], "entities": []}, {"text": "In this way, we can compare the system ranking of the automatically generated topic sets with the system ranking of a manually crafted topic set.", "labels": [], "entities": []}, {"text": "We made 9 different runs with each topic set, using 3 different values (0.10, 0.50 and 0.90) for the smoothing parameter \u03bb, corresponding to heavy, average and little smoothing respectively, and 3 different values (0, 1 and 2) for the length prior \u03b2 corresponding to no length normalization and length normalization proportional to the document length.", "labels": [], "entities": []}, {"text": "To measure the correlation of the system rankings resulting from the different topic sets, we look at Kendall's tau coefficient.", "labels": [], "entities": []}, {"text": "shows the detailed results for all runs overall topics sets.", "labels": [], "entities": []}, {"text": "As noted above, we will focus on the relative system rankings over topic sets.", "labels": [], "entities": []}, {"text": "We limit our analysis to the performance in terms of mean-: Mean Reciprocal Rank and Success@10 for all topic sets on the website objects.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank", "start_pos": 60, "end_pos": 80, "type": "METRIC", "confidence": 0.9213260412216187}, {"text": "Success@10", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.917227566242218}]}, {"text": "reciprocal rank (i.e., 1 over the rank at which the first relevant document is found).", "labels": [], "entities": []}, {"text": "The rankings over the four different topic sets are given in (based on the labeling introduced in).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the extracted topic sets.", "labels": [], "entities": []}, {"text": " Table 2: Parameter settings for the different systems.", "labels": [], "entities": [{"text": "Parameter", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9024918675422668}]}, {"text": " Table 3: Mean Reciprocal Rank and Success@10 for all topic sets on the web site objects.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9084546764691671}, {"text": "Success", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.8917211294174194}]}, {"text": " Table 5: Rank correlation coefficients between the  topic sets.", "labels": [], "entities": [{"text": "Rank correlation", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8258149921894073}]}]}