{"title": [{"text": "(Meta-) Evaluation of Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7308327257633209}]}], "abstractContent": [{"text": "This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7451382875442505}]}, {"text": "We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9834327697753906}]}, {"text": "We measured timing and intra-and inter-annotator agreement for three types of subjective evaluation.", "labels": [], "entities": [{"text": "timing", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9928920269012451}, {"text": "agreement", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.8285913467407227}]}, {"text": "We measured the correlation of automatic evaluation metrics with human judgments.", "labels": [], "entities": []}, {"text": "This meta-evaluation reveals surprising facts about the most commonly used methodologies.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents the results for the shared translation task of the 2007 ACL Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "shared translation task of the 2007 ACL Workshop on Statistical Machine Translation", "start_pos": 40, "end_pos": 123, "type": "TASK", "confidence": 0.7817389716704687}]}, {"text": "The goals of this paper are twofold: First, we evaluate the shared task entries in order to determine which systems produce translations with the highest quality.", "labels": [], "entities": []}, {"text": "Second, we analyze the evaluation measures themselves in order to try to determine \"best practices\" when evaluating machine translation research.", "labels": [], "entities": [{"text": "machine translation research", "start_pos": 116, "end_pos": 144, "type": "TASK", "confidence": 0.8508989016215006}]}, {"text": "Previous ACL Workshops on Machine Translation were more limited in scope ().", "labels": [], "entities": [{"text": "ACL Workshops on Machine Translation", "start_pos": 9, "end_pos": 45, "type": "TASK", "confidence": 0.5968751668930053}]}, {"text": "The 2005 workshop evaluated translation quality only in terms of Bleu score.", "labels": [], "entities": [{"text": "translation", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9773538708686829}, {"text": "Bleu score", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9106411039829254}]}, {"text": "The 2006 workshop additionally included a limited manual evaluation in the style of NIST machine translation evaluation workshop.", "labels": [], "entities": [{"text": "NIST machine translation evaluation workshop", "start_pos": 84, "end_pos": 128, "type": "TASK", "confidence": 0.757166188955307}]}, {"text": "Here we apply eleven different automatic evaluation metrics, and conduct three different types of manual evaluation.", "labels": [], "entities": []}, {"text": "Beyond examining the quality of translations produced by various systems, we were interested in examining the following questions about evaluation methodologies: How consistent are people when they judge translation quality?", "labels": [], "entities": []}, {"text": "To what extent do they agree with other annotators?", "labels": [], "entities": []}, {"text": "Can we improve human evaluation?", "labels": [], "entities": [{"text": "human evaluation", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.697282075881958}]}, {"text": "Which automatic evaluation metrics correlate most strongly with human judgments of translation quality?", "labels": [], "entities": []}, {"text": "This paper is organized as follows: \u2022 Section 2 gives an overview of the shared task.", "labels": [], "entities": []}, {"text": "It describes the training and test data, reviews the baseline system, and lists the groups that participated in the task.", "labels": [], "entities": []}, {"text": "\u2022 Section 3 describes the manual evaluation.", "labels": [], "entities": []}, {"text": "We performed three types of evaluation: scoring with five point scales, relative ranking of translations of sentences, and ranking of translations of phrases.", "labels": [], "entities": []}, {"text": "\u2022 Section 4 lists the eleven different automatic evaluation metrics which were also used to score the shared task submissions.", "labels": [], "entities": []}, {"text": "\u2022 Section 5 presents the results of the shared task, giving scores for each of the systems in each of the different conditions.", "labels": [], "entities": []}, {"text": "\u2022 Section 6 provides an evaluation of the different types of evaluation, giving intra-and inter-annotator agreement figures for the manual evaluation, and correlation numbers for the automatic metrics.", "labels": [], "entities": [{"text": "correlation", "start_pos": 155, "end_pos": 166, "type": "METRIC", "confidence": 0.972859799861908}]}], "datasetContent": [{"text": "We evaluated the shared task submissions using both manual evaluation and automatic metrics.", "labels": [], "entities": []}, {"text": "While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are an imperfect substitute for human assessment of translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7342880070209503}]}, {"text": "Manual evaluation is time consuming and expensive to perform, so comprehensive comparisons of multiple systems are rare.", "labels": [], "entities": []}, {"text": "For our manual evaluation we distributed the workload across a number of people, including participants in the shared task, interested volunteers, and a small number of paid annotators.", "labels": [], "entities": []}, {"text": "More than 100 people participated in the manual evaluation, with 75 of those people putting in at least an hour's worth of effort.", "labels": [], "entities": []}, {"text": "A total of 330 hours of labor was invested, nearly doubling last year's all-volunteer effort which yielded 180 hours of effort.", "labels": [], "entities": []}, {"text": "Beyond simply ranking the shared task submissions, we had a number of scientific goals for the manual evaluation.", "labels": [], "entities": []}, {"text": "Firstly, we wanted to collect data which could be used to assess how well automatic metrics correlate with human judgments.", "labels": [], "entities": []}, {"text": "Secondly, we wanted to examine different types of manual evaluation and assess which was the best.", "labels": [], "entities": []}, {"text": "A number of criteria could be adopted for choosing among different types of manual evaluation: the ease with which people are able to perform the task, their agreement with other annotators, their reliability when asked to repeat judgments, or the number of judgments which can be collected in a fixed time period.", "labels": [], "entities": [{"text": "reliability", "start_pos": 197, "end_pos": 208, "type": "METRIC", "confidence": 0.9828841686248779}]}, {"text": "There area range of possibilities for how human evaluation of machine translation can be done.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.701074555516243}]}, {"text": "For instance, it can be evaluated with reading comprehension tests (), or by assigning subjective scores to the translations of individual sentences).", "labels": [], "entities": []}, {"text": "We examined three different ways of manually evaluating machine translation quality: \u2022 Assigning scores based on five point adequacy and fluency scales \u2022 Ranking translated sentences relative to each other \u2022 Ranking the translations of syntactic constituents drawn from the source sentence   The past two ACL workshops on machine translation used Bleu as the sole automatic measure of translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 322, "end_pos": 341, "type": "TASK", "confidence": 0.7699428498744965}, {"text": "Bleu", "start_pos": 347, "end_pos": 351, "type": "METRIC", "confidence": 0.9695112705230713}]}, {"text": "Bleu was used exclusively since it is the most widely used metric in the field and has been shown to correlate with human judgments of translation quality in many instances).", "labels": [], "entities": [{"text": "Bleu", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9886409044265747}]}, {"text": "However, recent work suggests that Bleu's correlation with human judgments may not be as strong as previously thought).", "labels": [], "entities": [{"text": "Bleu", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9585126042366028}]}, {"text": "The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems ().", "labels": [], "entities": [{"text": "Bleu", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.8624565601348877}, {"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.683667004108429}]}, {"text": "We used the manual evaluation data as a means of testing the correlation of a range of automatic metrics in addition to Bleu.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9605075716972351}]}, {"text": "In total we used eleven different automatic evaluation measures to rank the shared task submissions.", "labels": [], "entities": []}, {"text": "They are: \u2022 Meteor ().", "labels": [], "entities": []}, {"text": "\u2022 Bleu ()-Bleu is currently the de facto standard in machine translation evaluation.", "labels": [], "entities": [{"text": "Bleu ()-Bleu", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.7475720842679342}, {"text": "machine translation evaluation", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.8826629122098287}]}, {"text": "It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as away of capturing some of the allowable variation in translation.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9830080270767212}]}, {"text": "We use a single reference translation in our experiments.", "labels": [], "entities": []}, {"text": "\u2022 GTM ()-GTM generalizes precision, recall, and F-measure to measure overlap between strings, rather than overlap between bags of items.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.998779833316803}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9968547821044922}, {"text": "F-measure", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.996901273727417}]}, {"text": "An \"exponent\" parameter which controls the relative importance of word order.", "labels": [], "entities": []}, {"text": "A value of 1.0 reduces GTM to ordinary unigram overlap, with higher values emphasizing order.", "labels": [], "entities": [{"text": "GTM", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.898617148399353}]}, {"text": "4 \u2022 Translation Error Rate ()- The GTM scores presented here are an F-measure with a weight of 0.1, which counts recall at 10x the level of precision.", "labels": [], "entities": [{"text": "Translation Error Rate", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.7176093359788259}, {"text": "F-measure", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9940855503082275}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9982341527938843}, {"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9972507357597351}]}, {"text": "The exponent is set at 1.2, which puts a mild preference towards items with words in the correct order.", "labels": [], "entities": []}, {"text": "These parameters could be optimized empirically for better results.", "labels": [], "entities": []}, {"text": "TER calculates the number of edits required to change a hypothesis translation into a reference translation.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8403770327568054}]}, {"text": "The possible edits in TER include insertion, deletion, and substitution of single words, and an edit which moves sequences of contiguous words.", "labels": [], "entities": []}, {"text": "\u2022 ParaEval precision and ParaEval recall ()-ParaEval matches hypothesis and reference translations using paraphrases that are extracted from parallel corpora in an unsupervised fashion).", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.8566879034042358}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.573567271232605}]}, {"text": "It calculates precision and recall using a unigram counting strategy.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9994162321090698}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9988321661949158}]}, {"text": "\u2022 Dependency overlap)-This metric uses dependency trees for the hypothesis and reference translations, by computing the average overlap between words in the two trees which are dominated by grammatical relationships of the same type.", "labels": [], "entities": []}, {"text": "\u2022 Semantic role overlap ()-This metric calculates the lexical overlap between semantic roles (i.e., semantic arguments or adjuncts) of the same type in the the hypothesis and reference translations.", "labels": [], "entities": []}, {"text": "It uniformly averages lexical overlap overall semantic role types.", "labels": [], "entities": []}, {"text": "\u2022 Word Error Rate over verbs-WER' creates anew reference and anew hypothesis for each POS class by extracting all words belonging to this class, and then to calculate the standard WER.", "labels": [], "entities": [{"text": "Word Error Rate", "start_pos": 2, "end_pos": 17, "type": "METRIC", "confidence": 0.5987047751744589}, {"text": "WER", "start_pos": 180, "end_pos": 183, "type": "METRIC", "confidence": 0.9740633368492126}]}, {"text": "We show results for this metric over verbs.", "labels": [], "entities": []}, {"text": "\u2022 Maximum correlation training on adequacy and on fluency ()-a linear combination of different evaluation metrics (Bleu, Meteor, Rouge, WER, and stochastic iterative alignment) with weights set to maximize Pearson's correlation with adequacy and fluency judgments.", "labels": [], "entities": [{"text": "WER", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.917629063129425}]}, {"text": "Weights were trained on WMT-06 data.", "labels": [], "entities": [{"text": "WMT-06 data", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.9577454328536987}]}, {"text": "The scores produced by these are given in the tables at the end of the paper, and described in Section 5.", "labels": [], "entities": []}, {"text": "We measured the correlation of the automatic evaluation metrics with the different types of human judgments on 12 data conditions, and report these in Section 6.", "labels": [], "entities": []}, {"text": "In addition to evaluating the translation quality of the shared task entries, we also performed a \"metaevaluation\" of our evaluation methodologies.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The number of items that were judged for each task during the manual evaluation", "labels": [], "entities": []}, {"text": " Table 3: The proportion of time that participants'  entries were top-ranked in the human evaluation", "labels": [], "entities": []}, {"text": " Table 4: The proportion of time that participants'  entries were top-ranked by the automatic evaluation  metrics", "labels": [], "entities": []}, {"text": " Table 5: Kappa coefficient values representing the  inter-annotator agreement for the different types of  manual evaluation", "labels": [], "entities": [{"text": "Kappa coefficient", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9243150651454926}]}, {"text": " Table 6: Kappa coefficient values for intra-annotator  agreement for the different types of manual evalua- tion", "labels": [], "entities": [{"text": "Kappa coefficient", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9601934850215912}]}, {"text": " Table 7: Average corrections for the different auto- matic metrics when they are used to evaluate trans- lations into English", "labels": [], "entities": [{"text": "corrections", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.7175184488296509}, {"text": "trans- lations", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.646211306254069}]}, {"text": " Table 8: Average corrections for the different auto- matic metrics when they are used to evaluate trans- lations into the other languages", "labels": [], "entities": [{"text": "trans- lations", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.6461915969848633}]}, {"text": " Table 12: Human evaluation for Czech-English sub- missions", "labels": [], "entities": [{"text": "Human evaluation", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7966445088386536}]}, {"text": " Table 13: Automatic evaluation scores for German-English submissions", "labels": [], "entities": [{"text": "Automatic", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9333862066268921}]}, {"text": " Table 14: Automatic evaluation scores for Spanish-English submissions", "labels": [], "entities": [{"text": "Automatic", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9564560055732727}]}, {"text": " Table 15: Automatic evaluation scores for French-English submissions", "labels": [], "entities": [{"text": "Automatic", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9471206665039062}]}, {"text": " Table 16: Automatic evaluation scores for Czech-English submissions", "labels": [], "entities": [{"text": "Automatic", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9367888569831848}]}, {"text": " Table 17: Correlation of the automatic evaluation metrics with the human judgments when translating into English", "labels": [], "entities": []}]}