{"title": [{"text": "Experiences of an In-Service Wizard-of-Oz Data Collection for the Deployment of a Call-Routing Application", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our experiences of collecting a corpus of 42,000 dialogues fora call-routing application using a Wizard-of-Oz approach.", "labels": [], "entities": []}, {"text": "Contrary to common practice in the industry, we did not use the kind of automated application that elicits some speech from the customers and then sends all of them to the same destination, such as the existing touch-tone menu, without paying attention to what they have said.", "labels": [], "entities": []}, {"text": "Contrary to the traditional Wizard-of-Oz paradigm, our data-collection application was fully integrated within an existing service, replacing the existing touch-tone navigation system with a simulated call-routing system.", "labels": [], "entities": []}, {"text": "Thus, the subjects were real customers calling about real tasks, and the wizards were service agents from our customer care.", "labels": [], "entities": []}, {"text": "We provide a detailed exposition of the data collection as such and the application used, and compare our approach to methods previously used.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Several design experiments were run during the data collection.", "labels": [], "entities": []}, {"text": "Here, we shall only very briefly describe one of them, in which we compared two styles of disambiguation prompts, one completely open and one more directed.", "labels": [], "entities": []}, {"text": "As can be seen in  , and the open prompt ('Could you please tell me a little bit more about the reason for you call?')", "labels": [], "entities": []}, {"text": "Totals and ratios are given for utterances/words, disfluencies and number of concepts acquired before the disambiguation prompt was played (\"In\") and after the customer had replied to the disambiguation prompt (\"Out\").", "labels": [], "entities": []}, {"text": "Also, ratios are given for number of concepts compared to number of utterances and words, as well as totals and ratios for the differences (DIFFs) between concepts in and concepts out, i.e., how many concepts you \"win\" by asking the disambiguation prompt.", "labels": [], "entities": [{"text": "DIFFs)", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9299274981021881}]}, {"text": "Furthermore, in order to see to what extent these prompts also made callers provide more information, we manually tagged the transcribed utterances with semantic categories.", "labels": [], "entities": []}, {"text": "Following the evaluation methodology suggested by, we then computed the difference with respect to \"concepts\" for utterances immediately following and preceding the two kinds of prompts.", "labels": [], "entities": []}, {"text": "Although the number of concepts gained is only slightly higher for the open prompt (as a function of concepts per utterance), there are some palpable differences between the directed and the open prompt.", "labels": [], "entities": []}, {"text": "One, shown in TABLE 1, is that there are no instances where an already instantiated concept (e.g. fixedTelephony) is changed to something else (e.g. broadband), while this happens 18 times following the open prompt.", "labels": [], "entities": [{"text": "TABLE 1", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.619502067565918}]}, {"text": "The other, not shown in TABLE 1, is that, following the directed prompt, one never \"gains\" more than one new concept, while there are 26 instances following the open prompt where the gain is two concepts, and even two instances where the gain is three concepts (which also means that one concept is changed).", "labels": [], "entities": []}, {"text": "Finally, when one analyses the syntactic characteristics following the two different types of prompts, there is an obvious shift from the telegraphic \"noun-only\" responses that amount to more than 70% of the directed prompt responses, to the responses following the open prompt, where 40% are complete sentences and 21% are noun phrases.", "labels": [], "entities": []}, {"text": "Also, the syntax is more varied following the open prompt.", "labels": [], "entities": []}, {"text": "7 6 However, the difference is not statistically significant, either using at test (two-sampled, two-tailed: p=0.16 with equal variances assumed; p=0.158 equal variances not assumed) or Mann-Whitney U test (two-tailed: p=0.288).", "labels": [], "entities": []}, {"text": "The distributions are, in descending order, for the directed prompt: Noun=85, Sentence=11, Yes/No=8, Noun Phrase=8, no response=3, Yes/No+Noun=2, Adverbial Phrase=1, Adjective Phrase=1; for the open prompt: Sentence=49, Noun Phrase=26, Noun=24, Verb", "labels": [], "entities": []}], "tableCaptions": []}