{"title": [{"text": "Perceptron Training fora Wide-Coverage Lexicalized-Grammar Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper investigates perceptron training fora wide-coverage CCG parser and compares the perceptron with a log-linear model.", "labels": [], "entities": []}, {"text": "The CCG parser uses a phrase-structure parsing model and dynamic programming in the form of the Viterbi algorithm to find the highest scoring derivation.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.7288477867841721}]}, {"text": "The difficulty in using the perceptron fora phrase-structure parsing model is the need for an efficient de-coder.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.6965372115373611}]}, {"text": "We exploit the lexicalized nature of CCG by using a finite-state supertagger to do much of the parsing work, resulting in a highly efficient decoder.", "labels": [], "entities": [{"text": "parsing", "start_pos": 95, "end_pos": 102, "type": "TASK", "confidence": 0.9610918760299683}]}, {"text": "The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model.", "labels": [], "entities": []}, {"text": "We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results.", "labels": [], "entities": []}], "introductionContent": [{"text": "A recent development in data-driven parsing is the use of discriminative training methods ().", "labels": [], "entities": [{"text": "data-driven parsing", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.5962887704372406}]}, {"text": "One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function).", "labels": [], "entities": []}, {"text": "Maximising the likelihood involves calculating feature expectations, which is computationally expensive.", "labels": [], "entities": [{"text": "Maximising", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9619208574295044}, {"text": "likelihood", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.8512519001960754}]}, {"text": "Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local); however, the memory requirements can be prohibitive, especially for automatically extracted, wide-coverage grammars.", "labels": [], "entities": []}, {"text": "In we use cluster computing resources to solve this problem.", "labels": [], "entities": []}, {"text": "Parsing research has also begun to adopt discriminative methods from the Machine Learning literature, such as the perceptron) and the largemargin methods underlying Support Vector Machines ().", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9263312816619873}]}, {"text": "Parser training involves decoding in an iterative process, updating the model parameters so that the decoder performs better on the training data, according to some training criterion.", "labels": [], "entities": [{"text": "Parser training", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8628085851669312}]}, {"text": "Hence, for efficient training, these methods require an efficient decoder; in fact, for methods like the perceptron, the update procedure is so trivial that the training algorithm essentially is decoding.", "labels": [], "entities": []}, {"text": "This paper describes a decoder fora lexicalizedgrammar parser which is efficient enough for practical discriminative training.", "labels": [], "entities": []}, {"text": "We use a lexicalized phrase-structure parser, the CCG parser of, together with a DP-based decoder.", "labels": [], "entities": []}, {"text": "The key idea is to exploit the properties of lexicalized grammars by using a finite-state supertagger prior to parsing.", "labels": [], "entities": []}, {"text": "The decoder still uses the CKY algorithm, so the worst case complexity of the parsing is unchanged; however, by allowing the supertagger to do much of the parsing work, the efficiency of the decoder is greatly increased in practice.", "labels": [], "entities": []}, {"text": "We chose the perceptron for the training algorithm because it has shown good performance on other NLP tasks; in particular, reported good performance fora perceptron tagger compared to a Maximum Entropy tagger.", "labels": [], "entities": []}, {"text": "Like, the decoder is the same for both the perceptron and the log-linear parsing models; the only change is the method for setting the weights.", "labels": [], "entities": []}, {"text": "The perceptron model performs as well as the loglinear model, but is considerably easier to train.", "labels": [], "entities": []}, {"text": "Another contribution of this paper is to advance wide-coverage CCG parsing.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.7611106634140015}]}, {"text": "Previous discriminative models for CCG) required cluster computing resources to train.", "labels": [], "entities": []}, {"text": "In this paper we reduce the memory requirements from 20 GB of RAM to only a few hundred MB, but without greatly increasing the training time or reducing parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 153, "end_pos": 160, "type": "TASK", "confidence": 0.9615045785903931}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.8963829278945923}]}, {"text": "This provides state-of-the-art CCG parsing with a practical development environment.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.7406385540962219}]}, {"text": "More generally, this work provides a practical environment for experimenting with discriminative models for phrase-structure parsing; because the training time for the CCG parser is relatively short (a few hours), experiments such as comparing alternative feature sets can be performed.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.7428330481052399}]}, {"text": "As an example, we investigate the order in which the training examples are presented to the perceptron learner.", "labels": [], "entities": []}, {"text": "Since the perceptron training is an online algorithm -updating the weights one training sentence at a time -the order in which the data is processed affects the resulting model.", "labels": [], "entities": []}, {"text": "We consider random ordering; presenting the shortest sentences first; and presenting the longest sentences first; and find that the order does not significantly affect the final results.", "labels": [], "entities": []}, {"text": "We also use the random orderings to investigate model averaging.", "labels": [], "entities": [{"text": "model averaging", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.6996130794286728}]}, {"text": "We produced 10 different models, by randomly permuting the data, and averaged the weights.", "labels": [], "entities": []}, {"text": "Again the averaging was found to have no impact on the results, showing that the perceptron learner -at least for this parsing task -is robust to the order of the training examples.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "First, we compare perceptron and log-linear parsing models fora wide-coverage phrase-structure parser, the first work we are aware of to do so.", "labels": [], "entities": [{"text": "phrase-structure parser", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.6897727996110916}]}, {"text": "Second, we provide a practical framework for developing discriminative models for CCG, reducing the memory requirements from over 20 GB to a few hundred MB.", "labels": [], "entities": []}, {"text": "And third, given the significantly shorter training time compared to other discriminative parsing models (), we provide a practical framework for investigating discriminative training methods more generally.", "labels": [], "entities": []}], "datasetContent": [{"text": "The feature forests were created as follows.", "labels": [], "entities": []}, {"text": "First, the value of the \u03b2 parameter for the supertagger was fixed (for the first set of experiments at 0.004).", "labels": [], "entities": []}, {"text": "The supertagger was then run over the sentences in Sections 2-21 of CCGbank.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.965183436870575}]}, {"text": "We made sure that every word was assigned the correct lexical category among its set (we did not do this for testing).", "labels": [], "entities": []}, {"text": "Then the parser was run on the supertagged sentences, using the CKY algorithm and the CCG combinatory rules.", "labels": [], "entities": []}, {"text": "We applied the same normal-form restrictions used in: categories can only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.9111231565475464}]}, {"text": "This part of the process requires a few hundred MB of RAM to run the parser, and takes a few hours for Sections 2-21 of CCGbank.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 120, "end_pos": 127, "type": "DATASET", "confidence": 0.8812404274940491}]}, {"text": "Any further training times or memory requirements reported do not include the resources needed to create the forests.", "labels": [], "entities": [{"text": "memory", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.985898494720459}]}, {"text": "The feature forests are extracted from the packed chart representation used in the parser.", "labels": [], "entities": []}, {"text": "We only use a feature forest for training if it contains the correct derivation (according to CCGbank).", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.9795997142791748}]}, {"text": "Some forests do not have the correct derivation, even though we ensure the correct lexical categories are present, because the grammar used by the parser is missing some low-frequency rules in CCGbank.", "labels": [], "entities": []}, {"text": "The total number of forests used for the experiments was 35,370 (89% of Sections 2-21) . Only features which occur at least twice in the training data were used, of which there are 477,848.", "labels": [], "entities": []}, {"text": "The complete set of forests used to obtain the final perceptron results in Section 4.1 require 21 GB of disk space.", "labels": [], "entities": []}, {"text": "The perceptron is an online algorithm, updating the weights after each forest is processed.", "labels": [], "entities": []}, {"text": "Each forest is read into memory one at a time, decoding is performed, and the weight values are updated.", "labels": [], "entities": []}, {"text": "Each forest is discarded from memory after it has been used.", "labels": [], "entities": []}, {"text": "Constantly reading forests off disk is expensive, but since the perceptron converges in so few iterations the training times are reasonable.", "labels": [], "entities": []}, {"text": "In contrast, log-linear training takes hundreds of iterations to converge, and so it would be impractical to keep reading the forests off disk.", "labels": [], "entities": []}, {"text": "Also, since loglinear training uses a batch algorithm, it is more convenient to keep the forests in memory at all times.", "labels": [], "entities": []}, {"text": "In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM.", "labels": [], "entities": [{"text": "Clark and Curran (2004b", "start_pos": 3, "end_pos": 26, "type": "DATASET", "confidence": 0.7892126917839051}, {"text": "BFGS", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.7177632451057434}]}, {"text": "The feature forest representation, and our implementation of it, is so compact that the perceptron training requires only 20 MB of RAM.", "labels": [], "entities": []}, {"text": "Since the supertagger has already removed much of the practical parsing complexity, decoding one of the forests is extremely quick, and much of the training time is taken with continually reading the forests off disk.", "labels": [], "entities": []}, {"text": "However, the training time for the perceptron is still only around 5 hours for 10 iterations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy on the development data for the averaged perceptron (\u03b2 = 0.004)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999401330947876}]}, {"text": " Table 3: Comparison of the perceptron and log- linear models on the development data", "labels": [], "entities": []}, {"text": " Table 4: Comparison of the perceptron and log- linear models on the test data", "labels": [], "entities": []}, {"text": " Table 5: F-score of the averaged perceptron on the development data for different data orderings (\u03b2 = 0.002)", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9987180233001709}]}, {"text": " Table 6: Comparison of various perceptron models  on the test data", "labels": [], "entities": []}]}