{"title": [{"text": "Domain Adaptation in Statistical Machine Translation with Mixture Modelling *", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6868306547403336}, {"text": "Statistical Machine Translation", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.764468272527059}]}], "abstractContent": [{"text": "Mixture modelling is a standard technique for density estimation, but its use in statistical machine translation (SMT) has just started to be explored.", "labels": [], "entities": [{"text": "Mixture modelling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9630193412303925}, {"text": "density estimation", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7048144340515137}, {"text": "statistical machine translation (SMT)", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.8092563897371292}]}, {"text": "One of the main advantages of this technique is its capability to learn specific probability distributions that better fit subsets of the training dataset.", "labels": [], "entities": []}, {"text": "This feature is even more important in SMT given the difficulties to translate polysemic terms whose semantic depends on the context in which that term appears.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9964120984077454}]}, {"text": "In this paper , we describe a mixture extension of the HMM alignment model and the derivation of Viterbi alignments to feed a state-of-the-art phrase-based system.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.7051824629306793}]}, {"text": "Experiments carried out on the Europarl and News Commentary corpora show the potential interest and limitations of mixture modelling.", "labels": [], "entities": [{"text": "Europarl and News Commentary corpora", "start_pos": 31, "end_pos": 67, "type": "DATASET", "confidence": 0.8463849782943725}, {"text": "mixture modelling", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.8196519613265991}]}], "introductionContent": [{"text": "Mixture modelling is a popular approach for density estimation in many scientific areas (G. J.).", "labels": [], "entities": [{"text": "Mixture modelling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.96358922123909}, {"text": "density estimation", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7188218384981155}, {"text": "G. J.)", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.6496027509371439}]}, {"text": "One of the most interesting properties of mixture modelling is its capability to model multimodal datasets by defining soft partitions on these datasets, and learning specific probability distributions for each partition, that better explains the general data generation process.", "labels": [], "entities": [{"text": "mixture modelling", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8118967413902283}]}, {"text": "* Work supported by the EC (FEDER) and the Spanish MEC under grant TIN2006-15694-CO2-01, the Conseller\u00eda d'Empresa, Universitat iC\u00ec encia -Generalitat Valenciana under contract GV06/252, the Universidad Polit\u00e9cnica de Valencia with ILETA project and Ministerio de Educaci\u00f3n y Ciencia.", "labels": [], "entities": [{"text": "Spanish MEC", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.8563483357429504}, {"text": "TIN2006-15694-CO2-01", "start_pos": 67, "end_pos": 87, "type": "METRIC", "confidence": 0.7860962748527527}]}, {"text": "In Machine Translation (MT), it is common to encounter large parallel corpora devoted to heterogeneous topics.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.8668461084365845}]}, {"text": "These topics usually define sets of topic-specific lexicons that need to be translated taking into the semantic context in which they are found.", "labels": [], "entities": []}, {"text": "This semantic dependency problem could be overcome by learning topic-dependent translation models that capture together the semantic context and the translation process.", "labels": [], "entities": []}, {"text": "However, there have not been until very recently that the application of mixture modelling in SMT has received increasing attention.", "labels": [], "entities": [{"text": "mixture modelling", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.817932665348053}, {"text": "SMT", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.9863858222961426}]}, {"text": "In (), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.", "labels": [], "entities": [{"text": "topical translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.6332294642925262}, {"text": "IBM Model 1", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.8697814345359802}]}, {"text": "These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.", "labels": [], "entities": []}, {"text": "The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an English-Chinese task.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.7751489281654358}]}, {"text": "In (), a mixture extension of IBM model 2 along with a specific dynamicprogramming decoding algorithm were proposed.", "labels": [], "entities": []}, {"text": "This IBM-2 mixture model offers a significant gain in translation quality over the conventional IBM model 2 on a semi-synthetic task.", "labels": [], "entities": [{"text": "translation", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.973747730255127}]}, {"text": "In this work, we present a mixture extension of the well-known HMM alignment model first proposed in (Vogel and others, 1996) and refined in.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.8709793984889984}]}, {"text": "This model possesses appealing properties among which are worth mentioning, the simplicity of the first-order word alignment distribution that can be made independent of absolute positions while taking advantage of the localization phenomenon of word alignment in European languages, and the efficient and exact computation of the E-step and Viterbi alignment by using a dynamic-programming approach.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 110, "end_pos": 124, "type": "TASK", "confidence": 0.6804443448781967}, {"text": "word alignment", "start_pos": 246, "end_pos": 260, "type": "TASK", "confidence": 0.7438296973705292}]}, {"text": "These properties have made this model suitable for extensions () and integration in a phrase-based model) in the past.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data that was employed in the experiments to train the HMM mixture model corresponds to the concatenation of the Spanish-English partitions of the Europarl and the News Commentary corpora.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 151, "end_pos": 159, "type": "DATASET", "confidence": 0.9918660521507263}, {"text": "News Commentary corpora", "start_pos": 168, "end_pos": 191, "type": "DATASET", "confidence": 0.9419733285903931}]}, {"text": "The idea behind this decision was to let the mixture model distinguish which bilingual pairs should contribute to learn a given HMM component in the mixture.", "labels": [], "entities": []}, {"text": "Both corpora were preprocessed as suggested for the baseline system by tokenizing, filtering sentences longer than 40 words and lowercasing.", "labels": [], "entities": []}, {"text": "Regarding the components of the translation system, 5-gram language models were trained on the monolingual version of the corpora for English and Spanish(Es), while phrase-based models with lexicalized reordering model were trained using the Moses toolkit (P., but replacing the Viterbi alignments, usually provided by GIZA++ (, by those of the HMM mixture model with training scheme mix 1 5 H 5 . This configuration was used to translate both test development sets, Europarl and News Commentary.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 467, "end_pos": 475, "type": "DATASET", "confidence": 0.9869385957717896}]}, {"text": "Concerning the weights of the different models, we tuned those weights by minimum error rate training and we employed the same weighting scheme for all the experiments in the same language pair.", "labels": [], "entities": []}, {"text": "Therefore, the same weighting scheme was used over different number of components.", "labels": [], "entities": []}, {"text": "BLEU scores are reported in as a function of the number of components in the HMM mixture model on the preprocessed development test sets of the Europarl and News Commentary corpora.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9875307083129883}, {"text": "Europarl and News Commentary corpora", "start_pos": 144, "end_pos": 180, "type": "DATASET", "confidence": 0.8187845826148987}]}, {"text": "As observed in, if we compare the BLEU scores of the conventional single-component HMM model to those of the HMM mixture model, it seems that there is little or no gain from incorporating more topics into the mixture for the Europarl corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.999218225479126}, {"text": "Europarl corpus", "start_pos": 225, "end_pos": 240, "type": "DATASET", "confidence": 0.9921994209289551}]}, {"text": "However, in, the BLEU scores on the English-Spanish pair significantly increase as the number of components is incremented.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9992114305496216}]}, {"text": "We believe that this is due to the fact that the News Commentary corpus seems to have greater influence on the mixture model than on the single-component model, specializing Viterbi alignments to favour this corpus.", "labels": [], "entities": [{"text": "News Commentary corpus", "start_pos": 49, "end_pos": 71, "type": "DATASET", "confidence": 0.8817599415779114}]}], "tableCaptions": [{"text": " Table 1: BLEU scores on the Europarl development  test data  T  1  2  3  4  En-Es 31.27 31.08 31.12 31.11  Es-En 31.74 31.70 31.80 31.71", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984301924705505}, {"text": "Europarl development  test data  T", "start_pos": 29, "end_pos": 63, "type": "DATASET", "confidence": 0.9518226265907288}]}, {"text": " Table 2: BLEU scores on the News-Commentary  development test data  T  1  2  3  4  En-Es 29.62 30.01 30.17 29.95  Es-En 29.15 29.22 29.11 29.02", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9977864027023315}, {"text": "News-Commentary  development test data  T", "start_pos": 29, "end_pos": 70, "type": "DATASET", "confidence": 0.9378623843193055}]}]}