{"title": [{"text": "Different measurements metrics to evaluate a chatbot system", "labels": [], "entities": []}], "abstractContent": [{"text": "A chatbot is a software system, which can interact or \"chat\" with a human user in natural language such as English.", "labels": [], "entities": []}, {"text": "For the annual Loebner Prize contest, rival chat-bots have been assessed in terms of ability to fool a judge in a restricted chat session.", "labels": [], "entities": [{"text": "Loebner Prize contest", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.5418086846669515}]}, {"text": "We are investigating methods to train and adapt a chatbot to a specific user's language use or application, via a user-supplied training corpus.", "labels": [], "entities": []}, {"text": "We advocate open-ended trials by real users, such as an example Afrikaans chatbot for Afrikaans-speaking researchers and students in South Africa.", "labels": [], "entities": []}, {"text": "This is evaluated in terms of \"glass box\" dialogue efficiency metrics, and \"black box\" dialogue quality metrics and user satisfaction feedback.", "labels": [], "entities": []}, {"text": "The other examples presented in this paper are the Qur'an and the FAQchat prototypes.", "labels": [], "entities": [{"text": "FAQchat", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.8456128239631653}]}, {"text": "Our general conclusion is that evaluation should be adapted to the application and to user needs.", "labels": [], "entities": []}], "introductionContent": [{"text": "\"Before there were computers, we could distinguish persons from non-persons on the basis of an ability to participate in conversations.", "labels": [], "entities": []}, {"text": "But now, we have hybrids operating between person and non persons with whom we can talk in ordinary language.\").", "labels": [], "entities": []}, {"text": "Human machine conversation as a technology integrates different areas where the core is the language, and the computational methodologies facilitate communication between users and computers using natural language.", "labels": [], "entities": []}, {"text": "A related term to machine conversation is the chatbot, a conversational agent that interacts with users turn by turn using natural language.", "labels": [], "entities": []}, {"text": "Different chatbots or human-computer dialogue systems have been developed using text communication such as Eliza, PARRY (Colby 1999b), CONVERSE (Batacharia etc 1999), ALICE . Chatbots have been used in different domains such as: customer service, education, website help, and for fun.", "labels": [], "entities": [{"text": "PARRY", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.9933695793151855}, {"text": "CONVERSE", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9484649300575256}, {"text": "ALICE", "start_pos": 167, "end_pos": 172, "type": "METRIC", "confidence": 0.9877288341522217}]}, {"text": "Different mechanisms are used to evaluate Spoken Dialogue Systems (SLDs), ranging from glass box evaluation that evaluates individual components, to black box evaluation that evaluates the system as a whole.", "labels": [], "entities": [{"text": "Spoken Dialogue Systems (SLDs)", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.7078073521455129}]}, {"text": "For example, glass box evaluation was applied on the (Hirschman 1995) ARPA Spoken Language system, and it shows that the error rate for sentence understanding was much lower than that for sentence recognition.", "labels": [], "entities": [{"text": "ARPA Spoken Language system", "start_pos": 70, "end_pos": 97, "type": "DATASET", "confidence": 0.7804332375526428}, {"text": "error rate", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9755977392196655}, {"text": "sentence understanding", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.7432579100131989}, {"text": "sentence recognition", "start_pos": 188, "end_pos": 208, "type": "TASK", "confidence": 0.7552389800548553}]}, {"text": "On the other hand black box evaluation evaluates the system as a whole based on user satisfaction and acceptance.", "labels": [], "entities": [{"text": "acceptance", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.953830897808075}]}, {"text": "The black box approach evaluates the performance of the system in terms of achieving its task, the cost of achieving the task in terms of time taken and number of turns, and measures the quality of the interaction, normally summarised by the term 'user satisfaction', which indicates whether the user \" gets the information s/he wants, is s/he comfortable with the system, and gets the information within acceptable elapsed time, etc.\".", "labels": [], "entities": []}, {"text": "The Loebner prize 2 competition has been used to evaluate machine conversation chatbots.", "labels": [], "entities": []}, {"text": "The Loebner Prize is a Turing test, which evaluates the ability of the machine to fool people that they are talking to human.", "labels": [], "entities": []}, {"text": "In essence, judges are allowed a short chat (10 to 15 minutes) with each chatbot, and asked to rank them in terms of \"naturalness\".", "labels": [], "entities": []}, {"text": "ALICE In this paper we present other methods to evaluate the chatbot systems.", "labels": [], "entities": [{"text": "ALICE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5613581538200378}]}, {"text": "ALICE chtabot system was used for this purpose, where a Java program has been developed to read from a corpus and convert the text to the AIML format.", "labels": [], "entities": []}, {"text": "The Corpus of Spoken Afrikaans (Korpus Gesproke Afrikaans, KGA), the corpus of the holy book of Islam (Qur'an), and the FAQ of the School of Computing at University of Leeds 3 were used to produce two KGA prototype, the Qur'an prototype and the FAQchat one consequently.", "labels": [], "entities": [{"text": "FAQchat", "start_pos": 245, "end_pos": 252, "type": "DATASET", "confidence": 0.8474370241165161}]}, {"text": "Section 2 presents Loebner Prize contest, section 3 illustrates the ALICE/AIMLE architecture.", "labels": [], "entities": [{"text": "ALICE/AIMLE architecture", "start_pos": 68, "end_pos": 92, "type": "DATASET", "confidence": 0.667724996805191}]}, {"text": "The evaluation techniques of the KGA prototype, the Qur'an prototype, and the FAQchat prototype are discussed in sections 4, 5, and 6 consequently.", "labels": [], "entities": [{"text": "FAQchat prototype", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.8837768733501434}]}, {"text": "The conclusion is presented in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We developed two versions of the ALICE that speaks Afrikaans language, Afrikaana that speaks only Afrikaans and AVRA that speaks English and Afrikaans; this was inspired by our observation that the Korpus Gesproke Afrikaans actually includes some English, as Afrikaans speakers are generally bilingual and \"code-switch\" comfortably.", "labels": [], "entities": []}, {"text": "We mounted prototypes of the chatbots on websites using Pandorabot service , and encouraged open-ended testing and feedback from remote users in South Africa; this allowed us to refine the system more effectively.", "labels": [], "entities": []}, {"text": "We adopted three evaluation metrics: \u2022 Dialogue efficiency in terms of matching type.", "labels": [], "entities": []}, {"text": "\u2022 Dialogue quality metrics based on response type.", "labels": [], "entities": []}, {"text": "\u2022 Users' satisfaction assessment based on an open-ended request for feedback.", "labels": [], "entities": []}, {"text": "In this prototype a parallel corpus of English/Arabic of the holy book of Islam was used, the aim of the Qur'an prototype is to explore the problem of using the Arabic language and of using a text which is not conversational in its nature like the Qur'an.", "labels": [], "entities": []}, {"text": "The Qur'an is composed of 114 soora (chapters), and each soora is composed of different number of verses.", "labels": [], "entities": []}, {"text": "The same learning technique as the KGA prototype were applied, wherein this case if an input was a whole verse, the response will be the next verse of the same soora; or if an input was a question or a statement, the output will be all verses which seems appropriate based on the significant word.", "labels": [], "entities": []}, {"text": "To measure the quality of the answers of the Qur'an chatbot version, the following approach was applied: 1.", "labels": [], "entities": []}, {"text": "Random sentences from Islamic sites were selected and used as inputs of the English/Arabic version of the Qur'an.", "labels": [], "entities": []}, {"text": "To evaluate FAQchat, an interface was built, which has a box to accept the user input, and a button to send this to the system.", "labels": [], "entities": [{"text": "FAQchat", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.6028459668159485}]}, {"text": "The outcomes ap-pear in two columns: one holds the FAQchat answers, and the other holds the Google answers after filtering Google to the FAQ database only.", "labels": [], "entities": [{"text": "FAQchat answers", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.8927532732486725}, {"text": "FAQ database", "start_pos": 137, "end_pos": 149, "type": "DATASET", "confidence": 0.948105663061142}]}, {"text": "Google allows search to be restricted to a given URL, but this still yields all matches from the whole SoC website (http://www.comp.leeds.ac.uk) so a Perl script was required to exclude matches not from the FAQ sub-pages.", "labels": [], "entities": [{"text": "SoC website", "start_pos": 103, "end_pos": 114, "type": "DATASET", "confidence": 0.9424281716346741}, {"text": "FAQ sub-pages", "start_pos": 207, "end_pos": 220, "type": "DATASET", "confidence": 0.9048739969730377}]}, {"text": "An evaluation sheet was prepared which contains 15 information-seeking tasks or questions on a range of different topics related to the FAQ database.", "labels": [], "entities": [{"text": "FAQ database", "start_pos": 136, "end_pos": 148, "type": "DATASET", "confidence": 0.9143777787685394}]}, {"text": "The tasks were suggested by a range of users including SoC staff and research students to cover the three possibilities where the FAQchat could find a direct answer, links to more than one possible answer, and where the FAQchat could not find any answer.", "labels": [], "entities": [{"text": "FAQchat", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.8136194944381714}, {"text": "FAQchat", "start_pos": 220, "end_pos": 227, "type": "DATASET", "confidence": 0.880521833896637}]}, {"text": "In order not to restrict users to these tasks, and not to be biased to specific topics, the evaluation sheet included spaces for users to try 5 additional tasks or questions of their own choosing.", "labels": [], "entities": []}, {"text": "Users were free to decide exactly what input-string to give to FAQchat to find an answer: they were not required to type questions verbatim; users were free to try more than once: if no appropriate answer was found; users could reformulate the query.", "labels": [], "entities": [{"text": "FAQchat", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9207164645195007}]}, {"text": "The evaluation sheet was distributed among 21 members of the staff and students.", "labels": [], "entities": []}, {"text": "Users were asked to try using the system, and state whether they were able to find answers using the FAQchat responses, or using the Google responses; and which of the two they preferred and why.", "labels": [], "entities": [{"text": "FAQchat responses", "start_pos": 101, "end_pos": 118, "type": "DATASET", "confidence": 0.8864824771881104}]}, {"text": "Twenty-one users tried the system; nine members of the staff and the rest were postgraduates.", "labels": [], "entities": []}, {"text": "The analysis was tackled in two directions: the preference and the number of matches found per question and per user.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Response type frequency", "labels": [], "entities": []}, {"text": " Table 2: Proportion of users finding answers", "labels": [], "entities": []}]}