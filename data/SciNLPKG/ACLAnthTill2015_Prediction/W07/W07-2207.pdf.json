{"title": [{"text": "Efficiency in Unification-Based N -Best Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "We extend a recently proposed algorithm for n-best unpacking of parse forests to deal efficiently with (a) Maximum Entropy (ME) parse selection models containing important classes of non-local features, and (b) forests produced by unification grammars containing significant proportions of globally inconsistent analyses.", "labels": [], "entities": []}, {"text": "The new algorithm empirically exhibits a linear relationship between processing time and the number of analyses unpacked at all degrees of ME feature non-locality; in addition, compared with agenda-driven best-first parsing and exhaustive parsing with post-hoc parse selection it leads to improved parsing speed, coverage, and accuracy.", "labels": [], "entities": [{"text": "agenda-driven best-first parsing", "start_pos": 191, "end_pos": 223, "type": "TASK", "confidence": 0.5607784589131674}, {"text": "parsing", "start_pos": 298, "end_pos": 305, "type": "TASK", "confidence": 0.9605592489242554}, {"text": "coverage", "start_pos": 313, "end_pos": 321, "type": "METRIC", "confidence": 0.9919307827949524}, {"text": "accuracy", "start_pos": 327, "end_pos": 335, "type": "METRIC", "confidence": 0.9979793429374695}]}, {"text": "\u2020 1 Background-Motivation Technology for natural language analysis using linguistically precise grammars has matured to a level of coverage and efficiency that enables parsing of large amounts of running text.", "labels": [], "entities": [{"text": "natural language analysis", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.6542668342590332}, {"text": "coverage", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9845256209373474}]}, {"text": "Research groups working within grammatical frameworks like CCG successfully integrated broad-coverage computational grammars with sophisticated statistical parse selection models.", "labels": [], "entities": [{"text": "statistical parse selection", "start_pos": 144, "end_pos": 171, "type": "TASK", "confidence": 0.6684181094169617}]}, {"text": "The former delineate the space of possible analyses , while the latter provide a probability distribu-\u2020 The first author warmly acknowledges the guidance of his PhD advisors, Valia Kordoni and Hans Uszkoreit.", "labels": [], "entities": []}, {"text": "We are grateful to Ulrich Callmeier, Berthold Crysmann, Dan Flickinger, and Erik Velldal for many discussions and their support.", "labels": [], "entities": []}, {"text": "We thank Ron Kaplan, Martin Kay, and Bob Moore for providing insightful information about related approaches, notably the XLE and CLE parsers.", "labels": [], "entities": []}, {"text": "Parse selection approaches for these frameworks often use discrimi-native Maximum Entropy (ME) models, where the probability of each parse tree, given an input string, is estimated on the basis of select properties (called features) of the tree (Abney, 1997; Johnson, Ge-man, Canon, Chi, & Riezler, 1999).", "labels": [], "entities": [{"text": "Parse selection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9019534885883331}]}, {"text": "Such features, in principle, are not restricted in their domain of locality, and enable the parse selection process to take into account properties that extend beyond local contexts (i.e. sub-trees of depth one).", "labels": [], "entities": [{"text": "parse selection", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.9254722893238068}]}, {"text": "There is a trade-off in this setup between the accuracy of the parse selection model, on the one hand, and the efficiency of the search for the best solu-tion(s), on the other hand.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9996107220649719}]}, {"text": "Extending the context size of ME features, within the bounds of available training data, enables increased parse selection accuracy.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 107, "end_pos": 122, "type": "TASK", "confidence": 0.9089199900627136}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9130330681800842}]}, {"text": "However, the interplay of the core parsing algorithm and the probabilistic ranking of alternate (sub-)hypotheses becomes considerably more complex and costly when the feature size exceeds the domain of locality (of depth-one trees) that is characteristic of phrase structure grammar-based formalisms.", "labels": [], "entities": []}, {"text": "One current line of research focuses on finding the best balance between parsing efficiency and parse selection techniques of increasing complexity, aiming to identify the most probable solution(s) with minimal effort.", "labels": [], "entities": [{"text": "parsing", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.976989209651947}, {"text": "parse selection", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.9445570111274719}]}, {"text": "This paper explores a range of techniques, combining a broad-coverage, high-efficiency HPSG parser with a series of parse selection models with varying context size of features.", "labels": [], "entities": [{"text": "HPSG parser", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.6564978957176208}]}, {"text": "We sketch three general scenarios for the integration: (a) a baseline sequential configuration, where all results are enumerated first, and subsequently ranked; (b) an in-terleaved but approximative solution, performing a greedy search for an n-best list of results; and (c) a two-phase approach, where a complete packed for-48", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Coverage on the ERG for different configurations, with  fixed resource consumption limits (of 100k passive edges or 300  seconds). In all cases, up to ten 'best' results were searched,  and Coverage shows the percentage of inputs that succeed to  parse within the available resource. Time shows the end-to-end  processing time for each batch.", "labels": [], "entities": [{"text": "Time", "start_pos": 294, "end_pos": 298, "type": "METRIC", "confidence": 0.9857650399208069}]}, {"text": " Table 3: Parse selection accuracy for various levels of grandpar- enting. The exact match column shows the percentage of cases  in which the correct tree, according to the treebank, was ranked  highest by the model; conversely, the top ten column indicates  how often the correct tree was among the ten top-ranking re- sults.", "labels": [], "entities": [{"text": "Parse selection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6706335544586182}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9781430959701538}]}, {"text": " Table 4: Contrasting the efficiency of various (un-)packing settings in use with ERG on short (top) and medium-length (bottom)  inputs; in each configuration, up to ten trees are extracted. Unification and Copies is the count of top-level FS operations, where  only successful unifications require a subsequent copy (when creating a new edge). Unpack and Total are unpacking and total parse  time, respectively.", "labels": [], "entities": [{"text": "Total", "start_pos": 356, "end_pos": 361, "type": "METRIC", "confidence": 0.9776560068130493}]}]}