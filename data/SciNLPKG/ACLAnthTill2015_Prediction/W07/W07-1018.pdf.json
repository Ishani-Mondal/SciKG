{"title": [{"text": "Interpreting Comparative Constructions in Biomedical Text", "labels": [], "entities": [{"text": "Interpreting Comparative Constructions in Biomedical Text", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.7761302391688029}]}], "abstractContent": [{"text": "We propose a methodology using underspecified semantic interpretation to process comparative constructions in MEDLINE citations, concentrating on two structures that are prevalent in the research literature reporting on clinical trials for drug therapies.", "labels": [], "entities": []}, {"text": "The method exploits an existing semantic processor, SemRep, which constructs predications based on the Unified Medical Language System.", "labels": [], "entities": []}, {"text": "Results of a preliminary evaluation were recall of 70%, precision of 96%, and F-score of 81%.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9998414516448975}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9998288154602051}, {"text": "F-score", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9997702240943909}]}, {"text": "We discuss the generalization of the methodology to other entities such as therapeutic and diagnostic procedures.", "labels": [], "entities": []}, {"text": "The available structures in computable format are potentially useful for interpreting outcome statements in MEDLINE citations.", "labels": [], "entities": [{"text": "interpreting outcome statements in MEDLINE citations", "start_pos": 73, "end_pos": 125, "type": "TASK", "confidence": 0.6925013413031896}]}], "introductionContent": [{"text": "As natural language processing (NLP) is increasingly able to support advanced information management techniques for research in medicine and biology, it is being incrementally improved to provide extended coverage and more accurate results.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.7664172450701395}]}, {"text": "In this paper, we discuss the extension of an existing semantic interpretation system to address comparative structures.", "labels": [], "entities": []}, {"text": "These structures provide away of explicating the characteristics of one entity in terms of a second, thereby enhancing the description of the first.", "labels": [], "entities": []}, {"text": "This phenomenon is important in clinical research literature reporting the results of clinical trials.", "labels": [], "entities": []}, {"text": "In the abstracts of these reports, a treatment for some disease is typically discussed using two types of comparative structures.", "labels": [], "entities": []}, {"text": "The first announces that the (primary) therapy focused on in the study will be compared to some other (secondary) therapy.", "labels": [], "entities": []}, {"text": "A typical example is (1).", "labels": [], "entities": []}, {"text": "(1) Lansoprazole compared with ranitidine for the treatment of nonerosive gastroesophageal reflux disease.", "labels": [], "entities": [{"text": "nonerosive gastroesophageal reflux disease", "start_pos": 63, "end_pos": 105, "type": "TASK", "confidence": 0.6422481909394264}]}, {"text": "An outcome statement (2) often appears near the end of the abstract, asserting results in terms of the relative merits of the primary therapy compared to the secondary.", "labels": [], "entities": []}, {"text": "(2) Lansoprazole is more effective than ranitidine in patients with endoscopically confirmed non-erosive reflux esophagitis.", "labels": [], "entities": []}, {"text": "The processing of comparative expressions such as (1) and (2) was incorporated into an existing system, SemRep, which constructs semantic predications by mapping assertions in biomedical text to the Unified Medical Language System \u00ae (UMLS) \u00ae].", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the effectiveness of the developed methods we created a test set of 300 sentences containing comparative structures.", "labels": [], "entities": []}, {"text": "These were extracted by the second author (who did not participate in the development of the methodology) from 3000 MEDLINE citations published later in date than the citations used to develop the methodology.", "labels": [], "entities": [{"text": "MEDLINE", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.8257272243499756}]}, {"text": "The citations were retrieved with a PubMed query specifying randomized controlled studies and comparative studies on drug therapy.", "labels": [], "entities": []}, {"text": "Sentences containing direct comparisons of the pharmacological actions of two drugs expressed in the target structures (comp1 and comp2) were extracted starting from the latest retrieved citation and continuing until 300 sentences with comparative structures had been examined.", "labels": [], "entities": []}, {"text": "These were annotated with the PubMed ID of the citation, names of two drugs (COMPARED_WITH predication), the scale on which they are compared (SCALE), and the relative position of the primary drug with respect to the secondary (SAME_AS.", "labels": [], "entities": [{"text": "PubMed ID", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.8824494481086731}, {"text": "COMPARED_WITH predication)", "start_pos": 77, "end_pos": 103, "type": "METRIC", "confidence": 0.9284546732902527}, {"text": "SCALE)", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9617313146591187}, {"text": "SAME_AS", "start_pos": 228, "end_pos": 235, "type": "METRIC", "confidence": 0.9307141502698263}]}, {"text": "The test sentences were processed using SemRep and evaluated against the annotated test set.", "labels": [], "entities": []}, {"text": "We then computed recall and precision in several ways: overall for all comparative structures, for comp1 structures only, and for comp2 structures only.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9983548521995544}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9866840243339539}]}, {"text": "To understand how the overall identification of comparatives is influenced by the components of the construction, we also computed recall and precision separately for drug names, scale, and position on scale (SAME_AS, HIGHER_THAN and LOWER_THAN taken together).", "labels": [], "entities": [{"text": "identification of comparatives", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.7639829715092977}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9991682767868042}, {"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9953343272209167}, {"text": "SAME_AS", "start_pos": 209, "end_pos": 216, "type": "METRIC", "confidence": 0.9206131100654602}, {"text": "HIGHER_THAN", "start_pos": 218, "end_pos": 229, "type": "METRIC", "confidence": 0.8579858938852946}, {"text": "LOWER", "start_pos": 234, "end_pos": 239, "type": "METRIC", "confidence": 0.9390618801116943}]}, {"text": "Recall measures the proportion of manually annotated categories that have been correctly identified automatically.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7893080115318298}]}, {"text": "Precision measures what proportion of the automatically annotated categories is correct.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9868525862693787}]}, {"text": "In addition, the overall identification of comparative structures was evaluated using the Fmeasure, which combines recall and precision.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9589800238609314}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9991612434387207}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9969815611839294}]}, {"text": "The F-measure was computed using macro-averaging and micro-averaging.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9691545367240906}]}, {"text": "Macroaveraging was computed over each category first and then averaged over the three categories (drug names, scale, and position on scale).", "labels": [], "entities": []}, {"text": "This approach gives equal weight to each category.", "labels": [], "entities": []}, {"text": "In microaveraging (which gives an equal weight to the performance on each sentence) recall and precision were obtained by summing overall individual sentences.", "labels": [], "entities": [{"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9990274906158447}, {"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.999509334564209}]}, {"text": "Because it is impossible to enumerate all entities and relations which are not drugs, scale, or position we did not use the classification error rate and other metrics that require computing of true negative values.", "labels": [], "entities": [{"text": "classification error rate", "start_pos": 124, "end_pos": 149, "type": "METRIC", "confidence": 0.7307637731234232}]}], "tableCaptions": [{"text": " Table 1. SemRep performance  Task  Recall Precision F-score  Overall  0.70  0.96  0.81  Drug extraction  0.69  0.96  0.81  Comp1  0.74  0.98  0.84  Comp2  0.62  0.92  0.74  Scale  0.62  1.00  0.77  Position on scale 0.62  0.98  0.76", "labels": [], "entities": [{"text": "SemRep", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9141732454299927}, {"text": "Recall Precision F-score  Overall  0.70  0.96  0.81  Drug extraction  0.69  0.96  0.81  Comp1  0.74  0.98  0.84  Comp2  0.62  0.92  0.74  Scale  0.62  1.00", "start_pos": 36, "end_pos": 191, "type": "METRIC", "confidence": 0.7748214161914327}]}]}