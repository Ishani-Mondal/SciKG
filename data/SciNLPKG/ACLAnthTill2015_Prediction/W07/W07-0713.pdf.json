{"title": [{"text": "Human Evaluation of Machine Translation Through Binary System Comparisons", "labels": [], "entities": [{"text": "Human Evaluation of Machine Translation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5201619625091553}]}], "abstractContent": [{"text": "We introduce a novel evaluation scheme for the human evaluation of different machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7211505770683289}]}, {"text": "Our method is based on direct comparison of two sentences at a time by human judges.", "labels": [], "entities": []}, {"text": "These binary judgments are then used to decide between all possible rankings of the systems.", "labels": [], "entities": []}, {"text": "The advantages of this new method are the lower dependency on extensive evaluation guidelines , and a tighter focus on atypical evaluation task, namely the ranking of systems.", "labels": [], "entities": []}, {"text": "Furthermore we argue that machine translation evaluations should be regarded as statistical processes, both for human and automatic evaluation.", "labels": [], "entities": [{"text": "machine translation evaluations", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.8646154801050822}]}, {"text": "We show how confidence ranges for state-of-the-art evaluation measures such as WER and TER can be computed accurately and efficiently without having to resort to Monte Carlo estimates.", "labels": [], "entities": [{"text": "WER", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.7436100244522095}, {"text": "TER", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9804624319076538}]}, {"text": "We give an example of our new evaluation scheme, as well as a comparison with classical automatic and human evaluation on data from a recent international evaluation campaign .", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluation of machine translation (MT) output is a difficult and still open problem.", "labels": [], "entities": [{"text": "Evaluation of machine translation (MT) output", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7954317964613438}]}, {"text": "As in other natural language processing tasks, automatic measures which try to asses the quality of the translation can be computed.", "labels": [], "entities": []}, {"text": "The most widely known are the Word Error Rate (WER), the Position independent word Error Rate (PER), the NIST score) and, especially in recent years, the BLEU score () and the Translation Error Rate (TER)).", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 30, "end_pos": 51, "type": "METRIC", "confidence": 0.8115617136160532}, {"text": "Position independent word Error Rate (PER)", "start_pos": 57, "end_pos": 99, "type": "METRIC", "confidence": 0.8877915516495705}, {"text": "NIST score", "start_pos": 105, "end_pos": 115, "type": "DATASET", "confidence": 0.7271803915500641}, {"text": "BLEU score", "start_pos": 154, "end_pos": 164, "type": "METRIC", "confidence": 0.9835847914218903}, {"text": "Translation Error Rate (TER))", "start_pos": 176, "end_pos": 205, "type": "METRIC", "confidence": 0.8634508550167084}]}, {"text": "All of theses measures compare the system output with one or more gold standard references and produce a numerical value (score or error rate) which measures the similarity between the machine translation and a human produced one.", "labels": [], "entities": [{"text": "numerical value (score or error rate)", "start_pos": 105, "end_pos": 142, "type": "METRIC", "confidence": 0.8077324777841568}]}, {"text": "Once such reference translations are available, the evaluation can be carried out in a quick, efficient and reproducible manner.", "labels": [], "entities": []}, {"text": "However, automatic measures also have big disadvantages;) describes some of them.", "labels": [], "entities": []}, {"text": "A major problem is that a given sentence in one language can have several correct translations in another language and thus, the measure of similarity with one or even a small amount of reference translations will never be flexible enough to truly reflect the wide range of correct possibilities of a translation.", "labels": [], "entities": []}, {"text": "This holds in particular for long sentences and wide-or open-domain tasks like the ones dealt within current MT projects and evaluations.", "labels": [], "entities": [{"text": "MT projects and evaluations", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.8959414511919022}]}, {"text": "If the actual quality of a translation in terms of usefulness for human users is to be evaluated, human evaluation needs to be carried out.", "labels": [], "entities": []}, {"text": "This is however a costly and very time-consuming process.", "labels": [], "entities": []}, {"text": "In this work we present a novel approach to human evaluation that simplifies the task for human judges.", "labels": [], "entities": []}, {"text": "Instead of having to assign numerical scores to each sentence to be evaluated, as is done in current evaluation procedures, human judges choose the best one out of two candidate translations.", "labels": [], "entities": []}, {"text": "We show how this method can be used to rank an arbitrary number of systems and present a detailed analysis of the statistical significance of the method.", "labels": [], "entities": []}], "datasetContent": [{"text": "We can generalize our method to find a ranking of several systems as follows: In this setting, we have a set of n systems.", "labels": [], "entities": []}, {"text": "Furthermore, we have defined an order relationship \"is better than\" between pairs of these systems.", "labels": [], "entities": []}, {"text": "Our goal now is to find an ordering of the systems, such that each system is better than its predecessor.", "labels": [], "entities": []}, {"text": "In other words, this is just a sorting problem -as widely known in computer science.", "labels": [], "entities": [{"text": "sorting problem", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.9081308543682098}]}, {"text": "Several efficient sorting algorithms can be found in the literature.", "labels": [], "entities": []}, {"text": "Generally, the efficiency of sorting algorithms is measured in terms of the number of comparisons carried out.", "labels": [], "entities": [{"text": "sorting algorithms", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9120147824287415}]}, {"text": "State-of-the-art sorting algorithms have a worst-case running time of O(n log n), where n is the number of elements to sort.", "labels": [], "entities": [{"text": "O", "start_pos": 70, "end_pos": 71, "type": "METRIC", "confidence": 0.9968957901000977}]}, {"text": "In our case, because such binary comparisons are very time consuming, we want to minimize the absolute number of comparisons needed.", "labels": [], "entities": []}, {"text": "This minimization should be carried out in the strict sense, not just in an asymptotic manner.", "labels": [], "entities": []}, {"text": "( discusses this issue in detail.", "labels": [], "entities": []}, {"text": "It is relatively straightforward to show that, in the worst case, the minimum number of comparisons to be carried out to sort n elements is at least log n!", "labels": [], "entities": []}, {"text": "(for which n log n is an approximation).", "labels": [], "entities": []}, {"text": "It is not always possible to reach this minimum, however, as was proven e.g. for the case n = 12 in and for n = 13 in. propose an algorithm called merge insertion which comes very close to the theoretical limit.", "labels": [], "entities": [{"text": "merge insertion", "start_pos": 147, "end_pos": 162, "type": "TASK", "confidence": 0.879119485616684}]}, {"text": "This algorithm is sketched in.", "labels": [], "entities": []}, {"text": "There are also algorithms with a better asymptotic runtime, but they only take effect for values of n too large for our purposes (e.g., more than 100).", "labels": [], "entities": []}, {"text": "Thus, using the algorithm from we can obtain the ordering of the systems with a (nearly) optimal number of comparisons.", "labels": [], "entities": []}, {"text": "The evaluation procedure was carried out on the data generated in the second evaluation campaign of the TC-STAR project . The goal of this project is to build a speech-to-speech translation system that can deal with real life data.", "labels": [], "entities": [{"text": "TC-STAR project", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.8013217449188232}, {"text": "speech-to-speech translation", "start_pos": 161, "end_pos": 189, "type": "TASK", "confidence": 0.6985733211040497}]}, {"text": "Three translation directions are dealt within the project: Spanish to English, English to Spanish and Chinese to English.", "labels": [], "entities": []}, {"text": "For the system comparison we concentrated only in the English to Spanish direction.", "labels": [], "entities": []}, {"text": "The corpus for the Spanish-English language pair consists of the official version of the speeches held in the European Parliament Plenary Sessions (EPPS), as available on the web page of the European Parliament.", "labels": [], "entities": []}, {"text": "A more detailed description of the EPPS data can be found in ( ).", "labels": [], "entities": [{"text": "EPPS data", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.95464026927948}]}, {"text": "shows the statistics of the corpus.", "labels": [], "entities": []}, {"text": "A total of 9 different MT systems participated in this condition in the evaluation campaign that took place in February 2006.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9732221961021423}]}, {"text": "We selected five representative systems for our study.", "labels": [], "entities": []}, {"text": "Henceforth we shall refer to these systems as System A through System E. We restricted the number of systems in order to keep the evaluation effort manageable fora first experimental setup to test the feasibility of our method.", "labels": [], "entities": []}, {"text": "The ranking of 5 systems can be carried outwith as few as 7 comparisons, but the ranking of 9 systems requires 19 comparisons.", "labels": [], "entities": []}, {"text": "Seven human bilingual evaluators (6 native speakers and one near-native speaker of Spanish) carried out the evaluation.", "labels": [], "entities": []}, {"text": "100 sentences were randomly chosen and assigned to each of the evaluators for every system comparison, as discussed in Section 3.3.", "labels": [], "entities": []}, {"text": "The results can be seen in    missing to 100 and 700 respectively denote \"same quality\" decisions.", "labels": [], "entities": []}, {"text": "As can be seen from the results, inmost of the cases the judges clearly favor one of the systems.", "labels": [], "entities": []}, {"text": "The most notable exception is found when comparing systems A and C, where a difference of only 3 sentences is clearly not enough to decide between the two.", "labels": [], "entities": []}, {"text": "Thus, the two bottom positions in the final ranking could be swapped.(a) shows the outcome for the binary comparisons separately for each judge, together with an analysis of the statistical significance of the results.", "labels": [], "entities": []}, {"text": "As can be seen, the number of samples would have been too low to show significant results in many experiments (data points in the hatched area).", "labels": [], "entities": []}, {"text": "In some cases, the evaluator even judged better the system which was scored to be worse by the majority of the other evaluators (data points above the bisector).", "labels": [], "entities": []}, {"text": "As shows, \"the only thing better than data is more data\": When we summarize Rover all judges, we see a significant difference (with a confidence of 95%) at all comparisons but two (A vs. C, and E vs. B).", "labels": [], "entities": []}, {"text": "It is interesting to note that exactly these two pairs do not show a significant difference when using a majority vote strategy.", "labels": [], "entities": []}, {"text": "shows also the standard evaluation metrics.", "labels": [], "entities": []}, {"text": "Three BLEU scores are given in this table, the one computed on the whole corpus, the one computed on the set used for standard adequacy and fluency computations and the ones on the set we selected for this task . It can be seen that the BLEU scores are consistent across all data subsets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9963191747665405}, {"text": "BLEU", "start_pos": 237, "end_pos": 241, "type": "METRIC", "confidence": 0.9940152764320374}]}, {"text": "In this case the ranking according to this automatic measure matches exactly the ranking found by our method.", "labels": [], "entities": []}, {"text": "When comparing with the adequacy and fluency scores, however, the ranking of the systems changes considerably: B DE CA.", "labels": [], "entities": [{"text": "B DE CA", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.7956413427988688}]}, {"text": "However, the difference between the three top systems is quite small.", "labels": [], "entities": []}, {"text": "This can be seen in, which shows some automatic and human scores for the five systems in our experiments, along with the estimated 95% confidence range.", "labels": [], "entities": [{"text": "automatic", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9550735950469971}]}, {"text": "The bigger difference is found when comparing the bottom systems, namely System A and System C. While our method produces nearly no difference the adequacy and fluency scores indicate System C as clearly superior to System A. It is worth noting that the both groups use quite different translation approaches (statistical vs. rule-based).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the EPPS Corpus.", "labels": [], "entities": [{"text": "EPPS Corpus", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.9520921111106873}]}, {"text": " Table 2: Result of the binary system comparison.  Numbers of sentences for which each system was  judged better by each evaluator (E1-E7).", "labels": [], "entities": []}, {"text": " Table 3: BLEU scores and Adequacy and Fluency  scores for the different systems and subsets of the  whole test set. BLEU values in %, Adequacy (A)  and Fluency (F) from 1 (worst) to 5 (best).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989953637123108}, {"text": "Adequacy and Fluency  scores", "start_pos": 26, "end_pos": 54, "type": "METRIC", "confidence": 0.8011802434921265}, {"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9990004897117615}, {"text": "Adequacy (A)", "start_pos": 135, "end_pos": 147, "type": "METRIC", "confidence": 0.9181048721075058}, {"text": "Fluency (F)", "start_pos": 153, "end_pos": 164, "type": "METRIC", "confidence": 0.9526236057281494}]}]}