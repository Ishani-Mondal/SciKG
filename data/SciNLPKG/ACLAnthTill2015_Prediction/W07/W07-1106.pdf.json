{"title": [{"text": "Pulling their Weight: Exploiting Syntactic Forms for the Automatic Identification of Idiomatic Expressions in Context", "labels": [], "entities": [{"text": "Automatic Identification of Idiomatic Expressions in Context", "start_pos": 57, "end_pos": 117, "type": "TASK", "confidence": 0.7897669332368034}]}], "abstractContent": [{"text": "Much work on idioms has focused on type identification, i.e., determining whether a sequence of words can form an idiomatic expression.", "labels": [], "entities": [{"text": "type identification", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8526248335838318}]}, {"text": "Since an idiom type often has a literal interpretation as well, token classification of potential idioms in context is critical for NLP.", "labels": [], "entities": [{"text": "token classification of potential idioms in context", "start_pos": 64, "end_pos": 115, "type": "TASK", "confidence": 0.8423830270767212}]}, {"text": "We explore the use of informative prior knowledge about the overall syntactic behaviour of a potentially-idiomatic expression (type-based knowledge) to determine whether an instance of the expression is used idiomatically or literally (token-based knowledge).", "labels": [], "entities": []}, {"text": "We develop unsuper-vised methods for the task, and show that their performance is comparable to that of state-of-the-art supervised techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "Identification of multiword expressions (MWEs), such as car park, make a decision, and kick the bucket, is extremely important for accurate natural language processing (NLP) ().", "labels": [], "entities": [{"text": "Identification of multiword expressions (MWEs)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8581615260669163}, {"text": "accurate natural language processing (NLP)", "start_pos": 131, "end_pos": 173, "type": "TASK", "confidence": 0.6771627366542816}]}, {"text": "Most MWEs need to be treated as single units of meaning, e.g., make a decision roughly means \"decide\".", "labels": [], "entities": []}, {"text": "Nonetheless, the components of an MWE can be separated, making it hard for an NLP system to identify the expression as a whole.", "labels": [], "entities": []}, {"text": "Many researchers have recently developed methods for the automatic acquisition of various properties of MWEs from corpora).", "labels": [], "entities": []}, {"text": "These studies look into properties, such as the collocational behaviour of MWEs, their semantic non-compositionality, and their lexicosyntactic fixedness, in order to distinguish them from similar-on-the-surface literal combinations.", "labels": [], "entities": []}, {"text": "Most of these methods have been aimed at recognizing MWE types; less attention has been paid to the identification of instances (tokens) of MWEs in context.", "labels": [], "entities": [{"text": "MWE types", "start_pos": 53, "end_pos": 62, "type": "TASK", "confidence": 0.7541066408157349}, {"text": "identification of instances (tokens) of MWEs in context", "start_pos": 100, "end_pos": 155, "type": "TASK", "confidence": 0.6505509525537491}]}, {"text": "For example, most such techniques (if successful) would identify make a face as a potential MWE.", "labels": [], "entities": [{"text": "MWE", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.8269495964050293}]}, {"text": "This expression is, however, ambiguous between an idiom, as in The little girl made a funny face at her mother, and a literal combination, as in She made a face on the snowman using a carrot and two buttons.", "labels": [], "entities": []}, {"text": "Despite the common perception that phrases that can be idioms are mainly used in their idiomatic sense, our analysis of \ud97b\udf59\u00bc idioms has shown otherwise.", "labels": [], "entities": []}, {"text": "We found that close to half of these idioms also have a clear literal meaning; and of the expressions with a literal meaning, on average around \ud97b\udf59\u00bc\u00b1 of their usages are literal.", "labels": [], "entities": []}, {"text": "Distinguishing token phrases as MWEs or literal combinations of words is thus essential for NLP applications that require the identification of multiword semantic units, such as semantic parsing and machine translation.", "labels": [], "entities": [{"text": "Distinguishing token phrases as MWEs", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7687744259834289}, {"text": "semantic parsing", "start_pos": 178, "end_pos": 194, "type": "TASK", "confidence": 0.7285907417535782}, {"text": "machine translation", "start_pos": 199, "end_pos": 218, "type": "TASK", "confidence": 0.7610740661621094}]}, {"text": "Recent studies addressing MWE token classification mainly perform the task as one of word sense disambiguation, and draw on the local context of an expression to disambiguate it.", "labels": [], "entities": [{"text": "MWE token classification", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.9369596441586813}, {"text": "word sense disambiguation", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.6211088299751282}]}, {"text": "Such techniques either do not use any information regarding the linguistic properties of MWEs (), or mainly focus on their noncompositionality ().", "labels": [], "entities": []}, {"text": "Pre-vious work on the identification of MWE types, however, has found other properties of MWEs, such as their syntactic fixedness, to be relevant to their identification ().", "labels": [], "entities": [{"text": "identification of MWE types", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.8228154629468918}]}, {"text": "In this paper, we propose techniques that draw on this property to classify individual tokens of a potentially idiomatic phrase as literal or idiomatic.", "labels": [], "entities": []}, {"text": "We also put forward classification techniques that combine such information with evidence from the local context of an MWE.", "labels": [], "entities": []}, {"text": "We explore the hypothesis that informative prior knowledge about the overall syntactic behaviour of an idiomatic expression (type-based knowledge) can be used to determine whether an instance of the expression is used literally or idiomatically (tokenbased knowledge).", "labels": [], "entities": []}, {"text": "Based on this hypothesis, we develop unsupervised methods for token classification, and show that their performance is comparable to that of a standard supervised method.", "labels": [], "entities": [{"text": "token classification", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.94538414478302}]}, {"text": "Many verbs can be combined with one or more of their arguments to form).", "labels": [], "entities": []}, {"text": "Here, we focus on a broadly documented class of idiomatic MWEs that are formed from the combination of a verb with a noun in its direct object position, as in make a face.", "labels": [], "entities": []}, {"text": "In the rest of the paper, we refer to these verb+noun combinations, which are potentially idiomatic, as VNCs.", "labels": [], "entities": []}, {"text": "In Section 2, we propose unsupervised methods that classify a VNC token as an idiomatic or literal usage.", "labels": [], "entities": []}, {"text": "Section 3 describes our experimental setup, including experimental expressions and their annotation.", "labels": [], "entities": []}, {"text": "In Section 4, we present a detailed discussion of our results.", "labels": [], "entities": []}, {"text": "Section 5 compares our work with similar previous studies, and Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use data provided by FS06, which consists of a list of VNCs and their canonical forms.", "labels": [], "entities": [{"text": "FS06", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.9800563454627991}]}, {"text": "From this data, we discarded expressions whose frequency in the British National Corpus 2 (BNC) is lower than \u00be\u00bc, in an effort to make sure that there would be literal and idiomatic usages of each expression.", "labels": [], "entities": [{"text": "British National Corpus 2 (BNC)", "start_pos": 64, "end_pos": 95, "type": "DATASET", "confidence": 0.9569339837346759}]}, {"text": "The frequency cut-off further ensures an accurate estimate of the vectors representing each of the literal and idiomatic meanings of the expression.", "labels": [], "entities": []}, {"text": "We also discarded expressions that were not found in at least one of two dictionaries of idioms (.", "labels": [], "entities": []}, {"text": "This process resulted in the selection of \ud97b\udf59\u00bc candidate expressions.", "labels": [], "entities": []}, {"text": "For each of these \ud97b\udf59\u00bc expressions, \u00bd\u00bc\u00bc sentences containing its usage were randomly selected from the automatically parsed BNC), using the automatic VNC identification method described by FS06.", "labels": [], "entities": [{"text": "BNC", "start_pos": 122, "end_pos": 125, "type": "DATASET", "confidence": 0.6889070272445679}, {"text": "FS06", "start_pos": 187, "end_pos": 191, "type": "DATASET", "confidence": 0.9842473268508911}]}, {"text": "For an expression which occurs less than \u00bd\u00bc\u00bc times in the BNC, all of its usages were extracted.", "labels": [], "entities": [{"text": "BNC", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.9753769040107727}]}, {"text": "Our primary judge, a native English speaker and an author of this paper, then annotated each use of each candidate expression as one of literal, idiomatic, or unknown.", "labels": [], "entities": []}, {"text": "When annotating a token, the judge had access to only the sentence in which it occurred, and not the surrounding sentences.", "labels": [], "entities": []}, {"text": "If this context was insufficient to determine the class of the expression, the judge assigned the unknown label.", "labels": [], "entities": []}, {"text": "Idiomaticity is not a binary property, rather it is known to fall on a continuum from completely semantically transparent, or literal, to entirely opaque, or idiomatic.", "labels": [], "entities": []}, {"text": "The human annotators were required to pick the label, literal or idiomatic, that best fit the 2 http://www.natcorp.ox.ac.uk usage in their judgment; they were not to use the unknown label for intermediate cases.", "labels": [], "entities": []}, {"text": "Figurative extensions of literal meanings were classified as literal if their overall meaning was judged to be fairly transparent, as in You turn right when we hit the road at the end of this track (taken from the BNC).", "labels": [], "entities": [{"text": "BNC)", "start_pos": 214, "end_pos": 218, "type": "DATASET", "confidence": 0.9194145202636719}]}, {"text": "Sometimes an idiomatic usage, such as had words in I was in a bad mood, and he kept pestering me, so we had words, is somewhat directly related to its literal meaning, which is not the case for more semantically opaque idioms such as hit the roof.", "labels": [], "entities": []}, {"text": "The above sentence was classified as idiomatic since the idiomatic meaning is much more salient than the literal meaning.", "labels": [], "entities": []}, {"text": "Based on the primary judge's annotations, we re- A second human judge, also a native Englishspeaking author of this paper, then annotated DEV and TEST.", "labels": [], "entities": [{"text": "DEV", "start_pos": 138, "end_pos": 141, "type": "DATASET", "confidence": 0.5198096632957458}, {"text": "TEST", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.8980165719985962}]}, {"text": "The observed agreement and unweighted kappa score on TEST were \ud97b\udf59\ud97b\udf59\u00b1 and \u00bc\ud97b\udf59\ud97b\udf59\u00be respectively.", "labels": [], "entities": [{"text": "agreement", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9992269277572632}, {"text": "kappa score", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.83417609333992}, {"text": "TEST", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.7188369035720825}, {"text": "\ud97b\udf59\ud97b\udf59\u00b1", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.951744556427002}]}, {"text": "The judges discussed tokens on which they disagreed to achieve a consensus annotation.", "labels": [], "entities": []}, {"text": "Final annotations were generated by removing tokens that received the unknown label as the consensus annotation, leaving DEV and TEST with a total of \ud97b\udf59\ud97b\udf59\u00bf and \ud97b\udf59\u00bc\u00bc tokens, and an average of \ud97b\udf59\u00bd and \ud97b\udf59\u00bf tokens per expression, respectively.", "labels": [], "entities": [{"text": "DEV", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.8019121289253235}, {"text": "TEST", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.701370358467102}]}, {"text": "Our baseline for comparison is that of always predicting an idiomatic label, the most frequent class in our development data.", "labels": [], "entities": []}, {"text": "We also compare our unsupervised methods against the supervised method proposed by.", "labels": [], "entities": []}, {"text": "In this study, co-occurrence vectors for the tokens were formed from uses of a German idiom manually annotated as literal or idiomatic.", "labels": [], "entities": []}, {"text": "Tokens were classified in a leave-one-out methodology using \ud97b\udf59-nearest neighbours, with \ud97b\udf59 \ud97b\udf59 \u00bd . We report results using this method (\u00bdAEAE) as well as one which considers a token's \ud97b\udf59 nearest neighbours (\ud97b\udf59AEAE).", "labels": [], "entities": [{"text": "AEAE", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9977871179580688}, {"text": "AEAE", "start_pos": 203, "end_pos": 207, "type": "METRIC", "confidence": 0.9982736110687256}]}, {"text": "In all cases, we report the accuracy macro-averaged across the experimental expressions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.999488115310669}]}, {"text": "In Section 4.1, we discuss the overall performance of our proposed unsupervised methods.", "labels": [], "entities": []}, {"text": "Section 4.2 explores possible causes of the differences observed in the performance of the methods.", "labels": [], "entities": []}, {"text": "We examine our estimated idiomatic and literal vectors, and compare them with the actual vectors calculated from We also considered \u00bd\u00bc and \u00be\u00bc word windows on either side of the target expression, but experiments on development data indicated that using the sentence as a window performed better.", "labels": [], "entities": []}, {"text": "We employed singular value decomposition) to reduce the dimensionality of the co-occurrence vectors.", "labels": [], "entities": []}, {"text": "This had a negative effect on the results, likely because information about determiners, which occur frequently with many expressions, is lost in the dimensionality reduction..1 shows the macro-averaged accuracy on TEST of our three unsupervised methods, as well as that of the baseline and the two supervised methods for comparison (see Section 3.3).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 203, "end_pos": 211, "type": "METRIC", "confidence": 0.9710918664932251}, {"text": "TEST", "start_pos": 215, "end_pos": 219, "type": "METRIC", "confidence": 0.8095733523368835}]}, {"text": "The best supervised performance and the best unsupervised performance are indicated in boldface.", "labels": [], "entities": []}, {"text": "As the table shows, all three unsupervised methods outperform the baseline, confirming that the canonical forms of an expression, and local context, are both informative in distinguishing literal and idiomatic instances of the expression.", "labels": [], "entities": []}], "tableCaptions": []}