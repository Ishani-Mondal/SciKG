{"title": [], "abstractContent": [{"text": "We investigate away to partially automate corpus annotation for named entity recognition , by requiring only binary decisions from an annotator.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.6414158542950948}]}, {"text": "Our approach is based on a linear sequence model trained using a k-best MIRA learning algorithm.", "labels": [], "entities": []}, {"text": "We ask an an-notator to decide whether each mention produced by a high recall tagger is a true mention or a false positive.", "labels": [], "entities": []}, {"text": "We conclude that our approach can reduce the effort of extending a seed training corpus by up to 58%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semi-automated text annotation has been the subject of several previous studies.", "labels": [], "entities": [{"text": "text annotation", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.6759620308876038}]}, {"text": "Typically, a human annotator corrects the output of an automatic system.", "labels": [], "entities": []}, {"text": "The idea behind our approach is to start annotation manually and to partially automate the process in the later stages.", "labels": [], "entities": []}, {"text": "We assume that some data has already been manually tagged and use it to train a tagger specifically for high recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.996429979801178}]}, {"text": "We then run this tagger on the rest of our corpus and ask an annotator to filter the list of suggested gene names.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the model and learning algorithm.", "labels": [], "entities": []}, {"text": "Section 3 relates our approach to previous work.", "labels": [], "entities": []}, {"text": "Section 4 describes our experiments and Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now evaluate to what extent our semi-automated annotation framework can be useful, and how much effort it requires.", "labels": [], "entities": []}, {"text": "For both questions we compare semi-automatic to fully manual annotation.", "labels": [], "entities": []}, {"text": "In our first set of experiments, we measured the usefulness of semi-automatically annotated corpora for training a gene mention tagger.", "labels": [], "entities": [{"text": "gene mention tagger", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.6130930781364441}]}, {"text": "In the second set of experiments, we measured the annotation effort for gene mentions with the standard fully manual method and with the semi-automated methods.", "labels": [], "entities": [{"text": "gene mentions", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.6949863880872726}]}], "tableCaptions": []}