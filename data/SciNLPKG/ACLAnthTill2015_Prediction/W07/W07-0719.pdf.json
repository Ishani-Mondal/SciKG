{"title": [{"text": "Context-aware Discriminative Phrase Selection for Statistical Machine Translation", "labels": [], "entities": [{"text": "Context-aware Discriminative Phrase Selection", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7250751331448555}, {"text": "Statistical Machine Translation", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.842659056186676}]}], "abstractContent": [{"text": "In this work we revise the application of discriminative learning to the problem of phrase selection in Statistical Machine Translation.", "labels": [], "entities": [{"text": "phrase selection", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.7960874438285828}, {"text": "Statistical Machine Translation", "start_pos": 104, "end_pos": 135, "type": "TASK", "confidence": 0.8117854992548624}]}, {"text": "Inspired by common techniques used in Word Sense Disambiguation, we train classifiers based on local context to predict possible phrase translations.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6968386471271515}, {"text": "predict possible phrase translations", "start_pos": 112, "end_pos": 148, "type": "TASK", "confidence": 0.61758953332901}]}, {"text": "Our work extends that of Vickrey et al.", "labels": [], "entities": []}, {"text": "(2005) in two main aspects.", "labels": [], "entities": []}, {"text": "First, we move from word translation to phrase translation.", "labels": [], "entities": [{"text": "word translation", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7473377287387848}, {"text": "phrase translation", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7547293305397034}]}, {"text": "Second, we move from the 'blank-filling' task to the 'full translation' task.", "labels": [], "entities": [{"text": "full translation' task", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7533440962433815}]}, {"text": "We report results on a set of highly frequent source phrases, obtaining a significant improvement, specially with respect to adequacy, according to a rigorous process of manual evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Translations tables in Phrase-based Statistical Machine Translation (SMT) are often built on the basis of Maximum-likelihood Estimation (MLE), being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored (.", "labels": [], "entities": [{"text": "Phrase-based Statistical Machine Translation (SMT)", "start_pos": 23, "end_pos": 73, "type": "TASK", "confidence": 0.7246133429663522}]}, {"text": "In this work, inspired by state-of-the-art Word Sense Disambiguation (WSD) techniques, we suggest using Discriminative Phrase Translation (DPT) models which take into account a wider feature context.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.759198933839798}, {"text": "Discriminative Phrase Translation (DPT)", "start_pos": 104, "end_pos": 143, "type": "TASK", "confidence": 0.841994841893514}]}, {"text": "Following the approach by, we deal with the 'phrase translation' problem as a classification problem.", "labels": [], "entities": [{"text": "phrase translation'", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.843047559261322}]}, {"text": "We use Support Vector Machines (SVMs) to predict phrase translations in the context of the whole source sentence.", "labels": [], "entities": [{"text": "predict phrase translations", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.6299073398113251}]}, {"text": "We extend the work by in two main aspects.", "labels": [], "entities": []}, {"text": "First, we move from 'word translation' to 'phrase translation'.", "labels": [], "entities": [{"text": "word translation", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.7423862218856812}, {"text": "phrase translation", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7150081545114517}]}, {"text": "Second, we move from the 'blank-filling' task to the 'full translation' task.", "labels": [], "entities": [{"text": "full translation' task", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7533440962433815}]}, {"text": "Our approach is fully described in Section 2.", "labels": [], "entities": []}, {"text": "We apply it to the Spanish-to-English translation of European Parliament Proceedings.", "labels": [], "entities": [{"text": "European Parliament Proceedings", "start_pos": 53, "end_pos": 84, "type": "DATASET", "confidence": 0.5936437348524729}]}, {"text": "In Section 3, prior to considering the 'full translation' task, we analyze the impact of using DPT models for the isolated 'phrase translation' task.", "labels": [], "entities": [{"text": "full translation' task", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.8206662237644196}, {"text": "phrase translation' task", "start_pos": 124, "end_pos": 148, "type": "TASK", "confidence": 0.8314188495278358}]}, {"text": "In spite of working on a very specific domain, a large room for improvement, coherent with WSD performance, and results by, is predicted.", "labels": [], "entities": [{"text": "WSD", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.8737034201622009}]}, {"text": "Then, in Section 4, we tackle the full translation task.", "labels": [], "entities": [{"text": "translation", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.9796273708343506}]}, {"text": "DPT models are integrated in a 'soft' manner, by making them available to the decoder so they can fully interact with other models.", "labels": [], "entities": []}, {"text": "Results using a reduced set of highly frequent source phrases show a significant improvement, according to several automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "Interestingly, the BLEU metric) is notable to reflect this improvement.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9721897542476654}]}, {"text": "Through a rigorous process of manual evaluation we have verified the gain.", "labels": [], "entities": []}, {"text": "We have also observed that it is mainly related to adequacy.", "labels": [], "entities": []}, {"text": "These results confirm that better phrase translation probabilities maybe helpful for the full translation task.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7524187862873077}, {"text": "full translation task", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.7316190203030905}]}, {"text": "However, the fact that no gain in fluency is reported indicates that the integration of these probabilities into the statistical framework requires further study.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluating the effects of using DPT predictions, directed towards a better word selection, in the full translation task presents two serious difficulties.", "labels": [], "entities": [{"text": "word selection", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.726342111825943}, {"text": "full translation task", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.7157528797785441}]}, {"text": "In first place, the actual room for improvement caused by a better translation modeling is smaller than estimated in Section 3.", "labels": [], "entities": [{"text": "translation modeling", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.9313507676124573}]}, {"text": "This is mainly due to the SMT architecture itself which relies on a search over a probability space in which several models cooperate.", "labels": [], "entities": [{"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.992784857749939}]}, {"text": "For instance, in many cases errors caused by a poor translation modeling maybe corrected by the language model.", "labels": [], "entities": []}, {"text": "Ina recent study, found that only around 25% of the errors are related to word selection.", "labels": [], "entities": [{"text": "word selection", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.8078505396842957}]}, {"text": "In half of these cases errors are caused by a wrong word sense disambiguation, and in the other half the word sense is correct but the lexical choice is wrong.", "labels": [], "entities": []}, {"text": "In second place, most conventional automatic evaluation metrics have not been designed for this purpose.", "labels": [], "entities": []}, {"text": "For instance, metrics such as BLEU) tend to favour longer n-gram matchings, and are, thus, biased towards word ordering.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9953504800796509}, {"text": "word ordering", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.7313399016857147}]}, {"text": "We might find better suited metrics, such as METEOR (, which is oriented towards word selection 8 . However, anew problem arises.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9674189686775208}]}, {"text": "Because different metrics are biased towards different aspects of quality, scores conferred by different metrics are often controversial.", "labels": [], "entities": []}, {"text": "In order to cope with evaluation difficulties we have applied several complementary actions: 1.", "labels": [], "entities": []}, {"text": "Based on the results from Section 3, we focus on a reduced set of 41 very promising phrases trained on more than 50,000 examples.", "labels": [], "entities": []}, {"text": "This set covers 25.8% of the words in the test set, and exhibits a potential absolute accuracy gain around 11% (See).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9132346510887146}]}, {"text": "2. With the purpose of evaluating the changes related only to this small set of very promising phrases, we introduce anew measure, Apt, which computes \"phrase translation\" accuracy fora given list of source phrases.", "labels": [], "entities": [{"text": "Apt", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.99891197681427}, {"text": "phrase translation\"", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.7678361733754476}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.8142470717430115}]}, {"text": "For every test case, Apt counts the proportion of phrases from the list appearing in the source sentence which have a valid 9 translation both in the target sentence and in any of the reference translations.", "labels": [], "entities": [{"text": "Apt", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9923843741416931}]}, {"text": "In fact, because in general source-totarget alignments are not known, Apt calculates an approximate 10 solution.", "labels": [], "entities": []}, {"text": "3. We evaluate overall MT quality on the basis of 'Human Likeness'.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9934592247009277}]}, {"text": "In particular, we use the QUEEN 11 meta-measure from the QARLA Framework ().", "labels": [], "entities": [{"text": "QUEEN 11 meta-measure", "start_pos": 26, "end_pos": 47, "type": "DATASET", "confidence": 0.8612358172734579}, {"text": "QARLA Framework", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.881508857011795}]}, {"text": "QUEEN operates under the assumption that a good translation must be similar to all human references according to all metrics.", "labels": [], "entities": [{"text": "QUEEN", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8084867596626282}]}, {"text": "Given a set of automatic translations A, a set of similarity metrics X, and a set of human references R, QUEEN is defined as the probability, over R \u00d7 R \u00d7 R, that for every metric in X the automatic translation a is more similar to a reference r than two other references rand r to each other.", "labels": [], "entities": [{"text": "QUEEN", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.997681736946106}]}, {"text": "Formally: QUEEN X,R (a) = P rob(\u2200x \u2208 X : x(a, r) \u2265 x(r , r )) QUEEN captures the features that are common to all human references, rewarding those automatic translations which share them, and penalizing those which do not.", "labels": [], "entities": []}, {"text": "Thus, QUEEN provides a robust means of combining several metrics into a single measure of quality.", "labels": [], "entities": [{"text": "QUEEN", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.6435390114784241}]}], "tableCaptions": [{"text": " Table 1: \"Phrase Translation\" Accuracy (test set).", "labels": [], "entities": [{"text": "Phrase Translation", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8674875199794769}, {"text": "Accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9802708625793457}]}, {"text": " Table 2: Automatic evaluation of the 'full translation' results on the test set.", "labels": [], "entities": []}, {"text": " Table 3: Manual evaluation of the 'full translation'  results on the test set. Counts on the number of  translation cases for which the 'MLE' system is bet- ter than (>), equal to (=), or worse than (<) the  'DPT' system, with respect to adequacy, fluency,  and overall MT quality, are presented.", "labels": [], "entities": [{"text": "MT", "start_pos": 271, "end_pos": 273, "type": "TASK", "confidence": 0.9243301749229431}]}]}