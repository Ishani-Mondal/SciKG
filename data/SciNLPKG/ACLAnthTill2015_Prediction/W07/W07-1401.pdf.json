{"title": [{"text": "The Third PASCAL Recognizing Textual Entailment Challenge", "labels": [], "entities": [{"text": "PASCAL Recognizing Textual Entailment", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.8190734684467316}]}], "abstractContent": [{"text": "This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems.", "labels": [], "entities": [{"text": "PASCAL Recognising Textual Entailment Challenge (RTE-3)", "start_pos": 30, "end_pos": 85, "type": "TASK", "confidence": 0.7982210740447044}]}, {"text": "In creating this year's dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios.", "labels": [], "entities": []}, {"text": "Additionally , a pool of resources was offered so that the participants could share common tools.", "labels": [], "entities": []}, {"text": "A pilot task was also setup, aimed at differentiating unknown en-tailments from identified contradictions and providing justifications for overall system decisions.", "labels": [], "entities": []}, {"text": "26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Each pair of the dataset was judged by three annotators.", "labels": [], "entities": []}, {"text": "As in previous challenges, pairs on which the annotators disagreed were filtered-out.", "labels": [], "entities": []}, {"text": "On the test set, the average agreement between each pair of annotators who shared at least 100 examples was 87.8%, with an average Kappa level of 0.75, regarded as substantial agreement according to.", "labels": [], "entities": [{"text": "agreement", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9664710164070129}, {"text": "Kappa level", "start_pos": 131, "end_pos": 142, "type": "METRIC", "confidence": 0.9628191590309143}]}, {"text": "19.2 % of the pairs in the dataset were removed from the test set due to disagreement.", "labels": [], "entities": []}, {"text": "The disagreement was generally due to the fact that the h was more specific than the t, for example because it contained more information, or made an absolute assertion where t proposed only a personal opinion.", "labels": [], "entities": []}, {"text": "In addition, 9.4 % of the remaining pairs were discarded, as they seemed controversial, too difficult, or too similar when compared to other pairs.", "labels": [], "entities": []}, {"text": "As far as the texts extracted from the web are concerned, spelling and punctuation errors were sometimes fixed by the annotators, but no major change was allowed, so that the language could be grammatically and stylistically imperfect.", "labels": [], "entities": []}, {"text": "The hypotheses were finally double-checked by a native English speaker.", "labels": [], "entities": []}, {"text": "The evaluation of all runs submitted in RTE-3 was automatic.", "labels": [], "entities": [{"text": "RTE-3", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.7695081233978271}]}, {"text": "The judgments (classifications) returned by the system were compared to the Gold Standard compiled by the human assessors.", "labels": [], "entities": [{"text": "Gold Standard compiled", "start_pos": 76, "end_pos": 98, "type": "DATASET", "confidence": 0.9108472466468811}]}, {"text": "The main evaluation measure was accuracy, i.e. the percentage of matching judgments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9995785355567932}]}, {"text": "For systems that provided a confidence-ranked list of the pairs, in addition to the YES/NO judgment, an Average Precision measure was also computed.", "labels": [], "entities": [{"text": "YES/NO judgment", "start_pos": 84, "end_pos": 99, "type": "METRIC", "confidence": 0.8453776985406876}, {"text": "Average Precision measure", "start_pos": 104, "end_pos": 129, "type": "METRIC", "confidence": 0.8453657627105713}]}, {"text": "This measure evaluates the ability of systems to rank all the T-H pairs in the test set according to their entailment confidence (in decreasing order from the most certain entailment to the least certain).", "labels": [], "entities": []}, {"text": "In other words, the more the system was confident that t entails h, the higher was the ranking of the pair.", "labels": [], "entities": []}, {"text": "A perfect ranking would have placed all the positive pairs (for which the entailment holds) before all the negative ones, yielding an average precision value of 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9970723390579224}]}], "tableCaptions": [{"text": " Table 2: Submission results and components of the systems.  .", "labels": [], "entities": []}]}