{"title": [{"text": "Dependency Parsing with Second-Order Feature Maps and Annotated Semantic Information", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7964606285095215}]}], "abstractContent": [{"text": "This paper investigates new design options for the feature space of a dependency parser.", "labels": [], "entities": []}, {"text": "We focus on one of the simplest and most efficient architectures, based on a determin-istic shift-reduce algorithm, trained with the perceptron.", "labels": [], "entities": []}, {"text": "By adopting second-order feature maps, the primal form of the perceptron produces models with comparable accuracy to more complex architectures, with no need for approximations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9954909682273865}]}, {"text": "Further gains inaccuracy are obtained by designing features for parsing extracted from semantic annotations generated by a tagger.", "labels": [], "entities": [{"text": "parsing extracted from semantic annotations generated by a tagger", "start_pos": 64, "end_pos": 129, "type": "TASK", "confidence": 0.7718448903825548}]}, {"text": "We provide experimental evaluations on the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.9928016364574432}]}], "introductionContent": [{"text": "A dependency tree represents a sentence as a labeled directed graph encoding syntactic and semantic information.", "labels": [], "entities": []}, {"text": "The labels on the arcs can represent basic grammatical relations such as \"subject\" and \"object\".", "labels": [], "entities": []}, {"text": "Dependency trees capture grammatical structures that can be useful in several language processing tasks such as information extraction) and machine translation).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.7872490882873535}, {"text": "machine translation", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.7821201086044312}]}, {"text": "Dependency treebanks are becoming available in many languages, and several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8097771406173706}, {"text": "CoNLL 2006 and 2007 shared tasks", "start_pos": 157, "end_pos": 189, "type": "DATASET", "confidence": 0.9115064342816671}]}, {"text": "Dependency parsing is simpler than constituency parsing, since dependency trees do not have extra non-terminal nodes and there is no need fora grammar to generate them.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8727976381778717}, {"text": "constituency parsing", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.8875426650047302}]}, {"text": "Approaches to dependency parsing either generate such trees by considering all possible spanning trees (), or build a singletree by means of shift-reduce parsing actions.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7556942403316498}]}, {"text": "Deterministic dependency parsers which run in linear time have also been developed ().", "labels": [], "entities": [{"text": "Deterministic dependency parsers", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6644972264766693}]}, {"text": "These parsers process the sentence sequentially, hence their efficiency makes them suitable for processing large amounts of text, as required, for example, in information retrieval applications.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 159, "end_pos": 180, "type": "TASK", "confidence": 0.7703242897987366}]}, {"text": "Recent work on dependency parsing has highlighted the benefits of using rich feature sets and high-order modeling.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8530574142932892}]}, {"text": "Yamada and Matsumoto showed that learning an SVM model in the dual space with higher-degree polynomial kernel functions improves significantly the parser's accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9978371262550354}]}, {"text": "have shown that incorporating second order features relating to adjacent edge pairs improves the accuracy of maximum spanning tree parsers (MST).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.999045193195343}, {"text": "maximum spanning tree parsers (MST)", "start_pos": 109, "end_pos": 144, "type": "TASK", "confidence": 0.6941690146923065}]}, {"text": "In the SVMbased approach, if the training data is large, it is not feasible to train a single model.", "labels": [], "entities": []}, {"text": "Rather, Yamada and Matsumoto (see also)) partition the training data in different sets, on the basis of Partof-Speech, then train one dual SVM model per set.", "labels": [], "entities": []}, {"text": "While this approach simplifies the learning task it makes the parser more sensitive to the error rate of the POS tagger.", "labels": [], "entities": []}, {"text": "The second-order MST algorithm has cubic time complexity.", "labels": [], "entities": [{"text": "MST", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9745464324951172}]}, {"text": "For non-projective languages the algorithm is NP-hard and introduce an approximate algorithm to handle such cases.", "labels": [], "entities": []}, {"text": "In this paper we extend shift reduce parsing with second-order feature maps which explicitly repre-sent all feature pairs.", "labels": [], "entities": [{"text": "shift reduce parsing", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.5925887624422709}]}, {"text": "Also the augmented feature sets impose additional computational costs.", "labels": [], "entities": []}, {"text": "However, excellent efficiency/accuracy trade-off is achieved by using the perceptron algorithm, without the need to resort to approximations, producing high-accuracy classifiers based on a single model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9985413551330566}]}, {"text": "We also evaluate a novel set of features for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.9787036776542664}]}, {"text": "Recently various forms of shallow semantic processing have been investigated such as namedentity recognition (NER), semantic role labeling (SRL) and relation extraction.", "labels": [], "entities": [{"text": "namedentity recognition (NER)", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.8239267885684967}, {"text": "semantic role labeling (SRL)", "start_pos": 116, "end_pos": 144, "type": "TASK", "confidence": 0.7770502169926962}, {"text": "relation extraction", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.902471661567688}]}, {"text": "Syntactic parsing can provide useful features for these tasks; e.g., show that full parsing is effective for semantic role labeling (see also related approaches evaluated within the CoNNL 2005 shared task ().", "labels": [], "entities": [{"text": "Syntactic parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7093389928340912}, {"text": "semantic role labeling", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.6896519362926483}, {"text": "CoNNL 2005 shared task", "start_pos": 182, "end_pos": 204, "type": "DATASET", "confidence": 0.899024248123169}]}, {"text": "However, no evidence has been provided so far that annotated semantic information can be leveraged for improving parser performance.", "labels": [], "entities": []}, {"text": "We report experiments showing that adding features extracted by an entity tagger improves the accuracy of a dependency parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9988263249397278}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 4. Results of the different models on WSJ section 23 using the CoNLL scores Labeled attachment score (LAS),  Unlabeled attachment score (UAS), and Label accuracy score (LAC). The column labeled \"Imp\" reports the improve- ment in terms of relative error reduction with respect to the BASE model for the UAS score. In bold the best results.", "labels": [], "entities": [{"text": "WSJ section 23", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.9217297236124674}, {"text": "CoNLL scores Labeled attachment score (LAS)", "start_pos": 70, "end_pos": 113, "type": "METRIC", "confidence": 0.7587528899312019}, {"text": "Unlabeled attachment score (UAS)", "start_pos": 116, "end_pos": 148, "type": "METRIC", "confidence": 0.911851704120636}, {"text": "Label accuracy score (LAC)", "start_pos": 154, "end_pos": 180, "type": "METRIC", "confidence": 0.9103389879067739}, {"text": "Imp", "start_pos": 202, "end_pos": 205, "type": "METRIC", "confidence": 0.9541661739349365}, {"text": "error reduction", "start_pos": 254, "end_pos": 269, "type": "METRIC", "confidence": 0.8479160666465759}, {"text": "BASE", "start_pos": 290, "end_pos": 294, "type": "METRIC", "confidence": 0.9652075171470642}, {"text": "UAS score", "start_pos": 309, "end_pos": 318, "type": "DATASET", "confidence": 0.7245046198368073}]}, {"text": " Table 5. Comparison of main results on the Penn Tree- bank dataset.", "labels": [], "entities": [{"text": "Penn Tree- bank dataset", "start_pos": 44, "end_pos": 67, "type": "DATASET", "confidence": 0.9906104445457459}]}, {"text": " Table 6. Parsing times for the CoNNL 2007 English and  Chinese datasets for MST and DeSR.", "labels": [], "entities": [{"text": "CoNNL 2007 English and  Chinese datasets", "start_pos": 32, "end_pos": 72, "type": "DATASET", "confidence": 0.9595502316951752}, {"text": "MST", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.8474825620651245}, {"text": "DeSR", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.825530469417572}]}]}