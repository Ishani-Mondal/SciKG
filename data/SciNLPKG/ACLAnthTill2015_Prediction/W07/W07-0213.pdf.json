{"title": [{"text": "TextGraphs-2: Graph-Based Algorithms for Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "Degree distributions for word forms co-occurrences for large Russian text collections are obtained.", "labels": [], "entities": []}, {"text": "Two power laws fit the distributions pretty good, thus supporting Dorogovtsev-Mendes model for Russian.", "labels": [], "entities": []}, {"text": "Few different Russian text collections were studied, and statistical errors are shown to be negligible.", "labels": [], "entities": []}, {"text": "The model exponents for Russian are found to differ from those for English, the difference probably being due to the difference in the collections structure.", "labels": [], "entities": []}, {"text": "On the contrary, the estimated size of the supposed kernel lexicon appeared to be almost the same for the both languages, thus supporting the idea of importance of word forms fora perceptual lexicon of a human.", "labels": [], "entities": []}], "introductionContent": [{"text": "Few years ago draw the attention of researchers to the fact that the lexicon of a big corpus (British National Corpus -BNC -in the case) most probably consists of two major components: a compact kernel lexicon of about 10 3 -10 4 words, and a cloud of all other words.", "labels": [], "entities": [{"text": "British National Corpus -BNC", "start_pos": 94, "end_pos": 122, "type": "DATASET", "confidence": 0.9648948788642884}]}, {"text": "Ferrer and Sol\u00e9 studied word co-occurrence in BNC in (2001b).", "labels": [], "entities": [{"text": "BNC in (2001b)", "start_pos": 46, "end_pos": 60, "type": "DATASET", "confidence": 0.8155867457389832}]}, {"text": "Two word forms 1 in BNC were considered as \"interacting\" when they appeared in the same sentence and the words' distance didn't exceed 2.", "labels": [], "entities": [{"text": "BNC", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.8533688187599182}]}, {"text": "treated also some other no-1 Strictly speaking, word forms, not words.", "labels": [], "entities": []}, {"text": "tions of word interaction, but the results obtained don't differ qualitatively.", "labels": [], "entities": []}, {"text": "The interacting words form a graph, where the vertices are the words themselves, and the edges are the words' cooccurrences.", "labels": [], "entities": []}, {"text": "The fact of the collocation considered to be important, not the number of collocations of the same pair of words.", "labels": [], "entities": []}, {"text": "Ferrer and Sol\u00e9 (2001b) studied vertices degree distribution and found two power laws for that distribution with a crossover at a degree approximately corresponding to the previously found size of the supposed kernel lexicon of about 10 3 -10 4 words.", "labels": [], "entities": []}, {"text": "In () word co-occurrence networks were studied for small (about 10 4 lines of text) corpora of English, Basque, and Russian.", "labels": [], "entities": []}, {"text": "The authors claim the same tworegime word degree distribution behavior for all the languages.) offered an abstract model of language evolution, which provides for two power laws for word degree distribution with almost no fitting, and also explains that the volume of the region of large degrees (the kernel lexicon) is almost independent of the corpus volume.", "labels": [], "entities": [{"text": "language evolution", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7268718481063843}]}, {"text": "Difference between word (lemma) and word form for an analytic language (e.g. English) seems to be small.", "labels": [], "entities": []}, {"text": "Dorogovtsev-Mendes model certainly treats word forms, not lemmas, as vertices in a corpus graph.", "labels": [], "entities": []}, {"text": "Is it really true for inflecting languages like Russian?", "labels": [], "entities": []}, {"text": "Many researchers consider a word form, not a word (lemma) be a perceptual lexicon unit.", "labels": [], "entities": []}, {"text": "So a hypothesis that word forms in a corpus of an inflecting language should exhibit degree distribution similar to that of BNC looks appealing.", "labels": [], "entities": []}, {"text": "An attempt to investigate word frequency rank sta-tistics for Russian was made by, but they studied only Zipf law on too small texts to reveal the kernel lexicon effects.", "labels": [], "entities": []}, {"text": "To study the hypothesis one needs a corpus or a collection 2 of texts comparable in volume with the BNC part that was examined in, i.e. about 4.10 7 word occurrences.", "labels": [], "entities": [{"text": "BNC part", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.890400230884552}]}, {"text": "Certainly, texts that were analyzed in () were much smaller.", "labels": [], "entities": []}, {"text": "Recently and Kapustin (2006) studied a big (~5.10 7 word occurrences) collection of Russian.", "labels": [], "entities": []}, {"text": "The collection exhibited power law behavior similar to that of BNC except that the vertex degree at the crossover point and the average degree were about 4-5 times less than that of BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 182, "end_pos": 185, "type": "DATASET", "confidence": 0.8774453401565552}]}, {"text": "These differences could be assigned either to a collection nature (legislation texts specifics) or to the properties of the (Russian) language itself.", "labels": [], "entities": []}, {"text": "We shall reference the collection studied in () as \"RuLegal\".", "labels": [], "entities": [{"text": "RuLegal", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.8997490406036377}]}, {"text": "In this paper we present a study of another big collection of Russian texts.", "labels": [], "entities": []}, {"text": "We have found that degree distributions (for different big sub-collections) are similar to those of BNC and of RuLegal.", "labels": [], "entities": [{"text": "BNC", "start_pos": 100, "end_pos": 103, "type": "DATASET", "confidence": 0.8331237435340881}]}, {"text": "While the exponents and the kernel lexicon size are also similar to those of BNC, the average degree for these collections are almost twice less than the average degree of BNC, and the nature of this difference is unclear still.", "labels": [], "entities": []}, {"text": "The rest of the paper has the following structure.", "labels": [], "entities": []}, {"text": "Technology section briefly describes the collection and the procedures of building of cooccurrence graph and of calculation of exponents of power laws.", "labels": [], "entities": []}, {"text": "In Discussion section we compare the results obtained with those of,, and).", "labels": [], "entities": []}, {"text": "In Conclusion some considerations for future research are discussed.", "labels": [], "entities": [{"text": "Conclusion", "start_pos": 3, "end_pos": 13, "type": "TASK", "confidence": 0.9075969457626343}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Fitting power laws to averaged degree  distributions -Schwarz information criterion", "labels": [], "entities": []}, {"text": " Table 2. Parameters of the best fit two power laws  for the cumulative distributions  Clearly two power laws fit the curves better.  The exponents, the crossover degree and estimated  size of the kernel lexicon (number of vertices with  high degrees above the crossover) for the best fits  (two powers, zero/one omitted point) are shown in", "labels": [], "entities": [{"text": "crossover degree", "start_pos": 153, "end_pos": 169, "type": "METRIC", "confidence": 0.9383443593978882}]}]}