{"title": [{"text": "Sentence Level Machine Translation Evaluation as a Ranking Problem: one step aside from BLEU", "labels": [], "entities": [{"text": "Sentence Level Machine Translation Evaluation", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.9063801646232605}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9947408437728882}]}], "abstractContent": [{"text": "The paper proposes formulating MT evaluation as a ranking problem, as is often done in the practice of assessment by human.", "labels": [], "entities": [{"text": "formulating MT evaluation", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.8249635895093282}]}, {"text": "Under the ranking scenario, the study also investigates the relative utility of several features.", "labels": [], "entities": []}, {"text": "The results show greater correlation with human assessment at the sentence level, even when using an n-gram match score as a baseline feature.", "labels": [], "entities": []}, {"text": "The feature contributing the most to the rank order correlation between automatic ranking and human assessment was the dependency structure relation rather than BLEU score and reference language model feature.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 161, "end_pos": 171, "type": "METRIC", "confidence": 0.977578341960907}]}], "introductionContent": [{"text": "In recent decades, alongside the growing research on Machine Translation (MT), automatic MT evaluation has become a critical problem for MT system developers, who are interested in quick turnaround development cycles.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.8658655762672425}, {"text": "MT evaluation", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.9545750617980957}, {"text": "MT", "start_pos": 137, "end_pos": 139, "type": "TASK", "confidence": 0.9823408126831055}]}, {"text": "The state-of-the-art automatic MT evaluation is an n-gram based metric represented by BLEU () and its variants.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9508466124534607}, {"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9977689981460571}]}, {"text": "Ever since its creation, the BLEU score has been the gauge of Machine Translation system evaluation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9753395617008209}, {"text": "Machine Translation system evaluation", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.9315050840377808}]}, {"text": "Nevertheless, the research community has been largely aware of the deficiency of the BLEU metric.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.9306042492389679}]}, {"text": "BLEU captures only a single dimension of the vitality of natural languages: a candidate translation gets acknowledged only if it uses exactly the same lexicon as the reference translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9764837622642517}]}, {"text": "Natural languages, however, are characterized by their extremely rich mechanisms for reproduction via a large number of syntactic, lexical and semantic rewriting rules.", "labels": [], "entities": []}, {"text": "Although BLEU has been shown to correlate positively with human assessments at the document level (), efforts to improve state-of-the-art MT require that human assessment be approximated at sentence level as well.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9955790638923645}, {"text": "MT", "start_pos": 138, "end_pos": 140, "type": "TASK", "confidence": 0.9698901176452637}]}, {"text": "Researchers report the BLEU score at document level in order to combat the sparseness of n-grams in BLEU scoring.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9771363139152527}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.8560153245925903}]}, {"text": "But, ultimately, document-level MT evaluation has to be pinned down to the granularity of the sentence.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.8880595862865448}]}, {"text": "Unfortunately, the correlation between human assessment and BLEU score at sentence level is extremely low ().", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9821307361125946}]}, {"text": "While acknowledging the appealing simplicity of BLEU as away to access one perspective of an MT candidate translation's quality, we observe the following facts of n-gram based MT metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9952031373977661}, {"text": "MT candidate translation", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.935981810092926}]}, {"text": "First, they may not reflect the mechanism of how human beings evaluate sentence translation quality.", "labels": [], "entities": [{"text": "sentence translation quality", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.7673044204711914}]}, {"text": "More specifically, optimizing BLEU does not guarantee the optimization of sentence quality approved by human assessors.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9869660139083862}]}, {"text": "Therefore, BLEU is likely to have a low correlation with human assessment at sentence level for most candidate translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9982665777206421}]}, {"text": "Second, it is conceivable that human beings are more reliable ranking the quality of multiple candidate translations than assigning a numeric value to index the quality of the candidate translation even with significant deliberation.", "labels": [], "entities": []}, {"text": "Consequently, a more intuitive approach for automatic MT evaluation is to replicate the quality ranking ability of human assessors.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.9747726023197174}]}, {"text": "Thirdly, the BLEU score is elusive and hard to interpret; for example, what can be concluded fora candidate translation's quality if the BLEU score is 0.0168, particularly when we are aware that even a human translation can receive an embarrassingly low BLEU score?", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9765291810035706}, {"text": "BLEU score", "start_pos": 137, "end_pos": 147, "type": "METRIC", "confidence": 0.9786489903926849}, {"text": "BLEU score", "start_pos": 254, "end_pos": 264, "type": "METRIC", "confidence": 0.9758498072624207}]}, {"text": "In light of the discussion above, we propose an alternative scenario for MT evaluation, where, instead of assigning a numeric score to a candidate translation under evaluation, we predict its rank with regard to its peer candidate translations.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.9714079201221466}]}, {"text": "This formulation of the MT evaluation task fills the gap between an automatic scoring function and human MT evaluation practice.", "labels": [], "entities": [{"text": "MT evaluation task", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9458870887756348}, {"text": "MT evaluation", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.96640744805336}]}, {"text": "The results from the current study will not only interest MT system evaluation moderators but will also inform the research community about which features are useful in improving the correlation between human rankings and automatic rankings.", "labels": [], "entities": [{"text": "MT system evaluation moderators", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.8875517398118973}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Data Sets Information", "labels": [], "entities": []}, {"text": " Table 2: Ranking Consisteny Scores for LDC2003T17 Data", "labels": [], "entities": [{"text": "Ranking Consisteny Scores", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8671258687973022}, {"text": "LDC2003T17", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8349474668502808}]}, {"text": " Table 3: Training and Testing on Within-year Data  (Test on 7 MT and 4 Human)", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "DATASET", "confidence": 0.7589344382286072}]}, {"text": " Table 4: Training and Testing on Within-year Data  (Test on MT only)", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.8360062837600708}]}, {"text": " Table 6: Document Level Ranking Testing Results", "labels": [], "entities": []}]}