{"title": [{"text": "Deep Linguistic Processing for Spoken Dialogue Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a framework for deep linguistic processing for natural language understanding in task-oriented spoken dialogue systems.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.6893817583719889}]}, {"text": "The goal is to create domain-general processing techniques that can be shared across all domains and dialogue tasks, combined with domain-specific optimization based on an ontology mapping from the generic LF to the application on-tology.", "labels": [], "entities": []}, {"text": "This framework has been tested in six domains that involve tasks such as interactive planning, coordination operations, tutoring, and learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep linguistic processing is essential for spoken dialogue systems designed to collaborate with users to perform collaborative tasks.", "labels": [], "entities": []}, {"text": "We describe the TRIPS natural language understanding system, which is designed for this purpose.", "labels": [], "entities": [{"text": "TRIPS natural language understanding", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.6225980147719383}]}, {"text": "As we develop the system, we are constantly balancing two competing needs: (1) deep semantic accuracy: the need to produce the semantically and pragmatically deep interpretations fora specific application; and (2) portability: the need to reuse our grammar, lexicon and discourse interpretation processes across domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9134703874588013}]}, {"text": "We work to accomplish portability by using a multi-level representation.", "labels": [], "entities": [{"text": "portability", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9721251726150513}]}, {"text": "The central components are all based on domain general representations, including a linguistically based detailed semantic representation (the Logical Form, or LF), illocutionary acts, and a collaborative problem-solving model.", "labels": [], "entities": []}, {"text": "Each application then involves using a domain-specific ontology and reasoning components.", "labels": [], "entities": []}, {"text": "The generic LF is linked to the domain-specific representations by a set of ontology mapping rules that must be defined for each domain.", "labels": [], "entities": []}, {"text": "Once the ontology mapping is defined, we then can automatically specialize the generic grammar to use the stronger semantic restrictions that arise from the specific domain.", "labels": [], "entities": []}, {"text": "In this paper we mainly focus on the generic components for deep processing.", "labels": [], "entities": []}, {"text": "The work on ontology mapping and rapid grammar adaptation is described elsewhere (; forthcoming).", "labels": [], "entities": [{"text": "ontology mapping", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.9060027003288269}, {"text": "rapid grammar adaptation", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.7004067301750183}]}], "datasetContent": [{"text": "Our evaluation is aimed at assessing two main features of the grammar and lexicon: portability and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.998760461807251}]}, {"text": "We use two main evaluation criteria: full sentence accuracy, that takes into account both syntactic and semantic accuracy of the system, and sense tagging accuracy, to demonstrate that the word senses included in the system can be distinguished with a combination of syntactic and domain-independent semantic information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.8941689729690552}, {"text": "sense tagging", "start_pos": 141, "end_pos": 154, "type": "TASK", "confidence": 0.6758497357368469}, {"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.6370490193367004}]}, {"text": "As a measure of the breadth of grammatical coverage of our system, we have evaluated our coverage on the CSLI LKB (Linguistic Knowledge Building) test suite).", "labels": [], "entities": [{"text": "coverage", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9631437063217163}, {"text": "CSLI LKB (Linguistic Knowledge Building) test suite", "start_pos": 105, "end_pos": 156, "type": "DATASET", "confidence": 0.9092306031121148}]}, {"text": "The test suite contains approximately 1350 sentences, of which about 400 are ungrammatical.", "labels": [], "entities": []}, {"text": "We use a fullsentence accuracy measure to evaluate our coverage, since this is the most meaningful measure in terms of what we require as parser output in our applications.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.8619003295898438}]}, {"text": "For a sentence representation to be counted as correct by this measure, both the syntactic structure and the semantic representation must be correct, which includes the correct assignment of word senses, dependency relations among terms, and speech act type.", "labels": [], "entities": []}, {"text": "Our current coverage for the diverse grammatical phenomena in the corpus is 64% full-sentence accuracy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9690825343132019}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9763393402099609}]}, {"text": "We also report the number of spanning parses found, because in our system there are cases in which the syntactic parse is correct, but an incorrect word sense may have been assigned, since we disambiguate senses using not only syntactic structure but also semantic features as selectional restrictions on arguments.", "labels": [], "entities": []}, {"text": "For example, in The manager interviewed Browne after working, the parser assigns working the sense LF::FUNCTION, used with non-agentive subjects, instead of the correct sense for agentive subjects, LF::WORKING.", "labels": [], "entities": [{"text": "FUNCTION", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.8040205240249634}]}, {"text": "For the grammatical utterances in the test suite, our parser found spanning parses for 80%.", "labels": [], "entities": []}, {"text": "While the ungrammatical sentences in the set are an important tool for constraining grammar output, our grammar is designed to find a reasonable interpretation for natural speech, which often is less than perfect.", "labels": [], "entities": []}, {"text": "For example, we have low preference grammar rules that allow dropped subjects, missing determiners, and wrong subject verb agreement.", "labels": [], "entities": []}, {"text": "In addition, utterances are often fragmentary, so even those without spanning parses maybe considered correct.", "labels": [], "entities": []}, {"text": "Our grammar allows all major constituents (NP, VP, ADJP, ADVP) as valid utterances.", "labels": [], "entities": []}, {"text": "As a result, our system produces spanning parses for 46% of the \"ungrammatical\" utterances.", "labels": [], "entities": []}, {"text": "We have not yet done a detailed error analysis.", "labels": [], "entities": []}, {"text": "As a measure of system portability to new domains, we have evaluated our system coverage on the ATIS (Airline Travel Information System) speech corpus, which we have never used before.", "labels": [], "entities": [{"text": "ATIS (Airline Travel Information System) speech corpus", "start_pos": 96, "end_pos": 150, "type": "DATASET", "confidence": 0.7349918054209815}]}, {"text": "For this evaluation, the proper names (cities, airports, airline companies) in the ATIS corpus were added to our lexicon, but no other development work was performed.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 83, "end_pos": 94, "type": "DATASET", "confidence": 0.9552868604660034}]}, {"text": "We parsed 116 randomly selected test sentences and hand-checked the results using our full-sentence accuracy measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9255793690681458}]}, {"text": "Our baseline coverage of these utterances is 53% full-sentence semantic accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9493216872215271}]}, {"text": "Of the 55 utterances that were not completely correct, we found spanning parses for 36%.", "labels": [], "entities": []}, {"text": "Reasons that spanning parses were marked as wrong include incorrect word senses (e.g., for stop in I would like it to have a stop in Phoenix) or PP-attachment.", "labels": [], "entities": []}, {"text": "Reasons that no spanning parse was found include missing senses for existing words (e.g., serve as in Does that flight serve dinner).", "labels": [], "entities": []}], "tableCaptions": []}