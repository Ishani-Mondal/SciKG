{"title": [{"text": "Using Self-Trained Bilexical Preferences to Improve Disambiguation Accuracy", "labels": [], "entities": [{"text": "Improve Disambiguation Accuracy", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.6853295763333639}]}], "abstractContent": [{"text": "A method is described to incorporate bilex-ical preferences between phrase heads, such as selection restrictions, in a Maximum-Entropy parser for Dutch.", "labels": [], "entities": []}, {"text": "The bilexical preferences are modelled as association rates which are determined on the basis of a very large parsed corpus (about 500M words).", "labels": [], "entities": []}, {"text": "We show that the incorporation of such self-trained preferences improves parsing accuracy significantly.", "labels": [], "entities": [{"text": "parsing", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.9822607636451721}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9685728549957275}]}], "introductionContent": [], "datasetContent": [{"text": "The output of the parser is evaluated by comparing the generated dependency structure fora corpus sentence to the gold standard dependency structure in a treebank.", "labels": [], "entities": []}, {"text": "For this comparison, we represent the dependency structure (a directed acyclic graph) as a set of named dependency relations.", "labels": [], "entities": []}, {"text": "The dependency graph in is represented with the following set of dependencies:: Dependency graph example.", "labels": [], "entities": []}, {"text": "Reentrant nodes are visualized using a bold-face index.", "labels": [], "entities": []}, {"text": "Root forms of head words are explicitly included in separate nodes, and different types of head receive a different relation label such as hd, crd (for coordination), whd (for WH-phrases) etc.", "labels": [], "entities": []}, {"text": "In this case, the WH-phrase is both the whd element of the top-node, as well as a mod dependent of drink.", "labels": [], "entities": []}, {"text": "Comparing these sets, we count the number of dependencies that are identical in the generated parse and the stored structure, which is expressed traditionally using f-score ().", "labels": [], "entities": []}, {"text": "We prefer to express similarity between dependency structures by concept accuracy: where Di p is the number of dependencies produced by the parser for sentence i, D g is the number of dependencies in the treebank parse, and D f is the number of incorrect and missing dependencies produced by the parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9670447111129761}]}, {"text": "The standard version of Alpino that we use here as baseline system is trained on the 145,000 word Alpino treebank, which contains dependency structures for the cdbl (newspaper) part of the Eindhoven corpus.", "labels": [], "entities": [{"text": "Alpino treebank", "start_pos": 98, "end_pos": 113, "type": "DATASET", "confidence": 0.8548352718353271}, {"text": "Eindhoven corpus", "start_pos": 189, "end_pos": 205, "type": "DATASET", "confidence": 0.8404855728149414}]}, {"text": "The parameters for training the model are the same for the baseline model, as well as the model that includes the self-trained bilexical preferences (introduced below).", "labels": [], "entities": []}, {"text": "These parameters include #sentences 100% 30,000,000 #words 500,000,000 #sentences without parse 0.2% 100,000 #sentences with fragments 8% 2,500,000 #single full parse 92% 27,500,000).", "labels": [], "entities": []}, {"text": "Some quantitative information of this parsed corpus is listed in table 1.", "labels": [], "entities": []}, {"text": "In the experiments described below, we do not distinguish between full and fragment parses; sentences without a parse are obviously ignored.", "labels": [], "entities": []}, {"text": "3 Bilexical preferences  We report on two experiments.", "labels": [], "entities": []}, {"text": "In the first experiment, we report on the results of tenfold crossvalidation on the Alpino treebank.", "labels": [], "entities": [{"text": "Alpino treebank", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.9801346659660339}]}, {"text": "This is the material that is standardly used for training and testing.", "labels": [], "entities": []}, {"text": "For each of the sentences of this corpus, the system produces atmost the first 1000 parses.", "labels": [], "entities": []}, {"text": "For every parse we compute the quality by comparing its dependency structure with the gold standard dependency structure in the treebank.", "labels": [], "entities": []}, {"text": "For training, atmost 100 parses are selected randomly for each sentence.", "labels": [], "entities": []}, {"text": "For (tenfold cross-validated) testing, we use all available parses fora given sentence.", "labels": [], "entities": []}, {"text": "In order to test the quality of the model, we check for each given sentence which of its atmost 1000 parses is selected by the disambiguation model.", "labels": [], "entities": []}, {"text": "The quality of that parse is used in the computation of the accuracy, as listed in table 6.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9996523857116699}]}, {"text": "The column labeled exact measures the proportion of sentences for which the model selected the best possible parse (there can be multiple: Results with ten-fold cross-validation on the Eindhoven-cdbl part of the Alpino treebank.", "labels": [], "entities": [{"text": "Eindhoven-cdbl part of the Alpino treebank", "start_pos": 185, "end_pos": 227, "type": "DATASET", "confidence": 0.8767008384068807}]}, {"text": "In these experiments, the models are used to select a parse from a given set of atmost 1000 parses per sentence.", "labels": [], "entities": []}, {"text": "The baseline row reports on the quality of a disambiguation model which simply selects the first parse for each sentence.", "labels": [], "entities": []}, {"text": "The oracle row reports on the quality of the best-possible disambiguation model, which would (by magic) always select the best possible parse (some parses are outside the coverage of the system, and some parses are generated only after more than 1000 inferior parses).", "labels": [], "entities": []}, {"text": "The error reduction column measures which part of the disambiguation problem (difference between the baseline and oracle scores) is solved by the model.", "labels": [], "entities": []}, {"text": "The results show a small but clear increase in error reduction, if the standard model (without the association score features) is compared with a (retrained) model that includes the association score features.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.9329476356506348}]}, {"text": "The relatively large improvement of the exact score suggests that the bilexical preference features are particularly good at choosing between very good parses.", "labels": [], "entities": [{"text": "exact score", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.9803970456123352}]}, {"text": "For the second experiment, we evaluate how well the resulting model performs in the full system.", "labels": [], "entities": []}, {"text": "First of all, this is the only really convincing evaluation which measures progress for the system as a whole by virtue of including bilexical preferences.", "labels": [], "entities": []}, {"text": "The second motivation for this experiment is for methodological reasons: we now test on a truly unseen test-set.", "labels": [], "entities": []}, {"text": "The first experiment can be criti-: Results on the WR-P-P-H part of the D-Coi corpus (2267 sentences from the newspaper Trouw, from 2001).", "labels": [], "entities": [{"text": "WR-P-P-H part of the D-Coi corpus (2267 sentences from the newspaper Trouw", "start_pos": 51, "end_pos": 125, "type": "DATASET", "confidence": 0.6452095004228445}]}, {"text": "In these experiments, we report on the full system.", "labels": [], "entities": []}, {"text": "In the full system, the disambiguation model is used to guide a best-first beam-search procedure which extracts a parse from the parse forest.", "labels": [], "entities": []}, {"text": "Difference in CA was found to be significant (using paired T-test on the per sentence CA scores).", "labels": [], "entities": [{"text": "Difference", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9737063646316528}, {"text": "CA", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.8835806846618652}, {"text": "T-test", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.8056825399398804}]}, {"text": "cized on methodological grounds as follows.", "labels": [], "entities": []}, {"text": "The Alpino Treebank was used to train the disambiguation model which was used to construct the large parsed treebank from which we extracted the counts for the association scores.", "labels": [], "entities": [{"text": "Alpino Treebank", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9313876330852509}]}, {"text": "Those scores might somehow therefore indirectly reflect certain aspects of the Alpino Treebank training data.", "labels": [], "entities": [{"text": "Alpino Treebank training data", "start_pos": 79, "end_pos": 108, "type": "DATASET", "confidence": 0.9620824605226517}]}, {"text": "Testing on that data later (with the inclusion of the association scores) is therefore not sound.", "labels": [], "entities": []}, {"text": "For this second experiment we used the WR-P-P-H (newspaper) part of the D-Coi corpus.", "labels": [], "entities": [{"text": "WR-P-P-H (newspaper) part of the D-Coi corpus", "start_pos": 39, "end_pos": 84, "type": "DATASET", "confidence": 0.7415112621254392}]}, {"text": "This part contains 2256 sentences from the newspaper.", "labels": [], "entities": []}, {"text": "In table 7 we show the resulting f-score and CA fora system with and without the inclusion of the z(t, r) features.", "labels": [], "entities": [{"text": "CA", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9804114103317261}]}, {"text": "The improvement found in the previous experiment is confirmed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of lexical dependencies in parsed  corpora (approximate counts)", "labels": [], "entities": []}, {"text": " Table 3: Pairs involving a direct object relationship  with the highest pointwise mutual information score.", "labels": [], "entities": []}, {"text": " Table 4: Pairs involving a direct object relationship  with the highest pointwise mutual information score  for the verb drink.", "labels": [], "entities": []}, {"text": " Table 5: Pairs involving a modifier relationship be- tween a verb and an adverbial with the highest asso- ciation score.", "labels": [], "entities": []}, {"text": " Table 6: Results with ten-fold cross-validation on  the Eindhoven-cdbl part of the Alpino treebank. In  these experiments, the models are used to select a  parse from a given set of atmost 1000 parses per sen- tence.", "labels": [], "entities": [{"text": "Eindhoven-cdbl part of the Alpino treebank", "start_pos": 57, "end_pos": 99, "type": "DATASET", "confidence": 0.7926526168982188}]}, {"text": " Table 7: Results on the WR-P-P-H part of the D-Coi  corpus (2267 sentences from the newspaper Trouw,  from 2001). In these experiments, we report on the  full system. In the full system, the disambiguation  model is used to guide a best-first beam-search pro- cedure which extracts a parse from the parse forest.  Difference in CA was found to be significant (using  paired T-test on the per sentence CA scores).", "labels": [], "entities": [{"text": "D-Coi  corpus (2267 sentences from the newspaper Trouw", "start_pos": 46, "end_pos": 100, "type": "DATASET", "confidence": 0.6471021936999427}, {"text": "Difference", "start_pos": 315, "end_pos": 325, "type": "METRIC", "confidence": 0.9726396203041077}, {"text": "CA", "start_pos": 329, "end_pos": 331, "type": "METRIC", "confidence": 0.9298647046089172}]}]}