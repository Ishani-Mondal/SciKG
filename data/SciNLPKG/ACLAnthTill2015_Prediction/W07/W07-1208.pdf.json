{"title": [{"text": "Self-or Pre-Tuning? Deep linguistic processing of language variants", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a design strategy for deep language processing grammars to appropriately handle language variants.", "labels": [], "entities": []}, {"text": "It allows a grammar to be restricted as to what language variant it is tuned to, but also to detect the variant a given input pertains to.", "labels": [], "entities": []}, {"text": "This is evaluated and compared to results obtained with an alternative strategy by which the relevant variant is detected with current language identification methods in a pre-processing step.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper addresses the issue of handling different variants of a given language by a deep language processing grammar for that language.", "labels": [], "entities": []}, {"text": "In the benefit of generalization and grammar writing economy, it is desirable that a grammar can handle language variants -that share most grammatical structures and lexicon -in order to avoid endless multiplication of individual grammars, motivated by inessential differences.", "labels": [], "entities": []}, {"text": "From the viewpoint of analysis, however, increased variant coverage typically opens the way to increased spurious overgeneration.", "labels": [], "entities": []}, {"text": "Consequently, the ability for the grammar to be tuned to the relevant dialect of the input is important to control overgeneration arising from its flexibility.", "labels": [], "entities": []}, {"text": "Control on what is generated is also desirable.", "labels": [], "entities": []}, {"text": "In general one wants to be able to parse as much variants as possible, but at the same time be selective in generation, by consistently generating only in a given selected variant.", "labels": [], "entities": [{"text": "parse", "start_pos": 35, "end_pos": 40, "type": "TASK", "confidence": 0.9634039998054504}]}, {"text": "Closely related to the setting issue (addressed in the next Section 2) is the tuning issue: if a system can be restricted to a particular variety, what is the best way to detect the variety of the input?", "labels": [], "entities": []}, {"text": "We discuss two approaches to this issue.", "labels": [], "entities": []}, {"text": "One of them consists in using pre-processing components that can detect the language variety at stake.", "labels": [], "entities": []}, {"text": "This pre-tuning approach explores the hypothesis that methods developed for language identification can be used also to detect language variants (Section 5).", "labels": [], "entities": [{"text": "language identification", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.7480693757534027}]}, {"text": "The other approach is to have the computational grammar prepared for self-tuning to the language variant of the input in the course of processing that input (Section 4).", "labels": [], "entities": []}, {"text": "We evaluate the two approaches and compare them (last Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "Before reporting on the results obtained with the experiments on the performance of the two approaches (self-and pre-tuning), it is important to introduce the experimental conditions under which such exercises were conducted.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Baseline: Confusion matrix.", "labels": [], "entities": []}, {"text": " Table 2: Self-tuning: Confusion matrix.", "labels": [], "entities": []}, {"text": " Table 3: Originating corpora: Accuracy", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9905835390090942}]}, {"text": " Table 4: Two-way classification: Accuracy", "labels": [], "entities": [{"text": "Two-way classification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.651038259267807}, {"text": "Accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9986022114753723}]}, {"text": " Table 5: Three-way classification: Accuracy", "labels": [], "entities": [{"text": "Three-way classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6408416926860809}, {"text": "Accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9984622001647949}]}, {"text": " Table 5. As expected, the  classifier based in bigrams has the best perfor- mance for every size of the input, which im- proves from 0.59 to 0.76 as the size of the input  gets from 1 line to 20 lines.", "labels": [], "entities": []}]}