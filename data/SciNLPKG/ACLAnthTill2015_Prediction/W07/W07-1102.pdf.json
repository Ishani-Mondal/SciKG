{"title": [{"text": "Distinguishing Subtypes of Multiword Expressions Using Linguistically-Motivated Statistical Measures", "labels": [], "entities": [{"text": "Distinguishing Subtypes of Multiword Expressions", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8488473892211914}]}], "abstractContent": [{"text": "We identify several classes of multiword expressions that each require a different encoding in a (computational) lexicon, as well as a different treatment within a computational system.", "labels": [], "entities": []}, {"text": "We examine linguistic properties pertaining to the degree of semantic idiosyncrasy of these classes of expressions.", "labels": [], "entities": []}, {"text": "Accordingly , we propose statistical measures to quantify each property, and use the measures to automatically distinguish the classes.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We use the British National Corpus (BNC), automatically parsed using the Collins parser, and further processed with TGrep2.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 11, "end_pos": 40, "type": "DATASET", "confidence": 0.9665270249048868}, {"text": "Collins parser", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.9183423221111298}, {"text": "TGrep2", "start_pos": 116, "end_pos": 122, "type": "DATASET", "confidence": 0.8628401756286621}]}, {"text": "We select our potential experimental expressions from pairs of verb and direct object that have a minimum frequency of 25 in the BNC and that involve one of a predefined list of basic (transitive) verbs.", "labels": [], "entities": [{"text": "BNC", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.7408941388130188}]}, {"text": "Basic verbs, which in their literal uses refer to states or acts central to human experience (e.g., give and put), commonly form MWEs in combination with their direct object argument ().", "labels": [], "entities": []}, {"text": "We use 12 such verbs ranked highly according to the number of different nouns they appear within the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.9361184239387512}]}, {"text": "Here are the verbs in alphabetical order: bring, find, get, give, hold, keep, lose, make, put, see, set, take To guarantee that the final set of expressions contains pairs from all four classes, we pseudo-randomly select them from the initial list of pairs extracted from the BNC as explained above.", "labels": [], "entities": [{"text": "BNC", "start_pos": 276, "end_pos": 279, "type": "DATASET", "confidence": 0.9198130965232849}]}, {"text": "To ensure the inclusion of IDMs, we consult two idioms dictionaries ().", "labels": [], "entities": []}, {"text": "To ensure we include LVCs, we select pairs in which the noun has a morphologically related verb according to WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9824187159538269}]}, {"text": "We also select pairs whose noun is not morphologically related to any verb to ensure the inclusion of LIT combinations.", "labels": [], "entities": [{"text": "LIT", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9374135136604309}]}, {"text": "This selection process resulted in 632 pairs, reduced to 563 after annotation (see Section 4.2 for details on annotation).", "labels": [], "entities": []}, {"text": "Out of these, 148 are LIT, 196 are ABS, 102 are LVC, and 117 are IDM.", "labels": [], "entities": []}, {"text": "We randomly choose 102 pairs from each class as our final experimental expressions.", "labels": [], "entities": []}, {"text": "We then pseudorandomly divide these into training (TRAIN), development (DEV), and test (TEST) data sets, so that each set has an equal number of pairs from each class.", "labels": [], "entities": []}, {"text": "In addition, we ensure that pairs with the same verb that belong to the same class are divided equally among the three sets.", "labels": [], "entities": []}, {"text": "Our final TRAIN, DEV, and TEST sets contain 240, 84, and 84 pairs, respectively.", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.7997185587882996}, {"text": "DEV", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.7055796980857849}, {"text": "TEST", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9424671530723572}]}, {"text": "We performed experiments on DEV to find features most relevant for classification.", "labels": [], "entities": [{"text": "DEV", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.9472035765647888}, {"text": "classification", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.9716268181800842}]}, {"text": "These experiments revealed that removing Sim dist (t, v ) resulted in better performance.", "labels": [], "entities": []}, {"text": "This is not surprising given that basic verbs are highly polysemous, and hence the distributional context of a basic verb may not correspond to any particular sense of it.", "labels": [], "entities": []}, {"text": "We thus remove this feature (from COMP) in experiments on TEST.", "labels": [], "entities": [{"text": "TEST", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.79130619764328}]}, {"text": "Results presented here are on the TEST set; those on the DEV set have similar trends.", "labels": [], "entities": [{"text": "TEST set", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.8788350820541382}, {"text": "DEV set", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9713878035545349}]}, {"text": "Here, we first look at the overall performance of classification in Section 5.1.", "labels": [], "entities": [{"text": "Section 5.1", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.857609748840332}]}, {"text": "Section 5.2 presents the results of classification for the individual classes.", "labels": [], "entities": []}, {"text": "presents the results of classification-in terms of average accuracy (%Acc) and relative error reduction (%RER)-for the individual feature groups, as well as for all groups combined.", "labels": [], "entities": [{"text": "classification-in", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.9419996738433838}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.924210250377655}, {"text": "Acc)", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9526347815990448}, {"text": "relative error reduction (%RER)-", "start_pos": 79, "end_pos": 111, "type": "METRIC", "confidence": 0.8549847503503164}]}, {"text": "The baseline (chance) accuracy is 25% since we have four equal-sized classes in TEST.", "labels": [], "entities": [{"text": "baseline (chance)", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.8262700885534286}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.7410488724708557}, {"text": "TEST", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.8066213726997375}]}, {"text": "As can be seen, INST features yield the lowest overall accuracy, around 36%, with a relative error reduction of only 14% over the baseline.", "labels": [], "entities": [{"text": "INST", "start_pos": 16, "end_pos": 20, "type": "TASK", "confidence": 0.8010910153388977}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9993569254875183}, {"text": "error", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.8299199938774109}]}, {"text": "This shows that institutionalization, although relevant, is not sufficient for distinguishing among different levels of semantic idiosyncrasy.", "labels": [], "entities": []}, {"text": "Interestingly, FIXD features achieve the highest accuracy, 50%, with a relative error reduction of 33%, showing that fixedness is a salient aspect of semantic idiosyncrasy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9989535808563232}, {"text": "relative error reduction", "start_pos": 71, "end_pos": 95, "type": "METRIC", "confidence": 0.7052875359853109}]}, {"text": "COMP features achieve reasonably good accuracy, around 40%, though still notably lower than the accuracy of FIXD features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9994218349456787}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.999203622341156}]}, {"text": "This is especially interesting since much previous research has focused solely on the non-compositionality of MWEs to identify them ().", "labels": [], "entities": []}, {"text": "Our results confirm the relevance of this property, while at the same time revealing its insufficiency.", "labels": [], "entities": []}, {"text": "Interestingly, features related to the semantic properties of the constituents, VERB and NSEM, overall perform comparably to the compositionality features.", "labels": [], "entities": [{"text": "VERB", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.5082831978797913}]}, {"text": "However, a closer look at their performance on the individual classes (see Section 5.2) reveals that, unlike COMP, they are mainly good at identifying items from certain classes.", "labels": [], "entities": []}, {"text": "As hypothesized, we achieve the highest performance, an accuracy of 58% and a relative error reduction of 44%, when we combine all features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9996054768562317}, {"text": "relative error reduction", "start_pos": 78, "end_pos": 102, "type": "METRIC", "confidence": 0.8401599923769633}]}, {"text": "displays classification performance, when we use all the feature groups except one.", "labels": [], "entities": []}, {"text": "These results are more or less consistent with those in Ta-   ble 3 above, except some differences which we discuss below.", "labels": [], "entities": [{"text": "Ta-   ble 3", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.7428175359964371}]}, {"text": "Removing FIXD features results in a drastic decrease in performance (10.7%), while the removal of INST and COMP features cause much smaller drops in performance (4.7% and 2.3%, respectively).", "labels": [], "entities": [{"text": "INST", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.46267592906951904}]}, {"text": "Here again, we can see that features related to the semantics of the verb and the noun are salient features.", "labels": [], "entities": []}, {"text": "Removing either of these results in a substantial decrease in performance-9.5% and 11.9%, respectively-which is comparable to the decrease resulting from removing FIXD features.", "labels": [], "entities": []}, {"text": "This is an interesting observation, since VERB and NSEM features, on their own, do not perform nearly as well as FIXD features.", "labels": [], "entities": [{"text": "VERB", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.5803754329681396}]}, {"text": "It is thus necessary to futher investigate the performance of these groups on larger data sets with more variability in the verb and noun constituents of the expressions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: F -measures on TEST pairs, for individual feature", "labels": [], "entities": [{"text": "F -measures", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9847320715586344}]}, {"text": " Table 6: Per-class observed agreement and kappa score be-", "labels": [], "entities": [{"text": "Per-class observed agreement", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.640376607577006}, {"text": "kappa score", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.8905255496501923}]}]}