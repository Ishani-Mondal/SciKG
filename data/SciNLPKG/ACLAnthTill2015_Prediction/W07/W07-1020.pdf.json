{"title": [{"text": "What's in a gene name? Automated refinement of gene name dictionaries", "labels": [], "entities": [{"text": "Automated refinement of gene name dictionaries", "start_pos": 23, "end_pos": 69, "type": "TASK", "confidence": 0.7464274217685064}]}], "abstractContent": [{"text": "Many approaches for named entity recognition rely on dictionaries gathered from cu-rated databases (such as Entrez Gene for gene names.)", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6590100030104319}]}, {"text": "Strategies for matching entries in a dictionary against arbitrary text use either inexact string matching that allows for known deviations, dictionaries enriched according to some observed rules, or a combination of both.", "labels": [], "entities": []}, {"text": "Such refined dictionaries cover potential structural, lexical, ortho-graphical, or morphological variations.", "labels": [], "entities": []}, {"text": "In this paper, we present an approach to automatically analyze dictionaries to discover how names are composed and which variations typically occur.", "labels": [], "entities": []}, {"text": "This knowledge can be constructed by looking at single entries (names and synonyms for one gene), and then be transferred to entries that show similar patterns in one or more synonyms.", "labels": [], "entities": []}, {"text": "For instance, knowledge about words that are frequently missing in (or added to) a name (\"antigen\", \"protein\", \"human\") could automatically be extracted from dictionaries.", "labels": [], "entities": []}, {"text": "This paper should be seen as a vision paper, though we implemented most of the ideas presented and show results for the task of gene name recognition.", "labels": [], "entities": [{"text": "gene name recognition", "start_pos": 128, "end_pos": 149, "type": "TASK", "confidence": 0.8118042548497518}]}, {"text": "The automatically extracted name composition rules can easily be included in existing approaches, and provide valuable insights into the biomedi-cal sub-language.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recognition of named entities (NER), such as names referring to genes and proteins, forms a major building block for text mining systems.", "labels": [], "entities": [{"text": "Recognition of named entities (NER)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8863224983215332}, {"text": "text mining", "start_pos": 117, "end_pos": 128, "type": "TASK", "confidence": 0.8575536608695984}]}, {"text": "Especially in the life sciences, a large amount of different entity types and their instances exist.", "labels": [], "entities": []}, {"text": "Two basic strategies for NER are classification-and dictionary-based approaches.", "labels": [], "entities": [{"text": "NER", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9852768778800964}]}, {"text": "Classifiers learn (or are given) models to decide whether a sequence of tokens refers to an entity or not.", "labels": [], "entities": []}, {"text": "Such decisions are based on various forms of input, for instance, tokens and their sequence in a sentence, part-of-speech tags, characteristic suffixes, and trigger keywords).", "labels": [], "entities": []}, {"text": "Models can be learned from a given training sample.", "labels": [], "entities": []}, {"text": "Dictionary-based approaches rely on curated word lists containing (all known) representatives of an entity type.", "labels": [], "entities": []}, {"text": "Manual or automated refinement of the dictionary and inexact matching strategies allow to cover abroad spectrum of name variations).", "labels": [], "entities": []}, {"text": "Classificationbased approaches have proven to be very robust towards unseen tokens and names, because they also incorporate knowledge on names of the given class in general).", "labels": [], "entities": []}, {"text": "Dictionaries, on the other hand, reflect the knowledge about an entity class at a given time, and such approaches cannot find instances unknown to them.", "labels": [], "entities": []}, {"text": "However, the main advantage of dictionary-based NER is that they bring the explicit possibility to map recognized entities to the source of the entries (most times, a database.)", "labels": [], "entities": []}, {"text": "This alleviates the task of named entity identification (NEI) that is needed to annotate texts properly or link text-mined facts to database entries.", "labels": [], "entities": [{"text": "named entity identification (NEI)", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.822940836350123}]}, {"text": "In this paper, we want to concentrate on dictionary-based approaches and present ideas of how these could be automatically refined and enriched.", "labels": [], "entities": []}, {"text": "In such a setting, named entity recognition functions as a method of 'spotting' entities in a text, after which further identification (disambiguation) is needed.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.7205453713734945}, {"text": "spotting' entities in a text", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.8043386141459147}]}, {"text": "NER components thus should guarantee very high recall rates with a reasonable precision.", "labels": [], "entities": [{"text": "recall rates", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.9865201115608215}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9961768388748169}]}, {"text": "NEI then refines the predictions of NER, eliminating false positive annotations and identifying names.", "labels": [], "entities": [{"text": "NEI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8320557475090027}]}, {"text": "That such a setup would perform quite well is reflected, for example, in a study presented by.", "labels": [], "entities": []}, {"text": "They showed that sophisticated disambiguation strategies currently yield up to 93.9% precision (for mouse genes; yeast: 89.5%, fly: 77.8%.)", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9993071556091309}]}, {"text": "Participants in the BioCreAtIvE 2 challenge showed similar values for human genes (up to 84.1% precision, 87.5% recall, or 81.1% F1), see Morgan and Hirschman (2007) fora summary.", "labels": [], "entities": [{"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9827449917793274}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9982931017875671}, {"text": "F1", "start_pos": 129, "end_pos": 131, "type": "METRIC", "confidence": 0.9976617097854614}]}, {"text": "Hand-coded rules for creating spelling variations have been proposed before, see section on Related Work.", "labels": [], "entities": []}, {"text": "Such rules are applied to synonyms to generate morphological and orthographical variations (\"Fas ligand\" \u2192 \"Fas ligands\" and \"Ifn gamma\" \u2192 \"Ifn-\u03b3\", respectively).", "labels": [], "entities": []}, {"text": "In the same manner, systems use known patterns for structural changes of names and mappings for lexical variations to enrich existing dictionaries (\"CD95R\" \u2192 \"receptor of CD95\" and \"gastric alcohol dehydrogenase\" \u2192 \"stomach alcohol dehydrogenase\").", "labels": [], "entities": []}, {"text": "Our research question in this paper is, how such rules can be learned automatically from dictionaries that contain entries of the same entity class with multiple, typical synonyms each.", "labels": [], "entities": []}, {"text": "Learning about the composition of names comes down to an analysis of known names.", "labels": [], "entities": []}, {"text": "A human, given the same task, would look through a lot of examples to derive term formation patterns.", "labels": [], "entities": []}, {"text": "Questions to ask are: \u2022 What are frequent orthographical and morphological variations?", "labels": [], "entities": []}, {"text": "\u2022 Which parts of a name get abbreviated?", "labels": [], "entities": []}, {"text": "\u2022 How are abbreviations formed?", "labels": [], "entities": [{"text": "abbreviations formed", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8810863196849823}]}, {"text": "\u2022 Which identical abbreviations can be observed in multiple names?", "labels": [], "entities": []}, {"text": "\u2022 In which way can a name structurally and lexically change?", "labels": [], "entities": []}, {"text": "\u2022 Which are the parts of a name that can be exchanged with other terms or skipped entirely?", "labels": [], "entities": []}, {"text": "\u2022 Which are the important parts of a name, which are additional descriptive elements?", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate methods to analyze names in order to find the semantically important parts.", "labels": [], "entities": []}, {"text": "We map these parts to potential syntactic variations thereof observed within a name and its synonyms.", "labels": [], "entities": []}, {"text": "We assess the frequency of such mappings (exchange of tokens, different ordering of tokens, etc.) and transfer this knowledge to all other names in the same dictionary.", "labels": [], "entities": []}, {"text": "In this setup, understanding a name results in a structured decomposition of the name.", "labels": [], "entities": []}, {"text": "Such decompositions provide knowledge on how to find (and identify) the name in arbitrary text, as they give insights into its mandatory, unique, and ambiguous 2 parts.", "labels": [], "entities": [{"text": "find (and identify) the name in arbitrary text", "start_pos": 48, "end_pos": 94, "type": "TASK", "confidence": 0.7271543920040131}]}, {"text": "This paper should be seen as a vision paper, though we implemented most of the ideas presented herein and show first results.", "labels": [], "entities": []}, {"text": "We first explain the idea behind learning name composition rules, motivated by manual curation as described in Related Work.", "labels": [], "entities": [{"text": "learning name composition rules", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.6937754303216934}]}, {"text": "We then explain the basic techniques needed for our analysis.", "labels": [], "entities": []}, {"text": "We show how single entries (a name and all its synonyms) can be analyzed to find composition rules, and how these can be transferred to other entries.", "labels": [], "entities": []}, {"text": "Preliminary results using some of the ideas presented here are also given.", "labels": [], "entities": []}, {"text": "We conclude this paper with a discussion of the experimental methodology and an outlook.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated some ideas presented in this paper on the BioCreAtIvE 2 (BC2) dataset for the gene normalization task.", "labels": [], "entities": [{"text": "BioCreAtIvE 2 (BC2) dataset", "start_pos": 55, "end_pos": 82, "type": "DATASET", "confidence": 0.8389357626438141}, {"text": "gene normalization task", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.8598597844441732}]}, {"text": "For the purpose of this study, we were interested in how our method would perform concerning the recall, as compared to methods based on hand-curated dictionary refinement.", "labels": [], "entities": [{"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9890758991241455}]}, {"text": "We conducted the following experiment: the BC2 GN gold standard consists of references to abstracts (PubMed IDs), genes identified in each abstract (Entrez Gene IDs) and text snippets that comprise each gene's name.", "labels": [], "entities": [{"text": "BC2 GN gold standard", "start_pos": 43, "end_pos": 63, "type": "DATASET", "confidence": 0.8775919079780579}]}, {"text": "For one abstract, there could be multiple, different snippets representing the same gene, ADH7 (131): \"stomach alcohol dehydrogenase\", \"class IV alcohol dehydrogenase\", or \"sigma-ADH\", all in the same abstract.", "labels": [], "entities": []}, {"text": "For identification, it was sufficient in BC2 to report the ID, regardless of number of occurrences or name variations.", "labels": [], "entities": [{"text": "identification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9780520796775818}, {"text": "BC2", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.618081271648407}, {"text": "ID", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.8707470297813416}]}, {"text": "As the method presented in this paper lacks a matching strategy for spotting of names, we performed our initial evaluation on the text snippets only.", "labels": [], "entities": [{"text": "spotting of names", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.8997092048327128}]}, {"text": "Finding the right ID for each snippet thus ultimately yielded the recall performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9983982443809509}]}, {"text": "In the above example, we would try to identify ID 131 three times, counting every miss as a false negative.", "labels": [], "entities": []}, {"text": "The methods presented above were able to yield a recall of 73.1%.", "labels": [], "entities": [{"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9992608428001404}]}, {"text": "With the original BC2 evaluation scheme, we achieve a recall of 84.2%.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9997535347938538}]}, {"text": "Compared to the highest result for our system with a manually refined dictionary, this figure is more than 8% lower.", "labels": [], "entities": []}, {"text": "This shows that still, many name variations are not recognized.", "labels": [], "entities": []}, {"text": "Some errors could be accounted to ranges or enumerations of gene names (\"SMADs 1, 5 and 8\"), others to not far enough reaching analyses: for detecting \"SMAD8\", we only had the synonyms \"SMAD8A\", \"SMAD8B\", and \"SMAD9\" for the correct gene in the dictionary (all are synonyms for the same gene, according to Entrez Gene).", "labels": [], "entities": []}, {"text": "It should thus have been learned that the letter \"A\" can be left out (similar to \"1\", see rule no. 8 in.)", "labels": [], "entities": []}, {"text": "Another undetected example is \"G(olf) alpha\".", "labels": [], "entities": []}, {"text": "(2) Learn in this or another gene: \"alpha subunit\" can be expressed as \"alpha\" (or \"subunit\" skipped) \u21d2 \"G(olf) alpha\" could be a name.", "labels": [], "entities": []}, {"text": "We see that most orthographical and morphological variations (Greek symbols/English words, singular/plural forms, capitalization) can be integrated quite easily in matching techniques.", "labels": [], "entities": []}, {"text": "The general knowledge about such variations is far-reaching and can be applied to most domains.", "labels": [], "entities": []}, {"text": "In contrast, structural and lexical variations are much harder to pinpoint and express in general ways; mostly, such possible variations are specific to a sub-domain and thus present the main challenge for our method.", "labels": [], "entities": []}, {"text": "The ideas discussed in this paper originated from work on the aforementioned BioCreAtIvE 2 task.", "labels": [], "entities": [{"text": "BioCreAtIvE 2 task", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.6046542127927145}]}, {"text": "In that work, we used manually designed rules to generate variations of gene names.) and other groups propose similar methods all based on human observation and experience leading to refined dictionaries.", "labels": [], "entities": []}, {"text": "As many causes for name variations are easy to spot and express, we concluded it was entirely possible to gain such insights in an automated manner.", "labels": [], "entities": [{"text": "name variations", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.865522176027298}]}, {"text": "Left undetermined is the potential impact of composition rules on machine-learning techniques that use dictionaries as input for features.", "labels": [], "entities": []}, {"text": "However, the methodology should work for other task using the same or similar initial observations (This remains to be proven.)", "labels": [], "entities": []}, {"text": "We are currently applying the method to the analysis of Gene Ontology terms ().", "labels": [], "entities": [{"text": "analysis of Gene Ontology terms", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.7222743391990661}]}, {"text": "There, many terms are mere descriptions of concepts than precise labels, and there are less additional synonyms (with structural and lexical variations.)", "labels": [], "entities": []}, {"text": "A good starting point for assessing possible patterns in name composition could also be the MeSH controlled vocabulary.", "labels": [], "entities": [{"text": "name composition", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7914414703845978}, {"text": "MeSH controlled vocabulary", "start_pos": 92, "end_pos": 118, "type": "DATASET", "confidence": 0.8576639095942179}]}, {"text": "Entries in MeSH typically contain many structural and lexical variations, a deeper understanding of which bears more insights than of orthographical or morphological variations.", "labels": [], "entities": []}, {"text": "Readers of this manuscript should either gain more insights into name compositions of gene names -in order to help refining dictionaries based on manual rule sets-, or be convinced that the idea of learning composition rules can be tackled in automated ways, promising examples of and basic techniques for which we discussed herein.", "labels": [], "entities": []}], "tableCaptions": []}