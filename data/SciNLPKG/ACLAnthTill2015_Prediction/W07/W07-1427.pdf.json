{"title": [{"text": "Learning Alignments and Leveraging Natural Logic", "labels": [], "entities": [{"text": "Learning Alignments", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7398939728736877}]}], "abstractContent": [{"text": "We describe an approach to textual inference that improves alignments at both the typed dependency level and at a deeper semantic level.", "labels": [], "entities": [{"text": "textual inference", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7260048389434814}]}, {"text": "We present a machine learning approach to alignment scoring, a stochas-tic search procedure, and anew tool that finds deeper semantic alignments, allowing rapid development of semantic features over the aligned graphs.", "labels": [], "entities": [{"text": "alignment scoring", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.9819585382938385}]}, {"text": "Further, we describe a complementary semantic component based on natural logic, which shows an added gain of 3.13% accuracy on the RTE3 test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9992408752441406}, {"text": "RTE3 test set", "start_pos": 131, "end_pos": 144, "type": "DATASET", "confidence": 0.9592829942703247}]}], "introductionContent": [{"text": "Among the many approaches to textual inference, alignment of dependency graphs has shown utility in determining entailment without the use of deep understanding.", "labels": [], "entities": [{"text": "textual inference", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.729310929775238}]}, {"text": "However, discovering alignments requires a scoring function that accurately scores alignment and a search procedure capable of approximating the optimal mapping within a large search space.", "labels": [], "entities": [{"text": "discovering alignments", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.821826308965683}]}, {"text": "We address the former requirement through a machine learning approach for acquiring lexical feature weights, and we address the latter with an approximate stochastic approach to search.", "labels": [], "entities": []}, {"text": "Unfortunately, the most accurate aligner cannot capture deeper semantic relations between two pieces of text.", "labels": [], "entities": []}, {"text": "For this, we have developed a tool, Semgrex, that allows the rapid development of dependency rules to find specific entailments, such as familial or locative relations, a common occurence in textual entailment data.", "labels": [], "entities": []}, {"text": "Instead of writing code by hand to capture patterns in the dependency graphs, we develop a separate rule-base that operates over aligned dependency graphs.", "labels": [], "entities": []}, {"text": "Further, we describe a separate natural logic component that complements our textual inference system, making local entailment decisions based on monotonic assumptions.", "labels": [], "entities": []}, {"text": "The next section gives a brief overview of the system architecture, followed by our proposal for improving alignment scoring and search.", "labels": [], "entities": [{"text": "alignment scoring", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.9741117358207703}]}, {"text": "New coreference features and the Semgrex tool are then described, followed by a description of natural logic.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Perceptron and MIRA results on 10-fold cross- validation on RTE2 dev for 10 passes.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9675061106681824}, {"text": "RTE2 dev", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.8271547555923462}]}, {"text": " Table 4: Illustrative examples from the RTE3 test suite", "labels": [], "entities": [{"text": "RTE3 test", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.9148900806903839}]}, {"text": " Table 3. Parameter  values tuned on development data yielded the best  performance. The optimized hybrid system attained  an absolute accuracy gain of 3.12% over our RTE  system, corresponding to an extra 25 problems an- swered correctly. This result is statistically signifi- cant (p < 0.01, McNemar's test, 2-tailed).  The gain cannot be fully attributed to NatLog's  success in handling the kind of inferences about  monotonicity which are the staple of natural logic.  Indeed, such inferences are quite rare in the RTE", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9417597055435181}, {"text": "RTE", "start_pos": 520, "end_pos": 523, "type": "TASK", "confidence": 0.4972309172153473}]}]}