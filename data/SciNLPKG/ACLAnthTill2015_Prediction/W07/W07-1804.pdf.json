{"title": [], "abstractContent": [{"text": "This paper shows how we can combine the art of grammar writing with the power of statistics by bootstrapping statistical language models (SLMs) for Dialogue Systems from grammars written using the Grammatical Framework (GF) (Ranta, 2004).", "labels": [], "entities": []}, {"text": "Furthermore , to take into account that the probability of a user's dialogue moves is not static during a dialogue we show how the same methodology can be used to generate dialogue move specific SLMs where certain dialogue moves are more probable than others.", "labels": [], "entities": []}, {"text": "These models can be used at different points of a dialogue depending on contextual constraints.", "labels": [], "entities": []}, {"text": "By using grammar generated SLMs we can improve both recognition and understanding performance considerably over using the original grammar.", "labels": [], "entities": []}, {"text": "With dialogue move specific SLMs we would be able to get a further improvement if we had an optimal way of predicting the correct language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Speech recognition (ASR) for dialogue systems is often caught in the trap of the sparse data problem which excludes the possibility of using statistical language models.", "labels": [], "entities": [{"text": "Speech recognition (ASR)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.873196542263031}]}, {"text": "A common approach is to write a grammar for the domain either as a speech recognition grammar (SRG) or as an interpretation grammar which can be compiled into a speech recognition grammar (SRG) using some grammar development platform such as Gemini, Regulus or GF ().", "labels": [], "entities": [{"text": "speech recognition grammar (SRG)", "start_pos": 67, "end_pos": 99, "type": "TASK", "confidence": 0.7161631981531779}, {"text": "speech recognition grammar (SRG)", "start_pos": 161, "end_pos": 193, "type": "TASK", "confidence": 0.6837977667649587}, {"text": "Regulus", "start_pos": 250, "end_pos": 257, "type": "DATASET", "confidence": 0.7837163805961609}]}, {"text": "The last option will assure that the linguistic coverage of the ASR and interpretation are kept in sync.", "labels": [], "entities": [{"text": "ASR", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9478729367256165}]}, {"text": "ASR for commercial dialogue systems has mainly focused on grammar-based approaches despite the fact that SLMs seem to have a better overall performance;.", "labels": [], "entities": [{"text": "ASR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9781340956687927}]}, {"text": "This probably depends on the timeconsuming work of collecting corpora for training SLMs compared with the more rapid and straightforward development of SRGs.", "labels": [], "entities": [{"text": "SLMs", "start_pos": 83, "end_pos": 87, "type": "TASK", "confidence": 0.8659539818763733}]}, {"text": "However, SLMs are more robust for out-of-coverage input, perform better in difficult conditions and seem to work better for naive users as shown in ().", "labels": [], "entities": [{"text": "SLMs", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.9503174424171448}]}, {"text": "SRGs on the other hand are limited in their coverage depending on how well grammar writers succeed in predicting what users may say.", "labels": [], "entities": [{"text": "SRGs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.4449252784252167}, {"text": "coverage", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9453946948051453}, {"text": "predicting what users may say", "start_pos": 102, "end_pos": 131, "type": "TASK", "confidence": 0.8387110948562622}]}, {"text": "An approach taken in both dialogue systems and dictation applications is to write a grammar for the particular domain and generate an artificial corpus from the grammar to be used as training corpus for SLMs ().", "labels": [], "entities": [{"text": "SLMs", "start_pos": 203, "end_pos": 207, "type": "TASK", "confidence": 0.9392479062080383}]}, {"text": "These grammar-based models are not as accurate as the ones built from real data as the estimates are artificial, lacking a realistic distribution.", "labels": [], "entities": []}, {"text": "However, as has been shown in () these grammar-based statistical models seem to have a much more robust behaviour than their corresponding grammars which leaves us with a much better starting point in the first development stage in a dialogue system.", "labels": [], "entities": []}, {"text": "It is away of compromising between the ease of grammar writing and the robust-ness of SLMs.", "labels": [], "entities": []}, {"text": "With this methodology we can use the knowledge and intuition we have about the domain and include it in our first SLM and get a much more robust behaviour than with a grammar.", "labels": [], "entities": []}, {"text": "From this starting point we can then collect more data with our first prototype of the system to improve our SLM.", "labels": [], "entities": [{"text": "SLM", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.9620017409324646}]}, {"text": "In this paper the advantage of this method is shown further by evaluating a different domain in greater detail.", "labels": [], "entities": []}, {"text": "Context-specific models have shown important recognition performance gain () and have usually been of two types: created as state-specific grammars or built from collected data partitioned according to dialogue states.", "labels": [], "entities": []}, {"text": "Both methods have their disadvantages.", "labels": [], "entities": []}, {"text": "In the first case, we constrain the user heavily which makes them unsuitable for use in a more flexible system such as an information-state based system.", "labels": [], "entities": []}, {"text": "This can be solved by having a back-off method but leaves us with extra processing ().", "labels": [], "entities": []}, {"text": "In the latter case, we have an even more severe sparse data problem than when creating a general SLM as we need enough data to get a good distribution of data over dialogue states.", "labels": [], "entities": []}, {"text": "In an information-state based system where the user is not restricted to only a few dialogue states this problem gets even worse.", "labels": [], "entities": []}, {"text": "In addition, why we chose to work with grammar-based SLMs in the first place was because data is seldom available in the first stage of dialogue system development.", "labels": [], "entities": [{"text": "dialogue system development", "start_pos": 136, "end_pos": 163, "type": "TASK", "confidence": 0.7276855111122131}]}, {"text": "This leaves us with the requirement of an SLM that although being contextspecific does not constrain the user and which assures a minimal coverage of expressions fora certain context.", "labels": [], "entities": []}, {"text": "In () this is accomplished by dynamically populating a class-based SLMs with context-sensitive content words and utterances.", "labels": [], "entities": []}, {"text": "In this paper, we will show how we can use the same methodology as in) to create context-specific SLMs from grammars based on dialogue moves that match these criteria.", "labels": [], "entities": []}, {"text": "This study is organized as follows.", "labels": [], "entities": []}, {"text": "First, we introduce our methodology for developing SLMs from grammars.", "labels": [], "entities": [{"text": "SLMs from grammars", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.848801056543986}]}, {"text": "Section 3 describes the data collection of test utterances and how we have partitioned the data into different test sets depending on grammar coverage, types of users and types of dialogue moves.", "labels": [], "entities": []}, {"text": "In section 4, we show and discuss the results of the different models for different test sets and finally we draw some conclusions from the experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the recognition performance of our different types of models we ran several experiments on the different test sets.", "labels": [], "entities": []}, {"text": "We report results on word error rate (WER), sentence error rate (SER) and also on a semantic level by reporting what we call dialogue move error rate (DMER).", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 21, "end_pos": 42, "type": "METRIC", "confidence": 0.8524333188931147}, {"text": "sentence error rate (SER)", "start_pos": 44, "end_pos": 69, "type": "METRIC", "confidence": 0.8885214229424795}, {"text": "dialogue move error rate (DMER)", "start_pos": 125, "end_pos": 156, "type": "METRIC", "confidence": 0.770119139126369}]}, {"text": "The dialogue move error rate was obtained by parsing the recognized utterances and comparing these to a parsed version of the transcriptions, calculating the rate of correctly parsed dialogue moves.", "labels": [], "entities": [{"text": "error rate", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.8746006786823273}]}, {"text": "The calculation was done in the same way as calculation of concept error rate (CER) proposed by where the degree of correctly recognized concepts is considered.", "labels": [], "entities": [{"text": "concept error rate (CER)", "start_pos": 59, "end_pos": 83, "type": "METRIC", "confidence": 0.7956004391113917}]}, {"text": "In our case this means the degree of correctly recognized dialogue moves.", "labels": [], "entities": []}, {"text": "For parsing we have used a phrase-spotting grammar written in Prolog that pattern matches phrases to dialogue moves.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9742013216018677}, {"text": "Prolog", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.9699114561080933}]}, {"text": "Using the original GF interpretation grammar for parsing would have restricted us to the coverage of the grammar which is not an optimal choice together with SLMs.", "labels": [], "entities": [{"text": "parsing", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.9630150198936462}]}, {"text": "Ideally, we would like to use a robust version of GF to be able to use the original GF grammar both for parsing and SLM generation and by that assure the same linguistic coverage.", "labels": [], "entities": [{"text": "parsing", "start_pos": 104, "end_pos": 111, "type": "TASK", "confidence": 0.9681370258331299}, {"text": "SLM generation", "start_pos": 116, "end_pos": 130, "type": "TASK", "confidence": 0.96356201171875}]}, {"text": "Attempts to do this have been carried out in the TALK project for the MP3 domain by training a dialogue move tagger on the same type of corpus that was used for the DMSLMs where dialogue moves occur together with their corresponding utterances.", "labels": [], "entities": []}, {"text": "Other methods of relaxing the constraints of the GF parser are also under consideration.", "labels": [], "entities": []}, {"text": "Meanwhile, we are using a simple robust phrase spotting parser.", "labels": [], "entities": [{"text": "phrase spotting parser", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.8065442045529684}]}, {"text": "We have investigated both how our grammar-based SLMs perform in comparison to our grammar under different conditions to see how recognition and understanding performance varies as well as how our DMSLMs perform in comparison to the general grammar-based SLM.", "labels": [], "entities": []}, {"text": "The results are reported in the following sections.", "labels": [], "entities": []}, {"text": "All models have the same domain vocabulary and the OOV figures presented earlier thereby apply for all of them.", "labels": [], "entities": [{"text": "OOV", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9452181458473206}]}, {"text": "shows the results for our different language models on our unrestricted test set of 1000 utterances as well as for the part of this test set which is in-coverage.", "labels": [], "entities": []}, {"text": "As expected they all perform much better on the in-coverage test set with the lowest WER obtained with our grammar.", "labels": [], "entities": [{"text": "WER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9989114999771118}]}, {"text": "On the unrestricted test set we can see an important reduction of both WER (26% and 38% relative improvement) and DMER (24% and 40% relative improvement) for the SLMs in comparison to the grammar which indicates the robustness of these to new user input.", "labels": [], "entities": [{"text": "WER", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9989221096038818}, {"text": "DMER", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9972182512283325}]}], "tableCaptions": [{"text": " Table 1: Results on unrestricted vs in-coverage test set", "labels": [], "entities": []}, {"text": " Table 2: Results on naive vs expert users", "labels": [], "entities": []}, {"text": " Table 3: Ask Move SLM", "labels": [], "entities": [{"text": "SLM", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.6715819239616394}]}, {"text": " Table 4: Answer Move SLM", "labels": [], "entities": [{"text": "Answer Move SLM", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.5564400653044382}]}, {"text": " Table 5: Request Move SLM", "labels": [], "entities": [{"text": "Request Move SLM", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8260583480199178}]}, {"text": " Table 6: YN Move SLM", "labels": [], "entities": [{"text": "SLM", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.5022884607315063}]}, {"text": " Table 7: General SLM on rest of test data", "labels": [], "entities": [{"text": "General", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9530909657478333}, {"text": "SLM", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.7895240783691406}]}, {"text": " Table 8: DMSLMs on general test set", "labels": [], "entities": []}]}