{"title": [{"text": "Retrieving lost information from textual databases: rediscovering expeditions from an animal specimen database", "labels": [], "entities": [{"text": "Retrieving lost information from textual databases", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8866323033968607}]}], "abstractContent": [{"text": "Importing large amounts of data into databases does not always go without the loss of important information.", "labels": [], "entities": []}, {"text": "In this work, methods are presented that aim to rediscover this information by inferring it from the information that is available in the database.", "labels": [], "entities": []}, {"text": "From and animal specimen database, the information to which expedition an animal that was found belongs is rediscovered.", "labels": [], "entities": []}, {"text": "While the work is in an early stage, the obtained results are promising, and prove that it is possible to rediscover expedition information from the database.", "labels": [], "entities": []}], "introductionContent": [{"text": "Databases made up of textual material tend to contain a wealth of information that remains unexplored with simple keyword-based search.", "labels": [], "entities": []}, {"text": "Maintainers of the databases are often not aware of the possibilities offered by text mining methods to discover hidden information to enrich the basic data.", "labels": [], "entities": []}, {"text": "In this work several machine learning methods are explored to investigate whether 'hidden information' can be extracted from an animal specimen database belonging to the Dutch National Museum for Natural History, Naturalis . The database is a combination of information about objects in the museum collection from handwritten data sources in the museum, such as journal-like entries that are kept by biologists while collecting animal or plant specimens on expedition http://www.naturalis.nl and tables that link the journal entries to the museum register.", "labels": [], "entities": []}, {"text": "What is not preserved in the transition from the written sources to the database is the name of the expedition druing which an animal specimen was found.", "labels": [], "entities": []}, {"text": "By expedition, the following event is implied: a group of biologists went on expedition together in a country during a certain time period.", "labels": [], "entities": []}, {"text": "Entries in the database that belong to this expedition can be collected by one or a subset of the participating biologists.", "labels": [], "entities": []}, {"text": "For researchers at the natural history museum it would be helpful to have access to expedition information in their database, as for biodiversity research they sometimes need overviews of expeditions.", "labels": [], "entities": []}, {"text": "It may also help further enrichment of the database and cleansing, because if the expedition information is available, missing information in certain fields, such as the country where a specimen was found, maybe inferred from the information on other specimens found during the same expedition.", "labels": [], "entities": []}, {"text": "Currently, if one wants to retrieve all objects from the database that belong to an expedition, one would have to create a database query that contains the exact data boundaries of the expeditions and the names of all collectors involved.", "labels": [], "entities": []}, {"text": "Either one of these bits of information is not enough, as the same group of biologists may have participated in an expedition more than once, and the database may also contain expeditions that overlap in time.", "labels": [], "entities": []}, {"text": "In this paper a series of experiments is described to find away to infer expedition information from the information available in the database.", "labels": [], "entities": []}, {"text": "To this end, three approaches are compared: supervised machine learning, unsupervised machine learning, and rule-based methods.", "labels": [], "entities": []}, {"text": "The obtained results vary, but prove that it is possible to extract the expedition information from the data at hand.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since the data is annotated with expedition information it was possible to use external quality measures ().", "labels": [], "entities": []}, {"text": "Three different evaluation measures were used: accuracy, entropy, and the F-measure (van.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9996223449707031}, {"text": "F-measure", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.995236873626709}]}, {"text": "The evaluation of results for the supervised learning algorithms was calculated in a straightforward way: because the classifier knows which expeditions there are and which entries belong to which expedition, it checks the expeditions it assigned to the database entries to the manually assigned expeditions and reports the overlap as accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 335, "end_pos": 343, "type": "METRIC", "confidence": 0.9984983205795288}]}, {"text": "It gets a little bit more complicated with entropy.", "labels": [], "entities": []}, {"text": "Entropy is a measure of informativity, i.e., the minimum number of bits of information needed to encode the classification of each instance.", "labels": [], "entities": [{"text": "Entropy", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8526586890220642}]}, {"text": "If the expedition clusters are uniform, i.e., all items in the cluster are very similar, the entropy will below.", "labels": [], "entities": [{"text": "entropy", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9898058772087097}]}, {"text": "The main problem with using entropy for evaluation of clusters is that the best score (an entropy of 0) is reached when every cluster contains exactly on instance.", "labels": [], "entities": []}, {"text": "Entropy is calculated as follows: first, the main class distribution, i.e., per cluster the probability that a member of that cluster belongs to a certain cluster, is computed.", "labels": [], "entities": []}, {"text": "Using that distribution the entropy of each cluster is calculated via the formula in Equation 4.", "labels": [], "entities": [{"text": "Equation", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9003077149391174}]}, {"text": "For a set of clusters the total entropy is then computed via the formula in Equation 5, in which m is the total number of clusters, s y the size of cluster y and n the total number of instances.", "labels": [], "entities": []}, {"text": "The F-measure is the harmonic mean of precision and recall, and is commonly used in information retrieval.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9950103759765625}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9991257786750793}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9931923747062683}, {"text": "information retrieval", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.8144344091415405}]}, {"text": "In information retrieval recall is the proportion of relevant documents retrieved out of the total set of relevant documents.", "labels": [], "entities": [{"text": "information retrieval recall", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.7264608144760132}]}, {"text": "When applied to clustering a 'relevant document' is an instance that is assigned correctly to a certain expedition, the set of all relevant documents is the set of all instances beloning to that expedition.", "labels": [], "entities": []}, {"text": "Precision is the number of relevant documents retrieved from the total number of documents.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9907852411270142}]}, {"text": "So when applied to cluster evaluation this means the number of instances of an expedition that were retrieved from the total number of instances).", "labels": [], "entities": []}, {"text": "This boils down to Equations 6 and 7 in which x stands for expedition, y for cluster, n xy for the number of instances belonging to expedition x that were assigned to cluster y, and n x is the number of items in expedition x.", "labels": [], "entities": []}, {"text": "Recall(x, y) = n xy n x (6) P recision(x, y) = n xy n y The F-measure fora cluster y with respect to expedition x is then computed via Equation 8.", "labels": [], "entities": []}, {"text": "The F-measure of the entire set of clusters is computed through the function in Equation 9, which takes the weighted average of the maximum F-measure per expedition.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9908774495124817}, {"text": "F-measure", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9621015787124634}]}, {"text": "21  First, two baselines were set to illustrate the situation if no machine learning or other techniques would be applied to the database.", "labels": [], "entities": []}, {"text": "if one were to randomly assign one of the 60 expeditions t the entries this would go well in 1.7% of the cases.", "labels": [], "entities": []}, {"text": "If all entries were labelled as belonging to the largest expedition this would yield an accuracy of 28%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.999653697013855}]}, {"text": "In all machine learning experiments 10-fold cross validation was used for testing performance.", "labels": [], "entities": []}, {"text": "A series of supervised machine learning experiments was carried out first to investigate whether it is possible to extract the expeditions during which the animal specimens were found at all.", "labels": [], "entities": []}, {"text": "Three learning algorithms were applied to the complete data set, which yielded accuracies between 88% and 98%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9953931570053101}]}, {"text": "Feature selection experiments with the C4.5 decision tree algorithm indicated that features 'town/village', 'collection number', 'registration number', 'collector' and 'collection date' were considered most informative for this task, hence the experiments were repeated with a data set containing only those features.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.73723304271698}]}, {"text": "The results of both series of experiments are to be found in.", "labels": [], "entities": []}, {"text": "For the C4.5 and Naive Bayes experiments the accuracy deteriorates significantly when using only the selected features (\u03b1 = 0.05, computing using McNemar's test), but it stays stable for the k-NN classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.999512791633606}]}, {"text": "This indicates that not all data is needed to infer the expeditions, but that it matters greatly which approach is taken.", "labels": [], "entities": []}, {"text": "However, as neither of the algorithm benefits from it, feature selection was not further explored.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7456514835357666}]}, {"text": "k-NN 95.9% 95.9% C.4.5 98.3% 94.4% NaiveBayes 88.1% 73.5%: Accuracy of supervised machine learning experiments using all features and selected features In these experiments all database entries were annotated with expedition information, which in areal setting is of course not the case.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9629868268966675}]}, {"text": "Through running a series of experiments with significantly smaller amounts of training data it was found that by using only as little as 5% of the training data (amounting to 392 instances) already an accuracy of 85% is reached.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9992947578430176}]}, {"text": "Annotating this amount of data with expedition information would take on person less than an hour.", "labels": [], "entities": []}, {"text": "By only using 45% of the training data an accuracy of 97% is reached 5 . In the complete learning curve of the k-NN classifier is shown.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9997692704200745}]}, {"text": "Annotating this amount of data with expedition information would take one person less than an hour.", "labels": [], "entities": [{"text": "Annotating", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9651293754577637}]}, {"text": "By only using 45% of the training data an accuracy of 97% is reached 5 . In the complete learning curve of the k -NN classifier is shown.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9997689127922058}]}], "tableCaptions": [{"text": " Table 3: Results of the rule-based experiments", "labels": [], "entities": []}]}