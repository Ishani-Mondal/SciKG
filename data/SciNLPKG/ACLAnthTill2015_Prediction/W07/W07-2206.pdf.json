{"title": [{"text": "Improving the Efficiency of a Wide-Coverage CCG Parser", "labels": [], "entities": [{"text": "Improving", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9458286166191101}]}], "abstractContent": [{"text": "The C&C CCG parser is a highly efficient linguistically motivated parser.", "labels": [], "entities": [{"text": "C&C CCG parser", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.6375615119934082}]}, {"text": "The efficiency is achieved using a tightly-integrated supertagger, which assigns CCG lexical categories to words in a sentence.", "labels": [], "entities": []}, {"text": "The integration allows the parser to request more categories if it cannot find a spanning analysis.", "labels": [], "entities": []}, {"text": "We present several enhancements to the CKY chart parsing algorithm used by the parser.", "labels": [], "entities": [{"text": "CKY chart parsing", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7966810862223307}]}, {"text": "The first proposal is chart repair, which allows the chart to be efficiently updated by adding lexical categories individually , and we evaluate several strategies for adding these categories.", "labels": [], "entities": [{"text": "chart repair", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.7867419719696045}]}, {"text": "The second proposal is to add constraints to the chart which require certain spans to be constituents.", "labels": [], "entities": []}, {"text": "Finally , we propose partial beam search to further reduce the search space.", "labels": [], "entities": []}, {"text": "Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9847991466522217}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.99901282787323}, {"text": "coverage", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9274968504905701}]}], "introductionContent": [{"text": "A recent theme in parsing research has been the application of statistical methods to linguistically motivated grammars, for example LFG (),), TAG and).", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9751425385475159}, {"text": "TAG", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.7663850784301758}]}, {"text": "The attraction of linguistically motivated parsers is the potential to produce rich output, in particular the predicate-argument structure representing the underlying meaning of a sentence.", "labels": [], "entities": []}, {"text": "The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware).", "labels": [], "entities": []}, {"text": "The C&C CCG parser) is an order of magnitude faster, but is still limited to around 25 sentences per second.", "labels": [], "entities": [{"text": "C&C CCG parser", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.891688096523285}]}, {"text": "The key to efficient CCG parsing is a finite-state supertagger which performs much of the parsing work ().", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.701261579990387}]}, {"text": "CCG is a lexicalised grammar formalism, in which elementary syntactic structures -in CCG's case lexical categories expressing subcategorisation informationare assigned to the words in a sentence.", "labels": [], "entities": []}, {"text": "CCG supertagging can be performed accurately and efficiently by a Maximum Entropy tagger.", "labels": [], "entities": []}, {"text": "Since the lexical categories contain so much grammatical information, assigning them with low average ambiguity leaves the parser, which combines them together, with much less work to do at parse time.", "labels": [], "entities": []}, {"text": "Hence, in the context of LTAG parsing, refer to supertagging as almost parsing.", "labels": [], "entities": [{"text": "LTAG parsing", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.6591418236494064}]}, {"text": "Clark and Curran (2004a) presents a novel method of integrating the supertagger and parser: initially only a small number of categories, on average, is assigned to each word, and the parser attempts to find a spanning analysis using the CKY chart-parsing algorithm.", "labels": [], "entities": []}, {"text": "If one cannot be found, the parser requests more categories from the supertagger and builds the chart again from scratch.", "labels": [], "entities": []}, {"text": "This process repeats until the parser is able to build a chart containing a spanning analysis.", "labels": [], "entities": []}, {"text": "The supertagging accuracy is high enough that the parser fails to find a spanning analysis using the initial category assignment in approximately 4% of Wall Street Journal sentences (?).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9939479827880859}, {"text": "Wall Street Journal sentences", "start_pos": 152, "end_pos": 181, "type": "DATASET", "confidence": 0.9550106376409531}]}, {"text": "However, parsing this 4%, which largely consists of the longer sentences, is disproportionately expensive.", "labels": [], "entities": [{"text": "parsing", "start_pos": 9, "end_pos": 16, "type": "TASK", "confidence": 0.9768409729003906}]}, {"text": "This paper describes several modifications to the C&C parser which improve parsing efficiency without reducing accuracy or coverage by reducing the impact of the longer sentences.", "labels": [], "entities": [{"text": "C&C parser", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.6230447590351105}, {"text": "parsing", "start_pos": 75, "end_pos": 82, "type": "TASK", "confidence": 0.9747983813285828}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9970368146896362}, {"text": "coverage", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.8957977294921875}]}, {"text": "The first involves chart repair, where the CKY chart is repaired when extra lexical categories are added (according to the scheme described above), instead of being rebuilt from scratch.", "labels": [], "entities": [{"text": "chart repair", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.7450554966926575}]}, {"text": "This allows an even tighter integration of the supertagger, in that the parser is able to request individual categories.", "labels": [], "entities": []}, {"text": "We explore methods for choosing which individual categories to add, resulting in an 11% speed improvement.", "labels": [], "entities": [{"text": "speed", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9961763620376587}]}, {"text": "The next modification involves parsing with constraints, so that certain spans are required to be constituents.", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.963556706905365}]}, {"text": "This reduces the search space considerably by eliminating a large number of constituents which cross the boundaries of these spans.", "labels": [], "entities": []}, {"text": "The best set of constraints results in a 10% speed improvement over the original parser.", "labels": [], "entities": []}, {"text": "These constraints are general enough that they could be applied to any constituency-based parser.", "labels": [], "entities": []}, {"text": "Finally, we experiment with several beam strategies to reduce the search space, finding that a partial beam which operates on part of the chart is most effective, giving a further 6.1% efficiency improvement.", "labels": [], "entities": []}, {"text": "The chart repair and constraints interact in an interesting, and unexpected, manner when combined, giving a 35.7% speed improvement overall without any loss inaccuracy or coverage.", "labels": [], "entities": [{"text": "speed", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.9827107787132263}, {"text": "coverage", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9896682500839233}]}, {"text": "This speed improvement is particularly impressive because it involves techniques which only apply to 4% of Wall Street Journal sentences.", "labels": [], "entities": [{"text": "speed", "start_pos": 5, "end_pos": 10, "type": "METRIC", "confidence": 0.9913632869720459}, {"text": "Wall Street Journal sentences", "start_pos": 107, "end_pos": 136, "type": "DATASET", "confidence": 0.9551244676113129}]}], "datasetContent": [{"text": "The parser was trained on CCGbank sections 02-21 and section 00 was used for development.", "labels": [], "entities": [{"text": "CCGbank sections 02-21", "start_pos": 26, "end_pos": 48, "type": "DATASET", "confidence": 0.9514843424161276}]}, {"text": "The performance is measured in terms of coverage, F-score and parsing time.", "labels": [], "entities": [{"text": "coverage", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9988968372344971}, {"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.997607946395874}, {"text": "parsing", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.9439095258712769}]}, {"text": "The F-score is for labelled dependencies compared against the predicate-argument dependencies in CCGbank.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9984537363052368}, {"text": "CCGbank", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9720211625099182}]}, {"text": "The time reported includes loading the grammar and statistical model, which takes around 5 seconds, and parsing the 1913 sentences in section 00.", "labels": [], "entities": []}, {"text": "The failure rate (opposite of coverage) is broken down into sentences with length \u2264 40 and > 40 because longer sentences are more difficult to parse and the C&C parser already has very high coverage on shorter sentences.", "labels": [], "entities": [{"text": "coverage", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9960514903068542}]}, {"text": "There are 1784 1-40 word sentences and 129 41+ word sentences.", "labels": [], "entities": []}, {"text": "The average length and standard deviation in the 41+ set are 50.8 and 31.5 respectively.", "labels": [], "entities": [{"text": "length", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.831100583076477}, {"text": "standard deviation", "start_pos": 23, "end_pos": 41, "type": "METRIC", "confidence": 0.9198963940143585}]}, {"text": "All experiments used gold standard POS tags.", "labels": [], "entities": []}, {"text": "Original and REPAIR do not use constraints.", "labels": [], "entities": [{"text": "REPAIR", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.43277889490127563}]}, {"text": "The NP(GOLD) experiments use Penn Treebank gold standard NP chunks to determine an upper bound on the utility of chunk constraints.", "labels": [], "entities": [{"text": "Penn Treebank gold standard NP chunks", "start_pos": 29, "end_pos": 66, "type": "DATASET", "confidence": 0.9741028249263763}]}, {"text": "The times reported for NP(C&C) using the C&C chunker include the time to load the chunker model and run the chunker (around 1.3 seconds).", "labels": [], "entities": []}, {"text": "PUNCT adds all of the punctuation constraints.", "labels": [], "entities": [{"text": "PUNCT", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7878273129463196}]}, {"text": "Finally the best system was compared against the original parser on section 23, which has 2257 sentences of length 1-40 and 153 of length 41+.", "labels": [], "entities": []}, {"text": "The maximum length is only 65, which explains the high coverage for the 41+ section.", "labels": [], "entities": [{"text": "length", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.957913875579834}, {"text": "coverage", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9924317598342896}]}], "tableCaptions": [{"text": " Table 2: Category ordering for chart repair.", "labels": [], "entities": [{"text": "chart repair", "start_pos": 32, "end_pos": 44, "type": "TASK", "confidence": 0.7861737012863159}]}, {"text": " Table 1: Parsing performance on section 00 with constraints and chart repair", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9769982695579529}]}, {"text": " Table 3: Best performance on Section 00", "labels": [], "entities": [{"text": "Section 00", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.7009267508983612}]}, {"text": " Table 4: Best performance on Section 23", "labels": [], "entities": [{"text": "Section", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9353817105293274}]}]}