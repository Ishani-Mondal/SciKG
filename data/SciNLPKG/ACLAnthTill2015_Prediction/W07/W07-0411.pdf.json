{"title": [{"text": "Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Dependency-Based Automatic Evaluation for Machine Translation", "labels": [], "entities": [{"text": "SSST, NAACL-HLT 2007 / AMTA Workshop", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.5971161936010633}, {"text": "Machine Translation", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.6989780366420746}]}], "abstractContent": [{"text": "We present a novel method for evaluating the output of Machine Translation (MT), based on comparing the dependency structures of the translation and reference rather than their surface string forms.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.8627671599388123}]}, {"text": "Our method uses a treebank-based, wide-coverage, probabilistic Lexical-Functional Grammar (LFG) parser to produce a set of structural dependencies for each translation-reference sentence pair, and then calculates the precision and recall for these dependencies.", "labels": [], "entities": [{"text": "precision", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9994394183158875}, {"text": "recall", "start_pos": 231, "end_pos": 237, "type": "METRIC", "confidence": 0.9983733892440796}]}, {"text": "Our dependency-based evaluation, in contrast to most popular string-based evaluation metrics, will not unfairly penalize perfectly valid syntactic variations in the translation.", "labels": [], "entities": []}, {"text": "In addition to allowing for legitimate syntactic differences, we use paraphrases in the evaluation process to account for lexical variation.", "labels": [], "entities": []}, {"text": "In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores.", "labels": [], "entities": []}, {"text": "An experiment with two translations of 4,000 sentences from Spanish-English Europarl shows that, in contrast to most other metrics, our method does not display a high bias towards statistical models of translation.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.8463481068611145}]}], "introductionContent": [{"text": "Since their appearance, string-based evaluation metrics such as BLEU () and NIST) have been the standard tools used for evaluating MT quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9971725940704346}, {"text": "NIST", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.7538462281227112}, {"text": "MT", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.9929845333099365}]}, {"text": "Both score a candidate translation on the basis of the number of n-grams shared with one or more reference translations.", "labels": [], "entities": []}, {"text": "Automatic measures are indispensable in the development of MT systems, because they allow MT developers to conduct frequent, costeffective, and fast evaluations of their evolving models.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9856095910072327}]}, {"text": "These advantages come at a price, though: an automatic comparison of n-grams measures only the string similarity of the candidate translation to one or more reference strings, and will penalize any divergence from them.", "labels": [], "entities": []}, {"text": "In effect, a candidate translation expressing the source meaning accurately and fluently will be given a low score if the lexical and syntactic choices it contains, even though perfectly legitimate, are not present in at least one of the references.", "labels": [], "entities": []}, {"text": "Necessarily, this score would differ from a much more favourable human judgement that such a translation would receive.", "labels": [], "entities": []}, {"text": "The limitations of string comparison are the reason why it is advisable to provide multiple references fora candidate translation in BLEU-or NIST-based evaluations.", "labels": [], "entities": [{"text": "string comparison", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.6913045644760132}, {"text": "BLEU-or", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.726772665977478}]}, {"text": "While argue that increasing the size of the test set gives even more reliable system scores than multiple references, this still does not solve the inadequacy of BLEU and NIST for sentence-level or small set evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9915810227394104}, {"text": "NIST", "start_pos": 171, "end_pos": 175, "type": "DATASET", "confidence": 0.869274377822876}]}, {"text": "In addition, in practice even a number of references do not capture the whole potential variability of the translation.", "labels": [], "entities": []}, {"text": "Moreover, when designing a statistical MT system, the need for large amounts of training data limits the researcher to collections of parallel corpora such as Europarl (, which provides only one reference, namely the target text; and the cost of creating additional reference translations of the test set, usually a few thousand sentences long, is often prohibitive.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.8834817409515381}, {"text": "Europarl", "start_pos": 159, "end_pos": 167, "type": "DATASET", "confidence": 0.9813997745513916}]}, {"text": "Therefore, it would be desirable to find an evaluation method that accepts legitimate syntactic and lexical differences between the translation and the reference, thus better mirroring human assessment.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel method that automatically evaluates the quality of translation based on the dependency structure of the sentence, rather than its surface form.", "labels": [], "entities": []}, {"text": "Dependencies abstract away from the particulars of the surface string (and CFG tree) realization and provide a \"normalized\" representation of (some) syntactic variants of a given sentence.", "labels": [], "entities": [{"text": "CFG tree", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9035175740718842}]}, {"text": "The translation and reference files are analyzed by a treebank-based, probabilistic Lexical-Functional Grammar (LFG) parser), which produces a set of dependency triples for each input.", "labels": [], "entities": []}, {"text": "The translation set is compared to the reference set, and the number of matches is calculated, giving the precision, recall, and f-score for that particular translation.", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9997527003288269}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9985331296920776}, {"text": "f-score", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.9958601593971252}]}, {"text": "In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow and in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score.", "labels": [], "entities": []}, {"text": "Comparing the LFG-based evaluation method with other popular metrics: BLEU, NIST, General Text Matcher (GTM) (, Translation Error Rate (TER)) 1 , and METEOR (), we show that combining dependency representations with paraphrases leads to a more accurate evaluation that correlates better with human judgment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9979708790779114}, {"text": "NIST", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.7399303913116455}, {"text": "Translation Error Rate (TER)) 1", "start_pos": 112, "end_pos": 143, "type": "METRIC", "confidence": 0.8595440089702606}, {"text": "METEOR", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.9677479863166809}]}, {"text": "The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of two experiments on different sets of data: 4,000 sentences from Spanish-English Europarl and 16,800 sentences of Chinese-English newswire text from the Linguistic Data Consortium's (LDC) Multiple Translation project; Section 5 discusses ongoing work; Section 6 concludes.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 264, "end_pos": 272, "type": "DATASET", "confidence": 0.8578068614006042}, {"text": "Linguistic Data Consortium's (LDC) Multiple Translation", "start_pos": 336, "end_pos": 391, "type": "TASK", "confidence": 0.5238377451896667}]}, {"text": "As we focus on purely automatic metrics, we omit HTER (Human-Targeted Translation Error Rate) here.", "labels": [], "entities": [{"text": "HTER (Human-Targeted Translation Error Rate)", "start_pos": 49, "end_pos": 93, "type": "METRIC", "confidence": 0.7099702656269073}]}], "datasetContent": [{"text": "The process underlying the evaluation of fstructure quality against a gold standard can be used in automatic MT evaluation as well: we parse the translation and the reference, and then, for each sentence, we check the set of translation dependencies against the set of reference dependencies, counting the number of matches.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 109, "end_pos": 122, "type": "TASK", "confidence": 0.9778297543525696}]}, {"text": "As a result, we obtain the precision and recall scores for the translation, and we calculate the f-score for the given pair.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9997162222862244}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9971734285354614}]}, {"text": "Because we are comparing two outputs that were produced automatically, there is a possibility that the result will not be noise-free.", "labels": [], "entities": []}, {"text": "To assess the amount of noise that the parser may introduce we conducted an experiment where 100 English Europarl sentences were modified by hand in such away that the position of adjuncts was changed, but the sentence remained grammatical and the meaning was not changed.", "labels": [], "entities": []}, {"text": "This way, an ideal parser should give both the source and the modified sentence the same fstructure, similarly to the case presented in (1).", "labels": [], "entities": []}, {"text": "The modified sentences were treated like a translation file, and the original sentences played the part of the reference.", "labels": [], "entities": []}, {"text": "Each set was run through the parser.", "labels": [], "entities": []}, {"text": "We evaluated the dependency triples obtained from the \"translation\" against the dependency triples for the \"reference\", calculating the f-score, and applied other metrics (TER, METEOR, BLEU, NIST, and GTM) to the set in order to compare scores.", "labels": [], "entities": [{"text": "TER", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.9962957501411438}, {"text": "METEOR", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9553145170211792}, {"text": "BLEU", "start_pos": 185, "end_pos": 189, "type": "METRIC", "confidence": 0.9917502999305725}, {"text": "NIST", "start_pos": 191, "end_pos": 195, "type": "DATASET", "confidence": 0.7665483355522156}]}, {"text": "The results, inluding the distinction between f-scores for all dependencies and predicate-only dependencies, appear in The baseline column shows the upper bound fora given metric: the score which a perfect translation, word-for-word identical to the reference, would obtain.", "labels": [], "entities": []}, {"text": "In the other column we list the scores that the metrics gave to the \"translation\" containing reordered adjunct.", "labels": [], "entities": []}, {"text": "As can be seen, the dependency and predicate-only dependency scores are lower than the perfect 100, reflecting the noise introduced by the parser.", "labels": [], "entities": []}, {"text": "To show the difference between the scoring based on LFG dependencies and other metrics in an ideal situation, we created another set of a hundred sentences with reordered adjuncts, but this time selecting only those reordered sentences that were given the same set of dependencies by the parser (in other words, we simulated having the ideal parser).", "labels": [], "entities": []}, {"text": "As can be seen in dep_preds f-score 100 100.", "labels": [], "entities": []}, {"text": "Scores for sentences with reordered adjuncts in an ideal situation  In the first experiment, we attempted to determine whether the dependency-based measure is biased towards statistical MT output, a problem that has been observed for n-gram-based metrics like BLEU and NIST.", "labels": [], "entities": [{"text": "MT", "start_pos": 186, "end_pos": 188, "type": "TASK", "confidence": 0.9441601634025574}, {"text": "BLEU", "start_pos": 260, "end_pos": 264, "type": "METRIC", "confidence": 0.8138249516487122}, {"text": "NIST", "start_pos": 269, "end_pos": 273, "type": "DATASET", "confidence": 0.9501863718032837}]}, {"text": "report that BLEU and NIST favour n-gram-based MT models such as Pharaoh, so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9328876733779907}, {"text": "NIST", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.9439367055892944}, {"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9344000220298767}]}, {"text": "Others repeatedly observed this tendency in previous research as well; in one experiment, reported in, where the rule-based system Logomedia 7 was compared with Pharaoh, BLEU scored Pharaoh 0.0349 points higher, NIST scored Pharaoh 0.6219 points higher, but human judges scored Logomedia output 0.19 points higher (on a 5-point scale).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9966038465499878}, {"text": "NIST", "start_pos": 212, "end_pos": 216, "type": "DATASET", "confidence": 0.856387197971344}]}, {"text": "We used the data from the Linguistic Data Consortium Multiple Translation Chinese (MTC) Parts 2 and 4.", "labels": [], "entities": [{"text": "Linguistic Data Consortium Multiple Translation Chinese (MTC) Parts 2", "start_pos": 26, "end_pos": 95, "type": "DATASET", "confidence": 0.7253060720183633}]}, {"text": "The data consists of multiple translations of Chinese newswire text, four humanproduced references, and segment-level human scores fora subset of the translation-reference pairs.", "labels": [], "entities": []}, {"text": "Although a single translated segment was always evaluated by more than one judge, the judges used a different reference every time, which is why we treated each translation-referencehuman score triple as a separate segment.", "labels": [], "entities": []}, {"text": "In effect, the test set created from this data contained 16,800 segments.", "labels": [], "entities": []}, {"text": "As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and the dependency-based method.", "labels": [], "entities": [{"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9762751460075378}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9988020658493042}, {"text": "TER", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9961873888969421}, {"text": "METEOR", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9866586327552795}]}], "tableCaptions": [{"text": " Table 1. Scores for sentences with reordered adjuncts", "labels": [], "entities": []}, {"text": " Table 2. Scores for sentences with reordered adjuncts in  an ideal situation", "labels": [], "entities": []}]}