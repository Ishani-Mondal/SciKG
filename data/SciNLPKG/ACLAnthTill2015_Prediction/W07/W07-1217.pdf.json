{"title": [{"text": "Partial Parse Selection for Robust Deep Processing", "labels": [], "entities": [{"text": "Robust Deep Processing", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7312190334002177}]}], "abstractContent": [{"text": "This paper presents an approach to partial parse selection for robust deep processing.", "labels": [], "entities": [{"text": "partial parse selection", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.6619519392649332}]}, {"text": "The work is based on a bottom-up chart parser for HPSG parsing.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 50, "end_pos": 62, "type": "TASK", "confidence": 0.8099244236946106}]}, {"text": "Following the definition of partial parses in (Kasper et al., 1999), different partial parse selection methods are presented and evaluated on the basis of multiple metrics, from both the syntactic and semantic viewpoints.", "labels": [], "entities": [{"text": "partial parse selection", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.771422823270162}]}, {"text": "The application of the partial parsing in spontaneous speech texts processing shows promising competence of the method.", "labels": [], "entities": [{"text": "partial parsing", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.5993697941303253}, {"text": "spontaneous speech texts processing", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.7200958728790283}]}], "introductionContent": [{"text": "Linguistically deep processing is of high theoretical and application interest because of its ability to deliver fine-grained accurate analyses of natural language sentences.", "labels": [], "entities": []}, {"text": "Unlike shallow methods which usually return analyses for any input, deep processing methods with precision grammars normally make a clear grammaticality judgment on inputs, therefore avoiding the generation of erroneous analyses for less well-formed inputs.", "labels": [], "entities": []}, {"text": "This is a desirable feature, for it allows fora more accurate modeling of language itself.", "labels": [], "entities": []}, {"text": "However, this feature largely limits the robustness of deep processing, for when a sentence is judged to be ungrammatical, normally no analysis is generated.", "labels": [], "entities": []}, {"text": "When faced with the noisy inputs in real applications (e.g., input errors introduced by speech recognizers or other pre-processors, mildly ungrammatical sentences with fragmental utterances, selfediting chunks or filler words in spoken texts, and so forth), lack of robustness means poor coverage, and makes deep processing less competitive as compared to shallow methods.", "labels": [], "entities": [{"text": "coverage", "start_pos": 288, "end_pos": 296, "type": "METRIC", "confidence": 0.9542064070701599}]}, {"text": "Take the English Resource Grammar (ERG;), a large-scale accurate HPSG for English, for example.", "labels": [], "entities": []}, {"text": "() reported coverage of 57% of the strings with full lexical span from the British National Corpus (BNC).", "labels": [], "entities": [{"text": "coverage", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9541929364204407}, {"text": "British National Corpus (BNC)", "start_pos": 75, "end_pos": 104, "type": "DATASET", "confidence": 0.9789205690224966}]}, {"text": "Although recent extensions to the grammar and lexicon have improved the coverage significantly, full coverage over unseen texts by the grammar is still not anywhere insight.", "labels": [], "entities": []}, {"text": "Other domains are even more likely to not fit into ERG's universe, such as transcripts of spontaneously produced speech where speaker errors and disfluencies are common.", "labels": [], "entities": []}, {"text": "Using a recent version of the ERG, we are notable to parse 22.6% of a random sample of 500 utterances of conversational telephone speech data.", "labels": [], "entities": []}, {"text": "76.1% of the unparsed data was independently found to contain speaker errors and disfluencies, and the remaining data either contained filled pauses or other structures unaccounted for in the grammar.", "labels": [], "entities": []}, {"text": "Correctly recognizing and interpreting the substrings in the utterance which have coherent deep syntax is useful both for semantic analysis and as building blocks for attempts to reconstruct the disfluent spontaneously produced utterances into wellformed sentences.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.8420299589633942}]}, {"text": "For these reasons, it is preferable to exploit the intermediate syntactic and semantic analysis even if the full analysis is not available.", "labels": [], "entities": []}, {"text": "Various efforts have been made on the partiality of language processing.", "labels": [], "entities": []}, {"text": "In bottom-up chart parsing, the passive parser edges licensed by the grammar can betaken as partial analyses.", "labels": [], "entities": [{"text": "bottom-up chart parsing", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.6548064549763998}]}, {"text": "However, as pointed out in (, not all passive edges are good candidates, as not all of them provide useful syntactic/semantic information.", "labels": [], "entities": []}, {"text": "Moreover, the huge amount of passive edges suggests the need fora technique of selecting an optimal subset of them.", "labels": [], "entities": []}, {"text": "During recent development in statistical parse disambiguation, the use of log-linear models has been pretty much standardized.", "labels": [], "entities": [{"text": "statistical parse disambiguation", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.7828337550163269}]}, {"text": "However, it remains to be explored whether the techniques can be adapted for partial parse selection.", "labels": [], "entities": [{"text": "partial parse selection", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.696795125802358}]}, {"text": "In this paper, we adopt the same definition for partial parse as in) and define the task of partial parse selection.", "labels": [], "entities": [{"text": "partial parse selection", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.7471756438414255}]}, {"text": "Several dif-ferent partial parse selection models are presented and implemented for an efficient HPSG parser -PET.", "labels": [], "entities": []}, {"text": "One of the main difficulties in the research of partial analyses is the lack of good evaluation measurements.", "labels": [], "entities": []}, {"text": "Pure syntactic comparisons for parser evaluation are not good as they are very much specific to the annotation guidelines.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8598424196243286}]}, {"text": "Also, the deep grammars we are working with are not automatically extracted from annotated corpora.", "labels": [], "entities": []}, {"text": "Therefore, unless there are partial treebanks built specifically for the deep grammars, there is simply no 'gold' standard for non-golden partial analyses.", "labels": [], "entities": []}, {"text": "Instead, in this paper, we evaluate the partial analyses results on the basis of multiple metrics, from both the syntactic and semantic point of views.", "labels": [], "entities": []}, {"text": "Empirical evaluation has been done with the ERG on a small set of texts from the Wall Street Journal Section 22 of the Penn Treebank (.", "labels": [], "entities": [{"text": "Wall Street Journal Section 22 of the Penn Treebank", "start_pos": 81, "end_pos": 132, "type": "DATASET", "confidence": 0.9399145642916361}]}, {"text": "A pilot study of applying partial parsing in spontaneous speech text processing is also carried out.", "labels": [], "entities": [{"text": "partial parsing", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.6273553669452667}, {"text": "speech text processing", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.606734941403071}]}, {"text": "The remainder of the paper is organized as follow.", "labels": [], "entities": []}, {"text": "Section 2 provides background knowledge about partial analysis.", "labels": [], "entities": [{"text": "partial analysis", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.8066244721412659}]}, {"text": "Section 3 presents various partial parse selection models.", "labels": [], "entities": []}, {"text": "Section 4 describes the evaluation setup and results.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of partial parses is not as easy as the evaluation of full parses.", "labels": [], "entities": []}, {"text": "For full parsers, there are generally two ways of evaluation.", "labels": [], "entities": []}, {"text": "For parsers that are trained on a treebank using an automatically extracted grammar, an unseen set of manually annotated data is used as the test set.", "labels": [], "entities": []}, {"text": "The parser output on the test set is compared to the gold standard annotation, either with the widely used PARSEVAL measurement, or with more annotation-neutral dependency relations.", "labels": [], "entities": [{"text": "PARSEVAL measurement", "start_pos": 107, "end_pos": 127, "type": "METRIC", "confidence": 0.9345200955867767}]}, {"text": "For parsers based on manually compiled grammars, more human judgment is involved in the evaluation.", "labels": [], "entities": []}, {"text": "With the evolution of the grammar, the treebank as the output from the grammar changes overtime ).", "labels": [], "entities": []}, {"text": "The grammar writer inspects the parses generated by the grammar and either \"accepts\" or \"rejects\" the analysis.", "labels": [], "entities": []}, {"text": "In partial parsing for manually compiled grammars, the criterion for acceptable analyses is less evident.", "labels": [], "entities": [{"text": "partial parsing for manually compiled grammars", "start_pos": 3, "end_pos": 49, "type": "TASK", "confidence": 0.6800175408522288}]}, {"text": "Most current treebanking tools are not designed for annotating partial analyses.", "labels": [], "entities": []}, {"text": "Large-scale manually annotated treebanks do have the annotation for sentences that deep grammars are notable to fully analyze.", "labels": [], "entities": []}, {"text": "And the annotation difference in other language resources makes the comparison less straightforward.", "labels": [], "entities": []}, {"text": "More complication is involved with the platform and resources used in our experiment.", "labels": [], "entities": []}, {"text": "Since the DELPH-IN grammars (ERG, JaCY, GG) use MRS for semantics representation, there is no reliable way of evaluating the output with traditional metrics, i.e., dependency relations.", "labels": [], "entities": []}, {"text": "In this paper, we use both manual and automatic evaluation methods on the partial parsing results.", "labels": [], "entities": []}, {"text": "Different processing resources are used to help the evaluation from the syntactic, as well as the semantic point of view.", "labels": [], "entities": []}, {"text": "In order to evaluate the quality of the syntactic structures of the partial parses, we implemented the partial parse models described in the previous section in the PET parser.", "labels": [], "entities": [{"text": "PET parser", "start_pos": 165, "end_pos": 175, "type": "TASK", "confidence": 0.6700286865234375}]}, {"text": "The Nov-06 version of the ERG is used for the experiment.", "labels": [], "entities": [{"text": "ERG", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.678705096244812}]}, {"text": "As test set, we used a subset of sentences from the Wall Street Journal Section 22 from the Penn Treebank.", "labels": [], "entities": [{"text": "Wall Street Journal Section 22 from the Penn Treebank", "start_pos": 52, "end_pos": 105, "type": "DATASET", "confidence": 0.9483159515592787}]}, {"text": "The subset contains 143 sentences which do not receive any full analysis licensed by the grammar, and do not contain lexical gaps (input tokens for which the grammar cannot create any lexical edge).", "labels": [], "entities": []}, {"text": "The average sentence length is 24 words.", "labels": [], "entities": []}, {"text": "Due to the inconsistency of the tokenisation, bracketing and branching between the Penn Treebank annotation and the handling in ERG, we manually checked the partial parse derivation trees.", "labels": [], "entities": [{"text": "Penn Treebank annotation", "start_pos": 83, "end_pos": 107, "type": "DATASET", "confidence": 0.9804156223932902}, {"text": "ERG", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.8833580017089844}]}, {"text": "Each output is marked as one of the three cases: GBL if both the bracketing and the labeling of the partial parse derivation trees are good (with no more than two brackets crossing or four false labelings); GB if the bracketings of the derivation trees are good (with no more than two brackets crossing), but the labeling is bad (with more than four false labelings); or E if otherwise.", "labels": [], "entities": [{"text": "GBL", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.8287547826766968}, {"text": "GB", "start_pos": 207, "end_pos": 209, "type": "METRIC", "confidence": 0.9920857548713684}, {"text": "E", "start_pos": 371, "end_pos": 372, "type": "METRIC", "confidence": 0.980798065662384}]}, {"text": "The manual evaluation results are listed in Table 1.", "labels": [], "entities": []}, {"text": "The test set is processed with two models presented in Section 3.3 (M-I for model I, M-II for model II).", "labels": [], "entities": []}, {"text": "For comparison, we also evaluate for the approach using the shortest path with heuristic weights (denoted by SP).", "labels": [], "entities": []}, {"text": "In case there are more than one path found with the same weight, only the first one is recorded and evaluated.", "labels": [], "entities": []}, {"text": "The results show that the na\u00a8\u0131vena\u00a8\u0131ve shortest path approach based on the heuristic weights works pretty well at predicting the bracketing (with 83.3% of the partial parses having less than two brackets crossing).", "labels": [], "entities": [{"text": "predicting the bracketing", "start_pos": 114, "end_pos": 139, "type": "TASK", "confidence": 0.6297569970289866}]}, {"text": "But, when the labeling is also evaluated it is worse than model I, and even more significantly outperformed by model II.", "labels": [], "entities": []}, {"text": "Evaluation of the syntactic structure only reflects the partial parse quality from some aspects.", "labels": [], "entities": []}, {"text": "In order to get a more thorough comparison between different selection models, we look at the semantic output generated from the partial parses.", "labels": [], "entities": []}, {"text": "The same set of 143 sentences from the Wall Street Journal Section 22 of the Penn Treebank is used.", "labels": [], "entities": [{"text": "Wall Street Journal Section 22 of the Penn Treebank", "start_pos": 39, "end_pos": 90, "type": "DATASET", "confidence": 0.9524575471878052}]}, {"text": "The RMRS semantic representations are generated from the partial parses with different selection models.", "labels": [], "entities": [{"text": "RMRS semantic", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.7923363149166107}]}, {"text": "To compare with, we used RASP 2 ( ), a domain-independent robust parsing system for English.", "labels": [], "entities": []}, {"text": "According to), the parser achieves fairly good accuracy around 80%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.999177873134613}]}, {"text": "The reasons why we choose RASP for the evaluation are: i) RASP has reasonable coverage and accuracy; ii) its output can be converted into RMRS representation with the LKB system.", "labels": [], "entities": [{"text": "coverage", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9733917117118835}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9982755184173584}, {"text": "LKB", "start_pos": 167, "end_pos": 170, "type": "DATASET", "confidence": 0.9400441646575928}]}, {"text": "Since there is no large scale (R)MRS treebank with sentences not covered by the DELPH-IN precision grammars, we hope to use the RASP's RMRS output as a standalone annotation to help the evaluation of the different partial parse selection models.", "labels": [], "entities": [{"text": "DELPH-IN precision", "start_pos": 80, "end_pos": 98, "type": "METRIC", "confidence": 0.774025022983551}, {"text": "RASP's RMRS output", "start_pos": 128, "end_pos": 146, "type": "DATASET", "confidence": 0.8735783845186234}, {"text": "partial parse selection", "start_pos": 214, "end_pos": 237, "type": "TASK", "confidence": 0.7356472810109457}]}, {"text": "To compare the RMRS from the RASP and the partial parse selection models, we used the similarity measurement proposed in).", "labels": [], "entities": [{"text": "RMRS", "start_pos": 15, "end_pos": 19, "type": "TASK", "confidence": 0.8020444512367249}, {"text": "RASP", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.48776012659072876}]}, {"text": "The comparison outputs a distance value between two different RMRSes.", "labels": [], "entities": []}, {"text": "We normalized the distance value to be between 0 and 1.", "labels": [], "entities": []}, {"text": "For each selection model, the average RMRS distance from the RASP output is listed in.: RMRS distance to RASP outputs Again, we see that the outputs of model II achieve the highest similarity when compared with the RASP output.", "labels": [], "entities": [{"text": "RMRS distance", "start_pos": 38, "end_pos": 51, "type": "METRIC", "confidence": 0.8244335651397705}, {"text": "RMRS distance", "start_pos": 88, "end_pos": 101, "type": "METRIC", "confidence": 0.9405931532382965}, {"text": "similarity", "start_pos": 181, "end_pos": 191, "type": "METRIC", "confidence": 0.9648476243019104}]}, {"text": "With some manual validation, we do confirm that the different similarity does imply a significant difference in the quality of the output RMRS.", "labels": [], "entities": []}, {"text": "The shortest path with heuristic weights yielded very poor semantic similarity.", "labels": [], "entities": []}, {"text": "The main reason is that not every edge with the same span generates the same semantics.", "labels": [], "entities": []}, {"text": "Therefore, although the SP receives reasonable bracketing accuracy, it has less idea of the goodness of different edges with the same span.", "labels": [], "entities": [{"text": "bracketing", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.9294568300247192}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.8412246704101562}]}, {"text": "By incorporating P (t i |w i ) in the scoring model, the model I and II can produce RMRSes with much higher quality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Syntactic Evaluation Results", "labels": [], "entities": [{"text": "Syntactic Evaluation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.971567839384079}]}]}