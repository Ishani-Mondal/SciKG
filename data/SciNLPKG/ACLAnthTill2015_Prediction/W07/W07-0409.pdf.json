{"title": [{"text": "Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Combining Morphosyntactic Enriched Representation with n-best Reranking in Statistical Translation", "labels": [], "entities": [{"text": "SSST, NAACL-HLT 2007 / AMTA Workshop", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.6036366820335388}, {"text": "Statistical Combining Morphosyntactic Enriched Representation", "start_pos": 79, "end_pos": 140, "type": "TASK", "confidence": 0.8593950867652893}, {"text": "Statistical Translation", "start_pos": 166, "end_pos": 189, "type": "TASK", "confidence": 0.8607190549373627}]}], "abstractContent": [{"text": "The purpose of this work is to explore the integration of morphosyntactic information into the translation model itself, by enriching words with their morphosyntac-tic categories.", "labels": [], "entities": []}, {"text": "We investigate word dis-ambiguation using morphosyntactic categories , n-best hypotheses reranking, and the combination of both methods with word or morphosyntactic n-gram language model reranking.", "labels": [], "entities": [{"text": "word dis-ambiguation", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.697580099105835}]}, {"text": "Experiments are carried out on the English-to-Spanish translation task.", "labels": [], "entities": [{"text": "English-to-Spanish translation task", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.7492650747299194}]}, {"text": "Using the morphosyn-tactic language model alone does not results in any improvement in performance.", "labels": [], "entities": []}, {"text": "However, combining morphosyn-tactic word disambiguation with a word based 4-gram language model results in a relative improvement in the BLEU score of 2.3% on the development set and 1.9% on the test set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 137, "end_pos": 147, "type": "METRIC", "confidence": 0.98509681224823}]}], "introductionContent": [{"text": "Recent works in statistical machine translation (SMT) shows how phrase-based modeling) significantly outperform the historical word-based modeling.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.7937962462504705}]}, {"text": "Using phrases, i.e. sequences of words, as translation units allows the system to preserve local word order constraints and to improve the consistency of phrases during the translation process.", "labels": [], "entities": []}, {"text": "Phrase-based models provide some sort of context information as opposed to word-based models.", "labels": [], "entities": []}, {"text": "Training a phrase-based model typically requires aligning a parallel corpus, extracting phrases and scoring them using word and phrase counts.", "labels": [], "entities": []}, {"text": "The derived statistics capture the structure of natural language to some extent, including implicit syntactic and semantic relations.", "labels": [], "entities": []}, {"text": "The output of a SMT system maybe difficult to understand by humans, requiring re-ordering words to recover its syntactic structure.", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9942055344581604}]}, {"text": "Modeling language generation as a word-based Markovian source (an ngram language model) discards linguistic properties such as long term word dependency and word-order or phrase-order syntactic constraints.", "labels": [], "entities": []}, {"text": "Therefore, explicit introduction of structure in the language models becomes a major and promising focus of attention.", "labels": [], "entities": []}, {"text": "However, as of today, it seems difficult to outperform a 4-gram word language model.", "labels": [], "entities": []}, {"text": "Several studies have attempted to use morphosyntactic information (also known as part-of-speech or POS information) to improve translation.", "labels": [], "entities": []}, {"text": "() have explored many different feature functions.", "labels": [], "entities": []}, {"text": "Reranking n-best lists using POS has also been explored by).", "labels": [], "entities": []}, {"text": "In (, a factored language model using POS information showed similar performance to a 4-gram word language model.", "labels": [], "entities": []}, {"text": "Syntax-based language models have also been investigated in (.", "labels": [], "entities": []}, {"text": "All these studies use word phrases as translation units and POS information in just a post-processing step.", "labels": [], "entities": []}, {"text": "This paper explores the integration of morphosyntactic information into the translation model itself by enriching words with their morphosyntactic cat-egories.", "labels": [], "entities": []}, {"text": "The same idea has already been applied in ( to the Basic Travel Expression Corpus (BTEC).", "labels": [], "entities": [{"text": "Basic Travel Expression Corpus (BTEC)", "start_pos": 51, "end_pos": 88, "type": "DATASET", "confidence": 0.7721083036490849}]}, {"text": "To our knowledge, this approach has not been evaluated on a large realword translation problem.", "labels": [], "entities": [{"text": "realword translation", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.7183824628591537}]}, {"text": "We report results on the TC-STAR task (public European Parliament Plenary Sessions translation).", "labels": [], "entities": [{"text": "TC-STAR task (public European Parliament Plenary Sessions translation", "start_pos": 25, "end_pos": 94, "type": "DATASET", "confidence": 0.709031899770101}]}, {"text": "Furthermore, we propose to combine this approach with classical n-best list reranking.", "labels": [], "entities": []}, {"text": "Experiments are carried out on the English-to-Spanish task using a system based on the publicly available Moses decoder.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: In Section 2 we first describe the baseline statistical machine translation systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.6024034122625986}]}, {"text": "Section 3 presents the considered task and the processing of the corpora.", "labels": [], "entities": []}, {"text": "The experimental evaluation is summarized in section 4.", "labels": [], "entities": []}, {"text": "The paper concludes with a discussion of future research directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two baseline English-to-Spanish translation models were created with Moses.", "labels": [], "entities": []}, {"text": "The first model was trained on the whole parallel text -note that sentences with more than 100 words are excluded by Giza++.", "labels": [], "entities": []}, {"text": "The second model was trained on the corpus using only sentences with at most 40 words.", "labels": [], "entities": []}, {"text": "The BLEU score on the development set using good general purpose weights is 48.0 for the first model and 47.0 for the second.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.999397873878479}]}, {"text": "Because training on the whole bi-text is much slower, we decided to perform our experiments on the bi-texts restricted to the \"short\" sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the parallel texts used to train  the statistical machine translation system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.6822508970896403}]}, {"text": " Table 3: BLEU scores using enriched translation  units.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998177170753479}]}]}