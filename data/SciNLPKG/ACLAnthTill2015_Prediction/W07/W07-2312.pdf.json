{"title": [{"text": "Measuring Variability in Sentence Ordering for News Summarization", "labels": [], "entities": [{"text": "Sentence Ordering", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.8320621252059937}, {"text": "News Summarization", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.72882279753685}]}], "abstractContent": [{"text": "The issue of sentence ordering is an important one for natural language tasks such as multi-document summarization, yet there has not been a quantitative exploration of the range of acceptable sentence orderings for short texts.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7254728376865387}, {"text": "multi-document summarization", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.7101976275444031}]}, {"text": "We present results of a sentence reordering experiment with three experimental conditions.", "labels": [], "entities": [{"text": "sentence reordering", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7045038044452667}]}, {"text": "Our findings indicate a very high degree of variability in the orderings that the eighteen subjects produce.", "labels": [], "entities": []}, {"text": "In addition, the variability of reorderings is significantly greater when the initial ordering seen by subjects is different from the original summary.", "labels": [], "entities": []}, {"text": "We conclude that evaluation of sentence ordering should use multiple reference or-derings.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7019974291324615}]}, {"text": "Our evaluation presents several metrics that might prove useful in assessing against multiple references.", "labels": [], "entities": []}, {"text": "We conclude with a deeper set of questions: (a) what sorts of independent assessments of quality of the different reference orderings could be made and (b) whether a large enough test set would obviate the need for such independent means of quality assessment.", "labels": [], "entities": []}], "introductionContent": [{"text": "The issue of ordering content in a multi-document extractive summary is an important problem that has received little attention until recently.", "labels": [], "entities": [{"text": "ordering content in a multi-document extractive summary", "start_pos": 13, "end_pos": 68, "type": "TASK", "confidence": 0.7308444636208671}]}, {"text": "Sentence ordering, along with other factors that affect coherence and readability, is of particular concern for multi-document summarization, where different source articles contribute sentences to a summary.", "labels": [], "entities": [{"text": "Sentence ordering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9103318750858307}, {"text": "multi-document summarization", "start_pos": 112, "end_pos": 140, "type": "TASK", "confidence": 0.5716066956520081}]}, {"text": "We conducted an exploratory study to determine how much variation humans would produce in a reordering task under different experimental conditions, in order to assess the issues for evaluating automated reordering.", "labels": [], "entities": []}, {"text": "While a good ordering is essential for summary comprehension (), and recent work on sentence ordering () does show promise, it is important to note that determining an optimal sentence ordering fora given summary may not be feasible.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.740748256444931}]}, {"text": "The question for evaluation of ordering is whether there is a single best ordering that humans will converge on, or that would lead to maximum reading comprehension, or that would maximize another extrinsic summary evaluation measure.", "labels": [], "entities": []}, {"text": "On texts of approximately the same length as summaries we look at here, found that experts produce different sentence orderings for expressing database facts about archaeology.", "labels": [], "entities": []}, {"text": "We find that summaries of newswire have a relatively larger set of coherent orderings.", "labels": [], "entities": []}, {"text": "We conducted an experiment where human subjects were asked to reorder multi-document summaries in order to maximize their coherence.", "labels": [], "entities": []}, {"text": "The summaries used in this experiment were originally produced by a different set of human summarizers as part of a multi-document summarization task that was conducted by NIST in 2004.", "labels": [], "entities": [{"text": "multi-document summarization task", "start_pos": 116, "end_pos": 149, "type": "TASK", "confidence": 0.622874915599823}]}, {"text": "We present quantitative results that show that there is a large amount of variability among the reorderings considered coherent.", "labels": [], "entities": []}, {"text": "On this basis, we suggest that evaluation of sentence ordering should use multiple references.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.6961740255355835}]}, {"text": "For each such summary in the experiment, we create three initial sentence orderings: (a) original order (b) random order, and (c) the output of an automated ordering algorithm.", "labels": [], "entities": []}, {"text": "We show that: \u2022 The initial orderings presented to the human subjects have a statistically significant impact on the reorderings that they create.", "labels": [], "entities": []}, {"text": "\u2022 The set of individual human reorderings ex-hibits a significant amount of variability.", "labels": [], "entities": []}, {"text": "The next section provides some background for the sentence ordering task and presents the automated sentence ordering algorithm used in our experiments.", "labels": [], "entities": [{"text": "sentence ordering task", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.8471954862276713}, {"text": "sentence ordering", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.727413684129715}]}, {"text": "Section 3 describes the experimental design.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 present quantitative analyses of the results of the experiment.", "labels": [], "entities": []}, {"text": "Section 6 discusses related work.", "labels": [], "entities": []}, {"text": "We discuss our results in Section 7 and conclude in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We designed an experiment to test two hypotheses: (1) that the initial orderings presented to the human subjects have a statistically significant impact on the reorderings that they create, and (2) that the set of individual human reorderings exhibits a significant amount of variability.", "labels": [], "entities": []}, {"text": "For our experiment, we randomly chose nine 100-word human-written summaries \u2020 out of 200 human written summaries produced by NIST; they were used as references to evaluate extractive multidocument summaries in 2004).", "labels": [], "entities": [{"text": "NIST", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.9157761931419373}]}, {"text": "We later retrieved the quality judgments performed by NIST assessors on seven of the summaries; the remaining two were used as a reference model for assessors and had no quality judgments.", "labels": [], "entities": [{"text": "NIST", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.9556694030761719}]}, {"text": "The seven summaries for which we had judgments were all given high ratings of 1 or 2 (out of 5) on seven questions such as, Does the summary build from sentence to sentence to a coherent body of information about the topic?", "labels": [], "entities": []}, {"text": "The nine summaries were evenly divided into three different groups: S 1\u22123 , S 4\u22126 and S 7\u22129 . For each summary, we used three orderings: \u2022 O: the original ordering of sentences in the summary, as written by the author of the summary.", "labels": [], "entities": [{"text": "O", "start_pos": 139, "end_pos": 140, "type": "METRIC", "confidence": 0.9877350926399231}]}, {"text": "\u2022 R: a random ordering of the sentences \u2022 T: an ordering created by applying the TSP ordering algorithm described in the previous section.", "labels": [], "entities": []}, {"text": "We constrained the random and the TSP orderings so that the first sentence of the human summary appeared first.", "labels": [], "entities": [{"text": "TSP orderings", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.5873187482357025}]}, {"text": "Eighteen human subjects were divided into three groups (I, II, and III), 6 subjects per group.", "labels": [], "entities": []}, {"text": "We presented each subject with each of the nine summaries, in either its original ordering (condition CO ), random ordering (condition C R ), or TSP ordering (condition CT ), as described in the Latin square design of.", "labels": [], "entities": [{"text": "TSP ordering (condition CT )", "start_pos": 145, "end_pos": 173, "type": "METRIC", "confidence": 0.8593575855096182}]}, {"text": "For example, the six subjects in group II were presented with summaries 1 \u2212 3 in random order, 4 \u2212 6 in original order, and 7 \u2212 9 in TSP order.", "labels": [], "entities": [{"text": "TSP", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.745187520980835}]}, {"text": "Thus the experiment produced 18 reorderings for each of the nine summaries, six per initial order.", "labels": [], "entities": []}, {"text": "\u2020 D30024-C (Document set D30024, NIST author ID C), The human subjects chosen for the experiment were all native English speakers.", "labels": [], "entities": [{"text": "D30024-C", "start_pos": 2, "end_pos": 10, "type": "DATASET", "confidence": 0.7407687306404114}, {"text": "NIST author ID C)", "start_pos": 33, "end_pos": 50, "type": "METRIC", "confidence": 0.7186198592185974}]}, {"text": "Subjects accessed the task on a website, including the instructions, which explained that they would be reading a document on the screen and could reorder the sentences in that document so as to make the document more coherent.", "labels": [], "entities": []}, {"text": "In order to prevent the introduction of any bias, the order of presentation of summaries was randomized for every subject.", "labels": [], "entities": []}, {"text": "The instructions clearly specified the possibility that summaries might need little or no reordering.", "labels": [], "entities": [{"text": "summaries", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.9785311222076416}]}, {"text": "It would be difficult to measure whether the instructions led subjects to believe that all summaries could be improved by reordering.", "labels": [], "entities": []}, {"text": "We do not have objective criteria to identify a control set of summaries that cannot be improved by reordering; in fact, this is a subjective judgement that is likely to vary between individuals.", "labels": [], "entities": []}, {"text": "Because the three experimental conditions had the same instructions, we believe the significant differences in amount of reordering across conditions is areal effect rather than an artifact.", "labels": [], "entities": []}, {"text": "To measure the variability across the experimental conditions, we developed two methods that assign a global score to each set of reorderings by comparing them to a particular reference point.", "labels": [], "entities": []}], "tableCaptions": []}