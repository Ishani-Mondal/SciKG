{"title": [{"text": "Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Machine Translation as Tree Labeling", "labels": [], "entities": [{"text": "SSST, NAACL-HLT 2007 / AMTA Workshop", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.596386581659317}, {"text": "Statistical Machine Translation as Tree Labeling", "start_pos": 79, "end_pos": 127, "type": "TASK", "confidence": 0.7598618070284525}]}], "abstractContent": [{"text": "We present the main ideas behind anew syntax-based machine translation system, based on reducing the machine translation task to a tree-labeling task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7108830809593201}, {"text": "machine translation task", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.7595703701178232}]}, {"text": "This tree labeling is further reduced to a sequence of decisions (of four varieties), which can be discriminatively trained.", "labels": [], "entities": []}, {"text": "The optimal tree labeling (i.e. translation) is then found through a simple depth-first branch-and-bound search.", "labels": [], "entities": []}, {"text": "An early system founded on these ideas has been shown to be competitive with Pharaoh when both are trained on a small subsection of the Eu-roparl corpus.", "labels": [], "entities": [{"text": "Eu-roparl corpus", "start_pos": 136, "end_pos": 152, "type": "DATASET", "confidence": 0.7429715692996979}]}, {"text": "1 Motivation Statistical machine translation has, fora while now, been dominated by the phrase-based translation paradigm (Och and Ney, 2003).", "labels": [], "entities": [{"text": "Motivation Statistical machine translation", "start_pos": 2, "end_pos": 44, "type": "TASK", "confidence": 0.7582909166812897}, {"text": "phrase-based translation", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.687337651848793}]}, {"text": "In this paradigm, sentences are translated from a source language to a target language through the repeated substitution of contiguous word sequences (\"phrases\") from the source language for word sequences in the target language.", "labels": [], "entities": []}, {"text": "Training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus for identifying corresponding word blocks, assuming no further linguistic analysis of the source or target language.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8187417387962341}]}, {"text": "In decoding , these systems then typically rely on n-gram language models and simple statistical reordering models to shuffle the phrases into an order that is coherent in the target language.", "labels": [], "entities": []}, {"text": "There are limits to what such an approach can ultimately achieve.", "labels": [], "entities": []}, {"text": "Machine translation based on a deeper analysis of the syntactic structure of a sentence has long been identified as a desirable objective in principle (consider (Wu, 1997; Yamada and Knight, 2001)).", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7886747419834137}]}, {"text": "However, attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success (Koehn et al., 2003; Och et al., 2003) 1 , and purely phrase-based machine translation systems continue to outperform these syntax/phrase-based hybrids.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 172, "end_pos": 204, "type": "TASK", "confidence": 0.6564942002296448}]}, {"text": "In this work, we try to make afresh start with syntax-based machine translation, discarding the phrase-based paradigm and designing a machine translation system from the ground up, using syntax as our central guiding star.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7341688424348831}, {"text": "machine translation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.7223078310489655}]}, {"text": "Evaluation with BLEU and a detailed manual error analysis of our nascent system show that this new approach might well have the potential to finally realize some of the promises of syntax.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9914114475250244}]}, {"text": "2 Problem Formulation We want to build a system that can learn to translate sentences from a source language to a destination language.", "labels": [], "entities": [{"text": "Problem Formulation", "start_pos": 2, "end_pos": 21, "type": "TASK", "confidence": 0.7047040164470673}]}, {"text": "1 (Chiang, 2005) also reports that with his hierarchical generalization of the phrase-based approach, the addition of parser information doesn't lead to any improvements.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In this section, we evaluate a preliminary Englishto-German translation system based on the ideas outlined in this paper.", "labels": [], "entities": [{"text": "Englishto-German translation", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.49686263501644135}]}, {"text": "We first present a quantiative comparison with the phrase-based approach, using the BLEU metric; then we discuss two concrete translation examples as a preliminary qualitative evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9956125020980835}]}, {"text": "Finally, we present a detailed manual error analysis.", "labels": [], "entities": []}, {"text": "Our data was a subset of the Europarl corpus consisting of sentences of lengths ranging from 8 to 17 words.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.9932272434234619}]}, {"text": "Our training corpus contained 50000 sentences and our test corpus contained 300 sentences.", "labels": [], "entities": []}, {"text": "We also had a small number of reserved sentences for development.", "labels": [], "entities": []}, {"text": "The English sentences were parsed using the Bikel parser, and the sentences were aligned with GIZA++).", "labels": [], "entities": [{"text": "Bikel", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.8764435052871704}]}, {"text": "We used the WEKA machine learning package) to train the distributions (specifically, we used model trees).", "labels": [], "entities": [{"text": "WEKA machine learning package", "start_pos": 12, "end_pos": 41, "type": "DATASET", "confidence": 0.821255087852478}]}, {"text": "For comparison, we also trained and evaluated Pharaoh () on this limited corpus, using Pharaoh's default parameters.", "labels": [], "entities": []}, {"text": "Pharaoh achieved a BLEU score of 11.17 on the test set, whereas our system achieved a BLEU score of 11.52.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9885989129543304}, {"text": "BLEU score", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.988226979970932}]}, {"text": "What is notable here is not the scores themselves (low due to the size of the training corpus).", "labels": [], "entities": []}, {"text": "However our system managed to perform comparably with Pharaoh in a very early stage of its development, with rudimentary features and without the benefit of an n-gram language model.", "labels": [], "entities": []}, {"text": "Let's take a closer look at the sentences produced by our system, to gain some insight as to its current strengths and weaknesses.", "labels": [], "entities": []}, {"text": "Starting with the English sentence (note that all data is lowercase): i agree with the spirit of those amendments . Our system produces: The GHKM tree is depicted in.", "labels": [], "entities": [{"text": "GHKM tree", "start_pos": 141, "end_pos": 150, "type": "DATASET", "confidence": 0.9384082853794098}]}, {"text": "The key feature of this translation is how the English phrase \"agree with\" is translated as the German \"stimme ...", "labels": [], "entities": []}, {"text": "Such a feat is difficult to produce consistently with a purely phrase-based system, as phrases of arbitrary length can be placed between the words \"stimme\" and \"zu\", as we can see happening in this particular example.", "labels": [], "entities": []}, {"text": "By contrast, Pharaoh opts for the following (somewhat less desirable) translation: A weakness in our system is also evident here.", "labels": [], "entities": []}, {"text": "The German noun \"Geist\" is masculine, thus our system uses the wrong article (a problem that Pharaoh, with its embedded n-gram language model, does not encounter).", "labels": [], "entities": []}, {"text": "In general, it seems that our system is superior to Pharaoh at figuring out the proper way to arrange the words of the output sentence, and inferior to Pharaoh at finding what the actual translation of those words should be.", "labels": [], "entities": []}, {"text": "Consider the English sentence: we shall submit a proposal along these lines before the end of this year . Here we have an example of a double verb: \"shall submit.\"", "labels": [], "entities": []}, {"text": "In German, the second verb should goat the end of the sentence, and this is achieved by our system (translating \"shall\" as \"werden\", and \"submit\" as \"vorlegen\").", "labels": [], "entities": []}, {"text": "Pharaoh does not manage this (translating \"submit\" as \"unterbreiten\" and placing it mid-sentence).", "labels": [], "entities": []}, {"text": "It is worth noting that while our system gets the word order of the output system right, it makes several agreement mistakes and (like Pharaoh) doesn't get the translation of \"along these lines\" right.", "labels": [], "entities": []}, {"text": "To have a more systematic basis for comparison, we did a manual error analysis for 100 sentences from the test set.", "labels": [], "entities": []}, {"text": "A native speaker of German (in the present pilot study one of the authors) determined the editing steps required to transform the system output into an acceptable translation -both in terms of fluency and adequacy of translation.", "labels": [], "entities": []}, {"text": "In order to avoid a bias for our system, we randomized the presentation of output from one of the two systems.", "labels": [], "entities": []}, {"text": "We defined the following basic types of edits, with further subdistinctions depending on the word type: ADD, DELETE, CHANGE and MOVE.", "labels": [], "entities": [{"text": "ADD", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9728509783744812}, {"text": "DELETE", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9957028031349182}, {"text": "CHANGE", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9864571690559387}, {"text": "MOVE", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.8846317529678345}]}, {"text": "A special type TRANSLATE-untranslated was assumed for untranslated source words in the output.", "labels": [], "entities": [{"text": "TRANSLATE-untranslated", "start_pos": 15, "end_pos": 37, "type": "METRIC", "confidence": 0.8673157691955566}]}, {"text": "For the CHANGE, more fine-grained distinctions were made.", "labels": [], "entities": [{"text": "CHANGE", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.5623617768287659}]}, {"text": "2 A single MOVE operation was assumed to displace an entire phrase; the distance of the movement in terms of the number of words was calculated.", "labels": [], "entities": []}, {"text": "The table in shows the edits required for correcting the output of the two systems on 100 sentences.", "labels": [], "entities": []}, {"text": "We again observe that our system, which is at an early stage of development and contrary to the Pharaoh system does not include an n-gram language model trained on a large corpus, already yields promising results.", "labels": [], "entities": []}, {"text": "The higher proportion of CHANGE operations, in particular CHANGEinflection and CHANGE-function-word edits is presumably a direct consequence of providing a language model or not.", "labels": [], "entities": []}, {"text": "An interesting observation is that our system currently tends to overtranslate, i.e., redundantly produce several translations fora word, which leads to the need of DELETE operations.", "labels": [], "entities": [{"text": "DELETE", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.7830706834793091}]}, {"text": "The Pharaoh system had a tendency to undertranslate, often with crucial words missing.: Edits required for an acceptable system output, based on 100 test sentences.", "labels": [], "entities": [{"text": "Edits", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9964969754219055}]}], "tableCaptions": []}