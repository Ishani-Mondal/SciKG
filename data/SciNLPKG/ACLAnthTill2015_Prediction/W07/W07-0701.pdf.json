{"title": [{"text": "Using Dependency Order Templates to Improve Generality in Translation", "labels": [], "entities": [{"text": "Improve Generality in Translation", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.7834781110286713}]}], "abstractContent": [{"text": "Today's statistical machine translation systems generalize poorly to new domains.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.6737071772416433}]}, {"text": "Even small shifts can cause precipitous drops in translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9463525414466858}]}, {"text": "Phrasal systems rely heavily, for both reordering and contextual translation, on long phrases that simply fail to match out-of-domain text.", "labels": [], "entities": [{"text": "contextual translation", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.6837246865034103}]}, {"text": "Hierarchical systems attempt to generalize these phrases but their learned rules are subject to severe constraints.", "labels": [], "entities": []}, {"text": "Syntactic systems can learn lexicalized and unlexicalized rules, but the joint modeling of lexical choice and reordering can narrow the applicability of learned rules.", "labels": [], "entities": []}, {"text": "The treelet approach models reordering separately from lexical choice, using a discriminatively trained order model, which allows treelets to apply broadly, and has shown better generalization to new domains, but suffers a factorially large search space.", "labels": [], "entities": []}, {"text": "We introduce anew reordering model based on dependency order templates, and show that it outperforms both phrasal and treelet systems on in-domain and out-of-domain text, while limiting the search space.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern phrasal SMT systems such as () derive much of their power from being able to memorize and use long phrases.", "labels": [], "entities": [{"text": "SMT", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.7440009117126465}]}, {"text": "Phrases allow for non-compositional translation, local reordering and contextual lexical choice.", "labels": [], "entities": []}, {"text": "However the phrases are fully lexicalized, which means they generalize poorly to even slightly outof-domain text.", "labels": [], "entities": []}, {"text": "In an open competition) systems trained on parliamentary proceedings were tested on text from 'news commentary' web sites, a very slightly different domain.", "labels": [], "entities": []}, {"text": "The 9 phrasal systems in the English to Spanish track suffered an absolute drop in BLEU score of between 4.4% and 6.34% (14% to 27% relative).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9689078629016876}]}, {"text": "The treelet system of) fared somewhat better but still suffered an absolute drop of 3.61%.", "labels": [], "entities": []}, {"text": "Clearly there is a need for approaches with greater powers of generalization.", "labels": [], "entities": []}, {"text": "There are multiple facets to this issue, including handling of unknown words, new senses of known words etc.", "labels": [], "entities": [{"text": "handling of unknown words", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.8910549879074097}]}, {"text": "In this work, we will focus on the issue of reordering, i.e. can we learn how to transform the sentence structure of one language into the sentence structure of another, in away that is not tied to a specific domain or sub-domains, or indeed, sequences of individual words.", "labels": [], "entities": []}, {"text": "An early attempt at greater generality in a purely phrasal setting was the alignment template approach); newer approaches include formally syntactic, and linguistically syntactic approaches (), ().", "labels": [], "entities": []}, {"text": "In the next section, we examine these representative approaches to the reordering problem.", "labels": [], "entities": [{"text": "reordering problem", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.9185895919799805}]}], "datasetContent": [{"text": "We evaluated the translation quality of the system using the BLEU metric ().", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9491811990737915}, {"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9967858791351318}]}, {"text": "We compared our system to Pharaoh, a leading phrasal SMT decoder (, and our treelet system.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.8955521583557129}]}, {"text": "We report numbers for English to Spanish.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 7.1: System Comparisons across domains", "labels": [], "entities": []}, {"text": " Table 7.2: n-gram recall across domains", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9834858179092407}]}, {"text": " Table  7.3 shows that in practice, the drastic search space  reduction allows the decoder to explore a wider  beam and more rules, leading to reduced search  error and increased translation speed.", "labels": [], "entities": []}, {"text": " Table 7.4 we see that this is not the  case. 8 (Differences are significant at p>=0.99.)", "labels": [], "entities": [{"text": "Differences", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9802675247192383}]}, {"text": " Table 7.4: Templates and discriminative order model", "labels": [], "entities": [{"text": "Templates", "start_pos": 12, "end_pos": 21, "type": "TASK", "confidence": 0.93133544921875}]}, {"text": " Table 7.5: Effect of template count cutoffs", "labels": [], "entities": []}]}