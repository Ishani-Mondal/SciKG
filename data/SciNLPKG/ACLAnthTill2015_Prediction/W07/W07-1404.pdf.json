{"title": [], "abstractContent": [{"text": "This paper reports on LCC's participation at the Third PASCAL Recognizing Textual Entailment Challenge.", "labels": [], "entities": [{"text": "PASCAL Recognizing Textual Entailment Challenge", "start_pos": 55, "end_pos": 102, "type": "TASK", "confidence": 0.6870462536811829}]}, {"text": "First, we summarize our semantic logical-based approach which proved successful in the previous two challenges.", "labels": [], "entities": []}, {"text": "Then we highlight this year's innovations which contributed to an overall accuracy of 72.25% for the RTE 3 test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9995761513710022}, {"text": "RTE 3 test data", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.8910113275051117}]}, {"text": "The novelties include new resources, such as eXtended WordNet KB which provides a large number of world knowledge axioms, event and temporal information provided by the TARSQI toolkit, logic form representations of events, negation, coreference and context, and new improvements of lexical chain axiom generation.", "labels": [], "entities": [{"text": "eXtended WordNet KB", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.8065348267555237}, {"text": "lexical chain axiom generation", "start_pos": 282, "end_pos": 312, "type": "TASK", "confidence": 0.7118181884288788}]}, {"text": "Finally, the system's performance and error analysis are discussed.", "labels": [], "entities": [{"text": "error", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9633258581161499}]}], "introductionContent": [{"text": "Continuing a two-year tradition, the PASCAL Network organized the Third Recognizing Textual Entailment Challenge 1 (RTE 3) to further the research on reasoning systems able to decide whether the meaning of one text (the entailed hypothesis, H) can be inferred from another text (the entailing text, T ).", "labels": [], "entities": [{"text": "PASCAL Network", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.875633031129837}, {"text": "Third Recognizing Textual Entailment Challenge 1 (RTE 3)", "start_pos": 66, "end_pos": 122, "type": "TASK", "confidence": 0.4733911633491516}]}, {"text": "Among this year's challenges, approximately 15% of the (T, H) pairs contained long texts (more details in Section 5.1).", "labels": [], "entities": []}, {"text": "We approach the textual entailment problem as a logical implication between meanings ().", "labels": [], "entities": []}, {"text": "Our system transforms the two text snippets into three-layered semanticallyrich logic form representations, generates an abundant set of lexical, syntactic, semantic, and world knowledge axioms and, iteratively, searches fora proof for the entailment between the text T and a possibly relaxed version of the hypothesis H.", "labels": [], "entities": []}, {"text": "A pair is labeled as positive if the score of the found proof (reflecting H's degree of relaxation) is above a threshold learned on the training data.", "labels": [], "entities": []}, {"text": "summarizes our approach to RTE.", "labels": [], "entities": [{"text": "RTE", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9317857623100281}]}], "datasetContent": [{"text": "The RTE 3 data set was derived with four NLP applications in mind: Information Extraction (IE), Information Retrieval (IR), Question Answering (QA), and Multi-document Summarization (SUM).", "labels": [], "entities": [{"text": "RTE 3 data set", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8504727929830551}, {"text": "Information Extraction (IE)", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.8073497116565704}, {"text": "Information Retrieval (IR)", "start_pos": 96, "end_pos": 122, "type": "TASK", "confidence": 0.8121558129787445}, {"text": "Question Answering (QA)", "start_pos": 124, "end_pos": 147, "type": "TASK", "confidence": 0.8634974956512451}, {"text": "Multi-document Summarization (SUM)", "start_pos": 153, "end_pos": 187, "type": "TASK", "confidence": 0.8124013006687164}]}, {"text": "Statistics for this year's dataset are shown in.", "labels": [], "entities": []}, {"text": "On average, the long texts contain twice the number of words found in texts from pairs marked as short.", "labels": [], "entities": []}, {"text": "extra information captured in the logic representations used in Run #1 (as compared with Run #2) was not the focus of the entailment; the understanding it brings was not exercised during the entailment recognition process.", "labels": [], "entities": [{"text": "entailment recognition process", "start_pos": 191, "end_pos": 221, "type": "TASK", "confidence": 0.7823623617490133}]}, {"text": "For the IR and QA tasks, Run #1 results are better when compared to Run #2's.", "labels": [], "entities": [{"text": "IR", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.910888671875}, {"text": "Run #1", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9268680612246195}]}, {"text": "For these tasks, the performance of the system is much higher when compared with the results obtained for IE and SUM.", "labels": [], "entities": [{"text": "IE", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.6857172250747681}]}, {"text": "Even tough the thresholds learned for these two tasks best separate the positive from the negative pairs on the development set, they prove to be fairly low for the test set.", "labels": [], "entities": []}, {"text": "Almost all positive IE and SUM pairs are identified as such (very high recall for both tasks), but a lot of negatives are also labeled as positives (low precision, smaller accuracy).: NE heuristic's performance for Run #1", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9967672824859619}, {"text": "precision", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9922324419021606}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.996265709400177}]}], "tableCaptions": [{"text": " Table 3: Data split between true and false classes.  The number of pairs with long text is shown in  parenthesis.", "labels": [], "entities": []}, {"text": " Table 4: Results for Run #1 and Run #2", "labels": [], "entities": [{"text": "Run #1", "start_pos": 22, "end_pos": 28, "type": "DATASET", "confidence": 0.6071069439252218}, {"text": "Run #2", "start_pos": 33, "end_pos": 39, "type": "TASK", "confidence": 0.5816431244214376}]}, {"text": " Table 5: NE heuristic's performance for Run #1", "labels": [], "entities": [{"text": "Run #1", "start_pos": 41, "end_pos": 47, "type": "TASK", "confidence": 0.6924019257227579}]}]}