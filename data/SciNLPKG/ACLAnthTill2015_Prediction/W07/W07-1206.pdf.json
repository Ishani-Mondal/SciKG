{"title": [{"text": "Question Answering based on Semantic Roles", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7929158210754395}]}], "abstractContent": [{"text": "This paper discusses how lexical resources based on semantic roles (i.e. FrameNet, PropBank, VerbNet) can be used for Question Answering, especially Web Question Answering.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.874372661113739}, {"text": "Web Question Answering", "start_pos": 149, "end_pos": 171, "type": "TASK", "confidence": 0.594011386235555}]}, {"text": "Two algorithms have been implemented to this end, with quite different characteristics.", "labels": [], "entities": []}, {"text": "We discuss both approaches when applied to each of the resources and a combination of these and give an evaluation.", "labels": [], "entities": []}, {"text": "We argue that employing semantic roles can indeed be highly beneficial fora QA system.", "labels": [], "entities": []}], "introductionContent": [{"text": "A large part of the work done in NLP deals with exploring how different tools and resources can be used to improve performance on a task.", "labels": [], "entities": []}, {"text": "The quality and usefulness of the resource certainly is a major factor for the success of the research, but equally so is the creativity with which these tools or resources are used.", "labels": [], "entities": []}, {"text": "There usually is more than one way to employ these, and the approach chosen largely determines the outcome of the work.", "labels": [], "entities": []}, {"text": "This paper illustrates the above claims with respect to three lexical resources -FrameNet (),) and VerbNet ( -that convey information about lexical predicates and their arguments.", "labels": [], "entities": []}, {"text": "We describe two new and complementary techniques for using these resources and show the improvements to be gained when they are used individually and then together.", "labels": [], "entities": []}, {"text": "We also point out problems that must be overcome to achieve these results.", "labels": [], "entities": []}, {"text": "Compared with WordNet (which has been used widely in QA-FrameNet, PropBank and VerbNet are still relatively new, and therefore their usefulness for QA has still to be proven.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.9464085102081299}, {"text": "PropBank", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9213752150535583}, {"text": "VerbNet", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.8579695820808411}]}, {"text": "They offer the following features which can be used to gain a better understanding of questions, sentences containing answer candidates, and the relations between them: \u2022 They all provide verb-argument structures fora large number of lexical entries.", "labels": [], "entities": []}, {"text": "\u2022 FrameNet and PropBank contain semantically annotated sentences that exemplify the underlying frame.", "labels": [], "entities": []}, {"text": "\u2022 FrameNet contains not only verbs but also lexical entries for other part-of-speeches.", "labels": [], "entities": []}, {"text": "\u2022 FrameNet provides inter-frame relations that can be used for more complex paraphrasing to link the question and answer sentences.", "labels": [], "entities": []}, {"text": "In this paper we describe two methods that use these resources to annotate both questions and sentences containing answer candidates with semantic roles.", "labels": [], "entities": []}, {"text": "If these annotations can successfully be matched, an answer candidate can be extracted.", "labels": [], "entities": []}, {"text": "We are able, for example, to give a complete framesemantic analysis of the following sentences and to recognize that they all contain an answer to the question \"When was Alaska purchased?\": The first algorithm we present uses the three lexical resources to generate potential answercontaining templates.", "labels": [], "entities": []}, {"text": "While the templates contain holes -in particular, for the answer -the parts that are known can be used to create exact quoted search queries.", "labels": [], "entities": []}, {"text": "Sentences can then be extracted from the output of the search engine and annotated with respect to the resource being used.", "labels": [], "entities": []}, {"text": "From this, an answer candidate (if present) can be extracted.", "labels": [], "entities": []}, {"text": "The second algorithm analyzes the dependency structure of the annotated example sentences in FrameNet and PropBank.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.8925696611404419}]}, {"text": "It then poses rather abstract queries to the web, but can in its candidate sentence analysis stage deal with a wider range of syntactic possibilities.", "labels": [], "entities": []}, {"text": "As we will see, the two algorithms are nicely complementary.", "labels": [], "entities": []}], "datasetContent": [{"text": "We choose to evaluate our experiments with the TREC 2002 QA test set because test sets from 2004 and beyond contain question series that pose problems that are separate from the research described in this paper.", "labels": [], "entities": [{"text": "TREC 2002 QA test set", "start_pos": 47, "end_pos": 68, "type": "DATASET", "confidence": 0.9354423880577087}]}, {"text": "While we participated in, with an anaphora-resolution component that performed quite well, we feel that if one wants to evaluate a particular method, adding an additional module, unrelated to the actual problem, can distort the results.", "labels": [], "entities": []}, {"text": "Additionally, because we are searching for answers on the web rather than in the AQUAINT corpus, we do not distinguish between supported and unsupported judgments.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 81, "end_pos": 95, "type": "DATASET", "confidence": 0.9095498919487}]}, {"text": "Of the 500 questions in the TREC 2002 test set, 236 have be as their head verb.", "labels": [], "entities": [{"text": "TREC 2002 test set", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.9047326445579529}]}, {"text": "As the work described here essentially concerns verb semantics, such questions fall outside its scope.", "labels": [], "entities": [{"text": "verb semantics", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.7292328774929047}]}, {"text": "Evaluation has thus been carried out on only the remaining 264 questions.", "labels": [], "entities": []}, {"text": "For the first method (cf. Section 2), we evaluated system accuracy separately for each of the three resources, and then together, obtaining the following values: For the combined run we looked up the verb in all three resources simultaneously and all entries from every resource were used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9773237109184265}]}, {"text": "As can be seen, PropBank and VerbNet perform equally well, while FrameNet's performance is significantly lower.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 16, "end_pos": 24, "type": "DATASET", "confidence": 0.9288557767868042}, {"text": "VerbNet", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.8622017502784729}]}, {"text": "These differences are due to coverage issues: FrameNet is still in development, and further versions with a higher coverage will be released.", "labels": [], "entities": [{"text": "coverage", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9419315457344055}, {"text": "FrameNet", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.8516538143157959}]}, {"text": "However, a closer look shows that coverage is a problem for all of the resources.", "labels": [], "entities": [{"text": "coverage", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.8066664934158325}]}, {"text": "The following table shows the percentage of the head verbs that were looked up during the above experiments based on the 2002 question set, that could not be found (not found).", "labels": [], "entities": [{"text": "2002 question set", "start_pos": 121, "end_pos": 138, "type": "DATASET", "confidence": 0.7481244405110677}]}, {"text": "It also lists the percentage of lexical entries that contain no annotated sentences (s = 0), five or fewer (s <= 5), tenor fewer (s <= 10), or more than 50 (s > 50).", "labels": [], "entities": []}, {"text": "Furthermore, the table lists the average number of lexical entries found per head verb (avg senses) and the average number of annotated sentences found per lexical entry (avg sent).", "labels": [], "entities": []}], "tableCaptions": []}