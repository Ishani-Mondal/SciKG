{"title": [{"text": "Are Very Large Context-Free Grammars Tractable?", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we present a method which, in practice, allows to use parsers for languages defined by very large context-free grammars (over a million symbol occurrences).", "labels": [], "entities": []}, {"text": "The idea is to split the parsing process in two passes.", "labels": [], "entities": [{"text": "parsing process", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.8968778252601624}]}, {"text": "A first pass computes a sub-grammar which is a specialized part of the large grammar selected by the input text and various filtering strategies.", "labels": [], "entities": []}, {"text": "The second pass is a traditional parser which works with the sub-grammar and the input text.", "labels": [], "entities": []}, {"text": "This approach is validated by practical experiments performed on a Earley-like parser running on a test set with two large context-free grammars .", "labels": [], "entities": []}], "introductionContent": [{"text": "More and more often, in real-word natural language processing (NLP) applications based upon grammars, these grammars are no more written by hand but are automatically generated, this has several consequences.", "labels": [], "entities": []}, {"text": "This paper will consider one of these consequences: the generated grammars maybe very large.", "labels": [], "entities": []}, {"text": "Indeed, we aim to deal with grammars that have, say, over a million symbol occurrences and several hundred thousands rules.", "labels": [], "entities": []}, {"text": "Traditional parsers are not usually prepared to handle them, either because these grammars are simply too big (the parser's internal structures blow up) or the time spent to analyze a sentence becomes prohibitive.", "labels": [], "entities": []}, {"text": "This paper will concentrate on context-free grammars (CFG) and their associated parsers.", "labels": [], "entities": [{"text": "context-free grammars (CFG)", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.6545511603355407}]}, {"text": "However, virtually all Tree Adjoining Grammars (TAG, see e.g., () used in NLP applications can (almost) be seen as lexicalized Tree Insertion Grammars (TIG), which can be converted into strongly equivalent CFGs (.", "labels": [], "entities": []}, {"text": "Hence, the parsing techniques and tools described here can be applied to most TAGs used for NLP, with, in the worst case, alight over-generation which can be easily and efficiently eliminated in a complementary pass.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9711681008338928}]}, {"text": "This is indeed what we have achieved with a TAG automatically extracted from (Villemonte de La)'s large-coverage factorized French TAG, as we will see in Section 4.", "labels": [], "entities": [{"text": "Villemonte de La)'s large-coverage factorized French TAG", "start_pos": 78, "end_pos": 134, "type": "DATASET", "confidence": 0.7750247750017378}]}, {"text": "Even (some kinds of) non CFGs may benefit from the ideas described in this paper.", "labels": [], "entities": []}, {"text": "The reason why the run-time of context-free (CF) parsers for large CFGs is damaged relies on a theoretical result.", "labels": [], "entities": []}, {"text": "A well-known result is that CF parsers may reach a worst-case running time of O(|G|\u00d7 n 3 ) where |G| is the size of the CFG and n is the length of the source text.", "labels": [], "entities": [{"text": "CF parsers", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.8573629856109619}, {"text": "O", "start_pos": 78, "end_pos": 79, "type": "METRIC", "confidence": 0.9881686568260193}]}, {"text": "In typical NLP applications which mainly work at the sentence level, the length of a sentence does not often go beyond a value of say 100, while its average length is around 20-30 words.", "labels": [], "entities": []}, {"text": "In these conditions, the size of the grammar, despite its linear impact on the complexity, maybe the prevailing factor: in, the author remarks that \"the real limiting factor in practice is the size of the grammar\".", "labels": [], "entities": []}, {"text": "The idea developed in this paper is to split the parsing process in two passes.", "labels": [], "entities": [{"text": "parsing process", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.8859646320343018}]}, {"text": "A first pass called filtering pass computes a sub-grammar which is the sub-part of the large input grammar selected by the input sentence and various filtering strategies.", "labels": [], "entities": []}, {"text": "The second pass is a traditional parser which works with the sub-grammar and the input sentence.", "labels": [], "entities": []}, {"text": "The purpose is to find a filtering strategy which, in typical practical situations, minimizes on the average the total run-time of the filtering pass followed by the parser pass.", "labels": [], "entities": []}, {"text": "A filtering pass maybe seen as a (filtering) function that uses the input sentence to select a subgrammar out of a large input CFG.", "labels": [], "entities": []}, {"text": "Our hope, using such a filter, is that the time saved by the parser pass which uses a (smaller) sub-grammar will not totally be used by the filter pass to generate this subgrammar.", "labels": [], "entities": []}, {"text": "It must be clear that this method cannot improve the worst-case parse-time because there exists grammars for which the sub-grammar selected by the filtering pass is the input grammar itself.", "labels": [], "entities": []}, {"text": "In such a case, the filtering pass is simply a waste of time.", "labels": [], "entities": []}, {"text": "Our purpose in this paper is to argue that this technique may profit from typical grammars used in NLP.", "labels": [], "entities": []}, {"text": "To do that we put aside the theoretical viewpoint and we will consider instead the average behaviour of our processors.", "labels": [], "entities": []}, {"text": "More precisely we will study on two large NL CFGs the behaviour of our filtering strategies on a set of test sentences.", "labels": [], "entities": []}, {"text": "The purpose being to choose the best filtering strategy, if any.", "labels": [], "entities": []}, {"text": "By best, we mean the one which, on the average, minimizes the total run-time of both the filtering pass followed by the parsing pass.", "labels": [], "entities": []}, {"text": "Useful formal notions and notations are recalled in Section 2.", "labels": [], "entities": []}, {"text": "The filtering strategies are presented in Section 3 while the associated experiments are reported in Section 4.", "labels": [], "entities": []}, {"text": "This paper ends with some concluding remarks in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The measures presented in this section have been taken on a 1.7GHz AMD Athlon PC with 1.5 Gb of RAM running Linux.", "labels": [], "entities": [{"text": "AMD Athlon PC", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.6395248770713806}]}, {"text": "All parsers are written in C and have been compiled with gcc 2.96 with the O2 optimization flag.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Sizes of the grammars G T >N and G T IG  used in our experiments", "labels": [], "entities": []}, {"text": " Table 3: Average precision of six different filtering  strategies on our test corpus with G T >N and G T IG .", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9700272083282471}, {"text": "IG", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.8095608353614807}]}]}