{"title": [{"text": "On the Complexity of Non-Projective Data-Driven Dependency Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we investigate several non-projective parsing algorithms for dependency parsing, providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others, called here the edge-factored model.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.8248603343963623}]}, {"text": "We also investigate algorithms for non-projective parsing that account for non-local information, and present several hardness results.", "labels": [], "entities": []}, {"text": "This suggests that it is unlikely that exact non-projective dependency parsing is tractable for any model richer than the edge-factored model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency representations of natural language area simple yet flexible mechanism for encoding words and their syntactic dependencies through directed graphs.", "labels": [], "entities": []}, {"text": "These representations have been thoroughly studied in descriptive linguistics and have been applied in numerous language processing tasks.", "labels": [], "entities": [{"text": "descriptive linguistics", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.8827292919158936}]}, {"text": "gives an example dependency graph for the sentence Mr. Tomash will remain as a director emeritus, which has been extracted from the Penn Treebank (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.9948765933513641}]}, {"text": "Each edge in this graph represents a single syntactic dependency directed from a word to its modifier.", "labels": [], "entities": []}, {"text": "In this representation all edges are labeled with the specific syntactic function of the dependency, e.g., SBJ for subject and NMOD for modifier of a noun.", "labels": [], "entities": []}, {"text": "To simplify computation and some important definitions, an artificial token is inserted into the sentence as the leftmost word and will always represent the root of the dependency graph.", "labels": [], "entities": []}, {"text": "We assume all dependency graphs are directed trees originating out of a single node, which is a common constraint.", "labels": [], "entities": []}, {"text": "The dependency graph in is an example of a nested or projective graph.", "labels": [], "entities": []}, {"text": "Under the assumption that the root of the graph is the leftmost word of the sentence, a projective graph is one where the edges can be drawn in the plane above the sentence with no two edges crossing.", "labels": [], "entities": []}, {"text": "Conversely, a non-projective dependency graph does not satisfy this property.", "labels": [], "entities": []}, {"text": "gives an example of a nonprojective graph fora sentence that has also been extracted from the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.9963308572769165}]}, {"text": "Non-projectivity arises due to long distance dependencies or in languages with flexible word order.", "labels": [], "entities": []}, {"text": "For many languages, a significant portion of sentences require a non-projective dependency analysis).", "labels": [], "entities": []}, {"text": "Thus, the ability to learn and infer nonprojective dependency graphs is an important problem in multilingual language processing.", "labels": [], "entities": [{"text": "multilingual language processing", "start_pos": 96, "end_pos": 128, "type": "TASK", "confidence": 0.6559229493141174}]}, {"text": "Syntactic dependency parsing has seen a number of new learning and inference algorithms which have raised state-of-the-art parsing accuracies for many languages.", "labels": [], "entities": [{"text": "Syntactic dependency parsing", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8350432515144348}]}, {"text": "In this work we focus on datadriven models of dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8430663049221039}]}, {"text": "These models are not driven by any underlying grammar, but instead learn to predict dependency graphs based on a set of parameters learned solely from a labeled corpus.", "labels": [], "entities": []}, {"text": "The advantage of these models is that they negate the need for the development of grammars when adapting the model to new languages.", "labels": [], "entities": []}, {"text": "One interesting class of data-driven models are  those that assume each dependency decision is independent modulo the global structural constraint that dependency graphs must be trees.", "labels": [], "entities": []}, {"text": "Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph).", "labels": [], "entities": []}, {"text": "Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time).", "labels": [], "entities": []}, {"text": "The primary problem in treating each dependency as independent is that it is not a realistic assumption.", "labels": [], "entities": []}, {"text": "Non-local information, such as arity (or valency) and neighbouring dependencies, can be crucial to obtaining high parsing accuracies).", "labels": [], "entities": []}, {"text": "However, in the data-driven parsing setting this can be partially adverted by incorporating rich feature representations over the input ().", "labels": [], "entities": []}, {"text": "The goal of this work is to further our current understanding of the computational nature of nonprojective parsing algorithms for both learning and inference within the data-driven setting.", "labels": [], "entities": []}, {"text": "We start by investigating and extending the edge-factored model of.", "labels": [], "entities": []}, {"text": "In particular, we appeal to the Matrix Tree Theorem for multi-digraphs to design polynomial-time algorithms for calculating both the partition function and edge expectations overall possible dependency graphs fora given sentence.", "labels": [], "entities": []}, {"text": "To motivate these algorithms, we show that they can be used in many important learning and inference problems including min-risk decoding, training globally normalized log-linear models, syntactic language modeling, and unsupervised learning via the EM algorithm -none of which have previously been known to have exact non-projective implementations.", "labels": [], "entities": [{"text": "syntactic language modeling", "start_pos": 187, "end_pos": 214, "type": "TASK", "confidence": 0.6454921861489614}]}, {"text": "We then switch focus to models that account for non-local information, in particular arity and neighbouring parse decisions.", "labels": [], "entities": []}, {"text": "For systems that model arity constraints we give a reduction from the Hamiltonian graph problem suggesting that the parsing problem is intractable in this case.", "labels": [], "entities": []}, {"text": "For neighbouring parse decisions, we extend the work of and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods.", "labels": [], "entities": []}, {"text": "A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}