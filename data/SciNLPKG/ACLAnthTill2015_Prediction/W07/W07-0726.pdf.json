{"title": [{"text": "Multi-Engine Machine Translation with an Open-Source Decoder for Statistical Machine Translation", "labels": [], "entities": [{"text": "Multi-Engine Machine Translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.606135348478953}, {"text": "Statistical Machine Translation", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.7647260030110677}]}], "abstractContent": [{"text": "We describe an architecture that allows to combine statistical machine translation (SMT) with rule-based machine translation (RBMT) in a multi-engine setup.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 51, "end_pos": 88, "type": "TASK", "confidence": 0.8000718504190445}, {"text": "rule-based machine translation (RBMT)", "start_pos": 94, "end_pos": 131, "type": "TASK", "confidence": 0.766218513250351}]}, {"text": "We use a variant of standard SMT technology to align translations from one or more RBMT systems with the source text.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9921635389328003}]}, {"text": "We incorporate phrases extracted from these alignments into the phrase table of the SMT system and use the open-source decoder Moses to find good combinations of phrases from SMT training data with the phrases derived from RBMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9803469181060791}, {"text": "SMT training", "start_pos": 175, "end_pos": 187, "type": "TASK", "confidence": 0.8904193639755249}, {"text": "RBMT", "start_pos": 223, "end_pos": 227, "type": "DATASET", "confidence": 0.8793612718582153}]}, {"text": "First experiments based on this hybrid architecture achieve promising results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work on statistical machine translation has led to significant progress in coverage and quality of translation technology, but so far, most of this work focuses on translation into English, where relatively simple morphological structure and abundance of monolingual training data helped to compensate for the relative lack of linguistic sophistication of the underlying models.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.6471329132715861}, {"text": "coverage", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.888481616973877}]}, {"text": "As SMT systems are trained on massive amounts of data, they are typically quite good at capturing implicit knowledge contained in co-occurrence statistics, which can serve as a shallow replacement for the world knowledge that would be required for the resolution of ambiguities and the insertion of information that happens to be missing in the source text but is required to generate wellformed text in the target language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9915498495101929}, {"text": "resolution of ambiguities", "start_pos": 252, "end_pos": 277, "type": "TASK", "confidence": 0.8919933438301086}]}, {"text": "Already before, decades of work went into the implementation of MT systems (typically rule-based) for frequently used language pairs 1 , and these systems quite often contain a wealth of linguistic knowledge about the languages involved, such as fairly complete mechanisms for morphological and syntactic analysis and generation, as well as a large number of bilingual lexical entries spanning many application domains.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9850388765335083}, {"text": "morphological and syntactic analysis and generation", "start_pos": 277, "end_pos": 328, "type": "TASK", "confidence": 0.7485733230908712}]}, {"text": "It is an interesting challenge to combine the different types of knowledge into integrated systems that could then exploit both explicit linguistic knowledge contained in the rules of one or several conventional MT system(s) and implicit knowledge that can be extracted from large amounts of text.", "labels": [], "entities": [{"text": "MT", "start_pos": 212, "end_pos": 214, "type": "TASK", "confidence": 0.9506831169128418}]}, {"text": "The recently started EuroMatrix 2 project will explore this integration of rule-based and statistical knowledge sources, and one of the approaches to be investigated is the combination of existing rulebased MT systems into a multi-engine architecture.", "labels": [], "entities": [{"text": "EuroMatrix", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.9363231658935547}]}, {"text": "The work described in this paper is one of the first incarnations of such a multi-engine architecture within the project, and a careful analysis of the results will guide us in the choice of further steps within the project.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Untranslated tokens (excl. numbers and  punctuations) in output for news commentary task  (de-en) from different systems", "labels": [], "entities": [{"text": "news commentary task  (de-en)", "start_pos": 78, "end_pos": 107, "type": "TASK", "confidence": 0.7493778417507807}]}, {"text": " Table 2: Performance comparison (BLEU scores)  between baseline and hybrid systems, on in-domain  (test) and out-of-domain (nc-test) test data", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9199318289756775}]}]}