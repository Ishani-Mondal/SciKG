{"title": [{"text": "A log-linear model with an n-gram reference distribution for accurate HPSG parsing", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 70, "end_pos": 82, "type": "TASK", "confidence": 0.7256329655647278}]}], "abstractContent": [{"text": "This paper describes a log-linear model with an n-gram reference distribution for accurate probabilistic HPSG parsing.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 105, "end_pos": 117, "type": "TASK", "confidence": 0.7473499476909637}]}, {"text": "In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and POS n-gram as defined in the CCG/HPSG/CDG supertagging.", "labels": [], "entities": [{"text": "CCG/HPSG/CDG supertagging", "start_pos": 252, "end_pos": 277, "type": "DATASET", "confidence": 0.9089288910230001}]}, {"text": "Recently, supertagging becomes well known to drastically improve the parsing accuracy and speed, but su-pertagging techniques were heuristically introduced , and hence the probabilistic models for parse trees were not well defined.", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9808295965194702}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9543666243553162}]}, {"text": "We introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic HPSG.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.8502767086029053}]}, {"text": "This is the first model which properly incorporates the supertagging probabilities into parse tree's probabilistic model.", "labels": [], "entities": []}], "introductionContent": [{"text": "For the last decade, fast, accurate and wide-coverage parsing for real-world text has been pursued in sophisticated grammar formalisms, such as headdriven phrase structure grammar (HPSG), combinatory categorial grammar (CCG)) and lexical function grammar (LFG).", "labels": [], "entities": [{"text": "headdriven phrase structure grammar (HPSG)", "start_pos": 144, "end_pos": 186, "type": "TASK", "confidence": 0.7098425115857806}]}, {"text": "They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses.", "labels": [], "entities": []}, {"text": "The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures.", "labels": [], "entities": []}, {"text": "This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model) with many features for parse trees).", "labels": [], "entities": []}, {"text": "Following this discriminative approach, techniques for efficiency were investigated for estimation) and parsing).", "labels": [], "entities": [{"text": "estimation", "start_pos": 88, "end_pos": 98, "type": "TASK", "confidence": 0.9376373291015625}, {"text": "parsing", "start_pos": 104, "end_pos": 111, "type": "TASK", "confidence": 0.9842053055763245}]}, {"text": "An interesting approach to the problem of parsing efficiency was using supertagging), which was originally developed for lexicalized tree adjoining grammars (LTAG)).", "labels": [], "entities": [{"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9706487059593201}]}, {"text": "Supertagging is a process where words in an input sentence are tagged with 'supertags,' which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 221, "end_pos": 225, "type": "DATASET", "confidence": 0.9471494555473328}]}, {"text": "The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser) with the result of a drastic improvement in the parsing speed.", "labels": [], "entities": [{"text": "parsing", "start_pos": 183, "end_pos": 190, "type": "TASK", "confidence": 0.9584800601005554}]}, {"text": "also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser by showing accuracy as high as the state-of-the-art parsers, and  and  reported that accuracy was significantly improved by incorporating the supertagging probabilities into manually tuned Weighted CDG.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9989269375801086}, {"text": "accuracy", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.99937504529953}]}, {"text": "showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9655072689056396}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9993696808815002}]}, {"text": "This means that syntactic structures are almost determined by supertags as is claimed by.", "labels": [], "entities": []}, {"text": "However, supertaggers themselves were heuristically used as an external tagger.", "labels": [], "entities": []}, {"text": "They filter out unlikely lexical entries just to help parsing, or the probabilistic models for phrase structures were trained independently of the supertagger's probabilistic models ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 54, "end_pos": 61, "type": "TASK", "confidence": 0.964117705821991}]}, {"text": "In the case of supertagging of Weighted CDG ( ), parameters for Weighted CDG are manually tuned, i.e., their model is not a well-defined probabilistic model.", "labels": [], "entities": []}, {"text": "We propose a log-linear model for probabilistic HPSG parsing in which the supertagging probabilities are introduced as a reference distribution for the probabilistic HPSG.", "labels": [], "entities": [{"text": "probabilistic HPSG parsing", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.5957704285780588}]}, {"text": "The reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and part-of-speech (POS) n-gram as defined in the CCG/HPSG/CDG supertagging.", "labels": [], "entities": [{"text": "CCG/HPSG/CDG supertagging", "start_pos": 248, "end_pos": 273, "type": "DATASET", "confidence": 0.9016495048999786}]}, {"text": "This is the first model which properly incorporates the supertagging probabilities into parse tree's probabilistic model.", "labels": [], "entities": []}, {"text": "We compared our model with the probabilistic model for phrase structures).", "labels": [], "entities": []}, {"text": "This model uses word and POS unigram for its reference distribution, i.e., the probabilities of unigram supertagging.", "labels": [], "entities": []}, {"text": "Our model can be regarded as an extension of a unigram reference distribution to an n-gram reference distribution with features that are used in supertagging.", "labels": [], "entities": []}, {"text": "We also compared with a probabilistic model in ().", "labels": [], "entities": []}, {"text": "The probabilities of their model are defined as the product of probabilities of supertagging and probabilities of the probabilistic model for phrase structures, but their model was trained independently of supertagging probabilities, i.e., the supertagging probabilities are not used for reference distributions.) is a syntactic theory based on lexicalized grammar formalism.", "labels": [], "entities": []}, {"text": "In HPSG, a small number of schemata describe general construction rules, and a large number of lexical entries express word-specific characteristics.", "labels": [], "entities": []}, {"text": "The structures of sentences are explained using combinations of schemata and lexical entries.", "labels": [], "entities": []}, {"text": "Both schemata and lexical entries are represented by typed feature structures, and constraints represented by feature structures are checked with unification.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the speed and accuracy of parsing by using Enju 2.1, the HPSG grammar for English ( ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9993815422058105}, {"text": "parsing", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9756386876106262}, {"text": "HPSG grammar", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.9485230445861816}]}, {"text": "The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (   abilistic models were trained using the same portion of the treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.9895544350147247}]}, {"text": "We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing ( ) and quick check ().", "labels": [], "entities": [{"text": "quick check", "start_pos": 100, "end_pos": 111, "type": "METRIC", "confidence": 0.926754355430603}]}, {"text": "We measured the accuracy of the predicateargument relations output of the parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9995762705802917}]}, {"text": "A predicate-argument relation is defined as a tuple \u03c3, w h , a, w a , where \u03c3 is the predicate type (e.g., adjective, intransitive verb), w h is the headword of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and w a is the headword of the argument.", "labels": [], "entities": [{"text": "MODARG", "start_pos": 201, "end_pos": 207, "type": "DATASET", "confidence": 0.8825451135635376}, {"text": "ARG1", "start_pos": 209, "end_pos": 213, "type": "DATASET", "confidence": 0.6760377287864685}, {"text": "ARG4", "start_pos": 220, "end_pos": 224, "type": "DATASET", "confidence": 0.9007173180580139}]}, {"text": "Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser 2 . Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label.", "labels": [], "entities": [{"text": "Labeled precision (LP)/labeled recall (LR)", "start_pos": 0, "end_pos": 42, "type": "METRIC", "confidence": 0.8503256499767303}, {"text": "Unlabeled precision (UP)/unlabeled recall (UR)", "start_pos": 105, "end_pos": 151, "type": "METRIC", "confidence": 0.9071309328079223}]}, {"text": "This evaluation scheme was the same as used in previous evaluations of lexicalized grammars The HPSG treebank is used for training the probabilistic model for lexical entry selection, and hence, those lexical entries that do not appear in the treebank are rarely selected by the probabilistic model.", "labels": [], "entities": [{"text": "HPSG treebank", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.9791238605976105}, {"text": "lexical entry selection", "start_pos": 159, "end_pos": 182, "type": "TASK", "confidence": 0.6093132595221201}]}, {"text": "The 'effective' tag set size, therefore, is around 1,361, the number of lexical entries without those never-seen lexical entries.", "labels": [], "entities": []}, {"text": "When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly.).", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9994829893112183}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9993564486503601}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.999148964881897}]}, {"text": "The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU.", "labels": [], "entities": []}, {"text": "Section 22 of the Treebank was used as the development set, and the performance was evaluated using sentences of \u2264 100 words in Section 23.", "labels": [], "entities": [{"text": "Section 22 of the Treebank", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.6919061064720153}]}, {"text": "The performance of each model was analyzed using the sentences in Section 24 of \u2264 100 words. and average lengths of the tested sentences of \u2264 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24.", "labels": [], "entities": []}, {"text": "The parsing performance for Section 23 is shown in.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9655219316482544}]}, {"text": "The upper half of the table shows the performance using the correct POSs in the Penn Treebank, and the lower half shows the performance using the POSs given by a POS tagger ).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9961899816989899}]}, {"text": "LF and UF in the figure are labeled F-score and unlabeled F-score.", "labels": [], "entities": [{"text": "UF", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.8012507557868958}, {"text": "F-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.992893397808075}, {"text": "F-score", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9258247017860413}]}, {"text": "F-score is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9758203029632568}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9994860887527466}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9967036843299866}]}, {"text": "We evaluated our model in two settings.", "labels": [], "entities": []}, {"text": "One is implemented with a narrow beam width ('our model 1' in the, and the other is implemented with a wider beam width ('our model 2' in the figure): F-score versus average parsing time for sentences in Section 24 of \u2264 100 words.", "labels": [], "entities": [{"text": "F-score", "start_pos": 151, "end_pos": 158, "type": "METRIC", "confidence": 0.9990594983100891}]}, {"text": "1' was introduced to measure the performance with balanced F-score and speed, which we think appropriate for practical use.", "labels": [], "entities": [{"text": "F-score", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9987847208976746}, {"text": "speed", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9904618859291077}]}, {"text": "'our model 2' was introduced to measure how high the precision and recall could reach by sacrificing speed.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9995797276496887}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9991349577903748}]}, {"text": "Our models increased the parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9787592887878418}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9817339181900024}]}, {"text": "'our model 1' was around 2.6 times faster and had around 2.65 points higher F-score than 's model.", "labels": [], "entities": [{"text": "F-score", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9994025230407715}]}, {"text": "'our model 2' was around 2.3 times slower but had around 2.9 points higher F-score than.", "labels": [], "entities": [{"text": "F-score", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9994993209838867}]}, {"text": "They achieved drastic improvement in efficiency.", "labels": [], "entities": []}, {"text": "Their parser ran around 6 times faster than's model 3, 9 times faster than 'our model 1' and 60 times faster than 'our model 2.'", "labels": [], "entities": []}, {"text": "Instead, our models achieved better accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9985198378562927}]}, {"text": "'our model 1' had around 0.5 higher F-score, and 'our model 2' had around 0.8 points higher F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9992976188659668}, {"text": "F-score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9967740178108215}]}, {"text": "Their efficiency is mainly due to elimination of ungrammatical lexical entries by the CFG filtering.", "labels": [], "entities": [{"text": "CFG filtering", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.8928243517875671}]}, {"text": "They first parse a sentence with a CFG grammar compiled from an HPSG grammar, and then eliminate lexical entries that are not in the parsed CFG trees.", "labels": [], "entities": [{"text": "HPSG grammar", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.9161342978477478}]}, {"text": "Obviously, this technique can also be applied to the HPSG parsing of our models.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.7790412902832031}]}, {"text": "We think that efficiency of HPSG parsing with our models will be drastically improved by applying this technique.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.6880864650011063}]}, {"text": "The average parsing time and labeled F-score curves of each probabilistic model for the sentences in Section 24 of \u2264 100 words are graphed in.", "labels": [], "entities": [{"text": "F-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9797001481056213}]}, {"text": "The graph clearly shows the difference of our model and other models.", "labels": [], "entities": []}, {"text": "As seen in the graph, our model achieved higher F-score than other model when beam threshold was widen.", "labels": [], "entities": [{"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9996097683906555}]}, {"text": "This implies that other models were probably difficult to reach the Fscore of 'our model 1' and 'our model 2' for Section 23 even if we changed the beam thresholding parameters.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9990602135658264}]}, {"text": "However, F-score of our model dropped eas-ily when we narrow down the beam threshold, compared to other models.", "labels": [], "entities": [{"text": "F-score", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9993870258331299}]}, {"text": "We think that this is mainly due to its bad implementation of parser interface.", "labels": [], "entities": []}, {"text": "The n-gram reference distribution is incorporated into the kernel of the parser, but the n-gram features and a maximum entropy estimator are defined in other modules; n-gram features are defined in a grammar module, and a maximum entropy estimator for the n-gram reference distribution is implemented with a general-purpose maximum entropy estimator module.", "labels": [], "entities": []}, {"text": "Consequently, strings that represent the ngram information are very frequently changed into feature structures and vice versa when they go in and out of the kernel of the parser.", "labels": [], "entities": []}, {"text": "On the other hand,'s model 3 uses the supertagger as an external module.", "labels": [], "entities": []}, {"text": "Once the parser acquires the supertagger's outputs, the n-gram information never goes in and out of the kernel.", "labels": [], "entities": []}, {"text": "This advantage of's model can apparently be implemented in our model, but this requires many parts of rewriting of the implemented parser.", "labels": [], "entities": []}, {"text": "We estimate that the overhead of the interface is around from 50 to 80 ms/sentence.", "labels": [], "entities": []}, {"text": "We think that re-implementation of the parser will improve the parsing speed as estimated.", "labels": [], "entities": [{"text": "parsing", "start_pos": 63, "end_pos": 70, "type": "TASK", "confidence": 0.9514336585998535}]}, {"text": "In, the line of our model crosses the line of's model.", "labels": [], "entities": []}, {"text": "If the estimation is correct, our model will be faster and more accurate so that the lines in the figure do not cross.", "labels": [], "entities": [{"text": "estimation", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.9450610876083374}]}, {"text": "Speed-up in our model is left as a future work.", "labels": [], "entities": [{"text": "Speed-up", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9431283473968506}]}], "tableCaptions": [{"text": " Table 3: Statistics of the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9645024836063385}]}, {"text": " Table 4: Experimental results for Section 23.", "labels": [], "entities": []}]}