{"title": [{"text": "Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Generation in Machine Translation from Deep Syntactic Trees", "labels": [], "entities": [{"text": "SSST, NAACL-HLT 2007 / AMTA Workshop", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.5899289931569781}, {"text": "Statistical Generation in Machine Translation from Deep Syntactic Trees", "start_pos": 79, "end_pos": 150, "type": "TASK", "confidence": 0.7648928827709622}]}], "abstractContent": [{"text": "In this paper we explore a generative model for recovering surface syntax and strings from deep-syntactic tree structures.", "labels": [], "entities": []}, {"text": "Deep analysis has been proposed fora number of language and speech processing tasks, such as machine translation and paraphrasing of speech transcripts.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8079490661621094}]}, {"text": "In an effort to validate one such formalism of deep syntax, the Praguian Tectogrammat-ical Representation (TR), we present a model of synthesis for English which generates surface-syntactic trees as well as strings.", "labels": [], "entities": [{"text": "Praguian Tectogrammat-ical Representation (TR)", "start_pos": 64, "end_pos": 110, "type": "TASK", "confidence": 0.6502301047245661}]}, {"text": "We propose a generative model for function word insertion (prepositions, definite/indefinite articles, etc.) and sub-phrase reordering.", "labels": [], "entities": [{"text": "function word insertion", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.6424636840820312}, {"text": "sub-phrase reordering", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.5916846841573715}]}, {"text": "We show byway of empirical results that this model is effective in constructing acceptable English sentences given impoverished trees.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactic models for language are being reintroduced into language and speech processing systems thanks to the success of sophisticated statistical models of parsing.", "labels": [], "entities": []}, {"text": "Representing deep syntactic relationships is an open area of research; examples of such models are exhibited in a variety of grammatical formalisms, such as Lexical Functional Grammars, Head-driven Phrase Structure Grammars ( and the Tectogrammatical Representation (TR) of the Functional Generative Description (.", "labels": [], "entities": [{"text": "Representing deep syntactic relationships", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.9013786911964417}, {"text": "Tectogrammatical Representation (TR) of the Functional Generative Description", "start_pos": 234, "end_pos": 311, "type": "TASK", "confidence": 0.678504753112793}]}, {"text": "In this paper we do not attempt to analyze the differences of these formalisms; instead, we show how one particular formalism is sufficient for automatic analysis and synthesis.", "labels": [], "entities": []}, {"text": "Specifically, in this paper we provide evidence that TR is sufficient for synthesis in English.", "labels": [], "entities": [{"text": "TR", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.8575640916824341}]}, {"text": "Augmenting models of machine translation (MT) with syntactic features is one of the main fronts of the MT research community.", "labels": [], "entities": [{"text": "Augmenting models of machine translation (MT)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8131663277745247}, {"text": "MT research", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.9173029065132141}]}, {"text": "The Hiero model has been the most successful to date by incorporating syntactic structure amounting to simple tree structures (.", "labels": [], "entities": []}, {"text": "Synchronous parsing models have been explored with moderate success).", "labels": [], "entities": []}, {"text": "An extension to this work is the exploration of deeper syntactic models, such as TR.", "labels": [], "entities": []}, {"text": "However, a better understanding of the synthesis of surface structure from the deep syntax is necessary.", "labels": [], "entities": []}, {"text": "This paper presents a generative model for surface syntax and strings of English given tectogrammatical trees.", "labels": [], "entities": []}, {"text": "Sentence generation begins by inserting auxiliary words associated with autosemantic nodes; these include prepositions, subordinating conjunctions, modal verbs, and articles.", "labels": [], "entities": [{"text": "Sentence generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9576815962791443}]}, {"text": "Following this, the linear order of nodes is modeled by a similar generative process.", "labels": [], "entities": []}, {"text": "These two models are combined in order to synthesize a sentence.", "labels": [], "entities": []}, {"text": "The Amalgam system provides a similar model for generation from a logical form).", "labels": [], "entities": [{"text": "Amalgam system", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.953830361366272}]}, {"text": "The primary difference between our approach and that of the Amalgam system is that we focus on an impoverished deep structure (akin to logical form); we restrict the deep analysis to contain only the features which transfer directly across languages; specifically, those that transfer directly in our Czech-English machine translation system.", "labels": [], "entities": [{"text": "Czech-English machine translation", "start_pos": 301, "end_pos": 334, "type": "TASK", "confidence": 0.5863295594851176}]}, {"text": "For example, Amalgam's generation of prepositions and subordinating conjunctions is severely restricted as most of these are considered part of the logical form.", "labels": [], "entities": [{"text": "Amalgam's generation of prepositions and subordinating conjunctions", "start_pos": 13, "end_pos": 80, "type": "TASK", "confidence": 0.5903680808842182}]}, {"text": "The work of Langkilde-Geary (2002) on the Halogen system is similar to the work we present here.", "labels": [], "entities": []}, {"text": "The differences that distinguish their work from ours stem from the type of deep representation from which strings are generated.", "labels": [], "entities": []}, {"text": "Although their syntactic and semantic representations appear similar to the Tectogrammatical Representation, more explicit information is preserved in their representation.", "labels": [], "entities": [{"text": "Tectogrammatical Representation", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.6155726462602615}]}, {"text": "For example, the Halogen representation includes markings for determiners, voice, subject position, and dative position which simplifies the generation process.", "labels": [], "entities": []}, {"text": "We believe their minimally specified results are based on input which most closely resembles the input from which we generate in our experiments.", "labels": [], "entities": []}, {"text": "Amalgam's reordering model is similar to the one presented here; their model reorders constituents in a similar way that we reorder subtrees.", "labels": [], "entities": [{"text": "Amalgam", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.941978394985199}]}, {"text": "Both the model of Amalgam and that presented here differ considerably from the n-gram models of, the TAG models of, and the stochastic generation from semantic representation approach of.", "labels": [], "entities": []}, {"text": "In our work, we order the localsubtrees 1 of an augmented deep-structure tree based on the syntactic features of the nodes in the tree.", "labels": [], "entities": []}, {"text": "By factoring these decisions to be independent for each local-subtree, the set of strings we consider is only constrained by the projective strucutre of the input tree and the local permutation limit described below.", "labels": [], "entities": []}, {"text": "In the following sections we first provide a brief description of the Tectogrammatical Representation as used in our work.", "labels": [], "entities": [{"text": "Tectogrammatical Representation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.9375361502170563}]}, {"text": "Both manually annotated and synthetic TR trees are utilized in our experiments; we present a description of each type of tree as well as the motivation for using it.", "labels": [], "entities": []}, {"text": "We then describe the generative statistical process used to model the synthesis of analytical (surface-syntactic) trees based on the TR trees.", "labels": [], "entities": []}, {"text": "Details of the model's features are presented in the following section.", "labels": [], "entities": []}, {"text": "Finally we present empirical results for experiments using both the manually annotated and automatically generated data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have experimented with the above models on both manually annotated TR trees and synthetic trees (i.e., automatically generated trees).", "labels": [], "entities": []}, {"text": "The data comes from the PCEDT 1.0 corpus 8 , aversion of the Penn WSJ Treebank that has been been translated to Czech and automatically transformed to TR in both English and Czech.", "labels": [], "entities": [{"text": "PCEDT 1.0 corpus 8", "start_pos": 24, "end_pos": 42, "type": "DATASET", "confidence": 0.929506465792656}, {"text": "Penn WSJ Treebank", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.9605069557825724}]}, {"text": "The English TR was automatically generated from the Penn Treebank's manually annotated surface syntax trees (English phrasestructure trees).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9933943152427673}]}, {"text": "Additionally, a small set of 497 sentences were manually annotated at the TR level: 248: Classification accuracy for insertion models on development data from PCEDT 1.0.", "labels": [], "entities": [{"text": "TR", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.8975715637207031}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9502092599868774}, {"text": "PCEDT 1.0", "start_pos": 159, "end_pos": 168, "type": "DATASET", "confidence": 0.9068311154842377}]}, {"text": "Article accuracy is computed over the set of nouns.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.961672306060791}]}, {"text": "Preposition and subordinating conjunction accuracy (P & SC) is computed over the set of nodes that appear on the surface (excluding hidden nodes in the TR -these will not exist in automatically generated data).", "labels": [], "entities": [{"text": "Preposition and subordinating conjunction accuracy (P & SC)", "start_pos": 0, "end_pos": 59, "type": "METRIC", "confidence": 0.8083522021770477}]}, {"text": "Models are shown for all features minus the specified feature.", "labels": [], "entities": []}, {"text": "Features with the prefix \"g.\" indicate governor features, otherwise the features are from the node's attributes.", "labels": [], "entities": []}, {"text": "The Baseline model is one which never inserts any nodes (i.e., the model which inserts the most probable value -NOAUX).", "labels": [], "entities": [{"text": "NOAUX", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.9416793584823608}]}, {"text": "for development and 249 for evaluation; results are presented for these two datasets.", "labels": [], "entities": []}, {"text": "All models were trained on the PCEDT 1.0 data set, approximately 49,000 sentences, of which 4,200 were randomly selected as held-out training data, the remainder was used for training.", "labels": [], "entities": [{"text": "PCEDT 1.0 data set", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.9338867515325546}]}, {"text": "We estimate the model distributions with a smoothed maximum likelihood estimator, using Jelinek-Mercer EM smoothing (i.e., linearly interpolated backoff distributions).", "labels": [], "entities": []}, {"text": "Lower order distributions used for smoothing are estimated by deleting the rightmost conditioning variable (as presented in the above models).", "labels": [], "entities": []}, {"text": "Similar experiments were performed at the 2002 Johns Hopkins summer workshop.", "labels": [], "entities": [{"text": "Johns Hopkins summer workshop", "start_pos": 47, "end_pos": 76, "type": "DATASET", "confidence": 0.7938328087329865}]}, {"text": "The results reported here are substantially better than those reported in the workshop report); however, the details of the workshop experiments are not clear enough to ensure the experimental conditions are identical.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classification accuracy for insertion models on development data from PCEDT 1.0. Article accuracy is computed over", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8545859456062317}, {"text": "PCEDT 1.0", "start_pos": 80, "end_pos": 89, "type": "DATASET", "confidence": 0.926629364490509}, {"text": "Article accuracy", "start_pos": 91, "end_pos": 107, "type": "METRIC", "confidence": 0.6895772814750671}]}, {"text": " Table 2: Reordering accuracy for TR trees on development data from PCEDT 1.0. We include performance on the interior nodes", "labels": [], "entities": [{"text": "Reordering", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9460367560386658}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9189717769622803}, {"text": "TR", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9429638385772705}, {"text": "PCEDT 1.0", "start_pos": 68, "end_pos": 77, "type": "DATASET", "confidence": 0.9093346297740936}]}, {"text": " Table 3: Article classifier errors on development data.", "labels": [], "entities": [{"text": "Article classifier", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7207840383052826}]}, {"text": " Table 4: Accuracy of best models on the evaluation data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9984667301177979}]}, {"text": " Table 5: BLEU scores for complete generation system for TR", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993163347244263}, {"text": "TR", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.938766598701477}]}]}