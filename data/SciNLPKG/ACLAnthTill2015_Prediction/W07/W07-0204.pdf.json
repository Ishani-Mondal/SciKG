{"title": [{"text": "TextGraphs-2: Graph-Based Algorithms for Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "Current graph-based approaches to automatic text summarization, such as Le-xRank and TextRank, assume a static graph which does not model how the input texts emerge.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.6631383895874023}, {"text": "TextRank", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.9252787828445435}]}, {"text": "A suitable evolutionary text graph model may impart a better understanding of the texts and improve the summarization process.", "labels": [], "entities": [{"text": "summarization", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.980379581451416}]}, {"text": "We propose a timestamped graph (TSG) model that is motivated by human writing and reading processes, and show how text units in this model emerge overtime.", "labels": [], "entities": []}, {"text": "In our model, the graphs used by LexRank and Tex-tRank are specific instances of our time-stamped graph with particular parameter settings.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9658889174461365}, {"text": "Tex-tRank", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.9600551724433899}]}, {"text": "We apply timestamped graphs on the standard DUC multi-document text summarization task and achieve comparable results to the state of the art.", "labels": [], "entities": [{"text": "DUC multi-document text summarization task", "start_pos": 44, "end_pos": 86, "type": "TASK", "confidence": 0.7723888039588929}]}], "introductionContent": [{"text": "Graph-based ranking algorithms such as Kleinberg's HITS) or Google's PageRank () have been successfully applied in citation network analysis and ranking of webpages.", "labels": [], "entities": [{"text": "citation network analysis", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.7200750708580017}]}, {"text": "These algorithms essentially decide the weights of graph nodes based on global topological information.", "labels": [], "entities": []}, {"text": "Recently, a number of graph-based approaches have been suggested for NLP applications.", "labels": [], "entities": []}, {"text": "introduced LexRank for multi-document text summarization.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.9073432683944702}, {"text": "multi-document text summarization", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.6044019361337026}]}, {"text": "introduced TextRank for keyword and sentence extractions.", "labels": [], "entities": [{"text": "keyword and sentence extractions", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.610226571559906}]}, {"text": "Both LexRank and TextRank assume a fully connected, undirected graph, with text units as nodes and similarity as edges.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 5, "end_pos": 12, "type": "DATASET", "confidence": 0.9644374847412109}, {"text": "TextRank", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.8897725939750671}]}, {"text": "After graph construction, both algorithms use a random walk on the graph to redistribute the node weights.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7561925053596497}]}, {"text": "Many graph-based algorithms feature an evolutionary model, in which the graph changes over timesteps.", "labels": [], "entities": []}, {"text": "An example is a citation network whose edges point backward in time: papers (usually) only reference older published works.", "labels": [], "entities": []}, {"text": "References in old papers are static and are not updated.", "labels": [], "entities": []}, {"text": "Simple models of Web growth are exemples of this: they model the chronological evolution of the Web in which anew webpage must be linked by an incoming edge in order to be publicly accessible and may embed links to existing webpages.", "labels": [], "entities": []}, {"text": "These models differ in that they allow links in previously generated webpages to be updated or rewired.", "labels": [], "entities": []}, {"text": "However, existing graph models for summarizationLexRank and TextRank -assume a static graph, and do not model how the input texts evolve.", "labels": [], "entities": [{"text": "summarizationLexRank", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.958737850189209}]}, {"text": "The central hypothesis of this paper is that modeling the evolution of input texts may improve the subsequent summarization process.", "labels": [], "entities": [{"text": "summarization", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.9781017899513245}]}, {"text": "Such a model maybe based on human writing/reading process and should show how just composed/consumed units of text relate to previous ones.", "labels": [], "entities": []}, {"text": "By applying this model over a series of timesteps, we obtain a representation of how information flows in the construction of the document set and leverage this to construct automatic summaries.", "labels": [], "entities": []}, {"text": "We first introduce and formalize our timestamped graph model in next section.", "labels": [], "entities": []}, {"text": "In particular, our formalization subsumes previous works: we show in Section 3 that the graphs used by LexRank and TextRank are specific instances of our timestamped graph.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9547768235206604}, {"text": "TextRank", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.9047957062721252}]}, {"text": "In Section 4, we discuss how the resulting graphs are applied to automatic multidocument text summarization: by counting node in-degree or applying a random walk algorithm to smooth the information flow.", "labels": [], "entities": [{"text": "multidocument text summarization", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.595464160044988}]}, {"text": "We apply these models to create an extractive summarization program and apply it to the standard Document Understanding Conference (DUC) datasets.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC) datasets", "start_pos": 97, "end_pos": 145, "type": "DATASET", "confidence": 0.5540911938462939}]}, {"text": "We discuss the resulting performance in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have generalized and formalized evolutionary timestamped graph model.", "labels": [], "entities": []}, {"text": "We want to apply it on automatic text summarization to confirm that these evolutionary models help in extracting important sentences.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.6680740863084793}]}, {"text": "However, the parameter space is too large to test all possible TSG algorithms.", "labels": [], "entities": [{"text": "TSG", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.8662406802177429}]}, {"text": "We conduct experiments to focus on the following research questions that relating to 3 TSG parameters -e, u and s, and the topic-sensitivity of PageRank.", "labels": [], "entities": [{"text": "PageRank", "start_pos": 144, "end_pos": 152, "type": "DATASET", "confidence": 0.9655600190162659}]}, {"text": "In the first experiment, we analyze how e, the number of chosen edges for each node at each timestep, affects the performance, with other parameters fixed.", "labels": [], "entities": []}, {"text": "Specifically the TSG algorithm we use is the tuple (f, e, 1, 1, max-cosine-based, sentence, 1, 0, null), where e is being tested for different values.", "labels": [], "entities": []}, {"text": "The node selection function maxcosine-based takes in a sentence sand the current graph G, computes the TFIDF-based cosine similarities between sand other sentences in G, and connects s toe sentence(s) that has(have) the highest cosine score(s) and is(are) not yet chosen by sin previous iterations.", "labels": [], "entities": [{"text": "TFIDF-based", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.9687992334365845}]}, {"text": "We run topic-sensitive PageRank with damping factor \u03b1 set to 0.5 on the graphs.", "labels": [], "entities": []}, {"text": "shows the ROUGE-1 and ROUGE-2 scores withe set to 1, 2, 3, 4, 5, 6, 7, 10, 15, 20 and N, where N is the total number of sentences in the cluster.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7778121829032898}, {"text": "ROUGE-2", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.8126388192176819}]}, {"text": "We succinctly represent LexRank graphs by the tuple, cosinebased, sentence, L max , 0, null) in Section 3; it can also be represented by a slightly different tuple (f, N, 1, 1, max-cosine-based, sentence, 1, 0, null).", "labels": [], "entities": []}, {"text": "It differs from the first representation in that we iteratively add 1 sentence for each document in each timestep and let all nodes in the current graph connect to every other node in the graph.", "labels": [], "entities": []}, {"text": "In this experiment, when e is set to N, the timestamped graph is equivalent to a LexRank graph.", "labels": [], "entities": [{"text": "LexRank graph", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9091166853904724}]}, {"text": "We do not use any reranker in this experiment.", "labels": [], "entities": []}, {"text": "The results allow us to make several observations.", "labels": [], "entities": []}, {"text": "First, when e = 2, the system gives the best performance, with ROUGE-1 score 0.37728 and ROUGE-2 score 0.07692.", "labels": [], "entities": [{"text": "ROUGE-1 score 0.37728", "start_pos": 63, "end_pos": 84, "type": "METRIC", "confidence": 0.9666595856348673}, {"text": "ROUGE-2 score 0.07692", "start_pos": 89, "end_pos": 110, "type": "METRIC", "confidence": 0.964449942111969}]}, {"text": "Some values of e give better scores than LexRank graph configuration, in which e = N.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9140946269035339}]}, {"text": "Second, the system gives very bad performance when e = 1.", "labels": [], "entities": []}, {"text": "This is because when e is set to 1, the graph is too loosely connected and is not suitable to apply random walk on it.", "labels": [], "entities": []}, {"text": "Third, the system gives similar performance when e is set greater than 10.", "labels": [], "entities": []}, {"text": "The reason for this is that the higher values of e make the graph converge to a fully connected graph so that the performance starts to converge and display less variability.", "labels": [], "entities": []}, {"text": "We run a second experiment to analyze how topic-sensitivity and edge weighting affect the system performance.", "labels": [], "entities": []}, {"text": "We use concept links () as the similarity function and a MMR reranker to remove redundancy.", "labels": [], "entities": []}, {"text": "We observe that both topic-sensitive PageRank and weighted edges perform better than generic PageRank on unweighted timestamped graphs.", "labels": [], "entities": []}, {"text": "When topic-sensitivity and edge weighting are both set to true, the system gives the best performance.", "labels": [], "entities": []}, {"text": "To evaluate how skew degree s affects summarization performance, we use the parameter setting from the first experiment, withe fixed to 1.", "labels": [], "entities": [{"text": "summarization", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.9832937717437744}]}, {"text": "Specifically, we use the tuple (f, 1, 1, 1, concept-linkbased, sentence, 1, s, null), with s set to 0, 1 and 2.", "labels": [], "entities": []}, {"text": "We observe that s = 1 gives the best ROUGE-1 and ROUGE-2 scores.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9953091740608215}, {"text": "ROUGE-2", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9880015254020691}]}, {"text": "Compared to the system without skewing (s = 0), s = 2 gives slightly better ROUGE-1 score but worse ROUGE-2 score.", "labels": [], "entities": [{"text": "ROUGE-1 score", "start_pos": 76, "end_pos": 89, "type": "METRIC", "confidence": 0.979097306728363}, {"text": "ROUGE-2 score", "start_pos": 100, "end_pos": 113, "type": "METRIC", "confidence": 0.9751402139663696}]}, {"text": "The reason for this is that s = 2 introduces a delay interval that is too large.", "labels": [], "entities": []}, {"text": "We expect that a freely skewed graph (s = -1) will give more reasonable delay intervals.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE-1 and ROUGE-2 scores for different com- binations of topic-sensitivity and edge weighting(u) settings.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9724341630935669}, {"text": "ROUGE-2", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9638780951499939}]}, {"text": " Table 2: ROUGE-1 and ROUGE-2 scores for  different skew degrees.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9952266216278076}, {"text": "ROUGE-2", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.995008111000061}]}, {"text": " Table 3: top ROUGE-2 and ROUGE-SU4  scores in DUC 2005. TSG is our system.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9778627157211304}, {"text": "ROUGE-SU4", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9750319123268127}, {"text": "DUC 2005", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.9387822449207306}, {"text": "TSG", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.8801004886627197}]}]}