{"title": [{"text": "Chunk-Level Reordering of Source Language Sentences with Automatically Learned Rules for Statistical Machine Translation", "labels": [], "entities": [{"text": "Chunk-Level Reordering of Source Language Sentences", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.7069057275851568}, {"text": "Statistical Machine Translation", "start_pos": 89, "end_pos": 120, "type": "TASK", "confidence": 0.7928265929222107}]}], "abstractContent": [{"text": "In this paper, we describe a source-side reordering method based on syntactic chunks for phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 89, "end_pos": 133, "type": "TASK", "confidence": 0.6003928631544113}]}, {"text": "First, we shallow parse the source language sentences.", "labels": [], "entities": []}, {"text": "Then, reordering rules are automatically learned from source-side chunks and word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.6787464320659637}]}, {"text": "During translation, the rules are used to generate a reordering lattice for each sentence.", "labels": [], "entities": [{"text": "translation", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.9629215002059937}]}, {"text": "Experimental results are reported fora Chinese-to-English task, showing an improvement of 0.5%-1.8% BLEU score absolute on various test sets and better computational efficiency than reordering during decoding.", "labels": [], "entities": [{"text": "BLEU score absolute", "start_pos": 100, "end_pos": 119, "type": "METRIC", "confidence": 0.9755236903826395}]}, {"text": "The experiments also show that the reordering at the chunk-level performs better than at the POS-level.", "labels": [], "entities": []}], "introductionContent": [{"text": "In machine translation, reordering is one of the major problems, since different languages have different word order requirements.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7757081687450409}]}, {"text": "Many reordering constraints have been used for word reorderings, such as ITG constraints, IBM constraints () and local constraints).", "labels": [], "entities": [{"text": "word reorderings", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7226186096668243}]}, {"text": "These approaches do not make use of any linguistic knowledge.", "labels": [], "entities": []}, {"text": "Several methods have been proposed to use syntactic information to handle the reordering problem, e.g. ().", "labels": [], "entities": []}, {"text": "One approach makes use of bitext grammars to parse both the source and target languages.", "labels": [], "entities": []}, {"text": "Another approach makes use of syntactic information only in the target language.", "labels": [], "entities": []}, {"text": "Note that these models have radically different structures and parameterizations than phrase-based models for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9894263744354248}]}, {"text": "Another kind of approaches is to use syntactic information in rescoring methods.", "labels": [], "entities": []}, {"text": "() apply a reranking approach to the sub-task of noun-phrase translation.", "labels": [], "entities": [{"text": "noun-phrase translation", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.7541961967945099}]}, {"text": "( ) and ) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains.", "labels": [], "entities": []}, {"text": "In this paper, we present a strategy to reorder a source sentence using rules based on syntactic chunks.", "labels": [], "entities": []}, {"text": "It is possible to integrate reordering rules directly into the search process, but here, we consider a more modular approach: easy to exchange reordering strategy.", "labels": [], "entities": []}, {"text": "To avoid hard decisions before SMT, we generate a source-reordering lattice instead of a single reordered source sentence as input to the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9924625158309937}, {"text": "SMT", "start_pos": 138, "end_pos": 141, "type": "TASK", "confidence": 0.9707782864570618}]}, {"text": "Then, the decoder uses the reordered source language model as an additional feature function.", "labels": [], "entities": []}, {"text": "A language model trained on the reordered source-side chunks gives a score for each path in the lattice.", "labels": [], "entities": []}, {"text": "The novel ideas in this paper are: \u2022 reordering of the source sentence at the chunk level, \u2022 representing linguistic chunks-reorderings in a lattice.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents a review of related work.", "labels": [], "entities": []}, {"text": "In Sections 3, we review the phrase-based translation system used in this work and propose the framework of the new reordering method.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.7135915756225586}]}, {"text": "In Section 4, we introduce the details of the reordering rules, how they are defined and how to extract them.", "labels": [], "entities": []}, {"text": "In Section 5, we explain how to apply the rules and how to generate reordering lattice.", "labels": [], "entities": []}, {"text": "In Section 6, we present some results that show that the chunk-level source reordering is helpful for phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 102, "end_pos": 146, "type": "TASK", "confidence": 0.6292127147316933}]}, {"text": "Finally, we conclude this paper and discuss future work in Section 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Statistics of training and test corpus for  chunk parsing.  train  test  sentences  17 785  1 000  words  486 468 21 851  chunks  105 773  4 680  words out of chunks 244 416 10 282", "labels": [], "entities": [{"text": "chunk parsing", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.8605780005455017}]}, {"text": " Table 3: Chunk parsing result on 1000 sentences.", "labels": [], "entities": [{"text": "Chunk parsing", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.7533302903175354}]}, {"text": " Table 4: Statistics of training and test corpora for the  IWSLT tasks.  Chinese English  Train  Sentences  40k  Words  308k  377k  Dev  Sentences  489  Words  5 478  6 008  Test  Sentences  500  IWSLT04  Words  3 866  3 581  Test  Sentences  506  IWSLT05  Words  3 652  3 579  Test  Sentences  500  IWSLT06  Words  5 846  -", "labels": [], "entities": [{"text": "Chinese English  Train  Sentences  40k  Words  308k  377k  Dev  Sentences  489  Words", "start_pos": 73, "end_pos": 158, "type": "TASK", "confidence": 0.6644225517908732}]}, {"text": " Table 5.  The baseline system is a non-monotone translation  system, in which the decoder does reordering on  the target language side. Compared to the base- line system, the source reordering method improves  the BLEU score by 0.5% \u2212 1.8% absolute. It also  achieves a better WER. Note that the used chun- ker here is out-of-domain 3 . An improvement is  achieved even with a low F-measure for chunking.  So, we could hope that larger improvement is possi- ble using a high-accuracy chunker.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 215, "end_pos": 225, "type": "METRIC", "confidence": 0.9808498620986938}, {"text": "WER", "start_pos": 278, "end_pos": 281, "type": "METRIC", "confidence": 0.9967395663261414}, {"text": "F-measure", "start_pos": 382, "end_pos": 391, "type": "METRIC", "confidence": 0.9977340698242188}]}, {"text": " Table 5: Translation performance for the Chinese-English IWSLT task  WER[%] PER[%] NIST BLEU[%]  IWSLT04  baseline  47.3  38.2  7.78  39.1  source reordering  46.3  37.2  7.70  40.9  IWSLT05  baseline  45.0  37.3  7.40  41.8  source reordering  44.6  36.8  7.51  42.3  IWSLT06  baseline  67.4  50.0  6.65  22.4  source reordering  65.6  50.4  6.46  23.3  source reordering+non-monotone decoder  66.5  50.3  6.52  22.4", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9577295184135437}, {"text": "WER", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9163703322410583}, {"text": "PER", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9662317037582397}, {"text": "NIST", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.492632657289505}, {"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.8500587344169617}]}, {"text": " Table 6: Translation performance of reordering  methods on IWSLT 2004 test set  WER PER NIST BLEU  [%]  [%]  [%]  Baseline 47.3 38.2 7.78  39.1", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9696531295776367}, {"text": "IWSLT 2004 test set", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.9633336365222931}, {"text": "WER PER NIST", "start_pos": 81, "end_pos": 93, "type": "METRIC", "confidence": 0.7441985805829366}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.6097766757011414}]}, {"text": " Table 7: Lattice information for the Chinese-English  IWSLT 2004 test data  avg. density used  translation  pro sent  rules time [min/sec]", "labels": [], "entities": [{"text": "IWSLT 2004 test data", "start_pos": 55, "end_pos": 75, "type": "DATASET", "confidence": 0.9088399708271027}]}]}