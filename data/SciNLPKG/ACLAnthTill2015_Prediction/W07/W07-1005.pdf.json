{"title": [{"text": "An Unsupervised Method for Extracting Domain-specific Affixes in Biological Literature", "labels": [], "entities": [{"text": "Extracting Domain-specific Affixes in Biological Literature", "start_pos": 27, "end_pos": 86, "type": "TASK", "confidence": 0.8592212895552317}]}], "abstractContent": [{"text": "We propose an unsupervised method to automatically extract domain-specific prefixes and suffixes from biological corpora based on the use of PATRICIA tree.", "labels": [], "entities": [{"text": "PATRICIA tree", "start_pos": 141, "end_pos": 154, "type": "DATASET", "confidence": 0.7327524423599243}]}, {"text": "The method is evaluated by integrating the extracted affixes into an existing learning-based biological term annotation system.", "labels": [], "entities": []}, {"text": "The system based on our method achieves comparable experimental results to the original system in locating biological terms and exact term matching annotation.", "labels": [], "entities": [{"text": "exact term matching annotation", "start_pos": 128, "end_pos": 158, "type": "TASK", "confidence": 0.6560140252113342}]}, {"text": "However, our method improves the system efficiency by significantly reducing the feature set size.", "labels": [], "entities": []}, {"text": "Additionally, the method achieves a better performance with a small training data set.", "labels": [], "entities": []}, {"text": "Since the affix extraction process is unsupervised, it is assumed that the method can be generalized to extract domain-specific affixes from other domains, thus assisting in domain-specific concept recognition.", "labels": [], "entities": [{"text": "affix extraction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.770232081413269}, {"text": "domain-specific concept recognition", "start_pos": 174, "end_pos": 209, "type": "TASK", "confidence": 0.6046497623125712}]}], "introductionContent": [{"text": "Biological term annotation is a preparatory step in information retrieval in biological science.", "labels": [], "entities": [{"text": "Biological term annotation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6270464360713959}, {"text": "information retrieval", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.7728621065616608}]}, {"text": "A biological term is generally defined as any technical term related to the biological domain.", "labels": [], "entities": []}, {"text": "Considering term structure, there are two types of biological terms: single word terms and multi-word terms.", "labels": [], "entities": []}, {"text": "Many systems () have been proposed to annotate biological terms based on different methodologies in which determining term boundaries is usually the first task.", "labels": [], "entities": []}, {"text": "It has been demonstrated), however, that accurately locating term boundaries is difficult.", "labels": [], "entities": [{"text": "locating term boundaries", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7970201770464579}]}, {"text": "This is so because of the ambiguity of terms, and the peculiarity of the language used in biological literature.", "labels": [], "entities": []}, {"text": "( proposed an automatic biological term annotation system (ABTA) which applies supervised learning methods to annotate biological terms in the biological literature.", "labels": [], "entities": []}, {"text": "Given unstructured texts in biological research, the annotation system first locates biological terms based on five word position classes, \"Start\", \"Middle\", \"End\", \"Single\" and \"Non-relevant\".", "labels": [], "entities": []}, {"text": "Therefore, multi-word biological terms should be in a consistent sequence of classes \"Start (Middle)* End\" while single word terms will be indicated by the class \"Single\".", "labels": [], "entities": [{"text": "Start (Middle)* End", "start_pos": 86, "end_pos": 105, "type": "METRIC", "confidence": 0.8453979969024659}]}, {"text": "Word n-grams are used to define each input sentence into classification instances.", "labels": [], "entities": []}, {"text": "For each element in an n-gram, the system extracts feature attributes as input for creating the classification model.", "labels": [], "entities": []}, {"text": "The extracted feature attributes include word feature patterns(e.g., Greek letters, uppercase letters, digits and other symbols), part-of-speech (POS) tag information, prefix and suffix characters.", "labels": [], "entities": []}, {"text": "Without using other specific domain resources, the system achieves comparable results to some other state-of-the-art systems () which resort to external knowledge, such as protein dictionaries.", "labels": [], "entities": []}, {"text": "It has been demonstrated) that the part-of-speech tag information is the most effective attribute in aiding the system to annotate biological terms because most biological terms are partial noun phrases.", "labels": [], "entities": []}, {"text": "The ABTA system learns the affix feature by recording only the first and the last n characters (e.g., n = 3) of each word in classification instances, and the authors claimed that then characters could provide enough affix information for the term annotation task.", "labels": [], "entities": [{"text": "ABTA", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.6498180627822876}, {"text": "term annotation task", "start_pos": 243, "end_pos": 263, "type": "TASK", "confidence": 0.8075163761774699}]}, {"text": "Instead of using a certain number of characters to provide affix information, however, it is more likely that a specific list of typically used prefixes and suffixes of biological words would provide more accurate information to classifying some biological terms and boundaries.", "labels": [], "entities": []}, {"text": "We hypothesize that a more flexible affix definition will improve the performance of the taks of biological term annotation.", "labels": [], "entities": []}, {"text": "Inspired by), we propose a method to automatically extract domainspecific prefixes and suffixes from biological corpora.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of the extracted affixes by integrating them into the parametrization of an existing biological term annotation system, ABTA, to evaluate the impact on performance of term annotation.", "labels": [], "entities": [{"text": "ABTA", "start_pos": 150, "end_pos": 154, "type": "DATASET", "confidence": 0.5504063963890076}]}, {"text": "The proposed method is completely unsupervised.", "labels": [], "entities": []}, {"text": "For this reason, we suggest that our method can be generalized for extracting domain-specific affixes from many domains.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: In section 2, we review recent research advances in biological term annotation.", "labels": [], "entities": [{"text": "biological term annotation", "start_pos": 99, "end_pos": 125, "type": "TASK", "confidence": 0.6492844820022583}]}, {"text": "Section 3 describes the methodology proposed for affix extraction in detail.", "labels": [], "entities": [{"text": "affix extraction", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.8333312273025513}]}, {"text": "The experiment results are presented and evaluated in section 4.", "labels": [], "entities": []}, {"text": "Finally, section 5 summarizes the paper and introduces future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we have designed the experiments to extract domain-specific prefixes and suffixes of biological words from a biological corpus, and investigate whether the extracted affix information could facilitate better biological term annotation.", "labels": [], "entities": [{"text": "biological term annotation", "start_pos": 222, "end_pos": 248, "type": "TASK", "confidence": 0.6625497241814932}]}, {"text": "The overall design of our experiments consists of three major processes: affix extraction, affix refining and evaluation of experimental results.", "labels": [], "entities": [{"text": "affix extraction", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.6844890117645264}]}, {"text": "It is seen that every node in PATRICIA tree contains exactly one string of 1 or more characters, which is the preceding substring of its descendant nodes.", "labels": [], "entities": [{"text": "PATRICIA tree", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.791673481464386}]}, {"text": "Meanwhile, every word is a path of substrings from the root node to a leaf.", "labels": [], "entities": []}, {"text": "Therefore, we propose that every substring that can be formed from traversing the internal nodes of the tree is a potential affix.", "labels": [], "entities": []}, {"text": "In the affix extraction process, we first populate a PATRICIA tree using all words in the combined corpus(CC) of a Biological Corpus (BC) and a General English Corpus (GEC).", "labels": [], "entities": [{"text": "affix extraction", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.738675981760025}, {"text": "General English Corpus (GEC)", "start_pos": 144, "end_pos": 172, "type": "DATASET", "confidence": 0.7328732907772064}]}, {"text": "GEC is used against BC in order to extract more accurate biological affix information.", "labels": [], "entities": [{"text": "GEC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7221080660820007}, {"text": "BC", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.33859702944755554}]}, {"text": "Two PATRICIA trees are populated separately for extracting prefixes and suffixes.", "labels": [], "entities": []}, {"text": "The suffix tree is based on strings derived by reversing all the input words from the combined corpus.", "labels": [], "entities": []}, {"text": "All the potential prefixes and suffixes are then extracted from the populated PATRICIA trees.", "labels": [], "entities": [{"text": "PATRICIA trees", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.7863298654556274}]}, {"text": "In the affix refining process, for each extracted potential affix, we compute its joint probability of being both an English affix and a biological affix, P (D = Biology, A = Yes|PA), where D stands for Domain, A stands for Affix and PA represents Potential Affix.", "labels": [], "entities": [{"text": "Affix", "start_pos": 224, "end_pos": 229, "type": "METRIC", "confidence": 0.9695620536804199}]}, {"text": "This joint probability can be further decomposed as shown in Eq.(1).", "labels": [], "entities": []}, {"text": "In the formula, P (A = Yes|P A) denotes the probability that a given potential affix is a true English affix while P (D = Biology|A = Yes, PA) refers to the probability that a given English affix is actually a biological affix.", "labels": [], "entities": []}, {"text": "To calculate P (A = Yes|P A), the probabilities of prefixes and suffixes are measured separately.", "labels": [], "entities": []}, {"text": "In linguistics, a prefix is described as a type of affix that precedes the morphemes to which it can attach ().", "labels": [], "entities": []}, {"text": "Simply speaking, a prefix is a substring that can be found at the beginning of a word.", "labels": [], "entities": []}, {"text": "Our functional definition of a prefix is a substring which precedes words existing in the English language.", "labels": [], "entities": []}, {"text": "This can be done by enumerating, for each node, all descendant substring and assessing their existence as stand-alone words.", "labels": [], "entities": []}, {"text": "For example, \"radioimmunoassay\", \"radioiodine\" and \"radio-labeled\" are three words and have a common starting string \"radio\".", "labels": [], "entities": []}, {"text": "If we takeout the remaining part of each word, three new strings are obtained, \"immunoassay\", \"iodine\" and \"labeled\".", "labels": [], "entities": []}, {"text": "Since all the input words are already stored in PATRICIA tree, we lookup these three strings in PATRICIA tree and find that \"immunoassay\", \"iodine\" and \"labeled\" are also meaningful words in the tree.", "labels": [], "entities": [{"text": "PATRICIA tree", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.8853855729103088}, {"text": "PATRICIA tree", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.8670252561569214}]}, {"text": "This indicates that \"radio\" is a prefix among the input words.", "labels": [], "entities": []}, {"text": "On the other hand, it is obvious that \"radioimmunoassay\" and \"radioiodine\" share another string \"radioi\".", "labels": [], "entities": []}, {"text": "However, \"mmunoassay\" and \"odine\" are not meaningful words due to their absence in the PATRICIA tree.", "labels": [], "entities": [{"text": "mmunoassay", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8473037481307983}, {"text": "PATRICIA tree", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.7757830321788788}]}, {"text": "This suggests that \"radioi\" is not a prefix.", "labels": [], "entities": []}, {"text": "For each extracted potential prefix, P (A = Yes|P A) is computed as the proportion of strings formed by traversing all descendant nodes that are meaningful terms.", "labels": [], "entities": []}, {"text": "In our experiments, the measure of determining a string meaningful is to lookup whether the string is an existing word present in the built prefix PATRICIA tree.", "labels": [], "entities": []}, {"text": "Algorithm 1 shows the procedure of populating a PATRICIA tree and calculating P (A = Yes|P A) for each potential prefix.", "labels": [], "entities": []}, {"text": "Likewise, in linguistics a suffix is an affix that follows the morphemes to which it can attach ().", "labels": [], "entities": []}, {"text": "Simply speaking, a suffix of a word is a substring exactly matching the last part of the word.", "labels": [], "entities": []}, {"text": "Similar to the idea of calculating P (A = Yes|P A) for potential prefix, we conjecture that the extracted potential suffix could be a reasonable English suffix if the inverted strings formed from traversing the descendant nodes of the potential suffix in the suffix PA-TRICIA tree are meaningful words.", "labels": [], "entities": []}, {"text": "For instance, \"Calcium-dependent\", \"Erythropoietin-dependent\" and \"Ligand-dependent\" share a common ending string \"-dependent\".", "labels": [], "entities": []}, {"text": "Since the remaining strings of each word, \"Calcium\", \"Erythropoietin\" and \"Ligand\" can be found in the \"forward\" PATRICIA tree, \"-dependent\" is a potentially useful suffix.", "labels": [], "entities": []}, {"text": "For our experiments, it is necessary to use a corpus that includes widely used biological terms and common English words.", "labels": [], "entities": []}, {"text": "This dataset, therefore, will allow us to accurately extract the information of biology related affixes.", "labels": [], "entities": []}, {"text": "As a proof-of-concept prototype, our experiments are conducted on two widely used corpora: Genia corpus (v3.02) and Brown corpus 2 .The Genia version 3.02 corpus is used as the biological corpus BC in our experiments.", "labels": [], "entities": [{"text": "Genia corpus (v3.02", "start_pos": 91, "end_pos": 110, "type": "DATASET", "confidence": 0.8578406125307083}, {"text": "Brown corpus", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.8064474165439606}, {"text": "Genia version 3.02 corpus", "start_pos": 136, "end_pos": 161, "type": "DATASET", "confidence": 0.5380776152014732}]}, {"text": "It contains 2,000 biological research paper abstracts.", "labels": [], "entities": []}, {"text": "They were selected from the search results in the MEDLINE database 3 , and each biological term has been annotated into different terminal classes based on the opinions of experts in biology.", "labels": [], "entities": [{"text": "MEDLINE database 3", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.957644005616506}]}, {"text": "Used as the general English corpus GEC, Brown corpus includes 500 samples of common English words, totalling about a million words drawn from 15 different text categories.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.972029447555542}]}, {"text": "All the experiments were executed on a Sun Solaris server Sun-Fire-880.", "labels": [], "entities": [{"text": "Sun Solaris server Sun-Fire-880", "start_pos": 39, "end_pos": 70, "type": "DATASET", "confidence": 0.7868347764015198}]}, {"text": "Our experiments were mainly implemented using Perl and Python.", "labels": [], "entities": []}, {"text": "We extracted 15,718 potential prefixes and 21,282 potential suffixes from the combined corpus of Genia and Brown.", "labels": [], "entities": []}, {"text": "Among them, there are 2,306 potential prefixes and 1,913 potential suffixes with joint probability value P (D = Biology, A = Yes|P A) greater than 0.5.", "labels": [], "entities": [{"text": "joint probability value P", "start_pos": 81, "end_pos": 106, "type": "METRIC", "confidence": 0.7470846846699715}, {"text": "A = Yes|P A)", "start_pos": 121, "end_pos": 133, "type": "METRIC", "confidence": 0.7512510418891907}]}, {"text": "shows a few examples of extracted potential affixes whose joint probability value is equal to 1.0.", "labels": [], "entities": []}, {"text": "It is seen that most of these potential affixes are understandable biological affixes which directly carry specific semantic meanings about certain biological terms.", "labels": [], "entities": []}, {"text": "However, some substrings are also captured as potential affixes although they may not be recognized as \"affixes\" in linguistics, for example \"adenomyo\" in prefixes, and \"mopoiesis\" in suffixes.", "labels": [], "entities": []}, {"text": "In Genia corpus, \"adenomyo\" is the common beginning substring of biological terms \"adenomyoma\", \"adenomyosis\" and \"adenomyotic\" , while \"plasias\" is the common ending substring of biological terms \"neoplasias\" and \"hyperplasias\".", "labels": [], "entities": [{"text": "Genia corpus", "start_pos": 3, "end_pos": 15, "type": "DATASET", "confidence": 0.7138611227273941}]}, {"text": "The whole list of extracted potential affixes is available upon request.", "labels": [], "entities": []}, {"text": "In order to investigate whether the extracted affixes improves the performance of biological term annotation, it is necessary to obtain the experimental results of both original ABTA system and the ABTA system using our extracted affix information.", "labels": [], "entities": [{"text": "ABTA system", "start_pos": 178, "end_pos": 189, "type": "DATASET", "confidence": 0.8492576777935028}, {"text": "ABTA system", "start_pos": 198, "end_pos": 209, "type": "DATASET", "confidence": 0.914836049079895}]}, {"text": "In ABTA, the extraction of feature attributes is performed on the whole 2000 abstracts of Genia corpus, and then 1800 abstracts are used as training set while the rest 200 abstracts are used as testing set.", "labels": [], "entities": [{"text": "ABTA", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.5496600270271301}, {"text": "Genia corpus", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.7501437664031982}]}, {"text": "The evaluation measures are precision, recall and F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9998231530189514}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.999777615070343}, {"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9986768364906311}]}, {"text": "C4.5 decision tree classifier) is reported as the most efficient classifier which leads to the best performance among all the classifiers experimented in ().", "labels": [], "entities": []}, {"text": "Therefore, C4.5 is used as the main classifier in our experiments.", "labels": [], "entities": []}, {"text": "The experimental results of ABTA system with 10 fold cross-validation based on different combinations of the original features are presented in in which feature \"WFP\" is short for Word Feature Patterns, feature \"AC\" denotes Affix Characters, and feature \"POS\" refers to POS tag information.", "labels": [], "entities": [{"text": "ABTA", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.6983048319816589}]}, {"text": "The setting of parameters in the experiments with ABTA is: the word n-gram size is 3, the number of word feature patterns is 3, and the number of affix characters is 4.", "labels": [], "entities": []}, {"text": "We have reported the F-score and the classification accuracy of the experiments in the table.", "labels": [], "entities": [{"text": "F-score", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.9988270401954651}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9482001066207886}]}, {"text": "It is seen that there is a tendency with the experimental performance that fora multi-word biological term, the middle position is most difficult to detect while the ending position is generally easier to be identified than the starting position.", "labels": [], "entities": []}, {"text": "The assumed reason for this tendency is that for multi-: Examples of Extracted Potential Affixes with Joint Probability Value 1.0 word biological terms, many middle words of are seemingly unrelated to biology domain while many ending words directly indicate their identity, for instances, \"receptor\", \"virus\" or \"expression\".", "labels": [], "entities": []}, {"text": "shows the experimental results of ABTA system after replacing the original affix feature with our obtained joint probability values for each word in Genia corpus.", "labels": [], "entities": [{"text": "ABTA", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.5850826501846313}, {"text": "Genia corpus", "start_pos": 149, "end_pos": 161, "type": "DATASET", "confidence": 0.7443691194057465}]}, {"text": "\"JPV\" is used to denote Joint Probability Values.", "labels": [], "entities": []}, {"text": "It is seen that based on all three features the system achieves a classification accuracy of 87.5%, which is comparable to the results of the original ABTA system.", "labels": [], "entities": [{"text": "classification", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.8104915022850037}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9655842781066895}, {"text": "ABTA system", "start_pos": 151, "end_pos": 162, "type": "DATASET", "confidence": 0.9173797369003296}]}, {"text": "However, the size of the feature set of the system is significantly reduced, and the classification accuracy of 87.5% is achieved based on only 18 parameters, which is 1/2 of the size of the original feature set.", "labels": [], "entities": [{"text": "classification", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.8939220905303955}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9771759510040283}]}, {"text": "Meanwhle, the execution time of the experiments generally reduces to nearly half of the original ABTA system (e.g., reduces from 4 hours to 1.7 hours).", "labels": [], "entities": [{"text": "Meanwhle", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.985453724861145}, {"text": "ABTA", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.4886969327926636}]}, {"text": "Furthermore, when the feature set contains only our extracted affix information, the system reaches a classification accuracy of 81.46% based on only 6 parameters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9303752183914185}]}, {"text": "It is comparable with the classification accuracy achieved by using only POS information in the system.", "labels": [], "entities": [{"text": "classification", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.9469478130340576}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9187033772468567}, {"text": "POS", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.7087923884391785}]}, {"text": "In addition, also presents the experimental results when our extracted affix information is used as an addtional feature to the original feature set.", "labels": [], "entities": []}, {"text": "It is expected that the system performance is further improved when the four features are applied together.", "labels": [], "entities": []}, {"text": "However, the size of the feature set increases to 42 parameters, which increases the data redundancy.", "labels": [], "entities": []}, {"text": "This proves that the extracted affix information has a positive impact on locating biological terms, and it could be a good replacement of the original affix feature.", "labels": [], "entities": []}, {"text": "Moreover, we also evaluated the performance of the exact matching biological term annotation based on the obtained experimental results of ABTA system.", "labels": [], "entities": [{"text": "ABTA", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.4281103014945984}]}, {"text": "The exact matching annotation in ABTA system is to accurately identify every biological term, including both multi-word terms and single word terms, therefore, all the word position classes of a term have to be classified correctly at the same time.", "labels": [], "entities": [{"text": "ABTA", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.37407761812210083}]}, {"text": "An error occurring in anyone of \"Start\" \"Middle\" and \"End\" classes leads the system to annotate multi-word terms incorrectly.", "labels": [], "entities": []}, {"text": "Consequently, the accumulated errors will influence the exact matching annotation performance.", "labels": [], "entities": []}, {"text": "presents the exact matching annotation results of different combination of features based on 10 fold cross-validation over Genia corpus.", "labels": [], "entities": [{"text": "Genia corpus", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.7958757877349854}]}, {"text": "It is seen that after replacing the original affix feature of ABTA system with our obtained joint probability values for each word in Genia corpus, the system achieves an 0.664 F-score on exact matching of biological term annotation, comparable to the exact matching performance of the original ABTA system.", "labels": [], "entities": [{"text": "ABTA system", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.8595481812953949}, {"text": "Genia corpus", "start_pos": 134, "end_pos": 146, "type": "DATASET", "confidence": 0.7800295352935791}, {"text": "F-score", "start_pos": 177, "end_pos": 184, "type": "METRIC", "confidence": 0.9982739686965942}, {"text": "ABTA system", "start_pos": 295, "end_pos": 306, "type": "DATASET", "confidence": 0.8625033795833588}]}, {"text": "In addition, when the feature set contains only our extracted affix information, the system reaches an 0.536 F-score on exact matching.", "labels": [], "entities": [{"text": "F-score", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9984884262084961}]}, {"text": "Although it is a little lower than the exact matching performance achieved by using only the original affix features in the system, the feature set size of the system is significantly reduced from 24 to 6.", "labels": [], "entities": []}, {"text": "In order to further compare our method with the original ABTA system, we attempted eleven different sizes of training data set to run the experiments separately based on our method and the original ABTA system.", "labels": [], "entities": [{"text": "ABTA", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.8836095333099365}, {"text": "ABTA system", "start_pos": 198, "end_pos": 209, "type": "DATASET", "confidence": 0.9345902800559998}]}, {"text": "They can then be evaluated in terms of their performance on each training set size.", "labels": [], "entities": []}, {"text": "These eleven different training set sizes are: 0.25%, 0.5%, 1%, 2.5%, 5%, 7.5%, 10%, 25%, 50%, 75% and 90%.", "labels": [], "entities": []}, {"text": "For instance, 0.25% denotes that the training data set is 0.25% of Genia corpus while the rest 99.75% becomes the testing data set for experiments.", "labels": [], "entities": [{"text": "Genia corpus", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.8242583870887756}]}, {"text": "It is observed that there are about 21 paper abstracts in training set when its size is 1% , and 52 abstracts when its size is 2.5%.", "labels": [], "entities": []}, {"text": "It is expected that larger training set size leads to better classification accuracy of experiments.", "labels": [], "entities": [{"text": "classification", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.955758273601532}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9452151656150818}]}, {"text": "For each training set size, we randomly extracted 10 different training sets from Genia corpus to run the experiments.", "labels": [], "entities": [{"text": "Genia corpus", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.7863627970218658}]}, {"text": "We then computed the mean classification accuracy (MCA) of 10 obtained classification accuracies. that the change patterns of MCA obtained by our method and the original ABTA system are similar.", "labels": [], "entities": [{"text": "mean classification accuracy (MCA)", "start_pos": 21, "end_pos": 55, "type": "METRIC", "confidence": 0.756796807050705}, {"text": "MCA", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.9794154167175293}, {"text": "ABTA system", "start_pos": 170, "end_pos": 181, "type": "DATASET", "confidence": 0.9110470116138458}]}, {"text": "It is also seen that our method achieves marginally better classification performance when the proportion of training data is under 2.5%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental Results of Original ABTA System", "labels": [], "entities": [{"text": "ABTA", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.4805624186992645}]}, {"text": " Table 3: Experimental Results of ABTA System with Extracted Affix Information", "labels": [], "entities": [{"text": "ABTA", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.47707200050354004}]}, {"text": " Table 4: Exact Matching Annotation Performance", "labels": [], "entities": [{"text": "Exact Matching Annotation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.906468411286672}]}]}