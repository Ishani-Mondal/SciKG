{"title": [{"text": "Biology Based Alignments of Paraphrases for Sentence Compression", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8787331879138947}]}], "abstractContent": [{"text": "1 In this paper, we present a study for extracting and aligning paraphrases in the context of Sentence Compression.", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 94, "end_pos": 114, "type": "TASK", "confidence": 0.9470984041690826}]}, {"text": "First, we justify the application of anew measure for the automatic extraction of paraphrase corpora.", "labels": [], "entities": [{"text": "automatic extraction", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.5889361500740051}]}, {"text": "Second, we discuss the work done by (Barzi-lay & Lee, 2003) who use clustering of paraphrases to induce rewriting rules.", "labels": [], "entities": []}, {"text": "We will see, through classical visualization method-ologies (Kruskal & Wish, 1977) and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification.", "labels": [], "entities": [{"text": "automatic pattern identification", "start_pos": 160, "end_pos": 192, "type": "TASK", "confidence": 0.619824101527532}]}, {"text": "Finally, we will provide some results of different biology based methodolo-gies for pairwise paraphrase alignment.", "labels": [], "entities": [{"text": "paraphrase alignment", "start_pos": 93, "end_pos": 113, "type": "TASK", "confidence": 0.7319801151752472}]}], "introductionContent": [{"text": "Sentence Compression can be seen as the removal of redundant words or phrases from an input sentence by creating anew sentence in which the gist of the original meaning of the sentence remains unchanged.", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9156230390071869}]}, {"text": "Sentence Compression takes an important place for Natural Language Processing (NLP) tasks where specific constraints must be satisfied, such as length in summarization (;), style in text simplification) or sentence simplification for subtitling ().", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8988703489303589}, {"text": "sentence simplification", "start_pos": 206, "end_pos": 229, "type": "TASK", "confidence": 0.7271836996078491}]}, {"text": "Generally, Sentence Compression involves performing the following three steps: (1) Extraction of paraphrases from comparable corpora, (2) Alignment of paraphrases and (3) Induction of rewriting rules.", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.9569157361984253}]}, {"text": "Obviously, each of these steps can be performed in many different ways going from totally unsupervised to totally supervised.", "labels": [], "entities": []}, {"text": "In this paper, we will focus on the first two steps.", "labels": [], "entities": []}, {"text": "In particular, we will first justify the application of anew measure for the automatic extraction of paraphrase corpora.", "labels": [], "entities": [{"text": "automatic extraction of paraphrase corpora", "start_pos": 77, "end_pos": 119, "type": "TASK", "confidence": 0.7449931025505065}]}, {"text": "Second, we will discuss the work done by) who use clustering of paraphrases to induce rewriting rules.", "labels": [], "entities": []}, {"text": "We will see, through classical visualization methodologies and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification.", "labels": [], "entities": [{"text": "automatic pattern identification", "start_pos": 136, "end_pos": 168, "type": "TASK", "confidence": 0.6049601236979166}]}, {"text": "Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment.", "labels": [], "entities": [{"text": "paraphrase alignment", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.7822334170341492}]}], "datasetContent": [{"text": "We experimented four clustering algorithms on a corpus of web news stories and then three human judges manually cross-classified a random sample of the generated clusters.", "labels": [], "entities": []}, {"text": "They were asked to classify a cluster as a \"wrong cluster\" if it contained at least two sentences without any entailment relation between them.", "labels": [], "entities": []}, {"text": "Results are shown in the next table 1.", "labels": [], "entities": []}, {"text": "The \"BASE\" column is the baseline, where the Sumo-Metric was applied rather than clustering.", "labels": [], "entities": [{"text": "BASE", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9977403879165649}, {"text": "Sumo-Metric", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.6087580323219299}]}, {"text": "Columns \"S-HAC\" and \"C-HAC\" express the results for Single-link and Complete-link Hierarchical Agglomerative Clustering (Jain et al., 1999).", "labels": [], "entities": []}, {"text": "The \"QT\" column shows the Quality Threshold algorithm) and the last column \"EM\" is the Expectation Maximization clustering algorithm).", "labels": [], "entities": [{"text": "Quality Threshold algorithm", "start_pos": 26, "end_pos": 53, "type": "METRIC", "confidence": 0.8022169272104899}, {"text": "EM", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9755281805992126}, {"text": "Expectation Maximization clustering", "start_pos": 87, "end_pos": 122, "type": "TASK", "confidence": 0.8547812898953756}]}, {"text": "One main conclusion, from table 1 is that clustering tends to achieve worst results than simple paraphrase pair extraction.", "labels": [], "entities": [{"text": "paraphrase pair extraction", "start_pos": 96, "end_pos": 122, "type": "TASK", "confidence": 0.7256289521853129}]}, {"text": "Only the QT achieves better results, but if we take the average of the four clustering algorithms it is equal to 0.568, smaller than the 0.618 baseline.", "labels": [], "entities": []}, {"text": "Moreover, these results with the QT algorithm were applied with a very restrictive value for cluster attribution as it is shown in table 2 with an average of almost two sentences per cluster.", "labels": [], "entities": []}, {"text": "In fact, table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by who only keep the clusters that contain more than 10 sentences.", "labels": [], "entities": []}, {"text": "In fact, the first conclusion is that the number of experimented clusters is very low, and more important, all clusters with more than 10 sentences showed to be of very bad quality.", "labels": [], "entities": []}, {"text": "The next subsection will reinforce the sight that clustering is a worthless effort for automatic paraphrase corpora construction.", "labels": [], "entities": [{"text": "automatic paraphrase corpora construction", "start_pos": 87, "end_pos": 128, "type": "TASK", "confidence": 0.630881629884243}]}], "tableCaptions": [{"text": " Table 1: Precision of clustering algorithms", "labels": [], "entities": []}, {"text": " Table 5: Precision of alignments.", "labels": [], "entities": [{"text": "Precision of alignments", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.6704431573549906}]}]}