{"title": [{"text": "The Best of Two Worlds: Cooperation of Statistical and Rule-Based Taggers for Czech", "labels": [], "entities": [{"text": "Rule-Based Taggers", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.6214862018823624}]}], "abstractContent": [{"text": "Several hybrid disambiguation methods are described which combine the strength of handwritten disambiguation rules and statistical taggers.", "labels": [], "entities": []}, {"text": "Three different statistical (HMM, Maximum-Entropy and Averaged Perceptron) taggers are used in a tagging experiment using Prague Dependency Tree-bank.", "labels": [], "entities": [{"text": "Prague Dependency Tree-bank", "start_pos": 122, "end_pos": 149, "type": "DATASET", "confidence": 0.9483341375986735}]}, {"text": "The results of the hybrid systems are better than any other method tried for Czech tagging so far.", "labels": [], "entities": [{"text": "Czech tagging", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.8303351104259491}]}], "introductionContent": [{"text": "Inflective languages pose a specific problem in tagging due to two phenomena: highly inflective nature (causing sparse data problem in any statistically based system), and free word order (causing fixedcontext systems, such as n-gram HMMs, to be even less adequate than for English).", "labels": [], "entities": [{"text": "tagging", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.964743971824646}]}, {"text": "The average tagset contains about 1,000 -2,000 distinct tags; the size of the set of possible and plausible tags can reach several thousands.", "labels": [], "entities": []}, {"text": "There have been attempts at solving this problem for some of the highly inflective European languages, such as,) for Slovenian and) for five Central and Eastern European languages.", "labels": [], "entities": []}, {"text": "Several taggers already exist for Czech, e.g. (,,) and).", "labels": [], "entities": []}, {"text": "The last one reaches the best accuracy for Czech so far (95.12 %).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9994256496429443}]}, {"text": "Hence no system has reached -in the absolute terms -a performance comparable to English tagging (such as), which stands above 97 %.", "labels": [], "entities": [{"text": "English tagging", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.5144581198692322}]}, {"text": "We are using the Prague Dependency Treebank () (PDT) with about 1.8 million hand annotated tokens of Czech for training and testing.", "labels": [], "entities": [{"text": "Prague Dependency Treebank () (PDT)", "start_pos": 17, "end_pos": 52, "type": "DATASET", "confidence": 0.9230218614850726}]}, {"text": "The tagging experiments in this paper all use the Czech morphological (pre)processor, which includes a guesser for \"unknown\" tokens and which is available from the PDT website) to disambiguate only among those tags which are morphologically plausible.", "labels": [], "entities": [{"text": "Czech morphological (pre)processor", "start_pos": 50, "end_pos": 84, "type": "DATASET", "confidence": 0.8013811111450195}, {"text": "PDT website", "start_pos": 164, "end_pos": 175, "type": "DATASET", "confidence": 0.8860280513763428}]}, {"text": "The meaning of the Czech tags (each tag has 15 positions) we are using is explained in.", "labels": [], "entities": []}, {"text": "The detailed linguistic description of the individual positions can be found in the documentation to the PDT (  The HMM tagger is based on the well known formula of HMM tagging: where The trigram probability P (W | T ) in formula 2 replaces) the common (and less accurate) bigram approach.", "labels": [], "entities": [{"text": "HMM tagging", "start_pos": 165, "end_pos": 176, "type": "TASK", "confidence": 0.7544604241847992}]}, {"text": "We will use this tagger as a baseline system for further improvements.", "labels": [], "entities": []}, {"text": "Initially, we change the formula 1 by introducing a scaling mechanism 1 : We tag the word sequence from right to left, i.e. we change the trigram probability P (W | T ) from formula 2 to P (w i | ti , t i+1 ).", "labels": [], "entities": []}, {"text": "Both the output probability P (w i | ti , t i+1 ) and the transition probability P (T ) suffer a lot due to the data sparseness problem.", "labels": [], "entities": []}, {"text": "We introduce a component P (ending i | ti , t i+1 ), where ending consists of the last three characters of w i . Also, we introduce another component P (t * i | t * i+1 , t * i+2 ) based on a reduced tagset T * that contains positions POS, GENDER, NUMBER and CASE only (chosen on linguistic grounds).", "labels": [], "entities": [{"text": "POS", "start_pos": 235, "end_pos": 238, "type": "METRIC", "confidence": 0.8542344570159912}, {"text": "GENDER", "start_pos": 240, "end_pos": 246, "type": "METRIC", "confidence": 0.8974589705467224}, {"text": "CASE", "start_pos": 259, "end_pos": 263, "type": "METRIC", "confidence": 0.9636467695236206}]}, {"text": "We upgrade all trigrams to fourgrams; the smoothing mechanism for fourgrams is historybased bucketing (.", "labels": [], "entities": []}, {"text": "The final fine-tuned HMM tagger thus uses all the enhancements and every component contains its scaling factor which has been computed using heldout data.", "labels": [], "entities": [{"text": "HMM tagger", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.8378859162330627}]}, {"text": "The total error rate reduction is 13.98 % relative on development data, measured against the baseline HMM tagger.", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9784072041511536}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Evaluation of the combinations", "labels": [], "entities": []}, {"text": " Table 5: Relative error rate reduction", "labels": [], "entities": [{"text": "Relative error rate", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8394095301628113}]}]}