{"title": [], "abstractContent": [{"text": "We describe a preliminary version of Mu-taphrase, a system that generates paraphrases of semantically labeled input sentences using the semantics and syntax encoded in FrameNet, a freely available lexico-semantic database.", "labels": [], "entities": []}, {"text": "The algorithm generates a large number of paraphrases with a wide range of syntactic and semantic distances from the input.", "labels": [], "entities": []}, {"text": "For example, given the input \"I like eating cheese\", the system outputs the syntactically distant \"Eating cheese is liked by me\", the semantically distant \"I fear sipping juice\", and thousands of other sentences.", "labels": [], "entities": []}, {"text": "The wide range of generated paraphrases makes the algorithm ideal fora range of statistical machine learning problems such as machine translation and language modeling as well as other semantics-dependent tasks such as query and language generation.", "labels": [], "entities": [{"text": "machine translation and language modeling", "start_pos": 126, "end_pos": 167, "type": "TASK", "confidence": 0.6893120408058167}, {"text": "language generation", "start_pos": 229, "end_pos": 248, "type": "TASK", "confidence": 0.7066307216882706}]}], "introductionContent": [{"text": "A central tenet of statistical natural language processing (NLP) is \"there's no data like more data\".", "labels": [], "entities": [{"text": "statistical natural language processing (NLP)", "start_pos": 19, "end_pos": 64, "type": "TASK", "confidence": 0.7507633396557399}]}, {"text": "One method for generating more data is to restate each phrase in a corpus, keeping similar semantics while changing both the words and the word sequence.", "labels": [], "entities": []}, {"text": "The efficacy of this approach has been well-established in many areas, including automated evaluation of machine translation systems), text summarization), question answering (, document retrieval (), and many others.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7403053939342499}, {"text": "text summarization", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.820470780134201}, {"text": "question answering", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.9225134253501892}, {"text": "document retrieval", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.7777464985847473}]}, {"text": "Most of the reported work on paraphrase generation from arbitrary input sentences uses machine learning techniques trained on sentences that are known or can be inferred to be paraphrases of each other; Barzilay and).", "labels": [], "entities": [{"text": "paraphrase generation from arbitrary input sentences", "start_pos": 29, "end_pos": 81, "type": "TASK", "confidence": 0.9303030172983805}]}, {"text": "Mutaphrase instead generates paraphrases algorithmically using an input sentence and FrameNet, a freely available lexico-semantic resource (information regarding FrameNet, including relevant terminology, is presented in Section 2).", "labels": [], "entities": []}, {"text": "Conceptually, the Mutaphrase algorithm takes a semantic specification of a sentence, provided by an automatic semantic parser such as Shalmaneser), and recursively replaces each semantically parsed phrase with a semantically similar phrase.", "labels": [], "entities": []}, {"text": "To generate each new phrase, each of the semantic parts of the original phrase is mapped, using FrameNet data, onto anew word or phrase whose position and syntactic marking maybe quite different.", "labels": [], "entities": [{"text": "FrameNet data", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.9327173829078674}]}, {"text": "The Mutaphrase algorithm outputs a large set of paraphrases with a variety of distances from the input in terms of both syntax and semantics; see Figure 1.", "labels": [], "entities": []}, {"text": "Depending on the needs of the application, filtering can be applied to limit the distance to a desired range.", "labels": [], "entities": []}, {"text": "For example, language modeling may benefit from a wider variety of semantic outputs, since if I like eating cheese is in-domain, then I like sipping juice is also likely in-domain.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7229819595813751}]}, {"text": "Other applications, e.g. Question Answering, require more stringent limits on semantic distance.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7696544229984283}]}], "datasetContent": [], "tableCaptions": []}