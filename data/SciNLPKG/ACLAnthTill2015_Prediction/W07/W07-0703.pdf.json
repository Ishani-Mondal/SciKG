{"title": [{"text": "Integration of an Arabic Transliteration Module into a Statistical Machine Translation System", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.7069312731424967}]}], "abstractContent": [{"text": "We provide an in-depth analysis of the integration of an Arabic-to-English translit-eration system into a general-purpose phrase-based statistical machine translation system.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 122, "end_pos": 166, "type": "TASK", "confidence": 0.6273870393633842}]}, {"text": "We study the integration from different aspects and evaluate the improvement that can be attributed to the integration using the BLEU metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.99666827917099}]}, {"text": "Our experiments show that a transliteration module can help significantly in the situation where the test data is rich with previously unseen named entities.", "labels": [], "entities": []}, {"text": "We obtain 70% and 53% of the theoretical maximum improvement we could achieve, as measured by an oracle on development and test sets respectively for OOV words (out of vocabulary source words not appearing in the phrase table).", "labels": [], "entities": []}], "introductionContent": [{"text": "Transliteration is the practice of transcribing a word or text written in one writing system into another writing system.", "labels": [], "entities": [{"text": "transcribing a word or text written", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.7826811571915945}]}, {"text": "The most frequent candidates for transliteration are person names, locations, organizations and imported words.", "labels": [], "entities": []}, {"text": "The lack of a fully comprehensive bilingual dictionary including the entries for all named entities (NEs) renders the task of transliteration necessary for certain natural language processing applications dealing with named entities.", "labels": [], "entities": []}, {"text": "Two applications where transliteration can be particularly useful are machine translation (MT) and cross lingual information retrieval.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.8522282361984252}, {"text": "cross lingual information retrieval", "start_pos": 99, "end_pos": 134, "type": "TASK", "confidence": 0.7266402393579483}]}, {"text": "While transliteration itself is a relatively wellstudied problem, its effect on the aforementioned applications is still under investigation.", "labels": [], "entities": []}, {"text": "Transliteration as a self-contained task has its own challenges, but applying it to areal application introduces new challenges.", "labels": [], "entities": []}, {"text": "In this paper we analyze the efficacy of integrating a transliteration module into areal MT system and evaluate the performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9283739924430847}]}, {"text": "When working on a limited domain, given a sufficiently large amount of training data, almost all of the words in the unseen data (in the same domain) will have appeared in the training corpus.", "labels": [], "entities": []}, {"text": "But this argument does not hold for NEs, because no matter how big the training corpus is, there will always be unseen names of people and locations.", "labels": [], "entities": [{"text": "NEs", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9706137776374817}]}, {"text": "Current MT systems either leave such unknown names as they are in the final target text or remove them in order to obtain a better evaluation score.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9842457175254822}]}, {"text": "None of these methods can give the reader who is not familiar with the source language any information about those out-of-vocabulary (OOV) words, especially when the source and target languages use different scripts.", "labels": [], "entities": []}, {"text": "If these words are not names, one can usually guess what they are, by using the partial information of other parts of speech.", "labels": [], "entities": []}, {"text": "But, in the case of names, there is noway to determine the individual or location the sentence is talking about.", "labels": [], "entities": []}, {"text": "So, to improve the usability of a translation, it is particularly important to handle NEs well.", "labels": [], "entities": []}, {"text": "The importance of NEs is not yet reflected in the evaluation methods used in the MT community, the most common of which is the BLEU metric.", "labels": [], "entities": [{"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9722704887390137}, {"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9964589476585388}]}, {"text": "BLEU () was devised to provide automatic evaluation of MT output.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9811500310897827}, {"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9852181077003479}]}, {"text": "In this metric n-gram similarity of the MT output is computed with one or more references made by human translators.", "labels": [], "entities": [{"text": "similarity", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.900632381439209}, {"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9745025038719177}]}, {"text": "BLEU does not distinguish between different words and gives equal weight to all.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9713976383209229}]}, {"text": "In this paper, we base our evaluation on the BLEU metric and show that using transliteration has impact on it (and in some cases significant impact).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9906134605407715}]}, {"text": "However, we believe that such integration is more important for practical uses of MT than BLEU indicates.", "labels": [], "entities": [{"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.945814847946167}, {"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9978424310684204}]}, {"text": "Other than improving readability and raising the BLEU score, another advantage of using a transliteration system is that having the right translation fora name helps the language model select a better ordering for other words.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9825606644153595}]}, {"text": "The richer the training data, the higher the chance fora given name in the test data to be found in the translation tables.", "labels": [], "entities": []}, {"text": "In other words, an MT system with a relatively rich phrase table is able to translate many of the common names in the test data, with all the remaining words being rare and foreign.", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9479790925979614}]}, {"text": "So unlike a self-contained transliteration module, which typically deals with a mix of 'easy' and 1 A table where the conditional probabilities of target phrases given source phrases (and vice versa) is kept.", "labels": [], "entities": []}, {"text": "Note that the language model can be trained on more text, and hence can know more NEs than the translation model does.", "labels": [], "entities": []}, {"text": "'hard' names, the primary use fora transliteration module embedded in an SMT system will be to deal with the 'hard' names leftover after the phrase tables have provided translations for the 'easy' ones.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9767388105392456}]}, {"text": "That means that when measuring the performance improvements caused by embedding a transliteration module in an MT system, one must keep in mind that such improvements are difficult to attain: they are won mainly by correctly transliterating 'hard' names.", "labels": [], "entities": []}, {"text": "Another issue with OOV words is that some of them remained untranslated due to misspellings in the source text.", "labels": [], "entities": []}, {"text": "For example, we encountered \"\u202b\"\u06be\ufe9c\ufbff\ufeae\u0648\u202c (\"Hthearow\") instead of \"\u202b\"\u06be\ufbff\ufe9c\ufeae\u0648\u202c (\"Heathrow\") or \"\u202b\"\ufe91\ufeae\ufbfe\ufeb0\u0631\u202c (\"Brezer\") instead of \"\u202b\"\ufe91\ufeae\ufbfe\ufee4\ufeae\u202c (\"Bremer\") in our development test set.", "labels": [], "entities": [{"text": "Heathrow", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.9714438319206238}]}, {"text": "Also, evaluation by BLEU (or a similar automatic metric) is problematic.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9972575306892395}]}, {"text": "Almost all of the MT evaluations use one or more reference translations as the gold standard and, using some metrics, they give a score to the MT output.", "labels": [], "entities": [{"text": "MT evaluations", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.8996324837207794}, {"text": "MT", "start_pos": 143, "end_pos": 145, "type": "TASK", "confidence": 0.9456853270530701}]}, {"text": "The problem with NEs is that they usually have more than a single equivalent in the target language (especially if they don't originally come from the target language) which mayor may not have been captured in the gold standard.", "labels": [], "entities": [{"text": "NEs", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9622679352760315}]}, {"text": "So even if the transliteration module comes up with a correct interpretation of a name it might not receive credit as far as the limited number of correct names in the references are concerned.", "labels": [], "entities": []}, {"text": "Our first impression was that having more interpretations fora name in the references would raise the transliteration module's chance to generate at least one of them, hence improving the performance.", "labels": [], "entities": []}, {"text": "But, in practice, when references do not agree on a name's transliteration that is the sign of an ambiguity.", "labels": [], "entities": []}, {"text": "In these cases, the transliteration module often suggests a correct transliteration that the decoder outputs correctly, but which fails to receive credit from the BLEU metric because this transliteration is not found in the references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9858483076095581}]}, {"text": "As an example, for the name \"\u202b,\"\ufeb3\ufeee\ufbfe\ufeae\ufbfe\ufeee\u0633\u202c four references came up with four different interpretations: swerios, swiriyus, severius, sweires.", "labels": [], "entities": []}, {"text": "A quick query in Google showed us another four acceptable interpretations (severios, sewerios, sweirios, sawerios).", "labels": [], "entities": []}, {"text": "Machine transliteration has been an active research field for quite a while) but to our knowledge there is little published work on evaluating transliteration within areal MT system.", "labels": [], "entities": [{"text": "Machine transliteration", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7568749189376831}, {"text": "MT", "start_pos": 172, "end_pos": 174, "type": "TASK", "confidence": 0.9165518879890442}]}, {"text": "The closest work to ours is described in) where they have a list of names in Arabic and feed this list as the input text to their MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 130, "end_pos": 132, "type": "TASK", "confidence": 0.9458996653556824}]}, {"text": "They evaluate their system in three different cases: as a word-based NE translation, phrase-based NE translation and in presence of a transliteration module.", "labels": [], "entities": [{"text": "phrase-based NE translation", "start_pos": 85, "end_pos": 112, "type": "TASK", "confidence": 0.6000249783198038}]}, {"text": "Then, they report the BLEU score on the final output.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9842118620872498}]}, {"text": "Since their text is comprised of only NEs, the BLEU increase is quite high.", "labels": [], "entities": [{"text": "NEs", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9035512804985046}, {"text": "BLEU increase", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.9824548065662384}]}, {"text": "Combining all three models, they get a 24.9 BLEU point increase over the na\u00efve baseline.", "labels": [], "entities": [{"text": "BLEU point increase", "start_pos": 44, "end_pos": 63, "type": "METRIC", "confidence": 0.978610614935557}]}, {"text": "The difference they report between their best method without transliteration and the one including transliteration is 8.12 BLEU points for person names (their best increase).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9969874024391174}]}, {"text": "In section 2, we introduce different methods for incorporating a transliteration module into an MT system and justify our choice.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9659488797187805}]}, {"text": "In section 3, the transliteration module is briefly introduced and we explain how we prepared its output for use by the MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 120, "end_pos": 122, "type": "TASK", "confidence": 0.9645515084266663}]}, {"text": "In section 4, an evaluation of the integration is provided.", "labels": [], "entities": []}, {"text": "Finally, section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Although there are metrics that directly address NE translation performance , we chose to use BLEU because our purpose is to assess NE translation within MT, and BLEU is currently the standard metric for MT.", "labels": [], "entities": [{"text": "NE translation", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.9157425761222839}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9977802634239197}, {"text": "NE translation", "start_pos": 132, "end_pos": 146, "type": "TASK", "confidence": 0.8556906580924988}, {"text": "MT", "start_pos": 154, "end_pos": 156, "type": "TASK", "confidence": 0.5775236487388611}, {"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9976372718811035}, {"text": "MT", "start_pos": 204, "end_pos": 206, "type": "TASK", "confidence": 0.9692299962043762}]}], "tableCaptions": [{"text": " Table 1: Distribution of sentences in test sets.", "labels": [], "entities": [{"text": "Distribution of sentences", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8610860506693522}]}, {"text": " Table 2: BLEU score on different test sets.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9575187563896179}]}, {"text": " Table 3: BLEU score on different  portions of the test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992412328720093}]}]}