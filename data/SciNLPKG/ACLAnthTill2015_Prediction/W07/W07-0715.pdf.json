{"title": [{"text": "An Iteratively-Trained Segmentation-Free Phrase Translation Model for Statistical Machine Translation", "labels": [], "entities": [{"text": "Iteratively-Trained Segmentation-Free Phrase Translation", "start_pos": 3, "end_pos": 59, "type": "TASK", "confidence": 0.6529327481985092}, {"text": "Statistical Machine Translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.7988905509312948}]}], "abstractContent": [{"text": "Attempts to estimate phrase translation probablities for statistical machine translation using iteratively-trained models have repeatedly failed to produce translations as good as those obtained by estimating phrase translation probablities from surface statistics of bilingual word alignments as described by Koehn, et al.", "labels": [], "entities": [{"text": "phrase translation probablities", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.784578541914622}, {"text": "statistical machine translation", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.647146979967753}]}, {"text": "We propose anew iteratively-trained phrase translation model that produces translations of quality equal to or better than those produced by Koehn, et al.'s model.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7853678464889526}]}, {"text": "Moreover, with the new model, translation quality degrades much more slowly as pruning is tigh-tend to reduce translation time.", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9513310790061951}]}], "introductionContent": [{"text": "Estimates of conditional phrase translation probabilities provide a major source of translation knowledge in phrase-based statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "phrase translation probabilities", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.7632589737574259}, {"text": "phrase-based statistical machine translation (SMT)", "start_pos": 109, "end_pos": 159, "type": "TASK", "confidence": 0.7224485320704324}]}, {"text": "The most widely used method for estimating these probabilities is that of, in which phrase pairs are extracted from word-aligned bilingual sentence pairs, and their translation probabilities estimated heuristically from surface statistics of the extracted phrase pairs.", "labels": [], "entities": []}, {"text": "We will refer to this approach as \"the standard model\".", "labels": [], "entities": []}, {"text": "There have been several attempts to estimate phrase translation probabilities directly, using generative models trained iteratively on a parallel corpus using the Expectation Maximization (EM) algorithm.", "labels": [], "entities": [{"text": "phrase translation probabilities", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.7921196122964224}, {"text": "Expectation Maximization (EM)", "start_pos": 163, "end_pos": 192, "type": "TASK", "confidence": 0.7845892190933228}]}, {"text": "The first of these models, that of, was found by , to produce translations not quite as good as their method.", "labels": [], "entities": []}, {"text": "Recently, tried the Marcu and Wong model constrained by a word alignment and also found that Koehn, et al.'s model worked better, with the advantage of the standard model increasing as more features were added to the overall translation model.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.6992729306221008}]}, {"text": "tried a different generative phrase translation model analogous to IBM word-translation Model 3, and again found that the standard model outperformed their generative model.", "labels": [], "entities": [{"text": "generative phrase translation", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.8445677359898885}]}, {"text": "attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM algorithm to maximize the probability of the training data without really improving the quality of the model.", "labels": [], "entities": []}, {"text": "We propose an iteratively-trained phrase translation model that does not require different segmentations to compete against one another, and we show that this produces translations of quality equal to or better than those produced by the standard model.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7571782767772675}]}, {"text": "We find, moreover, that with the new model, translation quality degrades much more slowly as pruning is tightend to reduce translation time.", "labels": [], "entities": [{"text": "translation", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.9564141631126404}]}, {"text": "Decoding efficiency is usually considered only in the design and implementation of decoding algorithms, or the choice of model structures to support faster decoding algorithms.", "labels": [], "entities": []}, {"text": "We are not aware of any attention previously having been paid to the effect of different methods of parameter estimation on translation efficiency fora given model structure.", "labels": [], "entities": []}, {"text": "The time required for decoding is of great importance in the practical application of SMT tech-nology.", "labels": [], "entities": [{"text": "SMT tech-nology", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.8780226409435272}]}, {"text": "One of the criticisms of SMT often made by adherents of rule-based machine translation is that SMT is too slow for practical application.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9936811923980713}, {"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7210993766784668}, {"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.988645076751709}]}, {"text": "The rapidly falling price of computer hardware has ameliorated this problem to a great extent, but the fact remains that every factor of 2 improvement in translation efficiency means a factor of 2 decrease in hardware cost for intensive applications of SMT, such as a web-based translation service (\"Translate this page\").", "labels": [], "entities": [{"text": "SMT", "start_pos": 253, "end_pos": 256, "type": "TASK", "confidence": 0.9882881045341492}]}, {"text": "SMT surely needs all the help in can get in this regard.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8270822167396545}]}, {"text": "method of estimating phrasetranslation probabilities is very simple.", "labels": [], "entities": [{"text": "estimating phrasetranslation probabilities", "start_pos": 10, "end_pos": 52, "type": "TASK", "confidence": 0.8412554065386454}]}, {"text": "They start with an automatically word-aligned corpus of bilingual sentence pairs, in which certain words are linked, indicating that they are translations of each other, or that they are parts of phrases that are translations of each other.", "labels": [], "entities": []}, {"text": "They extract every possible phrase pair (up to a given length limit) that (a) contains at least one pair of linked words, and (b) does not contain any words that have links to other words not included in the phrase pair.", "labels": [], "entities": []}, {"text": "1 In other words, word alignment links cannot cross phrase pair boundaries.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7420870959758759}]}, {"text": "Phrase translation probabilities are estimated simply by marginalizing the counts of phrase instances:", "labels": [], "entities": [{"text": "Phrase translation probabilities", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9266142646471659}]}], "datasetContent": [{"text": "We evaluated our phrase translation model compared to the standard model of in the context of a fairly typical end-to-end phrase-based SMT system.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8527152240276337}, {"text": "SMT", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.8662655353546143}]}, {"text": "The lexical scores are computed as the (unnormalized) log probability of the Viterbi alignment fora phrase pair under IBM word-translation Model 1).", "labels": [], "entities": []}, {"text": "The feature weights for the overall translation models were trained using minimum-error-rate training procedure.", "labels": [], "entities": []}, {"text": "The weights were optimized separately for our model and for the standard phrase translation model.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.7979708909988403}]}, {"text": "Our decoder is a reimplementation in Perl of the algorithm used by the Pharaoh decoder as described by.", "labels": [], "entities": []}, {"text": "The data we used comes from an English-French bilingual corpus of Canadian Hansards parliamentary proceedings supplied for the bilingual word alignment workshop held at HLT-NAACL 2003.", "labels": [], "entities": [{"text": "English-French bilingual corpus of Canadian Hansards parliamentary proceedings", "start_pos": 31, "end_pos": 109, "type": "DATASET", "confidence": 0.8014817759394646}, {"text": "word alignment workshop held at HLT-NAACL 2003", "start_pos": 137, "end_pos": 183, "type": "TASK", "confidence": 0.7701931893825531}]}, {"text": "Automatic sentence alignment of this data was provided by Ulrich Germann.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7028389871120453}]}, {"text": "We used 500,000 sentences pairs from this corpus for training both the phrase translation models and IBM Model 1 lexical scores.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7732027471065521}]}, {"text": "These 500,000 sentence pairs were word-aligned using a state-ofthe-art word-alignment method ().", "labels": [], "entities": []}, {"text": "A separate set of 500 sentence pairs was used to train the translation model weights, and two additional held-out sets of 2000 sentence pairs each were used as test data.", "labels": [], "entities": []}, {"text": "The two phrase translation models were trained using the same set of possible phrase pairs extracted from the word-aligned 500,000 sentence pair corpus, finding all possible phrase pairs permitted by the criteria followed by Koehn et al., up to a phrase length of seven words.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.7474466264247894}]}, {"text": "This produced approximately 69 million distinct phrase pair types.", "labels": [], "entities": []}, {"text": "No pruning of the set of possible phrase pairs was done during or before training the phrase translation models.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.744902104139328}]}, {"text": "Our phrase translation model and IBM Model 1 were both trained for five iterations.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8558766841888428}]}, {"text": "The training procedure for our phrase translation model trains models in both directions simultaneously, but for IBM Model 1, models were trained separately in each direction.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7823765277862549}]}, {"text": "The models were then pruned to include only phrase pairs that matched the source sides of the small training and test sets.", "labels": [], "entities": []}, {"text": "We wanted to look at the trade-off between decoding time and translation quality for our new phrase translation model compared to the standard model.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.8137840032577515}]}, {"text": "Since this trade-off is also affected by the settings of various pruning parameters, we compared decoding time and translation quality, as measured by BLEU score (), for the two models on our first test set over abroad range of settings for the decoder pruning parameters.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 151, "end_pos": 161, "type": "METRIC", "confidence": 0.9812915623188019}]}, {"text": "The Pharaoh decoding algorithm, has five pruning parameters that affect decoding time: \u2022 Distortion limit \u2022 Translation table threshold \u2022 Beam limit The distortion limit is the maximum distance allowed between two source phrases that produce adjacent target phrases in the decoder output.", "labels": [], "entities": []}, {"text": "The distortion limit can be viewed as a model parameter, as well as a pruning paramter, because setting it to an optimum value usually improves translation quality over leaving it unrestricted.", "labels": [], "entities": []}, {"text": "We carried out experiments with the distortion limit set to 1, which seemed to produce the highest BLEU scores on our data set with the standard model, and also set to 5, which is perhaps a more typical value for phrasebased SMT systems.", "labels": [], "entities": [{"text": "distortion limit set", "start_pos": 36, "end_pos": 56, "type": "METRIC", "confidence": 0.9401282072067261}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9993389248847961}, {"text": "SMT", "start_pos": 225, "end_pos": 228, "type": "TASK", "confidence": 0.8204113245010376}]}, {"text": "Translation model weights were trained separately for these two settings, because the greater the distortion limit, the higher the distortion penalty weight needed for optimal translation quality.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9337586760520935}]}, {"text": "The translation table limit and translation table threshold are applied statically to the phrase translation table, which combines all components of the overall translation model score that can be computed for each phrase pair in isolation.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7272011637687683}]}, {"text": "This includes all information except the distortion penalty score and the part of the language model score that looks at n-grams that cross target phrase boundaries.", "labels": [], "entities": [{"text": "distortion penalty score", "start_pos": 41, "end_pos": 65, "type": "METRIC", "confidence": 0.9734893441200256}]}, {"text": "The translation table limit is the maximum number of translations allowed in the table for any given source phrase.", "labels": [], "entities": []}, {"text": "The translation table threshold is the maximum difference in combined translation table score allowed between the highest scoring translation and lowest scoring translation for any given source phrase.", "labels": [], "entities": []}, {"text": "The beam limit and beam threshold are defined similarly, but they apply dynamically to the sets of competing partial hypotheses that cover the same number of source words in the beam search for the highest scoring translation.", "labels": [], "entities": []}, {"text": "For each of the two distortion limits we tried, we carried out a systematic search for combinations of settings of the other four pruning parameters that gave the best trade-offs between decoding time and BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 205, "end_pos": 215, "type": "METRIC", "confidence": 0.9849100112915039}]}, {"text": "Starting at a setting of 0.5 for the threshold parameters 3 and 5 for the limit parameters we performed a hill-climbing search over step-wise relaxations of all combinations of the four parame- ters, incrementing the threshold parameters by 0.5 and the limit parameters by 5 at each step.", "labels": [], "entities": []}, {"text": "For each resulting point that provided the best BLEU score yet seen for the amount of decoding time used, we iterated the search.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9793403446674347}]}, {"text": "The resulting possible combinations of BLEU score and decoding time for the two phrase translation models are displayed in, fora distortion limit of 1, and, fora distortion limit of 5.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9819501042366028}, {"text": "phrase translation", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7267663180828094}]}, {"text": "BLEU score is reported on a scale of 1-100 (BLEU), and decoding time is measured in milliseconds per word.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9637778997421265}, {"text": "BLEU)", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9871033132076263}]}, {"text": "Note that the decoding time axis is presented on a log scale.", "labels": [], "entities": []}, {"text": "The points that represent pruning parameter settings one might consider using in a practical system are those on or near the upper convex hull of the set of points for each model.", "labels": [], "entities": []}, {"text": "These upper-convexhull points are highlighted in the figures.", "labels": [], "entities": []}, {"text": "Points far from these boundaries represent settings of one or more of the parameters that are too restrictive to obtain good translation quality, together with settings of other parameters that are too permissive to obtain good translation time.", "labels": [], "entities": []}, {"text": "Examining the results fora distortion limit of 1, we found that the BLEU score obtained with the loosest pruning parameter settings (2.5 for both threshold paramters, and 25 for both limit parameters) were essentially identical for the two models: 30.42 BLEU.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9758331179618835}, {"text": "BLEU", "start_pos": 254, "end_pos": 258, "type": "METRIC", "confidence": 0.9985936284065247}]}, {"text": "As the pruning parameters are tightened to reduce decoding time, however, the new model performs much better.", "labels": [], "entities": []}, {"text": "At a decoding time almost 6 times faster than for the settings that produced the highest BLEU score, the change in score was only \u22120.07 BLEU with the new model.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9804318249225616}, {"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9954740405082703}]}, {"text": "To obtain a slightly worse 4 BLEU score (\u22120.08 BLEU) using the standard model took 90% more decoding time.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9802913963794708}, {"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9533594846725464}]}, {"text": "It does appear, however, that the best BLEU score for the standard model is slightly better than the best BLEU score for the new model: 30.43 vs. 30.42.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9803464412689209}, {"text": "BLEU score", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9814162850379944}]}, {"text": "It is in fact currious that there seem to be numerous points where the standard model gets a slightly better BLEU score than it does with with the loosest pruning settings, which should have the lowest search error.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9851324260234833}]}, {"text": "We conjectured that this might bean artifact of our test procedure.", "labels": [], "entities": []}, {"text": "If a model is at all reasonable, most search errors will reduce the ultimate objective function, in our case the BLEU score, but occasionally a search error will increase the objective function just by chance.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.985986053943634}]}, {"text": "The smaller the number of search errors in a particular test, the greater the likelihood that, by chance, more search errors will increase the objective function than decrease it.", "labels": [], "entities": []}, {"text": "Since we are sampling a fairly large number of combinations of pruning parameter settings (179 for the standard model with a distortion limit of 1), it is possible that a small number of these have more \"good\" search errors than \"bad\" search errors simply by chance, and that this accounts for the small number of points at which the BLEU score exceeds that of the point which should have the fewest search errors.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 334, "end_pos": 344, "type": "METRIC", "confidence": 0.9826847612857819}]}, {"text": "This effect maybe more pronounced with the standard model than with the new model, simply because there is more noise in the standard model.", "labels": [], "entities": []}, {"text": "To test the hypothesis that the BLEU scores greater than the score for the loosest pruning settings simply represent noise in the data, we collected all the pruning settings that produced BLEU scores greater than or equal to the the one for the loosest pruning settings, and evaluated the standard model at those settings on our second held-out test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9989431500434875}, {"text": "BLEU", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.9983764886856079}]}, {"text": "We then looked at the correlation between the BLEU scores for these settings on the two test sets, and found that it was very small and negative, with r = \u22120.099.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9994379878044128}]}, {"text": "The standard F-test for the significance of a correlation yielded p = 0.74; in other words, completely insignificant.", "labels": [], "entities": [{"text": "F-test", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9986324906349182}]}, {"text": "This strongly suggests that the apparent improvement in BLEU score for certain tighter pruning settings is illusory.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.981543242931366}]}, {"text": "As a sanity check, we tested the BLEU score correlation between the two test sets for the points on the upper convex hull of the plot for the standard model, between the point with the fastest decoding time and the point with the highest BLEU score.", "labels": [], "entities": [{"text": "BLEU score correlation", "start_pos": 33, "end_pos": 55, "type": "METRIC", "confidence": 0.9763124783833822}, {"text": "BLEU", "start_pos": 238, "end_pos": 242, "type": "METRIC", "confidence": 0.9981536269187927}]}, {"text": "That correlation was very high, with r = 0.94, which was significant at the level p = 0.0004 according to the F-test.", "labels": [], "entities": [{"text": "correlation", "start_pos": 5, "end_pos": 16, "type": "METRIC", "confidence": 0.9676624536514282}, {"text": "F-test", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.49722614884376526}]}, {"text": "Thus the BLEU score differences along most of the upper convex hull seem to reflect reality, but not in the region where they equal or exceed the score for the loosest pruning settings.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9775230586528778}]}, {"text": "At a distortion limit of 5, there seems no question that the new model performs better than the standard model.", "labels": [], "entities": [{"text": "distortion", "start_pos": 5, "end_pos": 15, "type": "METRIC", "confidence": 0.9804343581199646}]}, {"text": "The difference BLEU scores for the upperconvex-hull points ranges from about 0.8 to 0.2 BLEU[%] for comparable decoding times.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9994766116142273}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9990177154541016}]}, {"text": "Again, the advantage of the new model is greater at shorter decoding times.", "labels": [], "entities": []}, {"text": "Compared to the results with a distortion limit of 1, the standard model loses translation quality, with a change of about \u22120.2 BLEU for the loosest pruning settings, while the new model gains very slightly (+0.04 BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9975202679634094}, {"text": "BLEU", "start_pos": 214, "end_pos": 218, "type": "METRIC", "confidence": 0.9942535758018494}]}], "tableCaptions": []}