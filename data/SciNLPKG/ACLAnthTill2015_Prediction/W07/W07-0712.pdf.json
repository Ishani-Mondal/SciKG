{"title": [{"text": "Efficient Handling of N -gram Language Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.856154998143514}]}], "abstractContent": [{"text": "Statistical machine translation, as well as other areas of human language processing, have recently pushed toward the use of large scale n-gram language models.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6739378670851389}, {"text": "human language processing", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.7151671051979065}]}, {"text": "This paper presents efficient algorithmic and architectural solutions which have been tested within the Moses decoder, an open source toolkit for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 146, "end_pos": 177, "type": "TASK", "confidence": 0.7491694092750549}]}, {"text": "Experiments are reported with a high performing baseline, trained on the Chinese-English NIST 2006 Evaluation task and running on a standard Linux 64-bit PC architecture.", "labels": [], "entities": [{"text": "Chinese-English NIST 2006 Evaluation task", "start_pos": 73, "end_pos": 114, "type": "DATASET", "confidence": 0.7801028311252594}]}, {"text": "Comparative tests show that our representation halves the memory required by SRI LM Toolkit, at the cost of 44% slower translation speed.", "labels": [], "entities": [{"text": "SRI LM Toolkit", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.8743162949879965}]}, {"text": "However, as it can take advantage of memory mapping on disk, the proposed implementation seems to scale-up much better to very large language models: decoding with a 289-million 5-gram language model runs in 2.1Gb of RAM.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, we have seen an increasing interest toward the application of n-gram Language Models (LMs) in several areas of computational linguistics (), such as machine translation, word sense disambiguation, text tagging, named entity recognition, etc.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 166, "end_pos": 185, "type": "TASK", "confidence": 0.8145751953125}, {"text": "word sense disambiguation", "start_pos": 187, "end_pos": 212, "type": "TASK", "confidence": 0.6925782561302185}, {"text": "text tagging", "start_pos": 214, "end_pos": 226, "type": "TASK", "confidence": 0.7648870050907135}, {"text": "named entity recognition", "start_pos": 228, "end_pos": 252, "type": "TASK", "confidence": 0.6583758691946665}]}, {"text": "The original framework of n-gram LMs was principally automatic speech recognition, under which most of the standard LM estimation techniques) were developed.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.6640294790267944}, {"text": "LM estimation", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.7738591432571411}]}, {"text": "Nowadays, the availability of larger and larger text corpora is stressing the need for efficient data structures and algorithms to estimate, store and access LMs.", "labels": [], "entities": []}, {"text": "Unfortunately, the rate of progress in computer technology seems for the moment below the space requirements of such huge LMs, at least by considering standard lab equipment.", "labels": [], "entities": []}, {"text": "Statistical machine translation (SMT) is today one of the research areas that, together with speech recognition, is pushing mostly toward the use of huge n-gram LMs.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8669426441192627}, {"text": "speech recognition", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7988742887973785}]}, {"text": "In the 2006 NIST Machine Translation Workshop), best performing systems employed 5-grams LMs estimated on at least 1.3 billion-word texts.", "labels": [], "entities": [{"text": "NIST Machine Translation Workshop", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.7741329669952393}]}, {"text": "In particular, Google Inc.", "labels": [], "entities": []}, {"text": "presented SMT results with LMs trained on 8 trillion-word texts, and announced the availability of n-gram statistics extracted from one trillion of words.", "labels": [], "entities": [{"text": "SMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9883661866188049}]}, {"text": "The n-gram Google collection is now publicly available through LDC, but their effective use requires either to significantly expand computer memory, in order to use existing tools, or to develop new ones.", "labels": [], "entities": [{"text": "n-gram Google collection", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.9191896120707194}, {"text": "LDC", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.9133014678955078}]}, {"text": "This work presents novel algorithms and data structures suitable to estimate, store, and access very large LMs.", "labels": [], "entities": []}, {"text": "The software has been integrated into a popular open source SMT decoder called Moses.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.8989448845386505}]}, {"text": "Experimental results are reported on the Chinese-English NIST task, starting from a quite well-performing baseline, that exploits a large 5-gram LM.", "labels": [], "entities": [{"text": "NIST task", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.6942131817340851}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents techniques for the estimation and represen-tation in memory of n-gram LMs that try to optimize space requirements.", "labels": [], "entities": []}, {"text": "Section 3 describes methods implemented in order to efficiently access the LM at run time, namely by the Moses SMT decoder.", "labels": [], "entities": [{"text": "Moses SMT decoder", "start_pos": 105, "end_pos": 122, "type": "DATASET", "confidence": 0.8372025291124979}]}, {"text": "Section 4 presents a list of experiments addressing specific questions on the presented implementation.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to assess the quality of our implementation, henceforth named IRSTLM, we have designed a suite of experiments with a twofold goal: from one side the comparison of IRSTLM against a popular LM library, namely the SRILM toolkit); from the other, to measure the actual impact of the implementation solution discussed in previous sections.", "labels": [], "entities": []}, {"text": "Experiments were performed on a common statistical MT platform, namely Moses, in which both the IRSTLM and SRILM toolkits have been integrated.", "labels": [], "entities": [{"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.7729430794715881}]}, {"text": "The  The task chosen for our experiments is the translation of news from Chinese to English, as proposed by the NIST MT Evaluation Workshop of 2006.", "labels": [], "entities": [{"text": "translation of news from Chinese to English", "start_pos": 48, "end_pos": 91, "type": "TASK", "confidence": 0.9076235038893563}, {"text": "NIST MT Evaluation Workshop of 2006", "start_pos": 112, "end_pos": 147, "type": "DATASET", "confidence": 0.868025153875351}]}, {"text": "3 A translation system was trained according to the large-data condition.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.980305552482605}]}, {"text": "In particular, all the allowed bilingual corpora have been used for estimating the phrase-table.", "labels": [], "entities": []}, {"text": "The target side of these texts was also employed for the estimation of three 5-gram LMs, henceforth named large.", "labels": [], "entities": []}, {"text": "In particular, two LMs were estimated with the SRILM toolkit by pruning singletons events and by employing the WittenBell and the absolute discounting () smoothing methods; the shorthand for these two LMs will be \"lrg-sri-wb\" and \"lrg-sri-kn\", respectively.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.8838533759117126}]}, {"text": "Another large LM was estimated with the IRSTLM toolkit, by employing the only smoothing method available in the package (Witten-Bell) and by pruning singletons n-grams; its shorthand will be \"lrg\".", "labels": [], "entities": [{"text": "IRSTLM toolkit", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.928167849779129}]}, {"text": "An additional, much larger, 5-gram LM was instead trained with the IRSTLM toolkit on the socalled English Gigaword corpus, one of the allowed monolingual resources for this task.", "labels": [], "entities": [{"text": "IRSTLM toolkit", "start_pos": 67, "end_pos": 81, "type": "DATASET", "confidence": 0.9053797125816345}, {"text": "English Gigaword corpus", "start_pos": 98, "end_pos": 121, "type": "DATASET", "confidence": 0.6777057548364004}]}, {"text": "Automatic translation was performed by means of Moses which, among other things, permits the contemporary use of more LMs, feature we exploited in our experiments as specified later.", "labels": [], "entities": [{"text": "translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6974430680274963}]}, {"text": "Optimal interpolation weights for the log-linear model were estimated by running a minimum error training algorithm, available in the Moses toolkit, on the evaluation set of the NIST 2002 campaign.", "labels": [], "entities": [{"text": "evaluation set of the NIST 2002 campaign", "start_pos": 156, "end_pos": 196, "type": "DATASET", "confidence": 0.7105822605746133}]}, {"text": "Tests were performed on the evaluation sets of the successive campaigns).", "labels": [], "entities": []}, {"text": "Concerning the NIST 2006 evaluation set, results are given separately for three different types of texts, namely newswire (nw) and newsgroup (ng) texts, and broadcast news transcripts (bn).", "labels": [], "entities": [{"text": "NIST 2006 evaluation set", "start_pos": 15, "end_pos": 39, "type": "DATASET", "confidence": 0.970339298248291}]}, {"text": "gives figures about training, development and test corpora, while", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of LMs.", "labels": [], "entities": []}, {"text": " Table 3: Estimation of the \"giga\" LM: dictionary  and 5-gram statistics (K = 14).", "labels": [], "entities": [{"text": "Estimation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9842958450317383}]}, {"text": " Table 5: BLEU scores on NIST evaluation sets for  different LM configurations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990183115005493}, {"text": "NIST evaluation sets", "start_pos": 25, "end_pos": 45, "type": "DATASET", "confidence": 0.8636284470558167}]}, {"text": " Table 6: NIST scores on NIST evaluation sets for  different LM configurations.", "labels": [], "entities": [{"text": "NIST evaluation sets", "start_pos": 25, "end_pos": 45, "type": "DATASET", "confidence": 0.8191032409667969}]}, {"text": " Table 7: Process size and decoding speed with/wo  caching for different LM configurations.", "labels": [], "entities": []}, {"text": " Table 8: Case insensitive (ci) and sensitive (cs)  scores of the best performing system.", "labels": [], "entities": [{"text": "Case insensitive (ci) and sensitive (cs)  scores", "start_pos": 10, "end_pos": 58, "type": "METRIC", "confidence": 0.8346584466370669}]}]}