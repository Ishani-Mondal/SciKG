{"title": [{"text": "Cognate identification and alignment using practical orthographies", "labels": [], "entities": [{"text": "Cognate identification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8629342913627625}]}], "abstractContent": [{"text": "We use an iterative process of multi-gram alignment between associated words in different languages in an attempt to identify cognates.", "labels": [], "entities": []}, {"text": "To maximise the amount of data, we use practical orthographies instead of consistently coded phonetic transcriptions.", "labels": [], "entities": []}, {"text": "First results indicate that using practical or-thographies can be useful, the more so when dealing with large amounts of data.", "labels": [], "entities": []}], "introductionContent": [{"text": "The comparison of lexemes across languages is a powerful method to investigate the historical relations between languages.", "labels": [], "entities": []}, {"text": "A central prerequisite for any interpretation of historical relatedness is to establish lexical cognates, i.e. lexemes in different languages that are of shared descend (in contrast to similarity by chance).", "labels": [], "entities": [{"text": "interpretation of historical relatedness", "start_pos": 31, "end_pos": 71, "type": "TASK", "confidence": 0.800495520234108}]}, {"text": "If a pair of lexemes in two different languages stem from the same origin, this can be due to the fact that both languages derive from a common ancestor language, but it can also be caused by influence from one language on another (or influence on both language from a third language).", "labels": [], "entities": []}, {"text": "To decide whether cognates are indicative of a common ancestor language (\"vertical transmission\") or due to language influence (\"horizontal transmission\") is a difficult problem with no shortcuts.", "labels": [], "entities": []}, {"text": "We do not think that one kind of cognacy is more interesting that another.", "labels": [], "entities": []}, {"text": "Both loans (be it from a substrate or a superstrate) and lexemes derived from a shared ancestor are indicative of the history of a language, and both should be acknowledged in the unravelling of linguistic (pre)history.", "labels": [], "entities": []}, {"text": "In this paper, we approach the identification of cognate lexemes on the basis of large parallel lexica between languages.", "labels": [], "entities": [{"text": "identification of cognate lexemes", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.8671897053718567}]}, {"text": "This approach is an explicit attempt to reverse the \"Swadesh-style\" wordlist method.", "labels": [], "entities": []}, {"text": "In the Swadesh-style approach, first meanings are selected that are assumed to be less prone to borrowing, then cognates are identified in those lists, and these cognates are then interpreted as indicative of shared descend.", "labels": [], "entities": []}, {"text": "In contrast, we propose to first identify (possible) cognates among all available information, then divide these cognates into strata, and then interpret these strata in historical terms.", "labels": [], "entities": []}, {"text": "(Because of limitations of space, we will only deal with the first step, the identification of cognates, in this paper.)", "labels": [], "entities": [{"text": "identification of cognates", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.8660116195678711}]}, {"text": "This is of course exactly the route of the traditional historical-comparative approach to language comparison.", "labels": [], "entities": [{"text": "language comparison", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.6713488399982452}]}, {"text": "However, we think that much can be gained by applying computational approaches to this approach.", "labels": [], "entities": []}, {"text": "A major problem arises when dealing with large quantities of lexical material from many different languages.", "labels": [], "entities": []}, {"text": "In most cases it will be difficult (or very costly and time consuming in the least) to use coherent and consistent phonetic transcriptions of all available information.", "labels": [], "entities": []}, {"text": "Even if we would have dictionaries with phonetic transcriptions for all languages that we are interested in, this would not necessarily help, as the details of phonetic transcription are normally not consistent across different authors.", "labels": [], "entities": [{"text": "phonetic transcription", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.7504501342773438}]}, {"text": "In this paper, we will therefore attempt to deal with unprocessed material in practical orthographies.", "labels": [], "entities": []}, {"text": "This will of course pose problems for history-ridden orthographies like in English or French.", "labels": [], "entities": []}, {"text": "However, we beleve that for most of the world's languages the practical orthographies are not as inconsistent as those (because they are much younger) and might very well be useful for linguistic purposes.", "labels": [], "entities": []}, {"text": "In this paper, we will first discuss the data used in this investigation.", "labels": [], "entities": []}, {"text": "Then we will describe the algorithm that we used to infer alignments between word pairs.", "labels": [], "entities": []}, {"text": "Finally, we will discuss a few of the results using this algorithm on large wordlists in practical orthography.", "labels": [], "entities": []}], "datasetContent": [{"text": "As mentioned above, we applied our model to some test data from the IDS database.", "labels": [], "entities": [{"text": "IDS database", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.9345938265323639}]}, {"text": "For later analyses, we also constructed some random wordlists.", "labels": [], "entities": []}, {"text": "With these we are able to say something about how significant our results are.", "labels": [], "entities": []}, {"text": "To make these random wordlists we remap each word w a from La to an arbitrarily chosen word w b from collection L b . This new mapped word was adjusted to the size of the originally associated word from L b . The adjustment works by stretching or shrinking the new word to the required length by doubling the word several times and cutting of the overlaying head or tail afterwards.", "labels": [], "entities": []}, {"text": "In this way, we controlled for word length and multigram frequencies.", "labels": [], "entities": []}, {"text": "This randomization process was performed five times from La to L b , and five the times from L b to La , and the results were averaged overall these ten cases.", "labels": [], "entities": []}, {"text": "For the calculation process, we stored all lists in SQL tables.", "labels": [], "entities": [{"text": "calculation", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9848887920379639}]}, {"text": "We first built a preprocessed working table with the lexemes from the languages to be compared, and afterwards we constructed the resulting tables that holdall the results: \u2022 compare table: the word pairs, their alignments and alignment goodness; \u2022 subsequence table: the subsequence pairs found and their co-occurrence coefficients; \u2022 random compare table: pseudo random word pairs like the compare table; \u2022 random subsequence table: the subsequence pairs found from random compare table.", "labels": [], "entities": []}, {"text": "consists of the best alignments for word pairs of English and French after 30 iterations, and shows the best alignments for the comparison of English and Hunzib (a Caucasian language).", "labels": [], "entities": []}, {"text": "First note that our algorithm works independently of the orthography used.", "labels": [], "entities": []}, {"text": "We do not assume that the same UTF-8 characters in the two languages are identical.", "labels": [], "entities": []}, {"text": "The fact that c is mapped between English clan and French clan is a result of the statistical distribution of these characters in the two languages.", "labels": [], "entities": []}, {"text": "This orthography-independence means that we can apply our algorithm without modifications to cyrillic scripts as shown with the English-Hunzib comparison.", "labels": [], "entities": []}, {"text": "Second, we payed close attention to the fact that the word similarity values are comparable among different language comparisons.", "labels": [], "entities": []}, {"text": "This means that it is highly significant that the highest word similarities between English and French are much higher than those between English and Hunzib (actually, the alignments between English and Hunzib are nonsensical, but more about that later).", "labels": [], "entities": []}, {"text": "Further, our algorithm finds vowel-consonant multi-grams in some cases (e.g. see).", "labels": [], "entities": []}, {"text": "As far as we can see, there are not linguistically meaningful and should be considered an artifact of our current approach.", "labels": [], "entities": []}, {"text": "We hope to fine-tune the algorithm in the future to prevent this behavior.", "labels": [], "entities": []}, {"text": "Our method finds alignments, but also the subsequences in the alignments are of interest.", "labels": [], "entities": []}, {"text": "The best mapped multi-grams between English and French are illustrated in.", "labels": [], "entities": []}, {"text": "Strangely, the highest ranked ones area few vowel+consonant bigrams, that occur not very often.", "labels": [], "entities": []}, {"text": "Since the Dice coefficient depends on the size of the investigated collection, we assumed a minimum frequency of co-occurrences in each calculation step of 2% of the collection size (which is 20 cases in the English-French comparison).", "labels": [], "entities": [{"text": "Dice coefficient", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8349388837814331}]}, {"text": "The high-ranked bigrams are all just above this threshold.", "labels": [], "entities": []}, {"text": "Therefore, we might argue that all the bigrams from the top of the list area side-effect of the collection size itself.", "labels": [], "entities": []}, {"text": "Following these bigrams are many one-toone matches of all alphabetic characters except j,k,q,w,x,y,z.", "labels": [], "entities": []}, {"text": "These mappings are found without assuming any similarity based on the UTF-8 encoding of the characters.", "labels": [], "entities": []}, {"text": "What we actually find here is a mapping for the orthography of the stratum of the French loan words in English.", "labels": [], "entities": []}, {"text": "As can be seen in the histogram in, the mapping between multigrams falls off dramatically after these links.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 6: Best English (E) and Russian (R) multi- gram mappings after 30 iterations.", "labels": [], "entities": []}, {"text": " Table 7: Spanish (S) and Portuguese (P) multi-gram  mappings after 30 iterations. Only the  highest ranking non-identical mappings are  shown", "labels": [], "entities": []}]}