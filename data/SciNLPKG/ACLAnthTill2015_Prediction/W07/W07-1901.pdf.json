{"title": [{"text": "Comparing Rule-based and Data-driven Selection of Facial Displays", "labels": [], "entities": []}], "abstractContent": [{"text": "The non-verbal behaviour of an embodied conversational agent is normally based on recorded human behaviour.", "labels": [], "entities": []}, {"text": "There are two main ways that the mapping from human behaviour to agent behaviour has been implemented.", "labels": [], "entities": []}, {"text": "In some systems, human behaviour is analysed, and then rules for the agent are created based on the results of that analysis; in others, the recorded behaviour is used directly as a resource for decision-making, using data-driven techniques.", "labels": [], "entities": []}, {"text": "In this paper, we implement both of these methods for selecting the conversational facial displays of an animated talking head and compare them in two user evaluations.", "labels": [], "entities": []}, {"text": "In the first study, participants were asked for subjective preferences: they tended to prefer the output of the data-driven strategy, but this trend was not statistically significant.", "labels": [], "entities": []}, {"text": "In the second study, the data-driven facial displays affected the ability of users to perceive user-model tailoring in synthesised speech, while the rule-based displays did not have any effect.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is no longer any question that the production of language and its accompanying non-verbal behaviour are tightly linked (e.g.,).", "labels": [], "entities": []}, {"text": "The communicative functions of body language listed by include conversation initiation and termination, turn-taking and interruption, content elaboration and emphasis, and feedback and error correction; non-verbal behaviours that can achieve these functions include gaze modification, facial expressions, hand gestures, and posture shifts, among others.", "labels": [], "entities": [{"text": "conversation initiation and termination", "start_pos": 63, "end_pos": 102, "type": "TASK", "confidence": 0.8325778841972351}, {"text": "content elaboration and emphasis", "start_pos": 134, "end_pos": 166, "type": "TASK", "confidence": 0.79111098498106}, {"text": "gaze modification", "start_pos": 266, "end_pos": 283, "type": "TASK", "confidence": 0.7138475924730301}]}, {"text": "When choosing non-verbal behaviours to accompany the speech of an embodied conversational agent (ECA), it is necessary to translate general findings from observing human behaviour into concrete selection strategies.", "labels": [], "entities": []}, {"text": "There are two main implementation techniques that have been used for making this decision.", "labels": [], "entities": []}, {"text": "In some systems, recorded behaviours are analysed and rules are created by hand based on the analysis; in others, recorded human data is used directly in the decision process.", "labels": [], "entities": []}, {"text": "The former technique is similar to the classic role of corpora in naturallanguage generation described by, while the latter is more similar to the more recent data-driven techniques that have been adopted).", "labels": [], "entities": [{"text": "naturallanguage generation", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.7250342667102814}]}, {"text": "Researchers that have used rule-based techniques to create embodied-agent systems include:, who concentrated on generating appropriate affective facial displays based on descriptions of typical facial expressions of emotion;, who selected gestures and facial expressions to accompany text using heuristics derived from studies of typical North American non-verbal-displays; and, who generated typical certain and uncertain facial displays fora talking head in an informationretrieval system.", "labels": [], "entities": []}, {"text": "Researchers that used data-driven techniques include: , who captured the motions of an actor performing scripted output and then used that data to create performance specifications on the fly;, who selected posture shifts for an embodied agent based on recorded human behaviour, who annotated the gesturing behaviour of skilled public speakers and derived \"gesture profiles\" to use in the generation process.", "labels": [], "entities": []}, {"text": "Using rules derived from the data can produce displays that are easily identifiable and is straightforward to implement.", "labels": [], "entities": []}, {"text": "On the other hand, making direct use of the data can produce output that is more similar to actual human behaviour by incorporating naturalistic variation, although it generally requires a more complex selection algorithm.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the relative utility of the two implementation strategies fora particular decision: selecting the conversational facial displays of an animated talking head.", "labels": [], "entities": []}, {"text": "We use two methods for comparison: gathering users' subjective preferences, and measuring the impact of both selection strategies on users' ability to perceive user tailoring in speech.", "labels": [], "entities": []}, {"text": "In Section 2, we first describe how we recorded and annotated a corpus of facial displays in the domain of the target generation system.", "labels": [], "entities": []}, {"text": "Section 3 then presents the two strategies that were implemented to select facial displays based on this corpus: one using a simple rule derived from the most characteristic behaviours in the corpus, and one that made a weighted choice among all of the options found in the corpus for each context.", "labels": [], "entities": []}, {"text": "The next sections describe two user studies comparing these strategies: in Section 4, we compare users' subjective preferences, while in Section 5 we measure the impact of each strategy on user's ability to select spoken descriptions correctly tailored to a given set of user preferences.", "labels": [], "entities": []}, {"text": "Finally, in Section 6, we discuss the results of these two studies, draw some conclusions, and outline potential future work.", "labels": [], "entities": []}, {"text": "2 Corpus collection and annotation The recording scripts for the corpus were created by the output planner of the COMIC multimodal dialogue system ) and consisted of a total of 444 sentences describing and comparing various tile-design options.", "labels": [], "entities": [{"text": "Corpus collection", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.5906480550765991}, {"text": "COMIC multimodal dialogue system", "start_pos": 114, "end_pos": 146, "type": "DATASET", "confidence": 0.7415819615125656}]}, {"text": "The surface form of each sentence was created by the OpenCCG surface realiser), using a grammar that spec-ified both the words and the intended prosody for the speech synthesiser.", "labels": [], "entities": [{"text": "OpenCCG surface realiser", "start_pos": 53, "end_pos": 77, "type": "DATASET", "confidence": 0.8364535570144653}]}, {"text": "We attached all of the relevant contextual, syntactic, and prosodic information to each node in the OpenCCG derivation tree, including the user-model evaluation of the object being described (positive, negative, or neutral), the predicted pitch accent, the clause of the sentence (first, second, or only), and whether the information being presented was new to the discourse.", "labels": [], "entities": [{"text": "OpenCCG derivation tree", "start_pos": 100, "end_pos": 123, "type": "DATASET", "confidence": 0.9114327430725098}]}, {"text": "The sentences in the script were presented one at a time to a speaker who was instructed to read each out loud as expressively as possible into a camera directed at his face.", "labels": [], "entities": []}, {"text": "The following facial displays were then annotated on the recordings: eyebrow motions (up or down), eye squinting, and rigid head motion on all three axes (nodding, leaning, and turning).", "labels": [], "entities": []}, {"text": "Each of these displays was attached to the node or nodes in the OpenCCG derivation tree that exactly covered the span of words temporally associated with the display.", "labels": [], "entities": [{"text": "OpenCCG derivation tree", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.9092279473940531}]}, {"text": "Two coders separately processed the sentences in the corpus.", "labels": [], "entities": []}, {"text": "Using aversion of the \u03b2 weighted agreement measure proposed by-which allows fora range of agreement levels-the agreement on the sentences processed by both coders was 0.561.", "labels": [], "entities": [{"text": "agreement", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.8110038042068481}]}, {"text": "When the distribution of facial displays in the corpus was analysed, it was found that the single biggest influence on the speaker's behaviour was the user-model evaluation of the features being described.", "labels": [], "entities": []}, {"text": "When he described features of the design that had positive user-model evaluations, he was more likely to turn to the right and to raise his eyebrows); on the other hand, on features with negative user-model evaluations, he was more likely to lean to the left, lower his eyebrows, and squint his eyes).", "labels": [], "entities": []}, {"text": "The overall most frequent display in all contexts was a downward nod on its own.", "labels": [], "entities": []}, {"text": "Other factors that had a significant effect on the facial displays included the predicted pitch accent, the clause of the sentence (first or second), and the number of words spanned by anode.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}