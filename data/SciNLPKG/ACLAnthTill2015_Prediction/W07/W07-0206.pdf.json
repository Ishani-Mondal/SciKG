{"title": [{"text": "TextGraphs-2: Graph-Based Algorithms for Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "We extend the Blum and Chawla (2001) graph min-cut algorithm to structured problems.", "labels": [], "entities": []}, {"text": "This extension can alternatively be viewed as a joint inference method over a set of training and test instances where parts of the instances interact through a pre-specified associative network.", "labels": [], "entities": []}, {"text": "The method has has an efficient approximation through a linear-programming relaxation.", "labels": [], "entities": []}, {"text": "On small training data sets, the method achieves up to 34.8% relative error reduction.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 61, "end_pos": 85, "type": "METRIC", "confidence": 0.7726815342903137}]}], "introductionContent": [{"text": "We describe a method for transductive classification in structured problems.", "labels": [], "entities": [{"text": "transductive classification", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.8714547455310822}]}, {"text": "Our method extends the algorithm for transductive classification.", "labels": [], "entities": [{"text": "transductive classification", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.8584380745887756}]}, {"text": "In that algorithm, each training and test instance is represented by a vertex in a graph.", "labels": [], "entities": []}, {"text": "The algorithm finds the min-cut that separates the positively and negatively labeled instances.", "labels": [], "entities": []}, {"text": "We give a linear program that implements an approximation of this algorithm and extend it in several ways.", "labels": [], "entities": []}, {"text": "First, our formulation can be used in cases where there are more than two labels.", "labels": [], "entities": []}, {"text": "Second, we can use the output of a classifier to provide a prior preference of each instance fora particular label.", "labels": [], "entities": []}, {"text": "This lets us trade off the strengths of the mincut algorithm against those of a standard classifier.", "labels": [], "entities": []}, {"text": "Finally, we extend the algorithm further to deal with structured output spaces, by encoding parts of instances as well as constraints that ensure a consistent labeling of an entire instance.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 explains what we mean by transductive classification and by structured problems.", "labels": [], "entities": [{"text": "transductive classification", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.8333443105220795}]}, {"text": "Section 3 reviews the algorithm, how we formulate it as a linear program and our proposed extensions.", "labels": [], "entities": []}, {"text": "Section 4 relates our proposal to previous work.", "labels": [], "entities": []}, {"text": "Section 5 describes our experimental results on real and synthetic data and Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments using our approach on three different datasets using a conditional random field as the base classifier.", "labels": [], "entities": []}, {"text": "Unless otherwise noted this was regularized using a zeromean Gaussian prior with a variance of 1.", "labels": [], "entities": []}, {"text": "The first dataset is the pitch-accent prediction dataset used in semi-supervised learning by.", "labels": [], "entities": [{"text": "pitch-accent prediction", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.6601001620292664}]}, {"text": "There are 31 real and binary features (all are encoded as real values) and only two labels.", "labels": [], "entities": []}, {"text": "Instances correspond to an utterance and each token corresponds to a word.", "labels": [], "entities": []}, {"text": "perform experiments on 4 and 40 training instances using at most 200 unlabeled instances.", "labels": [], "entities": []}, {"text": "The second dataset is the reference part of the Cora information extraction dataset.", "labels": [], "entities": [{"text": "Cora information extraction", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.5812000135580698}]}, {"text": "1 This consists of 500 computer science research paper citations.", "labels": [], "entities": []}, {"text": "Each token in a citation is labeled as being part of the name of an author, part of the title, part of the date or one of several other labels that we combined into a single category (\"other\").", "labels": [], "entities": []}, {"text": "The third dataset is the chunking dataset from the CoNLL 2000) shared task restricted to noun phrases.", "labels": [], "entities": [{"text": "CoNLL 2000) shared task", "start_pos": 51, "end_pos": 74, "type": "DATASET", "confidence": 0.9289329648017883}]}, {"text": "The task for this dataset is, given the words in a sentence as well as automatically assigned parts of speech for these words, label each word with B-NP if it is the first word in abase noun phrase, I-NP if it is part of abase noun phrase but not the first word and O if it is not part of a noun phrase.", "labels": [], "entities": [{"text": "O", "start_pos": 266, "end_pos": 267, "type": "METRIC", "confidence": 0.996612012386322}]}, {"text": "For all experiments, we let each word be a token and consider parts consisting of two consecutive tokens.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the pitch accent prediction  task. The methods we compare are as follows.  CRF is supervised CRF training. MinCut is our  method with a CRF as base classifier. STR and  SVM are the semi-supervised results reported in  Altun et al. (2006). The experiments are 4 la- beled and 80 unlabeled, 40 labeled and 80 unla- beled and 40 labeled and 200 unlabeled respec- tively.", "labels": [], "entities": [{"text": "pitch accent prediction  task", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.7859649509191513}]}, {"text": " Table 2: Accuracy on the Cora-IE dataset as  a percentage of tokens correctly classified at dif- ferent settings for the CRF variance. Results for  training on 40 instances and testing on 80. In  all cases the scores are the mean of 10 random  selections of 120 instances from the set of 500  available.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9956560134887695}, {"text": "Cora-IE dataset", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.9395214319229126}]}]}