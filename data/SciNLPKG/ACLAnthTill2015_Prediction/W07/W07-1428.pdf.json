{"title": [{"text": "A Discourse Commitment-Based Framework for Recognizing Textual Entailment", "labels": [], "entities": [{"text": "Recognizing Textual Entailment", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.8573768536249796}]}], "abstractContent": [{"text": "In this paper, we introduce anew framework for recognizing textual entailment which depends on extraction of the set of publicly-held beliefs-known as discourse commitments that can be ascribed to the author of a text or a hypothesis.", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6808177133401235}]}, {"text": "Once a set of commitments have been extracted from a t-h pair, the task of recognizing textual entailment is reduced to the identification of the commitments from at which support the inference of the h.", "labels": [], "entities": []}, {"text": "Promising results were achieved: our system correctly identified more than 80% of examples from the RTE-3 Test Set correctly, without the need for additional sources of training data or other web-based resources.", "labels": [], "entities": [{"text": "RTE-3 Test Set", "start_pos": 100, "end_pos": 114, "type": "DATASET", "confidence": 0.927160640557607}]}], "introductionContent": [{"text": "While relatively \"shallow\" approaches have shown much promise in RTE for entailment pairs where the text and hypothesis remain short, we expect that performance of these types of systems will ultimately degrade as longer and more syntactically complex entailment pairs are considered.", "labels": [], "entities": [{"text": "RTE", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9786468148231506}]}, {"text": "In order to remain effective as texts get longer, we believe that RTE systems will need to employ techniques that will enable them to enumerate the set of propositions which are inferable -whether asserted, presupposed, or conventionally or conversationally implicated -from a text-hypothesis pair.", "labels": [], "entities": [{"text": "RTE", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9613817930221558}]}, {"text": "In this paper, we introduce anew framework for recognizing textual entailment which depends on extraction of the set of publicly-held beliefs -or discourse commitments -that can be ascribed to the author of a text or a hypothesis.", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6721444527308146}]}, {"text": "We show that once a set of discourse commitments have been extracted from a text-hypothesis pair, the task of recognizing textual entailment can be reduced to the identification of the one (or more) commitments from the text which are most likely to support the inference of each commitment extracted from the hypothesis.", "labels": [], "entities": []}, {"text": "More formally, we assume that given a commitment set {c t } consisting of the set of discourse commitments inferable from a text t and a hypothesis h, we define the task of RTE as a search for the commitment c \u2208 {c t } which maximizes the likelihood that c textually entails h.", "labels": [], "entities": [{"text": "RTE", "start_pos": 173, "end_pos": 176, "type": "TASK", "confidence": 0.9368518590927124}]}, {"text": "The rest of this paper is organized in the following way.", "labels": [], "entities": []}, {"text": "Section 2 provides a sketch of the system we used in the PASCAL RTE-3 Challenge.", "labels": [], "entities": [{"text": "PASCAL RTE-3 Challenge", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.48410119613011676}]}, {"text": "Sections 3, 4, and 5 describe details of our systems for Commitment Extraction, Commitment Se- lection, and Entailment Classification, respectively.", "labels": [], "entities": [{"text": "Commitment Extraction", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.7185488939285278}, {"text": "Commitment Se- lection", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.5857998505234718}, {"text": "Entailment Classification", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.8846128582954407}]}, {"text": "Finally, Section 6 discusses results from this year's evaluation, and Section 7 provides our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We submitted one ranked run in our official submission for this year's evaluation.", "labels": [], "entities": []}, {"text": "Official results from the RTE-3 Test Set are presented in  Accuracy and average precision varied significantly (p < 0.05) across each of the four tasks.", "labels": [], "entities": [{"text": "RTE-3 Test Set", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.8690212766329447}, {"text": "Accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9996688365936279}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9222564697265625}]}, {"text": "Performance (in terms of accuracy and average precision) was highest on the QA set (90.0% precision) and lowest on the IE set (67.5%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.999546468257904}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9253609776496887}, {"text": "QA set", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.8970663845539093}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9950271248817444}, {"text": "IE set", "start_pos": 119, "end_pos": 125, "type": "DATASET", "confidence": 0.8934825658798218}]}, {"text": "The length of the text (either short or long) did not significantly impact performance, however; in fact, ALIGNMENT FEATURES: Derived from the results of the alignment of each pair of commitments performed during Commitment Selection.", "labels": [], "entities": [{"text": "ALIGNMENT", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9954598546028137}, {"text": "FEATURES", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.8480511903762817}]}, {"text": "\u22c41\u22c4 LONGEST COMMON STRING: This feature represents the longest contiguous string common to both texts.", "labels": [], "entities": [{"text": "LONGEST COMMON STRING", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.7372322082519531}]}, {"text": "\u22c42\u22c4 UNALIGNED CHUNK: This feature represents the number of chunks in one text that are not aligned with a chunk from the other \u22c43\u22c4 LEXICAL ENTAILMENT PROBABILITY: Defined as in).", "labels": [], "entities": [{"text": "UNALIGNED CHUNK", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.8854997158050537}, {"text": "LEXICAL ENTAILMENT PROBABILITY", "start_pos": 131, "end_pos": 161, "type": "METRIC", "confidence": 0.8658372561136881}]}, {"text": "DEPENDENCY FEATURES: Computed from the semantic dependencies identified by the PropBank-and NomBank-based semantic parsers.", "labels": [], "entities": [{"text": "DEPENDENCY", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.924604058265686}, {"text": "FEATURES", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.8084686994552612}, {"text": "PropBank-and NomBank-based semantic parsers", "start_pos": 79, "end_pos": 122, "type": "DATASET", "confidence": 0.859810471534729}]}, {"text": "\u22c41\u22c4 ENTITY-ARG MATCH: This is a boolean feature which fires when aligned entities were assigned the same argument role label.", "labels": [], "entities": [{"text": "ENTITY-ARG MATCH", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.8395310640335083}]}, {"text": "\u22c42\u22c4 ENTITY-NEAR-ARG MATCH: This feature is collapsing the arguments Arg1 and Arg2 (as well as the ArgM subtypes) into single categories for the purpose of counting matches.", "labels": [], "entities": [{"text": "ENTITY-NEAR-ARG MATCH", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.8454397618770599}]}, {"text": "\u22c43\u22c4 PREDICATE-ARG MATCH: This boolean feature is flagged when at least two aligned arguments have the same role.", "labels": [], "entities": [{"text": "PREDICATE-ARG MATCH", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.5420048832893372}]}, {"text": "In experiments conducted following the RTE-3 submission deadline, we found that using a system for recognizing textual contradiction to validate judgments output by the entailment classifier had only a slight positive impact on the overall performance of our system.", "labels": [], "entities": [{"text": "RTE-3 submission deadline", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.4892454445362091}]}, {"text": "compares performance of our RTE system when four different configurations of our system for recognizing textual contradiction was used.", "labels": [], "entities": []}, {"text": "When used with its default threshold (\u03bb = 0.85), we discovered that using textual contradiction enabled us to identify 17 additional examples (2.13% overall) that were not available when using our sys-  tem for RTE alone.", "labels": [], "entities": []}, {"text": "When we hand-tuned \u03bb to maximize performance on the RTE-3 Test Set, we found that accuracy could be increased by 3.0% over the baseline (to 81.25% overall).", "labels": [], "entities": [{"text": "RTE-3 Test Set", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.9521510799725851}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9995875954627991}]}, {"text": "Despite its limited effectiveness on this year's Test Set, we believe that net positive effect of using textual contradiction to validate textual entailment judgments suggests that this technique has merit and should be explored in future evaluations.", "labels": [], "entities": [{"text": "year's Test Set", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.8016743063926697}]}, {"text": "Ina second post hoc experiment, we sought to quantify the impact that additional sources of training data could have on the performance of our RTE system.", "labels": [], "entities": []}, {"text": "Although our official submission was only trained on the 800 t-h pairs found in the RTE-3 Development Set, we followed ( ) in using a large, hand-crafted training set of 100,000 text-hypothesis pairs in order to train our entailment classifier.", "labels": [], "entities": [{"text": "RTE-3 Development Set", "start_pos": 84, "end_pos": 105, "type": "DATASET", "confidence": 0.9408603310585022}]}, {"text": "Even though previous work has shown that RTE accuracy increased with the size of the training set, our experiments showed no correlation between the size of the training corpus and the overall accuracy of the system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.90221107006073}, {"text": "accuracy", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.9975491166114807}]}, {"text": "summarizes the performance of our RTE system when trained on increasing amounts of training data.", "labels": [], "entities": [{"text": "RTE", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.8308836817741394}]}, {"text": "While increasing the training data to approximately 10,000 training examples did positively impact performance, we discovered that using a training corpus of a size equal to ( )'s had nearly no measurable impact on the observed performance of our system.", "labels": [], "entities": []}, {"text": "While large training corpora (like ( )'s or the one compiled for this work) may provide an important source of lexico-semantic information that can be leveraged in performing an entailment classification, these results suggest that our approach based on commitment extraction may nullify the gains in performance seen by these approaches.", "labels": [], "entities": [{"text": "commitment extraction", "start_pos": 254, "end_pos": 275, "type": "TASK", "confidence": 0.7231610119342804}]}], "tableCaptions": [{"text": " Table 1: Alignment and Selection Performance", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9577561616897583}]}, {"text": " Table 2: Entailment Classifier Performance.  A partial list of the features used in the Entailment  Classifier used in our official submission is provided  in", "labels": [], "entities": []}, {"text": " Table 3: Official RTE-3 Results.", "labels": [], "entities": [{"text": "Official", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.5010257363319397}, {"text": "RTE-3", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.3627653121948242}]}, {"text": " Table 4: Short vs. Long Pairs.", "labels": [], "entities": []}, {"text": " Table 5: Impact of Validation.", "labels": [], "entities": [{"text": "Impact of Validation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.48835238814353943}]}, {"text": " Table 6: Impact of Training Corpus Size.", "labels": [], "entities": [{"text": "Training Corpus Size", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.5883131225903829}]}]}