{"title": [], "abstractContent": [{"text": "This paper presents the first use of a computational model of natural logic-a system of logical inference which operates over natural language-for textual inference.", "labels": [], "entities": [{"text": "textual inference", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.711476743221283}]}, {"text": "Most current approaches to the PAS-CAL RTE textual inference task achieve ro-bustness by sacrificing semantic precision; while broadly effective, they are easily confounded by ubiquitous inferences involving monotonicity.", "labels": [], "entities": [{"text": "PAS-CAL RTE textual inference task", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.8577033519744873}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.8534401059150696}]}, {"text": "At the other extreme, systems which rely on first-order logic and theorem proving are precise, but excessively brittle.", "labels": [], "entities": [{"text": "theorem proving", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7468641996383667}]}, {"text": "This work aims at a middle way.", "labels": [], "entities": []}, {"text": "Our system finds a low-cost edit sequence which transforms the premise into the hypothesis; learns to classify entailment relations across atomic edits; and composes atomic entailments into a top-level entailment judgment.", "labels": [], "entities": []}, {"text": "We provide the first reported results for any system on the FraCaS test suite.", "labels": [], "entities": [{"text": "FraCaS test suite", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.9870505332946777}]}, {"text": "We also evaluate on RTE3 data, and show that hybridizing an existing RTE system with our natural logic system yields significant performance gains.", "labels": [], "entities": [{"text": "RTE3 data", "start_pos": 20, "end_pos": 29, "type": "DATASET", "confidence": 0.8185012638568878}]}], "introductionContent": [{"text": "The last five years have seen a surge of interest in the problem of textual inference, that is, automatically determining whether a natural-language hypothesis can be inferred from a given premise.", "labels": [], "entities": [{"text": "textual inference", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7242523580789566}]}, {"text": "A broad spectrum of approaches have been explored, ranging from shallow-but-robust to deep-but-brittle.", "labels": [], "entities": []}, {"text": "Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de), pattern-based relation extraction (), or approximate matching of predicate-argument structure).", "labels": [], "entities": [{"text": "pattern-based relation extraction", "start_pos": 167, "end_pos": 200, "type": "TASK", "confidence": 0.6357002456982931}]}, {"text": "Such methods, while robust and broadly effective, are imprecise, and are easily confounded by ubiquituous inferences involving monotonicity, particularly in negative polarity contexts, as in: P: No case of indigenously acquired rabies infection has been confirmed in the past 2 years.", "labels": [], "entities": []}, {"text": "H: No rabies cases have been confirmed.", "labels": [], "entities": []}, {"text": "Because it drops important qualifiers in a negative context, the hypothesis does not follow; yet both the lexical content and the predicate-argument structure of the hypothesis closely match the premise.", "labels": [], "entities": []}, {"text": "At the other extreme, textual inference can be approached as deduction, building on work informal computational semantics to translate sentences into first-order logic (FOL), and then applying a theorem prover or a model builder).", "labels": [], "entities": []}, {"text": "However, such approaches tend to founder on the difficulty of accurately translating natural language in FOL-tricky issues include idioms, intensionality and propositional attitudes, modalities, temporal and causal relations, certain quantifiers, and soon.", "labels": [], "entities": []}, {"text": "FOL-based systems that have attained high precision () have done so at the cost of very poor recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9986360669136047}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9988365769386292}]}, {"text": "In this work, we explore a different point on the spectrum, by developing a computational model of natural logic, that is, a logic whose vehicle of inference is natural language.", "labels": [], "entities": []}, {"text": "Natural logic eschews logical notation and model theory.", "labels": [], "entities": []}, {"text": "Its proofs proceed by incremental edits to expressions of natural language, and its inference rules specify conditions under which semantic expansions or contractions preserve truth.", "labels": [], "entities": []}, {"text": "It thus permits us to do precise reasoning about monotonicity, while sidestepping the difficulties of translating sentences into FOL.", "labels": [], "entities": []}, {"text": "It should be emphasized that there are many important kinds of inference which are not addressed by a natural logic system, including temporal reasoning, causal reasoning (Khan sold nuclear plans \u21d2 Khan possessed nuclear plans), paraphrase (McEwan flew to Rome \u21d2 McEwan took a flight to Rome), relation extraction (Bill Gates and his wife, Melinda... \u21d2 Melinda Gates is married to Bill Gates), etc.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 294, "end_pos": 313, "type": "TASK", "confidence": 0.866706132888794}]}, {"text": "Moreover, a natural logic system will struggle with inferences requiring modelbuilding or deep proof search, which are more suitable for formal deduction systems.", "labels": [], "entities": []}, {"text": "However, the applicability of natural logic is broader than it might at first appear, and a natural logic system can be designed to integrate with other kinds of reasoners.", "labels": [], "entities": []}], "datasetContent": [{"text": "The FraCaS test suite) was developed as part of a collaborative research effort in computational semantics.", "labels": [], "entities": [{"text": "FraCaS test suite", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9409567912419637}]}, {"text": "It contains 346 inference problems reminiscent of a textbook on formal semantics.", "labels": [], "entities": []}, {"text": "In the authors' view, \"inferencing tasks [are] the best way of testing an NLP system's semantic capacity.\"", "labels": [], "entities": []}, {"text": "Yet, to our knowledge, this work is the first to present a quantitative system evaluation using FraCaS.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.9725546836853027}]}, {"text": "The problems are divided into nine sections, each focused on a category of semantic phenomena, such as quantifiers or anaphora (see).", "labels": [], "entities": []}, {"text": "Each problem consists of one or more premise sentences, followed by a one-sentence question.", "labels": [], "entities": []}, {"text": "For this project, the questions were converted into declarative hypotheses.", "labels": [], "entities": []}, {"text": "Each problem also has an answer, which (usually) takes one of three values: yes (the hypothesis can be inferred from the premise(s)), no (the negation of the hypothesis can be inferred), or unk (neither the hypothesis nor its negation can be inferred).", "labels": [], "entities": []}, {"text": "Some examples are shown in  Not all of the 346 problems were used in this work.", "labels": [], "entities": []}, {"text": "First, 12 of the problems were excluded because they are degenerate, lacking either a hypothesis or a well-defined answer.", "labels": [], "entities": []}, {"text": "Second, an additional 151 problems (about 45% of the total) were excluded because they involve multiple premises.", "labels": [], "entities": []}, {"text": "While many of the multiple-premise problems should be feasible for NatLog in the future, such inferences require search, and for now we have chosen to sidestep this complexity.", "labels": [], "entities": []}, {"text": "Finally, it should be noted that several sections of the test suite involve semantic phenomena, such as ellipsis, which the NatLog system makes no attempt to model.", "labels": [], "entities": []}, {"text": "While we report results for these sections, we do not expect performance to be good, and in development we have concentrated on the sections where we expect NatLog to have relevant expertise.", "labels": [], "entities": []}, {"text": "In table 2, results for these sections are aggregated under the label \"applicable sections\".", "labels": [], "entities": []}, {"text": "Results are shown in table 2.", "labels": [], "entities": []}, {"text": "On the \"applicable\" sections, performance is good.", "labels": [], "entities": []}, {"text": "(Not suprisingly, we make little headway with, e.g., ellipsis.)", "labels": [], "entities": []}, {"text": "Of course, this does not constitute a proper evaluation on unseen test data-but on the other hand, the system was never trained on the FraCaS problems, and has had no opportunity to learn biases implicit in the data.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 135, "end_pos": 141, "type": "DATASET", "confidence": 0.9008247256278992}]}, {"text": "Our main goal in testing on FraCaS is to evaluate the representational and inferential adequacy of our model of natural logic, and from that perspective, the strong performance in quantifiers, \u00a7 Clients at the demonstration were all impressed by the system's performance.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.9561216831207275}]}, {"text": "Smith was a client at the demonstration.", "labels": [], "entities": []}, {"text": "Smith was impressed by the system's performance.", "labels": [], "entities": []}, {"text": "yes 9 335 Smith believed that ITEL had won the contract in 1992.", "labels": [], "entities": [{"text": "ITEL", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9490078687667847}]}, {"text": "ITEL won the contract in 1992.", "labels": [], "entities": [{"text": "ITEL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9648804664611816}]}, {"text": "unk  adjectives, and comparatives is satisfying.", "labels": [], "entities": []}, {"text": "The confusion matrix shown in table 4 is instructive.", "labels": [], "entities": []}, {"text": "By far the largest category of confusions comprise problems where we guess unk when the correct answer is yes.", "labels": [], "entities": []}, {"text": "This reflects both the bias toward yes in the FraCaS data, and the system's tendency to predict unk (entailment relation #) when confused: given the composition rules for entailment relations, the system can predict yes only if all atomic-level predictions are either or =.", "labels": [], "entities": [{"text": "FraCaS data", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.9820145964622498}]}, {"text": "On the other hand, there area number of problems where we predict yes mistakenly.", "labels": [], "entities": []}, {"text": "Several of these errors arise in a series of problems in \u00a75 which concern operator adjectives such as former.", "labels": [], "entities": []}, {"text": "The entailment model wrongly assumes that such modifiers, like any others, can safely be deleted in upward-monotone contexts, but in fact former student student.", "labels": [], "entities": []}, {"text": "If the feature set used by the entailment model were extended to represent occurrences of operator adjectives, and if appropriate examples were included in the training data, our accuracy in \u00a75-and the average accuracy for the \"applicable\" sections-could easily be boosted over 80%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9996336698532104}, {"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.9986030459403992}]}, {"text": "Textual inference problems from the PASCAL RTE Challenge () differ from FraCaS problems in several important ways.", "labels": [], "entities": [{"text": "PASCAL RTE Challenge", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.5880416631698608}, {"text": "FraCaS", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.8721453547477722}]}, {"text": "Instead of textbook examples of semantic phenomena, RTE problems are more naturalseeming, with premises collected \"in the wild\" from newswire text.", "labels": [], "entities": [{"text": "RTE", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.98296058177948}]}, {"text": "The premises are much longer, averaging 35 words (vs. 11 words for FraCaS).", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.9167331457138062}]}, {"text": "Also, the RTE task aims at a binary classification: the RTE no answer combines the no and unk answers in FraCaS.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.7344513237476349}, {"text": "FraCaS", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.9826022386550903}]}, {"text": "Due to the character of RTE problems, we do not expect NatLog to be a good general-purpose solution to solving RTE problems.", "labels": [], "entities": [{"text": "RTE", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.890838623046875}]}, {"text": "First, most RTE problems depend on forms of inference, such as paraphrase, temporal reasoning, or relation extraction, which NatLog is not designed to address.", "labels": [], "entities": [{"text": "RTE", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9906195402145386}, {"text": "relation extraction", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7976710200309753}]}, {"text": "Second, inmost RTE problems, the edit distance between premise and hypothesis is relatively large.", "labels": [], "entities": [{"text": "RTE", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9519720077514648}]}, {"text": "More atomic edits means a greater chance that prediction errors made by the atomic entailment model will propagate, via entailment composition, to the system's final output.", "labels": [], "entities": []}, {"text": "Rather, in applying NatLog to RTE, we hope to make reliable predictions on a subset of RTE problems, trading recall for precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.998656153678894}, {"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9965921640396118}]}, {"text": "If we succeed, then we maybe able to hybridize with a broad-coverage RTE system to obtain better results than either system individually-the same strategy that was adopted by) for their FOL-based system.", "labels": [], "entities": []}, {"text": "For this purpose, we have chosen to use the Stanford RTE system described in).", "labels": [], "entities": [{"text": "Stanford RTE system", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.8948202331860861}]}, {"text": "In applying NatLog to RTE problems, we use alignments from the Stanford system as input to our entailment model.", "labels": [], "entities": [{"text": "RTE", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9496289491653442}, {"text": "Stanford system", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.8523707091808319}]}, {"text": "A Stanford alignment is a map from hypothesis words to premise words.", "labels": [], "entities": []}, {"text": "When we translate such alignments into the NatLog representation described in section 3, each pair of aligned words generates a substitution edit (or, if the words are identical, an advance edit).", "labels": [], "entities": []}, {"text": "Unaligned premise words yield deletion edits, while unaligned hypothesis words yield insertion edits.", "labels": [], "entities": []}, {"text": "Where possible, contiguous sequences of word-level edits are then collected into equivalent span edits.", "labels": [], "entities": []}, {"text": "While the result of this translation method cannot be interpreted as a conventional edit script (there is no well-defined or-ID Premise(s) Hypothesis Answer 518 The French railway company SNCF is cooperating in the project.", "labels": [], "entities": []}, {"text": "The French railway company is called SNCF.", "labels": [], "entities": []}, {"text": "yes 601 NUCOR has pioneered a giant mini-mill in which steel is poured into continuous casting machines.", "labels": [], "entities": [{"text": "yes 601 NUCOR", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8802990317344666}]}, {"text": "Nucor has pioneered the first mini-mill.", "labels": [], "entities": [{"text": "Nucor", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9283096790313721}]}, {"text": "no  dering of edits, and multiple edits can operate on the same input spans), we find that this poses no great impediment to subsequent processing by the entailment model.", "labels": [], "entities": []}, {"text": "shows the performance of the NatLog system on RTE3 data.", "labels": [], "entities": [{"text": "NatLog", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9091617465019226}, {"text": "RTE3 data", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.9610770642757416}]}, {"text": "Relative to the Stanford RTE system, NatLog achieves high precision on its yes predictions-about 76% on the development set, and 68% on the test set-suggesting that hybridizing maybe effective.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9989838004112244}]}, {"text": "For comparison, the FOL-based system reported in) attained a similarly high precision of 76% on RTE2 problems, but was able to make a positive prediction in only about 4% of cases.", "labels": [], "entities": [{"text": "FOL-based", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.7173822522163391}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9985451698303223}]}, {"text": "NatLog makes positive predictions far more often-at a rate of 18% on the development set, and 24% on the test set.", "labels": [], "entities": [{"text": "NatLog", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9100607633590698}]}, {"text": "The Stanford RTE system makes yes/no predictions by thresholding a real-valued inference score.", "labels": [], "entities": []}, {"text": "To construct a hybrid system, we adjust the Stanford inference scores by +x or \u2212x, depending on whether NatLog predicts yes or no/unk.", "labels": [], "entities": []}, {"text": "We choose the value of x by optimizing development set accuracy, while adjusting the threshold to generate balanced predictions (that is, equal numbers of yes and no predictions).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9662114381790161}]}, {"text": "As an additional experiment, we fix x at this value and then adjust the threshold to optimize development set accuracy, resulting in an excess of yes predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9228319525718689}]}, {"text": "(Since this optimization is based solely on development data, its use on test data is fully legitimate.)", "labels": [], "entities": []}, {"text": "Results for these two cases are shown in table 6.", "labels": [], "entities": []}, {"text": "The parameters tuned on development data were found to yield good performance on test data.", "labels": [], "entities": []}, {"text": "The optimized hybrid system attained an absolute accuracy gain of 3.12% over the Stanford system, corresponding to an extra 25 problems answered correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9381530284881592}]}, {"text": "This result is statistically significant (p < 0.01, McNemar's test, 2-tailed).", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 52, "end_pos": 66, "type": "METRIC", "confidence": 0.6569368441899618}]}, {"text": "However, the gain cannot be attributed to NatLog's success in handling the kind of inferences about monotonicity which are the staple of natural logic.", "labels": [], "entities": []}, {"text": "Indeed, such inferences are quite rare in the RTE data.", "labels": [], "entities": [{"text": "RTE data", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.7902959287166595}]}, {"text": "Rather, NatLog seems to have gained primarily by being more precise.", "labels": [], "entities": [{"text": "NatLog", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.9023270606994629}]}, {"text": "In some cases, this precision works against it: NatLog answers no to problem 518 (table 5) because it cannot account for the insertion of called in the hypothesis.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9972136616706848}]}, {"text": "On the other hand, it correctly rejects the hypothesis in problem 601 because it cannot account for the insertion of first, whereas the less-precise Stanford system was happy to allow it. tonicity.", "labels": [], "entities": [{"text": "insertion", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9444695711135864}]}, {"text": "A small current of theoretical work has continued up to the present, for example ().", "labels": [], "entities": []}, {"text": "There has been surprisingly little work on building computational models of natural logic.", "labels": [], "entities": []}, {"text": "() describes a Prolog implementation fora small fragment of English, based on a categorial grammar parser.", "labels": [], "entities": []}, {"text": "In an unpublished draft,) describes a preliminary implementation in Haskell.", "labels": [], "entities": []}, {"text": "Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in.", "labels": [], "entities": []}, {"text": "To our knowledge, the FraCaS results reported here represent the first such evaluation.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 22, "end_pos": 28, "type": "DATASET", "confidence": 0.9038261771202087}]}, {"text": "describes applying a deductive system to some FraCaS inferences, but does not perform a complete evaluation or report quantitative results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Illustrative examples from the FraCaS test suite", "labels": [], "entities": [{"text": "FraCaS test", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9544917345046997}]}, {"text": " Table 5: Illustrative examples from the RTE3 test suite", "labels": [], "entities": [{"text": "RTE3 test", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.9088939726352692}]}]}