{"title": [{"text": "Pruning the Search Space of a Hand-Crafted Parsing System with a Probabilistic Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "The demand for deep linguistic analysis for huge volumes of data means that it is increasingly important that the time taken to parse such data is minimized.", "labels": [], "entities": []}, {"text": "In the XLE parsing model which is a hand-crafted, unification-based parsing system, most of the time is spent on unification, searching for valid f-structures (dependency attribute-value matrices) within the space of the many valid c-structures (phrase structure trees).", "labels": [], "entities": [{"text": "XLE parsing", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.6093853414058685}]}, {"text": "We carried out an experiment to determine whether pruning the search space at an earlier stage of the parsing process results in an improvement in the overall time taken to parse, while maintaining the quality of the f-structures produced.", "labels": [], "entities": [{"text": "parsing", "start_pos": 102, "end_pos": 109, "type": "TASK", "confidence": 0.9777550101280212}, {"text": "parse", "start_pos": 173, "end_pos": 178, "type": "TASK", "confidence": 0.9636632800102234}]}, {"text": "We retrained a state-of-the-art probabilistic parser and used it to pre-bracket input to the XLE, constraining the valid c-structure space for each sentence.", "labels": [], "entities": []}, {"text": "We evaluated against the PARC 700 Dependency Bank and show that it is possible to decrease the time taken to parse by \u223c18% while maintaining accuracy.", "labels": [], "entities": [{"text": "PARC 700 Dependency Bank", "start_pos": 25, "end_pos": 49, "type": "DATASET", "confidence": 0.9667927324771881}, {"text": "parse", "start_pos": 109, "end_pos": 114, "type": "TASK", "confidence": 0.9674323797225952}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9987605810165405}]}], "introductionContent": [{"text": "When deep linguistic analysis of massive data is required (e.g. processing Wikipedia), it is crucial that the parsing time be minimized.", "labels": [], "entities": [{"text": "parsing", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.9517714381217957}]}, {"text": "The XLE English parsing system is a large-scale, hand-crafted, deep, unification-based system that processes raw text and produces both constituent-structures (phrase structure trees) and feature-structures (dependency attribute-value matrices).", "labels": [], "entities": [{"text": "XLE English parsing", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.5752642949422201}]}, {"text": "A typical breakdown of parsing time of XLE components is Morphology (1.6%), Chart (5.8%) and Unifier (92.6%).", "labels": [], "entities": [{"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9722703695297241}, {"text": "Morphology", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.7044934630393982}, {"text": "Chart", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.8738972544670105}]}, {"text": "The unification process is the bottleneck in the XLE parsing system.", "labels": [], "entities": [{"text": "unification", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9660598635673523}, {"text": "XLE parsing", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.7334904968738556}]}, {"text": "The grammar generates many valid c-structure trees fora particular sentence: the Unifier then processes all of these trees (as packed structures), and a log-linear disambiguation module can choose the most probable f-structure from the resulting valid f-structures.", "labels": [], "entities": []}, {"text": "For example, the sentence \"Growth is slower.\" has 84 valid c-structure trees according to the current English grammar; 1 however once the Unifier has processed all of these trees (in a packed form), only one c-structure and f-structure pair is valid (see).", "labels": [], "entities": []}, {"text": "In this instance, the log-linear disambiguation does not need to choose the most probable result.", "labels": [], "entities": []}, {"text": "The research question we pose is whether the search space can be pruned earlier before unification takes place.", "labels": [], "entities": [{"text": "unification", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.9623416662216187}]}, {"text": "Bangalore and Joshi (1999), and show that by using a super tagger before (CCG and HPSG) parsing, the space required for discriminitive training is drastically reduced.", "labels": [], "entities": [{"text": "HPSG) parsing", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.5928566058476766}]}, {"text": "Supertagging is not widely used within the LFG framework, although there has been some work on using hypertags).", "labels": [], "entities": [{"text": "LFG framework", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.9049067497253418}]}, {"text": "propose a method for faster HPSG parsing while maintaining accuracy by only using the probabilities of lexical entry selections (i.e. the supertags) in their discriminitive model.", "labels": [], "entities": [{"text": "HPSG parsing", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.7093537151813507}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9990555644035339}]}, {"text": "In the work presented here, we con-centrate on reducing the number of c-structure trees that the Unifier has to process, ideally to one tree.", "labels": [], "entities": []}, {"text": "The hope was that this would speedup the parsing process, but how would it affect the quality of the fstructures?", "labels": [], "entities": [{"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.992000937461853}]}, {"text": "This is similar to the approach taken by who do not use a hand-crafted complete unification system (rather an automatically acquired probabilistic approximation).", "labels": [], "entities": []}, {"text": "They parse raw text into LFG f-structures by first parsing with a probabilistic CFG parser to choose the most probable c-structure.", "labels": [], "entities": []}, {"text": "This is then passed to an automatic f-structure annotation algorithm which deterministically generates one f-structure for that tree.", "labels": [], "entities": []}, {"text": "The most compact way of doing this would be to integrate a statistical component to the parser that could rank the c-structure trees and only pass the most likely forward to the unification process.", "labels": [], "entities": []}, {"text": "However, this would require a large rewrite of the system.", "labels": [], "entities": []}, {"text": "So, we first wanted to investigate a \"cheaper\" alternative to determine the viability of the pruning strategy; this is the experiment reported in this paper.", "labels": [], "entities": []}, {"text": "This is implemented by stipulating constituent boundaries in the input string, so that any c-structure that is incompatible with these constraints is invalid and will not be processed by the Unifier.", "labels": [], "entities": []}, {"text": "This was done to some extent in  to automatically generate training data for the log-linear disambiguation component of XLE.", "labels": [], "entities": []}, {"text": "Previous work obtained the constituent constraints (i.e. brackets) from the gold-standard trees in the Penn-II Treebank.", "labels": [], "entities": [{"text": "Penn-II Treebank", "start_pos": 103, "end_pos": 119, "type": "DATASET", "confidence": 0.9838117063045502}]}, {"text": "However, to parse novel text, gold-standard trees are unavailable.", "labels": [], "entities": []}, {"text": "We used a state-of-the-art probabilistic parser to provide the bracketing constraints to XLE.", "labels": [], "entities": []}, {"text": "These parsers are accurate (achieving accuracy of over 90% on Section 23 WSJ text), fast, and robust.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.999297022819519}, {"text": "Section 23 WSJ text", "start_pos": 62, "end_pos": 81, "type": "DATASET", "confidence": 0.8785078674554825}]}, {"text": "The idea is that pre-parsing of the input text by a fast and accurate parser can prune the c-structure search space, reducing the amount of work done by the Unifier, speedup parsing and maintain the high quality of the f-structures produced.", "labels": [], "entities": []}, {"text": "The structure of this paper is as follows: Section 2 introduces the XLE parsing system.", "labels": [], "entities": [{"text": "XLE parsing", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.7543107569217682}]}, {"text": "Section 3 describes a baseline experiment and based on the results suggests retraining the Bikel parser to improve results (Section 4).", "labels": [], "entities": []}, {"text": "Section 5 describes experiments on the development set, from which we evaluate the most successful system against the PARC 700 test", "labels": [], "entities": [{"text": "PARC 700", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.9290134906768799}]}], "datasetContent": [{"text": "We carried out a baseline experiment with two state-of-the-art parsers to establish what effect prebracketing the input to the XLE system has on the quality and number of the solutions produced.", "labels": [], "entities": []}, {"text": "We used the Bikel () multi-threaded, head-driven chartparsing engine developed at the University of Pennsylvania.", "labels": [], "entities": []}, {"text": "The second parser is that described in.", "labels": [], "entities": []}, {"text": "This parser uses a discriminative reranker that selects the most probable parse from the 50-best parses returned by a generative parser based on Charniak (2000).", "labels": [], "entities": []}, {"text": "We evaluated against the PARC 700 Dependency Bank () which provides goldstandard analyses for 700 sentences chosen at random from Section 23 of the Penn-II Treebank.", "labels": [], "entities": [{"text": "PARC 700 Dependency Bank", "start_pos": 25, "end_pos": 49, "type": "DATASET", "confidence": 0.8844323456287384}, {"text": "goldstandard", "start_pos": 68, "end_pos": 80, "type": "METRIC", "confidence": 0.9521687626838684}, {"text": "Penn-II Treebank", "start_pos": 148, "end_pos": 164, "type": "DATASET", "confidence": 0.8624869883060455}]}, {"text": "The Dependency Bank was bootstrapped by parsing the 700 sentences with the XLE English grammar, and then manually correcting the output.", "labels": [], "entities": [{"text": "Dependency Bank", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8361762464046478}, {"text": "XLE English grammar", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.7970330317815145}]}, {"text": "The data is divided into two sets, a 140-sentence development set and a test set of 560 sentences ().", "labels": [], "entities": []}, {"text": "We took the raw strings from the 140-sentence development set and parsed them with each of the state-of-the-art probabilistic parsers.", "labels": [], "entities": []}, {"text": "As an upper bound for the baseline experiment, we use the brackets in the original Penn-II treebank trees for the 140 development set.", "labels": [], "entities": [{"text": "Penn-II treebank trees", "start_pos": 83, "end_pos": 105, "type": "DATASET", "confidence": 0.9928960204124451}]}, {"text": "We then used the brackets from each parser output (or original treebank trees) to constrain the XLE parser.", "labels": [], "entities": []}, {"text": "If the input to the XLE parser is bracketed, the parser will only generate c-structures that respect these brackets (i.e., only c-structures with brackets that are compatible with the input brackets are considered during the unification stage).", "labels": [], "entities": []}, {"text": "gives an example of retained brackets from the parser output.", "labels": [], "entities": []}, {"text": "We do not retain brackets around PRN (parenthetical phrase) or NP nodes as their structure often differed too much from XLE analyses of the same phrases.", "labels": [], "entities": []}, {"text": "We passed pre-bracketed strings to the XLE and evaluated the output f-structures in terms of dependency triples against the 140-sentence subset of gives the upper bound results if we use the gold standard Penn treebank to bracket the input to XLE.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 205, "end_pos": 218, "type": "DATASET", "confidence": 0.9575645625591278}]}, {"text": "compares the XLE (fragment and non-fragment) grammar to the system where the input is pre-parsed by each parser.", "labels": [], "entities": []}, {"text": "XLE fragment grammars provide a back-off when parsing fails: the grammar is relaxed and the parser builds a fragment parse of the well-formed chunks.", "labels": [], "entities": []}, {"text": "We compare the parsers in terms of total number of parses (out of 140) and the f-score of the subset of sentences successfully parsed.", "labels": [], "entities": [{"text": "f-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9623386859893799}]}, {"text": "We also combine these scores to give an overall f-score, where the system scores 0 for each sentence it could not parse.", "labels": [], "entities": []}, {"text": "When testing for statistical significance between systems, we compare the overall f-score values.", "labels": [], "entities": []}, {"text": "Figures marked with an asterisk are not statistically significantly different at the 95% level.", "labels": [], "entities": []}, {"text": "The results show that using unlabeled brackets achieves reasonable f-scores with the non-fragment grammar.", "labels": [], "entities": []}, {"text": "Using the labeled bracketing from the output of both parsers causes XLE to always fail when parsing.", "labels": [], "entities": [{"text": "XLE", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9019585847854614}]}, {"text": "This is because the labels in the output of parsers trained on the Penn-II treebank differ considerably from the labels on c-structure trees pro-duced by XLE.", "labels": [], "entities": [{"text": "Penn-II treebank", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.9867287576198578}]}, {"text": "Interestingly, the f-scores for both the CJ-XLE and Bikel-XLE systems are very similar to the upper bounds.", "labels": [], "entities": [{"text": "CJ-XLE", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.9137181639671326}]}, {"text": "The gold standard upper bound is not as high as expected because the Penn trees used to produce the gold bracketed input are not always compatible with the XLE-style trees.", "labels": [], "entities": []}, {"text": "As a simple example, the tree in differs from the parse tree for the same sentence in the Penn Treebank).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.9946985244750977}]}, {"text": "The most obvious difference is the labels on the nodes.", "labels": [], "entities": []}, {"text": "However, even in this small example, there are structural differences, e.g. the position of the period.", "labels": [], "entities": []}, {"text": "In general, the larger the tree, the greater the difference in both labeling and structure between the Penn trees and the XLE-style trees.", "labels": [], "entities": [{"text": "Penn trees", "start_pos": 103, "end_pos": 113, "type": "DATASET", "confidence": 0.9379837214946747}]}, {"text": "Therefore, the next step was to retrain a parser to produce trees with structures the same as XLEstyle trees and with XLE English grammar labels on the nodes.", "labels": [], "entities": []}, {"text": "For this experiment we use the Bikel () parser, as it is more suited to being retrained on anew treebank annotation scheme.", "labels": [], "entities": [{"text": "Bikel", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.8387866616249084}]}, {"text": "Once we had a retrained version of the Bikel parser that parses novel text into XLE-style trees, we carried out a number of experiments on our development set in order to establish the optimum settings  for the evaluation against the PARC 700 test set.", "labels": [], "entities": [{"text": "PARC 700 test set", "start_pos": 234, "end_pos": 251, "type": "DATASET", "confidence": 0.9630393832921982}]}, {"text": "We evaluated the system that performs best on the development set against the 560-sentence test set of the PARC 700 Dependency Bank.", "labels": [], "entities": [{"text": "PARC 700 Dependency Bank", "start_pos": 107, "end_pos": 131, "type": "DATASET", "confidence": 0.9659269452095032}]}], "tableCaptions": [{"text": " Table 1: Upper-bound results for original Penn-II trees", "labels": [], "entities": []}, {"text": " Table 2: Bikel (2002) and Charniak and Johnson (2005) out-of-the-box baseline results", "labels": [], "entities": []}, {"text": " Table 3: Bikel-XLE Initial Experiments", "labels": [], "entities": [{"text": "Bikel-XLE Initial Experiments", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.6548113226890564}]}, {"text": " Table 4: MXPOST pre-tagged, Non-fragment gram- mar", "labels": [], "entities": [{"text": "MXPOST", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.6036373972892761}, {"text": "Non-fragment gram- mar", "start_pos": 29, "end_pos": 51, "type": "METRIC", "confidence": 0.7563469260931015}]}, {"text": " Table 5: Pruning with Non-fragment grammar, La- beled brackets, Levels default-3", "labels": [], "entities": []}, {"text": " Table 6: Hybrid systems compared to the XLE fragment grammar alone", "labels": [], "entities": []}, {"text": " Table 7: Hybrid systems with pruning compared to the XLE fragment grammar alone", "labels": [], "entities": []}, {"text": " Table 8. The hybrid system achieves an  18% decrease in parsing time, a slight improvement  in coverage of 0.9%, and a 1.12% improvement in  overall f-structure quality.", "labels": [], "entities": [{"text": "parsing", "start_pos": 57, "end_pos": 64, "type": "TASK", "confidence": 0.9604485034942627}, {"text": "coverage", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9984024167060852}]}, {"text": " Table 8: PARC 700 evaluation of the Hybrid system  compared to the XLE fragment grammar alone", "labels": [], "entities": []}]}