{"title": [{"text": "The Corpus and the Lexicon: Standardising Deep Lexical Acquisition Evaluation", "labels": [], "entities": [{"text": "Standardising Deep Lexical Acquisition", "start_pos": 28, "end_pos": 66, "type": "TASK", "confidence": 0.7833582907915115}]}], "abstractContent": [{"text": "This paper is concerned with the standard-isation of evaluation metrics for lexical acquisition over precision grammars, which are attuned to actual parser performance.", "labels": [], "entities": []}, {"text": "Specifically, we investigate the impact that lexicons at varying levels of lexical item precision and recall have on the performance of pre-existing broad-coverage precision grammars in parsing, i.e., on their coverage and accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.8514508605003357}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9956343770027161}, {"text": "accuracy", "start_pos": 223, "end_pos": 231, "type": "METRIC", "confidence": 0.9935114979743958}]}, {"text": "The grammars used for the experiments reported here are the LinGO English Resource Grammar (ERG; Flickinger (2000)) and JACY (Siegel and Bender, 2002), precision grammars of En-glish and Japanese, respectively.", "labels": [], "entities": [{"text": "LinGO English Resource Grammar (ERG; Flickinger (2000))", "start_pos": 60, "end_pos": 115, "type": "DATASET", "confidence": 0.9339395761489868}, {"text": "JACY", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.8095806837081909}, {"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9606067538261414}]}, {"text": "Our results show convincingly that traditional F-score-based evaluation of lexical acquisition does not correlate with actual parsing performance.", "labels": [], "entities": [{"text": "F-score-based", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.9355140924453735}]}, {"text": "What we argue for, therefore, is a recall-heavy interpretation of F-score in designing and optimising automated lexical acquisition algorithms.", "labels": [], "entities": [{"text": "recall-heavy", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.988751232624054}, {"text": "F-score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.950977087020874}]}], "introductionContent": [{"text": "Deep processing is the process of applying rich linguistic resources within NLP tasks, to arrive at a detailed (=deep) syntactic and semantic analysis of the data.", "labels": [], "entities": [{"text": "Deep processing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7000953704118729}]}, {"text": "It is conventionally driven by deep grammars, which encode linguistically-motivated predictions of language behaviour, are usually capable of both parsing and generation, and generate a highlevel semantic abstraction of the input data.", "labels": [], "entities": [{"text": "parsing and generation", "start_pos": 147, "end_pos": 169, "type": "TASK", "confidence": 0.734330803155899}]}, {"text": "While enjoying a resurgence of interest due to advances in parsing algorithms and stochastic parse pruning/ranking, deep grammars remain an underutilised resource predominantly because of their lack of coverage/robustness in parsing tasks.", "labels": [], "entities": [{"text": "parsing algorithms", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.9129133522510529}, {"text": "parse pruning/ranking", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.7413569241762161}, {"text": "parsing tasks", "start_pos": 225, "end_pos": 238, "type": "TASK", "confidence": 0.9044966101646423}]}, {"text": "As noted in previous work (), a significant cause of diminished coverage is the lack of lexical coverage.", "labels": [], "entities": []}, {"text": "Various attempts have been made to ameliorate the deficiencies of hand-crafted lexicons.", "labels": [], "entities": []}, {"text": "More recently, there has been an explosion of interest in deep lexical acquisition (DLA; ()) for broad-coverage deep grammars, either by exploiting the linguistic information encoded in the grammar itself (in vivo), or by using secondary language resources (in vitro).", "labels": [], "entities": []}, {"text": "Such approaches provide (semi-)automatic ways of extending the lexicon with minimal (or no) human interference.", "labels": [], "entities": []}, {"text": "Additionally, it is far from clear how the numbers generated by these evaluation metrics correlate with actual parsing performance when the output of a given DLA method is used.", "labels": [], "entities": []}, {"text": "This makes standardised comparison among the various different approaches to DLA very difficult, if not impossible.", "labels": [], "entities": [{"text": "DLA", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9229421019554138}]}, {"text": "It is far from clear which evaluation metrics are more indicative of the true \"goodness\" of the lexicon.", "labels": [], "entities": []}, {"text": "The aim of this research, therefore, is to analyse how the different evaluation metrics correlate with actual parsing performance using a given lexicon, and to work towards a standardised evaluation framework for future DLA research to ground itself in.", "labels": [], "entities": []}, {"text": "In this paper, we explore the utility of different evaluation metrics at predicting parse performance through a series of experiments over two broad coverage grammars: the English Resource Grammar) and JACY ().", "labels": [], "entities": [{"text": "English Resource Grammar", "start_pos": 172, "end_pos": 196, "type": "DATASET", "confidence": 0.8824827273686727}, {"text": "JACY", "start_pos": 202, "end_pos": 206, "type": "DATASET", "confidence": 0.619251012802124}]}, {"text": "We simulate the results of DLA by generating lexicons at different levels of precision and recall, and test the impact of such lexicons on grammar coverage and accuracy related to goldstandard treebank data.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9987227320671082}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9962283372879028}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9972706437110901}, {"text": "goldstandard treebank data", "start_pos": 180, "end_pos": 206, "type": "DATASET", "confidence": 0.7344540953636169}]}, {"text": "The final outcome of this analysis is a proposed evaluation framework for future DLA research.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows: Section 2 reviews previous work on DLA for the robust parsing task; Section 3 describes the experimental setup; Section 4 presents the experiment results; Section 5 analyses the experiment results; Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this research, we wish to evaluate the impact of different lexicons on grammar performance.", "labels": [], "entities": []}, {"text": "By grammar performance, we principally mean coverage and accuracy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9943008422851562}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.997081458568573}]}, {"text": "However, it should be noted that the efficiency of the grammar-e.g. the average number of edges in the parse chart, the average time to parse a sentence and/or the average number of analyses per sentence-is also an important performance measurement which we expect the quality of the lexicon to impinge on.", "labels": [], "entities": []}, {"text": "Here, however, we expect to be able to call on external processing optimisations 2 to dampen any loss in efficiency, in away which we cannot with coverage and accuracy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9827431440353394}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9956902861595154}]}, {"text": "The experiment consumes a considerable amount of computational resources.", "labels": [], "entities": []}, {"text": "For each lexicon config-P \\ R 0.6 0.8 1.0 0.6 44.56% 66.88% 75.51% 0.8 42.18% 65.82% 75.86% 1.0 40.45% 66.19% 76.15%: Parser coverage of the ERG with different lexicons uration of a given grammar, we need to i) process (parse) all the items in the treebank, ii) compare the resulting trees with the gold-standard trees and update the treebank, and iii) retrain the disambiguation models over 5 folds of cross validation.", "labels": [], "entities": []}, {"text": "Given the two grammars with 9 configurations each, the entire experiment takes over 1 CPU month and about 120GB of disk space.", "labels": [], "entities": []}, {"text": "The coverage results are shown in and for JACY and the ERG, respectively.", "labels": [], "entities": [{"text": "coverage", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9928117394447327}, {"text": "JACY", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.5837835073471069}, {"text": "ERG", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.7036101222038269}]}, {"text": "As expected, we see a significant increase in grammar coverage when the lexicon recall goes up.", "labels": [], "entities": [{"text": "grammar coverage", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.633872926235199}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9911303520202637}]}, {"text": "This increase is more significant for the ERG than JACY, mainly because the JACY lexicon is about twice as large as the ERG lexicon; thus, the most frequent entries are still in the lexicons even with low recall.", "labels": [], "entities": [{"text": "JACY", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.4848826825618744}, {"text": "JACY lexicon", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.849367767572403}, {"text": "recall", "start_pos": 205, "end_pos": 211, "type": "METRIC", "confidence": 0.9969720840454102}]}, {"text": "When the lexicon recall is fixed, the grammar coverage does not change significantly at different levels of lexicon precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9727783203125}]}, {"text": "Recall that we are not evaluating the correctness of such parses at this stage.", "labels": [], "entities": []}, {"text": "It is clear that the increase in lexicon recall boosts the grammar coverage, as we would expect.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9749617576599121}, {"text": "coverage", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.5281640291213989}]}, {"text": "The precision of the lexicon does not have a large influence on coverage.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9990713596343994}, {"text": "coverage", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.710497260093689}]}, {"text": "This result confirms that with DLA (where we hope to enhance lexical coverage relative to a given corpus/domain), the coverage of the grammar can be enhanced significantly.", "labels": [], "entities": []}, {"text": "The accuracy results are obtained with 5-fold cross validation, as shown in and 4 Note that even with the lexicons at 100% precision and recall level, there is no guarantee of 100% coverage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994367957115173}, {"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9935290217399597}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9989548921585083}]}, {"text": "As the contents of the Redwoods and Hinoki treebanks were determined independently of the respective grammars, rather than the grammars being induced from the treebanks e.g., they both still contain significant numbers of strings for which the grammar cannot produce a correct analysis.", "labels": [], "entities": [{"text": "Redwoods and Hinoki treebanks", "start_pos": 23, "end_pos": 52, "type": "DATASET", "confidence": 0.794459342956543}]}, {"text": "\u03c3 060-060 13269 62.65% 0.89% 060-080 19800 60.57% 0.83% 060-100 22361 59.61% 0.63% 080-060 14701 63.27% 0.62% 080-080 23184 60.97% 0.48% 080-100 27111 60.04% 0.56% 100-060 15696 63.91% 0.64% 100-080 26859 61.47% 0.68% 100-100 31870 60.48% 0.71%  for JACY and the ERG, respectively.", "labels": [], "entities": [{"text": "JACY", "start_pos": 250, "end_pos": 254, "type": "DATASET", "confidence": 0.5437427163124084}]}, {"text": "When the lexicon recall goes up, we observe a small but steady decrease in the accuracy of the disambiguation models, for both JACY and ERG.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9946870803833008}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.999535322189331}, {"text": "JACY", "start_pos": 127, "end_pos": 131, "type": "DATASET", "confidence": 0.7595013976097107}, {"text": "ERG", "start_pos": 136, "end_pos": 139, "type": "DATASET", "confidence": 0.7364494204521179}]}, {"text": "This is generally aside effect of change in coverage: as the grammar coverage goes up, the parse trees become more diverse, and are hence harder to discriminate.", "labels": [], "entities": []}, {"text": "When the recall is fixed and the precision of the lexicon goes up, we observe a very small accuracy gain for JACY (around 0.5% for each 20% increase in precision).", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9989224672317505}, {"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9991725087165833}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9991040825843811}, {"text": "JACY", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.5545220971107483}, {"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9985085129737854}]}, {"text": "This shows that the grammar accuracy gain is limited as the precision of the lexicon increases, i.e. that the disambiguation model is remarkably robust to the effects of noise.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9698931574821472}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9980030655860901}]}, {"text": "It should be noted that for the ERG we failed to observe any accuracy gain at all with a more precise lexicon.", "labels": [], "entities": [{"text": "ERG", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.5456477403640747}, {"text": "accuracy gain", "start_pos": 61, "end_pos": 74, "type": "METRIC", "confidence": 0.9844110310077667}]}, {"text": "This is partly due to the limited size of the updated treebanks.", "labels": [], "entities": []}, {"text": "For the lexicon configuration 060 \u2212 060, we obtained only 737 preferred readings/trees to train/test the disambiguation model over.", "labels": [], "entities": []}, {"text": "The 5-fold cross validation results vary within a margin of 10%, which means that the models are still not converging.", "labels": [], "entities": []}, {"text": "However, the result does confirm that there is no significant gain in grammar accuracy with a higher precision lexicon.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.983444333076477}]}, {"text": "Finally, we combine the coverage and accuracy scores into a single F-measure (\u03b2 = 1) value.", "labels": [], "entities": [{"text": "coverage", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9983532428741455}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9937633872032166}, {"text": "F-measure", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9960709810256958}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Again we see that the difference in lexicon recall has a more significant impact on the overall grammar performance than precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9843388795852661}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9986476302146912}]}, {"text": "As mentioned in Section 2, a number of relevant earlier works have evaluated DLA results via the unweighted F-score (relative to type precision and recall).", "labels": [], "entities": [{"text": "F-score", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9784406423568726}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.7464697957038879}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9983749389648438}]}, {"text": "This implicitly assumes that the precision and recall of the lexicon are equally important.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9992746710777283}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9980423450469971}]}, {"text": "However, this is clearly not the case as we can see in the results of the grammar performance.", "labels": [], "entities": []}, {"text": "For example, the lexicon configurations 060 \u2212 100 and 100 \u2212 060 of JACY (i.e. 60% precision, 100% recall vs. 100% precision, 60% recall, respectively) have the same unweighted F-scores, but their corresponding overall grammar performance (parser F-score) differs by up to 17%.", "labels": [], "entities": [{"text": "JACY", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.4859452247619629}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9941152334213257}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9490105509757996}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9187819957733154}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9896553754806519}, {"text": "F-scores", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9754239320755005}, {"text": "grammar performance (parser F-score)", "start_pos": 218, "end_pos": 254, "type": "METRIC", "confidence": 0.6114165236552557}]}], "tableCaptions": [{"text": " Table 1: Different lexicon configurations for the ERG with the number of correct (C), erroneous (E) and  combined (A) entries at each level of precision (P) and recall (R)", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 144, "end_pos": 157, "type": "METRIC", "confidence": 0.930695503950119}, {"text": "recall (R", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9351960817972819}]}, {"text": " Table 2: Different lexicon configurations for JACY with the number of correct (C), erroneous (E) and  combined (A) entries at each level of precision (P) and recall (R)", "labels": [], "entities": [{"text": "JACY", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.48130112886428833}, {"text": "precision (P)", "start_pos": 141, "end_pos": 154, "type": "METRIC", "confidence": 0.9349975883960724}, {"text": "recall (R", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9343592723210653}]}, {"text": " Table 3: Parser coverage of JACY with different lex- icons", "labels": [], "entities": [{"text": "JACY", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.5614763498306274}]}, {"text": " Table 4: Parser coverage of the ERG with different  lexicons", "labels": [], "entities": [{"text": "Parser coverage", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7657971978187561}]}, {"text": " Table 5: Accuracy of disambiguation models for  JACY with different lexicons", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9954289197921753}, {"text": "JACY", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.7617348432540894}]}, {"text": " Table 6: Accuracy of disambiguation models for the  ERG with different lexicons", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.995427131652832}]}]}