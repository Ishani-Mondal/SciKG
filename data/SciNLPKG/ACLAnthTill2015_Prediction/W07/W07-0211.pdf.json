{"title": [{"text": "TextGraphs-2: Graph-Based Algorithms for Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we attempt to deduce tex-tual entailment based on syntactic dependency trees of a given text-hypothesis pair.", "labels": [], "entities": [{"text": "tex-tual entailment", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.6864565461874008}]}, {"text": "The goals of this project are to provide an accurate and fast system, which we have called DLSITE-2, that can be applied in software systems that require a near-real-time interaction with the user.", "labels": [], "entities": []}, {"text": "To accomplish this we use MINIPAR to parse the phrases and construct their corresponding trees.", "labels": [], "entities": []}, {"text": "Later on we apply syntactic-based techniques to calculate the semantic similarity between text and hypothesis.", "labels": [], "entities": []}, {"text": "To measure our method's precision we used the test text corpus set from Second PASCAL Recognising Textual Entailment Challenge (RTE-2), obtaining an accuracy rate of 60.75%.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9988864064216614}, {"text": "PASCAL Recognising Textual Entailment Challenge (RTE-2)", "start_pos": 79, "end_pos": 134, "type": "TASK", "confidence": 0.6022669114172459}, {"text": "accuracy rate", "start_pos": 149, "end_pos": 162, "type": "METRIC", "confidence": 0.9896765947341919}]}], "introductionContent": [{"text": "There are several methods used to determine textual entailment fora given text-hypothesis pair.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.680586576461792}]}, {"text": "The one described in this paper uses the information contained in the syntactic dependency trees of such phrases to deduce whether there is entailment or not.", "labels": [], "entities": []}, {"text": "In addition, semantic knowledge extracted from WordNet () has been added to achieve higher accuracy rates.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.9244410991668701}, {"text": "accuracy rates", "start_pos": 91, "end_pos": 105, "type": "METRIC", "confidence": 0.9812250137329102}]}, {"text": "It has been proven in several competitions and other workshops that textual entailment is a complex task.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8532818555831909}]}, {"text": "One of these competitions is PASCAL Recognising Textual Entailment Challenge (), where each participating group develops a textual entailment recognizing system attempting to accomplish the best accuracy rate of all competitors.", "labels": [], "entities": [{"text": "PASCAL Recognising Textual Entailment Challenge", "start_pos": 29, "end_pos": 76, "type": "TASK", "confidence": 0.6721075892448425}, {"text": "accuracy rate", "start_pos": 195, "end_pos": 208, "type": "METRIC", "confidence": 0.9840999841690063}]}, {"text": "Such complexity is the reason why we use a combination of various techniques to deduce whether entailment is produced.", "labels": [], "entities": []}, {"text": "Currently there are few research projects related to the topic discussed in this paper.", "labels": [], "entities": []}, {"text": "Some systems use syntactic tree matching as the textual entailment decision core module, such as ().", "labels": [], "entities": [{"text": "syntactic tree matching", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.7151796420415243}]}, {"text": "It is based on maximal embedded syntactic subtrees to analyze the semantic relation between text and hypothesis.", "labels": [], "entities": []}, {"text": "Other systems use syntactic trees as a collaborative module, not being the core, such as ().", "labels": [], "entities": []}, {"text": "The application discussed in this paper belongs to the first set of systems, since syntactic matching is its main module.", "labels": [], "entities": [{"text": "syntactic matching", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7267171740531921}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In the second section we will describe the methods implemented in our system.", "labels": [], "entities": []}, {"text": "The third one contains the experimental results, and the fourth and last discusses such results and proposes future work based on our actual research.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experimental results shown in this paper were obtained processing a set of text-hypothesis pairs from RTE-2.", "labels": [], "entities": [{"text": "RTE-2", "start_pos": 106, "end_pos": 111, "type": "DATASET", "confidence": 0.8864274024963379}]}, {"text": "The organizers of this challenge provide development and test corpora to the participants, both of them containing 800 pairs manually annotated for logical entailment.", "labels": [], "entities": []}, {"text": "It is composed of four subsets, each of them corresponding to typical true and false entailments in different tasks, such as Information Extraction (IE), Information Retrieval (IR), Question Answering (QA), and Multi-document Summarization (SUM).", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 125, "end_pos": 152, "type": "TASK", "confidence": 0.8088324189186096}, {"text": "Information Retrieval (IR)", "start_pos": 154, "end_pos": 180, "type": "TASK", "confidence": 0.8224823772907257}, {"text": "Question Answering (QA)", "start_pos": 182, "end_pos": 205, "type": "TASK", "confidence": 0.8676328659057617}, {"text": "Multi-document Summarization (SUM)", "start_pos": 211, "end_pos": 245, "type": "TASK", "confidence": 0.8347917079925538}]}, {"text": "For each task, the annotators selected the same amount of true entailments as negative ones (50%-50% split).", "labels": [], "entities": []}, {"text": "The organizers have also defined two measures to evaluate the participating systems.", "labels": [], "entities": []}, {"text": "All judgments returned by the systems will be compared to those manually assigned by the human annotators.", "labels": [], "entities": []}, {"text": "The percentage of matching judgments will provide the accuracy of the system, i.e. the percentage of correct responses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9993797540664673}]}, {"text": "As a second measure, the average precision will be computed.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9293496012687683}]}, {"text": "This measure evaluates the ability of the systems to rank all the pairs in the corpus according to their entailment confidence, in decreasing order from the most certain entailment to the least.", "labels": [], "entities": []}, {"text": "Average precision is a common evaluation measure for system rankings that is defined as shown in Equation 6.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.6788082122802734}]}, {"text": "where n is the amount of the pairs in the test corpus, R is the total number of positive pairs in it, i ranges over the pairs, ordered by their ranking, and E(i) is defined as follows: 1 if the i \u2212 th pair is positive, 0 otherwise.", "labels": [], "entities": [{"text": "E", "start_pos": 157, "end_pos": 158, "type": "METRIC", "confidence": 0.9703829288482666}]}, {"text": "As we previously mentioned, we tested our system against RTE-2 development corpus, and used the test one to evaluate it.", "labels": [], "entities": [{"text": "RTE-2 development corpus", "start_pos": 57, "end_pos": 81, "type": "DATASET", "confidence": 0.8971359531084696}]}, {"text": "First, shows the accuracy (ACC) and average precision (AP), both as a percentage, obtained processing the development corpus from RTE-2 fora threshold value of 68.9%, which corresponds to the highest accuracy that can be obtained using our system for the mentioned corpus.", "labels": [], "entities": [{"text": "accuracy (ACC)", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.9088395535945892}, {"text": "average precision (AP)", "start_pos": 36, "end_pos": 58, "type": "METRIC", "confidence": 0.8549084663391113}, {"text": "RTE-2", "start_pos": 130, "end_pos": 135, "type": "DATASET", "confidence": 0.7620881199836731}, {"text": "accuracy", "start_pos": 200, "end_pos": 208, "type": "METRIC", "confidence": 0.9962902069091797}]}, {"text": "It also provides the rate of correctly predicted true and false entailments.", "labels": [], "entities": []}, {"text": "Next, let us show in the results obtained processing the test corpus, which is the one used to compare the different systems that participated in RTE-2, with the same threshold as before.", "labels": [], "entities": [{"text": "RTE-2", "start_pos": 146, "end_pos": 151, "type": "TASK", "confidence": 0.47007712721824646}]}, {"text": "As one can observe in the previous table, our system provides a high accuracy rate by using mainly syntactical measures.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.9801633656024933}]}, {"text": "The number of texthypothesis pairs that succeeded the graph embedding evaluation was three for the development corpus and one for the test set, which reflects the strictness of such module.", "labels": [], "entities": []}, {"text": "However, we would like to point out that the amount of pairs affected by the mentioned module will depend on the corpus nature, so it can vary significantly between different corpora.", "labels": [], "entities": []}, {"text": "Let us now compare our results with the ones that were achieved by the systems that participated in RTE-2.", "labels": [], "entities": [{"text": "RTE-2", "start_pos": 100, "end_pos": 105, "type": "DATASET", "confidence": 0.8065444827079773}]}, {"text": "One should note that the criteria for such ranking is based exclusively on the accuracy, ignoring the average precision value.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9995384216308594}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9971018433570862}]}, {"text": "In addition, each participating group was allowed to submit two different systems to RTE-2.", "labels": [], "entities": [{"text": "RTE-2", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.944714367389679}]}, {"text": "We will consider here the best result of both systems for each group.", "labels": [], "entities": []}, {"text": "The mentioned comparison is shown in, and contains only the systems that had higher accuracy rates than our approach.", "labels": [], "entities": [{"text": "accuracy rates", "start_pos": 84, "end_pos": 98, "type": "METRIC", "confidence": 0.9818946719169617}]}], "tableCaptions": [{"text": " Table 5: Results obtained for the development cor- pus.", "labels": [], "entities": []}, {"text": " Table 6: Results obtained for the test corpus.", "labels": [], "entities": []}, {"text": " Table 8: DLSITE-2 response times (in seconds).", "labels": [], "entities": [{"text": "DLSITE-2 response times", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8618946671485901}]}]}