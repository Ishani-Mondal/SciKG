{"title": [], "abstractContent": [{"text": "We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.7019115686416626}]}, {"text": "We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to.", "labels": [], "entities": [{"text": "language and translation model adaptation", "start_pos": 138, "end_pos": 179, "type": "TASK", "confidence": 0.6363206565380096}]}, {"text": "The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9991605281829834}]}], "introductionContent": [{"text": "Language varies significantly across different genres, topics, styles, etc.", "labels": [], "entities": []}, {"text": "This affects empirical models: a model trained on a corpus of car-repair manuals, for instance, will not be well suited to an application in the field of tourism.", "labels": [], "entities": []}, {"text": "Ideally, models should be trained on text that is representative of the area in which they will be used, but such text is not always available.", "labels": [], "entities": []}, {"text": "This is especially the case for bilingual applications, because parallel training corpora are relatively rare and tend to be drawn from specific domains such as parliamentary proceedings.", "labels": [], "entities": []}, {"text": "In this paper we address the problem of adapting a statistical machine translation system by adjusting its parameters based on some information about a test domain.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.610691487789154}]}, {"text": "We assume two basic settings.", "labels": [], "entities": []}, {"text": "In cross-domain adaptation, a small sample of parallel in-domain text is available, and it is used to optimize for translating future texts drawn from the same domain.", "labels": [], "entities": [{"text": "cross-domain adaptation", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.8451322913169861}]}, {"text": "In dynamic adaptation, no domain information is available ahead of time, and adaptation is based on the current source text under translation.", "labels": [], "entities": [{"text": "dynamic adaptation", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.7700626850128174}]}, {"text": "Approaches developed for the two settings can be complementary: an in-domain development corpus can be used to make broad adjustments, which can then be fine tuned for individual source texts.", "labels": [], "entities": []}, {"text": "Our method is based on the classical technique of mixture modeling ().", "labels": [], "entities": [{"text": "mixture modeling", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.806095540523529}]}, {"text": "This involves dividing the training corpus into different components, training a model on each part, then weighting each model appropriately for the current context.", "labels": [], "entities": []}, {"text": "Mixture modeling is a simple framework that encompasses many different variants, as described below.", "labels": [], "entities": [{"text": "Mixture modeling", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9453321397304535}]}, {"text": "It is naturally fairly low dimensional, because as the number of sub-models increases, the amount of text available to train each, and therefore its reliability, decreases.", "labels": [], "entities": [{"text": "reliability", "start_pos": 149, "end_pos": 160, "type": "METRIC", "confidence": 0.9860645532608032}]}, {"text": "This makes it suitable for discriminative SMT training, which is still a challenge for large parameter sets (;).", "labels": [], "entities": [{"text": "SMT training", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.9417247474193573}]}, {"text": "Techniques for assigning mixture weights depend on the setting.", "labels": [], "entities": []}, {"text": "In cross-domain adaptation, knowledge of both source and target texts in the in-domain sample can be used to optimize weights directly.", "labels": [], "entities": [{"text": "cross-domain adaptation", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.8103670179843903}]}, {"text": "In dynamic adaptation, training poses a problem because no reference text is available.", "labels": [], "entities": [{"text": "dynamic adaptation", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8494377732276917}]}, {"text": "Our solution is to construct a multi-domain development sample for learning parameter settings that are intended to generalize to new domains (ones not represented in the sample).", "labels": [], "entities": []}, {"text": "We do not learn mixture weights directly with this method, because there is little hope that these would be well suited to new domains.", "labels": [], "entities": []}, {"text": "Instead we attempt to learn how weights should beset as a function of distance.", "labels": [], "entities": []}, {"text": "To our knowledge, this approach to dynamic adaptation for SMT is novel, and it is one of the main contributions of the paper.", "labels": [], "entities": [{"text": "dynamic adaptation", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.6919074952602386}, {"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9657149910926819}]}, {"text": "A second contribution is a fairly broad investigation of the large space of alternatives defined by the mixture-modeling framework, using a simple genrebased corpus decomposition.", "labels": [], "entities": []}, {"text": "We experimented with the following choices: cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; various text distance metrics; different ways of converting distance metrics into weights; and granularity of the source unit being adapted to.", "labels": [], "entities": [{"text": "language and translation model adaptation", "start_pos": 118, "end_pos": 159, "type": "TASK", "confidence": 0.6083187639713288}]}, {"text": "The remainder of the paper is structured follows: section 2 briefly describes our phrase-based SMT system; section 3 describes mixture-model adaptation; section 4 gives experimental results; section 5 summarizes previous work; and section 6 concludes.", "labels": [], "entities": [{"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.8663853406906128}, {"text": "mixture-model adaptation", "start_pos": 127, "end_pos": 151, "type": "TASK", "confidence": 0.6734123229980469}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Corpora. In the genres column: nw =  newswire, sp = speeches, ed = editorial, ng = news- group, bn = broadcast news, and bc = broadcast con- versation.", "labels": [], "entities": []}, {"text": " Table 2: Linear versus loglinear combinations on  NIST04-nw.", "labels": [], "entities": [{"text": "NIST04-nw", "start_pos": 51, "end_pos": 60, "type": "DATASET", "confidence": 0.9560390710830688}]}, {"text": " Table 3: Distance metrics for linear combination on  the NIST04-nw development set. (Entries in the top  right corner are missing due to lack of time.)", "labels": [], "entities": [{"text": "NIST04-nw development set", "start_pos": 58, "end_pos": 83, "type": "DATASET", "confidence": 0.9839238325754801}]}, {"text": " Table 4: Weighting techniques for linear combina- tion on the NIST04-nw development set.", "labels": [], "entities": [{"text": "NIST04-nw development set", "start_pos": 63, "end_pos": 88, "type": "DATASET", "confidence": 0.9769885341326395}]}, {"text": " Table 5: Cross-Domain adaptation results.", "labels": [], "entities": [{"text": "Cross-Domain adaptation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8168634474277496}]}, {"text": " Table 6: Dynamic adaptation results, using src-side  EM distances.", "labels": [], "entities": [{"text": "Dynamic adaptation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7868905663490295}]}, {"text": " Table 7: Hybrid adaptation results.", "labels": [], "entities": [{"text": "Hybrid adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8525502383708954}]}, {"text": " Table 8: The effects of source granularity on dy- namic adaptation.", "labels": [], "entities": []}]}