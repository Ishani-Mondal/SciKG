{"title": [{"text": "Enhancing commercial grammar-based applications using robust approaches to speech understanding", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a series of measurements of the accuracy of speech understanding when grammar-based or robust approaches are used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9980095028877258}, {"text": "speech understanding", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.6746741533279419}]}, {"text": "The robust approaches considered here are based on statistical language models (SLMs) with the interpretation being carried out by phrase-spotting or robust parsing methods.", "labels": [], "entities": []}, {"text": "We propose a simple process to leverage existing grammars and logged utterances to upgrade grammar-based applications to become more robust to out-of-coverage inputs.", "labels": [], "entities": []}, {"text": "All experiments herein are run on data collected from deployed directed dialog applications and show that SLM-based techniques outperform grammar-based ones without requiring any change in the application logic.", "labels": [], "entities": []}], "introductionContent": [{"text": "The bulk of the literature on spoken dialog systems is based on the simple architecture in which the input speech is processed by a statistical language model-based recognizer (SLM-based recognizer) to produce a word string.", "labels": [], "entities": []}, {"text": "This word string is further processed by a robust parser or call router) to be converted in a semantic interpretation.", "labels": [], "entities": []}, {"text": "However, it is striking to see that a large portion of deployed commercial applications do not follow this architecture and approach the recognition/interpretation problem by relying on hand-crafted rules (context-free grammars -CFGs).", "labels": [], "entities": [{"text": "recognition/interpretation", "start_pos": 137, "end_pos": 163, "type": "TASK", "confidence": 0.8788981835047404}]}, {"text": "The apparent reasons for this are the up-front cost and additional delays of collecting domain-specific utterances to properly train the SLM (not to mention semantic tagging needed to train the call router) ().", "labels": [], "entities": []}, {"text": "Choosing to use a grammar-based approach also makes the application predictable and relatively easy to design.", "labels": [], "entities": []}, {"text": "On the other hand, these applications are usually very rigid: the users are allowed only a finite set of ways to input their requests and, byway of consequences, these applications suffer from high out-of-grammar (OOG) rates or outof-coverage rates.", "labels": [], "entities": []}, {"text": "A few studies have been published comparing grammar-based and SLM-based approaches to speech understanding.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.770674854516983}]}, {"text": "In (), a comparison of grammar-based and robust approaches is presented fora user-initiative home automation application.", "labels": [], "entities": []}, {"text": "The authors concluded that it was relatively easy to use the corpus collected during the course of the application development to train a SLM which would perform better on outof-coverage utterances, while degrading the accuracy on in-coverage utterances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 219, "end_pos": 227, "type": "METRIC", "confidence": 0.9981191754341125}]}, {"text": "They also reported that the SLM-based system showed slightly lower word error rate but higher semantic error rate for the users who know the application's coverage.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 67, "end_pos": 82, "type": "METRIC", "confidence": 0.7286666532357534}, {"text": "semantic error rate", "start_pos": 94, "end_pos": 113, "type": "METRIC", "confidence": 0.7654902935028076}]}, {"text": "In), a rigorous test protocol is presented to compare grammar-based and robust approaches in the context of a medical translation system.", "labels": [], "entities": [{"text": "medical translation system", "start_pos": 110, "end_pos": 136, "type": "TASK", "confidence": 0.7478913267453512}]}, {"text": "The paper highlights the difficulties to construct a clean experimental set-up.", "labels": [], "entities": []}, {"text": "Efforts are spent to control the training set of both approaches to have them align.", "labels": [], "entities": []}, {"text": "The training sets are defined as the set of data available to build each system: fora grammar-based system, it might be a series of sample dialogs.", "labels": [], "entities": []}, {"text": "(ten) presents experiments comparing grammar-based and SLM-based systems for na\u00a8\u0131vena\u00a8\u0131ve users and an expert user.", "labels": [], "entities": []}, {"text": "They conclude that the SLM-based system is most effective in reducing the error rate for na\u00a8\u0131vena\u00a8\u0131ve users.", "labels": [], "entities": [{"text": "error rate", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9813427925109863}]}, {"text": "Recently (see ()), a process was presented to automatically build SLMs from a wide variety of sources (in-service data, thesaurus, WordNet and world-wide web).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 131, "end_pos": 138, "type": "DATASET", "confidence": 0.9404309391975403}]}, {"text": "Results on data from commercial speech applications presented therein echo earlier results) while reducing the effort to build interpretation rules.", "labels": [], "entities": []}, {"text": "Most of the above studies are not based on data collected on deployed applications.", "labels": [], "entities": []}, {"text": "One of the conclusions from previous work, based on the measured fact that in-coverage accuracy of the grammar-based systems was far better than the SLM one, was that as people get more experience with the applications, they will naturally learn its coverage and gravitate towards it.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.8917744159698486}]}, {"text": "While this can bean acceptable option for some types of applications (when the user population tends to be experienced or captive), it certainly is not a possibility for large-scale commercial applications that are targeted at the general public.", "labels": [], "entities": []}, {"text": "A few examples of such applications are public transit schedules and fares information, self-help applications for utilities, banks, telecommunications business, and etc.", "labels": [], "entities": []}, {"text": "Steering application design and research based on in-coverage accuracy is not suitable for these types of applications because a large fraction of the users are na\u00a8\u0131vesna\u00a8\u0131ves and tend to use more natural and unconstrained speech inputs.", "labels": [], "entities": [{"text": "Steering application design", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8440607388814291}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9033080339431763}]}, {"text": "This paper exploits techniques known since the 90's (SLM with robust parsing,) and applies them to build robust speech understanding into existing large scale directed dialog grammarbased applications.", "labels": [], "entities": []}, {"text": "This practical application of) is cast as an upgrade problem which must obey the following constraints.", "labels": [], "entities": []}, {"text": "The first constraint dictates that, for each context, the interpretation engines (from the current and upgraded systems) must return the same semantics (i.e. same set of slots).", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next Section describes the applications from which the data was collected, the experimental set-up and the accuracy measures used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9990872144699097}]}, {"text": "Section 3 describes how the semantic truth is generated.", "labels": [], "entities": []}, {"text": "The main results of the upgrade from grammar-based to SLMbased recognition are presented in Section 4.", "labels": [], "entities": [{"text": "SLMbased recognition", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.8838760554790497}]}, {"text": "The target audience for this paper is composed of application developers and researchers that are interested in the robust information extraction from directed dialog speech applications targeted at the general public.", "labels": [], "entities": [{"text": "information extraction from directed dialog speech", "start_pos": 123, "end_pos": 173, "type": "TASK", "confidence": 0.8160374263922373}]}], "datasetContent": [{"text": "set-up  The baseline system is the grammar-based system; the recognizer uses, on a per-context basis, the grammars listed in in parallel.", "labels": [], "entities": []}, {"text": "The SLM systems studied all used the same interpretation engine: robust parsing with the grammars listed in as rules to fill slots.", "labels": [], "entities": []}, {"text": "Note that this allows the application logic to stay unchanged since the set of potential slots returned within any given context is the same as for the grammar-based systems (see first constraint in Sec. 1).", "labels": [], "entities": []}, {"text": "Adhering to this experimental set-up also guarantees that improvements measured in the lab will have a direct impact on the raw accuracy of the deployed application.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9900087118148804}]}, {"text": "We have considered two different SLM-based systems in this study: standard SLM (wordSLM) and class-based SLM (classSLM)).", "labels": [], "entities": []}, {"text": "In the classSLM systems, the classes are defined as the rules of the interpretation engine (i.e. the grammars active for each context as defined in).", "labels": [], "entities": []}, {"text": "The SLMs are all trained on a per-context basis) as bi-grams with Witten-Bell discounting.", "labels": [], "entities": []}, {"text": "To insure that the word-SLM system covered all sentences that the grammarbased system does, we augmented the training set of  the wordSLM (see) with the list of sentences that are covered by the baseline grammar-based system.", "labels": [], "entities": []}, {"text": "This acts as a backoff in case a word or bigram is not found in the training set (not to be confused with bi-gram to uni-gram backoffs found in standard SLM training).", "labels": [], "entities": []}, {"text": "This is particularly helpful when a little amount of data is available for training the wordSLM (see Sec. 4.3).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of utterances out-of-coverage for  each context.", "labels": [], "entities": []}]}