{"title": [{"text": "CCG Supertags in Factored Statistical Machine Translation", "labels": [], "entities": [{"text": "CCG Supertags", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8282880187034607}, {"text": "Statistical Machine Translation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.5525583724180857}]}], "abstractContent": [{"text": "Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level.", "labels": [], "entities": [{"text": "Combinatorial Categorial Grammar (CCG) supertags", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7834147427763257}, {"text": "phrase-based machine translation", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.6329604585965475}]}, {"text": "The challenge is incorporating this information into the translation process.", "labels": [], "entities": [{"text": "translation process", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.9148603975772858}]}, {"text": "Factored translation models allow the inclusion of su-pertags as a factor in the source or target language.", "labels": [], "entities": []}, {"text": "We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings.", "labels": [], "entities": []}], "introductionContent": [{"text": "In large-scale machine translation evaluations, phrase-based models generally outperform syntaxbased models . Phrase-based models are effective because they capture the lexical dependencies between languages.", "labels": [], "entities": [{"text": "machine translation evaluations", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.8026128609975179}]}, {"text": "However, these models, which are equivalent to finite-state machines, are unable to model long range word order differences.", "labels": [], "entities": []}, {"text": "Phrase-based models also lack the ability to incorporate the generalisations implicit in syntactic knowledge and they do not respect linguistic phrase boundaries.", "labels": [], "entities": []}, {"text": "This makes it difficult to improve reordering in phrase-based models.", "labels": [], "entities": []}, {"text": "Syntax-based models can overcome some of the problems associated with phrase-based models because they are able to capture the long range structural mappings that occur in translation.", "labels": [], "entities": []}, {"text": "Recently there have been a few syntax-based models that show performance comparable to the phrase-based models).", "labels": [], "entities": []}, {"text": "However, reliably learning powerful rules from parallel data is very difficult and prone to problems with sparsity and noise in the data.", "labels": [], "entities": []}, {"text": "These models also suffer from a large search space when decoding with an integrated language model, which can lead to search errors.", "labels": [], "entities": []}, {"text": "In this paper we investigate the idea of incorporating syntax into phrase-based models, thereby leveraging the strengths of both the phrase-based models and syntactic structures.", "labels": [], "entities": []}, {"text": "This is done using CCG supertags, which provide a rich source of syntactic information.", "labels": [], "entities": []}, {"text": "CCG contains most of the structure of the grammar in the lexicon, which makes it possible to introduce CCG supertags as a factor in a factored translation model ( ).", "labels": [], "entities": []}, {"text": "Factored models allow words to be vectors of features: one factor could be the surface form and other factors could contain linguistic information.", "labels": [], "entities": []}, {"text": "Factored models allow for the easy inclusion of supertags in different ways.", "labels": [], "entities": []}, {"text": "The first approach is to generate CCG supertags as a factor in the target and then apply an n-gram model over them, increasing the probability of more frequently seen sequences of supertags.", "labels": [], "entities": []}, {"text": "This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by.", "labels": [], "entities": []}, {"text": "For both Arabic-English ( and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation.", "labels": [], "entities": []}, {"text": "By preferring more likely sequences of supertags, it is conceivable that the output of the decoder is more grammatical.", "labels": [], "entities": []}, {"text": "However, its not clear exactly how syntactic information can benefit a flat structured model: the constraints contained within supertags are not enforced and relationships between supertags are not linear.", "labels": [], "entities": []}, {"text": "We perform experiments to explore the nature and limits of the contribution of supertags, using different orders of n-gram models, reordering models and focussed manual evaluation.", "labels": [], "entities": []}, {"text": "It seems that the benefit of using n-gram supertag sequence models is largely from improving reordering, as much of the gain is eroded by using a lexicalised reordering model.", "labels": [], "entities": []}, {"text": "This is supported by the manual evaluation which shows a 44% improvement in reordering Dutch-English verb final sentences.", "labels": [], "entities": [{"text": "reordering Dutch-English verb final sentences", "start_pos": 76, "end_pos": 121, "type": "TASK", "confidence": 0.579846453666687}]}, {"text": "The second and novel way we use supertags is to direct the translation process.", "labels": [], "entities": [{"text": "translation process", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8997996747493744}]}, {"text": "Supertags on the source sentence allows the decoder to make decisions based on the structure of the input.", "labels": [], "entities": []}, {"text": "The subcategorisation of a verb, for instance, might help select the correct translation.", "labels": [], "entities": []}, {"text": "Using multiple dependencies on factors in the source, we need a strategy for dealing with sparse data.", "labels": [], "entities": []}, {"text": "We propose using a logarithmic opinion pool () to combine the more specific models (which depend on both words and supertags) with more general models (which only depends on words).", "labels": [], "entities": []}, {"text": "This paper is the first to suggest this approach for combining multiple information sources in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7616951763629913}]}, {"text": "Although the addition of supertags to phrasebased translation does show some improvement, their overall impact is limited.", "labels": [], "entities": [{"text": "phrasebased translation", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.825862854719162}]}, {"text": "Sequence models over supertags clearly result in some improvements in local reordering but syntactic information contains long distance dependencies which are simply not utilised in phrase-based models.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first set of experiments explores the effect of CCG supertags on the target, translating from Dutch into English.", "labels": [], "entities": []}, {"text": "The last experiment shows the effect of CCG supertags on the source, translating from German into English.", "labels": [], "entities": []}, {"text": "These language pairs present a considerable reordering challenge.", "labels": [], "entities": []}, {"text": "For example, Dutch and German have SOV word order insubordinate clauses.", "labels": [], "entities": []}, {"text": "This means that the verb often appears at the end of the clause, far from the position of the English verb.", "labels": [], "entities": []}, {"text": "The experiments were run using Moses , an open source factored statistical machine translation system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.6649229129155477}]}, {"text": "The SRILM language modelling toolkit) was used with modified Kneser-Ney discounting and interpolation.", "labels": [], "entities": [{"text": "SRILM language modelling", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6481291651725769}]}, {"text": "The CCG supertagger) was provided with the C&C Language Processing Tools 3 . The supertagger was trained on the CCGBank in English () and in German).", "labels": [], "entities": [{"text": "CCGBank", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9627007842063904}]}, {"text": "The Dutch-English parallel training data comes from the Europarl corpus () and excludes the proceedings from the last quarter of 2000.", "labels": [], "entities": [{"text": "Dutch-English parallel training data", "start_pos": 4, "end_pos": 40, "type": "DATASET", "confidence": 0.5206979885697365}, {"text": "Europarl corpus", "start_pos": 56, "end_pos": 71, "type": "DATASET", "confidence": 0.9918923377990723}]}, {"text": "This consists of 855,677 sentences with a maximum of 50 words per sentence.", "labels": [], "entities": []}, {"text": "500 sentences of tuning data and the 2000 sentences of test data are taken from the ACL Workshop on Building and Using Parallel Texts . The German-English experiments use data from the NAACL 2006 Workshop on Statistical Machine Translation . The data consists of 751,088 sentences of training data, 500 sentences of tuning data and 3064 sentences of test data.", "labels": [], "entities": [{"text": "NAACL 2006 Workshop on Statistical Machine Translation", "start_pos": 185, "end_pos": 239, "type": "TASK", "confidence": 0.7424959710666111}]}, {"text": "The English and German training sets were POS tagged and supertagged before lowercasing.", "labels": [], "entities": [{"text": "English and German training sets", "start_pos": 4, "end_pos": 36, "type": "DATASET", "confidence": 0.692801958322525}, {"text": "POS", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.6142575740814209}]}, {"text": "The language models and the sequence models were trained on the Europarl training data.", "labels": [], "entities": [{"text": "Europarl training data", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.9848562479019165}]}, {"text": "Where not otherwise specified, the POS tag and supertag sequence models are 5-gram models and the language model is a 3-gram model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Analysis of % correct translation and reordering  of verbs for Dutch-English translation", "labels": [], "entities": [{"text": "Dutch-English translation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.6087670773267746}]}, {"text": " Table 3. BLUE scores for Dutch-English models which  apply CCG supertag sequence models of varying orders", "labels": [], "entities": [{"text": "BLUE", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.997848629951477}]}, {"text": " Table 4. BLEU scores for Dutch-English models which use  language models of increasing n-gram length. Column  None does not apply any language model. Model sw, tw  does not apply any sequence models, and model sw, twpc  applies both POS tag and supertag sequence models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988265633583069}]}, {"text": " Table 5. Dutch-English models with and without a lexi- calised reordering model.", "labels": [], "entities": []}]}