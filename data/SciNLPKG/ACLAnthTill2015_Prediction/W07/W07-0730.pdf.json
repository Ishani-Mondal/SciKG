{"title": [{"text": "UCB System Description for the WMT 2007 Shared Task", "labels": [], "entities": [{"text": "UCB System Description", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.9091993172963461}, {"text": "WMT 2007 Shared Task", "start_pos": 31, "end_pos": 51, "type": "DATASET", "confidence": 0.7549989074468613}]}], "abstractContent": [{"text": "For the WMT 2007 shared task, the UC Berkeley team employed three techniques of interest.", "labels": [], "entities": [{"text": "WMT 2007 shared task", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.6990841776132584}]}, {"text": "First, we used monolingual syntactic paraphrases to provide syntactic variety to the source training set sentences.", "labels": [], "entities": []}, {"text": "Second , we trained two language models: a small in-domain model and a large out-of-domain model.", "labels": [], "entities": []}, {"text": "Finally, we made use of results from prior research that shows that cog-nate pairs can improve word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.7655069828033447}]}, {"text": "We contributed runs translating English to Span-ish, French, and German using various combinations of these techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern Statistical Machine Translation (SMT) systems are trained on aligned sentences of bilingual corpora, typically from one domain.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 7, "end_pos": 44, "type": "TASK", "confidence": 0.8274546166261038}]}, {"text": "When tested on text from that same domain, such systems demonstrate state-of-the art performance; however, on out-of-domain text the results can get significantly worse.", "labels": [], "entities": []}, {"text": "For example, on the WMT 2006 Shared Task evaluation, the French to English translation BLEU scores dropped from about 30 to about 20 for nearly all systems, when tested on News Commentary rather than Europarl ().", "labels": [], "entities": [{"text": "WMT 2006 Shared Task evaluation", "start_pos": 20, "end_pos": 51, "type": "DATASET", "confidence": 0.8873923659324646}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9674650430679321}, {"text": "Europarl", "start_pos": 200, "end_pos": 208, "type": "DATASET", "confidence": 0.9841071367263794}]}, {"text": "Therefore, this year the shared task organizers have provided 1M words of bilingual News Commentary training data in addition to the Europarl data (about 30M words), thus challenging the participants to experiment with domain adaptation.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 133, "end_pos": 146, "type": "DATASET", "confidence": 0.9896032512187958}, {"text": "domain adaptation", "start_pos": 219, "end_pos": 236, "type": "TASK", "confidence": 0.7168942987918854}]}, {"text": "Below we describe our domain adaptation experiments, trying to achieve better results on the News Commentary data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7311941236257553}, {"text": "News Commentary data", "start_pos": 93, "end_pos": 113, "type": "DATASET", "confidence": 0.9799684484799703}]}, {"text": "In addition to training on both data sets, we make use of monolingual syntactic paraphrases of the English side of the data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Summary of our submissions. All runs are for the News Commentary test data. The official  submissions are marked with a star.", "labels": [], "entities": [{"text": "News Commentary test data", "start_pos": 59, "end_pos": 84, "type": "DATASET", "confidence": 0.9654411524534225}]}]}