{"title": [{"text": "Biological, translational, and clinical language processing", "labels": [], "entities": [{"text": "translational, and clinical language processing", "start_pos": 12, "end_pos": 59, "type": "TASK", "confidence": 0.5679321686426798}]}], "abstractContent": [{"text": "This paper is concerned with the evaluation of biomedical named entity recognition systems.", "labels": [], "entities": [{"text": "biomedical named entity recognition", "start_pos": 47, "end_pos": 82, "type": "TASK", "confidence": 0.6240199133753777}]}, {"text": "We compare two such systems, one based on a Hidden Markov Model and one based on Conditional Random Fields and syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.7112000435590744}]}, {"text": "In our experiments we used automatically generated data as well as manually annotated material, including anew dataset which consists of biomedi-cal full papers.", "labels": [], "entities": []}, {"text": "Through our evaluation, we assess the strengths and weaknesses of the systems tested, as well as the datasets themselves in terms of the challenges they present to the systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The domain of biomedical text mining has become of importance for the natural language processing (NLP) community.", "labels": [], "entities": [{"text": "biomedical text mining", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.6450331509113312}]}, {"text": "While there is a lot of textual information available in the domain, either in the form of publications or in model organism databases, there is paucity in material annotated explicitly for the purpose of developing NLP systems.", "labels": [], "entities": []}, {"text": "Most of the existing systems have been developed using data from the newswire domain.", "labels": [], "entities": []}, {"text": "Therefore, the biomedical domain is an appropriate platform to evaluate existing systems in terms of their portability and adaptability.", "labels": [], "entities": []}, {"text": "Also, it motivates the development of new systems, as well as methods for developing systems with these aspects in focus in addition to the performance.", "labels": [], "entities": []}, {"text": "The biomedical named entity recognition (NER) task in particular has attracted a lot of attention from the community recently.", "labels": [], "entities": [{"text": "biomedical named entity recognition (NER) task", "start_pos": 4, "end_pos": 50, "type": "TASK", "confidence": 0.8027800992131233}]}, {"text": "There have been three shared tasks), BioCreative () and BioCreative2 () which involved some flavour of NER using manually annotated training material and fully supervised machine learning methods.", "labels": [], "entities": [{"text": "BioCreative", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.8067158460617065}, {"text": "BioCreative2", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.8168078660964966}]}, {"text": "In parallel, there have been successful efforts in bootstrapping NER systems using automatically generated training material using domain resources).", "labels": [], "entities": []}, {"text": "These approaches have a significant appeal, since they don't require manual annotation of training material which is an expensive and lengthy process.", "labels": [], "entities": []}, {"text": "Named entity recognition is an important task because it is a prerequisite to other more complex ones.", "labels": [], "entities": [{"text": "Named entity recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7168144583702087}]}, {"text": "Examples include anaphora resolution) and gene normalization).", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7235210090875626}, {"text": "gene normalization", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.6921181827783585}]}, {"text": "An important point is that until now NER systems have been evaluated on abstracts, or on sentences selected from abstracts.", "labels": [], "entities": []}, {"text": "However, NER systems will be applied to full papers, either on their own or in order to support more complex tasks.", "labels": [], "entities": []}, {"text": "Full papers though are expected to present additional challenges to the systems than the abstracts, so it is important to evaluate on the former as well in order to obtain a clearer picture of the systems and the task (.", "labels": [], "entities": []}, {"text": "In this paper, we compare two NER systems in a variety of settings.", "labels": [], "entities": []}, {"text": "Most notably, we use automatically generated training data and we evaluate on abstracts as well as anew dataset consisting of full papers.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first evaluation of biomedical NER on full paper text instead of abstracts.", "labels": [], "entities": [{"text": "biomedical NER", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.5361945778131485}]}, {"text": "We assess the performance and the portability of the systems and using this evaluation we combine them in order to take advantage of their strengths.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran experiments using the two NER systems and the three datasets described in Sections 2 and 3.", "labels": [], "entities": []}, {"text": "In order to evaluate the performance of the systems, apart from the standard recall, precision and F-score metrics, we measured the performance on seen and unseen gene names independently, as suggested by . In brief, the gene names that are in the test set and the output generated by the system are separated according to whether they have been encountered in the training data as gene names.", "labels": [], "entities": [{"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9993306398391724}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9971895813941956}, {"text": "F-score", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9979214072227478}]}, {"text": "Then, the standard recall, precision and F-score metrics are calculated for each of these lists independently.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9862421751022339}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9988766312599182}, {"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9983121156692505}]}, {"text": "report in detail the performance of the two systems when trained on the noisy abstracts and evaluated on the manually annotated abstracts and full papers respectively.", "labels": [], "entities": []}, {"text": "As it can be seen, the performance of the HMM-based NER system is better than that of CRF+RASP when evaluating on abstracts and worse when evaluating on full papers (81.86 vs 74.72 and 67.87 vs 72.73 respectively).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the datasets", "labels": [], "entities": []}, {"text": " Table 2: Results on training on noisy abstracts and  testing on manually annotated abstracts", "labels": [], "entities": []}, {"text": " Table 3: Results on training on noisy abstracts and  testing on full papers", "labels": [], "entities": []}, {"text": " Table 4: Results on training on manually annotated  abstracts and testing on full papers", "labels": [], "entities": []}]}