{"title": [{"text": "A Perspective-Based Approach for Solving Textual Entailment Recognition\u00b4Oscar", "labels": [], "entities": [{"text": "Solving Textual Entailment Recognition\u00b4Oscar", "start_pos": 33, "end_pos": 77, "type": "TASK", "confidence": 0.775789238512516}]}], "abstractContent": [{"text": "The textual entailment recognition system that we discuss in this paper represents a perspective-based approach composed of two modules that analyze text-hypothesis pairs from a strictly lexical and syntactic perspectives, respectively.", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.7402369777361552}]}, {"text": "We attempt to prove that the textual entailment recognition task can be overcome by performing individual analysis that acknowledges us of the maximum amount of information that each single perspective can provide.", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.8291040658950806}]}, {"text": "We compare this approach with the system we presented in the previous edition of PASCAL Recognis-ing Textual Entailment Challenge, obtaining an accuracy rate 17.98% higher.", "labels": [], "entities": [{"text": "PASCAL Recognis-ing Textual Entailment Challenge", "start_pos": 81, "end_pos": 129, "type": "DATASET", "confidence": 0.7167114377021789}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9993834495544434}]}], "introductionContent": [{"text": "Textual entailment recognition has become a popular Natural Language Processing task within the last few years.", "labels": [], "entities": [{"text": "Textual entailment recognition", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8588067094484965}, {"text": "Natural Language Processing task", "start_pos": 52, "end_pos": 84, "type": "TASK", "confidence": 0.6913201659917831}]}, {"text": "It consists in determining whether one text snippet (hypothesis) entails another one (text)).", "labels": [], "entities": []}, {"text": "To overcome this problem several approaches have been studied, being the Recognising Textual Entailment Challenge (RTE) () the most referred source for determining which one is the most accurate.", "labels": [], "entities": [{"text": "Recognising Textual Entailment Challenge (RTE)", "start_pos": 73, "end_pos": 119, "type": "TASK", "confidence": 0.6902074345520565}]}, {"text": "Many of the participating groups in previous editions of RTE, including ourselves (), designed systems that combined a variety of lexical, syntactic and semantic techniques.", "labels": [], "entities": [{"text": "RTE", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.69329833984375}]}, {"text": "In our contribution to RTE-3 we attempt to solve the textual entailment recognition task by analyzing two different perspectives separately, in order to acknowledge the amount of information that an individual perspective can provide.", "labels": [], "entities": [{"text": "RTE-3", "start_pos": 23, "end_pos": 28, "type": "TASK", "confidence": 0.8127313256263733}, {"text": "textual entailment recognition task", "start_pos": 53, "end_pos": 88, "type": "TASK", "confidence": 0.8301043808460236}]}, {"text": "Later on, we combine both modules to obtain the highest possible accuracy rate.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.9849444031715393}]}, {"text": "For this purpose, we analyze the provided corpora by using a lexical module, namely DLSITE-1, and a syntactic one, namely DLSITE-2.", "labels": [], "entities": []}, {"text": "Once all results have been obtained we perform a voting process in order to take into account all system's judgments.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section two describes the system we have built, providing details of the lexical and syntactic perspectives, and explains the difference with the one we presented in RTE-2.", "labels": [], "entities": [{"text": "RTE-2", "start_pos": 166, "end_pos": 171, "type": "DATASET", "confidence": 0.907916247844696}]}, {"text": "Third section presents the experimental results, and the fourth one provides our conclusions and describes possible future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate our system we have generated several results using different combinations of all three mentioned modules.", "labels": [], "entities": []}, {"text": "Since the lexical one uses a machine learning algorithm, it has to be run within a training environment.", "labels": [], "entities": []}, {"text": "For this purpose we have trained our system with the corpora provided in the previous editions of RTE, and also with the development corpus from the current RTE-3 challenge.", "labels": [], "entities": [{"text": "RTE", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.8621273040771484}, {"text": "RTE-3 challenge", "start_pos": 157, "end_pos": 172, "type": "DATASET", "confidence": 0.8098255693912506}]}, {"text": "On the other hand, for the remainder modules the development corpora was used to set the thresholds that determine if the entailment holds.", "labels": [], "entities": []}, {"text": "The performed tests have been obtained by performing different combinations of the described modules.", "labels": [], "entities": []}, {"text": "First, we have calculated the accuracy rates using only each single module separately.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9994608759880066}]}, {"text": "Later on we have combined those developed by our research group for this year's RTE challenge, which are DLSITE-1 (the lexical one) and DLSITE-2 (the syntactic one).", "labels": [], "entities": [{"text": "RTE challenge", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.8809682428836823}]}, {"text": "Finally we have performed a voting process between these two systems and the one we presented in RTE-2.", "labels": [], "entities": [{"text": "RTE-2", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.9364148378372192}]}, {"text": "The combination of DLSITE-1 and DLSITE-2 is described as follows.", "labels": [], "entities": []}, {"text": "If both modules agree, then the judgement is straightforward, but if they do not, we then decide the judgment depending on the accuracy of each one for true and false entailment situations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9987609386444092}]}, {"text": "In our case, DLSITE-1 performs better while dealing with negative examples, so its decision will prevail over the rest.", "labels": [], "entities": [{"text": "DLSITE-1", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.6674713492393494}]}, {"text": "Regarding the combination of the three approaches, we have developed a voting strategy.", "labels": [], "entities": []}, {"text": "The results obtained by our system are represented in.", "labels": [], "entities": []}, {"text": "As it is reflected in such table, the highest accuracy rate obtained using the RTE-3 test corpus was achieved applying only the lexical module, namely DLSITE-1.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 46, "end_pos": 59, "type": "METRIC", "confidence": 0.9872627556324005}, {"text": "RTE-3 test corpus", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.92266712586085}]}, {"text": "On the other hand, the syntactic one had a significantly lower rate, and the same happened with the system we presented in RTE-2.", "labels": [], "entities": [{"text": "RTE-2", "start_pos": 123, "end_pos": 128, "type": "DATASET", "confidence": 0.8726799488067627}]}, {"text": "Therefore, a combination of them will most likely produce less accurate results than the lexical module, as it is shown in.", "labels": [], "entities": []}, {"text": "However, we would like to point out that these results depend heavily on the corpus idiosyncrasy.", "labels": [], "entities": []}, {"text": "This can be proven with the results obtained for the RTE-2 test corpus, where the grouping of the three modules provided the highest accuracy rates of all possible combinations.: Results obtained with the corpora from RTE-2 and RTE-3.", "labels": [], "entities": [{"text": "RTE-2 test corpus", "start_pos": 53, "end_pos": 70, "type": "DATASET", "confidence": 0.94308074315389}, {"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9984971284866333}, {"text": "RTE-2", "start_pos": 218, "end_pos": 223, "type": "DATASET", "confidence": 0.9284509420394897}, {"text": "RTE-3", "start_pos": 228, "end_pos": 233, "type": "DATASET", "confidence": 0.9173239469528198}]}], "tableCaptions": [{"text": " Table 1: Weights assigned to the relevant grammati- cal categories.", "labels": [], "entities": []}, {"text": " Table 3: Results obtained with the corpora from RTE-2 and RTE-3.", "labels": [], "entities": [{"text": "RTE-2", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9644283056259155}, {"text": "RTE-3", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.945890486240387}]}]}