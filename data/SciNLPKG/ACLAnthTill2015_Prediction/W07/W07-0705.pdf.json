{"title": [], "abstractContent": [{"text": "Current statistical machine translation systems handle the translation process as the transformation of a string of symbols into another string of symbols.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.6254221200942993}]}, {"text": "Normally the symbols dealt with are the words in different languages, sometimes with some additional information included, like morphological data.", "labels": [], "entities": []}, {"text": "In this work we try to push the approach to the limit, working not on the level of words, but treating both the source and target sentences as a string of letters.", "labels": [], "entities": []}, {"text": "We try to find out if a nearly unmodified state-of-the-art translation system is able to cope with the problem and whether it is capable to further generalize translation rules, for example at the level of word suffixes and translation of unseen words.", "labels": [], "entities": [{"text": "translation of unseen words", "start_pos": 224, "end_pos": 251, "type": "TASK", "confidence": 0.8428017497062683}]}, {"text": "Experiments are carried out for the translation of Catalan to Spanish.", "labels": [], "entities": [{"text": "translation of Catalan to Spanish", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.933691430091858}]}], "introductionContent": [{"text": "Most current statistical machine translation systems handle the translation process as a \"blind\" transformation of a sequence of symbols, which represent the words in a source language, to another sequence of symbols, which represent words in a target language.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.6288308898607889}]}, {"text": "This approach allows fora relative simplicity of the models, but also has drawbacks, as related word forms, like different verb tenses or pluralsingular word pairs, are treated as completely different entities.", "labels": [], "entities": []}, {"text": "Some efforts have been made e.g. to integrate more information about the words in the form of Part Of Speech tags), using additional information about stems and suffixes) or to reduce the morphological variability of the words ().", "labels": [], "entities": []}, {"text": "State of the art decoders provide the ability of handling different word forms directly in what has been called factored translation models).", "labels": [], "entities": []}, {"text": "In this work, we try to go a step further and treat the words (and thus whole sentences) as sequences of letters, which have to be translated into anew sequence of letters.", "labels": [], "entities": []}, {"text": "We try to find out if the translation models can generalize and generate correct words out of the stream of letters.", "labels": [], "entities": []}, {"text": "For this approach to work we need to translate between two related languages, in which a correspondence between the structure of the words can be found.", "labels": [], "entities": []}, {"text": "For this experiment we chose a Catalan-Spanish corpus.", "labels": [], "entities": []}, {"text": "Catalan is a romance language spoken in the north-east of Spain and Andorra and is considered by some authors as a transitional language between the Iberian Romance languages (e.g. Spanish) and Gallo-Romance languages (e.g. French).", "labels": [], "entities": []}, {"text": "A common origin and geographic proximity result in a similarity between Spanish and Catalan, albeit with enough differences to be considered different languages.", "labels": [], "entities": []}, {"text": "In particular, the sentence structure is quite similar in both languages and many times a nearly monotonical word to word correspondence between sentences can be found.", "labels": [], "entities": []}, {"text": "An example of Catalan and Spanish sentences is given in.", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows: In Section 2 we review the statistical approach to machine translation and consider how the usual techniques can be adapted to the letter translation task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.804461658000946}, {"text": "letter translation task", "start_pos": 173, "end_pos": 196, "type": "TASK", "confidence": 0.8636851708094279}]}, {"text": "In Sec-CatalanPerq\u00f9 ea mi m'agradaria estar-hi dues, una o dues setmanes, m\u00e9s o menys, depenent del preu i cada hotel.", "labels": [], "entities": []}, {"text": "Spanish Porque a m\u00ed me gustar\u00eda quedarme dos, una o dos semanas, m\u00e1s o menos, dependiendo del precio y cada hotel.", "labels": [], "entities": []}, {"text": "English Because I would like to be there two, one or two weeks, more or less, depending on the price of each hotel.", "labels": [], "entities": []}, {"text": "Catalan Si baixa aqu\u00ed tenim una guia de la ciutat que li podem facilitar en la que surt informaci\u00f3 sobre els llocs m\u00e9s interessants de la ciutat.", "labels": [], "entities": []}, {"text": "Spanish Si baja aqu\u00ed tenemos una gu\u00eda de la ciudad que le podemos facilitar en la que sale informaci\u00f3n sobre los sitios m\u00e1s interesantes de la ciudad.", "labels": [], "entities": []}, {"text": "English If you comedown here we have a guidebook of the city that you can use, in there is information about the most interesting places in the city.", "labels": [], "entities": []}, {"text": "tion 3 we present the results of the letter-based translation and show how to use it for improving translation quality.", "labels": [], "entities": []}, {"text": "Although the interest of this work is more academical, in Section 4 we discuss possible practical applications for this approach.", "labels": [], "entities": []}, {"text": "The paper concludes in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The corpus used for our experiment was builtin the framework of the LC-STAR project (.", "labels": [], "entities": [{"text": "LC-STAR project", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.9402528703212738}]}, {"text": "It consists of spontaneous dialogues in Spanish, Catalan and English 2 in the tourism and travelling domain.", "labels": [], "entities": []}, {"text": "The test corpus (and an additional development corpus for parameter optimization) was randomly extracted, the rest of the sentences were used as training data.", "labels": [], "entities": [{"text": "parameter optimization", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7781499028205872}]}, {"text": "Statistics for the corpus can be seen in.", "labels": [], "entities": []}, {"text": "Details of the translation system used can be found in ().", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9709095358848572}]}, {"text": "The results of the word-based and letter-based approaches can be seen in (rows with label \"Full Corpus\").", "labels": [], "entities": []}, {"text": "The high BLEU scores (up to nearly 80%) denote that the quality of the translation is quite good for both systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9991216063499451}, {"text": "translation", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9541873335838318}]}, {"text": "The word- 1.4 1.3: Corpus Statistics based system outperforms the letter-based one, as expected, but the letter-based system also achieves quite a good translation quality.", "labels": [], "entities": []}, {"text": "Example translations for both systems can be found in.", "labels": [], "entities": []}, {"text": "It can be observed that most of the words generated by the letter based system are correct words, and in many cases the \"false\" words that the system generates are very close to actual words (e.g. \"elos\" instead of \"los\" in the second example of).", "labels": [], "entities": []}, {"text": "We also investigated the generalization capabilities of both systems under scarce training data conditions.", "labels": [], "entities": []}, {"text": "It was expected that the greater flexibility of the letter-based system would provide an advantage of the approach when compared to the wordbased approach.", "labels": [], "entities": []}, {"text": "We randomly selected subsets of the training corpus of different sizes ranging from 1 000 sentences to 40 000 (i.e. the full corpus) and computed the translation quality on the same test corpus as before.", "labels": [], "entities": []}, {"text": "Contrary to our hopes, however, the difference in BLEU score between the wordbased and the letter-based system remained fairly constant, as can be seen in, and for representative training corpus sizes.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9820562601089478}]}, {"text": "Nevertheless, the second example in provides an interesting insight into one of the possible practical applications of this approach.", "labels": [], "entities": []}, {"text": "In the example translation of the word-based system, the word \"centreamericans\" was not known to the system (and has been explicitly marked as unknown in).", "labels": [], "entities": []}, {"text": "The letter-based system, however, was able to correctly learn the translation from \"centre-\" to \"centro-\" and that the ending \"-ans\" in Catalan is often translated as \"-anos\" in Spanish, and thus a correct translation has been found.", "labels": [], "entities": []}, {"text": "We thus chose to combine both systems, the word-based system doing most of the translation work, but using the letterbased system for the translation of unknown words.", "labels": [], "entities": [{"text": "translation of unknown words", "start_pos": 138, "end_pos": 166, "type": "TASK", "confidence": 0.8146281391382217}]}, {"text": "The results of this combined approach can be found in under the label \"Combined System\".", "labels": [], "entities": []}, {"text": "The combination of both approaches leads to a 0.5% increase in BLEU using the full corpus as training material.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.8758604526519775}]}, {"text": "This increase is not very big, but is it over a quite strong baseline and the percentage of out-ofvocabulary words in this corpus is around 1% of the total words (see).", "labels": [], "entities": []}, {"text": "When the corpus size is reduced, the gain in BLEU score becomes more important, and for the small corpus size of 1 000 sentences the gain is 2.5% BLEU. and show more details.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.980003833770752}, {"text": "BLEU.", "start_pos": 146, "end_pos": 151, "type": "METRIC", "confidence": 0.9991807341575623}]}], "tableCaptions": [{"text": " Table 2: Translation results for selected corpus sizes. All measures are percentages.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9665753841400146}]}]}