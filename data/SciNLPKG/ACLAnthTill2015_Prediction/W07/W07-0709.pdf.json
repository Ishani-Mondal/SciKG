{"title": [{"text": "Meta-Structure Transformation Model for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.8353867530822754}]}], "abstractContent": [{"text": "We propose a novel syntax-based model for statistical machine translation in which meta-structure (MS) and meta-structure sequence (SMS) of a parse tree are defined.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.6971394817034403}, {"text": "meta-structure sequence (SMS)", "start_pos": 107, "end_pos": 136, "type": "METRIC", "confidence": 0.7534008383750915}]}, {"text": "In this framework, a parse tree is decomposed into SMS to deal with the structure divergence and the alignment can be reconstructed at different levels of recombination of MS (RM).", "labels": [], "entities": []}, {"text": "RM pairs extracted can perform the mapping between the sub-structures across languages.", "labels": [], "entities": []}, {"text": "As a result, we have got not only the translation for the target language, but an SMS of its parse tree at the same time.", "labels": [], "entities": []}, {"text": "Experiments with BLEU metric show that the model significantly outperforms Pharaoh, a state-art-the-art phrase-based system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9918267726898193}]}], "introductionContent": [{"text": "The statistical approach has been widely used in machine translation, which use the noisy-channelbased model.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7965942621231079}]}, {"text": "A joint probability model, proposed by, is a kind of phrasebased one.", "labels": [], "entities": []}, {"text": "gave a framework of alignment templates for this kind of models.", "labels": [], "entities": []}, {"text": "All of the phrase-based models outperformed the word-based models, by automatically learning word and phrase equivalents from bilingual corpus and reordering at the phrase level.", "labels": [], "entities": []}, {"text": "But it has been found that phrases longer than three words have little improvement in the performance.", "labels": [], "entities": []}, {"text": "Above the phrase level, these models have a simple distortion model that reorders phrases independently, without consideration of their contents and syntactic information.", "labels": [], "entities": []}, {"text": "In recent years, applying different statistical learning methods to structured data has attracted various researchers.", "labels": [], "entities": []}, {"text": "Syntax-based MT approaches began with, who introduced the Inversion Transduction Grammars.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9693595767021179}]}, {"text": "Utilizing syntactic structure as the channel input was introduced into MT by.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9091800451278687}]}, {"text": "Syntax-based models have been presented in different grammar formalisms.", "labels": [], "entities": []}, {"text": "The model based on Head-transducer was presented by.", "labels": [], "entities": []}, {"text": "Daniel dealt with the problem of the parse tree isomorphism with a cloning operation to either tree-tostring or tree-to-tree alignment models.", "labels": [], "entities": []}, {"text": "introduced aversion of probabilistic extension of Synchronous Dependency Insertion Grammars (SDIG) to deal with the pervasive structure divergence.", "labels": [], "entities": [{"text": "Synchronous Dependency Insertion Grammars (SDIG)", "start_pos": 50, "end_pos": 98, "type": "TASK", "confidence": 0.7290483883449009}]}, {"text": "All these approaches don't model the translation process, but formalize a model that generates two languages at the same time, which can be considered as some kind of tree transducers.", "labels": [], "entities": []}, {"text": "described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6695377031962076}]}, {"text": "In this paper, we define a model based on the MS decomposition of the parse trees for statistical machine translation, which can capture structural variations and has a proven generation capacity.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.738357812166214}]}, {"text": "During the translation process of our model, the parse tree of the source language is decomposed into different levels of MS and then transformed into the ones of the target language in the form of RM.", "labels": [], "entities": []}, {"text": "The source language can be reordered according to the structure transformation.", "labels": [], "entities": []}, {"text": "At last, the target translation string is generated in the scopes of RM.", "labels": [], "entities": []}, {"text": "In the framework of this model,: MS and the SMS and RM fora given parser tree the RM transformation can be regarded as production rules and be extracted automatically from the bilingual corpus.", "labels": [], "entities": []}, {"text": "The overall translation probability is thus decomposed.", "labels": [], "entities": []}, {"text": "In the rest of this paper, we first give the definitions for MS, SMS, RM and the decomposition of the parse tree in section 2.1, we give a detailed description of our model in section 2.2, section 3 describes the training details and section 4 describes the decoding algorithms, and then the experiment (section 5) proves that our model can outperform the baseline model, pharaoh, under the same condition.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiment was conducted for the task of Chinese-to-English translation.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.6783417463302612}]}, {"text": "A corpus, which consists of 602,701 sentence pairs, was used as the training set.", "labels": [], "entities": []}, {"text": "We took CLDC 863 test set as our test set (http://www.chineseldc.org/resourse.asp), which consists of 467 sentences with an average length of 14.287 Chinese words and 4 references.", "labels": [], "entities": [{"text": "CLDC 863 test set", "start_pos": 8, "end_pos": 25, "type": "DATASET", "confidence": 0.9527646154165268}]}, {"text": "To evaluate the result of the translation, the BLEU metric ( was used.", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9630792140960693}, {"text": "BLEU metric", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9802402257919312}]}], "tableCaptions": [{"text": " Table 1 some examples of the RM transformation", "labels": [], "entities": [{"text": "RM transformation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.935471922159195}]}]}