{"title": [{"text": "Linguistic Features for Automatic Evaluation of Heterogenous MT Systems", "labels": [], "entities": [{"text": "Automatic Evaluation of Heterogenous MT", "start_pos": 24, "end_pos": 63, "type": "TASK", "confidence": 0.4663512110710144}]}], "abstractContent": [{"text": "Evaluation results recently reported by Callison-Burch et al.", "labels": [], "entities": []}, {"text": "(2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9970355033874512}, {"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9784333109855652}]}, {"text": "This happens, for instance , when the systems under evaluation are based on different paradigms, and therefore , do not share the same lexicon.", "labels": [], "entities": []}, {"text": "The reason is that, while MT quality aspects are diverse, BLEU limits its scope to the lexical dimension.", "labels": [], "entities": [{"text": "MT quality", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.8701200783252716}, {"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9915981292724609}]}, {"text": "In this work, we suggest using metrics which take into account linguistic features at more abstract levels.", "labels": [], "entities": []}, {"text": "We provide experimental results showing that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than metrics based on lexical matching alone, specially when the systems under evaluation are of a different nature.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most metrics used in the context of Automatic Machine Translation (MT) Evaluation are based on the assumption that 'acceptable' translations tend to share the lexicon (i.e., word forms) in a predefined set of manual reference translations.", "labels": [], "entities": [{"text": "Automatic Machine Translation (MT) Evaluation", "start_pos": 36, "end_pos": 81, "type": "TASK", "confidence": 0.8389185922486442}]}, {"text": "This assumption works well in many cases.", "labels": [], "entities": []}, {"text": "However, several results in recent MT evaluation campaigns have cast some doubts on its general validity.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.9134609997272491}]}, {"text": "For instance, and reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9900615215301514}]}, {"text": "In particular, they noted that when the systems under evaluation are of a different nature (e.g., rule-based vs. statistical, human-aided vs. fully automatical, etc.)", "labels": [], "entities": []}, {"text": "BLEU may not be a reliable MT quality indicator.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.970032274723053}, {"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9857648015022278}]}, {"text": "The reason is that BLEU favours MT systems which share the expected reference lexicon (e.g., statistical systems), and penalizes those which use a different one.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9755626916885376}, {"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9844838976860046}]}, {"text": "Indeed, the underlying cause is much simpler.", "labels": [], "entities": []}, {"text": "In general, lexical similarity is nor a sufficient neither a necessary condition so that two sentences convey the same meaning.", "labels": [], "entities": []}, {"text": "On the contrary, natural languages are expressive and ambiguous at different levels.", "labels": [], "entities": []}, {"text": "Consequently, the similarity between two sentences may involve different dimensions.", "labels": [], "entities": []}, {"text": "In this work, we hypothesize that, in order to 'fairly' evaluate MT systems based on different paradigms, similarities at more abstract linguistic levels must be analyzed.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9627211093902588}]}, {"text": "For that purpose, we have compiled a rich set of metrics operating at the lexical, syntactic and shallow-semantic levels (see Section 2).", "labels": [], "entities": []}, {"text": "We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by and (see Section 3).", "labels": [], "entities": []}, {"text": "We show that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than those produced by metrics which limit their scope to the lexical dimension, specially when the systems under evaluation are of a different nature.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we study the behavior of some of the metrics described in Section 2, according to the linguistic level at which they operate.", "labels": [], "entities": []}, {"text": "We have selected a set of coarse-grained metric variants (i.e., accumulated/average scores over linguistic units and structures of different kinds) . We analyze some of the cases reported by and.", "labels": [], "entities": []}, {"text": "We distinguish different evaluation contexts.", "labels": [], "entities": []}, {"text": "In Section 3.1, we study the case of a single reference translation being available.", "labels": [], "entities": []}, {"text": "In principle, this scenario should diminish the reliability of metrics based on lexical matching alone, and favour metrics based on deeper linguistic features.", "labels": [], "entities": [{"text": "reliability", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.978939414024353}]}, {"text": "In Section 3.2, we study the case of several reference translations available.", "labels": [], "entities": []}, {"text": "This scenario should alleviate the deficiencies caused by the shallowness of metrics based on lexical matching.", "labels": [], "entities": []}, {"text": "We also analyze separately the case of 'homogeneous' systems (i.e., all systems being of the same nature), and the case of 'heterogenous' systems (i.e., there exist systems based on different paradigms).", "labels": [], "entities": []}, {"text": "As to the metric meta-evaluation criterion, the two most prominent criteria are: Human Acceptability Metrics are evaluated on the basis of correlation with human evaluators.", "labels": [], "entities": [{"text": "Acceptability", "start_pos": 87, "end_pos": 100, "type": "METRIC", "confidence": 0.9377021789550781}]}, {"text": "Human Likeness Metrics are evaluated in terms of descriptive power, i.e., their ability to distinguish between human and automatic translations ().", "labels": [], "entities": []}, {"text": "In our case, metrics are evaluated on the basis of 'Human Acceptability'.", "labels": [], "entities": [{"text": "Acceptability", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.9698489904403687}]}, {"text": "Specifically, we use Pearson correlation coefficients between metric scores and the average sum of adequacy and fluency assessments at the document level.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 21, "end_pos": 40, "type": "METRIC", "confidence": 0.9576418101787567}]}, {"text": "The reason is that meta-evaluation based on 'Human Likeness' requires the availability of heterogenous test beds (i.e., representative sets of automatic outputs and human references), which, unfortunately, is not the case of all the tasks understudy.", "labels": [], "entities": []}, {"text": "First, because most translation systems are statistical.", "labels": [], "entities": []}, {"text": "Second, because inmost cases only one reference translation is available.", "labels": [], "entities": []}, {"text": "In four of the six translation tasks understudy, all the systems are statistical except 'Systran', which is rule-based.", "labels": [], "entities": []}, {"text": "This is the case of the German/Frenchto-English in-domain/out-of-domain tasks.", "labels": [], "entities": []}, {"text": "shows correlation with human assessments for some metric representatives at different linguistic levels.", "labels": [], "entities": []}, {"text": "French-to-English (fr2en) / German-toEnglish (de2en), in-domain and out-of-domain.", "labels": [], "entities": []}, {"text": "Although the four cases are different, we have identified several regularities.", "labels": [], "entities": []}, {"text": "For instance, BLEU and, in general, all metrics based on lexical matching alone, except METEOR, obtain significantly lower levels of correlation than metrics based on deeper linguistic similarities.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9966398477554321}, {"text": "METEOR", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.8029814958572388}, {"text": "correlation", "start_pos": 133, "end_pos": 144, "type": "METRIC", "confidence": 0.9663028120994568}]}, {"text": "The problem with lexical metrics is that they are unable to capture the actual quality of the 'Systran' system.", "labels": [], "entities": []}, {"text": "Interestingly, METEOR obtains a higher correlation, which, in the case of French-to-English, rivals the top-scoring metrics based on deeper linguistic features.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9209172129631042}, {"text": "correlation", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9682808518409729}]}, {"text": "The reason, however, does not seem to be related to its additional linguistic operations (i.e., stemming or synonymy lookup), but rather to the METEOR matching strategy itself (unigram precision/recall).", "labels": [], "entities": [{"text": "METEOR matching", "start_pos": 144, "end_pos": 159, "type": "TASK", "confidence": 0.7083752155303955}, {"text": "precision", "start_pos": 185, "end_pos": 194, "type": "METRIC", "confidence": 0.7874268889427185}, {"text": "recall", "start_pos": 195, "end_pos": 201, "type": "METRIC", "confidence": 0.7527257800102234}]}, {"text": "Metrics at the shallow syntactic level are in the same range of lexical metrics.", "labels": [], "entities": []}, {"text": "At the properly syntactic level, metrics obtain inmost cases high correlation coefficients.", "labels": [], "entities": []}, {"text": "However, the 'DP-HWCw-4' metric, which, although from the viewpoint of de-pendency relationships, still considers only lexical matching, obtains a lower level of correlation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 162, "end_pos": 173, "type": "METRIC", "confidence": 0.964432954788208}]}, {"text": "This reinforces the idea that metrics based on rewarding long n-grams matchings may not be a reliable quality indicator in these cases.", "labels": [], "entities": []}, {"text": "At the level of shallow semantics, while 'NE' metrics are not equally useful in all cases, 'SR' metrics prove very effective.", "labels": [], "entities": []}, {"text": "For instance, correlation attained by 'SR-Or-*' reveals that it is important to translate lexical items according to the semantic role they play inside the sentence.", "labels": [], "entities": [{"text": "correlation", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9690607786178589}, {"text": "translate lexical items", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.8513879776000977}]}, {"text": "Moreover, correlation attained by the 'SR-Mr-*' metric is a clear indication that in order to achieve a high quality, it is important to 'fully' translate 'whole' semantic structures (i.e., arguments/adjuncts).", "labels": [], "entities": [{"text": "correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9588425755500793}]}, {"text": "The existence of all the semantic structures ('SR-Or'), specially associated to the same verb ('SR-Orv'), is also important.", "labels": [], "entities": []}, {"text": "In the two remaining tasks, Spanish-to-English in-domain/out-of-domain, all the systems are statistical.", "labels": [], "entities": []}, {"text": "shows correlation with human assessments for some metric representatives.", "labels": [], "entities": []}, {"text": "In this case, BLEU proves very effective, both in-domain and out-of-domain.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9978826642036438}]}, {"text": "Indeed, all metrics based on lexical matching obtain high levels of correlation with human assessments.", "labels": [], "entities": []}, {"text": "However, still metrics based on deeper linguistic analysis attain inmost cases higher correlation coefficients, although not as significantly higher as in the case of heterogeneous systems.", "labels": [], "entities": [{"text": "correlation", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.9570497274398804}]}], "tableCaptions": [{"text": " Table 3: WMT 2006. 'in' and 'out' columns  show the number of sentences assessed for the 'in- domain' and 'out-of-domain' subtasks. The 'sys'", "labels": [], "entities": [{"text": "WMT 2006", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.923424631357193}]}, {"text": " Table 4: WMT 2006. Evaluation of Heterogeneous  Systems.", "labels": [], "entities": [{"text": "WMT 2006", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.7853901386260986}]}, {"text": " Table 5: WMT 2006. Evaluation of Homogeneous  Systems. Spanish-to-English (es2en), in-domain  and out-of-domain.", "labels": [], "entities": [{"text": "WMT 2006", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.8694272935390472}]}, {"text": " Table 6: NIST 2005. Arabic-to-English (ar2en) ex- ercise. 'ALL' refers to the evaluation of all systems.", "labels": [], "entities": [{"text": "NIST 2005", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.9686929881572723}]}]}