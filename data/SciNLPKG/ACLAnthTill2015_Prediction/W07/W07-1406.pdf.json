{"title": [{"text": "Recognizing Textual Entailment Using Sentence Similarity based on Dependency Tree Skeletons", "labels": [], "entities": [{"text": "Recognizing Textual Entailment", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8876610199610392}, {"text": "Sentence Similarity", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7986203730106354}]}], "abstractContent": [{"text": "We present a novel approach to RTE that exploits a structure-oriented sentence representation followed by a similarity function.", "labels": [], "entities": [{"text": "RTE", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9760069251060486}]}, {"text": "The structural features are automatically acquired from tree skeletons that are extracted and generalized from dependency trees.", "labels": [], "entities": []}, {"text": "Our method makes use of a limited size of training data without any external knowledge bases (e.g. WordNet) or hand-crafted inference rules.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.9248021245002747}]}, {"text": "We have achieved an accuracy of 71.1% on the RTE-3 development set performing a 10-fold cross validation and 66.9% on the RTE-3 test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9996390342712402}, {"text": "RTE-3 development set", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.9645618597666422}, {"text": "RTE-3 test data", "start_pos": 122, "end_pos": 137, "type": "DATASET", "confidence": 0.9578236738840739}]}], "introductionContent": [{"text": "Textual entailment has been introduced as a relation between text expressions, capturing the fact that the meaning of one expression can be inferred from the other ().", "labels": [], "entities": [{"text": "Textual entailment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7766458690166473}]}, {"text": "More precisely, textual entailment is defined as \"\u2026 a relationship between a coherent text T and a language expression, which is considered as a hypothesis, H. We say that T entails H (H is a consequent of T), denoted by T \u21d2 H, if the meaning of H, as interpreted in the context of T, can be inferred from the meaning of T.\" displays several examples from the RTE-3 development set.", "labels": [], "entities": [{"text": "RTE-3 development set", "start_pos": 360, "end_pos": 381, "type": "DATASET", "confidence": 0.9305209120114645}]}, {"text": "For the third pair (id=410) the key knowledge needed to decide whether the entailment relation holds is that \"'s wife,\" entails \"The name of's wife is\", although T contains much more (irrelevant) information.", "labels": [], "entities": []}, {"text": "On the other hand, the first pair (id=1) requires an understanding of concepts with opposite meanings (i.e. \"buy\" and \"sell\"), which is a case of semantic entailment.", "labels": [], "entities": []}, {"text": "The different sources of possible entailments motivated us to consider the development of specialized entailment strategies for different NLP tasks.", "labels": [], "entities": []}, {"text": "In particular, we want to find out the potential connections between entailment relations belonging to different linguistic layers for different applications.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach towards structure-oriented entailment based on our empirical discoveries from the RTE corpora: 1) H is usually textually shorter than T; 2) not all information in T is relevant to make decisions for the entailment; 3) the dissimilarity of relations among the same topics between T and H are of great importance.", "labels": [], "entities": [{"text": "RTE corpora", "start_pos": 125, "end_pos": 136, "type": "DATASET", "confidence": 0.7188206613063812}]}, {"text": "Based on the observations, our primary method starts from H to T (i.e. in the opposite direction of the entailment relation) so as to exclude irrelevant information from T.", "labels": [], "entities": []}, {"text": "Then corresponding key topics and predicates of both elements are extracted.", "labels": [], "entities": []}, {"text": "We then represent the structural differences between T and H by means of a set of Closed-Class Symbols.", "labels": [], "entities": []}, {"text": "Finally, these acquired representations (named Entailment Patterns -EPs) are classified by means of a subsequence kernel.", "labels": [], "entities": []}, {"text": "The Structure Similarity Function is combined with two robust backup strategies, which are responsible for cases that are not handled by the EPs.", "labels": [], "entities": [{"text": "Structure Similarity", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.6892430484294891}]}, {"text": "One is a Triple Similarity Function applied on top of the local dependency relations of T and H; the other is a simple Bag-of-Words (BoW) approach that calculates the overlapping ratio of H and T.", "labels": [], "entities": []}, {"text": "Together, these three methods deal with different entailment cases in practice.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have evaluated four methods: the two backup systems as baselines (BoW and TSM, the Triple Set Matcher) and the kernel method combined with the backup strategies using different parsers, Minipar (Mi+SK+BS) and the Stanford Parser (SP+SK+BS).", "labels": [], "entities": [{"text": "BoW", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.8918588161468506}]}, {"text": "The experiments are based on RTE-3 Data 4 . For the kernel-based classification, we used the classifier SMO from the WEKA toolkit).", "labels": [], "entities": [{"text": "RTE-3 Data 4", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.9352315465609232}, {"text": "SMO", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.8148292303085327}, {"text": "WEKA toolkit", "start_pos": 117, "end_pos": 129, "type": "DATASET", "confidence": 0.9443332552909851}]}, {"text": "Results on RTE-3 Data For the IE task, Mi+SK+BS obtained the highest improvement over the baseline systems, suggesting that the kernel method seems to be more appropriate if the underlying task conveys a more \"relational nature.\"", "labels": [], "entities": [{"text": "RTE-3 Data", "start_pos": 11, "end_pos": 21, "type": "DATASET", "confidence": 0.9050252139568329}, {"text": "IE task", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.926320493221283}, {"text": "BS", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.6544901132583618}]}, {"text": "Improvements in the other tasks are less convincing as compared to the baselines.", "labels": [], "entities": []}, {"text": "Nevertheless, the overall result obtained in experiment B would have been among the top 3 of the RTE-2 challenge.", "labels": [], "entities": [{"text": "RTE-2 challenge", "start_pos": 97, "end_pos": 112, "type": "TASK", "confidence": 0.555684968829155}]}, {"text": "We utilize the system description table of () to compare our system with the best two systems of RTE-2 in Comparison with the top 2 systems in RTE-2.", "labels": [], "entities": [{"text": "RTE-2", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.8655775189399719}]}, {"text": "Note that the best system () applies both shallow and deep techniques, especially in acquiring extra entailment corpora.", "labels": [], "entities": []}, {"text": "The second best system () contains many manually designed logical inference rules and background knowledge.", "labels": [], "entities": []}, {"text": "On the contrary, we exploit no additional knowledge sources besides the dependency trees computed by the parsers, nor any extra training corpora.", "labels": [], "entities": []}, {"text": "Performances of our method For IE pairs, we find good coverage, whereas for IR and QA pairs the coverage is low, though it achieves good accuracy.", "labels": [], "entities": [{"text": "IE pairs", "start_pos": 31, "end_pos": 39, "type": "TASK", "confidence": 0.9059580266475677}, {"text": "coverage", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9629924297332764}, {"text": "coverage", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9941932559013367}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9975395202636719}]}, {"text": "According to the experiments, BoW has already achieved the best performance for SUM pairs cf..", "labels": [], "entities": [{"text": "BoW", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.7467775344848633}]}], "tableCaptions": [{"text": " Table 2 Results on RTE-3 Data  For the IE task, Mi+SK+BS obtained the highest  improvement over the baseline systems, suggesting  that the kernel method seems to be more appropri- ate if the underlying task conveys a more \"rela- tional nature.\" Improvements in the other tasks are  less convincing as compared to the baselines. Nev- ertheless, the overall result obtained in experiment  B would have been among the top 3 of the RTE-2  challenge. We utilize the system description table  of (", "labels": [], "entities": [{"text": "RTE-3 Data", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.8047206401824951}, {"text": "IE task", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.9333349764347076}, {"text": "BS", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.863614022731781}]}, {"text": " Table 4 Performances of our method  For IE pairs, we find good coverage, whereas  for IR and QA pairs the coverage is low, though it  achieves good accuracy. According to the experi- ments, BoW has already achieved the best per- formance for SUM pairs cf.", "labels": [], "entities": [{"text": "coverage", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9867182374000549}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.99788898229599}, {"text": "BoW", "start_pos": 191, "end_pos": 194, "type": "DATASET", "confidence": 0.8582327365875244}]}]}