{"title": [{"text": "Automatically acquiring models of preposition use", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a machine-learning based approach to predict accurately, given a syntactic and semantic context, which preposition is most likely to occur in that context.", "labels": [], "entities": []}, {"text": "Each occurrence of a preposition in an English corpus has its context represented by a vector containing 307 features.", "labels": [], "entities": []}, {"text": "The vectors are processed by a voted perceptron algorithm to learn associations between contexts and prepositions.", "labels": [], "entities": []}, {"text": "In preliminary tests, we can associate contexts and prepositions with a success rate of up to 84.5%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Prepositions have recently become the focus of much attention in the natural language processing community, as evidenced for example by the ACL workshops, a dedicated Sem-Eval task, and The Preposition Project (TPP,.", "labels": [], "entities": []}, {"text": "This is because prepositions play a key role in determining the meaning of a phrase or sentence, and their correct interpretation is crucial for many NLP applications: AI entities which require spatial awareness, natural language generation (e.g. for automatic summarisation, QA, MT, to avoid generating sentences such as *I study at England), automatic error detection, especially for non-native English speakers.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 213, "end_pos": 240, "type": "TASK", "confidence": 0.7011374235153198}, {"text": "automatic error detection", "start_pos": 344, "end_pos": 369, "type": "TASK", "confidence": 0.6327683329582214}]}, {"text": "We present here an approach to learning which preposition is most appropriate in a given context by representing the context as a vector populated by features referring to its syntactic and semantic characteristics.", "labels": [], "entities": []}, {"text": "Preliminary tests on five prepositions -in, of, on, to, with -yield a success rate of between 71% and 84.5%.", "labels": [], "entities": []}, {"text": "In Section 2, we illustrate our motivations for using a vector-based approach.", "labels": [], "entities": []}, {"text": "Section 3 describes the vector creation, and Section 4 the learning procedure.", "labels": [], "entities": [{"text": "vector creation", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.7878254652023315}]}, {"text": "Section 5 presents a discussion of some preliminary results, and Section 6 offers an assessment of our method.", "labels": [], "entities": []}], "datasetContent": [{"text": "One of our motivations in this work was to investigate the practical utility of our context models in an error detection task.", "labels": [], "entities": [{"text": "error detection task", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.7809688647588094}]}, {"text": "The eventual aim is to be able, given a preposition context, to predict the most likely preposition to occur in it: if that differs from the one actually present, we have an error.", "labels": [], "entities": []}, {"text": "Using real learner English as testing material at our current stage of development is too complex, however.", "labels": [], "entities": []}, {"text": "This kind of text presents several challenges for NLP and for our task more specifically, such as spelling mistakes -misspelled words would not be recognised by WordNet or any other lexical item-based component.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 161, "end_pos": 168, "type": "DATASET", "confidence": 0.9465643167495728}]}, {"text": "Furthermore, often a learner's error cannot simply be described in terms of one word needing to be replaced by another, but has a more complex structure.", "labels": [], "entities": []}, {"text": "Although it is our intention to be able to process these kinds of texts eventually, as an interim evaluation we felt that it was best to focus just on texts where the only feature susceptible to error was a preposition.", "labels": [], "entities": []}, {"text": "We therefore devised a simple artificial error detection task using a corpus in which er-rors are artificially inserted in otherwise correct text, for which we present interim results (the dataset is currently quite small) and we compare it against a 'brute force' baseline, namely using the recently released Google n-gram data to predict the most likely preposition.", "labels": [], "entities": [{"text": "error detection", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.7336008250713348}, {"text": "Google n-gram data", "start_pos": 310, "end_pos": 328, "type": "DATASET", "confidence": 0.7563159465789795}]}, {"text": "We setup a task aimed at detecting errors in the use of of and to, for which we had obtained the best results in the basic classification tests reported earlier, and we created for this purpose a small corpus using BBC news articles, as we assume the presence of errors there, spelling or otherwise, is extremely unlikely.", "labels": [], "entities": [{"text": "BBC news articles", "start_pos": 215, "end_pos": 232, "type": "DATASET", "confidence": 0.9434450070063273}]}, {"text": "Errors were created by replacing correct occurrences of one of the prepositions with another, incorrect, one, or inserting of or to in place of other prepositions.", "labels": [], "entities": []}, {"text": "All sentences contained at least one preposition.", "labels": [], "entities": []}, {"text": "Together with a set of sentences where the prepositions were all correct, we obtained a set of 423 sentences for testing, consisting of 492 preposition instances.", "labels": [], "entities": []}, {"text": "The aim was to replicate both kinds of errors one can make in using prepositions . We present here some results from this small scale task; the data was classified by a model of the algorithm trained on the BNC data with all features included, 10 epochs, and d=2.", "labels": [], "entities": [{"text": "BNC data", "start_pos": 207, "end_pos": 215, "type": "DATASET", "confidence": 0.9584528207778931}]}, {"text": "This small task shows that it is possible to use our model to reliably check a text for preposition errors.", "labels": [], "entities": []}, {"text": "However, these results need some kind of baseline for comparison.", "labels": [], "entities": []}, {"text": "The most obvious baseline would be a random choice between positive and negative (i.e. the context matches or does not match the preposition) which we would expect to be successful 50% of the time.", "labels": [], "entities": []}, {"text": "Compared to that the observed accuracies of 75% or more on all of these various classification tasks is clearly significant, representing a 50% or more reduction in the error rate.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9941977262496948}, {"text": "error rate", "start_pos": 169, "end_pos": 179, "type": "METRIC", "confidence": 0.9811147153377533}]}, {"text": "However, we are also working on a more challenging baseline consisting of a simple 3-gram lookup in the Google n-gram corpus (ca. 980 million 3-grams).", "labels": [], "entities": [{"text": "Google n-gram corpus", "start_pos": 104, "end_pos": 124, "type": "DATASET", "confidence": 0.8500838875770569}]}, {"text": "For example, given the phrase fly Paris, we could decide to use to rather than at because we find 10,000 occurrences of fly to Paris and hardly any of fly at Paris.", "labels": [], "entities": []}, {"text": "Ina quick experiment, we extracted 106 three-word sequences, consisting of one word each side of the preposition, from a random sample of the BBC dataset, ensuring each type of error was equally represented.", "labels": [], "entities": [{"text": "BBC dataset", "start_pos": 142, "end_pos": 153, "type": "DATASET", "confidence": 0.9913237690925598}]}, {"text": "For each sequence, we queried the Google corpus for possible prepositions in that sequence, selecting the most frequent one as the answer.", "labels": [], "entities": [{"text": "Google corpus", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.8928899466991425}]}, {"text": "Despite the very general nature of some of the 3-grams (e.g. one of the), this method performs very well: the n-gram method scores 87.5% for of (vs. our 75.8%) and 72.5% for to (vs. our 81.35%).", "labels": [], "entities": []}, {"text": "This is only a suggestive comparison, because the datasets were not of the same size: by the time of the workshop we hope to have a more rigorous baseline to report.", "labels": [], "entities": []}, {"text": "Clearly, unless afflicted by data sparseness, the raw word n-gram method will be very hard to beat, since it will be based on frequently encountered examples of correct usage.", "labels": [], "entities": []}, {"text": "It is therefore encouraging that our method appears to be of roughly comparable accuracy even though we are using no actual word features at all, but only more abstract ones as described earlier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9975870847702026}]}, {"text": "An obvious next step, if this result holds up to further scrutiny, is to experiment with combinations of both types of information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The effect of the d value", "labels": [], "entities": []}, {"text": " Table 2: OF: the effect of various feature categories (d=1)", "labels": [], "entities": [{"text": "OF", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9923600554466248}]}]}