{"title": [{"text": "Active Learning for the Identification of Nonliteral Language *", "labels": [], "entities": [{"text": "Identification of Nonliteral Language", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.8571419268846512}]}], "abstractContent": [{"text": "In this paper we present an active learning approach used to create an annotated corpus of literal and nonliteral usages of verbs.", "labels": [], "entities": []}, {"text": "The model uses nearly unsu-pervised word-sense disambiguation and clustering techniques.", "labels": [], "entities": []}, {"text": "We report on experiments in which a human expert is asked to correct system predictions in different stages of learning: (i) after the last iteration when the clustering step has converged , or (ii) during each iteration of the clustering algorithm.", "labels": [], "entities": []}, {"text": "The model obtains an f-score of 53.8% on a dataset in which literal/nonliteral usages of 25 verbs were annotated by human experts.", "labels": [], "entities": [{"text": "f-score", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.9935053586959839}]}, {"text": "In comparison , the same model augmented with active learning obtains 64.91%.", "labels": [], "entities": []}, {"text": "We also measure the number of examples required when model confidence is used to select examples for human correction as compared to random selection.", "labels": [], "entities": [{"text": "human correction", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.7339845299720764}]}, {"text": "The results of this active learning system have been compiled into a freely available annotated corpus of literal/nonliteral usage of verbs in context.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we propose a largely automated method for creating an annotated corpus of literal vs. nonliteral usages of verbs.", "labels": [], "entities": []}, {"text": "For example, given the verb \"pour\", we would expect our method to identify the sentence \"Custom demands that cognac be poured from a freshly opened bottle\" as literal, and the sentence \"Salsa and rap music pour out of the windows\" as nonliteral, which, indeed, it does.", "labels": [], "entities": []}, {"text": "* This research was partially supported by.", "labels": [], "entities": []}, {"text": "We would like to thank Bill Dolan, Fred Popowich, Dan Fass, Katja Markert, Yudong Liu, and the anonymous reviewers for their comments.", "labels": [], "entities": []}, {"text": "We reduce the problem of nonliteral language recognition to one of word-sense disambiguation (WSD) by redefining literal and nonliteral as two different senses of the same word, and we adapt an existing similarity-based word-sense disambiguation method to the task of separating usages of verbs into literal and nonliteral clusters.", "labels": [], "entities": [{"text": "nonliteral language recognition", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.6542734702428182}, {"text": "word-sense disambiguation (WSD)", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.805292671918869}, {"text": "separating usages of verbs into literal and nonliteral clusters", "start_pos": 268, "end_pos": 331, "type": "TASK", "confidence": 0.7826297945446439}]}, {"text": "Note that treating this task as similar to WSD only means that we use features from the local context around the verb to identify it as either literal or non-literal.", "labels": [], "entities": [{"text": "WSD", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.7224137783050537}]}, {"text": "It does not mean that we can use a classifier trained on WSD annotated corpora to solve this issue, or use any existing WSD classification technique that relies on supervised learning.", "labels": [], "entities": [{"text": "WSD classification", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.9432419836521149}]}, {"text": "We do not have any annotated data to train such a classifier, and indeed our work is focused on building such a dataset.", "labels": [], "entities": []}, {"text": "Indeed our work aims to first discover reliable seed data and then bootstrap a literal/nonliteral identification model.", "labels": [], "entities": [{"text": "literal/nonliteral identification", "start_pos": 79, "end_pos": 112, "type": "TASK", "confidence": 0.6206811368465424}]}, {"text": "Also, we cannot use any semi-supervised learning algorithm for WSD which relies on reliably annotated seed data since we do not possess any reliably labeled data (except for our test data set).", "labels": [], "entities": [{"text": "WSD", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.926919162273407}]}, {"text": "However we do exploit a noisy source of seed data in a nearly unsupervised approach augmented with active learning.", "labels": [], "entities": []}, {"text": "Noisy data containing example sentences of literal and nonliteral usage of verbs is used in our model to cluster a particular instance of a verb into one class or the other.", "labels": [], "entities": []}, {"text": "This paper focuses on the use of active learning using this model.", "labels": [], "entities": []}, {"text": "We suggest that this approach produces a large saving of effort compared to creating such an annotated corpus manually.", "labels": [], "entities": []}, {"text": "An active learning approach to machine learning is one in which the learner has the ability to influence the selection of at least a portion of its training data.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7177790105342865}]}, {"text": "In our approach, a clustering algorithm for literal/nonliteral recognition tries to annotate the examples that it can, while in each iteration it sends a small set of examples to a human expert to annotate, which in turn provides additional benefit to the bootstrapping process.", "labels": [], "entities": [{"text": "literal/nonliteral recognition", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.5995988622307777}]}, {"text": "Our active learn-ing method is similar to the Uncertainty Sampling algorithm of but in our case interacts with iterative clustering.", "labels": [], "entities": [{"text": "Uncertainty Sampling", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.7357657551765442}]}, {"text": "As we shall see, some of the crucial criticisms leveled against uncertainty sampling and in favor of Committee-based sampling do not apply in our case, although the latter may still be more accurate in our task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were performed to determine the best time to send up to 30% of the sentences to the human annotator.", "labels": [], "entities": []}, {"text": "Sending everything after the first iteration produced an average accuracy of 66.8%; sending everything after the third iteration, 65.2%; sending a small amount at each iteration, 60.8%; sending everything after the last iteration, 64.9%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9965782761573792}]}, {"text": "Going just by the average accuracy, the first iteration option seems optimal.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9990159273147583}]}, {"text": "However, several of the individual word results fell catastrophically below the baseline, mainly due to original sentences having been moved into a feedback set too early, causing false attraction.", "labels": [], "entities": []}, {"text": "This risk was compounded in the distributed case, as predicted.", "labels": [], "entities": []}, {"text": "The third iteration option gave slightly better results (0.3%) than the last iteration option, but since the difference was minor, we opted for the stability of sending everything after the last iteration.", "labels": [], "entities": []}, {"text": "These results show an improvement of 11.1% over the model from Section 2.", "labels": [], "entities": []}, {"text": "Individual results for each verb are given in.", "labels": [], "entities": []}, {"text": "Ina second experiment, rather than letting our model select the sentences to send to the human, we selected them randomly.", "labels": [], "entities": []}, {"text": "We found no significant difference in the results.", "labels": [], "entities": []}, {"text": "For the random model to outperform the non-random one it would have to select only sentences that our model would have clustered incorrectly; to do worse it would have to select only sentences that our model could have handled on its own.", "labels": [], "entities": []}, {"text": "The likelihood of the random choices coming exclusively from these two sets is low.", "labels": [], "entities": []}, {"text": "Our third experiment considers the effort-savings of using our literal/nonliteral identification model.", "labels": [], "entities": [{"text": "literal/nonliteral identification", "start_pos": 63, "end_pos": 96, "type": "TASK", "confidence": 0.6239426285028458}]}, {"text": "The main question must be whether the 11.1% accuracy gain of active learning is worth the effort the human must contribute.", "labels": [], "entities": [{"text": "accuracy gain", "start_pos": 44, "end_pos": 57, "type": "METRIC", "confidence": 0.9704939723014832}]}, {"text": "In our experiments, the human annotator is given at most 30% of the sentences to classify manually.", "labels": [], "entities": []}, {"text": "It is expected that the human will classify these correctly and any additional accuracy gain is contributed by the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9993622899055481}]}, {"text": "Without semi-supervised learning, we might expect that if the human were to manually classify 30% of the sentences chosen at random, he would have 30% of the sentences classified correctly.", "labels": [], "entities": []}, {"text": "However, in order to be able to compare the human-only scenario to the active learning scenario, we must find what the average f-score of the manual process is.", "labels": [], "entities": []}, {"text": "The f-score depends on the distribution of literal and nonliteral sentences in the original set.", "labels": [], "entities": []}, {"text": "For example, in a set of 100 sentences, if there are exactly 50 of each, and of the 30 chosen for manual annotation, half come from the literal set and half come from the nonliteral set, the f-score will be exactly 30%.", "labels": [], "entities": [{"text": "f-score", "start_pos": 191, "end_pos": 198, "type": "METRIC", "confidence": 0.9819006323814392}]}, {"text": "We could compare our performance to this, but that would be unfair to the manual process since the sets on which we did our evaluation were by no means balanced.", "labels": [], "entities": []}, {"text": "We base a hypothetical scenario on the heavy imbalance often seen in our evaluation sets, and suggest a situation where 96 of our 100 sentences are literal and only 4 are nonliteral.", "labels": [], "entities": []}, {"text": "If it were to happen that all 4 of the nonliteral sentences were sent to the human, we would get a very high f-score, due to a perfect recall score for the nonliteral cluster and a perfect precision score for the literal cluster.", "labels": [], "entities": [{"text": "f-score", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9875543713569641}, {"text": "recall score", "start_pos": 135, "end_pos": 147, "type": "METRIC", "confidence": 0.9792430996894836}, {"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9987462759017944}]}, {"text": "If none of the four nonliteral sentences were sent to the human, the scores for the nonliteral cluster would be disastrous.", "labels": [], "entities": []}, {"text": "This situation is purely hypothetical, but should account for the fact that 30 out of 100 sentences annotated by a human will not necessarily result in an average f-score of 30%: in fact, averaging the results of the three sitatuations described above results: Active Learning evaluation results.", "labels": [], "entities": []}, {"text": "Baseline refers to the second baseline from Section 2.", "labels": [], "entities": []}, {"text": "Semisupervised: Trust Seed Data refers to the standard KE model that trusts the seed data.", "labels": [], "entities": []}, {"text": "Optimal Semisupervised refers to the augmented KE model described in).", "labels": [], "entities": []}, {"text": "Active Learning refers to the model proposed in this paper.", "labels": [], "entities": []}, {"text": "in an avarage f-score of nearly 36.9%.", "labels": [], "entities": [{"text": "avarage f-score", "start_pos": 6, "end_pos": 21, "type": "METRIC", "confidence": 0.8171217739582062}]}, {"text": "This is 23% higher than the 30% of the balanced case, which is 1.23 times higher.", "labels": [], "entities": []}, {"text": "For this reason, we give the human scores a boost by assuming that whatever the human annotates in the manual scenario will result in an f-score that is 1.23 times higher.", "labels": [], "entities": [{"text": "f-score", "start_pos": 137, "end_pos": 144, "type": "METRIC", "confidence": 0.9532190561294556}]}, {"text": "For our experiment, we take the number of sentences that our active learning method sent to the human for each word -note that this is not always 30% of the total number of sentences -and multiply that by 1.23 -to give the human the benefit of the doubt, so to speak.", "labels": [], "entities": []}, {"text": "Still we find that using active learning gives us an avarage accuracy across all words of 64.9%, while we get only 21.7% with the manual process.", "labels": [], "entities": [{"text": "avarage", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.96551114320755}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.6301470994949341}]}, {"text": "This means that for the same human effort, using the weakly supervised classifier produced a threefold improvement inaccuracy.", "labels": [], "entities": []}, {"text": "Looking at this conversely, this means that in order to obtain an accuracy of 64.9%, by a purely manual process, the human would have to classify nearly 53.6% of the sentences, as opposed to the 17.7% he needs to do using active learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.999139666557312}]}, {"text": "This is an effort-savings of about 35%.", "labels": [], "entities": []}, {"text": "To conclude, we claim that our model combined with active learning is a helpful tool fora literal/nonliteral clustering project.", "labels": [], "entities": [{"text": "literal/nonliteral clustering", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.5759089961647987}]}, {"text": "It can save the human significant effort while still producing reasonable results.", "labels": [], "entities": []}], "tableCaptions": []}