{"title": [{"text": "Using Paraphrases for Parameter Tuning in Statistical Machine Translation", "labels": [], "entities": [{"text": "Parameter Tuning", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.863391786813736}, {"text": "Statistical Machine Translation", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.7253284255663554}]}], "abstractContent": [{"text": "Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6296473443508148}]}, {"text": "It is standard to tune the feature weights in order to maximize a translation quality metric , using held-out test sentences and their corresponding reference translations.", "labels": [], "entities": []}, {"text": "However , obtaining reference translations is expensive.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without a significant decrease in translation quality.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 263, "end_pos": 279, "type": "TASK", "confidence": 0.7147699594497681}]}], "introductionContent": [{"text": "Viewed at a very high level, statistical machine translation involves four phases: language and translation model training, parameter tuning, decoding, and evaluation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.6980623106161753}, {"text": "parameter tuning", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.7263457328081131}]}, {"text": "Since their introduction in statistical MT by, log-linear models have been a standard way to combine sub-models in MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9102703928947449}, {"text": "MT", "start_pos": 115, "end_pos": 117, "type": "TASK", "confidence": 0.9921632409095764}]}, {"text": "Typically such a model takes the form where \u03c6 i are features of the hypothesis e and \u03bb i are weights associated with those features.", "labels": [], "entities": []}, {"text": "Selecting appropriate weights \u03bb i is essential in order to obtain good translation performance.", "labels": [], "entities": []}, {"text": "introduced minimum error rate training (MERT), a technique for optimizing log-linear model parameters relative to a measure of translation quality.", "labels": [], "entities": [{"text": "minimum error rate training (MERT", "start_pos": 11, "end_pos": 44, "type": "METRIC", "confidence": 0.8124793867270151}]}, {"text": "This has become much more standard than optimizing the conditional probability of the training data given the model (i.e., a maximum likelihood criterion), as was common previously.", "labels": [], "entities": []}, {"text": "Och showed that system performance is best when parameters are optimized using the same objective function that will be used for evaluation; BLEU () remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g.,).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9990952014923096}]}, {"text": "Minimum error rate training-and more generally, optimization of parameters relative to a translation quality measure-relies on data sets in which source language sentences are paired with (sets of) reference translations.", "labels": [], "entities": []}, {"text": "It is widely agreed that, at least for the widely used BLEU criterion, which is based on n-gram overlap between hypotheses and reference translations, the criterion is most accurate when computed with as many distinct reference translations as possible.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9973925352096558}]}, {"text": "Intuitively this makes sense: if there are alternative ways to phrase the meaning of the source sentence in the target language, then the translation quality criterion should take as many of those variations into account as possible.", "labels": [], "entities": [{"text": "phrase the meaning of the source sentence", "start_pos": 63, "end_pos": 104, "type": "TASK", "confidence": 0.7773666211536953}]}, {"text": "To do otherwise is to risk the possibility that the criterion might judge good translations to be poor when they fail to match the exact wording within the reference translations that have been provided.", "labels": [], "entities": []}, {"text": "This reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain.", "labels": [], "entities": []}, {"text": "A common source of translated data for MT research is the Linguistic Data Consortium (LDC), where an elaborate process is undertaken that involves translation agencies, detailed translation guidelines, and quality control processes ().", "labels": [], "entities": [{"text": "MT research", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.9545945823192596}, {"text": "Linguistic Data Consortium (LDC)", "start_pos": 58, "end_pos": 90, "type": "DATASET", "confidence": 0.8309308141469955}]}, {"text": "Some efforts have been made to develop alternative processes for eliciting translations, e.g., from users on the Web or from informants in lowdensity languages).", "labels": [], "entities": []}, {"text": "However, reference translations for parameter tuning and evaluation remain a severe data bottleneck for such approaches.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7199397683143616}]}, {"text": "Note, however, one crucial property of reference translations: they are paraphrases, i.e., multiple expressions of the same meaning.", "labels": [], "entities": [{"text": "reference translations", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6972206085920334}]}, {"text": "Automatic techniques exist for generating paraphrases.", "labels": [], "entities": []}, {"text": "Although one would clearly like to retain human translations as the benchmark for evaluation of translation, might it be possible to usefully increase the number of reference translations for tuning by using automatic paraphrase techniques?", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that it is, in fact, possible to do so.", "labels": [], "entities": []}, {"text": "Section 2 briefly describes our translation framework.", "labels": [], "entities": []}, {"text": "Section 3 lays out a novel technique for paraphrasing, designed with the application to parameter tuning in mind.", "labels": [], "entities": [{"text": "paraphrasing", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.9759982228279114}]}, {"text": "Section 4 presents evaluation results using a state of the art statistical MT system, demonstrating that half the human reference translations in a standard 4-reference tuning set can be replaced with automatically generated paraphrases, with no significant decrease in MT system performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9639877676963806}, {"text": "MT", "start_pos": 270, "end_pos": 272, "type": "TASK", "confidence": 0.9804490208625793}]}, {"text": "In Section 5 we discuss related work, and in Section 6 we summarize the results and discuss plans for future research.", "labels": [], "entities": [{"text": "summarize", "start_pos": 58, "end_pos": 67, "type": "TASK", "confidence": 0.9459860324859619}]}], "datasetContent": [{"text": "Having developed a paraphrasing approach based on English-to-English translation, we evaluated its use in improving minimum error rate training for translation from a second language into English.", "labels": [], "entities": [{"text": "English-to-English translation", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.7200755476951599}]}, {"text": "Generating paraphrases via English-to-English translation makes use of a parallel corpus, from which a weighted synchronous grammar is automatically acquired.", "labels": [], "entities": []}, {"text": "Although nothing about our approach requires that the paraphrase system's training bitext be the same one used in the translation experiments (see Section 6), doing so is not precluded, either, and it is a particularly convenient choice when the paraphrasing is being done in support of MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 287, "end_pos": 289, "type": "TASK", "confidence": 0.9758318662643433}]}, {"text": "O = Original Sentence, P = Paraphrase.", "labels": [], "entities": [{"text": "Original Sentence", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.7857929766178131}, {"text": "Paraphrase", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.5952075123786926}]}, {"text": "As the source of development data for minimum error rate training, we used the 919 source sentences and human reference translations from the 2003 NIST Chinese-English MT evaluation exercise.", "labels": [], "entities": [{"text": "NIST Chinese-English MT evaluation exercise", "start_pos": 147, "end_pos": 190, "type": "DATASET", "confidence": 0.7415570497512818}]}, {"text": "As raw material for experimentation, we generated a paraphrase for each reference sentence via 1-best decoding using the English-to-English translation approach of Section 3.", "labels": [], "entities": []}, {"text": "As our test data, we used the 1082 source sentences and human reference translations from the 2005 NIST Chinese-English MT evaluation.", "labels": [], "entities": [{"text": "NIST Chinese-English MT evaluation", "start_pos": 99, "end_pos": 133, "type": "DATASET", "confidence": 0.8441084772348404}]}, {"text": "Our core experiment involved three conditions where the only difference was the set of references for the development set used for tuning feature weights.", "labels": [], "entities": []}, {"text": "For each condition, once the weights were tuned, they were used to decode the test set.", "labels": [], "entities": []}, {"text": "Note that for all the conditions, the decoded test set was always scored against the same four high-quality human reference translations included with the set.", "labels": [], "entities": []}, {"text": "The three experimental conditions were designed around the constraint that our development set contains a total of four human reference translations per sentence, and therefore a maximum of four human references with which to compute an upper bound: \u2022 Baseline (2H): For each item in the development set, we randomly chose two of the four human-constructed reference translations as references for minimum error rate training.", "labels": [], "entities": []}, {"text": "\u2022 Expanded (2H + 2P): For each of the two human references in the baseline tuning set, we automatically generated a corresponding paraphrase using (1-best) English-to-English translation, decoding using the model developed in Section 3.", "labels": [], "entities": [{"text": "Expanded", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9785756468772888}]}, {"text": "This condition represents the critical casein which you have a limited number of hu-man references (two, in this case) and augment them with artificially generated reference translations.", "labels": [], "entities": []}, {"text": "This yields a set of four references for minimum error rate training (two human, two paraphrased), which permits a direct comparison against the upper bound of four humangenerated reference translations.", "labels": [], "entities": []}, {"text": "\u2022 Upper bound: 4H: We performed minimum error rate training using the four human references from the development set.", "labels": [], "entities": [{"text": "error rate", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.8569037616252899}]}, {"text": "In addition to these core experimental conditions, we added a fourth condition to assess the effect on performance when all four human reference translations are used in expanding the reference set via paraphrase: \u2022 Expanded (4H + 4P): This is the same as Condition 2, but using all four human references.", "labels": [], "entities": [{"text": "Expanded", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.961319088935852}]}, {"text": "Note that since we have only four human references per item, this fourth condition does not permit comparison with an upper bound of eight human references.", "labels": [], "entities": []}, {"text": "shows BLEU and TER scores on the test set for all four conditions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9993103742599487}, {"text": "TER", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9981879591941833}]}, {"text": "5 If only two human references were available (simulated by using only two of the available four), expanding to four using paraphrases would yield a clear improvement.", "labels": [], "entities": []}, {"text": "Using bootstrap resampling to compute confidence intervals, we find that the improvement in BLEU score is statistically significant at p < .01.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.983995258808136}]}, {"text": "Equally interesting, expanding the number of reference translations from two to four using paraphrases yields performance that approaches the upper bound obtained by doing MERT using all four human reference translations.", "labels": [], "entities": [{"text": "MERT", "start_pos": 172, "end_pos": 176, "type": "TASK", "confidence": 0.723406970500946}]}, {"text": "The difference in BLEU between conditions 2 and 3 is not significant.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9993341565132141}]}, {"text": "Finally, our fourth condition asks whether it is possible to improve MT performance given the typical four human reference translations used for MERT inmost statistical MT systems, by adding a paraphrase to each one fora total eight references per translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9963877201080322}, {"text": "MT", "start_pos": 169, "end_pos": 171, "type": "TASK", "confidence": 0.897294819355011}]}, {"text": "There is indeed further improvement, although the difference in BLEU score does not reach significance.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9600324332714081}]}, {"text": "We plan to include METEOR scores in future experiments.", "labels": [], "entities": [{"text": "METEOR scores", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.9645618200302124}]}, {"text": "We also evaluated our test set using TER) and observed that the TER scores follow the same trend as the BLEU scores.", "labels": [], "entities": [{"text": "TER", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9927807450294495}, {"text": "TER", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9985033273696899}, {"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9988609552383423}]}, {"text": "Specifically, the TER scores demonstrate that using paraphrases to artificially expand the reference set is better than using only 2 human reference translations and as good as using 4 human reference translations.", "labels": [], "entities": [{"text": "TER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9604883193969727}]}], "tableCaptions": [{"text": " Table 2: Example paraphrases with Chinese as the pivot language. O = Original Sentence, P = Paraphrase.", "labels": [], "entities": [{"text": "O", "start_pos": 66, "end_pos": 67, "type": "METRIC", "confidence": 0.9557543396949768}]}, {"text": " Table 3: Chinese-English corpora used as training  bitext both for paraphrasing and for evaluation.", "labels": [], "entities": []}, {"text": " Table 4: BLEU and TER scores showing utility of  paraphrased reference translations. H = human ref- erences, P = paraphrased references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999029278755188}, {"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9978896975517273}, {"text": "human ref- erences", "start_pos": 90, "end_pos": 108, "type": "METRIC", "confidence": 0.6938979178667068}]}]}