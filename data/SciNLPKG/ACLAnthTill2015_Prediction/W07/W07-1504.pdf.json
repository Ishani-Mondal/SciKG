{"title": [{"text": "Associating Facial Displays with Syntactic Constituents for Generation", "labels": [], "entities": [{"text": "Associating Facial Displays", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8752462069193522}]}], "abstractContent": [{"text": "We present an annotated corpus of conversational facial displays designed to be used for generation.", "labels": [], "entities": []}, {"text": "The corpus is based on a recording of a single speaker reading scripted output in the domain of the target generation system.", "labels": [], "entities": []}, {"text": "The data in the corpus consists of the syntactic derivation tree of each sentence annotated with the full syntactic and pragmatic context, as well as the eye and eyebrow displays and rigid head motion used by the the speaker.", "labels": [], "entities": []}, {"text": "The behaviours of the speaker show several contextual patterns, many of which agree with previous findings on conversational facial displays.", "labels": [], "entities": []}, {"text": "The corpus data has been used in several studies exploring different strategies for selecting facial displays fora synthetic talking head.", "labels": [], "entities": []}], "introductionContent": [{"text": "An increasing number of systems designed to automatically generate linguistic and multimodal output now make use of corpora to help in decisionmaking (cf..", "labels": [], "entities": []}, {"text": "Some implementations use corpora to help select output that is grammatical or fluent; for example, and both used n-gram language models to guide stochastic surface realisers.", "labels": [], "entities": []}, {"text": "In other systems, corpora are used to make decisions based on pragmatic factors such as the reading level of the target user) or the visual features of an object being described.", "labels": [], "entities": []}, {"text": "The latter type of domain-specific contextual information is not often included in generally-available corpora.", "labels": [], "entities": []}, {"text": "For this reason, developers of generation systems that need this type of information often create and make use of application-specific corpora.", "labels": [], "entities": []}, {"text": "The easiest method of including the necessary pragmatic information in a corpus is to base the corpus on output generated in situations where the contextual factors are known; this eliminates the need to annotate these factors explicitly., for example, created a multimodal corpus based on the voice and body language of an actor performing scripted output in the domain of the target generation system: an animated instructor character fora snowboarding video game.", "labels": [], "entities": []}, {"text": "The contextual information in the corpus scripts included the move that the player attempted in the game and the result of that attempt.", "labels": [], "entities": []}, {"text": "Similarly, van created a corpus of multimodal referring expressions produced in specific pragmatic contexts and used it to compare several referring-expression generation algorithms to human performance.", "labels": [], "entities": [{"text": "referring-expression generation", "start_pos": 139, "end_pos": 170, "type": "TASK", "confidence": 0.7277208417654037}]}, {"text": "In this work, the task is to select facial displays for an animated talking head to use while presenting output in the COMIC multimodal dialogue system (), which generates spoken descriptions and comparisons of bathroom-tile options.", "labels": [], "entities": []}, {"text": "The output of the COMIC text planner includes a range of information in addition to the text: the syntactic derivation tree, the user's evaluation of the object being described, the information status (new or old, contrastive) of each fact described, and the predicted speech-synthesiser prosody.", "labels": [], "entities": []}, {"text": "All of this contextual information can be used to help select appropriate facial displays to accompany the spoken presentation; however-as in the other systems mentioned above-this requires a corpus where the full context for every facial display is known.", "labels": [], "entities": []}, {"text": "To create such a corpus, we recorded a speaker performing scripted output in the domain of COMIC.", "labels": [], "entities": [{"text": "COMIC", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.8763729929924011}]}, {"text": "This paper is arranged as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we first describe how the scripts for the corpus were created and how the recording was made.", "labels": [], "entities": []}, {"text": "Section 3 then presents the annotation scheme and the tool that was used to perform the annotation, while Section 4 describes the measures that were taken to ensure that the annotation was reliable.", "labels": [], "entities": []}, {"text": "Section 5 then summarises the high-level patterns that were found in the displays annotated in the corpus and compares them to other findings on conversational facial displays.", "labels": [], "entities": []}, {"text": "At the end of the section, we use the corpus data to test two assumptions that were made in the annotation scheme.", "labels": [], "entities": []}, {"text": "After that, in Section 6, we describe several experiments in which different methods of using the data in this corpus to select facial displays fora synthetic head have been compared.", "labels": [], "entities": []}, {"text": "Finally, in Section 7, we summarise the contributions of this paper and draw some conclusions about the usefulness of this corpus for its intended task.", "labels": [], "entities": []}], "datasetContent": [{"text": "The primary reason for creating this corpus of facial displays was to use the resulting data to select facial displays for the artificial talking head in the COMIC multimodal dialogue system.", "labels": [], "entities": []}, {"text": "Several different strategies have been implemented to use the corpus data for this task, and a number of automated and human evaluations have been carried out comparing the different implementations.", "labels": [], "entities": []}, {"text": "As described in the preceding section, the factor with the largest influence on the displays of the recorded speaker was the user-model evaluation.", "labels": [], "entities": []}, {"text": "Two studies were carried out to test the generality of the characteristic positive and negative displays).", "labels": [], "entities": []}, {"text": "In the first study, users were asked to identify the intended user-model polarity of a description presented by the talking head based only on the facial displays.", "labels": [], "entities": []}, {"text": "The participants were generally able to recognise the characteristic positive and negative facial displays; they also identified the displays intended to be neutral (nodding alone) as positive, and tended to judge videos with no facial displays to be negative.", "labels": [], "entities": []}, {"text": "In the second study, users' subjective preferences were gathered between videos in which the user-model evaluation expressed in speech was either consistent or inconsistent with the facial displays.", "labels": [], "entities": []}, {"text": "In this study, the participants generally preferred the videos that showed consistent content on the two output channels.", "labels": [], "entities": []}, {"text": "In another study), two different data-driven strategies were implemented that used the corpus data to select facial displays to accompany speech.", "labels": [], "entities": []}, {"text": "One strategy always selected the highest-probability option in all contexts, while the other made a stochastic choice among all of the options weighted by the corpus probabilities.", "labels": [], "entities": []}, {"text": "These two strategies were compared against each other using both automated and human evaluation methods: the majority strategy scored more highly on the automated cross-validation, while the weighted strategy was strongly preferred by human judges.", "labels": [], "entities": []}, {"text": "The judges also preferred resynthesised versions of the original facial displays from the corpus to the output of either of the generation strategies.", "labels": [], "entities": []}, {"text": "Two further human evaluation studies compared the weighted data-driven generation strategy from the preceding study to a rule-based strategy that selected the most characteristic displays based only on the user-model evaluation.", "labels": [], "entities": []}, {"text": "When users' subjective judgements were gathered as above, they had a mild preference for the output of the weighted strategy over that of the rulebased strategy.", "labels": [], "entities": []}, {"text": "Ina second study, videos generated by the weighted strategy significantly decreased participants' ability to select descriptions that were correctly tailored to a given set of user preferences, while videos generated by the rule-based strategy had no such impact.", "labels": [], "entities": []}], "tableCaptions": []}