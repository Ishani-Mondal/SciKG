{"title": [], "abstractContent": [{"text": "Tokenization is a necessary and non-trivial step in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6480435232321421}]}, {"text": "In the case of Arabic, where a single word can comprise up to four independent tokens, morphological knowledge needs to be incorporated into the tokenizer.", "labels": [], "entities": []}, {"text": "In this paper we describe a rule-based tokenizer that handles tokenization as a full-rounded process with a preprocessing stage (white space normalizer), and a post-processing stage (token filter).", "labels": [], "entities": []}, {"text": "We also show how it handles multiword expressions, and how ambiguity is resolved.", "labels": [], "entities": []}], "introductionContent": [{"text": "Tokenization is a non-trivial problem as it is \"closely related to the morphological analysis\".", "labels": [], "entities": [{"text": "Tokenization", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.906484842300415}]}, {"text": "This is even more the case with languages with rich and complex morphology such as Arabic.", "labels": [], "entities": []}, {"text": "The function of a tokenizer is to split a running text into tokens, so that they can be fed into a morphological transducer or POS tagger for further processing.", "labels": [], "entities": []}, {"text": "The tokenizer is responsible for defining word boundaries, demarcating clitics, multiword expressions, abbreviations and numbers.", "labels": [], "entities": []}, {"text": "Clitics are syntactic units that do not have free forms but are instead attached to other words.", "labels": [], "entities": []}, {"text": "Deciding whether a morpheme is an affix or a clitic can be confusing.", "labels": [], "entities": []}, {"text": "However, we can generally say that affixes carry morpho-syntactic features (such as tense, person, gender or number), while clitics serve syntactic functions (such as negation, definition, conjunction or preposition) that would otherwise be served by an independent lexical item.", "labels": [], "entities": []}, {"text": "Therefore tokenization is a crucial step fora syntactic parser that needs to build a tree from syntactic units.", "labels": [], "entities": []}, {"text": "An example of clitics in English is the genitive suffix \"'s\" in the student's book.", "labels": [], "entities": []}, {"text": "Arabic clitics, however, are not as easily recognizable.", "labels": [], "entities": []}, {"text": "Clitics use the same alphabet as that of words with no demarcating mark as the English apostrophe, and they can be concatenated one after the other.", "labels": [], "entities": []}, {"text": "Without sufficient morphological knowledge, it is impossible to detect and mark clitics.", "labels": [], "entities": []}, {"text": "In this paper we will show different levels of implementation of the Arabic tokenizer, according to the levels of linguistic depth involved.", "labels": [], "entities": []}, {"text": "Arabic Tokenization has been described in various researches and implemented in many solutions as it is a required preliminary stage for further processing.", "labels": [], "entities": []}, {"text": "These solutions include morphological analysis, diacritization (Nelken and Shieber 2005), Information Retrieval (, and POS Tagging (.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.8680880069732666}, {"text": "Information Retrieval", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.854251891374588}, {"text": "POS Tagging", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.7050360441207886}]}, {"text": "None of these projects, however, show how multiword expressions are treated, or how ambiguity is filtered out.", "labels": [], "entities": []}, {"text": "In our research, tokenization is handled in a rule-based system as an independent process.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.9805071353912354}]}, {"text": "We show how the tokenizer interacts with other transducers, and how multiword expressions are identified and delimited.", "labels": [], "entities": []}, {"text": "We also show how incorrect tokenizations are filtered out, and how undesired tokenizations are marked.", "labels": [], "entities": []}, {"text": "All tools in this research are developed in Finite State Technology.", "labels": [], "entities": []}, {"text": "These tools have been developed to serve an Arabic Lexical Functional Grammar parser using XLE (Xerox Linguistics Environment) platform as part of the ParGram Project (Butt et al 2002).", "labels": [], "entities": [{"text": "Arabic Lexical Functional Grammar parser", "start_pos": 44, "end_pos": 84, "type": "TASK", "confidence": 0.5320546746253967}]}], "datasetContent": [], "tableCaptions": []}