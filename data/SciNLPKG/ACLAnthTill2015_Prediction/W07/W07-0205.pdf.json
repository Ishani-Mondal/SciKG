{"title": [{"text": "TextGraphs-2: Graph-Based Algorithms for Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose to use graph-based diffusion techniques with data-dependent kernels to build unigram language models.", "labels": [], "entities": []}, {"text": "Our approach entails building graphs, where each vertex corresponds uniquely to a word from a closed vocabulary, and the existence of an edge (with an appropriate weight) between two words indicates some form of similarity between them.", "labels": [], "entities": []}, {"text": "In one of our constructions, we place an edge between two words if the number of times these words were seen in a training set differs by at most one count.", "labels": [], "entities": []}, {"text": "This graph construction results in a similarity matrix with small intrinsic dimension, since words with the same counts have the same neighbors.", "labels": [], "entities": []}, {"text": "Experimental results from a benchmark task from language modeling show that our method is competitive with the Good-Turing estimator.", "labels": [], "entities": []}, {"text": "1 Diffusion over Graphs 1.1 Notation Let G = (V, E) bean undirected graph, where V is a finite set of vertices, and E \u2282 V \u00d7 V is the set of edges.", "labels": [], "entities": []}, {"text": "Also, let V be a vocabulary of words, whose probabilities we want to estimate.", "labels": [], "entities": []}, {"text": "Each ver-tex corresponds uniquely to a word, i.e., there is a one-to-one mapping between V and V.", "labels": [], "entities": []}, {"text": "Without loss of generality, we will use V to denote both the set of words and the set of vertices.", "labels": [], "entities": []}, {"text": "Moreover, to simplify notation, we assume that the letters x, y, z will always denote vertices of G.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In our experiments, we used Sections 00-22 (consisting of \u223c 10 6 words) of the UPenn Treebank corpus for training, and Sections 23-24 (consisting of \u223c 10 5 words) for testing.", "labels": [], "entities": [{"text": "UPenn Treebank corpus", "start_pos": 79, "end_pos": 100, "type": "DATASET", "confidence": 0.9911183714866638}]}, {"text": "We split the training set into 10 subsets, leading to 10 datasets of size \u223c 10 5 tokens each.", "labels": [], "entities": []}, {"text": "In all cases, we chose K = 10 5 as the fixed size of our vocabulary.", "labels": [], "entities": []}, {"text": "The results show that \u03c0 ND , the estimate obtained with the Normalized Diffusion, is competitive with the Good-Turing \u03c0 GT . We performed a Kolmogorov-Smirnov test in order to determine if the code-lengths obtained with \u03c0 ND and \u03c0 GT in Table 1 differ significantly.", "labels": [], "entities": []}, {"text": "The result is negative (Pvalue = .65), and the same holds for the larger training set in (P-value=.95).", "labels": [], "entities": [{"text": "Pvalue", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9961767196655273}, {"text": "P-value", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9661346673965454}]}, {"text": "On the other hand, \u03c0 KD (obtained with Kernel Diffusion) is not as efficient, but still better than add-\u03b2 with \u03b2 = 1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results with training set of size \u223c 10 4 .", "labels": [], "entities": []}, {"text": " Table 2: Results with training set of size \u223c 10 5 .", "labels": [], "entities": []}, {"text": " Table 3: Results with training set of size \u223c 10 6 .", "labels": [], "entities": []}]}