{"title": [{"text": "Using Word Dependent Transition Models in HMM based Word Alignment for Statistical Machine Translation", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.6941014528274536}, {"text": "Statistical Machine Translation", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.7613739967346191}]}], "abstractContent": [{"text": "In this paper, we present a Bayesian Learning based method to train word dependent transition models for HMM based word alignment.", "labels": [], "entities": [{"text": "HMM based word alignment", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.8079793602228165}]}, {"text": "We present word alignment results on the Canadian Hansards corpus as compared to the conventional HMM and IBM model 4.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7630444467067719}, {"text": "Canadian Hansards corpus", "start_pos": 41, "end_pos": 65, "type": "DATASET", "confidence": 0.912121057510376}]}, {"text": "We show that this method gives consistent and significant alignment error rate (AER) reduction.", "labels": [], "entities": [{"text": "alignment error rate (AER) reduction", "start_pos": 58, "end_pos": 94, "type": "METRIC", "confidence": 0.9248480200767517}]}, {"text": "We also conducted machine translation (MT) experiments on the Europarl corpus.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.8203010141849518}, {"text": "Europarl corpus", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.9928688406944275}]}, {"text": "MT results show that word alignment based on this method can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.4756641685962677}, {"text": "word alignment", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.7875235974788666}, {"text": "phrase-based machine translation", "start_pos": 74, "end_pos": 106, "type": "TASK", "confidence": 0.6264289617538452}, {"text": "BLEU score", "start_pos": 156, "end_pos": 166, "type": "METRIC", "confidence": 0.9820254147052765}]}], "introductionContent": [{"text": "Word alignment is an important step of most modern approaches to statistical machine translation (.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7551375925540924}, {"text": "statistical machine translation", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.6819023489952087}]}, {"text": "The classical approaches to word alignment are based on IBM models 1-5 ( and the HMM based alignment model ()), while recently discriminative approaches) and syntax based approaches (Zhang and) for word alignment are also studied.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.8370618224143982}, {"text": "word alignment", "start_pos": 198, "end_pos": 212, "type": "TASK", "confidence": 0.8082724213600159}]}, {"text": "In this paper, we present improvements to the HMM based alignment model originally proposed by).", "labels": [], "entities": [{"text": "HMM based alignment", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6021949350833893}]}, {"text": "Although HMM based word alignment approaches give good performance, one weakness of it is the coarse transition models.", "labels": [], "entities": [{"text": "HMM based word alignment", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.6644304990768433}]}, {"text": "In the HMM based alignment model (, it is assumed that the HMM transition probabilities depend only on the jump width from the last state to the next state.", "labels": [], "entities": []}, {"text": "Therefore, the knowledge of transition probabilities given a particular source word e is not sufficiently modeled.", "labels": [], "entities": []}, {"text": "In order to improve transition models in the HMM based alignment, extended the transition models to be word-class dependent.", "labels": [], "entities": []}, {"text": "In that approach, words of the source language are first clustered into a number of word classes, and then a set of transition parameters is estimated for each word class.", "labels": [], "entities": []}, {"text": "In (2002), Toutanova et al. modeled self-transition (i.e., jump width is zero) probability separately from other transition probabilities.", "labels": [], "entities": []}, {"text": "A word dependent self-transition model P(stay|e) is introduced to decide whether to stay at the current source word eat the next step, or jump to a different word.", "labels": [], "entities": []}, {"text": "It was also shown that with the assumption that a source word with fertility greater than one generates consecutive words in the target language, this probability approximates fertility modeling.", "labels": [], "entities": []}, {"text": "They proposed a word-to-phrase HMM in which a source word dependent phrase length model is used to model the approximate fertility, i.e., the length of consecutive target words generated by the source word.", "labels": [], "entities": []}, {"text": "It provides more powerful modeling of approximate fertility than the single P(stay|e) parameter.", "labels": [], "entities": [{"text": "P(stay|e) parameter", "start_pos": 76, "end_pos": 95, "type": "METRIC", "confidence": 0.9308754120554242}]}, {"text": "However, these methods only model the probability of state occupancy rather than a full set of transition probabilities.", "labels": [], "entities": []}, {"text": "Important knowledge of jumping from e to another position, e.g., jumping forward (monotonic alignment) or jumping backward (non-monotonic alignment), is not modeled.", "labels": [], "entities": []}, {"text": "In this paper, we present a method to further improve the transition models for HMM alignment model.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.9008737802505493}]}, {"text": "For each source word e, we not only model its self-transition probability, but also the probability of jumping from word e to a different word.", "labels": [], "entities": []}, {"text": "For this purpose, we estimate a full transition model for each source word.", "labels": [], "entities": []}, {"text": "A key problem for detailed word-dependent transition modeling is data sparsity.", "labels": [], "entities": [{"text": "word-dependent transition modeling", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.6604798138141632}]}, {"text": "In (), the word dependent self-transition probability P(stay|e) is interpolated with the global HMM self-transition probability to alleviate the data sparsity problem, where an interpolation weight is used for all words and that weight is tuned on a hold-out set.", "labels": [], "entities": []}, {"text": "In the proposed word dependent transition model, because there area large number of parameters to estimate, the data sparsity problem is even more severe.", "labels": [], "entities": []}, {"text": "Moreover, since the sparsity of different words are very different, it is difficult to find a one-size-fits-all interpolation weight, and therefore simple linear interpolation is not optimal.", "labels": [], "entities": []}, {"text": "In order to address this problem, we use Bayesian learning so that the transition model parameters are estimated by maximum a posteriori (MAP) training.", "labels": [], "entities": []}, {"text": "With the help of the prior distribution of the model, the training is regularized and results in robust models.", "labels": [], "entities": []}, {"text": "In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model ().", "labels": [], "entities": []}, {"text": "Then we describe the equations of MAP training for word dependent transition models.", "labels": [], "entities": [{"text": "MAP training", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.890961617231369}]}, {"text": "In section 5, we present word alignment results that show significant alignment error rate reductions compared to the baseline HMM and IBM model 4.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7748996317386627}, {"text": "alignment error rate", "start_pos": 70, "end_pos": 90, "type": "METRIC", "confidence": 0.7044750054677328}]}, {"text": "We also conducted phrase-based machine translation experiments on the Europarl corpus, EnglishFrench track, and shown that the proposed method can lead to significant BLEU score improvement compared to the HMM and IBM model 4.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.6412043968836466}, {"text": "Europarl corpus", "start_pos": 70, "end_pos": 85, "type": "DATASET", "confidence": 0.9958276152610779}, {"text": "EnglishFrench track", "start_pos": 87, "end_pos": 106, "type": "DATASET", "confidence": 0.9050901234149933}, {"text": "BLEU score", "start_pos": 167, "end_pos": 177, "type": "METRIC", "confidence": 0.9751373529434204}]}, {"text": ", and the transition probability only depends on the position of the last state and the length of the English sentence, i.e., In (Och and Ney, 2000a), the word null is introduced to generate the French words that don't align to any English words.", "labels": [], "entities": []}, {"text": "If we denote by j_ the position of the last French word before j that aligns to a non-null English word, the transition probabilities state i=0 denotes the state of a null word at the English side, and p 0 is the probability of jumping to state 0, which is estimated from hold-out data.", "labels": [], "entities": []}, {"text": "For convenience, we denote by the HMM parameter set.", "labels": [], "entities": [{"text": "HMM parameter set", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.8048170606295267}]}, {"text": "In the training stage, \u039b are usually estimated through maximum likelihood (ML) training, i.e., arg max ( | , ) and the efficient Expectation-Maximization algorithm can be used to optimize \u039b iteratively until convergence.", "labels": [], "entities": [{"text": "arg max", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9840063452720642}]}, {"text": "For the interest of this paper, we elaborate transition parameter estimation with more details.", "labels": [], "entities": []}, {"text": "These transition probabilities { } ( | , ) pi i I \u2032 is a multinomial distribution estimated according to, whereat each iteration the distortion set {c(i -i')} is the fractional count of transitions with jump width d = i -i', i.e., where \u039b' is the model obtained from the immediate previous iteration and these terms in (4) can be efficiently computed by using the ForwardBackward algorithm.", "labels": [], "entities": []}, {"text": "In practice, we can bucket the distortion parameters {c(d)} into a few buckets as implemented in ().", "labels": [], "entities": []}, {"text": "In our implementation, 15 buckets are used for c(\u2264-7), c(-6), ...", "labels": [], "entities": []}, {"text": "c(0), ..., c(\u22657).", "labels": [], "entities": []}, {"text": "The probability mass for transitions with jump width larger than 6 is uniformly divided.", "labels": [], "entities": []}, {"text": "As suggested in (), we also use two separate sets of distortion parameters for transitioning into the first state, and for transitioning out of the last state, respectively.", "labels": [], "entities": []}, {"text": "Finally, we further smooth transition probabilities with a uniform distribution as described in), After training, Viterbi decoding is used to find the best alignment sequence \u02c6 J a . i.e., Compared to which provides a much richer set of free parameters to model transition probabilities.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of test set AER between vari- ous models trained on 500K sentence pairs. All numbers  are in percentage.", "labels": [], "entities": [{"text": "AER", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9928330183029175}]}, {"text": " Table 2: Comparison of test set Precision between  various models trained on 500K sentence pairs. All  numbers are in percentage.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of test set Recall between vari- ous models trained on 500K sentence pairs. All numbers  are in percentage.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of BLEU scores on devtest, test,  and nc-test set between various word alignment models.  All numbers are in percentage.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9965044260025024}, {"text": "word alignment", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.6792501211166382}]}, {"text": " Table 5: Statistical significance test of the BLEU im- provement of WDHMM (\u03c4 = 100) vs. HMM baseline,  and WDHMM (\u03c4 = 100) vs. IBM model 4 on devtest,  test, and nc-test sets.", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.8292343616485596}, {"text": "BLEU im- provement", "start_pos": 47, "end_pos": 65, "type": "METRIC", "confidence": 0.902021735906601}, {"text": "WDHMM", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.8678830862045288}, {"text": "HMM baseline", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.8354919254779816}, {"text": "WDHMM", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.7438247203826904}]}, {"text": " Table 6: comparison of runtime performance bew- teen WDHMM training and IBM model 4 training using  GIZA++.", "labels": [], "entities": []}]}