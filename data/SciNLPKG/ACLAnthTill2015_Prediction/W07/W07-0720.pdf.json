{"title": [{"text": "Ngram-based statistical machine translation enhanced with multiple weighted reordering hypotheses", "labels": [], "entities": [{"text": "Ngram-based statistical machine translation", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.4769827201962471}]}], "abstractContent": [{"text": "This paper describes the 2007 Ngram-based statistical machine translation system developed at the TALP Research Center of the UPC (Uni-versitat Polit\u00e8cnica de Catalunya) in Barcelona.", "labels": [], "entities": [{"text": "Ngram-based statistical machine translation", "start_pos": 30, "end_pos": 73, "type": "TASK", "confidence": 0.532702125608921}, {"text": "TALP Research Center", "start_pos": 98, "end_pos": 118, "type": "DATASET", "confidence": 0.7237431704998016}]}, {"text": "Emphasis is put on improvements and extensions of the previous years system, being highlighted and empirically compared.", "labels": [], "entities": []}, {"text": "Mainly, these include a novel word ordering strategy based on: (1) statistically monotonizing the training source corpus and (2) a novel reordering approach based on weighted reordering graphs.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.7798338830471039}]}, {"text": "In addition, this system introduces a target language model based on statistical classes, a feature for out-of-domain units and an improved optimization procedure.", "labels": [], "entities": []}, {"text": "The paper provides details of this system participation in the ACL 2007 SECOND WORKSHOP ON STATISTICAL MACHINE TRANSLATION.", "labels": [], "entities": [{"text": "ACL 2007 SECOND WORKSHOP", "start_pos": 63, "end_pos": 87, "type": "DATASET", "confidence": 0.6907661557197571}, {"text": "STATISTICAL MACHINE TRANSLATION", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.5006140867869059}]}, {"text": "Results on three pairs of languages are reported, namely from Spanish, French and Ger-man into English (and the other way round) for both the in-domain and out-of-domain tasks.", "labels": [], "entities": [{"text": "Ger-man", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.7139107584953308}]}], "introductionContent": [{"text": "Based on estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternatively to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in ().", "labels": [], "entities": [{"text": "Ngram-based SMT", "start_pos": 91, "end_pos": 106, "type": "TASK", "confidence": 0.5506421327590942}]}, {"text": "Given the challenge of domain adaptation, efforts have been focused on improving strategies for Ngram-based SMT which could generalize better.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7599729001522064}, {"text": "Ngram-based SMT", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.5897835195064545}]}, {"text": "Specifically, a novel reordering strategy is explored.", "labels": [], "entities": []}, {"text": "It is based on extending the search by using precomputed statistical information.", "labels": [], "entities": []}, {"text": "Results are promising while keeping computational expenses at a similar level as monotonic search.", "labels": [], "entities": []}, {"text": "Additionally, a bonus for tuples from the out-of-domain corpus is introduced, as well as a target language model based on statistical classes.", "labels": [], "entities": []}, {"text": "One of the advantages of working with statistical classes is that they can easily be used for any pair of languages.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly reviews last year's system, including tuple definition and extraction, translation model and feature functions, decoding tool and optimization criterion.", "labels": [], "entities": [{"text": "tuple definition and extraction", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.8144812881946564}, {"text": "translation model", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.8837428092956543}]}, {"text": "Section 3 delves into the word ordering problem, by contrasting last year strategy with the novel weighted reordering input graph.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.8533782064914703}]}, {"text": "Section 4 focuses on new features: both tuple-domain bonus and target language model based on classes.", "labels": [], "entities": []}, {"text": "Later on, Section 5 reports on all experiments carried out for WMT 2007.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.8215326368808746}, {"text": "WMT 2007", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.8854418098926544}]}, {"text": "Finally, Section 6 sums up the main conclusions from the paper and discusses future research lines.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main difference between this year's and last year's systems are: the amount of data provided; the word alignment; the Spanish morphology reduction; the reordering technique; the extra target language model based on statistical classes (except for the En2Es); and the bonus for the out-of-domain task (only for the En2Es task).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 102, "end_pos": 116, "type": "TASK", "confidence": 0.7150917202234268}]}, {"text": "Among them, the most important is the reordering technique.", "labels": [], "entities": [{"text": "reordering", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.9617910385131836}]}, {"text": "That is why we provide a fair comparison between the reordering patterns ( ) technique and the SMR reordering technique.", "labels": [], "entities": [{"text": "SMR reordering", "start_pos": 95, "end_pos": 109, "type": "TASK", "confidence": 0.8907326459884644}]}, {"text": "shows the system described above using either reordering patterns or the SMR technique.", "labels": [], "entities": [{"text": "SMR", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9588598012924194}]}, {"text": "The BLEU calculation was case insensitive and sensitive to tokenization.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9951400756835938}, {"text": "tokenization", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.9699800610542297}]}, {"text": "2006 and reordering patterns in the English/Spanish in-domain task comes from the combination of: the additional corpora, the word alignment, the Spanish morphology reduction and the extra target language model based on classes (only in the Es2En direction).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 126, "end_pos": 140, "type": "TASK", "confidence": 0.6676588803529739}]}], "tableCaptions": [{"text": " Table 1: BLEU comparison: reordering patterns vs. SMR  technique.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9966615438461304}, {"text": "SMR", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9575164318084717}]}, {"text": " Table 2: BLEU scores for each of the six translation di- rections considered (computed over 2006 test set) com- paring last year's and this year's system results (in- domain and out-domain).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991723299026489}]}]}