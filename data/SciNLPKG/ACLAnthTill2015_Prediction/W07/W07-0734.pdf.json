{"title": [{"text": "Meteor: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments", "labels": [], "entities": [{"text": "MT Evaluation", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9693712890148163}]}], "abstractContent": [{"text": "Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric.", "labels": [], "entities": [{"text": "Machine Translation evaluation", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.9268844127655029}]}, {"text": "It is one of several automatic metrics used in this year's shared task within the ACL WMT-07 workshop.", "labels": [], "entities": [{"text": "ACL WMT-07 workshop", "start_pos": 82, "end_pos": 101, "type": "DATASET", "confidence": 0.8287467757860819}]}, {"text": "This paper recaps the technical details underlying the metric and describes recent improvements in the metric.", "labels": [], "entities": []}, {"text": "The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English.", "labels": [], "entities": [{"text": "MT", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.9789552688598633}]}], "introductionContent": [{"text": "Automatic Metrics for MT evaluation have been receiving significant attention in recent years.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9819630980491638}]}, {"text": "Evaluating an MT system using such automatic metrics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators.", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9845166802406311}]}, {"text": "Automatic metrics are useful for comparing the performance of different systems on a common translation task, and can be applied on a frequent and ongoing basis during MT system development.", "labels": [], "entities": [{"text": "MT system development", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.8958484530448914}]}, {"text": "The most commonly used MT evaluation metric in recent years has been IBM's Bleu metric ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9489502012729645}, {"text": "Bleu metric", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.8648054599761963}]}, {"text": "Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9011461138725281}, {"text": "parameter optimization training", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.7918300231297811}, {"text": "MT", "start_pos": 171, "end_pos": 173, "type": "TASK", "confidence": 0.7600134611129761}]}, {"text": "Various researchers have noted, however, various weaknesses in the metric.", "labels": [], "entities": []}, {"text": "Most notably, Bleu does not produce very reliable sentence-level scores.", "labels": [], "entities": []}, {"text": "Meteor , as well as several other proposed metrics such as GTM (), TER) and CDER () aim to address some of these weaknesses.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8557668924331665}, {"text": "TER", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9958951473236084}]}, {"text": "Meteor , initially proposed and released in 2004 () was explicitly designed to improve correlation with human judgments of MT quality at the segment level.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7914615273475647}, {"text": "MT", "start_pos": 123, "end_pos": 125, "type": "TASK", "confidence": 0.9895250797271729}]}, {"text": "Previous publications on Meteor () have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.8925666213035583}, {"text": "Bleu", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.948465883731842}, {"text": "MT evaluation", "start_pos": 158, "end_pos": 171, "type": "TASK", "confidence": 0.8994555771350861}]}, {"text": "This paper recaps the technical details underlying Meteor and describes recent improvements in the metric.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9493709802627563}]}, {"text": "The latest release extends Meteor to support evaluation of MT output in Spanish, French and German, in addition to English.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 27, "end_pos": 33, "type": "DATASET", "confidence": 0.7208858132362366}, {"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9750397205352783}]}, {"text": "Furthermore, several parameters within the metric have been optimized on language-specific training data.", "labels": [], "entities": []}, {"text": "We present experimental results that demonstrate the improvements in correlations with human judgments that result from these parameter tunings.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Corpus Statistics for Various Languages", "labels": [], "entities": []}, {"text": " Table 2: Optimal Values of Tuned Parameters for  Different Criteria for English", "labels": [], "entities": []}, {"text": " Table 3: Pearson Correlation with Human Judg- ments on Test Data for English", "labels": [], "entities": [{"text": "Pearson Correlation with Human Judg- ments", "start_pos": 10, "end_pos": 52, "type": "METRIC", "confidence": 0.6772121276174273}]}, {"text": " Table 4: Tuned Parameters for Different Languages", "labels": [], "entities": []}]}