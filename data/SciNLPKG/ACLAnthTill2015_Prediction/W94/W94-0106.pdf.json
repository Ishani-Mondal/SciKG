{"title": [{"text": "DO WE NEED LINGUISTICS WHEN WE HAVE STATISTICS? A COMPARATIVE ANALYSIS OF THE CONTRIBUTIONS OF LINGUISTIC CUES TO A STATISTICAL WORD GROUPING SYSTEM", "labels": [], "entities": [{"text": "ANALYSIS", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.6158857941627502}]}], "abstractContent": [{"text": "We present a comparative analysis of the performance of a statistics-based system for the formation of semantic groups of adjectives when various sources of linguistic knowledge are introduced.", "labels": [], "entities": []}, {"text": "We identify four different types of slufllow linguistic knowledge that are applicable to this system, and we quantify the performance gained by incorporating each such knowledge module, We perform experiments for different corpus sizes and different inputs (sets of adjectives to group), collect clam on the usaful.ness of each linguistic module, assess the statistical significance of the results, and compare the contributions of the linguistic knowledge sources against each other.", "labels": [], "entities": []}, {"text": "We also assess the overall effect linguistic knowledge has in our system.", "labels": [], "entities": []}, {"text": "Our results show that linguistic knowledge causes a significant increase in the performance of the system.", "labels": [], "entities": []}, {"text": "We conclude by discussing how these positive restdts can be generalized to other problems in statistical NLP.", "labels": [], "entities": []}], "introductionContent": [{"text": "The idea of integrating statistical and knowledge-based approaches for natural language problems, has been. recently.", "labels": [], "entities": []}, {"text": "gainin$ ground, in. the computational lingmsucs commumty, as it is expected that a combined approach will offer significantly better performance over either methodology alone.", "labels": [], "entities": []}, {"text": "This paper supplements this intuitive belief with actual evaluatzon data, obtained when several linguistics-based modules were integrated in a statistical system.", "labels": [], "entities": []}, {"text": "We used a system we previously developed for the separation of adjectives into semantic groups as the basis for our comparative analysis.", "labels": [], "entities": [{"text": "comparative analysis", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.8551986813545227}]}, {"text": "We identified several different types of shallow linguistic knowledge that can be efficiently introduced into our system.", "labels": [], "entities": []}, {"text": "We evaluated the system with and 43 without each such feature, obtaining an estimate of each feature's positive or negative contribution to the overall performance.", "labels": [], "entities": []}, {"text": "By matching cases where all system parameters are the same except for one teature, we assess the statistical significance of the differences found.", "labels": [], "entities": []}, {"text": "Also, a statistical model of the system's performance in terms of the active features for each run offers a view of the contributions of features from a different angle, contrasting the significance of linguistic features (or other modeled system parameters) against each other.", "labels": [], "entities": []}, {"text": "Our analysis of the experimental results showed that many forms of li%.uistic knowledge have a significant positive conmbution to the performance of the system.", "labels": [], "entities": []}, {"text": "We attribute to the combined effect of the linguistic knowledge modules the ability of our system to perform fine-tuned classification of adjectives into semantic classes.", "labels": [], "entities": []}, {"text": "Other statistical systems that address word classification probleans do not emphasize the use of linguistic knowledge and do not deal with a specific word class, or do not exploit as much linguistic knowledge as we do.", "labels": [], "entities": [{"text": "word classification", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7100093513727188}]}, {"text": "As a result, a coarser classification is usually produced.", "labels": [], "entities": [{"text": "coarser", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9849451184272766}]}, {"text": "In contrast, by limiting the system's input to adjectives, we can take advantage of specific syntactic relationships and additional faltering procedures that apply only to ,particular word classes.", "labels": [], "entities": []}, {"text": "These sources of lingmstic knowledge provide in turn the extra eedgc for discriminating among the adjectives at the semantic level.", "labels": [], "entities": []}, {"text": "Our\" adjective grouping system can be used for applications such as natural lansuage generation (where knowledge of the semanuc groups and of the ordering of the elements within them allows the precise lexiealization of semantic concepts) and computational lexicography (by automatically eompifing domain-dependent lists of synonyms and antonyms).", "labels": [], "entities": [{"text": "natural lansuage generation", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6927615602811178}]}, {"text": "The produced groups can also help correct erroneous usage of multiple qualifiers that are superfluc~ts or contradict each other, a phenomenon that has been observed in medical reports 1.", "labels": [], "entities": []}, {"text": "But in addition to the immediate applications of word classification, many other sfatistical NLP applications can be cast in a similar framework.", "labels": [], "entities": [{"text": "word classification", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7799563705921173}]}, {"text": "Therefore, the positive effects of linguistic knowledge on our system indicate that the incorpo/'ation of linguistic knowledge will probably result in similar b~efits for other applications as well.", "labels": [], "entities": []}, {"text": "In what follows, we briefly review our adjective grouping system, and then present the !ingui.'stic features we explored and the alternatives tor each of them.", "labels": [], "entities": []}, {"text": "In Section 5 we give the results of our evaluation on different combinations of features and we analyze their significance.", "labels": [], "entities": []}, {"text": "We also ~rresent these results in a predictor-response amework, and we conclude by discussing the applicability of our results to other NLP problems.", "labels": [], "entities": [{"text": "rresent", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9596202969551086}]}], "datasetContent": [{"text": "In the previous section we identified four parameters of the system, the effects of which we want to analyze.", "labels": [], "entities": []}, {"text": "But in addition to these parameters that can be directly varied and have predetermined possible values, several other variables can affect the performance of the system.", "labels": [], "entities": []}, {"text": "First, the performance of the system depends naturally on the adjective set that is to be clustered.", "labels": [], "entities": []}, {"text": "Presumably variations in the adjective set can be modeled by several parameters, such assize of the set, number of semantic groups in it, and strength of semantic relatedness among its members, plus several parameters describing the properties of the adjectives in the set in isolation, such as frequency, specificity, etc.", "labels": [], "entities": []}, {"text": "A second variable that affects the clustering is the corpus that is used as the main knowledge source, through the observed cooeeurrence patterns.", "labels": [], "entities": []}, {"text": "Again the effects of different corpora can be separatecl into several factors, e.g. the size of the corpus, its generality, the genre of the texts, etc.", "labels": [], "entities": []}, {"text": "Since in this paper we are interested in quantifying the effect of the linguistic knowlcdse in our system, or more precisely of the linguistic knowledge that we can explicitly control through the four parameters discussed above, we did not attempt to model in detail the various factors entering the system as a result of the choice of ad-  .~ective set and corpus.", "labels": [], "entities": []}, {"text": "However, we are interested in measuring the effects of the linguistic parameters in a wide range of contexts, and m correlating these effects with variables originating from the choice of corpus and adjective set.", "labels": [], "entities": []}, {"text": "For example, we would want to be able to detect that the linguistic parameter \"morphology\" is significant for small corpora but not for large ones, if that were the cease.", "labels": [], "entities": []}, {"text": "Therefore, we included in our model two additional parameters, representing the corpus and the adjective set used.", "labels": [], "entities": []}, {"text": "We used the Wall Street Journal articles from the ACL-DCI as our corpus.", "labels": [], "entities": [{"text": "Wall Street Journal articles from the ACL-DCI", "start_pos": 12, "end_pos": 57, "type": "DATASET", "confidence": 0.9626610279083252}]}, {"text": "We selected four subcorpora of decreasing size to study the relationship of corpus size with linguistic feature effects: all the 1987 articles (21 million words), every third of these articles (7 million words), every twenty-first (1 million words), and articles no.", "labels": [], "entities": []}, {"text": "For each corpus, we analyzed three different sets of adjectives, listed in figures 2-4.", "labels": [], "entities": []}, {"text": "The first of them was selected from a similar corpus, contains 21 frequent and ambiguous words that all associate strongly with a particular noun (problem), and was analyzed in.", "labels": [], "entities": []}, {"text": "The second set (43 adjectives) was saected with the constraint that it contain high frequency adjectives (more than 1,000 occurrences in the 21 million word corpus).", "labels": [], "entities": []}, {"text": "The third set (62 adjec.fives) satisfies the opposite constraint containing adjectives of relatively low frequency (between 50 and 250).", "labels": [], "entities": []}, {"text": "shows atypical \u2022 uping found by our system for the third set of a jectives, when the full corpus and all linguistic modules were used.", "labels": [], "entities": []}, {"text": "These three sets of adjectives represent various characteristics of the adjective sets that the system maybe c,~led to. duster.", "labels": [], "entities": []}, {"text": "First, they explicitly represent increasing sizes of the grouping problem.", "labels": [], "entities": []}, {"text": "The second and third sets also contrast the independent frequencies of their member adJfreCtives.", "labels": [], "entities": []}, {"text": "Furthermore, we have found that the less equent adjectives of the third set tend to be more specific than the more frequent ones.", "labels": [], "entities": []}, {"text": "The human evaluators reported that the task of classification was easier for the third set, and their models exhibited about the same degree of agreement for the second and third sets although the third set is significantly larger.", "labels": [], "entities": [{"text": "classification", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.9605664014816284}, {"text": "agreement", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9905834197998047}]}, {"text": "We plan to investigate the generality of this inverse correlation between frequency and specificity in the future.", "labels": [], "entities": []}, {"text": "By including the parameters \"corpus size\" and \"adjective set\", we have six parameters that we can vary in our experiments.", "labels": [], "entities": []}, {"text": "Any remaining tactors affecting the performance of our system are modeled as random noise, so staUstical methods are used to evaluate the effects of the selected parameters.", "labels": [], "entities": []}, {"text": "The six chosen parameters 48 are completely orthogonal, with the exception that parameter \"negative knowledge\" must have the value \"not used\" when parameter \"extraction model\" has the value \"nouns in vicinity\".", "labels": [], "entities": []}, {"text": "In order to avoid introducing imbalance in our experiment, we constructed a complete designed experimerit for all their (4x2-l)x2x2x 4 x 3 = 336 valid combinations 6.", "labels": [], "entities": []}, {"text": "In this way, summarizes the differences in the performance of the system caused by each parameter.", "labels": [], "entities": []}, {"text": "Because of the complete design of the experiment, each value in is obtained in runs that are identical to the runs used for estimating the other values of the same parameter except for the difference in the parameter itself 7.", "labels": [], "entities": []}, {"text": "shows that there is indeed improve-.ment with the introduction of any of the proposed linguistic teatures, or with the use of a lingnisticfilly more sophisticated extraction model.", "labels": [], "entities": []}, {"text": "To assess the statistical significance of these differences, we compared each run fora particular value of a parameter to the corresponding identical valueo '(exceptflOr that parameter) run fora different u me parameter.", "labels": [], "entities": []}, {"text": "Each pair of values fora paran~eter produces~ in this way a set of paired observations, on eacn of these sets, we performed a sign test of the null hypoth~is that there is no real difference in the system s performance between the two values, i.e. that any observed difference is due to chance.", "labels": [], "entities": []}, {"text": "We counted the number of times that the first of the two compared values led to superior performance relative to the second, distributing ties equally between the two cases.", "labels": [], "entities": []}, {"text": "Under the null hypothesis, the number of times that the first value 6RecaU that a designed experiment is complete when at least one trial, or run. is performed for every valid combination of the modeled predictors.", "labels": [], "entities": []}, {"text": "7The slight asymmetry in parameters \"extraction model\" and \"negative knowledge\" is accounted for by leaving out non-matching runs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Performance of a random classifier, of the system on the 21 million word corpus,  and of the humans.", "labels": [], "entities": []}]}