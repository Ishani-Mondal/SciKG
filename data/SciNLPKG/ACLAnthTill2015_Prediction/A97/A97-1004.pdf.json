{"title": [{"text": "A Maximum Entropy Approach to Identifying Sentence Boundaries", "labels": [], "entities": [{"text": "Identifying Sentence Boundaries", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.8967434167861938}]}], "abstractContent": [{"text": "We present a trainable model for identifying sentence boundaries in raw text.", "labels": [], "entities": [{"text": "identifying sentence boundaries in raw text", "start_pos": 33, "end_pos": 76, "type": "TASK", "confidence": 0.808069239060084}]}, {"text": "Given a corpus annotated with sentence boundaries , our model learns to classify each occurrence of., ?, and / as either a valid or invalid sentence boundary.", "labels": [], "entities": []}, {"text": "The training procedure requires no hand-crafted rules, lex-ica, part-of-speech tags, or domain-specific information.", "labels": [], "entities": []}, {"text": "The model can therefore be trained easily on any genre of English, and should be trainable on any other Roman-alphabet language.", "labels": [], "entities": []}, {"text": "Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of identifying sentence boundaries in text has not received as much attention as it deserves.", "labels": [], "entities": [{"text": "identifying sentence boundaries in text", "start_pos": 12, "end_pos": 51, "type": "TASK", "confidence": 0.8523184895515442}]}, {"text": "Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g.).", "labels": [], "entities": []}, {"text": "Others perform the division implicitly without discussing performance (e.g. ().", "labels": [], "entities": []}, {"text": "On first glance, it may appear that using a shortlist of sentence-final punctuation marks, such as., ?, and /, is sufficient.", "labels": [], "entities": []}, {"text": "However, these punctuation marks are not used exclusively to mark sentence breaks.", "labels": [], "entities": []}, {"text": "For example, embedded quotations may contain any of the sentence-ending punctuation marks and . is used as a decimal point, in email addresses, to indicate ellipsis and in abbreviations.", "labels": [], "entities": []}, {"text": "Both / and ? are somewhat less ambiguous * The authors would like to aclmowledge the support of ARPA grant N66001-94-C-6043, ARO grant DAAH04-94-G-0426 and NSF grant but appear in proper names and maybe used multiple times for emphasis to mark a single sentence boundary.", "labels": [], "entities": [{"text": "ARPA grant N66001-94-C-6043", "start_pos": 96, "end_pos": 123, "type": "DATASET", "confidence": 0.7155190706253052}, {"text": "ARO grant DAAH04-94-G-0426", "start_pos": 125, "end_pos": 151, "type": "DATASET", "confidence": 0.8083560864130656}, {"text": "NSF grant", "start_pos": 156, "end_pos": 165, "type": "DATASET", "confidence": 0.9029962420463562}]}, {"text": "Lexically-based rules could be written and exception lists used to disambiguate the difficult cases described above.", "labels": [], "entities": []}, {"text": "However, the lists will never be exhaustive, and multiple rules may interact badly since punctuation marks exhibit absorption properties.", "labels": [], "entities": []}, {"text": "Sites which logically should be marked with multiple punctuation marks will often only have one ( as summarized in).", "labels": [], "entities": []}, {"text": "For example, a sentence-ending abbreviation will most likely not be followed by an additional period if the abbreviation already contains one (e.g. note that D.", "labels": [], "entities": []}, {"text": "C is followed by only a single . in The president lives in Washington, D.C.).", "labels": [], "entities": []}, {"text": "As a result, we believe that manually writing rules is not a good approach.", "labels": [], "entities": []}, {"text": "Instead, we present a solution based on a maximum entropy model which requires a few hints about what information to use and a corpus annotated with sentence boundaries.", "labels": [], "entities": []}, {"text": "The model trains easily and performs comparably to systems that require vastly more information.", "labels": [], "entities": []}, {"text": "Training on 39441 sentences takes 18 minutes on a Sun Ultra Sparc and disambiguating the boundaries in a single Wall Street Journal article requires only 1.4 seconds.", "labels": [], "entities": [{"text": "Sun Ultra Sparc", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.8995349804560343}, {"text": "Wall Street Journal article", "start_pos": 112, "end_pos": 139, "type": "DATASET", "confidence": 0.9408435970544815}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Our best pertbrmance on two corpora.", "labels": [], "entities": [{"text": "pertbrmance", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9588421583175659}]}, {"text": " Table 2: Performance on the sa.me two corpora, using  the highly portable system.", "labels": [], "entities": []}, {"text": " Table 3: Performance on Wall \u00a3't~vet Journal test data a.s a. flmction of training set. size for both systems.", "labels": [], "entities": [{"text": "vet Journal test data", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.8150434792041779}]}]}