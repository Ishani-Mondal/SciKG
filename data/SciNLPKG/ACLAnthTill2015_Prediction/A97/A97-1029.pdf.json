{"title": [{"text": "Nymble: a High-Performance Learning Name-finder", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a statistical, learned approach to finding names and other non-recursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model.", "labels": [], "entities": []}, {"text": "We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMM's to their problems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8250166177749634}]}, {"text": "More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal and other, more recent efforts (.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7464049458503723}]}, {"text": "We would now propose that HMM's have successfully been applied to the problem of name-finding.", "labels": [], "entities": []}, {"text": "We have built a named-entity (NE) recognition system using a slightly-modified version of an HMM; we call our system \"Nymble\".", "labels": [], "entities": [{"text": "named-entity (NE) recognition", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.6712284564971924}]}, {"text": "To our knowledge, Nymble out-performs the best published results of any other learning name-finder.", "labels": [], "entities": []}, {"text": "Furthermore, it performs at or above the 90% accuracy level, often considered \"near-human performance\".", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9946106672286987}]}, {"text": "The system arose from the NE task as specified in the last Message Understanding Conference (MUC), where organization names, person names, location names, times, dates, percentages and money amounts were to be delimited in text using SGML-markup.", "labels": [], "entities": [{"text": "NE task", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9149192571640015}, {"text": "Message Understanding Conference (MUC)", "start_pos": 59, "end_pos": 97, "type": "TASK", "confidence": 0.757959226767222}]}, {"text": "We will describe the various models employed, the methods for training these models and the method for \"decoding\" on test data (the term \"decoding\" borrowed from the speech recognition community, since one goal of traversing an HMM is to recover the hidden state sequence).", "labels": [], "entities": []}, {"text": "To date, we have successfully gained and used the model on both English and Spanish, the latter for MET, the multi-lingual entity task.", "labels": [], "entities": [{"text": "MET", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.8705708980560303}]}], "datasetContent": [{"text": "In this section we report the results of evaluating the final version of the learning software.", "labels": [], "entities": []}, {"text": "We report the results for English and for Spanish and then the results of a set of experiments to determine the impact of the training set size on the algorithm's performance in both English and Spanish.", "labels": [], "entities": []}, {"text": "For each language, we have a held-out development test set and a held-out, blind test set.", "labels": [], "entities": []}, {"text": "We only report results on the blind test set for each respective language.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3.1 Word features, examples and intuition behind them", "labels": [], "entities": []}]}