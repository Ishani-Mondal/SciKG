{"title": [{"text": "An Automatic Extraction of Key Paragraphs Based on Context Dependency", "labels": [], "entities": [{"text": "Automatic Extraction of Key Paragraphs", "start_pos": 3, "end_pos": 41, "type": "TASK", "confidence": 0.6993119120597839}]}], "abstractContent": [{"text": "In this paper, we propose a method for extracting key paragraphs in articles based on the degree of context dependency.", "labels": [], "entities": []}, {"text": "Like Luhn's technique, our method assumes that the words related to theme in an article appear throughout paragraphs.", "labels": [], "entities": []}, {"text": "Our extraction technique of keywords is based on the degree of context dependency that how strongly a word is related to a given context.", "labels": [], "entities": []}, {"text": "The results of experiments demonstrate the applicability of our proposed method.", "labels": [], "entities": []}], "introductionContent": [{"text": "With increasing numbers of machine readable documents becoming available, automatic document summarisation has become one of the major research topics in IR and NLP studies.", "labels": [], "entities": [{"text": "automatic document summarisation", "start_pos": 74, "end_pos": 106, "type": "TASK", "confidence": 0.5762802163759867}, {"text": "IR", "start_pos": 154, "end_pos": 156, "type": "TASK", "confidence": 0.9903432130813599}]}, {"text": "In the field of an automatic summarisation, there are at least two approaches.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.8178138136863708}]}, {"text": "One is knowledge-based approach with particular subject fields,.", "labels": [], "entities": []}, {"text": "This approach, based on deep knowledge of particular subject fields, is useful for restricted tasks, such as, for example, the construction of 'weather forecasts' summaries.", "labels": [], "entities": [{"text": "construction of 'weather forecasts' summaries", "start_pos": 127, "end_pos": 172, "type": "TASK", "confidence": 0.8059025804201762}]}, {"text": "However, when unrestricted subject matter must be treated, as is often the casein practice, the passage retrieval and text summarisation methods proposed have not proven equal to the need, since deep knowledge of particular subject fields is required,.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.8113507330417633}, {"text": "text summarisation", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.6714600771665573}]}, {"text": "The other, alternative strategy is the approach that relies mainly on corpus statistics,.", "labels": [], "entities": []}, {"text": "The main task of this approach is the sentence scoring process.", "labels": [], "entities": [{"text": "sentence scoring process", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8687571883201599}]}, {"text": "Typically, weights are assigned to the individual words in a text, and the complete sentence scores are then based on the occurrence characteristics of highly-weighted terms (keywords) in the respective sentences.", "labels": [], "entities": []}, {"text": "Term weighting technique has been widely investigated in information retrieval and lots of techniques such as location heuristics, rhetorical relations, and title information have been proposed.", "labels": [], "entities": [{"text": "Term weighting", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9003766477108002}, {"text": "information retrieval", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.810147613286972}, {"text": "rhetorical relations", "start_pos": 131, "end_pos": 151, "type": "TASK", "confidence": 0.9046814739704132}]}, {"text": "These techniques seem to be less dependent on the domain.", "labels": [], "entities": []}, {"text": "However, Salton claims that it is difficult to produce high accuracy of retrieval by using these termweighting approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9987961053848267}]}, {"text": "The other term weighting technique is based on keyword frequency.", "labels": [], "entities": []}, {"text": "Keyword frequency is further less dependent on the domain than other weighting methods and therefore, well studied.", "labels": [], "entities": []}, {"text": "Major approaches which are based on keyword frequency assume on the fact that the keywords of the article appear frequently in the article, but appear seldom in other articles,,,.", "labels": [], "entities": []}, {"text": "These approaches seem to show the effect in entirely different articles, such as 'weather forecasts', 'medical reports', and 'computer manuals'.", "labels": [], "entities": []}, {"text": "Because each different article is characterised by a larger number of words which appear frequently in one article, but appear seldom in other articles.", "labels": [], "entities": []}, {"text": "However, in some articles from the same domain such as 'weather forecasts', one encounters quite a number of words which appear frequently over articles.", "labels": [], "entities": [{"text": "weather forecasts'", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7911146481831869}]}, {"text": "Therefore, how to extract keyword from these words is a serious problem in such the restricted subject domain.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method for extracting key paragraphs in articles based on the degree of context dependency and show how the idea of context dependency can be used effectively to extract key paragraphs than other related work.", "labels": [], "entities": []}, {"text": "The basic idea of our approach is that whether a word is a key in an article or not depends on the domain to which the article belongs.", "labels": [], "entities": []}, {"text": "Let 'stake' be a keyword and 'today' not be a keyword in the article.", "labels": [], "entities": []}, {"text": "If the article belongs to a restricted subject domain, such as 'Stock market', there are other articles which are related to the article.", "labels": [], "entities": []}, {"text": "Therefore, the frequency of 'stake' and 'today' in other articles are similar with each other.", "labels": [], "entities": [{"text": "frequency", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9694004654884338}]}, {"text": "Let us consider further abroad coverage domain such as newspaper articles; i.e. the article containing the words 'stake' and 'today' belongs to a newspaper which consists of different subject domains such as 'Stock market' news, 'International' news, 'Weather forecasts' news.", "labels": [], "entities": []}, {"text": "'Today' should appear frequently with every article even in such a domain; i.e. newspaper articles, while 'stake' should not.", "labels": [], "entities": []}, {"text": "Our technique for extraction of keywords explicitly exploits this feature of context dependency of word: how strongly a word is related to a given context.", "labels": [], "entities": []}, {"text": "In the following sections, we first explain context dependency using newspaper articles, then we present our term weighting method and a method for extracting key paragraphs.", "labels": [], "entities": []}, {"text": "Finally, we report some experiments to show the effect of our method.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have conducted three experiments to examine the effect of our method.", "labels": [], "entities": []}, {"text": "The first experiment, Keywords Experiment, is concerned with the keywords extracting technique and with verifying the effect of our method which introduces context dependency.", "labels": [], "entities": [{"text": "keywords extracting", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7707460522651672}]}, {"text": "The second experiment, Key Paragraphs Experiment, shows how the extracted keywords can be used to extract key paragraphs.", "labels": [], "entities": []}, {"text": "In the third experiment, Comparison to Other Related Work, we applied Zechner's key sentences method to key paragraphs extraction (we call this method_A), and compared it with our method.", "labels": [], "entities": [{"text": "key paragraphs extraction", "start_pos": 104, "end_pos": 129, "type": "TASK", "confidence": 0.6965141296386719}]}], "tableCaptions": [{"text": " Table 2: The selected data", "labels": [], "entities": []}, {"text": " Table 3: The results of keyword experiment  Paragraph", "labels": [], "entities": [{"text": "Paragraph", "start_pos": 45, "end_pos": 54, "type": "TASK", "confidence": 0.2684202492237091}]}, {"text": " Table 4: The results of Key Paragraphs Experiment", "labels": [], "entities": []}, {"text": " Table 5: The results of comparative experiment", "labels": [], "entities": []}, {"text": " Table 6: Keywords and their X 2 values in the article", "labels": [], "entities": []}, {"text": " Table 7: The words and their frequencies  Para. 1  Ft. Word", "labels": [], "entities": [{"text": "Ft. Word", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.8939525087674459}]}, {"text": " Table 8: Our method and a vector model", "labels": [], "entities": []}, {"text": " Table 9: The location of key paragraphs  Articles  Hum. Method  (a)First  39  37  (b)First and Last  4  4  (c)First, Mid-position, and Last  1  1  (d)First and Mid-position  4  4  (e)Mid-position  0  1  (f) Otherwise  2  3  Total  50  50", "labels": [], "entities": [{"text": "Articles  Hum. Method", "start_pos": 42, "end_pos": 63, "type": "DATASET", "confidence": 0.795257605612278}]}]}