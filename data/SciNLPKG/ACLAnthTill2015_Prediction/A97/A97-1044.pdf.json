{"title": [{"text": "Building Effective Queries In Natural Language Information Retrieval", "labels": [], "entities": [{"text": "Natural Language Information Retrieval", "start_pos": 30, "end_pos": 68, "type": "TASK", "confidence": 0.6289638504385948}]}], "abstractContent": [{"text": "In this paper we report on our natural language information retrieval (NLIR) project as related to the recently concluded 5th Text Retrieval Conference (TREC-5).", "labels": [], "entities": [{"text": "natural language information retrieval (NLIR)", "start_pos": 31, "end_pos": 76, "type": "TASK", "confidence": 0.7508481017180851}, {"text": "Text Retrieval Conference (TREC-5)", "start_pos": 126, "end_pos": 160, "type": "TASK", "confidence": 0.8287754158178965}]}, {"text": "The main thrust of this project is to use natural language processing techniques to enhance the effectiveness of full-text document retrieval.", "labels": [], "entities": [{"text": "full-text document retrieval", "start_pos": 113, "end_pos": 141, "type": "TASK", "confidence": 0.7020329435666403}]}, {"text": "One of our goals was to demonstrate that robust if relatively shallow NLP can help to derive a better representation of text documents for statistical search.", "labels": [], "entities": []}, {"text": "Recently, we have turned our attention away from text representation issues and more towards query development problems.", "labels": [], "entities": [{"text": "text representation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8865785896778107}, {"text": "query development", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.858260452747345}]}, {"text": "While our NLIR system still performs extensive natural language processing in order to extract phrasal and other indexing terms, our focus has shifted to the problems of building effective search queries.", "labels": [], "entities": []}, {"text": "Specifically, we are interested in query construction that uses words, sentences, and entire passages to expand initial topic specifications in an attempt to cover their various angles, aspects and contexts.", "labels": [], "entities": [{"text": "query construction", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7232276350259781}]}, {"text": "Based on our earlier results indicating that NLP is more effective with long, descriptive queries, we allowed for long passages from related documents to be liberally imported into the queries.", "labels": [], "entities": []}, {"text": "This method appears to have produced a dramatic improvement in the performance of two different statistical search engines that we tested (Cornell's SMART and NIST's Prise) boosting the average precision by at least 40%.", "labels": [], "entities": [{"text": "Cornell's SMART", "start_pos": 139, "end_pos": 154, "type": "DATASET", "confidence": 0.8208673596382141}, {"text": "NIST's Prise", "start_pos": 159, "end_pos": 171, "type": "DATASET", "confidence": 0.7373255093892416}, {"text": "precision", "start_pos": 194, "end_pos": 203, "type": "METRIC", "confidence": 0.9603695273399353}]}, {"text": "In this paper we discuss both manual and automatic procedures for query expansion within anew stream-based information retrieval model.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.8466403484344482}]}], "introductionContent": [{"text": "A typical (full-text) information retrieval (IR) task is to select documents from a database in response to a user's query, and rank these documents according to relevance.", "labels": [], "entities": [{"text": "full-text) information retrieval (IR)", "start_pos": 11, "end_pos": 48, "type": "TASK", "confidence": 0.7878417457853045}]}, {"text": "This has been usually accomplished using statistical methods (often coupled with manual encoding) that (a) select terms (words, phrases, and other units) from documents that are deemed to best represent their content, and (b) create an inverted index file (or files) that provide an easy access to documents containing these terms.", "labels": [], "entities": []}, {"text": "A subsequent search process will attempt to match preprocessed user queries against term-based representations of documents in each case determining a degree of relevance between the two which depends upon the number and types of matching terms.", "labels": [], "entities": []}, {"text": "Although many sophisticated search and matching methods are available, the crucial problem remains to be that of an adequate representation of content for both the documents and the queries.", "labels": [], "entities": []}, {"text": "In term-based representation, a document (as well as a query) is transformed into a collection of weighted terms, derived directly from the document text or indirectly through thesauri or domain maps.", "labels": [], "entities": []}, {"text": "The representation is anchored on these terms, and thus their careful selection is critical.", "labels": [], "entities": []}, {"text": "Since each unique term can bethought to add anew dimensionality to the representation, it is equally critical to weigh them properly against one another so that the document is placed at the correct position in the N-dimensional term space.", "labels": [], "entities": []}, {"text": "Our goal here is to have the documents on the same topic placed close together, while those on different topics placed sufficiently apart.", "labels": [], "entities": []}, {"text": "Unfortunately, we often do not know how to compute terms weights.", "labels": [], "entities": []}, {"text": "The statistical weighting formulas, based on terms distribution within the database, such as tf*idf, are far from optimal, and the assumptions of term independence which are routinely made are false inmost cases.", "labels": [], "entities": []}, {"text": "This situation is even worse when single-word terms are intermixed with phrasal terms and the term independence becomes harder to justify.", "labels": [], "entities": []}, {"text": "The simplest word-based representations of content, while relatively better understood, are usually inadequate since single words are rarely specific enough for accurate discrimination, and their grouping is often accidental.", "labels": [], "entities": []}, {"text": "A better method is to identify groups of words that create meaningful phrases, especially if these phrases denote important concepts in the database domain.", "labels": [], "entities": []}, {"text": "For example, \"joint venture\" is an important term in the Wall Street Journal (WSJ henceforth) database, while neither \"joint\" nor \"venture\" are important by themselves.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ henceforth) database", "start_pos": 57, "end_pos": 102, "type": "DATASET", "confidence": 0.9521301686763763}]}, {"text": "There area number of ways to obtain \"phrases\" from text.", "labels": [], "entities": []}, {"text": "These include generating simple collocations, statistically validated N-grams, part-of-speech tagged sequences, syntactic structures, and even semantic concepts.", "labels": [], "entities": []}, {"text": "Some of these techniques are aimed primarily at identifying multi-word terms that have come to function like ordinary words, for example \"white collar\" or \"electric car\", and capturing other co-occurrence idiosyncrasies associated with certain types of texts.", "labels": [], "entities": []}, {"text": "This simple approach has proven quite effective for some systems, for example the Cornell group reported) that adding simple bigram collocations to the list of available terms can increase retrieval precision by as much as 10%.", "labels": [], "entities": [{"text": "precision", "start_pos": 199, "end_pos": 208, "type": "METRIC", "confidence": 0.9750564098358154}]}, {"text": "Other more advanced techniques of phrase extraction, including extended N-grams and syntactic parsing, attempt to uncover \"concepts\", which would capture underlying semantic uniformity across various surface forms of expression.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7974238097667694}, {"text": "syntactic parsing", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.7659931480884552}]}, {"text": "Syntactic phrases, for example, appear reasonable indicators of content, arguably better than proximity-based phrases, since they can adequately deal with word order changes and other structural variations (e.g., \"college junior\" vs. \"junior in college\" vs. \"junior college\").", "labels": [], "entities": []}, {"text": "A subsequent regularization process, where alternative structures are reduced to a \"normal form\", helps to achieve a desired uniformity, for example, \"college+junior\" will represent a college for juniors, while \"junior+college\" will represent a junior in a college.", "labels": [], "entities": []}, {"text": "A more radical normalization would have also \"verb object\", \"noun rel-clause\", etc.", "labels": [], "entities": []}, {"text": "convened into collections of such ordered pairs.", "labels": [], "entities": []}, {"text": "This head+modifier normalization has been used in our system, and is further described in this paper.", "labels": [], "entities": []}, {"text": "It has to be noted, however, that the use of full-scale syntactic analysis is severely pushing the limits of practicality of an information retrieval system because of the increased demand for computing power and storage.", "labels": [], "entities": []}, {"text": "At the same time, while the gain in recall and precision has not been negligible (we recorded 10-20% increases in precision), no dramatic breakthrough has occurred either.l Currently, the state-of-the art statistical and probabilistic IR system perform at about 20-40% precision range for arbitrary ad-hoc retrieval tasks.", "labels": [], "entities": [{"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9992196559906006}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9986252784729004}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9989587068557739}, {"text": "IR", "start_pos": 235, "end_pos": 237, "type": "TASK", "confidence": 0.9382818937301636}, {"text": "precision range", "start_pos": 269, "end_pos": 284, "type": "METRIC", "confidence": 0.9695546329021454}]}, {"text": "This state of affairs has prompted us take a closer look at the term selection and representation process.", "labels": [], "entities": [{"text": "term selection and representation process", "start_pos": 64, "end_pos": 105, "type": "TASK", "confidence": 0.7001146376132965}]}, {"text": "Our earlier experiments demonstrated that an improved weighting scheme for compound terms, including phrases and proper names, leads to an overall gain in retrieval accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9805107712745667}]}, {"text": "The fundamental problem, however, remained to be the system's inability to recognize, in the documents searched, the presence or absence of the concepts or topics that the query is asking for.", "labels": [], "entities": []}, {"text": "The main reason for this was, we noted, the limited amount of information that the queries could convey on various aspects of topics they represent.", "labels": [], "entities": []}, {"text": "Therefore, we started experimenting with manual and automatic query building techniques.", "labels": [], "entities": [{"text": "query building", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.7019418478012085}]}, {"text": "The purpose was to devise a method for full-text query expansion that would allow for creating exhaustive search queries such that: (1) the performance of any system using these queries would be significantly better than when the system is run using the original topics, and (2) the method could be eventually automated or semi-automated so as to be useful to a nonexpert user.", "labels": [], "entities": [{"text": "full-text query expansion", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6872555216153463}]}, {"text": "Our preliminary results from TREC-5 evaluations show that this approach is indeed very effective.", "labels": [], "entities": []}, {"text": "In the rest of this paper we describe the overall organization of our TREC-5 system, and then discuss some experiments that we performed and their results, as well as our future research plans.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}