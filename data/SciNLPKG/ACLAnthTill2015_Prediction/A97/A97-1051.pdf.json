{"title": [{"text": "Mixed-Initiative Development of Language Processing Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "Historically, tailoring language processing systems to specific domains and languages for which they were not originally built has required a great deal of effort.", "labels": [], "entities": []}, {"text": "Recent advances in corpus-based manual and automatic training methods have shown promise in reducing the time and cost of this porting process.", "labels": [], "entities": [{"text": "porting", "start_pos": 127, "end_pos": 134, "type": "TASK", "confidence": 0.9683377146720886}]}, {"text": "These developments have focused even greater attention on the bottleneck of acquiring reliable, manually tagged training data.", "labels": [], "entities": []}, {"text": "This paper describes anew set of integrated tools, collectively called the Alembic Workbench, that uses a mixed-initiative approach to \"bootstrapping\" the manual tagging process, with the goal of reducing the overhead associated with corpus development.", "labels": [], "entities": []}, {"text": "Initial empirical studies using the Alembic Workbench to annotate \"named entities\" demonstrates that this approach can approximately double the production rate.", "labels": [], "entities": [{"text": "Alembic Workbench", "start_pos": 36, "end_pos": 53, "type": "DATASET", "confidence": 0.7213739305734634}]}, {"text": "As an ~ benefit, the combined efforts of machine and user produce domain-specific annotation rules that can be used to annotate similar texts automatically through the Alembic NLP system.", "labels": [], "entities": []}, {"text": "The ultimate goal of this project is to enable end users to generate a practical domain-specific information extraction system within a single session.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.7259652018547058}]}], "introductionContent": [{"text": "In the absence of complete and deep text understanding, implementing information extraction systems remains a delicate balance between general theories of language processing and domain-specific heuristics.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.7223159968852997}]}, {"text": "Recent developments in the area of corpus-based language processing systems indicate that the successful application of any system to anew task depends to a very large extent on the careful and frequent evaluation of the evolving system against training and test corpora.", "labels": [], "entities": []}, {"text": "This has focused increased attention on the importance of obtaining reliable training corpora.", "labels": [], "entities": []}, {"text": "Unfortunately, acquiring such data has usually been a labor-intensive and time-consuming exercise.", "labels": [], "entities": []}, {"text": "The goal of the Alembic Workbench is to dramatically accelerate the process by which language processing systems are tailored to perform new tasks.", "labels": [], "entities": []}, {"text": "The philosophy motivating our work is to maximally reuse and re-apply every kernel of knowledge available at each step of the tailoring process.", "labels": [], "entities": []}, {"text": "In particular, our approach applies a bootstrapping procedure to the development of the training corpus itself.", "labels": [], "entities": []}, {"text": "By re-investing the knowledge available in the earliest training data to pretag subsequent un-tagged data, the Alembic Workbench can tralasform the process of manual tagging to one dominated by manual review.", "labels": [], "entities": [{"text": "Alembic Workbench", "start_pos": 111, "end_pos": 128, "type": "DATASET", "confidence": 0.8197353482246399}]}, {"text": "In the limit, if the pretagging process performs well enough, it becomes the domain-specific automatic tagging procedure itself, and can be applied to those new documents from which information is to be extracted.", "labels": [], "entities": []}, {"text": "As we and others in the information extraction arena have noticed, the quality of text processing heuristics is influenced critically not only by the power of one's linguistic theory, but also by the ability to evaluate those theories quickly and reliably.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7920290231704712}]}, {"text": "Therefore, building new information extraction systems requires an integrated environment that supports: (1) the development of a domain-specific annotated corpus; (2) the multi-faceted analysis of that corpus; (3) the ability to quickly generate hypotheses as to how to extractor tag information in that corpus; and (4) the ability to quickly evaluate and analyze the performance of those hypotheses.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7402375936508179}]}, {"text": "The Alembic Workbench is our attempt to build such an environment.", "labels": [], "entities": [{"text": "Alembic Workbench", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.858490377664566}]}, {"text": "As the Message Understanding Conferences move into their tenth year, we have seen a growing recognition of the value of balanced evaluations against a common test corpus.", "labels": [], "entities": [{"text": "Message Understanding Conferences", "start_pos": 7, "end_pos": 40, "type": "TASK", "confidence": 0.8400016824404398}]}, {"text": "What is unique in our approach is to integrate system development with the corpus annotation process itself.", "labels": [], "entities": []}, {"text": "The early indications are that at the very least this integration can significantly increase the productivity of the corpus annotator.", "labels": [], "entities": []}, {"text": "We believe that the benefits will flow in the other direction as well, and that a concomitant increase in system performance will follow as one applies the same mixed-initiative development environment to the problem of domainspecific tailoring of the language processing system.", "labels": [], "entities": []}], "datasetContent": [{"text": "We are still in the early stages of evaluating the performance of the Alembic Workbench along a number of different dimensions.", "labels": [], "entities": [{"text": "Alembic Workbench", "start_pos": 70, "end_pos": 87, "type": "DATASET", "confidence": 0.7418252378702164}]}, {"text": "However, the results from early experiments are encouraging.", "labels": [], "entities": []}, {"text": "compares the productivity rates using different corpus development utilities.", "labels": [], "entities": []}, {"text": "These are indicated by the four categories on the X-axis: (1) using SGML-mode in emacs (by an expert user); (2) using the Workbench interface and \"auto-tag\" string-matching utility only; (3) using the Workbench following the application of learned tagging rules derived from 5 short documents--approximately 1,500 words, and (4) using the Workbench following the application of learned tagging rules again, but this time with the learned rules having trained on 100 documents (approximately 48,000 words), instead of only five documents.", "labels": [], "entities": []}, {"text": "As can be seen in these experiments, there is a clear increase in the productivity as a function of both the user interface (second column) and the application of pre-tagging rules (third and fourth columns).", "labels": [], "entities": []}, {"text": "The large step in performance between columns three and four indicate that repeated invocation of the learning process during the intermediate stages of the corpus development cycle will likely result in acceleration of the annotation rate.", "labels": [], "entities": []}, {"text": "(As it happens, these results are probably underestimating the pre-tagging productivity.", "labels": [], "entities": []}, {"text": "The reason for this is that the version of the Workbench used was not yet able to incorporate date and time annotations generated by a separate pre-processing step; this date and time tagger performs at an extremely high level of precision for this genre---in the high nineties P&R.)", "labels": [], "entities": [{"text": "precision", "start_pos": 230, "end_pos": 239, "type": "METRIC", "confidence": 0.9949994087219238}]}, {"text": "These initial experiments involved a single expert annotator on a single tagging task (MUC6 named entity).", "labels": [], "entities": []}, {"text": "The annotator was very familiar with the tagging task.", "labels": [], "entities": [{"text": "tagging task", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.9184007048606873}]}, {"text": "To place this in the perspective of the human annotator, after only about 15 minutes of named entity tagging, having annotated some 1,500 words of text with approximately 150 phrases, the phrase rule learner can derive heuristic rules that produce a pre-tagging performance rate (P&R) of between 50 and 60 percent.", "labels": [], "entities": [{"text": "pre-tagging performance rate (P&R)", "start_pos": 250, "end_pos": 284, "type": "METRIC", "confidence": 0.8580704778432846}]}, {"text": "Of course, this performance is far short of what is needed fora practical extraction system, but it already constitutes a major source for labor savings, since 50 to 60 percent of the annotations that need to be moused (or clicked) in are already there.", "labels": [], "entities": []}, {"text": "Since the precision at this early stage is only around 60 percent, there will be extra phrases that need (1) to be removed, (2) their assigned category changed (from, say, organization to person), or (3) their boundaries adjusted.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9993791580200195}]}, {"text": "It turns out that for the first two of these kinds of precision errors, the manual corrections are extremely quick to perform.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9990363121032715}]}, {"text": "(Boundaries are not really difficult to modify, but the time required is approximately the same as inserting a tag from scratch.)", "labels": [], "entities": []}, {"text": "In addition, making these corrections removes both a precision and a recall error at the same time.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9991255402565002}, {"text": "recall error", "start_pos": 69, "end_pos": 81, "type": "METRIC", "confidence": 0.984666109085083}]}, {"text": "Therefore, it turns out that even at this very early stage, the modest pre-tagging performance gained from applying the learning procedure provides measurable performance improvement.", "labels": [], "entities": []}, {"text": "In order to obtain more detailed results on the effect of pre-tagging corpora, we conducted another experiment in which we made direct use of the iterative automatic generation of rules from a growing manually-tagged corpus.", "labels": [], "entities": []}, {"text": "Using the same skilled annotator, we inlroduced a completely new corpus for which namedentity tagging happened to be needed within our company.", "labels": [], "entities": [{"text": "namedentity tagging", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7363763451576233}]}, {"text": "We randomly divided approximately 50 documents of varying sizes into five groups.", "labels": [], "entities": []}, {"text": "The word counts for these five groups were: Groupl: 19,300; Group2: 13,800; Group3: 6,3130; Group4: 15,800; Group5: 8,000; fora total of 63,000 words.", "labels": [], "entities": []}, {"text": "After manually tagging the first group, we invoked the rule learning procedure.", "labels": [], "entities": [{"text": "rule learning", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.8212283551692963}]}, {"text": "Applying the learning procedure on each training set required two to three hours of elapsed time on a Sun Sparc Ultra.", "labels": [], "entities": [{"text": "Sun Sparc Ultra", "start_pos": 102, "end_pos": 117, "type": "DATASET", "confidence": 0.9178457260131836}]}, {"text": "The new tagging rules were then applied to the next ten documents prior to being manually tagged/edited.", "labels": [], "entities": []}, {"text": "This enlarged corpus was then used to derive anew rule set to be applied to the next group of documents, and soon.", "labels": [], "entities": []}, {"text": "A summarization of the results are presented in.", "labels": [], "entities": []}, {"text": "Clearly, more experiments are called for we plan to conduct these across different annotators, task types, and languages, to better evaluate productivity, quality and other aspects of the annotation process.", "labels": [], "entities": []}, {"text": "It is extremely difficult to control many of the features that influence the annotation process, such as the intrinsic complexity of the topic in a particular document, the variation in tag-density (tags per word) that may occur, the user's own training effect as the structure and content of documents become more familiar, office distractions, etc.", "labels": [], "entities": []}, {"text": "In order to gain a better understanding of the underlying tagging performance of the rule learner, and so separate out some of these human factors issues, we ran an automated experiment in which different random subsets of sentences were used to train rule sets, which were then evaluated on a static test corpus.", "labels": [], "entities": []}, {"text": "The results shown in give some indication of the ability of the rule-sequence learning procedure to glean useful generalizations from meager amounts of training data.", "labels": [], "entities": []}, {"text": "The first observation we make is that there is a clear and obvious direction of improvement--by the time 30 documents have been tagged, the annotation rate on Group 4 has increased considerably.", "labels": [], "entities": [{"text": "annotation rate", "start_pos": 140, "end_pos": 155, "type": "METRIC", "confidence": 0.9536620378494263}]}, {"text": "It is important to note, however, that there is still noise in the curve.", "labels": [], "entities": []}, {"text": "In addition, the granularity is perhaps still too coarse to measure the incremental influences of pre-tagging rules.", "labels": [], "entities": []}, {"text": "One clear effect of increasing training set size is a reduction in the sensitivity of the learning procedure to particular training sets.", "labels": [], "entities": []}, {"text": "We hypothesize that this effect is partly indicative of the generalization behavior on which the learning procedure is based, which amplifies the effects of choosing more or less representative training sentences by chance.", "labels": [], "entities": []}, {"text": "Since the learning process is not merely memorizing phrases, but generating contextual rules to try to predict phrase types and extents, the rules are very sensitive to extremely small selections of training sentences.", "labels": [], "entities": []}, {"text": "shows the F-measure performance smoothed by averaging neighboring data points, to get a clearer picture of the general tendency.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9713825583457947}]}, {"text": "We should note that the Alembic Workbench, having been developed only recently in our laboratory, was not available to us in the course of our effort to apply the Alembic system to the MUC6 and MET tasks.", "labels": [], "entities": [{"text": "Alembic Workbench", "start_pos": 24, "end_pos": 41, "type": "DATASET", "confidence": 0.9119369983673096}, {"text": "MUC6", "start_pos": 185, "end_pos": 189, "type": "DATASET", "confidence": 0.5756328701972961}]}, {"text": "Therefore we have not been able to measure its influence in preparing fora particular new text processing task.", "labels": [], "entities": [{"text": "text processing task", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.7854128976662954}]}, {"text": "We intend to use the system to prepare for future evaluations (including MUC7 and MET2) and to carefully evaluate the Alembic Workbench as an environment for the mixed-initiative development of information extraction systems in multiple languages.", "labels": [], "entities": [{"text": "MUC7", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9175035953521729}, {"text": "Alembic Workbench", "start_pos": 118, "end_pos": 135, "type": "DATASET", "confidence": 0.8677114844322205}, {"text": "information extraction", "start_pos": 194, "end_pos": 216, "type": "TASK", "confidence": 0.7732509076595306}]}], "tableCaptions": []}