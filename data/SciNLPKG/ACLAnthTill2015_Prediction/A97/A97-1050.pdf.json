{"title": [{"text": "Semi-Automatic Acquisition of Domain-Specific Translation Lexicons", "labels": [], "entities": [{"text": "Domain-Specific Translation Lexicons", "start_pos": 30, "end_pos": 66, "type": "TASK", "confidence": 0.7682879567146301}]}], "abstractContent": [{"text": "We investigate the utility of an algorithm for translation lexicon acquisition (SABLE), used previously on a very large corpus to acquire general translation lexicons , when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons.", "labels": [], "entities": [{"text": "translation lexicon acquisition (SABLE)", "start_pos": 47, "end_pos": 86, "type": "TASK", "confidence": 0.8900390168031057}]}], "introductionContent": [{"text": "Reliable translation lexicons are useful in many applications, such as cross-language text retrieval.", "labels": [], "entities": [{"text": "Reliable translation lexicons", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6842065751552582}, {"text": "cross-language text retrieval", "start_pos": 71, "end_pos": 100, "type": "TASK", "confidence": 0.7121159732341766}]}, {"text": "Although general purpose machine readable bilingual dictionaries are sometimes available, and although some methods for acquiring translation lexicons automatically from large corpora have been proposed, less attention has been paid to the problem of acquiring bilingual terminology specific to a domain, especially given domain-specific parallel corpora of only limited size.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the utility of an algorithm for translation lexicon acquisition, used previously on a very large corpus to acquire general translation lexicons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons.", "labels": [], "entities": [{"text": "translation lexicon acquisition", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.9092851479848226}]}, {"text": "The goal is to produce material suitable for postprocessing in a lexicon acquisition process like the following: 1.", "labels": [], "entities": []}, {"text": "Run the automatic lexicon acquisition algorithm on a domain-specific parallel corpus.", "labels": [], "entities": [{"text": "automatic lexicon acquisition", "start_pos": 8, "end_pos": 37, "type": "TASK", "confidence": 0.6492622594038645}]}, {"text": "2. Automatically filter out \"general usage\" entries that already appear in a machine readable dictionary (MRD) or other general usage lexical resources.", "labels": [], "entities": [{"text": "general usage\" entries that already appear in a machine readable dictionary (MRD)", "start_pos": 29, "end_pos": 110, "type": "TASK", "confidence": 0.511870123942693}]}, {"text": "3. Manually filter out incorrect or irrelevant entries from the remaining list.", "labels": [], "entities": []}, {"text": "Our aim, therefore, is to achieve sufficient recall and precision to make this process --in particular the time and manual effort required in Step 3 --a viable alternative to manual creation of translation lexicons without automated assistance.", "labels": [], "entities": [{"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9987548589706421}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9959264993667603}]}, {"text": "The literature on cross-lingual text retrieval (CLTR) includes work that is closely related to this research, in that recent approaches emphasize the use of dictionary-and corpus-based techniques for translating queries from a source language into the language of the document collection.", "labels": [], "entities": [{"text": "cross-lingual text retrieval (CLTR)", "start_pos": 18, "end_pos": 53, "type": "TASK", "confidence": 0.7743805646896362}]}, {"text": ", for example, generate target-language queries using a corpus-based technique that is similar in several respects to the work described here.", "labels": [], "entities": []}, {"text": "However, the approach does not attempt to distinguish domain-specific from general usage term pairs, and it involves no manual intervention.", "labels": [], "entities": []}, {"text": "The work reported here, focusing on semiautomating the process of acquiring translation lexicons specific to a domain, can be viewed as providing bilingual dictionary entries for CLTR methods like that used by Davis in later work, in which dictionary-based generation of an ambiguous target language query is followed by corpus-based disambiguation of that query.", "labels": [], "entities": [{"text": "dictionary-based generation of an ambiguous target language query", "start_pos": 240, "end_pos": 305, "type": "TASK", "confidence": 0.7798310071229935}]}, {"text": "Turning to the literature on bilingual terminology identification per se, although monolingual terminology extraction is a problem that has been previously explored, often with respect to identifying relevant multi-word terms (e.g.), less prior work exists for bilingual acquisition of domain-specific translations.", "labels": [], "entities": [{"text": "bilingual terminology identification", "start_pos": 29, "end_pos": 65, "type": "TASK", "confidence": 0.6522742112477621}, {"text": "monolingual terminology extraction", "start_pos": 83, "end_pos": 117, "type": "TASK", "confidence": 0.614830732345581}, {"text": "bilingual acquisition of domain-specific translations", "start_pos": 261, "end_pos": 314, "type": "TASK", "confidence": 0.7192375302314759}]}, {"text": "Termight is one method for analyzing parallel corpora to discover translations in technical terminology; Dagan and Church report accuracy of 40% given an English/German technical manual, and observe that even this relatively low accuracy permits the successful application of the system in a translation bureau, when used in conjunction with an appropriate user interface.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9991922974586487}, {"text": "accuracy", "start_pos": 229, "end_pos": 237, "type": "METRIC", "confidence": 0.9774912595748901}]}, {"text": "The Champollion system) moves toward higher accuracy (around 73%) and considerably greater flexibility in the handling of multi-word translations, though the algorithm has been applied primarily to very large corpora such as the Hansards (3-9 million words; Smadja et al. observe that the method has difficulty handling low-frequency cases), and no attempt is made to distinguish corpus-dependent translations from general ones.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9990410208702087}, {"text": "Hansards", "start_pos": 229, "end_pos": 237, "type": "DATASET", "confidence": 0.9532188177108765}]}, {"text": "report on a study in which a small (200,000 word) corpus was used as the basis for extracting bilingual terminology, using a combination of syntactic patterns for identifying simple twoword terms monolingually, and a statistical measure for selecting related terms across languages.", "labels": [], "entities": []}, {"text": "Using a manually constructed reference list, they report 70% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9992179870605469}]}, {"text": "The SABLE system (Melamed, 1996b) makes no attempt to handle collocations, but for single-word to single-word translations it offers a very accurate method for acquiring high quality translation lexicons from very large parallel corpora: Melamed reports 90+% precision at 90+% recall, when evaluated on sets of Hansards data of 6-7 million words.", "labels": [], "entities": [{"text": "precision", "start_pos": 259, "end_pos": 268, "type": "METRIC", "confidence": 0.9970067143440247}, {"text": "recall", "start_pos": 277, "end_pos": 283, "type": "METRIC", "confidence": 0.9960547685623169}, {"text": "Hansards data", "start_pos": 311, "end_pos": 324, "type": "DATASET", "confidence": 0.9669943451881409}]}, {"text": "Previous work with SABLE does not attempt to address the question of domain-specific vs. general translations.", "labels": [], "entities": []}, {"text": "This paper applies the SABLE system to a much smaller (approximately 400,000 word) corpus in a technical domain, and assesses its potential contribution to the semi-automatic acquisition process outlined above, very much in the spirit of and, but beginning with a higher accuracy starting point and focusing on mono-word terms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 271, "end_pos": 279, "type": "METRIC", "confidence": 0.9903920888900757}]}, {"text": "In the remainder of the paper we briefly outline translation lexicon acquisition in the SABLE system, describe its application to a corpus of technical documentation, and provide a quantitative assessment of its performance.", "labels": [], "entities": [{"text": "translation lexicon acquisition", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.9510366519292196}, {"text": "SABLE system", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.6897403597831726}]}], "datasetContent": [{"text": "Our assessment of the system was designed to reasonably approximate the post-processing that would be done in order to use this system for acquisition of translation lexicons in a real-world setting, which would necessarily involve subjective judgments.", "labels": [], "entities": [{"text": "acquisition of translation lexicons", "start_pos": 139, "end_pos": 174, "type": "TASK", "confidence": 0.7997771948575974}]}, {"text": "We hired six fluent speakers of both French and English at the University of Maryland; they were briefed on the general nature of the task, and given a data sheet containing the 400 candidate entries (pairs containing one French word and one English word) and a \"multiple choice\" style format for the annotations, along with the following instructions.", "labels": [], "entities": []}, {"text": "1. If the pair clearly cannot be of help in constructing a glossary, circle \"Invalid\" and goon to the next pair.", "labels": [], "entities": []}, {"text": "2. If the pair can be of help in constructing a glossary, choose one of the following: 1 V: The two words are of the \"plain vanilla\" type you might find in a bilingual dictionary.", "labels": [], "entities": []}, {"text": "P: The pair is a case where a word changes its part of speech during translation.", "labels": [], "entities": [{"text": "P", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9144058227539062}]}, {"text": "For example, \"to have protection\" in English is often translated as %tre prot6g6\" in Canadian parliamentary proceedings, so for that domain the pair protection/prot6g6 would be marked P.", "labels": [], "entities": []}, {"text": "I: The pair is a case where a direct translation is incomplete because the computer program only looked at single words.", "labels": [], "entities": []}, {"text": "For example, if French \"imm6diatement\" were paired with English \"right\", you could select I because the pair is almost certainly the computer's best but incomplete attempt to be pairing \"imm4diatement\" with \"right away\".", "labels": [], "entities": []}, {"text": "3. Then choose one or both of the following: \u2022 Specific.", "labels": [], "entities": []}, {"text": "Leaving aside the relationship between the two words (your choice of P, V, or I), the word pair would be of use in constructing a technical glossary.", "labels": [], "entities": []}, {"text": "Leaving aside the relationship between the two words (your choice of P, V, or I), the word pair would be of use in constructing a general usage glossary.", "labels": [], "entities": []}, {"text": "Notice that a word pair could make sense in both.", "labels": [], "entities": []}, {"text": "For example, \"corbeille/wastebasket\" makes sense in the computer domain (in many popular graphical interfaces there is a wastebasket icon that is used for deleting files), but also in more general usage.", "labels": [], "entities": []}, {"text": "So in this case you could in fact decide to choose both \"Specific\" and \"General\".", "labels": [], "entities": []}, {"text": "If you can't choose either \"Specific\" or \"General', chances are that you should reconsider whether or not to mark this word pair \"Invalid\".", "labels": [], "entities": []}, {"text": "i Since part-of-speech tagging was used in the version of SABLE that produced the candidates in this experiment, entries presented to the annotator also included a minimal form of part-of-speech information, e.g. distinguishing nouns from verbs.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 8, "end_pos": 30, "type": "TASK", "confidence": 0.6890000402927399}, {"text": "SABLE", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.7401584386825562}]}, {"text": "The annotator was informed that these annotations were the computer's best attempt to identify the part-of-speech for the words; it was suggested that they could be used as a hint as to why that word pair had been proposed, if so desired, and otherwise ignored.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Effect of in-context vs. out-of-context evaluation. All numbers are in ~o. n = 400.", "labels": [], "entities": []}, {"text": " Table 3: Domain-specificity of filtered translation  lexicon entries.", "labels": [], "entities": []}]}