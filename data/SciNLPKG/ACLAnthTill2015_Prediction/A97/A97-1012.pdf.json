{"title": [], "abstractContent": [{"text": "This paper describes anew finite-state shallow parser.", "labels": [], "entities": []}, {"text": "It merges constructive and reductionist approaches within a highly modular architecture.", "labels": [], "entities": []}, {"text": "Syntactic information is added at the sentence level in an incremental way, depending on the contex-tual information available at a given stage.", "labels": [], "entities": []}, {"text": "This approach overcomes the inefficiency of previous fully reductionist constraint-based systems, while maintaining broad coverage and linguistic granularity.", "labels": [], "entities": []}, {"text": "The implementation relies on a sequence of networks built with the replace operator.", "labels": [], "entities": []}, {"text": "Given the high level of modularity, the core grammar is easily augmented with corpus-specific sub-grammars.", "labels": [], "entities": []}, {"text": "The current system is implemented for French and is being expanded to new languages.", "labels": [], "entities": []}, {"text": "1 Background Previous work in finite-state parsing at sentence level falls into two categories: the constructive approach or the reductionist approach.", "labels": [], "entities": [{"text": "finite-state parsing", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.7116680592298508}]}, {"text": "The origins of the constructive approach go back to the parser developed by Joshi (Joshi, 1996).", "labels": [], "entities": []}, {"text": "It is based on a lexical description of large collections of syntactic patterns (up to several hundred thousand rules) using subcategorisation frames (verbs + essential arguments) and local grammars (Roche, 1993).", "labels": [], "entities": []}, {"text": "It is, however, still unclear whether this heavily lex-icalized method can account for all sentence structures actually found in corpora, especially due to the proliferation of non-argumental complements in corpus analysis.", "labels": [], "entities": []}, {"text": "Another constructive line of research concentrates on identifying basic phrases such as in the FASTUS information extraction system (Appelt et al., 1993) or in the chunking approach proposed in (Abney, 72 1991; Federici et al., 1996).", "labels": [], "entities": [{"text": "FASTUS information extraction", "start_pos": 95, "end_pos": 124, "type": "TASK", "confidence": 0.6903656522432963}, {"text": "Abney, 72 1991; Federici et al.", "start_pos": 195, "end_pos": 226, "type": "DATASET", "confidence": 0.8753160685300827}]}, {"text": "Attempts were made to mark the segments with additional syntactic information (e.g. subject or object) (Grefenstette, 1996) using simple heuristics, for the purpose of information retrieval, but not for robust parsing.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.7371727228164673}]}, {"text": "The reductionist approach starts from a large number of alternative analyses that get reduced through the application of constraints.", "labels": [], "entities": []}, {"text": "The constraints maybe expressed by a set of elimination rules applied in a sequence (Voutilainen, Tapanainen, 1993) or by a set of restrictions applied in parallel (Koskenniemi et al., 1992).", "labels": [], "entities": []}, {"text": "Ina finite-state constraint grammar (Chanod, Tapanainen, 1996), the initial sentence network represents all the combinations of the lexical readings associated with each token.", "labels": [], "entities": []}, {"text": "The acceptable readings result from the intersection of the initial sentence network with the constraint networks.", "labels": [], "entities": []}, {"text": "This approach led to very broad coverage analyzers, with good linguistic granularity (the information is richer than in typical chunking systems).", "labels": [], "entities": []}, {"text": "However, the size of the intermediate networks resulting from the intersection of the initial sentence network with the sets of constraints raises serious efficiency issues.", "labels": [], "entities": []}, {"text": "The new approach proposed in this paper aims at merging the constructive and the reductionist approaches , so as to maintain the coverage and gran-ularity of the constraint-based approach at a much lower computational cost.", "labels": [], "entities": [{"text": "coverage", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9766843318939209}]}, {"text": "In particular, segments (chunks) are defined by constraints rather than patterns , in order to ensure broader coverage.", "labels": [], "entities": []}, {"text": "At the same time, segments are defined in a cautious way, to ensure that clause boundaries and syntactic functions (e.g. subject, object, PP-Obj) can be defined with a high degree of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9950290322303772}]}, {"text": "2 The incremental parser 2.1 Overview The input to the parser is a tagged text.", "labels": [], "entities": []}, {"text": "We currently use a modified version of the Xerox French", "labels": [], "entities": [{"text": "Xerox French", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.929746687412262}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}