{"title": [], "abstractContent": [{"text": "Sophisticated grammar formalisms, such as LFG, allow concisely capturing complex linguistic phenomena.", "labels": [], "entities": []}, {"text": "The powerful operators provided by such formalisms can however introduce spurious ambiguity , making parsing inefficient.", "labels": [], "entities": []}, {"text": "A simple form of corpus-based grammar pruning is evaluated experimentally on two wide-coverage grammars, one En-giish and one French.", "labels": [], "entities": []}, {"text": "Speedups of up to a factor 6 were obtained, at a cost in grammatical coverage of about 13%.", "labels": [], "entities": [{"text": "Speedups", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9397264122962952}]}, {"text": "A two-stage architecture allows achieving significant speedups without introducing additional parse failures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Expressive grammar formalisms allow grammar developers to capture complex linguistic generalizations concisely and elegantly, thus greatly facilitating grammar development and maintenance.", "labels": [], "entities": []}, {"text": "found that the empirical performance when parsing with unification-based grammars is nowhere near the theoretical worst-case complexity.", "labels": [], "entities": []}, {"text": "Nonetheless, directly parsing with such grammars, in the form they were developed, can be very inefficient.", "labels": [], "entities": []}, {"text": "For this reason, grammars are typically compiled into representations that allow faster parsing.", "labels": [], "entities": []}, {"text": "This does however not solve the potential problem of the grammars overgenerating considerably, thus allowing large amounts of spurious ambiguity.", "labels": [], "entities": []}, {"text": "Indeed, a current trend in high-coverage parsing, especially when employing a statistical model of language, see, e.g., (Collins 97), is to allow the grammar to massively overgenerate and instead disambiguate by statistical means during or after parsing.", "labels": [], "entities": [{"text": "Collins 97)", "start_pos": 121, "end_pos": 132, "type": "DATASET", "confidence": 0.9453586935997009}]}, {"text": "If the benefits resulting from more concise grammatical descriptions are to outweigh the costs of spurious ambiguity, the latter must be brought down.", "labels": [], "entities": []}, {"text": "In such a situation, corpus-based compilation techniques can drastically improve parsing performance without burdening the grammar developer.", "labels": [], "entities": [{"text": "corpus-based compilation", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.6436890214681625}]}, {"text": "The initial, and much seminal work in this area was been carried out by Rayner and coworkers, see, and.", "labels": [], "entities": []}, {"text": "In the current article, we apply similar ideas to Lexical Functional Grammar (LFG) in the incarnation of the Xerox Linguistic Environment (XLE).", "labels": [], "entities": [{"text": "Lexical Functional Grammar (LFG)", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.7997203369935354}]}, {"text": "The goal is to investigate to what extent corpus-based compilation techniques can reduce overgeneration and spurious ambiguity, and increase parsing efficiency, without jeopardizing coverage.", "labels": [], "entities": []}, {"text": "The rest of the article is organized as follows: Section 2 presents the relevant aspects of the LFG formalism and the pruning strategy employed, Section 3 describes the experimental setup, Section 4 reports the experimental results and Section 5 relates this to other work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments carried out to determine the effectiveness of corpus-based specialization were performed as illustrated in.", "labels": [], "entities": []}, {"text": "Two broadcoverage LFG grammars were used, one for French and one for English, both of which were developed within the Pargram project () during several years time.", "labels": [], "entities": [{"text": "Pargram project", "start_pos": 118, "end_pos": 133, "type": "DATASET", "confidence": 0.9378888309001923}]}, {"text": "The French grammar consists of 133 rule schemata, the English grammar of 8.5 rule schemata.", "labels": [], "entities": []}, {"text": "Each gralmnar is equipped with a treebank, which was developed for other purposes than grammar specialization.", "labels": [], "entities": [{"text": "grammar specialization", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.82094606757164}]}, {"text": "Each treebank was produced by letting the system parse a corpus of technical documentation.", "labels": [], "entities": []}, {"text": "Any sentence that did not obtain any parse was discarded.", "labels": [], "entities": []}, {"text": "At this point, the French corpus was reduced to 960 sentences, and the English corpus to 970.", "labels": [], "entities": [{"text": "French corpus", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.9038141369819641}, {"text": "English corpus", "start_pos": 71, "end_pos": 85, "type": "DATASET", "confidence": 0.9558898210525513}]}, {"text": "The average sentence length was 9 for French and 8 for English.", "labels": [], "entities": []}, {"text": "For each sentence, a human expert then selected the most appropriate analysis among those returned by the parser.", "labels": [], "entities": []}, {"text": "In the current experiments, each treebank was used to specialize the grammar it had been developed with.", "labels": [], "entities": []}, {"text": "A set of 10-fold cross-validation experiments was carried out to measure several interesting quantities under different conditions.", "labels": [], "entities": []}, {"text": "This means that, for each language, the corpus was randomly split into ten equal parts, and one tenth at a time was held out for testing while the remaining nine tenths were used to specialize the grammar, and the results were averaged over the ten runs..", "labels": [], "entities": []}, {"text": "For each grammar the average number of parses per sentence, the fraction of sentences which still received at least one parse (angparse) and the fraction of sentences for which the parse selected by the expert was still derived (coverage) were measured 1.", "labels": [], "entities": [{"text": "coverage", "start_pos": 229, "end_pos": 237, "type": "METRIC", "confidence": 0.9728342890739441}]}, {"text": "The average CPU time required by parsing was also measured, and this was used to compute the speedup with respect to the original grammar.", "labels": [], "entities": [{"text": "parsing", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9717687368392944}]}, {"text": "The thus established results constitute one data point in the trade-off between ambiguity reduction on one side, which is in turn related to parsing speed, and loss in coverage on the other.", "labels": [], "entities": [{"text": "parsing", "start_pos": 141, "end_pos": 148, "type": "TASK", "confidence": 0.9674959778785706}, {"text": "coverage", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9449252486228943}]}, {"text": "In order to determine other points of this trade-off, the same set. of experiments was performed where speciMization was inhibited for certain rule schemata.", "labels": [], "entities": []}, {"text": "In particular, for each grammar, the two rule schemata that received the largest number of distinct expansions in the corpora were determined.", "labels": [], "entities": []}, {"text": "These proved to be those associated with the LHS symbols 'VPverb' and 'NP' for the French grammar, and 'VPv' and 'NPadj' for the English one.", "labels": [], "entities": []}, {"text": "2 The experiments were repeated while inhibiting specialization of first the scheme with the most expansions, and then the two most expanded schemata.", "labels": [], "entities": []}, {"text": "Measures of coverage and speedup are important The original rule: is replaced by the following: indicators of what can be achieved with this form of grammar pruning.", "labels": [], "entities": [{"text": "coverage", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9652407169342041}]}, {"text": "However, they could potentially be misleading, since failure times for uncovered sentences might be considerably lower than their parsing times, had they not been out of coverage.", "labels": [], "entities": []}, {"text": "If the pruned grammar fails more frequently on sentences which take longer to parse, the measured speedup might be artificiMly high.", "labels": [], "entities": []}, {"text": "This is easily realized, as simply removing the hardest sentences froln the corpus would cause a decrease ill the average parsing time, and thus result in a speedup, without any pruning at all.", "labels": [], "entities": []}, {"text": "To factor out the contribution of uncovered sentences fi'om the results, the performance of a two-stage architecture analogous to that of (Samuelsson and Rayner, 1991) was silnulated, in which the pruned grammar is attempted \"A Sentence\" Parser with specialized grammar first, and the sentence is passed onto the original unpruned grammar whenever the pruned grammar fails to return a parse (see).", "labels": [], "entities": []}, {"text": "The measured speedup of this simulated architecture, which preserves the anyparse measure of the original grammar, takes into account the contribution of uncovered sentences, as it penalizes sweeping difficult sentences under the carpet.", "labels": [], "entities": []}, {"text": "The results of the experiments described in the section above are summarized in the table in.", "labels": [], "entities": []}, {"text": "The upper part of the table refers to experiments with the French grammar, the lower part to experiments with the English grammar.", "labels": [], "entities": [{"text": "English grammar", "start_pos": 114, "end_pos": 129, "type": "DATASET", "confidence": 0.900449275970459}]}, {"text": "For each language, the first line presents data gathered for the original grammar for comparison with the pruned grammars.", "labels": [], "entities": []}, {"text": "The figures in the second line were collected by pruning the grammar based on the whole corpus, and then testing on the corpus itself.", "labels": [], "entities": []}, {"text": "The grammars obtained in this way contain 516 and 388 disjuncts --corresponding to purely concatenative rules --for French and English respectively.", "labels": [], "entities": []}, {"text": "Anyparse and coverage are not, of course, relevant in this case, but the statistics on parsing time are, especially the one on the maximum parsing time.", "labels": [], "entities": [{"text": "Anyparse", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9592030644416809}, {"text": "coverage", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9956458210945129}, {"text": "parsing", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.9808603525161743}]}, {"text": "For each iteration in the 10-fold cross-validation experiment, the maximum parsing time was retained, and those ten times were eventually averaged.", "labels": [], "entities": []}, {"text": "If pruning tended to leave sentences which take long to parse uncovered, then we would observe a significant difference between the average over ma.ximum times on the grammar trained and tested on the same corpus (which parses all sentences, including the hardest), and the average over maximum times for grammars trained and tested on different sets.", "labels": [], "entities": []}, {"text": "The fact that this does not seem to be the case indicates that pruning does not penalize difficult sentences.", "labels": [], "entities": []}, {"text": "Note also that the average number of parses per sentence is significantly smaller than with the full grammar, of almost a factor of 9 in the case of the French graminar.", "labels": [], "entities": [{"text": "French graminar", "start_pos": 153, "end_pos": 168, "type": "DATASET", "confidence": 0.9011027216911316}]}, {"text": "The third line contains results for the fully pruned grammar.", "labels": [], "entities": []}, {"text": "In the case of the French grammar a speedup of about 6 is obtained with a loss in coverage of 13%.", "labels": [], "entities": [{"text": "speedup", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9958427548408508}, {"text": "coverage", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9966700673103333}]}, {"text": "The smaller speedup gained with the English grammar can be explained by the fact that here, the parsing times are lower in general, and that a non-negligible part of this time, especially that needed for morphological analysis, is unaffected by pruning.", "labels": [], "entities": [{"text": "speedup", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9697170257568359}]}, {"text": "Even in the case of the English grammar, though, speedup is substantial (2.67).", "labels": [], "entities": [{"text": "speedup", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9968088269233704}]}, {"text": "For both grammars, the reduction in the average maxinmm parsing time is particularly good, confirming our hypothesis that trimming the grammar by removing heavy constructs makes it considerably more efficient.", "labels": [], "entities": [{"text": "maxinmm parsing time", "start_pos": 48, "end_pos": 68, "type": "METRIC", "confidence": 0.7720617850621542}]}, {"text": "A partially negative note comes from the average number of disjuncts in the prun.ed grainmars, which is 501 for French and 374 for English.", "labels": [], "entities": [{"text": "prun.ed grainmars", "start_pos": 76, "end_pos": 93, "type": "DATASET", "confidence": 0.6783809959888458}]}, {"text": "Comparing this figures to the number of disjuncts in grammars pruned on the full corpus (516 and 388), we find that after training on nine tenths of the corpus, adding the last tenth still leads to an increase of 3-4% in the size of the resulting grammars.", "labels": [], "entities": []}, {"text": "In other words, the marginal gain of further training examples is still significant after considering about 900 sentences, indicating that the training corpora are somewhat too small.", "labels": [], "entities": []}, {"text": "The last two lines for each language show figures for grammars with pruning inhibited on the most variable and the two most variable symbols respectively.", "labels": [], "entities": []}, {"text": "For both languages, inhibiting pruning on the most variable symbol has the expected effect of increasing both parsing time and coverage.", "labels": [], "entities": [{"text": "parsing", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.9743499159812927}, {"text": "coverage", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9718064665794373}]}, {"text": "Inhibiting pruning also on the second most variable symbol has ahnost no effect for French, and only a small effect for English.", "labels": [], "entities": []}, {"text": "The table in summarizes the measures on the simulated two-stage architecture.", "labels": [], "entities": []}, {"text": "For both languages the best trade-off, once the distribution of uncovered sentences has been taken into account, is achieved by the fully pruned grammars.", "labels": [], "entities": []}], "tableCaptions": []}