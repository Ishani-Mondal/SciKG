{"title": [{"text": "Ranking suspected answers to natural language questions using predictive annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we describe a system to rank suspected answers to natural language questions.", "labels": [], "entities": [{"text": "rank suspected answers to natural language questions", "start_pos": 39, "end_pos": 91, "type": "TASK", "confidence": 0.6765391400882176}]}, {"text": "We process both corpus and query using anew technique, predictive annotation, which augments phrases in texts with labels anticipating their being targets of certain kinds of questions.", "labels": [], "entities": []}, {"text": "Given a natural language question, our IR system returns a set of matching passages, which we then rank using a linear function of seven predictor variables.", "labels": [], "entities": [{"text": "IR", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9525832533836365}]}, {"text": "We provide an evaluation of the techniques based on results from the TREC Q&A evaluation in which our system participated .", "labels": [], "entities": [{"text": "TREC Q&A evaluation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.5912331819534302}]}], "introductionContent": [{"text": "Question Answering is a task that calls fora combination of techniques from Information Retrieval and Natural Language Processing.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8181628882884979}, {"text": "Information Retrieval", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.7582381665706635}]}, {"text": "The former has the advantage of years of development of efficient techniques for indexing and searching large collections of data, but lacks of any meaningful treatment of the semantics of the query or the texts indexed.", "labels": [], "entities": []}, {"text": "NLP tackles the semantics, but tends to be computationally expensive.", "labels": [], "entities": []}, {"text": "We have attempted to carve out a middle ground, whereby we use a modified IR system augmented by shallow NL parsing.", "labels": [], "entities": [{"text": "NL parsing", "start_pos": 105, "end_pos": 115, "type": "TASK", "confidence": 0.6537000089883804}]}, {"text": "Our approach was motivated by the following problem with traditional IR systems.", "labels": [], "entities": [{"text": "IR", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9761467576026917}]}, {"text": "Suppose the user asks \"Where did <some event> happen?\".", "labels": [], "entities": []}, {"text": "If the system does no pre-processing of the query, then \"where\" will be included in the bag of words submitted to the search engine, but this will not be helpful since the target text will be unlikely to contain the word \"where\".", "labels": [], "entities": []}, {"text": "If the word is stripped out as a stop-word, then * The work presented in this paper was performed while the first and third authors were at 1BM Research.", "labels": [], "entities": [{"text": "1BM Research", "start_pos": 140, "end_pos": 152, "type": "DATASET", "confidence": 0.9128116071224213}]}, {"text": "the search engine will have no idea that a location is sought.", "labels": [], "entities": []}, {"text": "Our approach, called predictive annotation, is to augment the query with semantic category markers (which we call QATokens), in this case with the PLACES token, and also to label with QA-Tokens all occurrences in text that are recognized entities, (for example, places).", "labels": [], "entities": []}, {"text": "Then traditional bag-ofwords matching proceeds successfully, and will return matching passages.", "labels": [], "entities": []}, {"text": "The answer-selection process then looks for and ranks in these passages occurrences of phrases containing the particular QA-Token(s) from the augmented query.", "labels": [], "entities": []}, {"text": "This classification of questions is conceptually similar to the query expansion in but is expected to achieve much better performance since potentially matching phrases in text are classified in a similar and synergistic way.", "labels": [], "entities": []}, {"text": "Our system participated in the official TREC Q&A evaluation.", "labels": [], "entities": [{"text": "TREC Q&A", "start_pos": 40, "end_pos": 48, "type": "TASK", "confidence": 0.7790414988994598}]}, {"text": "For 200 questions in the evaluation set, we were asked to provide a list of 50-byte and 250-byte extracts from a 2-GB corpus.", "labels": [], "entities": []}, {"text": "The results are shown in Section 7.", "labels": [], "entities": []}, {"text": "Some techniques used by other participants in the TREC evaluation are paragraph indexing, followed by abductive inference) and knowledge-representation combined with information retrieval).", "labels": [], "entities": [{"text": "TREC evaluation", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.8242478966712952}, {"text": "paragraph indexing", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7874113619327545}]}, {"text": "Some earlier systems related to our work are FaqFinder (), MURAX, which uses an encyclopedia as a knowledge base from which to extract answers, and PROFILE) which identifies named entities and noun phrases that describe them in text.", "labels": [], "entities": [{"text": "FaqFinder", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.8622307777404785}, {"text": "MURAX", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.5159569382667542}]}, {"text": "neat (AnSel/Werlect) that extracts and ranks potential answers from these texts.", "labels": [], "entities": [{"text": "AnSel", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.7322929501533508}]}, {"text": "This paper focuses on the process of ranking potential answers selected by the IR engine, which is itself described in).", "labels": [], "entities": [{"text": "IR engine", "start_pos": 79, "end_pos": 88, "type": "TASK", "confidence": 0.8048668503761292}]}, {"text": "~ lndexer: System Architecture.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the performance of our system using results from our four official runs.", "labels": [], "entities": []}, {"text": "For each question, the performance is computed as the reciprocal value of the rank (RAR) of the highest-ranked correct answer given by the system.", "labels": [], "entities": [{"text": "rank (RAR)", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.6934168785810471}]}, {"text": "For example, if the system has given the correct answer in three positions: second, third, and fifth, RAR for that question is ! 2\" The Mean Reciprocal Answer Rank (MRAR) is used to compute the overall performance of systems participating in the TREC evaluation:: Performance on groups often questions Finally, shows how our official runs compare to the rest of the 25 official submissions.", "labels": [], "entities": [{"text": "RAR", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9993234872817993}, {"text": "Mean Reciprocal Answer Rank (MRAR)", "start_pos": 136, "end_pos": 170, "type": "METRIC", "confidence": 0.9497408441134861}, {"text": "TREC evaluation", "start_pos": 246, "end_pos": 261, "type": "TASK", "confidence": 0.7923269867897034}]}, {"text": "Our performance using AnSel and 50-byte output was 0.430.", "labels": [], "entities": [{"text": "AnSel", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.507979691028595}]}, {"text": "The performance of Werlect was 0.395.", "labels": [], "entities": [{"text": "Werlect", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.5097297430038452}]}, {"text": "On 250 bytes, AnSel scored 0.319 and Werlect -0.280.", "labels": [], "entities": [{"text": "AnSel", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.8051543235778809}, {"text": "Werlect -0.280", "start_pos": 37, "end_pos": 51, "type": "METRIC", "confidence": 0.8767580389976501}]}], "tableCaptions": [{"text": " Table 3: Influence of frequency on span rank.", "labels": [], "entities": [{"text": "Influence", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.935086727142334}]}, {"text": " Table 4: Performance of A50 on T200", "labels": [], "entities": [{"text": "A50", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.8831268548965454}, {"text": "T200", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.8237262964248657}]}, {"text": " Table 7: Comparison of our system with the other participants", "labels": [], "entities": []}]}