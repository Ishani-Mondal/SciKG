{"title": [{"text": "Word-for-Word Glossing with Contextually Similar Words", "labels": [], "entities": [{"text": "Word-for-Word Glossing with Contextually Similar Words", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.711950957775116}]}], "abstractContent": [{"text": "Many corpus-based machine translation systems require parallel corpora.", "labels": [], "entities": [{"text": "corpus-based machine translation", "start_pos": 5, "end_pos": 37, "type": "TASK", "confidence": 0.689155101776123}]}, {"text": "In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus.", "labels": [], "entities": []}, {"text": "To gloss a word, we first identify its similar words that occurred in the same context in a large corpus.", "labels": [], "entities": []}, {"text": "We then determine the gloss by maximizing the similarity between the set of contextually similar words and the different translations of the word in a bilingual thesaurus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order.", "labels": [], "entities": [{"text": "Word-for-word glossing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.63363316655159}]}, {"text": "Automating this process would benefit many NLP applications.", "labels": [], "entities": []}, {"text": "For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts.", "labels": [], "entities": [{"text": "crosslanguage information retrieval", "start_pos": 16, "end_pos": 51, "type": "TASK", "confidence": 0.7003295322259268}]}, {"text": "Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system.", "labels": [], "entities": [{"text": "full-fledged machine translation (MT)", "start_pos": 73, "end_pos": 110, "type": "TASK", "confidence": 0.8023742834726969}]}, {"text": "Many corpus-based MT systems require parallel corpora (.", "labels": [], "entities": [{"text": "MT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.917881190776825}]}, {"text": "used a word sense disambiguation algorithm and a non-paralM bilingual corpus to resolve translation ambiguity.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 7, "end_pos": 32, "type": "TASK", "confidence": 0.7103098034858704}, {"text": "translation ambiguity", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.8342447876930237}]}, {"text": "In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus.", "labels": [], "entities": []}, {"text": "The intuitive idea behind our algorithm is the following.", "labels": [], "entities": []}, {"text": "Suppose w is a word to be translated.", "labels": [], "entities": []}, {"text": "We first identify a set of words similar tow that occurred in the same context as win a large corpus.", "labels": [], "entities": []}, {"text": "We then use this set (called the contextually similar words of w) to select a translation for w.", "labels": [], "entities": []}, {"text": "For example, the contextually similar words of duty in fiduciary duty include responsibility, obligation, role, ...", "labels": [], "entities": []}, {"text": "This list is then used to select a translation for duty.", "labels": [], "entities": []}, {"text": "In the next section, we describe the resources required by our algorithm.", "labels": [], "entities": []}, {"text": "In Section 3, we present an algorithm for constructing the contextually similar words of a word in a context.", "labels": [], "entities": []}, {"text": "Section 4 presents the word-for-word glossing algorithm and Section 5 describes the group similarity metric used in our algorithm.", "labels": [], "entities": []}, {"text": "In Section 6, we present some experimental results and finally, in Section 7, we conclude with a discussion of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The design of our glossing algorithm is applicable to any source/destination language pair as long as a source language parser is available.", "labels": [], "entities": []}, {"text": "We considered English-to-French translations in our experiments.", "labels": [], "entities": []}, {"text": "We experimented with six English nouns that have multiple French translations: account, duty, race, suit, check, and record.", "labels": [], "entities": [{"text": "check", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9458295702934265}, {"text": "record", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.968690037727356}]}, {"text": "Using the 1987 Wall Street Journal files on the LDC/DCI CD-ROM, we extracted a testing corpus 4 consisting of the first 100 to 300 sentences containing the non-idiomatic usage of the six nouns s.", "labels": [], "entities": [{"text": "Wall Street Journal files", "start_pos": 15, "end_pos": 40, "type": "DATASET", "confidence": 0.9502274394035339}, {"text": "LDC/DCI CD-ROM", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.8767084926366806}]}, {"text": "Then, we manually tagged each sentence with one of the candidate translations shown in.", "labels": [], "entities": []}, {"text": "Each noun in translates more frequently to one candidate translation than the other.", "labels": [], "entities": []}, {"text": "In fact, always choosing the candidate procbs as the translation for suit yields 94% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9987939596176147}]}, {"text": "A better measure for evaluating the system's classifications considers both the algorithm's precision and recall on each candidate translation.", "labels": [], "entities": [{"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9995962977409363}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9991833567619324}]}, {"text": "illustrates the precision and recall of our glossing algorithm for each candidate translation.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9995582699775696}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9989989399909973}]}, {"text": "Albeit precision and recall are used to evaluate the quality of the classifications, overall accuracy is sufficient for comparing different approaches with our system.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9995548129081726}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9996418952941895}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9993470311164856}]}, {"text": "In Section 3, we presented an algorithm for identifying the contextually similar words of a word in a context using a corpus-based thesaurus and a collocation database.", "labels": [], "entities": []}, {"text": "Each of the six nouns has similar words in the corpus-based thesaurus.", "labels": [], "entities": []}, {"text": "However, in order to find contextually similar words, at least one similar word for each noun must occur in the collocation database in a given context.", "labels": [], "entities": []}, {"text": "Thus, the algorithm for constructing contextually similar words is dependent on the coverage of the collocation database.", "labels": [], "entities": []}, {"text": "We estimated this coverage by counting the number of times each of the six nouns, in several different contexts, has at least one contextually similar word.", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9364305138587952}]}, {"text": "The result is shown in.", "labels": [], "entities": []}, {"text": "In Section 5, we described a group similarity metric, groupSim, which we use for comparing a WAT with a set of contextually similar words.", "labels": [], "entities": []}, {"text": "In, we compare the translation accuracy of our algorithm using other group similarity metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.6299048066139221}]}, {"text": "Suppose G~ and (/2 are two groups of words and w is the word that we wish to translate.", "labels": [], "entities": []}, {"text": "The metrics used are: I. closest& sum of similarity of the three closest pairs of words from each group.", "labels": [], "entities": []}, {"text": "as defined in Section 5.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Clustered similar words of duty as given by (Lin,", "labels": [], "entities": []}, {"text": " Table 3. Group similarity scores between the contextually  similar words of duty in corporate duty and fiduciary duty  with the WATs for candidate translations devoir and taxe.", "labels": [], "entities": [{"text": "Group similarity", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.5936817228794098}]}, {"text": " Table 5. Precision vs. Recall for each candidate translation.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9898597598075867}, {"text": "Recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.7116281986236572}]}, {"text": " Table 6. The coverage of the collocation database, shown  by the frequency with which a word in a given context has  at least one contextually similar word.", "labels": [], "entities": []}]}