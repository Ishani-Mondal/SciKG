{"title": [{"text": "Cut and Paste Based Text Summarization", "labels": [], "entities": [{"text": "Summarization", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.7448979616165161}]}], "abstractContent": [{"text": "We present a cut and paste based text summa-rizer, which uses operations derived from an analysis of human written abstracts.", "labels": [], "entities": []}, {"text": "The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge re-suiting phrases together as coherent sentences.", "labels": [], "entities": []}, {"text": "Our work includes a statistically based sentence decomposition program that identifies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which we used to develop the summarizer.", "labels": [], "entities": [{"text": "sentence decomposition", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7421757578849792}]}], "introductionContent": [{"text": "There is a big gap between the summaries produced by current automatic summarizers and the abstracts written by human professionals.", "labels": [], "entities": [{"text": "summaries produced", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9097810685634613}]}, {"text": "Certainly one factor contributing to this gap is that automatic systems cannot always correctly identify the important topics of an article.", "labels": [], "entities": []}, {"text": "Another factor, however, which has received little attention, is that automatic summarizers have poor text generation techniques.", "labels": [], "entities": [{"text": "text generation", "start_pos": 102, "end_pos": 117, "type": "TASK", "confidence": 0.7432961761951447}]}, {"text": "Most automatic summarizers rely on extracting key sentences or paragraphs from an article to produce a summary.", "labels": [], "entities": []}, {"text": "Since the extracted sentences are disconnected in the original article, when they are strung together, the resulting summary can be inconcise, incoherent, and sometimes even misleading.", "labels": [], "entities": []}, {"text": "We present a cut and paste based text summarization technique, aimed at reducing the gap between automatically generated summaries and human-written abstracts.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.5796803385019302}]}, {"text": "Rather than focusing on how to identify key sentences, as do other researchers, we study how to generate the text of a summary once key sentences have been extracted.", "labels": [], "entities": []}, {"text": "The main idea of cut and paste summarization is to reuse the text in an article to generate the summary.", "labels": [], "entities": []}, {"text": "However, instead of simply extracting sentences as current summarizers do, the cut and paste system will \"smooth\" the extracted sentences by editing them.", "labels": [], "entities": []}, {"text": "Such edits mainly involve cutting phrases and pasting them together in novel ways.", "labels": [], "entities": []}, {"text": "The key features of this work are: (1) The identification of cutting and pasting operations.", "labels": [], "entities": [{"text": "identification of cutting and pasting", "start_pos": 43, "end_pos": 80, "type": "TASK", "confidence": 0.8182534337043762}]}, {"text": "We identified six operations that can be used alone or together to transform extracted sentences into sentences in human-written abstracts.", "labels": [], "entities": []}, {"text": "The operations were identified based on manual and automatic comparison of human-written abstracts and the original articles.", "labels": [], "entities": []}, {"text": "Examples include sentence reduction, sentence combination, syntactic transformation, and lexical paraphrasing.", "labels": [], "entities": [{"text": "sentence reduction", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7721415162086487}, {"text": "sentence combination", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.7641488611698151}, {"text": "syntactic transformation", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.7332872301340103}]}, {"text": "(2) Development of an automatic system to perform cut and paste operations.", "labels": [], "entities": []}, {"text": "Two operations -sentence reduction and sentence combination -are most effective in transforming extracted sentences into summary sentences that are as concise and coherent as in human-written abstracts.", "labels": [], "entities": [{"text": "sentence reduction", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7248959839344025}, {"text": "sentence combination", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.7280414700508118}]}, {"text": "We implemented a sentence reduction module that removes extraneous phrases from extracted sentences, and a sentence combination module that merges the extracted sentences or the reduced forms resulting from sentence reduction.", "labels": [], "entities": [{"text": "sentence reduction", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.6982333958148956}, {"text": "sentence reduction", "start_pos": 207, "end_pos": 225, "type": "TASK", "confidence": 0.7394816875457764}]}, {"text": "Our sentence reduction model determines what to cut based on multiple sources of information, including syntactic knowledge, context, and statistics learned from corpus analysis.", "labels": [], "entities": [{"text": "sentence reduction", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7217719554901123}]}, {"text": "It improves the conciseness of extracted sentences, making them concise and on target.", "labels": [], "entities": []}, {"text": "Our sentence combination module implements combination rules that were identified by observing examples written by human professionals.", "labels": [], "entities": [{"text": "sentence combination", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.6990730315446854}]}, {"text": "It improves the coherence of extracted sentences.", "labels": [], "entities": []}, {"text": "(3) Decomposing human-wrltten summary sentences.", "labels": [], "entities": [{"text": "Decomposing human-wrltten summary sentences", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.8351747989654541}]}, {"text": "The cut and paste technique we propose here is anew computational model which we based on analysis of human-written abstracts.", "labels": [], "entities": []}, {"text": "To do this analysis, we developed an automatic system that can match a phrase in a human-written abstract to the corresponding phrase in the article, identifying its most likely location.", "labels": [], "entities": []}, {"text": "This decomposition program allows us to analyze the construction of sentences in a human-written abstract.", "labels": [], "entities": []}, {"text": "Its results have been used to train and test the sentence reduction and sentence combination module.", "labels": [], "entities": [{"text": "sentence reduction", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7704816162586212}, {"text": "sentence combination", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.733844518661499}]}, {"text": "In Section 2, we discuss the cut and paste technique in general, from both a professional and computational perspective.", "labels": [], "entities": []}, {"text": "We also describe the six cut and paste operations.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the system architecture.", "labels": [], "entities": []}, {"text": "The major components of the system, including sentence reduction, sentence combination, decomposition, and sentence selection, are described in Section 4.", "labels": [], "entities": [{"text": "sentence reduction", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7737716436386108}, {"text": "sentence combination", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.7548496127128601}, {"text": "sentence selection", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.76869136095047}]}, {"text": "The evaluation results are shown in Section 5.", "labels": [], "entities": []}, {"text": "Related work is discussed in Section 6.", "labels": [], "entities": []}, {"text": "Finally, we conclude and discuss future work.", "labels": [], "entities": []}, {"text": "Document sentence: When it arrives sometime next year in new TV sets, the V-chip will give parents anew and potentially revolutionary device to block out programs they don't want their children to see.", "labels": [], "entities": []}, {"text": "Summary sentence: The V-chip will give parents a device to block out programs they don't want their children to see.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our evaluation includes separate evaluations of each module and the final evaluations of the overall system.", "labels": [], "entities": []}, {"text": "We evaluated the decomposition program by two experiments, described in ().", "labels": [], "entities": []}, {"text": "In the first experiment, we selected 50 human-written abstracts, consisting of 305 sentences in total.", "labels": [], "entities": []}, {"text": "A human subject then read the decomposition results of these sentences to judge whether they are correct.", "labels": [], "entities": []}, {"text": "93.8% of the sentences were correctly decomposed.", "labels": [], "entities": []}, {"text": "In the second experiment, we tested the system in a summary alignment task.", "labels": [], "entities": [{"text": "summary alignment task", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7140129804611206}]}, {"text": "We ran the decomposition program to identify the source document sentences that were used to construct the sentences in human-written abstracts.", "labels": [], "entities": []}, {"text": "Human subjects were also asked to select the document sentences that are semantlc-equivalent to the sentences in the abstracts.", "labels": [], "entities": []}, {"text": "We compared the set of sentences identified by the program with the set of sentences selected by the majority of human subjects, which is used as the gold standard in the computation of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.9985507130622864}, {"text": "recall", "start_pos": 200, "end_pos": 206, "type": "METRIC", "confidence": 0.9653624892234802}]}, {"text": "The program achieved an average 81.5% precision, 78.5% recall, and 79.1% f-measure for 10 documents.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9966074228286743}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9991785883903503}, {"text": "f-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9980093836784363}]}, {"text": "The average performance of 14 human judges is 88.8% precision, 84.4% recall, and 85.7% f-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9996902942657471}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9997383952140808}, {"text": "f-measure", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9936623573303223}]}, {"text": "Recently, we have also tested the system on legal documents (the headnotes used by Westlaw company), and the program works well on those documents too.", "labels": [], "entities": [{"text": "Westlaw company", "start_pos": 83, "end_pos": 98, "type": "DATASET", "confidence": 0.9670539498329163}]}, {"text": "The evaluation of sentence reduction (see) for details) used a corpus of 500 sentences and their reduced forms in human-written abstracts.", "labels": [], "entities": [{"text": "sentence reduction", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7112656980752945}]}, {"text": "400 sentences were used to compute corpus probabilities and 100 sentences were used for testing.", "labels": [], "entities": []}, {"text": "The results show that 81.3% of the reduction decisions made by the system agreed with those of humans.", "labels": [], "entities": []}, {"text": "The humans reduced the length of the 500 sentences by 44.2% on average, and the system reduced the length of the 100 test sentences by 32.7%.", "labels": [], "entities": [{"text": "length", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9535454511642456}]}, {"text": "The evaluation of sentence combination module is not as straightforward as that of decomposition or reduction since combination happens later in the pipeline and it depends on the output from prior Example 1: add descriptions or names for people or organization Original document sentences: \"We're trying to prove that there are big benefits to the patients by involving them more deeply in their treatment\", said Paul Clayton, Chairman of the Department dealing with computerized medical information at Columbia.", "labels": [], "entities": []}, {"text": "\"The economic payoff from breaking into healthcare records is a lot less than for banks\", said Clayton at Columbia.", "labels": [], "entities": []}, {"text": "Combined sentence: \"The economic payoff from breaking into healthcare records is a lot less than for banks\", said Paul Clayton, Chairman of the Department dealing with computerized medical information at Columbia.", "labels": [], "entities": []}, {"text": "Professional: (the same) Example 2: extract common subjects Original document sentences: The new measure is an echo of the original bad idea, blurred just enough to cloud prospects both for enforcement and for court review.", "labels": [], "entities": []}, {"text": "Unlike the 1996 act, this one applies only to commercial Web sites -thus sidestepping 1996 objections to the burden such regulations would pose for museums, libraries and freewheeling conversation deemed \"indecent\" by somebody somewhere.", "labels": [], "entities": []}, {"text": "The new version also replaces the vague \"indecency\" standard, to which the court objected, with the better-defined one of material ruled \"harmful to minors.\"", "labels": [], "entities": []}, {"text": "Combined sentences: The new measure is an echo of the original bad idea.", "labels": [], "entities": []}, {"text": "The new version applies only to commercial web sites and replaces the vague \"indecency\" standard with the better-defined one of material ruled \"harmful to minors.\"", "labels": [], "entities": []}, {"text": "Professional: While the new law replaces the \"indecency\" standard with \"harmful to minors\" and now only applies to commercial Web sites, the \"new measure is an echo of the original bad idea.\"", "labels": [], "entities": []}, {"text": "To evaluate just the combination component, we assume that the system makes the same reduction decision as humans and the co-reference system has a perfect performance.", "labels": [], "entities": []}, {"text": "This involves manual tagging of some examples to prepare for the evaluation; this preparation is in progress.", "labels": [], "entities": []}, {"text": "The evaluation of sentence combination will focus on the accessment of combination rules.", "labels": [], "entities": [{"text": "sentence combination", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.7048145681619644}]}, {"text": "The overM1 system evMuation includes both intrinsic and extrinsic evaluation.", "labels": [], "entities": []}, {"text": "In the intrinsic evMuation, we asked human subjects to compare the quality of extraction-based summaries and their revised versions produced by our sentence reduction and combination modules.", "labels": [], "entities": []}, {"text": "We selected 20 documents; three different automatic summarizers were used to generate a summary for each document, producing 60 summaries in total.", "labels": [], "entities": []}, {"text": "These summaries are all extraction-based.", "labels": [], "entities": []}, {"text": "We then ran our sentence reduction and sentence combination system to revise the summaries, producing a revised version for each summary.", "labels": [], "entities": [{"text": "sentence reduction", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7270370572805405}]}, {"text": "We presented human subjects with the full documents, the extraction-based summaries, and their revised versions, and asked them to compare the extraction-based summaries and their revised versions.", "labels": [], "entities": []}, {"text": "The human subjects were asked to score the conciseness of the summaries (extractionbased or revised) based on a scale from 0 to 10 -the higher the score, the more concise a summary is.", "labels": [], "entities": []}, {"text": "They were also asked to score the coherence of the summaries based on a scale from 0 to 10.", "labels": [], "entities": []}, {"text": "On average, the extraction-based summaries have a score of 4.2 for conciseness, while the revised summaries have a score of 7.9 (an improvement of 88%).", "labels": [], "entities": []}, {"text": "The average improvement for the three systems are 78%, 105%, and 88% respectively.", "labels": [], "entities": []}, {"text": "The revised summaries are on average 41% shorter than the original extractionbased summaries.", "labels": [], "entities": []}, {"text": "For summary coherence, the average score for the extraction-based summaries is 3.9, while the average score for the revised summaries is 6.1 (an improvement of 56%).", "labels": [], "entities": []}, {"text": "The average improvement for the three systems are 69%, 57%, and 53% respectively.", "labels": [], "entities": []}, {"text": "We are preparing a task-based evaluation, in which we will use the data from the Summarization EvMuation Conference () and compare how our revised summaries can influence humans' performance in tasks like text categorization and ad-hoc retrieval.", "labels": [], "entities": [{"text": "Summarization EvMuation Conference", "start_pos": 81, "end_pos": 115, "type": "TASK", "confidence": 0.7403795917828878}, {"text": "text categorization", "start_pos": 205, "end_pos": 224, "type": "TASK", "confidence": 0.7629007399082184}]}], "tableCaptions": []}