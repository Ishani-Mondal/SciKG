{"title": [{"text": "A Finite State and Data-Oriented Method for Grapheme to Phoneme Conversion", "labels": [], "entities": [{"text": "Grapheme to Phoneme Conversion", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.8042016178369522}]}], "abstractContent": [{"text": "A finite-state method, based on leftmost longest-match replacement, is presented for segmenting words into graphemes, and for converting graphemes into phonemes.", "labels": [], "entities": []}, {"text": "A small set of hand-crafted conversion rules for Dutch achieves a phoneme accuracy of over 93%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9938827753067017}]}, {"text": "The accuracy of the system is further improved by using transformation-based learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994687438011169}]}, {"text": "The phoneme accuracy of the best system (using a large rule and a 'lazy' variant of Brill's algoritm), trained on only 40K words, reaches 99%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9884448051452637}]}, {"text": "1 Introduction Automatic grapheme to phoneme conversion (i.e. the conversion of a string of characters into a string of phonemes) is essential for applications of text to speech synthesis dealing with unrestricted text, where the input may contain words which do not occur in the system dictionary.", "labels": [], "entities": [{"text": "phoneme conversion", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7331569790840149}, {"text": "speech synthesis", "start_pos": 171, "end_pos": 187, "type": "TASK", "confidence": 0.7549982964992523}]}, {"text": "Furthermore, a transducer for grapheme to phoneme conversion can be used to generate candidate replacements in a (pronunciation-sensitive) spelling correction system.", "labels": [], "entities": [{"text": "phoneme conversion", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7480672001838684}, {"text": "pronunciation-sensitive) spelling correction", "start_pos": 114, "end_pos": 158, "type": "TASK", "confidence": 0.6057830825448036}]}, {"text": "When given the pronunciation of a misspelled word, the inverse of the grapheme to phoneme transducer will generate all identically pronounced words.", "labels": [], "entities": []}, {"text": "Below, we present a method for developing such grapheme to phoneme transducers based on a combination of hand-crafted conversion rules, implemented using finite state calculus, and automatically induced rules.", "labels": [], "entities": []}, {"text": "The hand-crafted system is defined as a two-step procedure: segmentation of the input into a sequence of graphemes (i.e. sequences of one or more characters typically corresponding to a single phoneme) and conversion of graphemes into (se-quences of) phonemes.", "labels": [], "entities": [{"text": "segmentation of the input into a sequence of graphemes (i.e. sequences of one or more characters typically corresponding to a single phoneme) and conversion of graphemes into (se-quences of) phonemes", "start_pos": 60, "end_pos": 259, "type": "Description", "confidence": 0.7669570533668294}]}, {"text": "The composition of the transducer which performs segmentation and the transducer defined by the conversion rules, is a transducer which converts sequences of characters into sequences of phonemes.", "labels": [], "entities": []}, {"text": "Specifying the conversion rules is a difficult task.", "labels": [], "entities": [{"text": "conversion", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.8929594159126282}]}, {"text": "Although segmentation of the input can in principle be dispensed with, we found that writing conversion rules for segmented input substantially reduces the context-sensitivity and order-dependence of such rules.", "labels": [], "entities": []}, {"text": "We manually developed a grapheme to phoneme transducer for Dutch data obtained from CELEX (Baayen et al., 1993) and achieved a word accuracy of 60.6% and a phoneme accuracy of 93.6%.", "labels": [], "entities": [{"text": "Dutch data obtained from CELEX (Baayen et al., 1993)", "start_pos": 59, "end_pos": 111, "type": "DATASET", "confidence": 0.8629985849062601}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.825203537940979}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.8937602043151855}]}, {"text": "To improve the performance of our system, we used transformation-based learning (TBL) (Brill, 1995).", "labels": [], "entities": []}, {"text": "Training data are obtained by aligning the output of the hand-crafted finite state transducer with the correct phoneme strings.", "labels": [], "entities": []}, {"text": "These data can then be used as input for TBL, provided that suitable rule templates are available.", "labels": [], "entities": [{"text": "TBL", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.6855096817016602}]}, {"text": "We performed several experiments, in which the amount of' training data, the algorithm (Brill's original formulation and 'lazy' variants (Samuel et al., 1998)), and the number of rule templates varied.", "labels": [], "entities": []}, {"text": "The best experiment (40K words, using a 'lazy' strategy with a large set of rule templates) induces over 2000 transformation rules, leading to 92.6% word accuracy and 99.0% phoneme accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9406853914260864}, {"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9491711854934692}]}, {"text": "This result, obtained using a relatively small set of training data, compares well with that of other systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic grapheme to phoneme conversion (i.e. the conversion of a string of characters into a string of phonemes) is essential for applications of text to speech synthesis dealing with unrestricted text, where the input may contain words which do not occur in the system dictionary.", "labels": [], "entities": [{"text": "phoneme conversion", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7521288692951202}, {"text": "speech synthesis", "start_pos": 156, "end_pos": 172, "type": "TASK", "confidence": 0.7605588436126709}]}, {"text": "Furthermore, a transducer for grapheme to phoneme conversion can be used to generate candidate replacements in a (pronunciation-sensitive) spelling correction system.", "labels": [], "entities": [{"text": "phoneme conversion", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7480672001838684}, {"text": "pronunciation-sensitive) spelling correction", "start_pos": 114, "end_pos": 158, "type": "TASK", "confidence": 0.6057830825448036}]}, {"text": "When given the pronunciation of a misspelled word, the inverse of the grapheme to phoneme transducer will generate all identically pronounced words.", "labels": [], "entities": []}, {"text": "Below, we present a method for developing such grapheme to phoneme transducers based on a combination of hand-crafted conversion rules, implemented using finite state calculus, and automatically induced rules.", "labels": [], "entities": []}, {"text": "The hand-crafted system is defined as a twostep procedure: segmentation of the input into a sequence of graphemes (i.e. sequences of one or more characters typically corresponding to a single phoneme) and conversion of graphemes into (sequences of) phonemes.", "labels": [], "entities": [{"text": "segmentation of the input into a sequence of graphemes (i.e. sequences of one or more characters typically corresponding to a single phoneme) and conversion of graphemes into (sequences of) phonemes", "start_pos": 59, "end_pos": 257, "type": "Description", "confidence": 0.7662211130647099}]}, {"text": "The composition of the transducer which performs segmentation and the transducer defined by the conversion rules, is a transducer which converts sequences of characters into sequences of phonemes.", "labels": [], "entities": []}, {"text": "Specifying the conversion rules is a difficult task.", "labels": [], "entities": [{"text": "conversion", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.8929595947265625}]}, {"text": "Although segmentation of the input can in principle be dispensed with, we found that writing conversion rules for segmented input substantially reduces the context-sensitivity and order-dependence of such rules.", "labels": [], "entities": []}, {"text": "We manually developed a grapheme to phoneme transducer for Dutch data obtained from CELEX ( and achieved a word accuracy of 60.6% and a phoneme accuracy of 93.6%.", "labels": [], "entities": [{"text": "Dutch data obtained from CELEX", "start_pos": 59, "end_pos": 89, "type": "DATASET", "confidence": 0.8815454959869384}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.7715979218482971}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9143873453140259}]}, {"text": "To improve the performance of our system, we used transformation-based learning (TBL).", "labels": [], "entities": []}, {"text": "Training data are obtained by aligning the output of the hand-crafted finite state transducer with the correct phoneme strings.", "labels": [], "entities": []}, {"text": "These data can then be used as input for TBL, provided that suitable rule templates are available.", "labels": [], "entities": [{"text": "TBL", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.6855096817016602}]}, {"text": "We performed several experiments, in which the amount of' training data, the algorithm (Brill's original formulation and 'lazy' variants (), and the number of rule templates varied.", "labels": [], "entities": []}, {"text": "The best experiment (40K words, using a 'lazy' strategy with a large set of rule templates) induces over 2000 transformation rules, leading to 92.6% word accuracy and 99.0% phoneme accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9406853914260864}, {"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9491711854934692}]}, {"text": "This result, obtained using a relatively small set of training data, compares well with that of other systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the experiments with TBL we used the #-TBLpackage.", "labels": [], "entities": []}, {"text": "This Prolog implementation of TBL is considerably more efficient (up to ten times faster) than Brill's original (C) implementation.", "labels": [], "entities": []}, {"text": "The speed-up results mainly from using Prolog's first-argument indexing to access large quantities of data efficiently.", "labels": [], "entities": [{"text": "speed-up", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9672003388404846}]}, {"text": "We constructed a set of 22 rule templates which replace a predicted phoneme with a (corrected) phoneme on the basis of the underlying segment, and a context consisting either of phoneme strings, with a maximum length of two on either side, or a context consisting of graphemes, with a maximal length of 1 on either side.", "labels": [], "entities": []}, {"text": "Using only 20K words (which corresponds to almost 180K segments), and Brill's algorithm, we achieved a phoneme accuracy of 98.0% (see) on a test set of 20K words of unseen data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.954448938369751}]}, {"text": "6 Going to 40K words resulted in 98.4% phoneme accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9909838438034058}]}, {"text": "Note, however, that in spite of the relative efficiency of the implementation, CPU time also goes up sharply.", "labels": [], "entities": []}, {"text": "The heavy computation costs of TBL are due to the fact that for each error in the training data, all possible instantiations of the rule templates which correct this error are generated, and for each of these instantiated rules the score on the whole training set has to be computed.", "labels": [], "entities": [{"text": "TBL", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.7535474896430969}]}, {"text": "therefore propose an efficient, 'lazy', alternative, based on Monte Carlo sampling of the rules.", "labels": [], "entities": []}, {"text": "For each error in the training set, only a sample of the rules is considered which might correct it.", "labels": [], "entities": []}, {"text": "As rules which correct a high number of errors have a higher chance 6The statistics for less time consuming experiments were obtained by 10-fold cross-validation and for the more expensive experiments by 5-fold cross-validation. of being sampled at some point, higher scoring rules are more likely to be generated than lower scoring rules, but no exhaustive search is required.", "labels": [], "entities": []}, {"text": "We experimented with sampling sizes 5 and 10.", "labels": [], "entities": []}, {"text": "As CPU requirements are more modest, we managed to perform experiments on 60K words in this case, which lead to results which are comparable with Brill's algoritm applied to 40K words.", "labels": [], "entities": []}, {"text": "Apart from being able to work with larger data sets, the 'lazy' strategy also has the advantage that it can cope with larger sets of rule templates.", "labels": [], "entities": []}, {"text": "Brill's algorithm slows down quickly when the set of rule templates is extended, but for an algorithm based on rule sampling, this effect is much less severe.", "labels": [], "entities": []}, {"text": "Thus, we also constructed a set of 500 rule templates, containing transformation rules which allowed up to three graphemes or phoneme sequences as left or right context, and also allowed for disjunctive contexts (i.e. the context must contain an 'a' at the first or second position to the right).", "labels": [], "entities": []}, {"text": "We used this rule set in combination with a 'lazy' strategy with sampling size 5 (lazy(5)+ in.", "labels": [], "entities": []}, {"text": "This led to a further improvement of phoneme accuracy to 99.0%, and word accuracy of 92.6%, using only 40K words of training material.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.977715253829956}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.8235234022140503}]}, {"text": "Finally, we investigated what the contribution was of using a relatively accurate training set.", "labels": [], "entities": []}, {"text": "To this end, we constructed an alternative training set, in which every segment was associated with its most probable phoneme (where frequencies were obtained from the aligned CELEX data).", "labels": [], "entities": [{"text": "CELEX data", "start_pos": 176, "end_pos": 186, "type": "DATASET", "confidence": 0.8542382121086121}]}, {"text": "As shown in, the initial accuracy for such as system is much lower than that of the hand-crafted system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9636428952217102}]}, {"text": "The experimental results, for the 'lazy' algorithm with sampling size 5, show that the phoneme accuracy for training on 20K words is 0.3% less than for the corresponding experiment in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.957277238368988}]}, {"text": "For 40K words, the difference is still 0.2%, which, in both cases, corresponds to a difference in error rate of around 10%.", "labels": [], "entities": [{"text": "difference in error rate", "start_pos": 84, "end_pos": 108, "type": "METRIC", "confidence": 0.6986208409070969}]}, {"text": "As might be expected, the number of induced rules is much higher now, and thus cPu-requirements also increase substantially.", "labels": [], "entities": []}], "tableCaptions": []}