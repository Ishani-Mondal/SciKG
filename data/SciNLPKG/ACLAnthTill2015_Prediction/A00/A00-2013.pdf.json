{"title": [{"text": "Morphological Tagging: Data vs. Dictionaries", "labels": [], "entities": [{"text": "Morphological Tagging", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.860150545835495}]}], "abstractContent": [{"text": "Part of Speech tagging for English seems to have reached the the human levels of error, but full morphological tagging for inflectionally rich languages, such as Romanian, Czech, or Hungarian, is still an open problem, and the results are far from being satisfactory.", "labels": [], "entities": [{"text": "Speech tagging", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.7526627779006958}, {"text": "error", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9848781228065491}]}, {"text": "This paper presents results obtained by using a universalized exponential feature-based model for five such languages.", "labels": [], "entities": []}, {"text": "It focuses on the data sparseness issue, which is especially severe for such languages (the more so that there are no extensive annotated data for those languages).", "labels": [], "entities": []}, {"text": "In conclusion, we argue strongly that the use of an independent morphological dictionary is the preferred choice to more annotated data under such circumstances.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Training data in numbers", "labels": [], "entities": []}, {"text": " Table 9: Exponential w/feature selection vs. Max- imum Entropy tagger (Words-only Error Rate, no  dictionary)", "labels": [], "entities": []}, {"text": " Table 2: Results (Error rate, ER) on full training data, only true words counted (no punctuation)", "labels": [], "entities": [{"text": "Error rate", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9895722270011902}, {"text": "ER)", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9526771306991577}]}, {"text": " Table 3: Error rate on reduced training data, dictionary: automatic", "labels": [], "entities": [{"text": "Error rate", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9843382239341736}]}, {"text": " Table 4: Error rate on reduced training data, dictionary: mixed", "labels": [], "entities": [{"text": "Error rate", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.98392254114151}]}, {"text": " Table 6: POS Error rate on reduced training data, dictionary: automatic", "labels": [], "entities": [{"text": "POS Error rate", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.7741816441218058}]}, {"text": " Table 7: POS Error rate on reduced training data, dictionary: mixed", "labels": [], "entities": [{"text": "POS Error rate", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.6922784646352133}]}, {"text": " Table 8: POS Error rate on reduced training data, dictionary: \"independent\"", "labels": [], "entities": [{"text": "POS Error rate", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.6749809384346008}]}, {"text": " Table 10: Error rate comparison on reduced training data, automatic dictionary", "labels": [], "entities": [{"text": "Error rate", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9518206119537354}]}]}