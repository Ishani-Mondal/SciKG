{"title": [{"text": "Categorizing Unknown Words: Using Decision Trees to Identify Names and Misspellings", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces a system for categorizing unknown words.", "labels": [], "entities": []}, {"text": "The system is based on a multi-component architecture where each component is responsible for identifying one class of unknown words.", "labels": [], "entities": []}, {"text": "The focus of this paper is the components that identify names and spelling errors.", "labels": [], "entities": []}, {"text": "Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word.", "labels": [], "entities": []}, {"text": "The system is evaluated using data from live closed captions-a genre replete with a wide variety of unknown words.", "labels": [], "entities": []}], "introductionContent": [{"text": "In any real world use, a Natural Language Processing (NLP) system will encounter words that are not in its lexicon, what we term 'unknown words'.", "labels": [], "entities": []}, {"text": "Unknown words are problematic because a NLP system will perform well only if it recognizes the words that it is meant to analyze or translate: the more words a system does not recognize the more the system's performance will degrade.", "labels": [], "entities": []}, {"text": "Even when unknown words are infrequent, they can have a disproportionate effect on system quality.", "labels": [], "entities": []}, {"text": "For example, found that while only 0.6% of words in 300 e-mails were misspelled, this meant that 12% of the sentences contained an error (discussed in).", "labels": [], "entities": []}, {"text": "Words maybe unknown for many reasons: the word maybe a proper name, a misspelling, an abbreviation, a number, a morphological variant of a known word (e.g. recleared), or missing from the dictionary.", "labels": [], "entities": []}, {"text": "The first step in dealing with unknown words is to identify the class of the unknown word; whether it is a misspelling, a proper name, an abbreviation etc.", "labels": [], "entities": []}, {"text": "Once this is known, the proper action can betaken, misspellings can be corrected, abbreviations can be expanded and soon, as deemed necessary by the particular text processing application.", "labels": [], "entities": []}, {"text": "In this paper we introduce a system for categorizing unknown words.", "labels": [], "entities": []}, {"text": "The system is based on a multi-component architecture where each component is responsible for identifying one category of unknown words.", "labels": [], "entities": []}, {"text": "The main focus of this paper is the components that identify names and spelling errors.", "labels": [], "entities": []}, {"text": "Both components use a decision tree architecture to combine multiple types of evidence about the unknown word.", "labels": [], "entities": []}, {"text": "Results from the two components are combined using a weighted voting procedure.", "labels": [], "entities": []}, {"text": "The system is evaluated using data from live closed captions -a genre replete with a wide variety of unknown words.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2 we outline the overall architecture of the unknown word categorizer.", "labels": [], "entities": []}, {"text": "The name identifier and the misspelling identifier are introduced in section 3.", "labels": [], "entities": []}, {"text": "Performance and evaluation issues are discussed in section 4.", "labels": [], "entities": []}, {"text": "Section 5 considers portability issues.", "labels": [], "entities": [{"text": "portability", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9709893465042114}]}, {"text": "Section 6 compares the current system with relevant preceding research.", "labels": [], "entities": []}, {"text": "Concluding comments can be found in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate the unknown word categorizer introduced above.", "labels": [], "entities": []}, {"text": "We begin by describing the training and test data.", "labels": [], "entities": []}, {"text": "Following this, we evaluate the individual components and finally, we evaluate the decision making component.", "labels": [], "entities": [{"text": "decision making", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.7858626246452332}]}, {"text": "The training and test data for the decision tree consists of 7000 cases of unknown words extracted from a 2.6 million word corpus of live business news captions.", "labels": [], "entities": []}, {"text": "Of the 7000 cases, 70.4% were manually identified as names and 21.3% were identified as misspellings.The remaining cases were other types of unknown words such as abbreviations, morphological variants, etc.", "labels": [], "entities": []}, {"text": "Seventy percent of the data was randomly selected to serve as the training corpus.", "labels": [], "entities": []}, {"text": "The remaining thirty percent, or 2100 records, was reserved as the test corpus.", "labels": [], "entities": []}, {"text": "The test data consists often samples of 2100 records selected randomly with replacement from the test corpus.", "labels": [], "entities": []}, {"text": "We now consider the results of training a decision tree to identify misspellings using those features we introduced in the section on the misspelling identifier.", "labels": [], "entities": []}, {"text": "The tree was trained on the training data described above.", "labels": [], "entities": []}, {"text": "The tree was evaluated using each of the ten test data sets.", "labels": [], "entities": []}, {"text": "The average precision and recall data for the ten test sets are given in Table 3, together with the base-line case of assuming that we categorize all unknown words as names (the most common category).", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9993232488632202}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9993477463722229}]}, {"text": "With the baseline case we achieve 70.4% precision but with 0% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9996166229248047}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9993336796760559}]}, {"text": "In contrast, the decision tree approach obtains 77.1% precision and 73.8% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9994858503341675}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9991251826286316}]}, {"text": "We also trained a decision tree using not only the features identified in our discussion on misspellings but also those features that we introduced in our discussion of name identification.", "labels": [], "entities": [{"text": "name identification", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.810373842716217}]}, {"text": "The results for this tree can be found in the second line of.", "labels": [], "entities": []}, {"text": "The inclusion of the additional features has increased precision by approximately 5%.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9996232986450195}]}, {"text": "However, it has also decreased recall by about the same amount.", "labels": [], "entities": [{"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9996737241744995}]}, {"text": "The overall F-score is quite similar.", "labels": [], "entities": [{"text": "F-score", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9991827607154846}]}, {"text": "It appears that the name features are not predictive for identifying misspellings in this domain.", "labels": [], "entities": []}, {"text": "This is not surprising considering that eight of the ten features specified for name identification are concerned with features of the two preceding and two following words.", "labels": [], "entities": [{"text": "name identification", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.9159079492092133}]}, {"text": "Such word-external information is of little use in identifying a misspelling.", "labels": [], "entities": [{"text": "identifying a misspelling", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.8188743789990743}]}, {"text": "An analysis of the cases where the misspelling decision tree failed to identify a misspelling revealed two major classes of omissions.", "labels": [], "entities": []}, {"text": "The first class contains a collection of words which have typical characteristics of English words, but differ from the intended word by the addition or deletion of a syllable.", "labels": [], "entities": []}, {"text": "Words in this class include creditability for credibility, coordmatored for coordinated, and representires for representatives.", "labels": [], "entities": []}, {"text": "The second class contains misspellings that differ from known words by the deletion of a blank.", "labels": [], "entities": []}, {"text": "Examples in this class include webpage, crewmembers, and rainshower.", "labels": [], "entities": []}, {"text": "The second class of misspellings can be addressed by adding a feature that specifies whether the unknown word can be split up into two component known words.", "labels": [], "entities": []}, {"text": "Such a feature should provide strong predictability for the second class of words.", "labels": [], "entities": []}, {"text": "The first class of words are more of a challenge.", "labels": [], "entities": []}, {"text": "These words have a close homophonic relationship with the intended word rather than a close homographic relationship (as captured by edit distance).", "labels": [], "entities": []}, {"text": "Perhaps this class of words would benefit from a feature representing phonetic distance rather than edit distance.", "labels": [], "entities": []}, {"text": "Among those words which were incorrectly identified as misspellings, it is also possible to identify common causes for the misidentification.", "labels": [], "entities": []}, {"text": "Among these words are many foreign words which have character sequences which are not common in English.", "labels": [], "entities": []}, {"text": "Examples include khanehanalak, phytopla~2k-ton, brycee1~.", "labels": [], "entities": []}, {"text": "The results for our name identifier are given in.", "labels": [], "entities": []}, {"text": "Again, the decision tree approach is a significant improvement over the baseline case.", "labels": [], "entities": []}, {"text": "If we take the baseline approach and assume that all unknown words are names, then we would achieve a precision of 70.4%.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9992924928665161}]}, {"text": "However, using the decision tree approach, we obtain 86.5% precision and 92.9% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9995121955871582}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9992351531982422}]}, {"text": "We also trained a tree using both the name and misspelling features.", "labels": [], "entities": []}, {"text": "The results can be found in the second line of.", "labels": [], "entities": []}, {"text": "Unlike the case when we trained the misspelling identifier on all the features, the extended tree for the name identifier provides increased recall as well as increased precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9994656443595886}, {"text": "precision", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.9981616139411926}]}, {"text": "Unlike the case with the misspelling decision-tree, the misspelling-identification features do provide predictive information for name identification.", "labels": [], "entities": [{"text": "name identification", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.8629295229911804}]}, {"text": "If we review the features, this result seems quite reasonable: features such as corpus frequency and non-English characters can provide evidence for/against name iden-: Precision and recall for misspelling identification tification as well as for/against misspelling identification.", "labels": [], "entities": [{"text": "Precision", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.9602652192115784}, {"text": "recall", "start_pos": 183, "end_pos": 189, "type": "METRIC", "confidence": 0.9988256096839905}, {"text": "misspelling identification tification", "start_pos": 194, "end_pos": 231, "type": "TASK", "confidence": 0.7013471126556396}, {"text": "misspelling identification", "start_pos": 255, "end_pos": 281, "type": "TASK", "confidence": 0.7477616965770721}]}, {"text": "For example, an unknown word that occurs quite frequently (such as clinton) is likely to be a name, whereas an unknown word that occurs infrequently (such as wful) is likely to be a misspelling.", "labels": [], "entities": []}, {"text": "A review of the errors made by the name identifier again provides insight for future development.", "labels": [], "entities": []}, {"text": "Among those unknown words that are names but which were not identified as such are predominantly names that can (and did) appear with determiners.", "labels": [], "entities": []}, {"text": "Examples of this class include steelers in the steelers, and pathfinder in the pathfinder.", "labels": [], "entities": []}, {"text": "Hence, the name identifier seems adept at finding the names of individual people and places, which typically cannot be combined with determiners.", "labels": [], "entities": []}, {"text": "But, the name identifier has more problems with names that have similar distributions to common nouns.", "labels": [], "entities": []}, {"text": "The cases where the name identifier incorrectly identifies unknown words as names also have identifiable characteristics.", "labels": [], "entities": []}, {"text": "These examples mostly include words with unusual character sequences such as the misspellings sxetion and fwlamg.", "labels": [], "entities": []}, {"text": "No doubt these have similar characteristics to foreign names.", "labels": [], "entities": []}, {"text": "As the misidentified words are also correctly identified as misspellings by the misspelling identifier, these are less problematic.", "labels": [], "entities": []}, {"text": "It is the task of the decisionmaking component to resolve issues such as these.", "labels": [], "entities": []}, {"text": "The final results we include are for the unknown word categorizer itself using the voting procedure outlined in previous discussion.", "labels": [], "entities": []}, {"text": "As introduced previously, confidence measure is used as a tie-breaker in cases where the two components make positive decision.", "labels": [], "entities": [{"text": "confidence measure", "start_pos": 26, "end_pos": 44, "type": "METRIC", "confidence": 0.883003443479538}]}, {"text": "We evaluate the categorizer using precision and recall metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9992843270301819}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9967633485794067}]}, {"text": "The precision metric identifies the number of correct misspelling or name categorizations over the total number of times a word was identified as a misspelling or a name.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9991635084152222}]}, {"text": "The recall metric identifies the number of times the system correctly identifies a misspelling or name over the number of misspellings and names existing in the data.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9985709190368652}]}, {"text": "As illustrated in, the unknown word categorizer achieves 86% precision and 89.9% recall on the task of identifying names and misspellings.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9996048808097839}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9997053742408752}]}, {"text": "An examination of the confusion matrix of the tiebreaker decisions is also revealing.", "labels": [], "entities": []}, {"text": "We include the confusion matrix for one test data set in.", "labels": [], "entities": []}, {"text": "Firstly, in only about 5% of the cases was it necessary to revert to confidence measure to determine the category of the unknown word.", "labels": [], "entities": []}, {"text": "In all other cases the predictions were compatible.", "labels": [], "entities": []}, {"text": "Secondly, in the majority of cases the decision-maker rules in favour of the name prediction.", "labels": [], "entities": [{"text": "name prediction", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.8541864454746246}]}, {"text": "In hindsight this is not surprising since the name decision tree has higher resuits and hence is likely to have higher confidence measures.", "labels": [], "entities": []}, {"text": "A review of the largest error category in this confusion matrix is also insightful.", "labels": [], "entities": []}, {"text": "These are cases where the decision-maker classifies the unknown word as a name when it should be a misspelling (37 cases).", "labels": [], "entities": []}, {"text": "The words in this category are typically examples where the misspelled word has a phonetic relationship with the intended word.", "labels": [], "entities": []}, {"text": "For example, temt for tempt, floyda for florida, and dimow part of the intended word democrat.", "labels": [], "entities": []}, {"text": "Not surprisingly, it was these types of words which were identified as problematic for the current misspelling identifier.", "labels": [], "entities": []}, {"text": "Augmenting the misspelling identifier with features to identify these types of misspellings should also lead to improvement in the decision-maker.", "labels": [], "entities": []}, {"text": "We find these results encouraging: they indicate that the approach we are taking is productive.", "labels": [], "entities": []}, {"text": "Our future work will focus on three fronts.", "labels": [], "entities": []}, {"text": "Firstly, we will improve our existing components by developing further features which are sensitive to the distinction between names and misspellings.", "labels": [], "entities": []}, {"text": "The discussion in this section has indicated several promising directions.", "labels": [], "entities": []}, {"text": "Secondly, we will develop components to identify the remaining types of unknown words, such as abbreviations, morphological variants, etc.", "labels": [], "entities": []}, {"text": "Thirdly, we will experiment with alternative decision-making processes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Precision and recall for misspelling identification", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9934521913528442}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9992591738700867}, {"text": "misspelling identification", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.9061424732208252}]}, {"text": " Table 6.  Firstly, in only about 5% of the cases was it nec- essary to revert to confidence measure to determine  the category of the unknown word. In all other cases  the predictions were compatible. Secondly, in the", "labels": [], "entities": []}, {"text": " Table 4: Precision and recall for name identification", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9930564761161804}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9978095889091492}, {"text": "name identification", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.874368280172348}]}, {"text": " Table 5: Precision and recall for decision-making component", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9913114309310913}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9988590478897095}]}, {"text": " Table 6: Confusion matrix for decision maker: includes only those examples where both components made  a positive prediction.", "labels": [], "entities": [{"text": "decision maker", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.8945826888084412}]}]}