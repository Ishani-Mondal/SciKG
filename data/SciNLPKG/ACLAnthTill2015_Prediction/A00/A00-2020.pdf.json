{"title": [{"text": "Detecting Errors within a Corpus using Anomaly Detection", "labels": [], "entities": [{"text": "Detecting Errors within a Corpus", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8684796690940857}, {"text": "Anomaly Detection", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.627416804432869}]}], "abstractContent": [{"text": "We present a method for automatically detecting errors in a manually marked corpus using anomaly detection.", "labels": [], "entities": [{"text": "anomaly detection", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7167770564556122}]}, {"text": "Anomaly detection is a method for determining which elements of a large data set do not conform to the whole.", "labels": [], "entities": [{"text": "Anomaly detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7096764743328094}]}, {"text": "This method fits a probability distribution over the data and applies a statistical test to detect anomalous elements.", "labels": [], "entities": []}, {"text": "In the corpus error detection problem, anomalous elements are typically marking errors.", "labels": [], "entities": [{"text": "corpus error detection", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.6536367336908976}]}, {"text": "We present the results of applying this method to the tagged portion of the Penn Treebank corpus.", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 76, "end_pos": 96, "type": "DATASET", "confidence": 0.986626148223877}]}], "introductionContent": [{"text": "Manually marking corpora is a time consuming and expensive process.", "labels": [], "entities": []}, {"text": "The process is subject to human error by the experts doing the marking.", "labels": [], "entities": []}, {"text": "Unfortunately, many natural language processing methods are sensitive to these errors.", "labels": [], "entities": []}, {"text": "In order to ensure accuracy in a corpus, typically several experts pass over the corpus to ensure consistency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9973244667053223}, {"text": "consistency", "start_pos": 98, "end_pos": 109, "type": "METRIC", "confidence": 0.9745930433273315}]}, {"text": "For large corpora this can be a tremendous expense.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method for automatically detecting errors in a marked corpus using an anomaly detection technique.", "labels": [], "entities": []}, {"text": "This technique detects anomalies or elements which do not fit in with the rest of the corpus.", "labels": [], "entities": []}, {"text": "When applied to marked corpora, the anomalies tend to be errors in the markings of the corpus.", "labels": [], "entities": []}, {"text": "To detect the anomalies, we first compute a probability distribution over the entire corpus.", "labels": [], "entities": []}, {"text": "Then we apply a statistical test which identifies which elements are anomalies.", "labels": [], "entities": []}, {"text": "In this case the anomalies are the elements with very low likelihood.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9647048115730286}]}, {"text": "These elements are marked as errors and are thrown out of the corpus.", "labels": [], "entities": []}, {"text": "The model is recomputed on the remaining elements.", "labels": [], "entities": []}, {"text": "At conclusion, we are left with two data sets: one the normal elements and the second the detected anomalous elements.", "labels": [], "entities": []}, {"text": "We evaluate this method over the part of speech tagged portion of the Penn Treebank corpus (.", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 70, "end_pos": 90, "type": "DATASET", "confidence": 0.9958206415176392}]}, {"text": "In one experiment, our method detected 1000 anomalies within a data set of 1.25 million tagged elements.", "labels": [], "entities": []}, {"text": "Human judges evaluated the results of the application of this method and verified that 69% of identified anomalies are in fact tagging errors.", "labels": [], "entities": []}, {"text": "In another experiment, our method detected 4000 anomalies of which 44% are tagging errors.", "labels": [], "entities": []}], "datasetContent": [{"text": "The method was applied to the Penn Treebank corpus and a set of anomalies were generated.", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.9954253236452738}]}, {"text": "These anomalies were evaluated by human judges to determine if they are in fact tagging errors in the corpus.", "labels": [], "entities": []}, {"text": "The human judges were natural language processing researchers (not the author) familiar with the Penn Treebank markings.", "labels": [], "entities": [{"text": "Penn Treebank markings", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.9936057130495707}]}, {"text": "In the experiments involving the sparse Markov transducers, after applying the method, 7055 anomalies were detected.", "labels": [], "entities": []}, {"text": "In the experiments involving the naive Bayes learning method, 6213 anomalies were detected.", "labels": [], "entities": []}, {"text": "Sample output from the system is shown in.", "labels": [], "entities": []}, {"text": "The error is shown in the context marked with !!!.", "labels": [], "entities": [{"text": "error", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9821511507034302}]}, {"text": "The likelihood of the tag is also given which is extremely low for the errors.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9973701238632202}]}, {"text": "The system also outputs a suggested tag and its likelihood which is the tag with the highest likelihood for that context.", "labels": [], "entities": []}, {"text": "As we can see, these errors are clearly annotation errors.", "labels": [], "entities": []}, {"text": "Since the anomalies detected from the two probability modeling methods differed only slightly, we performed human judge verification of the errors over only the results of the sparse Markov transducer experiments.", "labels": [], "entities": []}, {"text": "The anomalies were ordered based on their likelihood.", "labels": [], "entities": []}, {"text": "Using this ranking, the set of anomalies were broken up into sets of 1000 records.", "labels": [], "entities": []}, {"text": "We examined the first 4000 elements by randomly selecting 100 elements out of each 1000.", "labels": [], "entities": []}, {"text": "Human judges were presented with the system output for four sets of 100 anomalies.", "labels": [], "entities": []}, {"text": "The judges were asked to choose among three options for each example: 1.", "labels": [], "entities": []}, {"text": "Corpus Error-The tag in the corpus sentence is incorrect.", "labels": [], "entities": []}, {"text": "2. Unsure -The judge is unsure whether or not the corpus tag is correct.", "labels": [], "entities": [{"text": "Unsure", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.9942162036895752}]}, {"text": "3. System Error -The tag in the corpus sentence is correct and the system incorrectly marked it as an error.", "labels": [], "entities": []}, {"text": "The \"unsure\" choice was allowed because of the inherent subtleties in differentiating between types of tags such as \"VB vs. VBP\" or \"VBD vs. VBN\".", "labels": [], "entities": [{"text": "VB vs. VBP", "start_pos": 117, "end_pos": 127, "type": "DATASET", "confidence": 0.6447488268216451}]}, {"text": "Over the 400 examples evaluated, 158 were corpus errors, 202 were system errors and the judges were unsure in 40 of the cases.", "labels": [], "entities": []}, {"text": "The corpus error rate was computed by throwing out the unsure cases and computing:", "labels": [], "entities": [{"text": "error rate", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.8613101840019226}]}], "tableCaptions": [{"text": " Table 1: Results of error detection experiments on the tagged portion of the Penn Treebank", "labels": [], "entities": [{"text": "error detection", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.6757843792438507}, {"text": "Penn Treebank", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9709422290325165}]}]}