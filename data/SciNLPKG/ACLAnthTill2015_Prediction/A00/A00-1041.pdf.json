{"title": [], "abstractContent": [{"text": "Information retrieval systems have typically concentrated on retrieving a set of documents which are relevant to a user's query.", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7863428890705109}]}, {"text": "This paper describes a system that attempts to retrieve a much smaller section of text, namely, a direct answer to a user's question.", "labels": [], "entities": []}, {"text": "The SMART IR system is used to extract a ranked set of passages that are relevant to the query.", "labels": [], "entities": [{"text": "SMART IR", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.7380802035331726}]}, {"text": "Entities are extracted from these passages as potential answers to the question, and ranked for plausibility according to how well their type matches the query, and according to their frequency and position in the passages.", "labels": [], "entities": []}, {"text": "The system was evaluated at the TREC-8 question answering track: we give results and error analysis on these queries.", "labels": [], "entities": [{"text": "TREC-8 question answering", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.7145645618438721}]}], "introductionContent": [{"text": "In this paper, we describe and evaluate a questionanswering system based on passage retrieval and entity-extraction technology.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.8714726269245148}]}, {"text": "There has long been a concensus in the Information Retrieval (IR) community that natural language processing has little to offer for retrieval systems.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.834758198261261}]}, {"text": "Plausibly, this is creditable to the preeminence of ad hoc document retrieval as the task of interest in IR.", "labels": [], "entities": [{"text": "IR", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9617599844932556}]}, {"text": "However, there is a growing recognition of the limitations of ad hoc retrieval, both in the sense that current systems have reached the limit of achievable performance, and in the sense that users' information needs are often not well characterized by document retrieval.", "labels": [], "entities": []}, {"text": "In many cases, a user has a question with a specific answer, such as What city is it where the European Parliament meets? or Who discovered Pluto?", "labels": [], "entities": [{"text": "Who discovered Pluto?", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.4943258613348007}]}, {"text": "In such cases, ranked answers with links to supporting documentation are much more useful than the ranked list of documents that standard retrieval engines produce.", "labels": [], "entities": []}, {"text": "The ability to answer specific questions also provides a foundation for addressing quantitative inquiries such as How many times has the Fed raised interest rates this year?", "labels": [], "entities": []}, {"text": "which can be interpreted as the cardinality of the set of answers to a specific question that happens to have multiple correct answers, like On what date did the Fed raise interest rates this year?", "labels": [], "entities": []}, {"text": "We describe a system that extracts specific answers from a document collection.", "labels": [], "entities": []}, {"text": "The system's performance was evaluated in the question-answering track that has been introduced this year at the TREC information-retrieval conference.", "labels": [], "entities": [{"text": "TREC information-retrieval conference", "start_pos": 113, "end_pos": 150, "type": "DATASET", "confidence": 0.7354311545689901}]}, {"text": "The major points of interest are the following.", "labels": [], "entities": []}, {"text": "\u2022 Comparison of the system's performance to a system that uses the same passage retrieval component, but no natural language processing, shows that NLP provides significant performance improvements on the question-answering task.", "labels": [], "entities": []}, {"text": "\u2022 The system is designed to build on the strengths of both IR and NLP technologies.", "labels": [], "entities": []}, {"text": "This makes for much more robustness than a pure NLP system would have, while affording much greater precision than a pure IR system would have.", "labels": [], "entities": [{"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9964465498924255}]}, {"text": "\u2022 The task is broken into subtasks that admit of independent development and evaluation.", "labels": [], "entities": []}, {"text": "Passage retrieval and entity extraction are both recognized independent tasks.", "labels": [], "entities": [{"text": "Passage retrieval", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8909416496753693}, {"text": "entity extraction", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.8584136664867401}]}, {"text": "Other subtasks are entity classification and query classification--both being classification tasks that use features obtained by parsing--and entity ranking.", "labels": [], "entities": [{"text": "entity classification", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.8080942928791046}, {"text": "query classification", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7816719710826874}, {"text": "entity ranking", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.7891380488872528}]}, {"text": "In the following section, we describe the questionanswering system, and in section 3, we quantify its performance and give an error analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "The system was evaluated in the TREC-8 questionanswering track.", "labels": [], "entities": [{"text": "TREC-8 questionanswering track", "start_pos": 32, "end_pos": 62, "type": "DATASET", "confidence": 0.7972930073738098}]}, {"text": "TREC provided 198 questions as a blind test set: systems were required to provide five potential answers for each question, ranked in order of plausibility.", "labels": [], "entities": [{"text": "TREC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8989198803901672}]}, {"text": "The output from each system was then scored by hand by evaluators at NIST, each answer being marked as either corrector incorrect.", "labels": [], "entities": [{"text": "NIST", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.9816154837608337}]}, {"text": "The system's score on a particular question is a function of whether it got a correct answer in the five ranked answers, with higher scores for the answer appearing higher in the ranking.", "labels": [], "entities": []}, {"text": "The system receives a score of 1, 1/2, 1/3, 1/4, 1/5, or 0, re2perhaps less desirably, people would not be recognized as a synonym of lives in this example: 200 people would be indistinguishable from 200 pumpkins.", "labels": [], "entities": []}, {"text": "3This does introduce occasional errors, when two people with the same last name appear in retrieved passages.", "labels": [], "entities": [{"text": "3This", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9256399273872375}]}], "tableCaptions": []}