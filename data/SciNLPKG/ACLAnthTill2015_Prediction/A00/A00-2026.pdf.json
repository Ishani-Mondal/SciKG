{"title": [{"text": "Trainable Methods for Surface Natural Language Generation", "labels": [], "entities": [{"text": "Surface Natural Language Generation", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.8060946613550186}]}], "abstractContent": [{"text": "We present three systems for surface natural language generation that are trainable from annotated corpora.", "labels": [], "entities": [{"text": "surface natural language generation", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.6526895463466644}]}, {"text": "The first two systems, called NLG1 and NLG2, require a corpus marked only with domain-specific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information.", "labels": [], "entities": []}, {"text": "All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation.", "labels": [], "entities": []}, {"text": "NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase.", "labels": [], "entities": [{"text": "NLG1", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9666715860366821}, {"text": "NLG3", "start_pos": 113, "end_pos": 117, "type": "DATASET", "confidence": 0.9078413844108582}]}, {"text": "The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase.", "labels": [], "entities": [{"text": "NLG2", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.9291695952415466}]}, {"text": "We present experiments in which we generate phrases to describe flights in the air travel domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents three trainable systems for surface natural language generation (NLG).", "labels": [], "entities": [{"text": "surface natural language generation (NLG)", "start_pos": 48, "end_pos": 89, "type": "TASK", "confidence": 0.757181031363351}]}, {"text": "Surface NLG, for our purposes, consists of generating a grammatical natural language phrase that expresses the meaning of an input semantic representation.", "labels": [], "entities": []}, {"text": "The systems take a \"corpus-based\" or \"machinelearning\" approach to surface NLG, and learn to generate phrases from semantic input by statistically analyzing examples of phrases and their corresponding semantic representations.", "labels": [], "entities": []}, {"text": "The determination of the content in the semantic representation, or \"deep\" generation, is not discussed here.", "labels": [], "entities": []}, {"text": "Instead, the systems assume that the input semantic representation is fixed and only deal with how to express it in natural language.", "labels": [], "entities": []}, {"text": "This paper discusses previous approaches to surface NLG, and introduces three trainable systems for surface NLG, called NLG1, NLG2, and NLG3.", "labels": [], "entities": []}, {"text": "Quantitative evaluation of experiments in the air travel domain will also be discussed.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training and test sets used to evaluate NLG1, NLG2 and NLG3 were derived semi-automatically from a pre-existing annotated corpus of user queries in the air travel domain.", "labels": [], "entities": [{"text": "NLG1", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8690635561943054}]}, {"text": "The annotation scheme used a total of 26 attributes to represent flights.", "labels": [], "entities": []}, {"text": "The training set consisted of 6000 templates describing flights while the test set consisted of 1946 templates describing flights.", "labels": [], "entities": []}, {"text": "All systems used the same training set, and were tested on the attribute sets extracted from the phrases in the test set.", "labels": [], "entities": []}, {"text": "For example, if the test set contains the template \"flights to $city-to leaving at Stime-dep\", the surface generation systems will be told to generate a phrase for the attribute set { $city-to, Stime-dep }.", "labels": [], "entities": []}, {"text": "The output of NLG3 on the attribute set { $city-to, $city-fr, $time-dep } is shown in.", "labels": [], "entities": []}, {"text": "There does not appear to bean objective automatic evaluation method 2 for generated text that correlates with how an actual person might judge the output.", "labels": [], "entities": []}, {"text": "Therefore, two judges --the author and a colleague --manually evaluated the output of all three systems.", "labels": [], "entities": []}, {"text": "Each judge assigned each phrase from each of the three systems one of the following rankings: Correct: Perfectly acceptable OK: Tense or agreement is wrong, but word choice is correct.", "labels": [], "entities": [{"text": "Correct", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.981650173664093}]}, {"text": "(These errors could be corrected by post-processing with a morphological analyzer.)", "labels": [], "entities": []}, {"text": "Bad: Words are missing or extraneous words are present No Output: The system failed to produce any output While there were a total 1946 attribute sets from the test examples, the judges only needed to evaluate the 190 unique attribute sets, e.g., the attribute set { $city-fr $city-to } occurs 741 times in the test data.", "labels": [], "entities": []}, {"text": "Subjective evaluation of generation output is 2Measuring word overlap or edit distance between the system's output and a \"reference\" set would bean automatic scoring method.", "labels": [], "entities": [{"text": "2Measuring word overlap", "start_pos": 46, "end_pos": 69, "type": "METRIC", "confidence": 0.7235195636749268}]}, {"text": "We believe that such a method does not accurately measure the correctness or grammaticality of the text.", "labels": [], "entities": []}, {"text": "not ideal, but is arguably superior than an automatic evaluation that fails to correlate with human linguistic judgement.", "labels": [], "entities": []}, {"text": "The results of the manual evaluation, as well as the values of the search and feature selection parameters for all systems, are shown in account for multiple occurrences of attribute sets, whereas the unweighted results in attribute set once, i.e., { $city-fr $city-to } is counted 741 times in the weighted results but once in the unweighted results.", "labels": [], "entities": []}, {"text": "Using the weighted results, which represent testing conditions more realistically than the unweighted results, both judges found an improvement from NLG1 to NLG2, and from NLG2 to NLG3.", "labels": [], "entities": [{"text": "NLG3", "start_pos": 180, "end_pos": 184, "type": "DATASET", "confidence": 0.9230233430862427}]}, {"text": "NLG3 cuts the error rate from NLG1 by at least 33% (counting anything without a rank of Correct as wrong).", "labels": [], "entities": [{"text": "NLG3", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.939970076084137}, {"text": "error rate", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9821654260158539}, {"text": "Correct", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9849547147750854}]}, {"text": "NLG2 cuts the error rate by at least 22% and underperforms NLG3, but requires far less annotation in its training data.", "labels": [], "entities": [{"text": "NLG2", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9360703825950623}, {"text": "error rate", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9876008927822113}, {"text": "NLG3", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9080778360366821}]}, {"text": "NLG1 has no chance of generating anything for 3% of the data --it fails completely on novel attribute sets.", "labels": [], "entities": [{"text": "NLG1", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9056075215339661}]}, {"text": "Using the unweighted results, both judges found an improvement from NLG1 to NLG2, but, surprisingly, judge A found a slight decrease while judge B found an increase inaccuracy from NLG2 to NLG3.", "labels": [], "entities": []}, {"text": "The unweighted results show that the baseline NLG1 does well on the common attribute sets, since it correctly generates only less than 50% of the unweighted cases but over 80% of the weighted cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Features patterns for NLG3. Any occurrence of \"?\" will be instantiated with an actual value from  training data.", "labels": [], "entities": []}, {"text": " Table 6: Weighted evaluation of trainable surface generation systems by judge B", "labels": [], "entities": []}, {"text": " Table 7: Unweighted evaluation of trainable surface generation systems by judge A", "labels": [], "entities": []}, {"text": " Table 8: Unweighted evaluation of trainable surface generation systems by judge B", "labels": [], "entities": []}]}