{"title": [{"text": "Evaluating Automatic Dialogue Strategy Adaptation fora Spoken Dialogue System", "labels": [], "entities": [{"text": "Evaluating Automatic Dialogue Strategy Adaptation", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8006859362125397}]}], "abstractContent": [{"text": "In this paper, we describe an empirical evaluation of an adaptive mixed initiative spoken dialogue system.", "labels": [], "entities": []}, {"text": "We conducted two sets of experiments to evaluate the mixed initiative and automatic adaptation aspects of the system, and analyzed the resulting dialogues along three dimensions: performance factors, discourse features, and initiative distribution.", "labels": [], "entities": []}, {"text": "Our results show that 1) both the mixed initiative and automatic adaptation aspects led to better system performance in terms of user satisfaction and dialogue efficiency, and 2) the system's adaptation behavior better matched user expectations, more efficiently resolved dialogue anomalies, and resulted in higher overall dialogue quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent advances in speech technologies have enabled spoken dialogue systems to employ mixed initiative dialogue strategies (e.g. ().", "labels": [], "entities": []}, {"text": "Although these systems interact with users in a manner more similar to human-human interactions than earlier systems employing system initiative strategies, their response strategies are typically selected using only local dialogue context, disregarding dialogue history.", "labels": [], "entities": []}, {"text": "Therefore, their gain in naturalness and performance under optimal conditions is often overshadowed by their inability to cope with anomalies in dialogues by automatically adapting dialogue strategies.", "labels": [], "entities": []}, {"text": "In contrast, shows a dialogue in which the system automatically adapts dialogue strategies based on the current user utterance and dialogue history.", "labels": [], "entities": []}, {"text": "1 After failing to obtain a valid response to an informationseeking query in utterance (4), the system adapted dialogue strategies to provide additional information in (6) that assisted the user in responding to the query.", "labels": [], "entities": []}, {"text": "Furthermore, after the user responded to a limited system prompt in (10) with a fully-specified query in (11), implicitly indicating her intention to take charge of the problem-IS and U indicate system and user utterances, respectively.", "labels": [], "entities": []}, {"text": "The words appearing in square brackets are the output from the Lucent Automatic Speech Recognizer (), configured to use class-based probabilistic n-gram language models.", "labels": [], "entities": [{"text": "Lucent Automatic Speech Recognizer", "start_pos": 63, "end_pos": 97, "type": "TASK", "confidence": 0.5893850326538086}]}, {"text": "The task and dialogue initiative annotations are explained in Section 2.", "labels": [], "entities": []}, {"text": "solving process, the system again adapted strategies, hence providing an open-ended prompt in.", "labels": [], "entities": []}, {"text": "Previous work has shown that dialogue systems in which users can explicitly change the system's dialogue strategies result in better performance than nonadaptable systems).", "labels": [], "entities": []}, {"text": "However, no earlier system allowed for initiative-oriented automatic strategy adaptation based on information dynamically extracted from the user's spoken input.", "labels": [], "entities": [{"text": "initiative-oriented automatic strategy adaptation", "start_pos": 39, "end_pos": 88, "type": "TASK", "confidence": 0.6236564889550209}]}, {"text": "In this paper, we briefly introduce MIMIC, a mixed initiative spoken dialogue system that automatically adapts dialogue strategies.", "labels": [], "entities": []}, {"text": "We then describe two experiments that evaluated the effectiveness of MIMIC's mixed initiative and automatic adaptation capabilities.", "labels": [], "entities": [{"text": "MIMIC", "start_pos": 69, "end_pos": 74, "type": "TASK", "confidence": 0.805994987487793}]}, {"text": "Our results show that, when analyzed along the performance dimension, MIMIC's mixed initiative and automatic adaptation features lead to more efficient dialogues and higher user satisfaction.", "labels": [], "entities": []}, {"text": "Moreover, when analyzed along the discourse and initiative dimensions, MIMIC's adaptation capabilities result in dialogues in which system behavior better matches user expectations and dialogue anomalies are resolved more efficiently.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our main goal in evaluating MIMIC is to determine whether users find the mixed initiative and automatic adaptation aspects of its dialogue strategies useful.", "labels": [], "entities": [{"text": "MIMIC", "start_pos": 28, "end_pos": 33, "type": "TASK", "confidence": 0.8081172704696655}]}, {"text": "We compared MIMIC to two control systems: MIMIC-SI and MIMIC-MI, since they employ dialogue management strategies similar to those in many existing systems.", "labels": [], "entities": []}, {"text": "The comparison between MIMIC and MIMIC-SI   2.", "labels": [], "entities": [{"text": "MIMIC", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.8210417032241821}]}, {"text": "Order: For each experiment, all subjects were randomly divided into two groups.", "labels": [], "entities": []}, {"text": "One group performed tasks using MIMIC first, and the other group used the control system first.", "labels": [], "entities": [{"text": "MIMIC", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.602752149105072}]}, {"text": "Eight subjects 4 participated in each experiment.", "labels": [], "entities": []}, {"text": "Each of the subjects interacted with both systems to perform 4The subjects were Bell Labs researchers, summer students, and their friends.", "labels": [], "entities": []}, {"text": "Most of them are computer scientists, electrical engi-all tasks.", "labels": [], "entities": []}, {"text": "The subjects completed one task per call so that the dialogue history for one task did not affect the next task.", "labels": [], "entities": []}, {"text": "Once they had completed all tasks in sequence using one system, they filled out a questionnaire to assess user satisfaction by rating 8-9 statements, similar to those in (), on a scale of 1-5, where 5 indicated highest satisfaction.", "labels": [], "entities": []}, {"text": "Approximately two days later, they attempted the same tasks using the other system.", "labels": [], "entities": []}, {"text": "5 These experiments resulted in 112 dialogues with approximately 2,800 dialogue turns.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of Performance Features", "labels": [], "entities": []}, {"text": " Table 2: Comparison of User Utterance Features", "labels": [], "entities": []}, {"text": " Table 3: Correlation Between Task Initiative Distribution  and Features (n=56)", "labels": [], "entities": []}]}