{"title": [], "abstractContent": [{"text": "Simple measures can achieve high-accuracy cross-language retrieval in carefully chosen applications.", "labels": [], "entities": [{"text": "cross-language retrieval", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.7678090929985046}]}, {"text": "Image retrieval is one of those applications, with results ranging from 68% of human translator performance for German, to 100% for French.", "labels": [], "entities": [{"text": "Image retrieval", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8468231558799744}]}], "introductionContent": [{"text": "Typical queries are, as inmost Web search applications, two to three words in length.", "labels": [], "entities": []}, {"text": "At this point, all of the captions are in English.", "labels": [], "entities": []}, {"text": "eMotion hosts a large database of images for sale and for licensing, PictureQuest.", "labels": [], "entities": [{"text": "eMotion", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9581472873687744}, {"text": "PictureQuest", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.9610728621482849}]}, {"text": "At least 10% of PictureQuest's user base is outside the United States.", "labels": [], "entities": []}, {"text": "The tests were performed on the PictureQuest database of approximately 400,000 images.", "labels": [], "entities": [{"text": "PictureQuest database", "start_pos": 32, "end_pos": 53, "type": "DATASET", "confidence": 0.9707962870597839}]}, {"text": "Information is increasingly global, and the need to access it crosses language barriers.", "labels": [], "entities": []}, {"text": "The topic of this paper, cross-language information retrieval, concerns the automatic retrieval of text in one language via a query in a different language.", "labels": [], "entities": [{"text": "cross-language information retrieval", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.719000001748403}]}, {"text": "A considerable body of literature has grownup around cross-language information retrieval (e.g.).", "labels": [], "entities": [{"text": "cross-language information retrieval", "start_pos": 53, "end_pos": 89, "type": "TASK", "confidence": 0.6702390909194946}]}, {"text": "There are two basic approaches.", "labels": [], "entities": []}, {"text": "Either the query can be translated, or each entire document can be translated into the same language as the query.", "labels": [], "entities": []}, {"text": "The accuracy of retrieval across languages, however, is generally not good.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9992831349372864}]}, {"text": "One of the weaknesses that plagues crosslanguage retrieval is that we do not have a good sense of who the users are, or how best to interact with them.", "labels": [], "entities": [{"text": "crosslanguage retrieval", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.8557749092578888}]}, {"text": "In this paper we describe a multimedia application for which cross-language information retrieval works particularly well.", "labels": [], "entities": [{"text": "cross-language information retrieval", "start_pos": 61, "end_pos": 97, "type": "TASK", "confidence": 0.670620063940684}]}, {"text": "eMotion, Inc. has developed a natural language information retrieval application that retrieves images, such as photographs, based on short textual descriptions or captions.", "labels": [], "entities": [{"text": "eMotion", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.922674834728241}, {"text": "natural language information retrieval", "start_pos": 30, "end_pos": 68, "type": "TASK", "confidence": 0.677206739783287}]}, {"text": "The captions are typically one to three sentences, although they may also Recent Web utilization data for PictureQuest indicate that of the 10% of users from outside the United States, a significant portion come from Spanish-speaking, French-speaking, and German-speaking countries.", "labels": [], "entities": []}, {"text": "It is expected that adding appropriate language interfaces and listing PictureQuest in foreign-language search engines will dramatically increase nonEnglish usage.", "labels": [], "entities": [{"text": "PictureQuest", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.8723909854888916}]}], "datasetContent": [{"text": "Evaluating precision and recall on a large corpus is a difficult task.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9987315535545349}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9965171813964844}]}, {"text": "We used the evaluation methods detailed in Flank 1998.", "labels": [], "entities": [{"text": "Flank 1998", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.9327456951141357}]}, {"text": "Precision was evaluated using a crossing measure, whereby any image ranked higher than a better match was penalized.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9505821466445923}]}, {"text": "Recall per se was measured only with respect to a defined subset of the images.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9807400703430176}]}, {"text": "Ranking incorporates some recall measures into the precision score, since images ranked too low area recall problem, and images marked too high area precision problem.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.998616099357605}, {"text": "precision score", "start_pos": 51, "end_pos": 66, "type": "METRIC", "confidence": 0.9796748459339142}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.8349452614784241}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.7012837529182434}]}, {"text": "If there are three good matches, and the third shows up as #4, the bogus #3 is a precision problem, and the too-low #4 is a recall problem.", "labels": [], "entities": [{"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9992504715919495}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9988700747489929}]}, {"text": "For evaluation of the overall cross-language retrieval performance, we simply measured the ratio between the cross-language and monolingual retrieval accuracy (C/M%).", "labels": [], "entities": [{"text": "monolingual retrieval accuracy", "start_pos": 128, "end_pos": 158, "type": "METRIC", "confidence": 0.5553067326545715}]}, {"text": "This is standard; see, for example,.", "labels": [], "entities": []}, {"text": "illustrates the percentage of monolingual retrieval performance we achieved for the translation tests performed.", "labels": [], "entities": []}, {"text": "In this instance, we take the precision performance of the human-translated queries and normalize it to 100%, and adjust the other translation modalities relative to the human baseline.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9988904595375061}]}, {"text": "Several other factors make the PictureQuest application a particularly good application for machine translation technology.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8052683770656586}]}, {"text": "Unlike document translation, there is no need to match every word in the description; useful images maybe retrieved even if a word or two is lost.", "labels": [], "entities": [{"text": "document translation", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.729096844792366}]}, {"text": "There are no discourse issues at all: searches never use anaphora, and no one cares if the translated query sounds good or not.", "labels": [], "entities": []}], "tableCaptions": []}