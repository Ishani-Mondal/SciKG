{"title": [{"text": "Assigning Function Tags to Parsed Text*", "labels": [], "entities": [{"text": "Assigning Function Tags to Parsed Text", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7833474973837534}]}], "abstractContent": [{"text": "It is generally recognized that the common non-terminal labels for syntactic constituents (NP, VP, etc.) do not exhaust the syntactic and semantic information one would like about parts of a syntactic tree.", "labels": [], "entities": []}, {"text": "For example, the Penn Tree-bank gives each constituent zero or more 'func-tion tags' indicating semantic roles and other related information not easily encapsulated in the simple constituent labels.", "labels": [], "entities": [{"text": "Penn Tree-bank", "start_pos": 17, "end_pos": 31, "type": "DATASET", "confidence": 0.9916519820690155}]}, {"text": "We present a statistical algorithm for assigning these function tags that, on text already parsed to a simple-label level, achieves an F-measure of 87%, which rises to 99% when considering 'no tag' as a valid choice.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9991692304611206}]}], "introductionContent": [{"text": "Parsing sentences using statistical information gathered from a treebank was first examined a decade ago in and is by now a fairly well-studied problem,,).", "labels": [], "entities": [{"text": "Parsing sentences", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9194685816764832}]}, {"text": "But to date, the end product of the parsing process has for the most part been a bracketing with simple constituent labels like NP, VP, or SBAR.", "labels": [], "entities": [{"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9851603507995605}]}, {"text": "The Penn treebank contains a great deal of additional syntactic and semantic information from which to gather statistics; reproducing more of this information automatically is a goal which has so far been mostly ignored.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.988934725522995}]}, {"text": "This paper details a process by which some of this information--the function tags--may be recovered automatically.", "labels": [], "entities": []}, {"text": "In the Penn treebank, there are 20 tags (figure 1) that can be appended to constituent labels in order to indicate additional information about the syntactic or semantic role of the con-* This research was funded in part by NSF grants LIS-SBR-9720368 and IGERT-9870676. stituent.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9925577342510223}]}, {"text": "We have divided them into four categories (given in) based on those in the bracketing guidelines.", "labels": [], "entities": []}, {"text": "A constituent can be tagged with multiple tags, but never with two tags from the same category.", "labels": [], "entities": []}, {"text": "1 In actuality, the case where a constituent has tags from all four categories never happens, but constituents with three tags do occur (rarely).", "labels": [], "entities": []}, {"text": "At a high level, we can simply say that having the function tag information fora given text is useful just because any further information would help.", "labels": [], "entities": []}, {"text": "But specifically, there are distinct advantages for each of the various categories.", "labels": [], "entities": []}, {"text": "Grammatical tags are useful for any application trying to follow the thread of the text--they find the 'who does what' of each clause, which can be useful to gain information about the situation or to learn more about the behaviour of the words in the sentence.", "labels": [], "entities": []}, {"text": "The form/function tags help to find those constituents behaving in ways not conforming to their labelled type, as well as further clarifying the behaviour of adverbial phrases.", "labels": [], "entities": []}, {"text": "Information retrieval applications specialising in describing events, as with a number of the MUC applications, could greatly benefit from some of these in determining the where-when-why of things.", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8203449249267578}, {"text": "MUC", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.8245706558227539}]}, {"text": "Noting a topicalised constituent could also prove useful to these applications, and it might also help in discourse analysis, or pronoun resolution.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7555792331695557}, {"text": "pronoun resolution", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.7416545748710632}]}, {"text": "Finally, the 'miscellaneous' tags are convenient at various times; particularly the CLI~ 'closely related' tag, which among other things marks phrasal verbs and prepositional ditransitives.", "labels": [], "entities": []}, {"text": "To our knowledge, there has been no attempt so far to recover the function tags in parsing treebank text.", "labels": [], "entities": []}, {"text": "In fact, we know of only 1There is a single exception in the corpus: one constituent is tagged with -LOC-I~R.", "labels": [], "entities": [{"text": "LOC-I~R", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.8970317045847574}]}, {"text": "This appears to bean error.", "labels": [], "entities": [{"text": "error", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.7021015286445618}]}], "datasetContent": [{"text": "In the training phase of our experiment, we gathered statistics on the occurrence of function tags in sections 2-21 of the Penn treebank.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 123, "end_pos": 136, "type": "DATASET", "confidence": 0.9873901307582855}]}, {"text": "Specifically, for every constituent in the treebank, we recorded the presence of its function tags (or lack thereof) along with its conditioning information.", "labels": [], "entities": []}, {"text": "From this we calculated the empirical probabilities of each function tag referenced in section 2 of this paper.", "labels": [], "entities": []}, {"text": "Values of )~ were determined using EM on the development corpus (treebank section 24).", "labels": [], "entities": [{"text": "EM", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.782187283039093}]}, {"text": "To test, then, we simply took the output of our parser on the test corpus (treebank section 23), and applied a postprocessing step to add function tags.", "labels": [], "entities": []}, {"text": "For each constituent in the tree, we calculated the likelihood of each function tag according to the feature tree in, and for each category (see we assigned the most likely function tag (which might be the null tag).", "labels": [], "entities": [{"text": "likelihood", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9775160551071167}]}, {"text": "2The reader will note that the 'features' listed in the tree are in fact not boolean-valued; each node in the given tree can be assumed to stand fora chain of boolean features, one per potential value at that node, exactly one of which will be true.", "labels": [], "entities": []}, {"text": "To evaluate our results, we first need to determine what is 'correct'.", "labels": [], "entities": []}, {"text": "The definition we chose is to calla constituent correct if there exists in the correct parse a constituent with the same start and end points, label, and function tag (or lack thereof).", "labels": [], "entities": []}, {"text": "Since we treated each of the four function tag categories as a separate feature for the purpose of tagging, evaluation was also done on a per-category basis.", "labels": [], "entities": []}, {"text": "The denominator of the accuracy measure should be the maximum possible number we could get correct.", "labels": [], "entities": [{"text": "accuracy measure", "start_pos": 23, "end_pos": 39, "type": "METRIC", "confidence": 0.9704043865203857}]}, {"text": "In this case, that means excluding those constituents that were already wrong in the parser output; the parser we used attains 89% labelled precision-recall, so roughly 11% of the constituents are excluded from the function tag accuracy evaluation.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 140, "end_pos": 156, "type": "METRIC", "confidence": 0.796327531337738}, {"text": "accuracy", "start_pos": 228, "end_pos": 236, "type": "METRIC", "confidence": 0.9020010828971863}]}, {"text": "(For reference, we have also included the performance of our function tagger directly on treebank parses; the slight gain that resulted is discussed below.)", "labels": [], "entities": []}, {"text": "Another consideration is whether to count non-tagged constituents in our evaluation.", "labels": [], "entities": []}, {"text": "On the one hand, we could count as correct any constituent with the correct tag as well as any correctly non-tagged constituent, and use as our denominator the number of all correctlylabelled constituents.", "labels": [], "entities": []}, {"text": "(We will henceforth refer to this as the 'with-null' measure.)", "labels": [], "entities": []}, {"text": "On the other hand, we could just count constituents with the correct tag, and use as our denominators the total number of tagged, correctly-labelled constituents.", "labels": [], "entities": []}, {"text": "We believe the latter number ('nonull') to be a better performance metric, as it is not overwhelmed by the large number of untagged constituents.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance within each category", "labels": [], "entities": []}, {"text": " Table 3: Overall performance on different inputs", "labels": [], "entities": []}]}