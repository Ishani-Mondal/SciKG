{"title": [], "abstractContent": [{"text": "We present anew parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established [5,9,10,15,17] \"stan-dard\" sections of the Wall Street Journal tree-bank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9628251194953918}, {"text": "Penn tree-bank style parse trees", "start_pos": 43, "end_pos": 75, "type": "DATASET", "confidence": 0.9267066240310669}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9826535582542419}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9356938004493713}, {"text": "Wall Street Journal tree-bank", "start_pos": 301, "end_pos": 330, "type": "DATASET", "confidence": 0.9134891331195831}]}, {"text": "This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].", "labels": [], "entities": [{"text": "error rate", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9893824458122253}]}, {"text": "The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.", "labels": [], "entities": []}, {"text": "We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present anew parser for parsing down to Penn tree-bank style parse trees that achieves 90.1~ average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established \"standard\" sections of the Wall Street Journal tree-bank.", "labels": [], "entities": [{"text": "Penn tree-bank style parse trees", "start_pos": 43, "end_pos": 75, "type": "DATASET", "confidence": 0.9333354592323303}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9860095977783203}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9153524041175842}, {"text": "Wall Street Journal tree-bank", "start_pos": 273, "end_pos": 302, "type": "DATASET", "confidence": 0.9825393557548523}]}, {"text": "This represents a 13% decrease in error rate over the best single-parser results on this corpus.", "labels": [], "entities": [{"text": "error rate", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9892197847366333}]}, {"text": "Following, our parser is based upon a probabilistic generative model.", "labels": [], "entities": []}, {"text": "That is, for all sentences sand MI parses 7r, the parser assigns a probability p(s, ~) = p(Tr), the equality holding when we restrict consideration to ~r whose yield is s.", "labels": [], "entities": []}, {"text": "Then for any s the parser returns the parse 7r that maximizes this probability.", "labels": [], "entities": []}, {"text": "That is, the parser implements the function arg.", "labels": [], "entities": []}, {"text": "a= p( I = = arg maxTrp(lr).", "labels": [], "entities": [{"text": "I", "start_pos": 6, "end_pos": 7, "type": "METRIC", "confidence": 0.9513818621635437}]}, {"text": "What fundamentally distinguishes probabilistic generative parsers is how they compute p(~r), and it is to that topic we turn next.", "labels": [], "entities": [{"text": "generative parsers", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.8605084419250488}]}], "datasetContent": [{"text": "We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.", "labels": [], "entities": []}, {"text": "As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser, we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.", "labels": [], "entities": []}, {"text": "For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.", "labels": [], "entities": [{"text": "PCFG information", "start_pos": 147, "end_pos": 163, "type": "DATASET", "confidence": 0.9338942468166351}]}, {"text": "This allows the second pass to see expansions not present in the training corpus.", "labels": [], "entities": []}, {"text": "We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.", "labels": [], "entities": []}, {"text": "We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.", "labels": [], "entities": []}, {"text": "As noted above, the probability model uses  the (unsmoothed) conditional probability distribution fort is given in Equation 7.", "labels": [], "entities": []}, {"text": "The other four equations can be found in a longer version of this paper available on the author's website (www.cs.brown.edu/~.,ec).", "labels": [], "entities": []}, {"text": "Land R are conditioned on three previous labels so we are using a third-order Markov grammar.", "labels": [], "entities": []}, {"text": "Also, the label of the parent constituent Ip is conditioned upon even when it is not obviously related to the further conditioning events.", "labels": [], "entities": [{"text": "Ip", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.8936297297477722}]}, {"text": "This is due to the importance of this factor in parsing, as noted in, e.g.,.", "labels": [], "entities": [{"text": "parsing", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.9860848188400269}]}, {"text": "In keeping with the standard methodology, we used the Penn Wall Street Journal tree-bank with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).", "labels": [], "entities": [{"text": "Penn Wall Street Journal tree-bank", "start_pos": 54, "end_pos": 88, "type": "DATASET", "confidence": 0.9649033904075622}]}, {"text": "Performance on the test corpus is measured using the standard measures from.", "labels": [], "entities": []}, {"text": "In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (0CB), and percentage of sentences with < 2 cross brackets (2CB).", "labels": [], "entities": [{"text": "labeled precision (LP)", "start_pos": 26, "end_pos": 48, "type": "METRIC", "confidence": 0.8370368838310241}, {"text": "recall (LR)", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9751352667808533}, {"text": "average number of crossbrackets per sentence (CB)", "start_pos": 66, "end_pos": 115, "type": "METRIC", "confidence": 0.891728421052297}]}, {"text": "Again as standard, we take separate measurements for all sentences of length <_ 40 and all sentences of length < 100.", "labels": [], "entities": []}, {"text": "Note that the definitions of labeled precision and recall are those given in and used in all of the previous work.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9127439260482788}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9990076422691345}]}, {"text": "As noted in, these definitions typically give results about 0.4% higher than the more obvious ones.", "labels": [], "entities": []}, {"text": "The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in.", "labels": [], "entities": []}, {"text": "As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers.", "labels": [], "entities": []}, {"text": "Looking in particular at the precision and recall figures, the new parser's give us a 13% error reduction over the best of the previous work, Co1199.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9996331930160522}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9992842078208923}, {"text": "error", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.989803671836853}, {"text": "Co1199", "start_pos": 142, "end_pos": 148, "type": "DATASET", "confidence": 0.9596784710884094}]}], "tableCaptions": []}