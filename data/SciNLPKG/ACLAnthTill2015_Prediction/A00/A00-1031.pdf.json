{"title": [{"text": "TnT --A Statistical Part-of-Speech Tagger", "labels": [], "entities": [{"text": "Statistical Part-of-Speech Tagger", "start_pos": 8, "end_pos": 41, "type": "TASK", "confidence": 0.5928573906421661}]}], "abstractContent": [{"text": "Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger.", "labels": [], "entities": [{"text": "statistical part-of-speech tagger", "start_pos": 38, "end_pos": 71, "type": "TASK", "confidence": 0.6148612995942434}]}, {"text": "Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework.", "labels": [], "entities": []}, {"text": "A recent comparison has even shown that TnT performs significantly better for the tested corpora.", "labels": [], "entities": []}, {"text": "We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words.", "labels": [], "entities": [{"text": "handling unknown words", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.8471003174781799}]}, {"text": "Furthermore, we present evaluations on two corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "A large number of current language processing systems use a part-of-speech tagger for pre-processing.", "labels": [], "entities": [{"text": "part-of-speech tagger", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.7133734226226807}]}, {"text": "The tagger assigns a (unique or ambiguous) part-ofspeech tag to each token in the input and passes its output to the next processing level, usually a parser.", "labels": [], "entities": []}, {"text": "Furthermore, there is a large interest in part-ofspeech tagging for corpus annotation projects, who create valuable linguistic resources by a combination of automatic processing and human correction.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.7245275378227234}]}, {"text": "For both applications, a tagger with the highest possible accuracy is required.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9989230036735535}]}, {"text": "The debate about which paradigm solves the part-of-speech tagging problem best is not finished.", "labels": [], "entities": [{"text": "part-of-speech tagging problem", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.795925517876943}]}, {"text": "Recent comparisons of approaches that can be trained on corpora) have shown that inmost cases statistical aproaches yield better results than finite-state, rule-based, or memory-based taggers.", "labels": [], "entities": []}, {"text": "They are only surpassed by combinations of different systems, forming a \"voting tagger\".", "labels": [], "entities": []}, {"text": "Among the statistical approaches, the Maximum Entropy framework has a very strong position.", "labels": [], "entities": []}, {"text": "Nevertheless, a recent independent comparison of 7 taggets () has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.", "labels": [], "entities": []}, {"text": "This tagger, TnT, not only yielded the highest accuracy, it also was the fastest both in training and tagging.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.999491810798645}, {"text": "tagging", "start_pos": 102, "end_pos": 109, "type": "TASK", "confidence": 0.9652405381202698}]}, {"text": "The tagger comparison was organized as a \"blackbox test\": set the same task to every tagger and compare the outcomes.", "labels": [], "entities": []}, {"text": "This paper describes the models and techniques used by TnT together with the implementation.", "labels": [], "entities": []}, {"text": "The reader will be surprised how simple the underlying model is.", "labels": [], "entities": []}, {"text": "The result of the tagger comparison seems to support the maxime \"the simplest is the best\".", "labels": [], "entities": []}, {"text": "However, in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with Markov models.", "labels": [], "entities": [{"text": "tagging", "start_pos": 113, "end_pos": 120, "type": "TASK", "confidence": 0.9698003530502319}]}, {"text": "As two examples, and give good overviews of the techniques and equations used for Markov models and part-ofspeech tagging, but they are not very explicit in the details that are needed for their application.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.7488088309764862}]}, {"text": "We argue that it is not only the choice of the general model that determines the result of the tagger but also the various \"small\" decisions on alternatives.", "labels": [], "entities": []}, {"text": "The aim of this paper is to give a detailed account of the techniques used in TnT.", "labels": [], "entities": [{"text": "TnT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9784084558486938}]}, {"text": "Additionally, we present results of the tagger on the NEGRA corpus () and the Penn Treebank (.", "labels": [], "entities": [{"text": "NEGRA corpus", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.9815378785133362}, {"text": "Penn Treebank", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9949921369552612}]}, {"text": "The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9894843697547913}]}, {"text": "For a comparison to other taggers, the reader is referred to tl...", "labels": [], "entities": []}, {"text": "tT are elements of the tagset, the additional tags t-l, to, and tT+l are beginning-of-sequence and end-of-sequence markers.", "labels": [], "entities": []}, {"text": "Using these additional tags, even if they stem from rudimentary processing of punctuation marks, slightly improves tagging results.", "labels": [], "entities": [{"text": "tagging", "start_pos": 115, "end_pos": 122, "type": "TASK", "confidence": 0.9608476161956787}]}, {"text": "This is different from formulas presented in other publications, which just stop with a \"loose end\" at the last word.", "labels": [], "entities": []}, {"text": "If sentence boundaries are not marked in the input, TnT adds these tags if it encounters one of [.!?;] as a token.", "labels": [], "entities": []}, {"text": "Transition and output probabilities are estimated from a tagged corpus.", "labels": [], "entities": []}, {"text": "As a first step, we use the maximum likelihood probabilities /5 which are derived from the relative frequencies: Trigrams: Lexical: for all tl, t2, t3 in the tagset and w3 in the lexicon.", "labels": [], "entities": []}, {"text": "N is the total number of tokens in the training corpus.", "labels": [], "entities": []}, {"text": "We define a maximum likelihood probability to be zero if the corresponding nominators and denominators are zero.", "labels": [], "entities": []}, {"text": "As a second step, contextual frequencies are smoothed and lexical frequences are completed by handling words that are not in the lexicon (see below).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the tagger's performance under several aspects.", "labels": [], "entities": []}, {"text": "First of all, we determine the tagging accuracy averaged over ten iterations.", "labels": [], "entities": [{"text": "tagging", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9588389992713928}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9297218918800354}]}, {"text": "The overall accuracy, as well as separate accuracies for known and unknown words are measured.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995348453521729}, {"text": "accuracies", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9775258898735046}]}, {"text": "Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).", "labels": [], "entities": []}, {"text": "An important characteristic of statistical taggers is that they not only assign tags to words but also probabilities in order to rank different assignments.", "labels": [], "entities": []}, {"text": "We distinguish reliable from unreliable assignments by the quotient of the best and second best assignments 1.", "labels": [], "entities": []}, {"text": "All assignments for which this quotient is larger than some threshold are regarded as reliable, the others as unreliable.", "labels": [], "entities": []}, {"text": "As we will see below, accuracies for reliable assignments are much higher.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9961175918579102}]}, {"text": "The tests are performed on partitions of the corpora that use 90% as training set and 10% as test set, so that the test data is guaranteed to be unseen during training.", "labels": [], "entities": []}, {"text": "Each result is obtained by repeating the experiment 10 times with different partitions and averaging the single outcomes.", "labels": [], "entities": []}, {"text": "In all experiments, contiguous test sets are used.", "labels": [], "entities": []}, {"text": "The alternative is a round-robin procedure that puts every 10th sentence into the test set.", "labels": [], "entities": []}, {"text": "We argue that contiguous test sets yield more realistic results because completely unseen articles are tagged.", "labels": [], "entities": []}, {"text": "Using the round-robin procedure, parts of an article are already seen, which significantly reduces the percentage of unknown words.", "labels": [], "entities": []}, {"text": "Therefore, we expect even higher results when testing on every 10th sentence instead of a contiguous set of 10%.", "labels": [], "entities": []}, {"text": "In the following, accuracy denotes the number of correctly assigned tags divided by the number of tokens in the corpus processed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9995653033256531}]}, {"text": "The tagger is allowed to assign exactly one tag to each token.", "labels": [], "entities": []}, {"text": "We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9994338154792786}]}, {"text": "The latter are interesting, since usually unknown tokens are much more difficult to process than known tokens, for which a list of valid tags can be found in the lexicon.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Part-of-speech tagging accuracy for the NEGRA corpus, averaged over 10 test runs, training and  test set are disjoint. The table shows the percentage of unknown tokens, separate accuracies and standard  deviations for known and unknown tokens, as well as the overall accuracy.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7687338888645172}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9382855892181396}, {"text": "NEGRA corpus", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.8792683780193329}, {"text": "accuracy", "start_pos": 277, "end_pos": 285, "type": "METRIC", "confidence": 0.9988725781440735}]}, {"text": " Table 5: Part-of-speech tagging accuracy for the Penn Treebank. The table shows the percentage of unknown  tokens, separate accuracies and standard deviations for known and unknown tokens, as well as the overall  accuracy.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7479481995105743}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9774888753890991}, {"text": "Penn Treebank", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9940975606441498}, {"text": "accuracy", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.9990997314453125}]}]}