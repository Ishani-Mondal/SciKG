{"title": [{"text": "A Classification Approach to Word Prediction*", "labels": [], "entities": [{"text": "Word Prediction", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7573741972446442}]}], "abstractContent": [{"text": "The eventual goal of a language model is to accurately predict the value of a missing word given its context.", "labels": [], "entities": []}, {"text": "We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7837241888046265}]}, {"text": "This approach raises a few new questions that we address.", "labels": [], "entities": []}, {"text": "First, in order to learn good word representations it is necessary to use an expressive representation of the context.", "labels": [], "entities": []}, {"text": "We present away that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially , contribute to each prediction.", "labels": [], "entities": []}, {"text": "Second, since the number of words \"competing\" for each prediction is large, there is a need to \"focus the attention\" on a smaller subset of these.", "labels": [], "entities": []}, {"text": "We exhibit the contribution of a \"focus of attention\" mechanism to the performance of the word predictor.", "labels": [], "entities": [{"text": "word predictor", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.6616357862949371}]}, {"text": "Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks.", "labels": [], "entities": [{"text": "word prediction tasks", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.8860708276430765}]}], "introductionContent": [{"text": "The task of predicting the most likely word based on properties of its surrounding context is the archetypical prediction problem in natural language processing (NLP).", "labels": [], "entities": [{"text": "predicting the most likely word", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.8042705893516541}, {"text": "natural language processing (NLP)", "start_pos": 133, "end_pos": 166, "type": "TASK", "confidence": 0.7493508458137512}]}, {"text": "In many NLP tasks it is necessary to determine the most likely word, part-of-speech (POS) tag or any other token, given its history or context.", "labels": [], "entities": []}, {"text": "Examples include part-of speech tagging, word-sense disambiguation, speech recognition, accent restoration, word choice selection in machine translation, context-sensitive spelling correction and identifying discourse markers.", "labels": [], "entities": [{"text": "part-of speech tagging", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.6181050539016724}, {"text": "word-sense disambiguation", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.773939460515976}, {"text": "speech recognition", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7559848725795746}, {"text": "accent restoration", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7003681361675262}, {"text": "word choice selection in machine translation", "start_pos": 108, "end_pos": 152, "type": "TASK", "confidence": 0.6751436988512675}, {"text": "context-sensitive spelling correction", "start_pos": 154, "end_pos": 191, "type": "TASK", "confidence": 0.6029680768648783}]}, {"text": "Most approaches to these problems are based on n-gram-like modeling.", "labels": [], "entities": []}, {"text": "Namely, the learning methods make use of features which are conjunctions of typically (up to) three consecutive words or POS tags in order to derive the predictor.", "labels": [], "entities": []}, {"text": "In this paper we show that incorporating additional information into the learning process is very * This research is supported by NSF grants IIS-9801638 and SBR-987345. beneficial.", "labels": [], "entities": [{"text": "IIS-9801638", "start_pos": 141, "end_pos": 152, "type": "DATASET", "confidence": 0.5733259916305542}, {"text": "SBR-987345", "start_pos": 157, "end_pos": 167, "type": "DATASET", "confidence": 0.502905547618866}]}, {"text": "In particular, we provide the learner with a rich set of features that combine the information available in the local context along with shallow parsing information.", "labels": [], "entities": []}, {"text": "At the same time, we study a learning approach that is specifically tailored for problems in which the potential number of features is very large but only a fairly small number of them actually participates in the decision.", "labels": [], "entities": []}, {"text": "Word prediction experiments that we perform show significant improvements in error rate relative to the use of the traditional, restricted, set of features.", "labels": [], "entities": [{"text": "Word prediction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7264914661645889}, {"text": "error rate", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9787933528423309}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Word Error Rate results for linear  and non-linear features", "labels": [], "entities": [{"text": "Word Error Rate", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.6445728441079458}]}, {"text": " Table 2: Comparison of the improvement  achieved using similarity methods (Dagan et  al., 1999) and using the methods presented in  this paper. Results are shown in percentage  of improvement in accuracy over the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.9991064667701721}]}, {"text": " Table 3: Evaluating Focus of Attention: Word", "labels": [], "entities": [{"text": "Evaluating Focus of Attention", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.8128278851509094}]}, {"text": " Table 4: Simulating Speech Recognizer: Word", "labels": [], "entities": [{"text": "Simulating Speech Recognizer", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.9314969976743063}, {"text": "Word", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.6044682860374451}]}, {"text": " Table 5: Simulating Speech Recognizer: Word", "labels": [], "entities": [{"text": "Simulating Speech Recognizer", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.9313673575719198}, {"text": "Word", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.6045896410942078}]}]}