{"title": [{"text": "Message Classification in the Call Center", "labels": [], "entities": [{"text": "Message Classification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7571181356906891}]}], "abstractContent": [{"text": "Customer care in technical domains is increasingly based on e-mail communication, allowing for the reproduction of approved solutions.", "labels": [], "entities": []}, {"text": "Identifying the customer's problem is often time-consuming, as the problem space changes if new products are launched.", "labels": [], "entities": [{"text": "Identifying the customer's problem", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8302161574363709}]}, {"text": "This paper describes anew approach to the classification of e-mail requests based on shallow text processing and machine learning techniques.", "labels": [], "entities": [{"text": "classification of e-mail requests", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.8818853944540024}]}, {"text": "It is implemented within an assistance system for call center agents that is used in a commercial setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "Customer care in technical domains is increasingly based on e-mail communication, allowing for the reproduction of approved solutions.", "labels": [], "entities": []}, {"text": "For a call center agent, identifying the customer's problem is often time-consuming, as the problem space changes if new products are launched or existing regulations are modified.", "labels": [], "entities": []}, {"text": "The typical task of a call center agent processing e-mail requests consists of the following steps: Recognize the problem(s): read and understand the e-mail request; Search a solution: identify and select predefined text blocks; Provide the solution: if necessary, customize text blocks to meet the current request, and send the text.", "labels": [], "entities": []}, {"text": "This task can partly be automated by a system suggesting relevant solutions for an incoming e-mail.", "labels": [], "entities": []}, {"text": "This would cover the first two steps.", "labels": [], "entities": []}, {"text": "The last step can be delicate, as its primary goal is to keep the customer satisfied.", "labels": [], "entities": []}, {"text": "Thus human intervention seems mandatory to allow for individual, customized answers.", "labels": [], "entities": []}, {"text": "Such a system will \u2022 reduce the training effort required since agents don't have to know every possible solution for every possible problem; \u2022 increase the agents' performance since agents can more quickly select a solution among several offered than searching one; \u2022 improve the quality of responses since agents will behave more homogeneously -both as a group and overtime -and commit fewer errors.", "labels": [], "entities": []}, {"text": "Given that free text about arbitrary topics must be processed, in-depth approaches to language understanding are not feasible.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.7172732949256897}]}, {"text": "Given further that the topics may changeover time, a top-down approach to knowledge modeling is out of the question.", "labels": [], "entities": [{"text": "knowledge modeling", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7594076693058014}]}, {"text": "Rather a combination of shallow text processing (STP) with statistics-based machine learning techniques (SML) is called for.", "labels": [], "entities": []}, {"text": "STP gathers partial information about text such as part of speech, word stems, negations, or sentence type.", "labels": [], "entities": [{"text": "STP gathers partial information about text such as part of speech, word stems, negations, or sentence type", "start_pos": 0, "end_pos": 106, "type": "Description", "confidence": 0.764867228269577}]}, {"text": "These types of information can be used to identify the linguistic properties of a large training set of categorized e-mails.", "labels": [], "entities": []}, {"text": "SML techniques are used to build a classifier that is used for new, incoming messages.", "labels": [], "entities": [{"text": "SML", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.970559298992157}]}, {"text": "Obviously, the change of topics can be accommodated by adding new categories and e-mails and producing anew classifier on the basis of old and new data.", "labels": [], "entities": []}, {"text": "We call this replacement of a classifier \"relearning\".", "labels": [], "entities": []}, {"text": "This paper describes anew approach to the classification of e-mail requests along these lines.", "labels": [], "entities": [{"text": "classification of e-mail requests", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.8765216320753098}]}, {"text": "It is implemented within the ICe-MAIL system, which is an assistance system for call center agents that is currently used in a commercial setting.", "labels": [], "entities": []}, {"text": "Section 2 describes important properties of the input data, i.e. the e-mail texts on the one hand, and the categories on the other.", "labels": [], "entities": []}, {"text": "These properties influenced the system architecture, which is presented in Section 3.", "labels": [], "entities": []}, {"text": "Various publicly available SML systems have been tested with different methods of STP-based preprocessing.", "labels": [], "entities": [{"text": "SML", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9809838533401489}]}, {"text": "Section 4 describes the results.", "labels": [], "entities": []}, {"text": "The implementation and usage of the system including the graphical user interface is presented in Section 5.", "labels": [], "entities": []}, {"text": "We conclude by giving an outlook to further expected improvements (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe the experiments and results we achieved with different linguistic preprocessing and learning algorithms and provide some interpretations.", "labels": [], "entities": []}, {"text": "We start out from the corpus of categorized emails described in Section 2.", "labels": [], "entities": []}, {"text": "In order to normalize the vectors representing the preprocessing results of texts of different length, and to concentrate on relevant material (cf.), we define the relevancy vector as follows.", "labels": [], "entities": []}, {"text": "First, all documents are preprocessed, yielding a list of results for each category.", "labels": [], "entities": []}, {"text": "From each of these lists, the 100 most frequent results -according to a TF/IDF measure -are selected.", "labels": [], "entities": [{"text": "IDF", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.8046390414237976}]}, {"text": "The relevancy vector consists of all selected results, where doubles are eliminated.", "labels": [], "entities": []}, {"text": "Its length was about 2500 for the 47 categories; it slightly varied with the kind of preprocessing used.", "labels": [], "entities": [{"text": "length", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9897482395172119}]}, {"text": "During the learning phase, each document is preprocessed.", "labels": [], "entities": []}, {"text": "The result is mapped onto a vector of the same length as the relevancy vector.", "labels": [], "entities": []}, {"text": "For every position in the relevancy vector, it is determined whether the corresponding result has been found.", "labels": [], "entities": []}, {"text": "In that case, the value of the result vector element is 1, otherwise it is 0.", "labels": [], "entities": []}, {"text": "In the categorization phase, the new document is preprocessed, and a result vector is built as described above and handed over to the categorizer (cf. While we tried various kinds of linguistic preprocessing, systematic experiments have been carried outwith morphological analysis (MorphAna), shallow parsing heuristics (STP-Heuristics), and a combination of both (Combined).", "labels": [], "entities": [{"text": "shallow parsing heuristics", "start_pos": 293, "end_pos": 319, "type": "TASK", "confidence": 0.7292317052682241}]}, {"text": "MorphAna: Morphological Analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.", "labels": [], "entities": []}, {"text": "We are using a lexicon of approx. 100000 word stems of German ().", "labels": [], "entities": []}, {"text": "STP-Heuristics: Shallow parsing techniques are used to heuristically identify sentences containing relevant information.", "labels": [], "entities": [{"text": "STP-Heuristics", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.7047140002250671}, {"text": "Shallow parsing", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.6778364777565002}]}, {"text": "The e-mails usually contain questions and/or descriptions of problems.", "labels": [], "entities": []}, {"text": "The manual analysis of a sample of the data suggested some linguistic constructions frequently used to express the problem.", "labels": [], "entities": []}, {"text": "We expected that content words in these constructions should be particularly influential to the categorization.", "labels": [], "entities": []}, {"text": "Words in these constructions are extracted and processed as in MorphAna, and all other words are ignored.", "labels": [], "entities": []}, {"text": "5 The heuristics were implemented in ICC-MAIL using sines.", "labels": [], "entities": []}, {"text": "The constructions of interest include negations at the sentence and the phrasal level, yes-no and wh-questions, and declaratives immediately preceding questions.", "labels": [], "entities": []}, {"text": "Negations were found to describe a state to be changed or to refer to missing objects, as in I cannot read my email or There is no correct date.", "labels": [], "entities": []}, {"text": "We identified them through negation particles.", "labels": [], "entities": []}, {"text": "8 Questions most often refer to the problem in hand, either directly, e.g. How can I start my email program.", "labels": [], "entities": []}, {"text": "~ or indirectly, e.g. Why is this the case?.", "labels": [], "entities": []}, {"text": "The latter most likely refers to the preceding sentence, e.g. My system drops my e-mails.", "labels": [], "entities": []}, {"text": "Questions are identified by their word order, i.e. yes-no questions start with a verb and wh-questions with a wh-particle.", "labels": [], "entities": []}, {"text": "Combined: In order to emphasize words found relevant by the STP heuristics without losing other information retrieved by MorphAna, the previous two techniques are combined.", "labels": [], "entities": [{"text": "MorphAna", "start_pos": 121, "end_pos": 129, "type": "DATASET", "confidence": 0.8988701701164246}]}, {"text": "Emphasis is represented hereby doubling the number of occurrences of the tokens in the normalization phase, thus increasing their TF/IDF value.", "labels": [], "entities": [{"text": "TF/IDF value", "start_pos": 130, "end_pos": 142, "type": "METRIC", "confidence": 0.8403713256120682}]}, {"text": "Call center agents judge the performance of ICC-MAIL most easily in terms of accuracy: In what percentage of cases does the classifier suggest the correct text block?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9990119934082031}]}, {"text": "In, detailed information about the accuracy achieved is presented.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.999373733997345}]}, {"text": "All experiments were carried out using 10-fold cross-validation on the data described in Section 2.", "labels": [], "entities": []}, {"text": "In all experiments the SVM_Light system outperformed other learning algorithms, which confirms Yang's (Yang and Liu, 1999) results for SVMs fed with Reuters data.", "labels": [], "entities": [{"text": "Reuters data", "start_pos": 149, "end_pos": 161, "type": "DATASET", "confidence": 0.9412420392036438}]}, {"text": "The k-nearest neighbor algorithm IB performed surprisingly badly although different values ofk were used.", "labels": [], "entities": []}, {"text": "For IB, ID3, C4.5, C5.0, Naive Bayes, RIPPER and SVM_Light, linguistic preprocessing increased the overall performance.", "labels": [], "entities": []}, {"text": "In fact, the method performing best, SVM_Light, gained 3.5% by including the task-oriented heuristics.", "labels": [], "entities": []}, {"text": "However, the boosted RIPPER and LVQ scored a decreased accuracy value there.", "labels": [], "entities": [{"text": "RIPPER", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9911208748817444}, {"text": "LVQ", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.8536120057106018}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9995984435081482}]}, {"text": "For LVQ the decrease maybe due to the fact that no adaptations to 5If no results were found this way, MorphAna was applied instead.", "labels": [], "entities": [{"text": "MorphAna", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.7816469073295593}]}, {"text": "6We certainly would have benefited from lexical semantic information, e.g. The correct date is missing would not be captured by our approach.", "labels": [], "entities": []}, {"text": "the domain were made, such as adapting the number of codebook vectors, the initial learning parameters or the number of iterations during training (cf. ().", "labels": [], "entities": []}, {"text": "Neural networks are rather sensitive to misconfigurations.", "labels": [], "entities": []}, {"text": "The boosting for RIP-PER seems to run into problems of overfitting.", "labels": [], "entities": [{"text": "RIP-PER", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.548113226890564}]}, {"text": "We noted that in six trials the accuracy could be improved in Combined compared to MorphAna, but in four trials, boosting led to deterioration.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9996274709701538}, {"text": "Combined", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9957365989685059}, {"text": "MorphAna", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.8839040398597717}]}, {"text": "This effect is also mentioned in.", "labels": [], "entities": []}, {"text": "These figures are slightly lower than the ones reported by) that were obtained from a different data set.", "labels": [], "entities": []}, {"text": "Moreover, these data did not contain multiple queries in one e-mall.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of Experiments. Most SML tools deliver the best result only. SVM_Light produces ranked  results, allowing to measure the accuracy of the top five alternatives (Best5).", "labels": [], "entities": [{"text": "SML", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9721457958221436}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9971144199371338}]}]}