{"title": [{"text": "Word Sense Disambiguation for Cross-Language Information Retrieval", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7266749938329061}, {"text": "Cross-Language Information Retrieval", "start_pos": 30, "end_pos": 66, "type": "TASK", "confidence": 0.7449036041895548}]}], "abstractContent": [{"text": "We have developed a word sense disambiguation algorithm, following Cheng and Wilensky (1997), to disambiguate among WordNet synsets.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.7298644185066223}]}, {"text": "This algorithm is to be used in a cross-language information retrieval system, CINDOR, which indexes queries and documents in a language-neutral concept representation based on WordNet synsets.", "labels": [], "entities": [{"text": "cross-language information retrieval", "start_pos": 34, "end_pos": 70, "type": "TASK", "confidence": 0.6121210555235544}]}, {"text": "Our goal is to improve retrieval precision through word sense disambiguation.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9471370577812195}, {"text": "word sense disambiguation", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.7078886330127716}]}, {"text": "An evaluation against human disambiguation judgements suggests promise for our approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "The CINDOR cross-language information retrieval system () uses an information structure known as \"conceptual interlingua\" for query and document representation.", "labels": [], "entities": [{"text": "CINDOR cross-language information retrieval", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.6614540964365005}, {"text": "document representation", "start_pos": 136, "end_pos": 159, "type": "TASK", "confidence": 0.7054344415664673}]}, {"text": "This conceptual interlingua is a hierarchically organized multilingual concept lexicon, which is structured following WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 118, "end_pos": 125, "type": "DATASET", "confidence": 0.9612222909927368}]}, {"text": "By representing query and document terms by their WordNet synset numbers we arrive at essentially a language neutral representation consisting of synset numbers representing concepts.", "labels": [], "entities": []}, {"text": "This representation facilitates cross-language retrieval by matching tea-m synonyms in English as well as across languages.", "labels": [], "entities": [{"text": "cross-language retrieval", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.8485081791877747}]}, {"text": "However, many terms are polysemous and belong to multiple synsets, resulting in spurious matches in retrieval.", "labels": [], "entities": []}, {"text": "The nounfigure for example appears in 13 synsets in WordNet 1.6.", "labels": [], "entities": [{"text": "WordNet 1.6", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.9248153865337372}]}, {"text": "This research paper describes the early stages I of our efforts to develop a word sense disambiguation (WSD) algorithm aimed at improving the precision of our cross-language retrieval system.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.7737123072147369}, {"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9978501796722412}]}], "datasetContent": [{"text": "As suggested by the WSD literature, evaluation of word sense disambiguation systems is not yet standardized.", "labels": [], "entities": [{"text": "WSD", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.8361666798591614}, {"text": "word sense disambiguation", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6843061149120331}]}, {"text": "Some WSD evaluations have been done using the Brown Corpus as training and testing resources and comparing the results against SemCor 3, the sense-tagged version of the Brown Corpus (Agirre and).", "labels": [], "entities": [{"text": "WSD", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.9635916948318481}, {"text": "Brown Corpus", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.9668315649032593}, {"text": "Brown Corpus", "start_pos": 169, "end_pos": 181, "type": "DATASET", "confidence": 0.9256661236286163}]}, {"text": "Others have used common test suites such as the 2094-word line data of.", "labels": [], "entities": []}, {"text": "Still others have tended to use their own metrics.", "labels": [], "entities": []}, {"text": "We chose an evaluation with a userbased component that allowed a ranked list of sense selection for each target word and enabled a comprehensive comparison between automatic and manual WSD results.", "labels": [], "entities": []}, {"text": "In addition we wanted to base the disambiguation matrix on a corpus that we use for retrieval.", "labels": [], "entities": []}, {"text": "This approach allows fora much richer evaluation than a simple hit-ormiss test.", "labels": [], "entities": []}, {"text": "For vahdation purpose, we will conduct a fully automatic evaluation against SemCor in our future efforts.", "labels": [], "entities": []}, {"text": "We use in vitro evaluation in this study, i.e. the WSD algorithm is tested independent of the retrieval system.", "labels": [], "entities": []}, {"text": "The population consists of all the nouns in WordNet, after removal of monoseanous nouns, and after removal of a problematic class of polysemous nouns.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9624279737472534}]}, {"text": "4 We drew a random sample of 87 polysemous nouns 5 from this population.", "labels": [], "entities": []}, {"text": "In preparation, for each noun in our sample we identified all the documents containing that noun from the Associated Press (AP) newspaper corpus.", "labels": [], "entities": [{"text": "Associated Press (AP) newspaper corpus", "start_pos": 106, "end_pos": 144, "type": "DATASET", "confidence": 0.776448632989611}]}, {"text": "The testing document set was then formed by randomly selecting 10 documents from the set of identified documents for each of the 87 nouns.", "labels": [], "entities": []}, {"text": "In total, there are 867 documents in the 3 SemCor is a semantically sense-tagged corpus comprising approximately 250, 000 words.", "labels": [], "entities": []}, {"text": "The reported error rate is around 10% for polysemous words.", "labels": [], "entities": [{"text": "error rate", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9389932751655579}]}, {"text": "4 This class of nouns refers to nouns that are in synsets in which they are the sole word, or in synsets whose words were subsets of other synsets for that noun.", "labels": [], "entities": []}, {"text": "This situation makes disambiguation extremely problematic.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.986607551574707}]}, {"text": "This class of noun will be dealt within a future version of our algorithm but for now it is beyond the scope of this evaluation.", "labels": [], "entities": []}, {"text": "5 A polysemous noun is defined as a noun that belongs to two or more synsets.", "labels": [], "entities": []}, {"text": "The training document set consists of all the documents in the AP corpus excluding the above-mentioned 867 documents.", "labels": [], "entities": [{"text": "training document set", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.7343279123306274}, {"text": "AP corpus", "start_pos": 63, "end_pos": 72, "type": "DATASET", "confidence": 0.9141808152198792}]}, {"text": "For each noun in our sample, we selected all its corresponding WordNet noun synsets and randomly selected 10 sentence occurrences with each from one of the 10 random documents.", "labels": [], "entities": [{"text": "WordNet noun synsets", "start_pos": 63, "end_pos": 83, "type": "DATASET", "confidence": 0.8962606986363729}]}, {"text": "After collecting 87 polysemous nouns with 10 noun sentences each, we had 870 sentences for disambiguation.", "labels": [], "entities": []}, {"text": "Four human judges were randomly assigned to two groups with two judges each, and each judge was asked to disambiguate 275 word occurrences out of which 160 were unique and 115 were shared with the other judge in the same group.", "labels": [], "entities": []}, {"text": "For each word occurrence, the judge put the target word's possible senses in rank order according to their appropriateness given the context (ties are allowed).", "labels": [], "entities": []}, {"text": "Our WSD algorithm was also fed with the identical set of 870 word occurrences in the sense prediction phase and produced a ranked hst of senses for each word occurrence.", "labels": [], "entities": [{"text": "sense prediction", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7382038831710815}]}, {"text": "Since our study has a matched-group design in which the subjects (word occurrences) receive both the treatments and control, the measurement of variables is on an ordinal scale, and there is no apparently applicable parametric statistical procedure available, two nonparametric procedures -the Friedman two-way analysis of variance and the Spearman rank correlation coefficient -were originally chosen as candidates for the statistical analysis of our results.", "labels": [], "entities": [{"text": "Spearman rank correlation coefficient", "start_pos": 340, "end_pos": 377, "type": "METRIC", "confidence": 0.6258859038352966}]}, {"text": "However, the number of ties in our results renders the Spearman coefficient unreliable.", "labels": [], "entities": [{"text": "Spearman coefficient", "start_pos": 55, "end_pos": 75, "type": "METRIC", "confidence": 0.6415891945362091}]}, {"text": "We have therefore concentrated on the Friedman analysis of our experimental results.", "labels": [], "entities": []}, {"text": "We use the two-alternative test with o~=0.05.", "labels": [], "entities": []}, {"text": "The first tests of interest were aimed at estabhshing inter-judge reliability across the 115 shared sentenees by each pair of judges.", "labels": [], "entities": [{"text": "estabhshing", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9545904994010925}, {"text": "reliability", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.6980575919151306}]}, {"text": "The null hypothesis can be generalized as \"There is no difference in judgments on the same word occurrences between two judges in the same group\".", "labels": [], "entities": []}, {"text": "Following general steps of conducting a Friedman test as described by, we cast raw ranks in a two-way table having 2 conditions/columns (K = 2) with each of the human judges in the pair serving as one condition and 365 subjects/rows (N = 365) which are all the senses of the 115 word occurrences that were judged by both human judges.", "labels": [], "entities": []}, {"text": "Our second area of interest was the comparison of automatic WSD, manual WSD, and \"sense pooling\".", "labels": [], "entities": [{"text": "WSD", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9200700521469116}, {"text": "sense pooling", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.6849253922700882}]}, {"text": "Sense pooling equates to no disambiguation, where each sense of a word is considered equally likely (a tie).", "labels": [], "entities": [{"text": "Sense pooling", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7195385694503784}]}, {"text": "The null hypothesis (H0) is \"There is no difference among manual WSD, automatic WSD, and sense pooling (all the conditions come from the same population)\".", "labels": [], "entities": [{"text": "WSD", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.8160346746444702}, {"text": "sense pooling", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.6846426576375961}]}, {"text": "The steps for Friedman analysis were similar to what we did for the inter-judge reliability test while the conditions and subjects were changed in each test according to what we would like to compare.", "labels": [], "entities": [{"text": "Friedman analysis", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.9208860695362091}]}, {"text": "Test results are summarized in.", "labels": [], "entities": []}, {"text": "In the three-way comparison shown in the first row of the table, we rejected H0 so there was at least one condition that was from a different population.", "labels": [], "entities": []}, {"text": "By further conducting tests which examined each two of the above three conditions at a time we found that it was sense pooling that came from a different population while manual and automatic WSD were not significantly different.", "labels": [], "entities": [{"text": "WSD", "start_pos": 192, "end_pos": 195, "type": "TASK", "confidence": 0.8946234583854675}]}, {"text": "We can therefore conclude that our WSD algorithm is better than no disambiguation.", "labels": [], "entities": [{"text": "WSD", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.8562543392181396}]}], "tableCaptions": []}