{"title": [], "abstractContent": [{"text": "In this paper we tackle sentence boundary disam-biguation through a part-of-speech (POS) tagging framework.", "labels": [], "entities": [{"text": "sentence boundary disam-biguation", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.6302831073602041}, {"text": "part-of-speech (POS) tagging", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.6492766678333283}]}, {"text": "We describe necessary changes in text tokenization and the implementation of a POS tag-ger and provide results of an evaluation of this system on two corpora.", "labels": [], "entities": []}, {"text": "We also describe an extension of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.768001914024353}, {"text": "proper name identification", "start_pos": 116, "end_pos": 142, "type": "TASK", "confidence": 0.6064786016941071}, {"text": "abbreviation handling", "start_pos": 147, "end_pos": 168, "type": "TASK", "confidence": 0.7664121687412262}]}, {"text": "This made the resulting system robust to domain and topic shifts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence boundary disambiguation (SBD) is an important aspect in developing virtually any practical text processing application -syntactic parsing, Information Extraction, Machine Translation, Text Alignment, Document Summarization, etc.", "labels": [], "entities": [{"text": "Sentence boundary disambiguation (SBD)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8364712099234263}, {"text": "syntactic parsing", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.7276930510997772}, {"text": "Information Extraction", "start_pos": 148, "end_pos": 170, "type": "TASK", "confidence": 0.8036469221115112}, {"text": "Machine Translation", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.8275321424007416}, {"text": "Text Alignment", "start_pos": 193, "end_pos": 207, "type": "TASK", "confidence": 0.8110961616039276}, {"text": "Document Summarization", "start_pos": 209, "end_pos": 231, "type": "TASK", "confidence": 0.923288106918335}]}, {"text": "Segmenting text into sentences inmost cases is a simple matter-a period, an exclamation mark or a question mark usually signal a sentence boundary.", "labels": [], "entities": [{"text": "Segmenting text into sentences", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8746341913938522}]}, {"text": "However, there are cases when a period denotes a decimal point or is apart of an abbreviation and thus it does not signal a sentence break.", "labels": [], "entities": []}, {"text": "Furthermore, an abbreviation itself can be the last token in a sentence, in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop).", "labels": [], "entities": []}, {"text": "The first large class of sentence boundary disambiguators uses manually built rules which are usually encoded in terms of regular expression grammars supplemented with lists of abbreviations, common words, proper names, etc.", "labels": [], "entities": [{"text": "sentence boundary disambiguators", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.711388369401296}]}, {"text": "For instance, the Alembic workbench () contains a sentence splitting module which employs over 100 regular-expression rules written in Flex.", "labels": [], "entities": [{"text": "Alembic workbench", "start_pos": 18, "end_pos": 35, "type": "DATASET", "confidence": 0.9026908874511719}, {"text": "sentence splitting", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7347197234630585}]}, {"text": "To put together a few rules which do a job is fast and easy, but to develop a good rule-based system is quite a labour consuming enterprise.", "labels": [], "entities": []}, {"text": "Another potential shortcoming is that such systems are usually closely tailored to a particular corpus and are not easily portable across domains.", "labels": [], "entities": []}, {"text": "Automatically trainable software is generally seen as away of producing systems quickly re-trainable fora new corpus, domain or even for another language.", "labels": [], "entities": []}, {"text": "Thus, the second class of SBD systems employs machine learning techniques such as decision tree classifiers, maximum entropy modeling (MAXTERMINATOR)), neural networks (SATZ)), etc..", "labels": [], "entities": [{"text": "SBD", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9698796272277832}]}, {"text": "Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, word class, etc., found in the local context of potentim sentence breaking punctuation.", "labels": [], "entities": [{"text": "SBD task", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.9295519888401031}, {"text": "potentim sentence breaking punctuation", "start_pos": 183, "end_pos": 221, "type": "TASK", "confidence": 0.6850477978587151}]}, {"text": "There is, however, one catch -all machine learning approaches to the SBD task known to us require labeled examples for training.", "labels": [], "entities": [{"text": "SBD task", "start_pos": 69, "end_pos": 77, "type": "TASK", "confidence": 0.9287653267383575}]}, {"text": "This implies an investment in the annotation phase.", "labels": [], "entities": []}, {"text": "There are two corpora normally used for evaluation and development in a number of text processing tasks and in the SBD task in particular: the Brown Corpus and the Wall Street Journal (WSJ) corpus -both part of the Penn Treebank.", "labels": [], "entities": [{"text": "SBD task", "start_pos": 115, "end_pos": 123, "type": "TASK", "confidence": 0.9227670729160309}, {"text": "Brown Corpus", "start_pos": 143, "end_pos": 155, "type": "DATASET", "confidence": 0.9757013618946075}, {"text": "Wall Street Journal (WSJ) corpus", "start_pos": 164, "end_pos": 196, "type": "DATASET", "confidence": 0.9077556729316711}, {"text": "Penn Treebank", "start_pos": 215, "end_pos": 228, "type": "DATASET", "confidence": 0.992765337228775}]}, {"text": "Words in both these corpora are annotated with part-ofspeech (POS) information and the text is split into documents, paragraphs and sentences.", "labels": [], "entities": []}, {"text": "This gives all necessary information for the development of an SBD system and its evaluation.", "labels": [], "entities": [{"text": "SBD", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.979796826839447}]}, {"text": "State-of-theart machine-learning and rule-based SBD systems achieve the error rate of about 0.8-1.5% measured on the Brown Corpus and the WSJ.", "labels": [], "entities": [{"text": "error rate", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9903212785720825}, {"text": "Brown Corpus and the WSJ", "start_pos": 117, "end_pos": 141, "type": "DATASET", "confidence": 0.8592170119285584}]}, {"text": "The best performance on the WSJ was achieved by a combination of the SATZ system with the Alembic system -0.5% error rate.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.8440577387809753}, {"text": "SATZ", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.542578935623169}, {"text": "Alembic", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9703961610794067}, {"text": "error rate", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9322871267795563}]}, {"text": "The best performance on the Brown Corpus, 0.2% error rate, was reported by, who trained a decision tree classifier on a 25 million word corpus.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9880771338939667}, {"text": "error rate", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.981221616268158}]}], "datasetContent": [{"text": "Using the modified treebank we trained a tri-gram POS tagger (Mikheev, 1997) based on a combination of Hidden Markov Models (HMM) and Maximum Entropy (ME) technologies.", "labels": [], "entities": []}, {"text": "Words were clustered into ambiguity classes according to sets of POS tags they can take on.", "labels": [], "entities": []}, {"text": "This is a standard technique that was also adopted by the SATZ system 1.", "labels": [], "entities": [{"text": "SATZ system 1", "start_pos": 58, "end_pos": 71, "type": "DATASET", "confidence": 0.9460766712824503}]}, {"text": "The tagger predictions were based on the ambiguity class of the current word together with 1The SATZ system operated with a reduced set of 18 generic categories instead of 40 POS tags of the Penn Treebank tag-set.", "labels": [], "entities": [{"text": "Penn Treebank tag-set", "start_pos": 191, "end_pos": 212, "type": "DATASET", "confidence": 0.991697351137797}]}, {"text": "the POS trigrams: hypothesized current POS tag and partially disambiguated POS tags of two previous word-tokens.", "labels": [], "entities": []}, {"text": "We also collected a list of abbreviations as explained later in this paper and used the information about whether a word is an abbreviation, ordinary word or potential abbreviation (i.e. a word which could not be robustly classified in the first two categories).", "labels": [], "entities": []}, {"text": "This tagger employed Maximum Entropy models for tag transition and emission estimates and Viterbi algorithm for the optimal path search.", "labels": [], "entities": []}, {"text": "Using the forward-backward algorithm we trained our tagger in the unsupervised mode i.e. without using the annotation available in the Brown Corpus and the WSJ.", "labels": [], "entities": [{"text": "Brown Corpus and the WSJ", "start_pos": 135, "end_pos": 159, "type": "DATASET", "confidence": 0.8313328385353088}]}, {"text": "For evaluation purposes we trained our tagger on the Brown Corpus and applied it to the WSJ corpus and vice versa.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.9860367178916931}, {"text": "WSJ corpus", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.9653276205062866}]}, {"text": "We preferred this method to ten-fold cross-validation because this allowed us to produce only two tagging models instead of twenty and also this allowed us to test the tagger in harsher conditions when it is applied to texts which are very distant from the ones it was trained on.", "labels": [], "entities": []}, {"text": "In this research we concentrated on measuring the performance only on two categories of word-tokens: on periods and other sentence-ending punctuation and on word-tokens in mandatory positions.", "labels": [], "entities": []}, {"text": "Mandatory positions are positions which might require a word to be capitalized e.g. after a period, quotes, brackets, in all-capitalized titles, etc.", "labels": [], "entities": []}, {"text": "At the evaluation we considered proper nouns (NNP), plural proper nouns (NNPS) and proper adjectives 2 (JJP) to signal a proper name, all all other categories were considered to signal a common word or punctuation.", "labels": [], "entities": []}, {"text": "We also did not consider as an error the mismatch between \".\" and \"*\" categories because both of them signal that a period denotes the end of sentence and the difference between them is only whether this period follows an abbreviation or a regular word.", "labels": [], "entities": []}, {"text": "In all our experiments we treated embedded sentence boundaries in the same way as normal sentence boundaries.", "labels": [], "entities": []}, {"text": "The embedded sentence boundary occurs when there is a sentence inside a sentence.", "labels": [], "entities": []}, {"text": "This 2These are adjectives like \"American\" which are always written capitalized.", "labels": [], "entities": []}, {"text": "We identified and marked them in the WSJ and Brown Corpus, can be a quoted direct speech sub-sentence inside a sentence, this can be a sub-sentence embedded in brackets, etc.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.9647468328475952}, {"text": "Brown Corpus", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.8153956830501556}]}, {"text": "We considered closing punctuation of such sentences equal to closing punctuation of ordinary sentences.", "labels": [], "entities": []}, {"text": "There are two types of error the tagger can make when disambiguating sentence boundaries.", "labels": [], "entities": []}, {"text": "The first one comes from errors made by the tagger in identifying proper names and abbreviations.", "labels": [], "entities": []}, {"text": "The second one comes from the limitation of the POS tagging approach to the SBD task.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.7701237201690674}, {"text": "SBD task", "start_pos": 76, "end_pos": 84, "type": "TASK", "confidence": 0.9150565564632416}]}, {"text": "This is when an abbreviation is followed by a proper name: POS information normally is not sufficient to disambiguate such cases and the tagger opted to resolve all such cases as \"not sentence boundary\".", "labels": [], "entities": []}, {"text": "There are about 5-7% of such cases in the Brown Corpus and the WSJ and the majority of them, indeed, do not signal a sentence boundary.", "labels": [], "entities": [{"text": "Brown Corpus and the WSJ", "start_pos": 42, "end_pos": 66, "type": "DATASET", "confidence": 0.7388952970504761}]}, {"text": "We can estimate the upper bound for our approach by pretending that the tagger was able to identify all abbreviations and proper names with perfect accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.994687557220459}]}, {"text": "We can sinmlate this by using the information available in the treebank.", "labels": [], "entities": []}, {"text": "It turned out that the tagger marked all the cases when an abbreviation is followed by a proper name, punctuation, non-capitalized word or a number as \"not sentence boundary\".", "labels": [], "entities": []}, {"text": "All other periods were marked as sentence-terminal.", "labels": [], "entities": []}, {"text": "This produced 0.01% error rate on the Brown Corpus and 0.13% error rate on the WSJ as displayed in the first row of In practice, however, we cannot expect the tagger to be 100% correct and the second row of displays the actual results of applying our POS tagger to the Brown Corpus and tile WSJ.", "labels": [], "entities": [{"text": "error rate", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9776854813098907}, {"text": "Brown Corpus", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9933605492115021}, {"text": "error rate", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.958317220211029}, {"text": "WSJ", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.8174290657043457}, {"text": "Brown Corpus", "start_pos": 269, "end_pos": 281, "type": "DATASET", "confidence": 0.9841488599777222}, {"text": "WSJ", "start_pos": 291, "end_pos": 294, "type": "DATASET", "confidence": 0.7429064512252808}]}, {"text": "General tagging performance on both our corpora was a bit better than a 4% error rate which is inline with the standard performance of POS taggers reported on these two corpora.", "labels": [], "entities": [{"text": "error rate", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9231128692626953}, {"text": "POS taggers", "start_pos": 135, "end_pos": 146, "type": "TASK", "confidence": 0.6981822997331619}]}, {"text": "On the capitalized words in mandatory positions the tagger achieved a 3.1-4.7% error rate which is an improvement over the lexical lookup approach by 2-3 times.", "labels": [], "entities": [{"text": "error rate", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9877811968326569}]}, {"text": "On the sentence breaking punctuation the tagger performed extremely wellan error rate of 0.39% on the WSJ and 0.25% on the Brown Corpus.", "labels": [], "entities": [{"text": "sentence breaking punctuation", "start_pos": 7, "end_pos": 36, "type": "TASK", "confidence": 0.8315680821736654}, {"text": "error rate", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9911307692527771}, {"text": "WSJ", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.9213314056396484}, {"text": "Brown Corpus", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.9946392476558685}]}, {"text": "If we compare these results with the upper bound we see that the errors made by the tagger on the capitalized words and abbreviations instigated about a 0.25% error rate on the sentence boundaries.", "labels": [], "entities": []}, {"text": "We also applied our tagger to single-case texts.", "labels": [], "entities": []}, {"text": "We converted the WSJ and the Brown Corpus to upper-case only.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.9229117631912231}, {"text": "Brown Corpus", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.9771929383277893}]}, {"text": "In contrast to the mixed case texts where capitalization together with the syntactic information provided very reliable evidence, syntactic information without capitalization is not sufficient to disambiguate sentence boundaries.", "labels": [], "entities": []}, {"text": "For the majority of POS tags there is no clear preference as to whether they are used as sentence starting or sentence internal.", "labels": [], "entities": [{"text": "sentence starting", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7230944931507111}]}, {"text": "To minimize the error rate on single case texts, our tagger adopted a strategy to mark all periods which follow al)breviations as \"non-sentence boundaries\".", "labels": [], "entities": [{"text": "error rate", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9620400071144104}]}, {"text": "This gave a 1.98% error rate on the WSJ and a 0.51% error rate on the Brown Corpus.", "labels": [], "entities": [{"text": "error rate", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9829495847225189}, {"text": "WSJ", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.9341840147972107}, {"text": "error", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9896366000175476}, {"text": "Brown Corpus", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.9944012761116028}]}, {"text": "These results are inline with the results reported for the SATZ system on single case texts.", "labels": [], "entities": [{"text": "SATZ system", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.8904053866863251}]}], "tableCaptions": [{"text": " Table 1: POS Tagging on sentence splitting punctuation and ambiguously capitalized words", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7953118085861206}, {"text": "sentence splitting punctuation", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.7810389498869578}]}]}