{"title": [{"text": "Compound Noun Segmentation Based on Lexical Data Extracted from Corpus*", "labels": [], "entities": [{"text": "Compound Noun Segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6215920249621073}, {"text": "Lexical Data Extracted from Corpus", "start_pos": 36, "end_pos": 70, "type": "DATASET", "confidence": 0.6506155490875244}]}], "abstractContent": [{"text": "Compound noun analysis is one of the crucial problems in Korean language processing because a series of nouns in Korean may appear without white space in real texts, which makes it difficult to identify the morphological constituents.", "labels": [], "entities": [{"text": "Compound noun analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8310449322064718}, {"text": "Korean language processing", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.6171761353810629}]}, {"text": "This paper presents an effective method of Korean compound noun segmen-tation based on lexical data extracted from corpus.", "labels": [], "entities": []}, {"text": "The segmentation is done by two steps: First, it is based on manually constructed built-in dictionary for segmentation whose data were extracted from 30 million word corpus.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9732243418693542}, {"text": "segmentation", "start_pos": 106, "end_pos": 118, "type": "TASK", "confidence": 0.9687432050704956}]}, {"text": "Second, a segmentation algorithm using statistical data is proposed, where simple nouns and their frequencies are also extracted from corpus.", "labels": [], "entities": []}, {"text": "The analysis is executed based on CYK tabular parsing and min-max operation.", "labels": [], "entities": [{"text": "CYK tabular parsing", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7224885125954946}]}, {"text": "By experiments , its accuracy is about 97.29%, which turns out to be very effective.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9997424483299255}]}], "introductionContent": [{"text": "Morphological analysis is crucial for processing the agglutinative language like Korean since words in such languages have lots of morphological variants.", "labels": [], "entities": [{"text": "Morphological analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8743918240070343}]}, {"text": "A sentence is represented by a sequence of eojeols which are the syntactic unit~ delimited by spacing characters in Korean.", "labels": [], "entities": []}, {"text": "Unlike in English, an eojeol is not one word but composed of a series of words (content words and functional words).", "labels": [], "entities": []}, {"text": "In particular, since an eojeol can often contain more than one noun, we cannot get proper interpretation of the sentence or phrase without its accurate segmentation.", "labels": [], "entities": []}, {"text": "The problem in compound noun segmentation is that it is not possible to register all compound nouns in the dictionary since nouns are in the open set of words as well as the number of them is very large.", "labels": [], "entities": [{"text": "compound noun segmentation", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.6542348265647888}]}, {"text": "Thus, they must be treated as unseen words without a segmentation process.", "labels": [], "entities": []}, {"text": "Furthermore, accurate compound noun segmentation plays an important role in the application system.", "labels": [], "entities": [{"text": "accurate compound noun segmentation", "start_pos": 13, "end_pos": 48, "type": "TASK", "confidence": 0.5733759254217148}]}, {"text": "Compound noun segmentation is necessarily required for improving recall and precision in Korean information * This work was supported by a KOSEF's postdoctoral fellowship grant.", "labels": [], "entities": [{"text": "Compound noun segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5844550530115763}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.996624231338501}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9934661984443665}]}, {"text": "retrieval, and obtaining better translation in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.711737334728241}]}, {"text": "For example, suppose that a compound noun 'seol'agsan-gugrib-gongwon(Seol'ag Mountain National Park)' appear in documents.", "labels": [], "entities": []}, {"text": "A user might want to retrieve documents about 'seol'agsan(Seol'ag Mountain)', and then it is likely that the documents with seol'agsan-gugrib-gongwon' are also the ones in his interest.", "labels": [], "entities": []}, {"text": "Therefore, it should be exactly segmented before indexing in order for the documents to be retrieved with the query 'seol'agsan'.", "labels": [], "entities": []}, {"text": "Also, to translate 'seol'agsan-gugribgongwon' to Seol'ag Mountain National Park, the constituents should be identified first through the process of segmentation.", "labels": [], "entities": [{"text": "Seol'ag Mountain National Park", "start_pos": 49, "end_pos": 79, "type": "DATASET", "confidence": 0.6965598240494728}]}, {"text": "This paper presents two methods for segmentation of compound nouns.", "labels": [], "entities": [{"text": "segmentation of compound nouns", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.8997505903244019}]}, {"text": "First, we extract compound nouns from a large size of corpus, manually divide them into simple nouns and construct the hand built segmentation dictionary with them.", "labels": [], "entities": []}, {"text": "The dictionary includes compound nouns which are frequently used and need exceptional process.", "labels": [], "entities": []}, {"text": "The number of data are about 100,000.", "labels": [], "entities": []}, {"text": "Second, the segmentation algorithm is applied if the compound noun does not exist in the built-in dictionary.", "labels": [], "entities": []}, {"text": "Basically, the segmenter is based on frequency of individual nouns extracted from corpus.", "labels": [], "entities": []}, {"text": "However, the problem is that it is difficult to distinguish proper noun and common noun since there is no clue like capital letters in Korean.", "labels": [], "entities": []}, {"text": "Thus, just a large amount of lexical knowledge does not make good results if it contains incorrect data and also it is not appropriate to use frequencies obtained by automatically tagging large corpus.", "labels": [], "entities": []}, {"text": "Moreover, sufficient lexical data cannot be acquired from small amounts of tagged corpus.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method to get simple nouns and their frequencies from frequently occurring eojeols using repetitiveness of natural language.", "labels": [], "entities": []}, {"text": "The amount of eojeols investigated is manually tractable and frequently used nouns extracted from them are crucial for compound noun segmentation.", "labels": [], "entities": [{"text": "compound noun segmentation", "start_pos": 119, "end_pos": 145, "type": "TASK", "confidence": 0.684047927459081}]}, {"text": "Furthermore, we propose rain-max composition to divide a sequence of syllables, which would be proven to bean effective method by experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the test of compound noun segmentation, we first extracted compound noun from ETRI POS tagged corpus 3.", "labels": [], "entities": [{"text": "compound noun segmentation", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.6307227810223898}, {"text": "ETRI POS tagged corpus 3", "start_pos": 82, "end_pos": 106, "type": "DATASET", "confidence": 0.8986452341079711}]}, {"text": "By the processing, 1774 types of compound nouns were extracted, which was used as a gold standard test set.", "labels": [], "entities": []}, {"text": "We evaluated our system by two methods: (1) the precision and recall rate, and (2) segmentation accuracy per compound noun which we refer to as SA.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9996033310890198}, {"text": "recall rate", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9867909252643585}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.8067991137504578}]}, {"text": "They are defined respectively as follows: What influences on the Korean IR system is whether words are appropriately segmented or not.", "labels": [], "entities": [{"text": "Korean IR system", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.6610118945439657}]}, {"text": "The precision and recall estimate how appropriate the segmentation results are.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996821880340576}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.999701201915741}]}, {"text": "They are 98.04% and 97.80% respectively, which shows that our algorithm is very effective.", "labels": [], "entities": []}, {"text": "SA reflects how accurate the segmentation is fora compound noun at all.", "labels": [], "entities": [{"text": "SA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9853079915046692}]}, {"text": "We compared two methods: (1) using only the segmentation algorithm with default analysis which is a baseline of our system and so is needed to estimate the accuracy of the algorithm.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.9618861079216003}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9991996884346008}]}, {"text": "(2) using both the built-in dictionary and the segmentation algorithm which reflects system accuracy as a whole.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.9606583714485168}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9921947717666626}]}, {"text": "As shown in, the baseline performance using only distinct nouns and the algorithm is about 94.3% and fairly good.", "labels": [], "entities": []}, {"text": "From the results, we can find that the distinct nouns has great impact on compound noun segmentation.", "labels": [], "entities": [{"text": "compound noun segmentation", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.6800205806891123}]}, {"text": "Also, the overall segmentation accuracy for the gold standard is about 97.29% which is a very good result for the application system.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.9609640836715698}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.958922803401947}]}, {"text": "In addition, it shows that the built-in dictionary supplements the algorithm which results in better segmentation.", "labels": [], "entities": []}, {"text": "Lastly, we compare our system with the previous work by (.", "labels": [], "entities": []}, {"text": "It is impossible that we directly compare our result with theirs, since the test set is different.", "labels": [], "entities": []}, {"text": "It was reported that the accuracy given in the paper is about 95.6%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.999784529209137}]}, {"text": "When comparing the performance only in terms of the accuracy, our system outperforms theirs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9995273351669312}]}, {"text": "Embeded in the morphological analyzer, the compound noun segmentater is currently being used for some projects on MT and IE which are worked in several institutes and it turns out that the system is very effective.", "labels": [], "entities": [{"text": "MT and IE", "start_pos": 114, "end_pos": 123, "type": "TASK", "confidence": 0.5930508077144623}]}, {"text": "In this paper, we presented the new method for Korean compound noun segmentation.", "labels": [], "entities": [{"text": "Korean compound noun segmentation", "start_pos": 47, "end_pos": 80, "type": "TASK", "confidence": 0.6138532981276512}]}, {"text": "First, we proposed the lexical acquisition for compound noun analysis, which consists of the manually constructed segmentation dictionary (HBSD) and the dictionary for applying the segmentation algorithm (SND).", "labels": [], "entities": [{"text": "compound noun analysis", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.5901568730672201}]}, {"text": "The hand-built segmentation dictionary was made manually for compound nouns extracted from corpus.", "labels": [], "entities": []}, {"text": "The simple noun dictionary is based on very frequently occurring nouns which are called distinct nouns because they are clues for identifying constituents of compound nouns.", "labels": [], "entities": []}, {"text": "Second, the compound noun was segmented based on the modification of CYK tabular parsing and min-max composition, which was proven to be the very effective method by experiments.", "labels": [], "entities": [{"text": "CYK tabular parsing", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7651286323865255}]}, {"text": "The bottom up approach using min-max operation guarantees the most likely segmentation, being applied in the same way as dynamic programming.", "labels": [], "entities": []}, {"text": "With our new method, the result for segmentation is as accurate as 97.29%.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.9885966181755066}, {"text": "accurate", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9791558384895325}]}, {"text": "Especially, the algorithm made results good enough and the builtin dictionary supplemented the algorithm.", "labels": [], "entities": []}, {"text": "Consequently, the methodology is promising and the segmentation system would be helpful for the application system such as machine translation and information retrieval.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.8217644393444061}, {"text": "information retrieval", "start_pos": 147, "end_pos": 168, "type": "TASK", "confidence": 0.7804726362228394}]}], "tableCaptions": [{"text": " Table 3: Result 1: Precision and recall rate", "labels": [], "entities": [{"text": "Precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9938662648200989}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9988349080085754}]}, {"text": " Table 4: Result 2: Segmentation accuracy for Compound Noun", "labels": [], "entities": [{"text": "Result", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9406394958496094}, {"text": "Segmentation", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.9455751180648804}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9488282799720764}]}]}