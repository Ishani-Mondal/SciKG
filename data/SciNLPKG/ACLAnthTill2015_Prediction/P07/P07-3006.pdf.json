{"title": [{"text": "Exploiting Structure for Event Discovery Using the MDI Algorithm", "labels": [], "entities": [{"text": "Event Discovery", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.740590512752533}]}], "abstractContent": [{"text": "Effectively identifying events in unstruc-tured text is a very difficult task.", "labels": [], "entities": [{"text": "identifying events in unstruc-tured text", "start_pos": 12, "end_pos": 52, "type": "TASK", "confidence": 0.8256739974021912}]}, {"text": "This is largely due to the fact that an individual event can be expressed by several sentences.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the use of clustering methods for the task of grouping the text spans in a news article that refer to the same event.", "labels": [], "entities": [{"text": "grouping the text spans in a news article that refer to the same event", "start_pos": 76, "end_pos": 146, "type": "TASK", "confidence": 0.7507768443652562}]}, {"text": "The key idea is to cluster the sentences, using a novel distance metric that exploits regularities in the sequential structure of events within a document.", "labels": [], "entities": []}, {"text": "When this approach is compared to a simple bag of words baseline, a statistically significant increase in performance is observed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Accurately identifying events in unstructured text is an important goal for many applications that require natural language understanding.", "labels": [], "entities": [{"text": "identifying events in unstructured text", "start_pos": 11, "end_pos": 50, "type": "TASK", "confidence": 0.770351254940033}]}, {"text": "There has been an increased focus on this problem in recent years.", "labels": [], "entities": []}, {"text": "The Automatic Content Extraction (ACE) program 1 is dedicated to developing methods that automatically infer meaning from language data.", "labels": [], "entities": [{"text": "Automatic Content Extraction (ACE)", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.7496515562136968}]}, {"text": "Tasks include the detection and characterisation of Entities, Relations, and Events.", "labels": [], "entities": [{"text": "detection and characterisation of Entities, Relations, and Events", "start_pos": 18, "end_pos": 83, "type": "TASK", "confidence": 0.6427862524986268}]}, {"text": "Extensive research has been dedicated to entity recognition and binary relation detection with significant results ().", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.8592731654644012}, {"text": "binary relation detection", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.7012204726537069}]}, {"text": "However, event extraction is still considered as one of the most challenging tasks because an individual event can be expressed by several sentences ().", "labels": [], "entities": [{"text": "event extraction", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7923522591590881}]}, {"text": "In this paper, we primarily focus on techniques for identifying events within a given news article.", "labels": [], "entities": [{"text": "identifying events within a given news article", "start_pos": 52, "end_pos": 98, "type": "TASK", "confidence": 0.7795940807887486}]}, {"text": "Specifically, we describe and evaluate clustering http://www.nist.gov/speech/tests/ace/ methods for the task of grouping sentences in a news article that refer to the same event.", "labels": [], "entities": [{"text": "grouping sentences in a news article that refer to the same event", "start_pos": 112, "end_pos": 177, "type": "TASK", "confidence": 0.8326839506626129}]}, {"text": "We generate sentence clusters using three variations of the welldocumented Hierarchical Agglomerative Clustering (HAC)) as a baseline for this task.", "labels": [], "entities": []}, {"text": "We provide convincing evidence suggesting that inherent structures exist in the manner in which events appear in documents.", "labels": [], "entities": []}, {"text": "In Section 3.1, we present an algorithm which uses such structures during the clustering process and as a result a modest increase inaccuracy is observed.", "labels": [], "entities": []}, {"text": "Developing methods capable of identifying all types of events from free text is challenging for several reasons.", "labels": [], "entities": [{"text": "identifying all types of events from free text", "start_pos": 30, "end_pos": 76, "type": "TASK", "confidence": 0.7144013345241547}]}, {"text": "Firstly, different applications consider different types of events and with different levels of granularity.", "labels": [], "entities": []}, {"text": "A change instate, a horse winning a race and the race meeting itself can be considered as events.", "labels": [], "entities": []}, {"text": "Secondly, interpretation of events can be subjective.", "labels": [], "entities": [{"text": "interpretation of events", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8954539696375529}]}, {"text": "How people understand an event can depend on their knowledge and perspectives.", "labels": [], "entities": []}, {"text": "Therefore in this current work, the type of event to extract is known in advance.", "labels": [], "entities": []}, {"text": "As a detailed case study, we investigate event discovery using a corpus of news articles relating to the recent Iraqi War where the target event is the \"Death\" event type.", "labels": [], "entities": [{"text": "event discovery", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.7932095229625702}]}, {"text": "shows a sample article depicting such events.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organised as follows: We begin with a brief discussion of related work in Section 2.", "labels": [], "entities": []}, {"text": "We describe our approach to Event Discovery in Section 3.", "labels": [], "entities": [{"text": "Event Discovery", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.85919588804245}]}, {"text": "Our techniques are experimentally evaluated in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we conclude with a discussion of experimental observations and opportunities for future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we used a corpus of news articles which is a subset of the Iraq Body Count (IBC): The sequence-based clustering process.", "labels": [], "entities": [{"text": "Iraq Body Count (IBC)", "start_pos": 79, "end_pos": 100, "type": "DATASET", "confidence": 0.7023804783821106}]}, {"text": "dataset 2 . This is an independent public database of media-reported civilian deaths in Iraq resulting directly from military attack by the U.S. forces.", "labels": [], "entities": []}, {"text": "Casualty figures for each event reported are derived solely from a comprehensive manual survey of online media reports from various news sources.", "labels": [], "entities": [{"text": "Casualty", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8918755650520325}]}, {"text": "We obtained a portion of their corpus which consists of 342 new articles from 56 news sources.", "labels": [], "entities": []}, {"text": "The articles are of varying size (average sentence length per document is 25.96).", "labels": [], "entities": []}, {"text": "Most of the articles contain references to multiple events.", "labels": [], "entities": []}, {"text": "The average number of events per document is 5.09.", "labels": [], "entities": []}, {"text": "Excess HTML (image captions etc.) was removed, and sentence boundaries were identified using the Lingua::EN::Sentence perl module available from CPAN 3 . To evaluate our clustering methods, we use the definition of precision and recall proposed by.", "labels": [], "entities": [{"text": "precision", "start_pos": 215, "end_pos": 224, "type": "METRIC", "confidence": 0.9989088773727417}, {"text": "recall", "start_pos": 229, "end_pos": 235, "type": "METRIC", "confidence": 0.9967859983444214}]}, {"text": "We assign each pair of sentences into one of four categories: (i) clustered together (and annotated as referring to the same event); (ii) not clustered together (but annotated as referring to the same event); (iii) incorrectly clustered together; (iv) correctly not clustered together.", "labels": [], "entities": []}, {"text": "Precision and recall are thus found to be computed as P = a a+c and R = a a+b , and F 1 = 2P RP +R . The corpus was annotated by a set often volunteers.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9866386651992798}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9983265995979309}]}, {"text": "Within each article, events were uniquely identified by integers.", "labels": [], "entities": []}, {"text": "These values were then mapped to one of the four label categories, namely \"N\", \"C\", \"X\", and \"B\".", "labels": [], "entities": []}, {"text": "For instance, sentences describing previously unseen events were assigned anew integer.", "labels": [], "entities": []}, {"text": "This value was mapped to the label category \"N\" signifying anew event.", "labels": [], "entities": []}, {"text": "Similarly, sen-tences referring to events in a preceding sentence were assigned the same integer identifier as that assigned to the preceding sentence and mapped to the label category \"C\".", "labels": [], "entities": []}, {"text": "Sentences that referenced an event mentioned earlier in the document but not in the preceding sentence were assigned the same integer identifier as that sentence but mapped to the label category \"B\".", "labels": [], "entities": []}, {"text": "Furthermore, If a sentence did not refer to any event, it was assigned the label 0 and was mapped to the label category \"X\".", "labels": [], "entities": []}, {"text": "Finally, each document was also annotated with the distinct number of events reported in it.", "labels": [], "entities": []}, {"text": "In order to approximate the level of interannotation agreement, two annotators were asked to annotate a disjoint set of 250 documents.", "labels": [], "entities": []}, {"text": "Inter-rater agreements were calculated using the kappa statistic that was first proposed by.", "labels": [], "entities": []}, {"text": "This measure calculates and removes from the agreement rate the amount of agreement expected by chance.", "labels": [], "entities": []}, {"text": "Therefore, the results are more informative than a simple agreement average.", "labels": [], "entities": []}, {"text": "Some extensions were developed including.", "labels": [], "entities": []}, {"text": "In this paper the methodology proposed by was implemented.", "labels": [], "entities": []}, {"text": "Each sentence in the document set was rated by the two annotators and the assigned values were mapped into one of the four label categories (\"N\", \"C\", \"X\", and \"B\").", "labels": [], "entities": []}, {"text": "For complete instructions on how kappa was calculated, we refer the reader to.", "labels": [], "entities": []}, {"text": "Using the annotated data, a kappa score of 0.67 was obtained.", "labels": [], "entities": [{"text": "kappa score", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.968472957611084}]}, {"text": "This indicates that the annotations are somewhat inconsistent, but nonetheless are useful for producing tentative conclusions.", "labels": [], "entities": []}, {"text": "To determine why the annotators were having difficulty agreeing, we calculated the kappa score for each category.", "labels": [], "entities": [{"text": "kappa score", "start_pos": 83, "end_pos": 94, "type": "METRIC", "confidence": 0.8688452541828156}]}, {"text": "For the \"N\", \"C\" and \"X\" categories, reasonable scores of 0.69, 0.71 and 0.72 were obtained respectively.", "labels": [], "entities": []}, {"text": "For the \"B\" category a relatively poor score of 0.52 was achieved indicating that the raters found it difficult to identify sentences that referenced events mentioned earlier in the document.", "labels": [], "entities": []}, {"text": "To illustrate the difficulty of the annotation task an example where the raters disagreed is depicted in.", "labels": [], "entities": []}, {"text": "The raters both agreed when assigning labels to sentence 1 and 2 but disagreed when assigning a label to Sentence 23 . In order to correctly annotate this sentence as referring to the event de-Sentence 1: A suicide attacker set off a bomb that tore through a funeral tent jammed with Shiite mourners Thursday.", "labels": [], "entities": []}], "tableCaptions": []}