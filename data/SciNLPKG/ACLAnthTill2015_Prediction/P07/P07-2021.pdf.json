{"title": [{"text": "Using Error-Correcting Output Codes with Model-Refinement to Boost Centroid Text Classifier", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work, we investigate the use of error-correcting output codes (ECOC) for boosting centroid text classifier.", "labels": [], "entities": [{"text": "boosting centroid text classifier", "start_pos": 81, "end_pos": 114, "type": "TASK", "confidence": 0.5706165954470634}]}, {"text": "The implementation framework is to decompose one multi-class problem into multiple binary problems and then learn the individual binary classification problems by centroid classifier.", "labels": [], "entities": []}, {"text": "However, this kind of decomposition incurs considerable bias for centroid classifier, which results in noticeable degradation of performance for centroid classifier.", "labels": [], "entities": []}, {"text": "In order to address this issue, we use Model-Refinement to adjust this so-called bias.", "labels": [], "entities": []}, {"text": "The basic idea is to take advantage of misclassified examples in the training data to iteratively refine and adjust the centroids of text data.", "labels": [], "entities": []}, {"text": "The experimental results reveal that Model-Refinement can dramatically decrease the bias introduced by ECOC, and the combined classifier is comparable to or even better than SVM classifier in performance.", "labels": [], "entities": [{"text": "ECOC", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.7772650122642517}]}], "introductionContent": [{"text": "In recent years, ECOC has been applied to boost the na\u00efve bayes, decision tree and SVM classifier for text data.", "labels": [], "entities": [{"text": "SVM classifier", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.7838037610054016}]}, {"text": "Following this research direction, in this work, we explore the use of ECOC to enhance the performance of centroid classifier.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, no previous work has been conducted on exactly this problem.", "labels": [], "entities": []}, {"text": "The framework we adopted is to decompose one multi-class problem into multiple binary problems and then use centroid classifier to learn the individual binary classification problems.", "labels": [], "entities": []}, {"text": "However, this kind of decomposition incurs considerable bias ( for centroid classifier.", "labels": [], "entities": []}, {"text": "In substance, centroid classifier) relies on a simple decision rule that a given document should be assigned a particular class if the similarity (or distance) of this document to the centroid of the class is the largest (or smallest).", "labels": [], "entities": [{"text": "centroid classifier)", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.7999397913614908}]}, {"text": "This decision rule is based on a straightforward assumption that the documents in one category should share some similarities with each other.", "labels": [], "entities": []}, {"text": "However, this hypothesis is often violated by ECOC on the grounds that it ignores the similarities of original classes when disassembling one multi-class problem into multiple binary problems.", "labels": [], "entities": [{"text": "ECOC", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9419115781784058}]}, {"text": "In order to attack this problem, we use ModelRefinement () to reduce this socalled bias.", "labels": [], "entities": []}, {"text": "The basic idea is to take advantage of misclassified examples in the training data to iteratively refine and adjust the centroids.", "labels": [], "entities": []}, {"text": "This technique is very flexible, which only needs one classification method and there is no change to the method in anyway.", "labels": [], "entities": []}, {"text": "To examine the performance of proposed method, we conduct an extensive experiment on two commonly used datasets, i.e., Newsgroup and Industry Sector.", "labels": [], "entities": [{"text": "Newsgroup", "start_pos": 119, "end_pos": 128, "type": "DATASET", "confidence": 0.9737905859947205}]}, {"text": "The results indicate that ModelRefinement can dramatically decrease the bias introduce by ECOC, and the resulted classifier is comparable to or even better than SVM classifier in performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiment, we use two corpora: NewsGroup 1 , and Industry Sector 2 . NewsGroup The NewsGroup dataset contains approximately 20,000 articles evenly divided among 20 Usenet newsgroups.", "labels": [], "entities": [{"text": "NewsGroup dataset", "start_pos": 91, "end_pos": 108, "type": "DATASET", "confidence": 0.8591987192630768}]}, {"text": "We use a subset consisting of total categories and 19,446 documents.", "labels": [], "entities": []}, {"text": "To evaluate a text classification system, we use MicroF1 and MacroF1 measures (.", "labels": [], "entities": [{"text": "text classification", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7447544038295746}, {"text": "MicroF1", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.9245960116386414}]}, {"text": "We employ Information Gain as feature selection method because it consistently performs well inmost cases ().", "labels": [], "entities": [{"text": "feature selection", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.6855610013008118}]}, {"text": "We employ TFIDF (Sebastiani 2002) to compute feature weight.", "labels": [], "entities": [{"text": "TFIDF", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9207959175109863}]}, {"text": "For SVM classifier we employ SVMTorch.", "labels": [], "entities": [{"text": "SVMTorch", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9037702083587646}]}, {"text": "(www.idiap.ch/~bengio/projects/SVMTorch.html).", "labels": [], "entities": []}, {"text": "show the performance comparison of different method on two datasets when using 10,000 features.", "labels": [], "entities": []}, {"text": "For ECOC, we use 63-bit BCH coding; for Model-Refinement, we fix its MaxIteration as 8.", "labels": [], "entities": [{"text": "ECOC", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9035419225692749}]}, {"text": "For brevity, we use MR to denote Model-Refinement.", "labels": [], "entities": [{"text": "MR", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.9973264932632446}]}], "tableCaptions": [{"text": " Table 1: The MicroF1 of different methods", "labels": [], "entities": [{"text": "MicroF1", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.9641700387001038}]}, {"text": " Table 2: The MacroF1 of different methods", "labels": [], "entities": []}, {"text": " Table 3: the MicroF1 vs. the length of BCH coding  Bit", "labels": [], "entities": [{"text": "MicroF1", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.8167661428451538}, {"text": "BCH coding", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.7905882596969604}]}, {"text": " Table 4: the MacroF1 vs. the length of BCH coding  Bit", "labels": [], "entities": [{"text": "BCH coding", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8107075989246368}]}]}