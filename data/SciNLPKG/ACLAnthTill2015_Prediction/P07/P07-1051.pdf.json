{"title": [{"text": "Is the End of Supervised Parsing in Sight?", "labels": [], "entities": [{"text": "Supervised Parsing in Sight?", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.7333006680011749}]}], "abstractContent": [{"text": "How far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted?", "labels": [], "entities": []}, {"text": "We present anew algorithm for unsupervised parsing using an all-subtrees model, termed U-DOP*, which parses directly with packed forests of all binary trees.", "labels": [], "entities": []}, {"text": "We train both on Penn's WSJ data and on the (much larger) NANC corpus, showing that U-DOP* outperforms a treebank-PCFG on the standard WSJ test set.", "labels": [], "entities": [{"text": "Penn's WSJ data", "start_pos": 17, "end_pos": 32, "type": "DATASET", "confidence": 0.9300004541873932}, {"text": "NANC corpus", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.9755894541740417}, {"text": "WSJ test set", "start_pos": 135, "end_pos": 147, "type": "DATASET", "confidence": 0.9514190157254537}]}, {"text": "While U-DOP* performs worse than state-of-the-art supervised parsers on hand-annotated sentences, we show that the model outperforms supervised parsers when evaluated as a language model in syntax-based machine translation on Europarl.", "labels": [], "entities": [{"text": "syntax-based machine translation", "start_pos": 190, "end_pos": 222, "type": "TASK", "confidence": 0.7271840771039327}, {"text": "Europarl", "start_pos": 226, "end_pos": 234, "type": "DATASET", "confidence": 0.9904317259788513}]}, {"text": "We argue that supervised parsers miss the fluidity between constituents and non-constituents and that in the field of syntax-based language modeling the end of supervised parsing has come insight.", "labels": [], "entities": []}], "introductionContent": [{"text": "A major challenge in natural language parsing is the unsupervised induction of syntactic structure.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.6298285722732544}, {"text": "syntactic structure", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.6748922765254974}]}, {"text": "While most parsing methods are currently supervised or semi-supervised (, they depend on hand-annotated data which are difficult to come by and which exist only fora few languages.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9685131311416626}]}, {"text": "Unsupervised parsing methods are becoming increasingly important since they operate with raw, unlabeled data of which unlimited quantities are available.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.6737645268440247}]}, {"text": "There has been a resurgence of interest in unsupervised parsing during the last few years.", "labels": [], "entities": []}, {"text": "Where van and induced unlabeled phrase structure for small domains like the ATIS, obtaining around 40% unlabeled f-score, report 71.1% f-score on Penn WSJ part-of-speech strings \u2264 10 words (WSJ10) using a constituentcontext model called CCM.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.9655501246452332}, {"text": "Penn WSJ part-of-speech strings", "start_pos": 146, "end_pos": 177, "type": "DATASET", "confidence": 0.8693647235631943}, {"text": "WSJ10", "start_pos": 190, "end_pos": 195, "type": "DATASET", "confidence": 0.9103516936302185}]}, {"text": "further show that a hybrid approach which combines constituency and dependency models, yields 77.6% f-score on WSJ10.", "labels": [], "entities": [{"text": "f-score", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9938183426856995}, {"text": "WSJ10", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.9663886427879333}]}, {"text": "While Klein and Manning's approach maybe described as an \"all-substrings\" approach to unsupervised parsing, an even richer model consists of an \"all-subtrees\" approach to unsupervised parsing, called U-DOP).", "labels": [], "entities": []}, {"text": "U-DOP initially assigns all unlabeled binary trees to a training set, efficiently stored in a packed forest, and next trains subtrees thereof on a heldout corpus, either by taking their relative frequencies, or by iteratively training the subtree parameters using the EM algorithm (referred to as \"UML-DOP\").", "labels": [], "entities": []}, {"text": "The main advantage of an allsubtrees approach seems to be the direct inclusion of discontiguous context that is not captured by (linear) substrings.", "labels": [], "entities": []}, {"text": "Discontiguous context is important not only for learning structural dependencies but also for learning a variety of noncontiguous constructions such as nearest \u2026 to\u2026 or take \u2026 by surprise.", "labels": [], "entities": []}, {"text": "reports 82.9% unlabeled f-score on the same WSJ10 as used by.", "labels": [], "entities": [{"text": "f-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.8744338154792786}, {"text": "WSJ10", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.9841019511222839}]}, {"text": "Unfortunately, his experiments heavily depend on a priori sampling of subtrees, and the model becomes highly inefficient if larger corpora are used or longer sentences are included.", "labels": [], "entities": []}, {"text": "In this paper we will also test an alternative model for unsupervised all-subtrees parsing, termed U-DOP*, which is based on the DOP* estimator by, and which computes the shortest derivations for sentences from a held-out corpus using all subtrees from all trees from an extraction corpus.", "labels": [], "entities": []}, {"text": "While we do not achieve as high an f-score as the UML-DOP model in, we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in.", "labels": [], "entities": []}, {"text": "We will extend our experiments to 4 million sentences from the NANC corpus, showing that an f-score of 70.7% can be obtained on the standard Penn WSJ test set by means of unsupervised parsing.", "labels": [], "entities": [{"text": "NANC corpus", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.983676016330719}, {"text": "f-score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.978469967842102}, {"text": "Penn WSJ test set", "start_pos": 141, "end_pos": 158, "type": "DATASET", "confidence": 0.9739092141389847}]}, {"text": "Moreover, U-DOP* can be directly put to use in bootstrapping structures for concrete applications such as syntax-based machine translation and speech recognition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7175040990114212}, {"text": "speech recognition", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.7391343712806702}]}, {"text": "We show that U-DOP* outperforms the supervised DOP model if tested on the German-English Europarl corpus in a syntax-based MT system.", "labels": [], "entities": [{"text": "German-English Europarl corpus", "start_pos": 74, "end_pos": 104, "type": "DATASET", "confidence": 0.7249689102172852}]}, {"text": "In the following, we first explain the DOP* estimator and discuss how it can be extended to unsupervised parsing.", "labels": [], "entities": [{"text": "DOP* estimator", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.4016807476679484}]}, {"text": "In section 3, we discuss how a PCFG reduction for supervised DOP can be applied to packed parse forests.", "labels": [], "entities": []}, {"text": "In section 4, we will go into an experimental evaluation of U-DOP* on annotated corpora, while in section 5 we will evaluate U-DOP* on unlabeled corpora in an MT application.", "labels": [], "entities": [{"text": "MT application", "start_pos": 159, "end_pos": 173, "type": "TASK", "confidence": 0.8523519933223724}]}], "datasetContent": [{"text": "To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started outwith three corpora that are also used in) and Bod (2006): Penn's WSJ10 which contains 7422 sentences \u2264 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences \u2264 10 words after removing punctuation.", "labels": [], "entities": [{"text": "Penn's WSJ10", "start_pos": 146, "end_pos": 158, "type": "DATASET", "confidence": 0.8180813789367676}, {"text": "German NEGRA10 corpus", "start_pos": 251, "end_pos": 272, "type": "DATASET", "confidence": 0.8079062700271606}, {"text": "Chinese Treebank CTB10", "start_pos": 281, "end_pos": 303, "type": "DATASET", "confidence": 0.9099481701850891}]}, {"text": "As with most other unsupervised parsing models, we train and test on p-o-s strings rather than on word strings.", "labels": [], "entities": []}, {"text": "The extension to word strings is straightforward as there exist highly accurate unsupervised part-of-speech taggers (e.g.) which can be directly combined with unsupervised parsers, but for the moment we will stick to p-o-s strings (we will comeback to word strings in section 5).", "labels": [], "entities": []}, {"text": "Each corpus was divided into 10 training/test set splits of 90%/10% (n-fold testing), and each training set was randomly divided into two equal parts, that serve as EC and HC and vice versa.", "labels": [], "entities": [{"text": "EC", "start_pos": 165, "end_pos": 167, "type": "METRIC", "confidence": 0.9049111008644104}]}, {"text": "We used the same evaluation metrics for unlabeled precision (UP) and unlabeled recall (UR) as in).", "labels": [], "entities": [{"text": "precision (UP)", "start_pos": 50, "end_pos": 64, "type": "METRIC", "confidence": 0.9249066263437271}, {"text": "recall (UR)", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9628979563713074}]}, {"text": "The two metrics of UP and UR are combined by the unlabeled f-score F1 = 2*UP*UR/(UP+UR).", "labels": [], "entities": [{"text": "UP", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.8207628130912781}, {"text": "UR", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.854581356048584}, {"text": "F1", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.7715063095092773}]}, {"text": "All trees in the test set were binarized beforehand, in the same way as in.", "labels": [], "entities": []}, {"text": "For UML-DOP the decrease in crossentropy became negligible after maximally 18 iterations.", "labels": [], "entities": []}, {"text": "The training for U-DOP* consisted in the computation of the shortest derivations for the HC from which the subtrees and their relative frequencies were extracted.", "labels": [], "entities": []}, {"text": "We used the technique in) to include 'unknown' subtrees.", "labels": [], "entities": []}, {"text": "shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in, the CCM model in, the DMV dependency model in  It should be kept in mind that an exact comparison can only be made between U-DOP* and UML-DOP in table 1, since these two models were tested on 90%/10% splits, while the other models were applied to the full WSJ10, NEGRA10 and CTB10 corpora.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 342, "end_pos": 347, "type": "DATASET", "confidence": 0.965213418006897}, {"text": "NEGRA10 and CTB10 corpora", "start_pos": 349, "end_pos": 374, "type": "DATASET", "confidence": 0.7193336635828018}]}, {"text": "shows that U-DOP* performs worse than UML-DOP in all cases, although the differences are small and was statistically significant only for WSJ10 using paired t-testing.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 138, "end_pos": 143, "type": "DATASET", "confidence": 0.9363477826118469}]}, {"text": "As explained above, the main advantage of U-DOP* over UML-DOP is that it works with a more succinct grammar extracted from the shortest derivations of HC.", "labels": [], "entities": []}, {"text": "shows the size of the grammar (number of rules or subtrees) of the two models for resp.", "labels": [], "entities": []}, {"text": "Penn WSJ10, the entire Penn WSJ and the first 2 million sentences from the NANC (North American News Text) corpus which contains a total of approximately 24 million sentences from different news sources.", "labels": [], "entities": [{"text": "Penn WSJ10", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9623191356658936}, {"text": "Penn WSJ", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.965446263551712}, {"text": "NANC (North American News Text) corpus", "start_pos": 75, "end_pos": 113, "type": "DATASET", "confidence": 0.6525389924645424}]}, {"text": "which was trained on the Penn Treebank such that the same tag set was used.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9935972094535828}]}, {"text": "Next, we added subsets of increasing size from the NANC p-o-s strings to the 40,000 Penn WSJ p-o-s strings.", "labels": [], "entities": [{"text": "NANC p-o-s strings", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.8888282974561056}, {"text": "Penn WSJ p-o-s strings", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.9514916390180588}]}, {"text": "Each time the resulting corpus was split into two halfs and the shortest derivations were computed for one half by using the PCFGreduction from the other half and vice versa.", "labels": [], "entities": [{"text": "PCFGreduction", "start_pos": 125, "end_pos": 138, "type": "DATASET", "confidence": 0.9382875561714172}]}, {"text": "The resulting trees were used for extracting an STSG which in turn was used to parse section 23 of Penn's WSJ.", "labels": [], "entities": [{"text": "section 23 of Penn's WSJ", "start_pos": 85, "end_pos": 109, "type": "DATASET", "confidence": 0.6635286957025528}]}, {"text": "indicates that there is a monotonous increase in f-score on the WSJ test set if NANC text is added to our training data in both cases, independent of whether the sentences come from the WSJ domain or the LA Times domain.", "labels": [], "entities": [{"text": "f-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9693853259086609}, {"text": "WSJ test set", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.9797618786493937}, {"text": "WSJ domain", "start_pos": 186, "end_pos": 196, "type": "DATASET", "confidence": 0.9607280194759369}, {"text": "LA Times domain", "start_pos": 204, "end_pos": 219, "type": "DATASET", "confidence": 0.872934897740682}]}, {"text": "Although the effect of adding LA Times data is weaker than adding WSJ data, it is noteworthy that the unsupervised induction of trees from the LA Times domain still improves the f-score even if the test data are from a different domain.", "labels": [], "entities": [{"text": "LA Times data", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.8976644078890482}, {"text": "WSJ data", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9220280051231384}, {"text": "LA Times domain", "start_pos": 143, "end_pos": 158, "type": "DATASET", "confidence": 0.912666400273641}]}, {"text": "We also investigated the effect of adding the LA Times data to the total mix of Penn's WSJ and NANC's WSJ.", "labels": [], "entities": [{"text": "LA Times data", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.7964907685915629}, {"text": "Penn's WSJ", "start_pos": 80, "end_pos": 90, "type": "DATASET", "confidence": 0.9403247833251953}, {"text": "NANC's WSJ", "start_pos": 95, "end_pos": 105, "type": "DATASET", "confidence": 0.8543255925178528}]}, {"text": "shows the results of this experiment, where the baseline of 0 sentences thus starts with the 2,040k sentences from the combined Penn-NANC WSJ data..", "labels": [], "entities": [{"text": "Penn-NANC WSJ data.", "start_pos": 128, "end_pos": 147, "type": "DATASET", "confidence": 0.9573178688685099}]}, {"text": "Results of U-DOP* on section 23 from Penn's WSJ by mixing sentences from the combined Penn-NANC WSJ with additions from NANC's LA Times.", "labels": [], "entities": [{"text": "Penn's WSJ", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.9398798743883768}, {"text": "Penn-NANC WSJ", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.9723277390003204}, {"text": "NANC's LA Times", "start_pos": 120, "end_pos": 135, "type": "DATASET", "confidence": 0.7569465488195419}]}, {"text": "Our experiments so far have shown that despite the addition of large amounts of unlabeled training data, U-DOP* is still outperformed by the supervised DOP* model when tested on handannotated corpora like the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 209, "end_pos": 222, "type": "DATASET", "confidence": 0.9943415820598602}]}, {"text": "Yet it is well known that any evaluation on hand-annotated corpora unreasonably favors supervised parsers.", "labels": [], "entities": []}, {"text": "There is thus a quest for designing an evaluation scheme that is independent of annotations.", "labels": [], "entities": []}, {"text": "One way to go would be to compare supervised and unsupervised parsers as a syntax-based language model in a practical application such as machine translation (MT) or speech recognition.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 138, "end_pos": 162, "type": "TASK", "confidence": 0.8472876906394958}, {"text": "speech recognition", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.7050379812717438}]}, {"text": "In, we compared U-DOP* and DOP* in a syntax-based MT system known as Data-Oriented Translation or DOT (Poutsma 2000;).", "labels": [], "entities": [{"text": "Data-Oriented Translation or DOT", "start_pos": 69, "end_pos": 101, "type": "TASK", "confidence": 0.7526223957538605}]}, {"text": "The DOT model starts with a bilingual treebank where each tree pair constitutes an example translation and where translationally equivalent constituents are linked.", "labels": [], "entities": []}, {"text": "Similar to DOP, the DOT model uses all linked subtree pairs from the bilingual treebank to form an STSG of linked subtrees, which are used to compute the most probable translation of a target sentence given a source sentence (see).", "labels": [], "entities": []}, {"text": "What we did in Bod is to let both DOP* and U-DOP* compute the best trees directly for the word strings in the German-English Europarl corpus, which contains about 750,000 sentence pairs.", "labels": [], "entities": [{"text": "Bod", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.9647232294082642}, {"text": "DOP", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9333099126815796}, {"text": "Europarl corpus", "start_pos": 125, "end_pos": 140, "type": "DATASET", "confidence": 0.8657066524028778}]}, {"text": "Differently from U-DOP*, DOP* needed to be trained on annotated data, for which we used respectively the Negra and the Penn treebank.", "labels": [], "entities": [{"text": "DOP", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9657204747200012}, {"text": "Negra", "start_pos": 105, "end_pos": 110, "type": "DATASET", "confidence": 0.9839891791343689}, {"text": "Penn treebank", "start_pos": 119, "end_pos": 132, "type": "DATASET", "confidence": 0.9898006916046143}]}, {"text": "Of course, it is well-known that a supervised parser's f-score decreases if it is transferred to another domain: for example, the (non-binarized) WSJ-trained DOP model in Bod (2003) decreases from around 91% to 85.5% fscore if tested on the Brown corpus.", "labels": [], "entities": [{"text": "WSJ-trained DOP model in Bod (2003", "start_pos": 146, "end_pos": 180, "type": "DATASET", "confidence": 0.809660187789372}, {"text": "fscore", "start_pos": 217, "end_pos": 223, "type": "METRIC", "confidence": 0.9971565008163452}, {"text": "Brown corpus", "start_pos": 241, "end_pos": 253, "type": "DATASET", "confidence": 0.9388324618339539}]}, {"text": "Yet, this score is still considerably higher than the accuracy obtained by the unsupervised U-DOP model, which achieves 67.6% unlabeled f-score on Brown sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9994432330131531}]}, {"text": "Our main question of interest is in how far this difference inaccuracy on hand-annotated corpora carries over when tested in the context of a concrete application like MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 168, "end_pos": 170, "type": "TASK", "confidence": 0.8796149492263794}]}, {"text": "This is not a trivial question, since U-DOP* learns 'constituents' for word sequences such as Ich m\u00f6chte (\"I would like to\") and There are, which are usually hand-annotated as non-constituents.", "labels": [], "entities": []}, {"text": "While U-DOP* is punished for this 'incorrect' prediction if evaluated on the Penn Treebank, it maybe rewarded for this prediction if evaluated in the context of machine translation using the Bleu score (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.9964657723903656}, {"text": "machine translation", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.6813415884971619}, {"text": "Bleu score", "start_pos": 191, "end_pos": 201, "type": "METRIC", "confidence": 0.7959533333778381}]}, {"text": "Thus similar to, U-DOP can discover non-syntactic phrases, or simply \"phrases\", which are typically neglected by linguistically syntax-based MT systems.", "labels": [], "entities": []}, {"text": "At the same time, U-DOP* can also learn discontiguous constituents that are neglected by phrase-based MT systems (.", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.8537965416908264}]}, {"text": "In our experiments, we used both U-DOP* and DOP* to predict the best trees for the GermanEnglish Europarl corpus.", "labels": [], "entities": [{"text": "DOP", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9927456974983215}, {"text": "GermanEnglish Europarl corpus", "start_pos": 83, "end_pos": 112, "type": "DATASET", "confidence": 0.9614328940709432}]}, {"text": "Next, we assigned links between each two nodes in the respective trees for each sentence pair.", "labels": [], "entities": []}, {"text": "For a 2,000 sentence test set from a different part of the Europarl corpus we computed the most probable target sentence (using Viterbi n best).", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.9955416917800903}]}, {"text": "The Bleu score was used to measure translation accuracy, calculated by the NIST script with its default settings.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9634655117988586}, {"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9532818794250488}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.8770800232887268}, {"text": "NIST script", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.9552616477012634}]}, {"text": "As a baseline we compared our results with the publicly available phrase-based system Pharaoh (), using the default feature set.", "labels": [], "entities": []}, {"text": "shows for each system the Bleu score together with a description of the productive units.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9768631756305695}]}, {"text": "'U-DOT' refers to 'Unsupervised DOT' based on U-DOP*, while DOT is based on DOP*..", "labels": [], "entities": []}, {"text": "Comparing U-DOP* and DOP* in syntaxbased MT on the German-English Europarl corpus against the Pharaoh system.", "labels": [], "entities": [{"text": "DOP", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9579480290412903}, {"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.8084068894386292}, {"text": "Europarl corpus", "start_pos": 66, "end_pos": 81, "type": "DATASET", "confidence": 0.8741197884082794}]}], "tableCaptions": [{"text": " Table 1. F-scores of U-DOP* and UML-DOP  compared to other models on the same data.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9955247640609741}]}, {"text": " Table 2. Grammar size of U-DOP* and UML-DOP  for WSJ10 (7,7K sentences), WSJ (50K sentences)  and the first 2,000K sentences from NANC.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.873323917388916}, {"text": "WSJ", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.7746431827545166}, {"text": "NANC", "start_pos": 131, "end_pos": 135, "type": "DATASET", "confidence": 0.9777863621711731}]}, {"text": " Table 3. Results of U-DOP* on section 23 from  Penn's WSJ by adding sentences from NANC's  WSJ and NANC's LA Times", "labels": [], "entities": [{"text": "Penn's WSJ", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.9345373709996542}, {"text": "NANC's  WSJ", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.8206627170244852}, {"text": "NANC's LA Times", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.8516343683004379}]}, {"text": " Table 4. Results of U-DOP* on section 23 from  Penn's WSJ by mixing sentences from the  combined Penn-NANC WSJ with additions from  NANC's LA Times.", "labels": [], "entities": [{"text": "Penn's WSJ", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.9460022846857706}, {"text": "Penn-NANC WSJ", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.974494457244873}, {"text": "NANC's LA Times", "start_pos": 133, "end_pos": 148, "type": "DATASET", "confidence": 0.8119064569473267}]}, {"text": " Table 5. Comparison between the (best version of)  U-DOP*, the supervised treebank PCFG and the  supervised DOP* for section 23 of Penn's WSJ", "labels": [], "entities": [{"text": "Penn's WSJ", "start_pos": 132, "end_pos": 142, "type": "DATASET", "confidence": 0.9426714777946472}]}]}