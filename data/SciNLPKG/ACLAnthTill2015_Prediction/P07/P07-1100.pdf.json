{"title": [{"text": "Learning to Compose Effective Strategies from a Library of Dialogue Components", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a method for automatically learning effective dialogue strategies, generated from a library of dialogue content, using reinforcement learning from user feedback.", "labels": [], "entities": []}, {"text": "This library includes greetings, social dialogue, chitchat , jokes and relationship building, as well as the more usual clarification and verification components of dialogue.", "labels": [], "entities": [{"text": "greetings", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.9704060554504395}, {"text": "relationship building", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.7098531723022461}]}, {"text": "We tested the method through a mo-tivational dialogue system that encourages take-up of exercise and show that it can be used to construct good dialogue strategies with little effort.", "labels": [], "entities": []}], "introductionContent": [{"text": "Interactions between humans and machines have become quite common in our daily life.", "labels": [], "entities": []}, {"text": "Many services that used to be performed by humans have been automated by natural language dialogue systems, including information seeking functions, as in timetable or banking applications, but also more complex areas such as tutoring, health coaching and sales where communication is much richer, embedding the provision and gathering of information in e.g. social dialogue.", "labels": [], "entities": []}, {"text": "In the latter category of dialogue systems, a high level of naturalness of interaction and the occurrence of longer periods of satisfactory engagement with the system area prerequisite for task completion and user satisfaction.", "labels": [], "entities": []}, {"text": "Typically, such systems are based on a dialogue strategy that is manually designed by an expert based on knowledge of the system and the domain, and on continuous experimentation with test users.", "labels": [], "entities": []}, {"text": "In this process, the expert has to make many design choices which influence task completion and user satisfaction in a manner which is hard to assess, because the effectiveness of a strategy depends on many different factors, such as classification/ASR performance, the dialogue domain and task, and, perhaps most importantly, personality characteristics and knowledge of the user.", "labels": [], "entities": []}, {"text": "We believe that the key to maximum dialogue effectiveness is to listen to the user.", "labels": [], "entities": []}, {"text": "This paper describes the development of an adaptive dialogue system that uses the feedback of users to automatically improve its strategy.", "labels": [], "entities": []}, {"text": "The system starts with a library of generic and task-/domain-specific dialogue components, including social dialogue, chit-chat, entertaining parts, profiling questions, and informative and diagnostic parts.", "labels": [], "entities": []}, {"text": "Given this variety of possible dialogue actions, the system can follow many different strategies within the dialogue state space.", "labels": [], "entities": []}, {"text": "We conducted training sessions in which users interacted with aversion of the system which randomly generates a possible dialogue strategy for each interaction (restricted by global dialogue constraints).", "labels": [], "entities": []}, {"text": "After each interaction, the users were asked to reward different aspects of the conversation.", "labels": [], "entities": []}, {"text": "We applied reinforcement learning to use this feedback to compute the optimal dialogue policy.", "labels": [], "entities": []}, {"text": "The following section provides a brief overview of previous research related to this area and how our work differs from these studies.", "labels": [], "entities": []}, {"text": "We then proceed with a concise description of the dialogue system used for our experiments in section 3.", "labels": [], "entities": []}, {"text": "Section 4 is about the training process and the reward model.", "labels": [], "entities": []}, {"text": "Section 5 goes into detail about dialogue policy op-792 timization with reinforcement learning.", "labels": [], "entities": []}, {"text": "In section 6 we discuss our experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compared the learned dialogue policy with a policy which was independently hand-designed by experts 2 for this system.", "labels": [], "entities": []}, {"text": "The decisions made in the learned strategy were very similar to the ones made by the experts, with only a few differences, indicating that the automated method would indeed perform as well as an expert.", "labels": [], "entities": []}, {"text": "The main differences were the inclusion of a personal questionnaire for relation building at the beginning of the dialogue and a commitment question at the end of the dialogue.", "labels": [], "entities": [{"text": "relation building", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.8420771360397339}]}, {"text": "Another difference was the more restricted use of the humour element, described in section 6.2 which turns out to be intuitively better than the expert's decision to simply always include a joke.", "labels": [], "entities": []}, {"text": "Of course, we can only draw conclusions with regard to the effectiveness of these two policies if we empirically compare them with real test users.", "labels": [], "entities": []}, {"text": "Such evaluations are planned as part of our future research.", "labels": [], "entities": []}, {"text": "As some additional evidence against the possibility that the learned policy was generated by chance, we performed a simple experiment in which we took several random samples of 300 training dialogues from the complete training set.", "labels": [], "entities": []}, {"text": "For each sample, we learned the optimal policy.", "labels": [], "entities": []}, {"text": "We mutually compared these policies and found that they were very similar: only in 15-20% of the states, the policies disagreed on which action to take next.", "labels": [], "entities": []}, {"text": "On closer inspection we found that this disagreement mainly concerned states that were poorly visited (1-10 times) in these samples.", "labels": [], "entities": []}, {"text": "These results suggest that the learned policy is unreliable at infrequently visited states.", "labels": [], "entities": []}, {"text": "Note however, that all main decisions listed in   made at frequently visited states.", "labels": [], "entities": []}, {"text": "The only disagreement in frequently visited states concerned systemprompt choices.", "labels": [], "entities": []}, {"text": "We might conclude that these particular (often very subtle) system-prompt choices (e.g. careful versus direct formulation of the exercise barrier) are harder to learn than the more noticable dialogue structure-related choices.", "labels": [], "entities": []}], "tableCaptions": []}