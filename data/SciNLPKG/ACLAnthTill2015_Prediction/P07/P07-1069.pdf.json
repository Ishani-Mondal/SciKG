{"title": [], "abstractContent": [{"text": "This paper presents a method for the automatic generation of a table-of-contents.", "labels": [], "entities": []}, {"text": "This type of summary could serve as an effective navigation tool for accessing information in long texts, such as books.", "labels": [], "entities": []}, {"text": "To generate a coherent table-of-contents, we need to capture both global dependencies across different titles in the table and local constraints within sections.", "labels": [], "entities": []}, {"text": "Our algorithm effectively handles these complex dependencies by factoring the model into local and global components, and incrementally constructing the model's output.", "labels": [], "entities": []}, {"text": "The results of automatic evaluation and manual assessment confirm the benefits of this design: our system is consistently ranked higher than non-hierarchical baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current research in summarization focuses on processing short articles, primarily in the news domain.", "labels": [], "entities": [{"text": "summarization", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.9930483102798462}]}, {"text": "While in practice the existing summarization methods are not limited to this material, they are not universal: texts in many domains and genres cannot be summarized using these techniques.", "labels": [], "entities": [{"text": "summarization", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9609512090682983}]}, {"text": "A particularly significant challenge is the summarization of longer texts, such as books.", "labels": [], "entities": [{"text": "summarization of longer texts", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.8460888862609863}]}, {"text": "The requirement for high compression rates and the increased need for the preservation of contextual dependencies between summary sentences places summarization of such texts beyond the scope of current methods.", "labels": [], "entities": [{"text": "summarization", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.9861811995506287}]}, {"text": "In this paper, we investigate the automatic generation of tables-of-contents, a type of indicative summary particularly suited for accessing information in long texts.", "labels": [], "entities": [{"text": "automatic generation of tables-of-contents", "start_pos": 34, "end_pos": 76, "type": "TASK", "confidence": 0.7401948124170303}]}, {"text": "A typical table-of-contents lists topics described in the source text and provides information about their location in the text.", "labels": [], "entities": []}, {"text": "The hierarchical organization of information in the table further refines information access by specifying the relations between different topics and providing rich contextual information during browsing.", "labels": [], "entities": []}, {"text": "Commonly found in books, tables-of-contents can also facilitate access to other types of texts.", "labels": [], "entities": []}, {"text": "For instance, this type of summary could serve as an effective navigation tool for understanding along, unstructured transcript for an academic lecture or a meeting.", "labels": [], "entities": []}, {"text": "Given a text, our goal is to generate a tree wherein anode represents a segment of text and a title that summarizes its content.", "labels": [], "entities": []}, {"text": "This process involves two tasks: the hierarchical segmentation of the text, and the generation of informative titles for each segment.", "labels": [], "entities": []}, {"text": "The first task can be addressed by using the hierarchical structure readily available in the text (e.g., chapters, sections and subsections) or by employing existing topic segmentation algorithms.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.7384391129016876}]}, {"text": "In this paper, we take the former approach.", "labels": [], "entities": []}, {"text": "As for the second task, a naive approach would be to employ existing methods of title generation to each segment, and combine the results into a tree structure.", "labels": [], "entities": [{"text": "title generation", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.7302001416683197}]}, {"text": "However, the latter approach cannot guarantee that the generated table-of-contents forms a coherent representation of the entire text.", "labels": [], "entities": []}, {"text": "Since titles of different segments are generated in isolation, some of the generated titles maybe repetitive.", "labels": [], "entities": []}, {"text": "Even nonrepetitive titles may not provide sufficient information to discriminate between the content of one seg-544", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We apply our method to an undergraduate algorithms textbook.", "labels": [], "entities": []}, {"text": "For detailed statistics on the data see.", "labels": [], "entities": []}, {"text": "We split its of independent subtrees.", "labels": [], "entities": []}, {"text": "Given a table-of-contents of depth n with a root branching factor of r, we generate r subtrees, with a depth of at most n \u2212 1.", "labels": [], "entities": []}, {"text": "We randomly select 80% of these trees for training, and the rest are used for testing.", "labels": [], "entities": []}, {"text": "In our experiments, we use ten different randomizations to compensate for the small number of available trees.", "labels": [], "entities": []}, {"text": "Admittedly, this method of generating training and testing data omits some dependencies at the level of the table-of-contents as a whole.", "labels": [], "entities": []}, {"text": "However, the subtrees used in our experiments still exhibit a sufficiently deep hierarchical structure, rich with contextual dependencies.", "labels": [], "entities": []}, {"text": "Baselines As an alternative to our hierarchical discriminative method, we consider three baselines that build a table-of-contents by generating a title for each segment individually, without taking into account the tree structure, and one hierarchical generative baseline.", "labels": [], "entities": []}, {"text": "The first method generates a title fora segment by selecting the noun phrase from that segment with the highest TF*IDF.", "labels": [], "entities": [{"text": "TF*IDF", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9217636187871298}]}, {"text": "This simple method is commonly used to generate keywords for browsing applications in information retrieval, and has been shown to be effective for summarizing technical content ().", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.7575976550579071}, {"text": "summarizing technical content", "start_pos": 148, "end_pos": 177, "type": "TASK", "confidence": 0.903808077176412}]}, {"text": "The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by.", "labels": [], "entities": [{"text": "noisy-channel generative (flat generative, FG)", "start_pos": 36, "end_pos": 82, "type": "TASK", "confidence": 0.7409619688987732}]}, {"text": "Similar to our local model, this method captures both selection and grammatical constraints.", "labels": [], "entities": []}, {"text": "However, these constraints are modeled separately, and then combined in a generative framework.", "labels": [], "entities": []}, {"text": "We use our local model (Flat Discriminative model, FD) as the third baseline.", "labels": [], "entities": []}, {"text": "Like the second baseline, this model omits global dependencies, and only focuses on features that capture relations within individual segments.", "labels": [], "entities": []}, {"text": "In the hierarchical generative (HG) baseline we run our global model on the ranked list of titles produced for each section by the noisy-channel generative model.", "labels": [], "entities": []}, {"text": "The last three baselines and our algorithm are provided with the title length as a parameter.", "labels": [], "entities": []}, {"text": "In our experiments, the algorithms use the reference title length.", "labels": [], "entities": []}, {"text": "Experimental Design: Comparison with reference tables-of-contents Reference based evaluation is commonly used to assess the quality of machine-generated headlines ().", "labels": [], "entities": []}, {"text": "We compare our system's output with the table-ofcontents from the textbook using ROUGE metrics.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.8431258201599121}]}, {"text": "We employ a publicly available software package, with all the parameters set to default values.", "labels": [], "entities": []}, {"text": "Experimental Design: Human assessment The judges were each given 30 segments randomly selected from a set of 359 test segments.", "labels": [], "entities": []}, {"text": "For each test segment, the judges were presented with its text, and 3 alternative titles consisting of the reference and the titles produced by the hierarchical discriminative model, and the best performing baseline.", "labels": [], "entities": []}, {"text": "In addition, the judges had access to all of the segments in the book.", "labels": [], "entities": []}, {"text": "A total of 498 titles for 166 unique segments were ranked.", "labels": [], "entities": []}, {"text": "The system identities were hidden from the judges, and the titles were presented in random order.", "labels": [], "entities": []}, {"text": "The judges ranked the titles based on how well they represent the content of the segment.", "labels": [], "entities": []}, {"text": "Titles were ranked equal if they were judged to be equally representative of the segment.", "labels": [], "entities": []}, {"text": "Six people participated in this experiment.", "labels": [], "entities": []}, {"text": "All the participants were graduate students in computer science who had taken the algorithms class in the past and were reasonably familiar with the material.", "labels": [], "entities": []}, {"text": "shows fragments of the tables-of-contents generated by our method and the four baselines along with the reference counterpart.", "labels": [], "entities": []}, {"text": "These extracts illustrate three general phenomena that we observed in the test corpus.", "labels": [], "entities": []}, {"text": "First, the titles produced by keyword extraction exhibit a high degree of redundancy.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8347004055976868}]}, {"text": "In fact, 40% of the titles produced by this method are repeated more than once in the: Overall pairwise comparisons of the rankings given by the judges.", "labels": [], "entities": []}, {"text": "The improvement in title quality given by HD over FD is significant at p \u2264 0.0002 based on the Sign test.", "labels": [], "entities": [{"text": "HD", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.886091411113739}, {"text": "FD", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.554725706577301}]}], "tableCaptions": [{"text": " Table 2: Statistics on the corpus used in the experi- ments.", "labels": [], "entities": []}, {"text": " Table 3: Title quality as compared to the reference  for the hierarchical discriminative (HD), flat dis- criminative (FD), hierarchical generative (HG), flat  generative (FG) and Keyword models. The improve- ment given by HD over FD in all three Rouge mea- sures is significant at p \u2264 0.03 based on the Sign  test.", "labels": [], "entities": []}, {"text": " Table 4: Overall pairwise comparisons of the rank- ings given by the judges. The improvement in ti- tle quality given by HD over FD is significant at  p \u2264 0.0002 based on the Sign test.", "labels": [], "entities": [{"text": "ti- tle quality", "start_pos": 97, "end_pos": 112, "type": "METRIC", "confidence": 0.7054861485958099}, {"text": "HD", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9256760478019714}, {"text": "FD", "start_pos": 130, "end_pos": 132, "type": "METRIC", "confidence": 0.8785608410835266}]}]}