{"title": [{"text": "Instance-based Evaluation of Entailment Rule Acquisition", "labels": [], "entities": [{"text": "Instance-based Evaluation of Entailment Rule Acquisition", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.8699743648370107}]}], "abstractContent": [{"text": "Obtaining large volumes of inference knowledge , such as entailment rules, has become a major factor in achieving robust semantic processing.", "labels": [], "entities": []}, {"text": "While there has been substantial research on learning algorithms for such knowledge, their evaluation methodology has been problematic, hindering further research.", "labels": [], "entities": []}, {"text": "We propose a novel evaluation methodology for entailment rules which explicitly addresses their semantic properties and yields satisfactory human agreement levels.", "labels": [], "entities": []}, {"text": "The methodology is used to compare two state of the art learning algorithms, exposing critical issues for future progress.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many NLP applications, such as Question Answering (QA) and Information Extraction (IE), it is crucial to recognize that a particular target meaning can be inferred from different text variants.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.8816398978233337}, {"text": "Information Extraction (IE)", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.8597231388092041}]}, {"text": "For example, a QA system needs to identify that \"Aspirin lowers the risk of heart attacks\" can be inferred from \"Aspirin prevents heart attacks\" in order to answer the question \"What lowers the risk of heart attacks?\".", "labels": [], "entities": []}, {"text": "This type of reasoning has been recognized as a core semantic inference task by the generic textual entailment framework ( ).", "labels": [], "entities": []}, {"text": "A major obstacle for further progress in semantic inference is the lack of broad-scale knowledgebases for semantic variability patterns ().", "labels": [], "entities": [{"text": "semantic inference", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9075374007225037}]}, {"text": "One prominent type of inference knowledge representation is inference rules such as paraphrases and entailment rules.", "labels": [], "entities": []}, {"text": "We define an entailment rule to be a directional relation between two templates, text patterns with variables, e.g. 'X prevent Y \u2192 X lower the risk of Y '.", "labels": [], "entities": []}, {"text": "The left-handside template is assumed to entail the right-handside template in certain contexts, under the same variable instantiation.", "labels": [], "entities": []}, {"text": "Paraphrases can be viewed as bidirectional entailment rules.", "labels": [], "entities": []}, {"text": "Such rules capture basic inferences and are used as building blocks for more complex entailment inference.", "labels": [], "entities": []}, {"text": "For example, given the above rule, the answer \"Aspirin\" can be identified in the example above.", "labels": [], "entities": []}, {"text": "The need for large-scale inference knowledgebases triggered extensive research on automatic acquisition of paraphrase and entailment rules.", "labels": [], "entities": [{"text": "automatic acquisition of paraphrase and entailment rules", "start_pos": 82, "end_pos": 138, "type": "TASK", "confidence": 0.7336835605757577}]}, {"text": "Yet the current precision of acquisition algorithms is typically still mediocre, as illustrated in for DIRT () and TEASE), two prominent acquisition algorithms whose outputs are publicly available.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9989284873008728}, {"text": "TEASE", "start_pos": 115, "end_pos": 120, "type": "METRIC", "confidence": 0.9843586683273315}]}, {"text": "The current performance level only stresses the obvious need for satisfactory evaluation methodologies that would drive future research.", "labels": [], "entities": []}, {"text": "The prominent approach in the literature for evaluating rules, termed here the rule-based approach, is to present the rules to human judges asking whether each rule is corrector not.", "labels": [], "entities": []}, {"text": "However, it is difficult to explicitly define when a learned rule should be considered correct under this methodology, and this was mainly left undefined in previous works.", "labels": [], "entities": []}, {"text": "As the criterion for evaluating a rule is not well defined, using this approach often caused low agreement between human judges.", "labels": [], "entities": []}, {"text": "Indeed, the standards for evaluation in this field are lower than other fields: many papers 456 don't report on human agreement at all and those that do report rather low agreement levels.", "labels": [], "entities": []}, {"text": "Yet it is crucial to reliably assess rule correctness in order to measure and compare the performance of different algorithms in a replicable manner.", "labels": [], "entities": [{"text": "rule correctness", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.759911447763443}]}, {"text": "Lacking a good evaluation methodology has become a barrier for further advances in the field.", "labels": [], "entities": []}, {"text": "In order to provide a well-defined evaluation methodology we first explicitly specify when entailment rules should be considered correct, following the spirit of their usage in applications.", "labels": [], "entities": []}, {"text": "We then propose anew instance-based evaluation approach.", "labels": [], "entities": []}, {"text": "Under this scheme, judges are not presented only with the rule but rather with a sample of sentences that match its left hand side.", "labels": [], "entities": []}, {"text": "The judges then assess whether the rule holds under each specific example.", "labels": [], "entities": []}, {"text": "A rule is considered correct only if the percentage of examples assessed as correct is sufficiently high.", "labels": [], "entities": []}, {"text": "We have experimented with a sample of input verbs for both DIRT and TEASE.", "labels": [], "entities": [{"text": "TEASE", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.8412210941314697}]}, {"text": "Our results show significant improvement inhuman agreement over the rule-based approach.", "labels": [], "entities": [{"text": "agreement", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.984263002872467}]}, {"text": "It is also the first comparison between such two state-of-the-art algorithms, which showed that they are comparable in precision but largely complementary in their coverage.", "labels": [], "entities": [{"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9984503984451294}]}, {"text": "Additionally, the evaluation showed that both algorithms learn mostly one-directional rules rather than (symmetric) paraphrases.", "labels": [], "entities": []}, {"text": "While most NLP applications need directional inference, previous acquisition works typically expected that the learned rules would be paraphrases.", "labels": [], "entities": []}, {"text": "Under such an expectation, unidirectional rules were assessed as incorrect, underestimating the true potential of these algorithms.", "labels": [], "entities": []}, {"text": "In addition, we observed that many learned rules are context sensitive, stressing the need to learn contextual constraints for rule applications.", "labels": [], "entities": []}], "datasetContent": [{"text": "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (;).", "labels": [], "entities": [{"text": "automatic acquisition of rules", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.7218788415193558}]}, {"text": "However, there is still no common accepted framework for their evaluation.", "labels": [], "entities": []}, {"text": "Furthermore, all these methods learn rules as pairs of templates {L, R} in asymmetric manner, without addressing rule directionality.", "labels": [], "entities": []}, {"text": "Accordingly, previous works (except)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1).", "labels": [], "entities": []}, {"text": "One approach which was used for evaluating automatically acquired rules is to measure their contribution to the performance of specific systems, such as QA () or IE ().", "labels": [], "entities": []}, {"text": "While measuring the impact of learned rules on applications is highly important, it cannot serve as the primary approach for evaluating acquisition algorithms for several reasons.", "labels": [], "entities": []}, {"text": "First, developers of acquisition algorithms often do not have access to the different applications that will later use the learned rules as generic modules.", "labels": [], "entities": []}, {"text": "Second, the learned rules may affect individual systems differently, thus making observations that are based on different systems incomparable.", "labels": [], "entities": []}, {"text": "Third, within a complex system it is difficult to assess the exact quality of entailment rules independently of effects of other system components.", "labels": [], "entities": []}, {"text": "Thus, as in many other NLP learning settings, a direct evaluation is needed.", "labels": [], "entities": []}, {"text": "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (;.", "labels": [], "entities": [{"text": "rule acquisition algorithms", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.8143501480420431}]}, {"text": "In this evaluation scheme, termed here the rule-based approach, a sample of the learned rules is presented to the judges who evaluate whether each rule is corrector not.", "labels": [], "entities": []}, {"text": "The criterion for correctness is not explicitly described inmost previous works.", "labels": [], "entities": [{"text": "correctness", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9385789036750793}]}, {"text": "By the common view of context relevance for rules (see Section 2.1), a rule was considered correct if the judge could think of reasonable contexts under which it holds.", "labels": [], "entities": []}, {"text": "We have replicated the rule-based methodology but did not manage to reach a 0.6 Kappa agreement level between pairs of judges.", "labels": [], "entities": [{"text": "Kappa agreement level", "start_pos": 80, "end_pos": 101, "type": "METRIC", "confidence": 0.6863548358281454}]}, {"text": "This approach turns out to be problematic because the rule correctness criterion is not sufficiently well defined and is hard to apply.", "labels": [], "entities": [{"text": "rule correctness", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7663610875606537}]}, {"text": "While some rules might obviously be judged as corrector incorrect (see), judgment is often more difficult due to context relevance.", "labels": [], "entities": []}, {"text": "One judge might come up with a certain context that, to her opinion, justifies the rule, while another judge might not imagine that context or think that it doesn't sufficiently support rule correctness.", "labels": [], "entities": []}, {"text": "For example, in our experiments one of the judges did not identify the valid \"religious holidays\" context for the correct rule 'X observe Y \u2192 X celebrate Y '.", "labels": [], "entities": []}, {"text": "Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 () and 0.55 -0.63 ().", "labels": [], "entities": [{"text": "agreement level", "start_pos": 52, "end_pos": 67, "type": "METRIC", "confidence": 0.8184452056884766}, {"text": "Kappa", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.9539802670478821}]}, {"text": "To conclude, the prominent rule-based methodology for entailment rule evaluation is not sufficiently well defined.", "labels": [], "entities": [{"text": "entailment rule evaluation", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.8517571687698364}]}, {"text": "It results in low inter-judge agreement which prevents reliable and consistent assessments of different algorithms.", "labels": [], "entities": []}, {"text": "As discussed in Section 2.1, an evaluation methodology for entailment rules should reflect the expected validity of their application within NLP systems.", "labels": [], "entities": []}, {"text": "Following that line, an entailment rule 'L \u2192 R' should be regarded as correct if in all (or at least most) relevant contexts in which the instantiated template L is inferred from the given text, the instan  tiated template R is also inferred from the text.", "labels": [], "entities": []}, {"text": "This reasoning corresponds to the common definition of entailment in semantics, which specifies that a text L entails another text R if R is true in every circumstance (possible world) in which L is true).", "labels": [], "entities": []}, {"text": "It follows that in order to assess if a rule is correct we should judge whether R is typically entailed from those sentences that entail L (within relevant contexts for the rule).", "labels": [], "entities": []}, {"text": "We thus present anew evaluation scheme for entailment rules, termed the instance-based approach.", "labels": [], "entities": []}, {"text": "At the heart of this approach, human judges are presented not only with a rule but rather with a sample of examples of the rule's usage.", "labels": [], "entities": []}, {"text": "Instead of thinking up valid contexts for the rule the judges need to assess the rule's validity under the given context in each example.", "labels": [], "entities": []}, {"text": "The essence of our proposal is a (apparently non-trivial) protocol of a sequence of questions, which determines rule validity in a given sentence.", "labels": [], "entities": []}, {"text": "We shall next describe how we collect a sample of examples for evaluation and the evaluation process.", "labels": [], "entities": []}, {"text": "For each example, the judges are presented with the three questions above in the following order: (1) Q le (2) Q rc (3) Q re . If the answer to a certain question is negative then we do not need to present the next questions to the judge: if the left phrase is not entailed then we ignore the sentence altogether; and if the context is irrelevant then the right phrase cannot be entailed from the sentence and so the answer to Q re is already known as negative.", "labels": [], "entities": []}, {"text": "The above entailment judgments assume that we can actually ask whether the left or right phrases are correct given the sentence, that is, we assume that a truth value can be assigned to both phrases.", "labels": [], "entities": []}, {"text": "This is the case when the left and right templates correspond, as expected, to semantic relations.", "labels": [], "entities": []}, {"text": "Yet sometimes learned templates are (erroneously) not relational, e.g. 'X, Y , IBM' (representing a list).", "labels": [], "entities": []}, {"text": "We therefore let the judges initially mark rules that include such templates as non-relational, in which case their examples are not evaluated at all.", "labels": [], "entities": []}, {"text": "We applied the instance-based methodology to evaluate two state-of-the-art unsupervised acquisition algorithms, DIRT) and TEASE (), whose output is publicly available.", "labels": [], "entities": [{"text": "TEASE", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.9901290535926819}]}, {"text": "DIRT identifies semantically related templates in a local corpus using distributional similarity over the templates' variable instantiations.", "labels": [], "entities": []}, {"text": "TEASE acquires entailment relations from the Web fora given input template I by identifying characteristic variable instantiations shared by I and other templates.", "labels": [], "entities": []}, {"text": "460 For the experiment we used the published DIRT and TEASE knowledge-bases . For every given input template I, each knowledge-base provides a list of learned output templates {O j } n I 1 , where n I is the number of output templates learned for I.", "labels": [], "entities": [{"text": "DIRT", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.7227920889854431}, {"text": "TEASE", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.7556038498878479}]}, {"text": "Each output template is suggested as holding an entailment relation with the input template I, but the algorithms do not specify the entailment direction(s).", "labels": [], "entities": []}, {"text": "Thus, each pair {I, O j } induces two candidate directional entailment rules: 'I \u2192 O j ' and 'O j \u2192 I'.", "labels": [], "entities": []}, {"text": "We assessed the instance-based methodology by measuring the agreement level between judges.", "labels": [], "entities": []}, {"text": "The judges agreed on 75% of the 1287 shared examples, corresponding to a reasonable Kappa value of 0.64.", "labels": [], "entities": []}, {"text": "A similar kappa value of 0.65 was obtained for the examples that were judged as either entailment holds/no entailment by both judges.", "labels": [], "entities": [{"text": "kappa", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9379154443740845}]}, {"text": "Yet, our evaluation target is to assess rules, and the Kappa values for the final correctness judgments of the shared rules were 0.74 and 0.68 for the lower and upper bound evaluations.", "labels": [], "entities": []}, {"text": "These Kappa scores are regarded as 'substantial agreement' and are substantially higher than published agreement scores and those we managed to obtain using the standard rulebased approach.", "labels": [], "entities": []}, {"text": "As expected, the agreement on rules is higher than on examples, since judges may disagree on a certain example but their judgements would still yield the same rule assessment.", "labels": [], "entities": [{"text": "agreement", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9670217633247375}]}, {"text": "illustrates some disagreements that were still exhibited within the instance-based evaluation.", "labels": [], "entities": []}, {"text": "The primary reason for disagreements was the difficulty to decide whether a context is relevant fora rule or not, resulting in some confusion between 'Irrelevant context' and 'No entailment'.", "labels": [], "entities": []}, {"text": "This may explain the lower agreement for the upper bound precision, for which examples judged as 'Irrelevant context' are ignored, while for the lower bound both: Examples for disagreement between the two judges.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9831269979476929}]}, {"text": "judgments are conflated and represent no entailment.", "labels": [], "entities": []}, {"text": "Our findings suggest that better ways for distinguishing relevant contexts maybe sought in future research for further refinement of the instance-based evaluation methodology.", "labels": [], "entities": []}, {"text": "About 43% of all examples were judged as 'Left not entailed'.", "labels": [], "entities": []}, {"text": "The relatively low matching precision (57%) made us collect more examples than needed, since 'Left not entailed' examples are ignored.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.8306081295013428}]}, {"text": "Better matching capabilities will allow collecting and judging fewer examples, thus improving the efficiency of the evaluation process.", "labels": [], "entities": []}, {"text": "We evaluated the quality of the entailment rules produced by each algorithm using two scores: (1) micro average Precision, the percentage of correct rules out of all learned rules, and (2) average Yield, the average number of correct rules learned for each input template I, as extrapolated based on the sample . Since DIRT and TEASE do not identify rule directionality, we also measured these scores at the template level, where an output template O is considered correct if at least one of the rules 'I \u2192 O' or 'O \u2192 I' is correct.", "labels": [], "entities": [{"text": "Precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.7527380585670471}, {"text": "Yield", "start_pos": 197, "end_pos": 202, "type": "METRIC", "confidence": 0.49806931614875793}, {"text": "TEASE", "start_pos": 328, "end_pos": 333, "type": "METRIC", "confidence": 0.874037504196167}]}, {"text": "The results are presented in Table 4.", "labels": [], "entities": []}, {"text": "The major finding is that the overall quality of DIRT and TEASE is very similar.", "labels": [], "entities": [{"text": "TEASE", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.8538455963134766}]}, {"text": "Under the specific DIRT cutoff threshold chosen, DIRT exhibits somewhat higher Precision while TEASE has somewhat higher Yield (recall that there is no particular natural cutoff point for DIRT's output).", "labels": [], "entities": [{"text": "Precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.999556839466095}, {"text": "TEASE", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9871261715888977}, {"text": "Yield", "start_pos": 121, "end_pos": 126, "type": "METRIC", "confidence": 0.9985779523849487}]}], "tableCaptions": [{"text": " Table 4: Average Precision (P) and Yield (Y) at the  rule and template levels.", "labels": [], "entities": [{"text": "Average Precision (P)", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9034298419952392}, {"text": "Yield (Y)", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9736375063657761}]}]}