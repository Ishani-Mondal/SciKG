{"title": [{"text": "An Approximate Approach for Training Polynomial Kernel SVMs in Linear Time", "labels": [], "entities": []}], "abstractContent": [{"text": "Kernel methods such as support vector machines (SVMs) have attracted a great deal of popularity in the machine learning and natural language processing (NLP) communities.", "labels": [], "entities": []}, {"text": "Polynomial kernel SVMs showed very competitive accuracy in many NLP problems, like part-of-speech tagging and chunking.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9981281161308289}, {"text": "part-of-speech tagging", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.6984599530696869}]}, {"text": "However, these methods are usually too inefficient to be applied to large dataset and real time purpose.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approximate method to analogy polynomial kernel with efficient data mining approaches.", "labels": [], "entities": []}, {"text": "To prevent exponential scaled testing time complexity, we also present anew method for speeding up SVM classifying which does independent to the polynomial degree d.", "labels": [], "entities": [{"text": "SVM classifying", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.9091165959835052}]}, {"text": "The experimental results showed that our method is 16.94 and 450 times faster than traditional polynomial kernel in terms of training and testing respectively.", "labels": [], "entities": []}], "introductionContent": [{"text": "Kernel methods, for example support vector machines (SVM) are successfully applied to many natural language processing (NLP) problems.", "labels": [], "entities": []}, {"text": "They yielded very competitive and satisfactory performance in many classification tasks, such as part-of-speech (POS) tagging (, shallow parsing (, named entity recognition (), and parsing ().", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 97, "end_pos": 125, "type": "TASK", "confidence": 0.6163721144199371}, {"text": "shallow parsing", "start_pos": 129, "end_pos": 144, "type": "TASK", "confidence": 0.6105155050754547}, {"text": "named entity recognition", "start_pos": 148, "end_pos": 172, "type": "TASK", "confidence": 0.6072299281756083}, {"text": "parsing", "start_pos": 181, "end_pos": 188, "type": "TASK", "confidence": 0.9791784286499023}]}, {"text": "In particular, the use of polynomial kernel SVM implicitly takes the feature combinations into account instead of explicitly combines features.", "labels": [], "entities": []}, {"text": "By setting with polynomial kernel degree (i.e., d), different number of feature conjunctions can be implicitly computed.", "labels": [], "entities": []}, {"text": "In this way, polynomial kernel SVM is often better than linear kernel which did not use feature conjunctions.", "labels": [], "entities": []}, {"text": "However, the training and testing time costs for polynomial kernel SVM is far slow than the linear kernel.", "labels": [], "entities": []}, {"text": "For example, it took one day to train the CoNLL-2000 task with polynomial kernel SVM, while the testing speed is merely 20-30 words per second ().", "labels": [], "entities": []}, {"text": "Although the author provided the solution for fast classifying with polynomial kernel (), the training time is still inefficient.", "labels": [], "entities": []}, {"text": "Nevertheless, the testing time of their method exponentially scales with polynomial kernel degree d, i.e., O(|X| d ) where |X| denotes as the length of example X.", "labels": [], "entities": [{"text": "O", "start_pos": 107, "end_pos": 108, "type": "METRIC", "confidence": 0.9934349060058594}]}, {"text": "On the contrary, even the linear kernel SVM simply disregards the effect of feature combinations during training and testing, it performs not only more efficient than polynomial kernel, but also can be improved through directly appending features derived from the set of feature combinations.", "labels": [], "entities": []}, {"text": "Examples include bigram, trigram, etc.", "labels": [], "entities": []}, {"text": "Nevertheless, selecting the feature conjunctions was manually and heuristically encoded and should perform amount of validation trials to discover which is useful or not.", "labels": [], "entities": []}, {"text": "In recent years, several studies had reported that the training time of linear kernel SVM can be reduced to linear time).", "labels": [], "entities": []}, {"text": "But they did not and difficult to be extent to polynomial kernels.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approximate approach to extend the linear kernel SVM toward polynomial.", "labels": [], "entities": []}, {"text": "By introducing the well-known sequential pattern mining approach ( ), frequent feature conjunctions, namely patterns could be discovered and also kept as expand feature space.", "labels": [], "entities": [{"text": "sequential pattern mining", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.6352827052275339}]}, {"text": "We then adopt the mined patterns to rerepresent the training/testing examples.", "labels": [], "entities": []}, {"text": "Subsequently, we use the off-the-shelf linear kernel SVM algorithm to perform training and testing.", "labels": [], "entities": []}, {"text": "Besides, to exponential-scaled testing time complexity, we propose anew classification method for speeding up the SVM testing.", "labels": [], "entities": [{"text": "SVM", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.9373788237571716}]}, {"text": "Rather than enumerating all patterns for each example, our method requires O(F avg *N avg ) which is independent to the polynomial kernel degree.", "labels": [], "entities": [{"text": "F avg *N avg )", "start_pos": 77, "end_pos": 91, "type": "METRIC", "confidence": 0.890658309062322}]}, {"text": "F avg is the average number of frequent features per example, while the N avg is the average number of patterns per feature.", "labels": [], "entities": [{"text": "F avg", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.943300873041153}]}], "datasetContent": [{"text": "To evaluate our method, we examine the wellknown shallow parsing task which is the task of CoNLL-2000 1 . We also adopted the released perlevaluator to measure the recall/precision/f1 rates.", "labels": [], "entities": [{"text": "shallow parsing", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.5475077629089355}, {"text": "recall/precision/f1 rates", "start_pos": 164, "end_pos": 189, "type": "METRIC", "confidence": 0.832452783981959}]}, {"text": "The used feature consists of word, POS, orthographic, affix(2-4 prefix/suffix letters), and previous chunk tags in the two words context window size (the same as ().", "labels": [], "entities": []}, {"text": "We limited the features should at least appear more than twice in the training set.", "labels": [], "entities": []}, {"text": "For the learning algorithm, we replicate the modified finite Newton SVM as learner which can be trained in linear time ().", "labels": [], "entities": []}, {"text": "We also compare our method with the standard linear and polynomial kernels with SVM light 2 . lists the experimental results on the CoNLL-2000 shallow parsing task.", "labels": [], "entities": [{"text": "CoNLL-2000 shallow parsing task", "start_pos": 132, "end_pos": 163, "type": "TASK", "confidence": 0.761901319026947}]}, {"text": "compares the testing speed of different feature expansion techniques, namely, array visiting (our method) and enumeration.", "labels": [], "entities": []}, {"text": "It is not surprising that the best performance was obtained by the classical polynomial kernel.", "labels": [], "entities": []}, {"text": "But the limitation is that the slow in training and testing time costs.", "labels": [], "entities": []}, {"text": "The most efficient method is linear kernel SVM but it does not as accurate as polynomial kernel.", "labels": [], "entities": []}, {"text": "However, our method stands for both efficiency and accuracy in this experiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9995112419128418}]}, {"text": "In terms of training time, it slightly slower than the linear kernel, while it is 16.94 and ~450 times faster than polynomial kernel in training and test-ing.", "labels": [], "entities": []}, {"text": "Besides, the pattern mining time is far smaller than SVM training.", "labels": [], "entities": [{"text": "pattern mining", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.9071066677570343}, {"text": "SVM", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9276642203330994}]}], "tableCaptions": [{"text": " Table 2: Experimental results for CoNLL-2000 shal- low parsing task", "labels": [], "entities": [{"text": "CoNLL-2000 shal- low parsing", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.541995745897293}]}, {"text": " Table 3: Classification time performance of enu- meration and array visiting techniques", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9534831047058105}, {"text": "array visiting", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.7347189486026764}]}]}