{"title": [], "abstractContent": [{"text": "Historically, unsupervised learning techniques have lacked a principled technique for selecting the number of unseen components.", "labels": [], "entities": []}, {"text": "Research into non-parametric priors, such as the Dirichlet process, has enabled instead the use of infinite models, in which the number of hidden categories is not fixed, but can grow with the amount of training data.", "labels": [], "entities": []}, {"text": "Here we develop the infinite tree, anew infinite model capable of representing recursive branching structure over an arbitrarily large set of hidden categories.", "labels": [], "entities": []}, {"text": "Specifically, we develop three infinite tree models, each of which enforces different independence assumptions , and for each model we define a simple direct assignment sampling inference procedure.", "labels": [], "entities": []}, {"text": "We demonstrate the utility of our models by doing unsupervised learning of part-of-speech tags from treebank dependency skeleton structure, achieving an accuracy of 75.34%, and by doing unsupervised splitting of part-of-speech tags, which increases the accuracy of a generative dependency parser from 85.11% to 87.35%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9989757537841797}, {"text": "accuracy", "start_pos": 253, "end_pos": 261, "type": "METRIC", "confidence": 0.998783528804779}, {"text": "generative dependency parser", "start_pos": 267, "end_pos": 295, "type": "TASK", "confidence": 0.8661514520645142}]}], "introductionContent": [{"text": "Model-based unsupervised learning techniques have historically lacked good methods for choosing the number of unseen components.", "labels": [], "entities": []}, {"text": "For example, kmeans or EM clustering require advance specification of the number of mixture components.", "labels": [], "entities": [{"text": "EM clustering", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.7576135396957397}]}, {"text": "But the introduction of nonparametric priors such as the Dirichlet process enabled development of infinite mixture models, in which the number of hidden components is not fixed, but emerges naturally from the training data.", "labels": [], "entities": []}, {"text": "proposed the hierarchical Dirichlet process (HDP) as away of applying the Dirichlet process (DP) to more complex model forms, so as to allow multiple, group-specific, infinite mixture models to share their mixture components.", "labels": [], "entities": []}, {"text": "The closely related infinite hidden Markov model is an HMM in which the transitions are modeled using an HDP, enabling unsupervised learning of sequence models when the number of hidden states is unknown ().", "labels": [], "entities": []}, {"text": "We extend this work by introducing the infinite tree model, which represents recursive branching structure over a potentially infinite set of hidden states.", "labels": [], "entities": []}, {"text": "Such models are appropriate for the syntactic dependency structure of natural language.", "labels": [], "entities": []}, {"text": "The hidden states represent word categories (\"tags\"), the observations they generate represent the words themselves, and the tree structure represents syntactic dependencies between pairs of tags.", "labels": [], "entities": []}, {"text": "To validate the model, we test unsupervised learning of tags conditioned on a given dependency tree structure.", "labels": [], "entities": []}, {"text": "This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufficient distinctions to be the basis of accurate syntactic parsing.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 87, "end_pos": 106, "type": "DATASET", "confidence": 0.977243447303772}, {"text": "syntactic parsing", "start_pos": 167, "end_pos": 184, "type": "TASK", "confidence": 0.7403099536895752}]}, {"text": "Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves), manually split the tagset into a finer-grained one, or learn finer grained tag distinctions using a heuristic learning procedure ().", "labels": [], "entities": []}, {"text": "We demonstrate that the tags learned with our model are correlated with the PTB POS tags, and furthermore that they improve the accuracy of an automatic parser when used in training.", "labels": [], "entities": [{"text": "PTB POS tags", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.6357157727082571}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9989206790924072}]}], "datasetContent": [{"text": "We demonstrate infinite tree models on two distinct syntax learning tasks: unsupervised POS learning conditioned on untagged dependency trees and learning a split of an existing tagset, which improves the accuracy of an automatic syntactic parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.9981697797775269}]}, {"text": "For both tasks, we use a simple modification of the basic model structure, to allow the trees to generate dependents on the left and the right with different distributions -as is useful in modeling natural language.", "labels": [], "entities": []}, {"text": "The modification of the independent child tree is trivial: we have two copies of each of 277 the variables \u03c0 k , one each for the left and the right.", "labels": [], "entities": []}, {"text": "Generation of dependents on the right is completely independent of that for the left.", "labels": [], "entities": []}, {"text": "The modifications of the other models are similar, but now there are separate sets of \u03c0 k variables for the Markov child model, and separate L k and \u03bb k variables for the simultaneous child model, for each of the left and right.", "labels": [], "entities": []}, {"text": "For both experiments, we used dependency trees extracted from the Penn Treebank () using the head rules and dependency extractor from.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9966892898082733}]}, {"text": "As is standard, we used WSJ sections 2-21 for training, section 22 for development, and section 23 for testing.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.6713373064994812}]}], "tableCaptions": [{"text": " Table 1: Results of part unsupervised POS tagging  on the different models, using a greedy accuracy  measure.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.7874449193477631}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9036118984222412}]}, {"text": " Table 2: Results of untyped, directed dependency  parsing, where the POS tags in the training data have  been split according to the various models. At test  time, the POS tagging and parsing are done simulta- neously by the parser.", "labels": [], "entities": [{"text": "untyped, directed dependency  parsing", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.7378693342208862}, {"text": "POS tagging", "start_pos": 169, "end_pos": 180, "type": "TASK", "confidence": 0.7420243620872498}, {"text": "parsing", "start_pos": 185, "end_pos": 192, "type": "TASK", "confidence": 0.9101037979125977}]}]}