{"title": [{"text": "Bootstrapping a Stochastic Transducer for Arabic-English Transliteration Extraction", "labels": [], "entities": [{"text": "Transliteration Extraction", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.7497989535331726}]}], "abstractContent": [{"text": "We propose a bootstrapping approach to training a memoriless stochastic transducer for the task of extracting transliterations from an English-Arabic bitext.", "labels": [], "entities": []}, {"text": "The transducer learns its similarity metric from the data in the bitext, and thus can function directly on strings written in different writing scripts without any additional language knowledge.", "labels": [], "entities": []}, {"text": "We show that this boot-strapped transducer performs as well or better than a model designed specifically to detect Arabic-English transliterations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transliterations are words that are converted from one writing script to another on the basis of their pronunciation, rather than being translated on the basis of their meaning.", "labels": [], "entities": []}, {"text": "Transliterations include named entities (e.g. \u00a9 An algorithm to detect transliterations automatically in a bitext can bean effective tool for many tasks.", "labels": [], "entities": []}, {"text": "Models of machine transliteration such as those presented in) or () require a large set of sample transliterations to use for training.", "labels": [], "entities": []}, {"text": "If such a training set is unavailable fora particular language pair, a detection algorithm would lead to a significant gain in time over attempting to build the set manually.", "labels": [], "entities": []}, {"text": "Algorithms for cross-language information retrieval often encounter the problem of out-ofvocabulary words, or words not present in the algorithm's lexicon.", "labels": [], "entities": [{"text": "cross-language information retrieval", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.7155928413073221}]}, {"text": "Often, a significant proportion of these words are named entities and thus are candidates for transliteration.", "labels": [], "entities": []}, {"text": "A transliteration detection algorithm could be used to map named entities in a query to potential transliterations in the target language text.", "labels": [], "entities": [{"text": "transliteration detection", "start_pos": 2, "end_pos": 27, "type": "TASK", "confidence": 0.8788775205612183}]}, {"text": "The main challenge in transliteration detection lies in the fact that transliteration is a lossy process.", "labels": [], "entities": [{"text": "transliteration detection", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.9640661478042603}]}, {"text": "In other words, information can be lost about the original word when it is transliterated.", "labels": [], "entities": []}, {"text": "This can occur because of phonetic gaps in one language or the other.", "labels": [], "entities": []}, {"text": "For example, the English [p] sound does not exist in Arabic, and the Arabic is transliterated as Ali.", "labels": [], "entities": []}, {"text": "Another form of loss occurs when the relationship between the orthographic and phonetic representations of a word are unclear.", "labels": [], "entities": []}, {"text": "For example, the [k] sound will always be written with the letter \u00f8 in Arabic, but in English it can be written as c, k ch, ck, cc or kk (not to mention being one of the sounds produced by x).", "labels": [], "entities": []}, {"text": "Finally, letters maybe deleted in one language or the other.", "labels": [], "entities": []}, {"text": "In Arabic, short vowels will often be omitted (e.g. We explore the use of word similarity metrics on the task of Arabic-English transliteration detection and extraction.", "labels": [], "entities": [{"text": "Arabic-English transliteration detection and extraction", "start_pos": 113, "end_pos": 168, "type": "TASK", "confidence": 0.7001765370368958}]}, {"text": "One of our primary goals in exploring these metrics is to assess whether it is possible maintain high performance without making the algorithms language-specific.", "labels": [], "entities": []}, {"text": "Many word-similarity metrics require that the strings being compared be 864 written in the same script.", "labels": [], "entities": []}, {"text": "Levenshtein edit distance, for example, does not produce a meaningful score in the absence of character identities.", "labels": [], "entities": []}, {"text": "Thus, if these metrics are to be used for transliteration extraction, modifications must be made to allow them to compare different scripts.", "labels": [], "entities": [{"text": "transliteration extraction", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.9277690947055817}]}, {"text": "take the approach of manually encoding a great deal of language knowledge directly into their Arabic-English fuzzy matching algorithm.", "labels": [], "entities": []}, {"text": "They define equivalence classes between letters in the two scripts and perform several rule-based transformations to make word pairs more comparable.", "labels": [], "entities": []}, {"text": "This approach is unattractive for two reasons.", "labels": [], "entities": []}, {"text": "Firstly, predicting all possible relationships between letters in English and Arabic is difficult.", "labels": [], "entities": []}, {"text": "For example, allowances have to be made for unusual pronunciations in foreign words such as the chin clich\u00e9 or the c in Milosevic.", "labels": [], "entities": []}, {"text": "Secondly, the algorithm becomes completely language-specific, which means that it cannot be used for any other language pair.", "labels": [], "entities": []}, {"text": "We propose a method to learn letter relationships directly from the bitext containing the transliterations.", "labels": [], "entities": []}, {"text": "Our model is based on the memoriless stochastic transducer proposed by, which derives a probabilistic wordsimilarity function from a set of examples.", "labels": [], "entities": []}, {"text": "The transducer is able to learn edit distance costs between disjoint sets of characters representing different writing scripts without any language-specific knowledge.", "labels": [], "entities": []}, {"text": "The transducer approach, however, requires a large set of training examples, which is a limitation not present in the fuzzy matching algorithm.", "labels": [], "entities": []}, {"text": "Thus, we propose a bootstrapping approach to train the stochastic transducer iteratively as it extracts transliterations from a bitext.", "labels": [], "entities": []}, {"text": "The bootstrapped stochastic transducer is completely language-independent, and we show that it is able to perform at least as well as the Arabic-English specific fuzzy matching algorithm.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents our bootstrapping method to train a stochastic transducer.", "labels": [], "entities": []}, {"text": "Section 3 outlines the Arabic-English fuzzy matching algorithm.", "labels": [], "entities": [{"text": "fuzzy matching", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.6694157272577286}]}, {"text": "Section 4 discusses other word-similarity models used for comparison.", "labels": [], "entities": []}, {"text": "Section 5 describes the results of two experiments performed to test the models.", "labels": [], "entities": []}, {"text": "Section 6 briefly discusses previous approaches to detecting transliterations.", "labels": [], "entities": [{"text": "detecting transliterations", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.8943096399307251}]}, {"text": "Section 7 presents our conclusions and possibilities for future work.", "labels": [], "entities": []}, {"text": "propose a probabilistic framework for word similarity, in which the similarity of a pair of words is defined as the sum of the probabilities of all paths through a memoriless stochastic transducer that generate the pair of words.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7504774630069733}]}, {"text": "This is referred to as the forward score of the pair of words.", "labels": [], "entities": [{"text": "forward score", "start_pos": 27, "end_pos": 40, "type": "METRIC", "confidence": 0.9660700559616089}]}, {"text": "They outline a forward-backward algorithm to train the model and show that it outperforms Levenshtein edit distance on the task of pronunciation classification.", "labels": [], "entities": [{"text": "pronunciation classification", "start_pos": 131, "end_pos": 159, "type": "TASK", "confidence": 0.9145277440547943}]}, {"text": "The training algorithm begins by calling the forward (Equation 1) and backward (Equation 2) functions to fill in the F and B tables for training pair sand t with respective lengths I and J.", "labels": [], "entities": [{"text": "F", "start_pos": 117, "end_pos": 118, "type": "METRIC", "confidence": 0.9349825978279114}]}], "datasetContent": [{"text": "The word-similarity metrics were evaluated on two separate tasks.", "labels": [], "entities": []}, {"text": "In experiment 1 (Section 5.1) the task was to extract transliterations from a sentence aligned bitext.", "labels": [], "entities": []}, {"text": "Experiment 2 (Section 5.2) provides the algorithms with named entities from an English document and requires them to extract the transliterations from the document's Arabic translation.", "labels": [], "entities": []}, {"text": "The two bitexts used in the experiments were the 867 The Treebank data was used as a development set to optimize the acceptance threshold used by the bootstrapped transducer.", "labels": [], "entities": [{"text": "867 The Treebank data", "start_pos": 49, "end_pos": 70, "type": "DATASET", "confidence": 0.7267320081591606}]}, {"text": "Testing for the sentencealigned extraction task was done on the first 20k sentences (approx. 50k words) of the parallel news data, while the named entity extraction task was performed on the first 1000 documents of the parallel news data.", "labels": [], "entities": [{"text": "sentencealigned extraction", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.7742210328578949}, {"text": "named entity extraction", "start_pos": 141, "end_pos": 164, "type": "TASK", "confidence": 0.7066261967023214}]}, {"text": "The seed set for bootstrapping the stochastic transducer was manually constructed and consisted of 14 names and their transliterations.", "labels": [], "entities": []}, {"text": "The first task used to test the models was to compare and score the words remaining in each bitext sentence pair after preprocessing the bitext in the following way: \u2022 The English corpus is tokenized using a modified 1 version of Word Splitter 2 . \u2022 All uncapitalized English words are removed.", "labels": [], "entities": [{"text": "Word Splitter", "start_pos": 230, "end_pos": 243, "type": "TASK", "confidence": 0.7290399670600891}]}, {"text": "\u2022 Stop words (mainly prepositions and auxiliary The way the program handles apostrophes(') had to be modified since they are sometimes used to represent glottal stops in transliterations of Arabic words, e.g. qala'a.", "labels": [], "entities": []}, {"text": "Available at http://l2r.cs.uiuc.edu/\u02dccogcomp/tools.php.", "labels": [], "entities": []}, {"text": "verbs) are removed from both sides of the bitext.", "labels": [], "entities": []}, {"text": "\u2022 Any English words of length less than 4 and Arabic words of length less than 3 are removed.", "labels": [], "entities": []}, {"text": "Each algorithm finds the top match for each English word and the top match for each Arabic word.", "labels": [], "entities": []}, {"text": "If two words mark each other as their top scorers, then the pair is marked as a transliteration pair.", "labels": [], "entities": []}, {"text": "This one-to-one constraint is meant to boost precision, though it will also lower recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9995056390762329}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.999144434928894}]}, {"text": "This is because for many of the tasks in which transliteration extraction would be useful (such as building a lexicon), precision is deemed more important.", "labels": [], "entities": [{"text": "transliteration extraction", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.8138016760349274}, {"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9989928603172302}]}, {"text": "Transliteration pairs are sorted according to their scores, and the top 500 hundred scoring pairs are returned.", "labels": [], "entities": []}, {"text": "The results for the sentence-aligned extraction task are presented in.", "labels": [], "entities": [{"text": "sentence-aligned extraction task", "start_pos": 20, "end_pos": 52, "type": "TASK", "confidence": 0.7933389147122701}]}, {"text": "Since the number of actual transliterations in the data was unknown, there was noway to compute recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9931021928787231}]}, {"text": "The measure used here is the precision for each 100 words extracted up to 500.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9996671676635742}]}, {"text": "The bootstrapping method is equal to or outperforms the other methods at all levels, including the Arabic-English specific fuzzy match algorithm.", "labels": [], "entities": []}, {"text": "Fuzzy matching does well for the first few hundred words extracted, but eventually falls below the level of the baseline Levenshtein.", "labels": [], "entities": [{"text": "Fuzzy matching", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7454989850521088}]}, {"text": "Interestingly, the bootstrapped transducer does not seem to have trouble with digraphs, despite the one-to-one nature of the character operations.", "labels": [], "entities": []}, {"text": "Word pairs with two-to-one mappings such as sh/ \ud97b\udf59 \ud97b\udf59 or 868: A sample of the errors made by the wordsimilarity metrics.", "labels": [], "entities": []}, {"text": "x/\ud97b\udf59\u00f7 tend to score lower than their counterparts composed of only one-to-one mappings, but nevertheless score highly.", "labels": [], "entities": []}, {"text": "A sample of the errors made by each wordsimilarity metric is presented in.", "labels": [], "entities": []}, {"text": "Errors 1-6 are indicative of the weaknesses of each individual algorithm.", "labels": [], "entities": []}, {"text": "The bootstrapping method encounters problems when erroneous pairs become part of the training data, thereby reinforcing the errors.", "labels": [], "entities": []}, {"text": "The only problematic mapping in Error 1 is the \u00d4/g mapping, and thus the pair has little trouble getting into the training data.", "labels": [], "entities": []}, {"text": "Once the pair is part of training data, the algorithm learns that the mapping is acceptable and uses it to acquire other training pairs that contain the same erroneous mapping.", "labels": [], "entities": []}, {"text": "The problem with the fuzzy matching algorithm seems to be that it creates too large a class of equivalent words.", "labels": [], "entities": [{"text": "fuzzy matching", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.7301194369792938}]}, {"text": "The pairs in errors 3 and 4 are given a total edit cost of 0.", "labels": [], "entities": [{"text": "edit", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.958454966545105}]}, {"text": "This is possible because of the overly general letter and vowel transformations, as well as unusual choices made for letter equivalences (e.g. \ud97b\udf59/c in error 4).", "labels": [], "entities": []}, {"text": "ALINE's errors tend to occur when it links two letters, based on phonetic similarity, that are never mapped to each other in transliteration because they each have a more direct equivalent in the other language (error 5).", "labels": [], "entities": []}, {"text": "Although the Arabic \u00f8 is phonetically similar to the English g, they would never be mapped to each other since English has several ways of representing an actual sound.", "labels": [], "entities": []}, {"text": "Errors made by Levenshtein distance (error 6) are simply due to the fact that it considers all non-identity mappings to be equivalent.", "labels": [], "entities": [{"text": "Levenshtein distance (error 6", "start_pos": 15, "end_pos": 44, "type": "METRIC", "confidence": 0.6363744616508484}]}, {"text": "Errors 7-10 are examples of general errors made by all the algorithms.", "labels": [], "entities": []}, {"text": "The most common error was related to inflection (error 7).", "labels": [], "entities": []}, {"text": "The words are essentially transliterations of each other, but one or the other of the two words takes a plural or some other inflectional ending that corrupts the phonetic match.", "labels": [], "entities": []}, {"text": "Error 8 represents the common problem of incidental letter similarity.", "labels": [], "entities": [{"text": "Error", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9234515428543091}, {"text": "incidental letter similarity", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.6026436587174734}]}, {"text": "The English -ian ending used for nationalities is very similar to the Arabic \u00a9 \u00c2 \u00aa [ijun] and \u00a9 \ud97b\udf59 \ud97b\udf59 \u00aa \u00c2 \u00aa [ijin] endings which are used for the same purpose.", "labels": [], "entities": [{"text": "\u00a9", "start_pos": 94, "end_pos": 95, "type": "METRIC", "confidence": 0.8918461799621582}]}, {"text": "They are similar phonetically and, since they are functionally similar, will tend to co-occur.", "labels": [], "entities": []}, {"text": "Since neither can be said to be derived from the other, however, they cannot be considered transliterations.", "labels": [], "entities": []}, {"text": "Error 9 is a case of two words of common origin taking on language-specific derivational endings that corrupt the phonetic match.", "labels": [], "entities": [{"text": "Error", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.963760256767273}]}, {"text": "Finally, error 10 shows a mapping (\u00f8/c) that is often correct in transliteration, but is inappropriate in this particular case.", "labels": [], "entities": [{"text": "\u00f8/c)", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.8913872539997101}]}, {"text": "The second experiment provides a more challenging task for the evaluation of the models.", "labels": [], "entities": []}, {"text": "It is structured as a cross-language named entity recognition task similar to those outlined in ( and ().", "labels": [], "entities": [{"text": "cross-language named entity recognition task", "start_pos": 22, "end_pos": 66, "type": "TASK", "confidence": 0.651314640045166}]}, {"text": "Essentially, the goal is to use a language for which named entity recognition software is readily available as a reference for tagging named entities in a language for which such software is not available.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.7298462192217509}]}, {"text": "For this task, the sentence alignment of the bitext is ignored.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.68891841173172}]}, {"text": "For each named entity in an English document, the models must select a transliteration from within the document's entire Arabic translation.", "labels": [], "entities": []}, {"text": "This is meant to be a loose approximation of the \"comparable\" corpora used in ().", "labels": [], "entities": []}, {"text": "The comparable corpora are related documents in different languages that are not translations (e.g. news articles describing the same event), and thus sentence alignment is not possible.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.7469311654567719}]}, {"text": "The first 1000 documents in the parallel news data were used for testing.", "labels": [], "entities": [{"text": "parallel news data", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.636820008357366}]}, {"text": "The English side of the bitext was tagged with Named Entity Tagger 3 , which labels named entities as person, location, organiza-: A sample of errors made on the NER detection task.", "labels": [], "entities": [{"text": "NER detection task", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.9593491355578104}]}, {"text": "The words labeled as person were extracted.", "labels": [], "entities": []}, {"text": "Person names are almost always transliterated, while for the other categories this is far less certain.", "labels": [], "entities": []}, {"text": "The list was then hand-checked to ensure that all names were candidates for transliteration, leaving 822 names.", "labels": [], "entities": []}, {"text": "The restrictions on word length and stop words were the same as before, but in this task each of the English person names from a given document were compared to all valid words in the corresponding Arabic document, and the top scorer for each English name was returned.", "labels": [], "entities": []}, {"text": "The results for the NER detection task are presented in.", "labels": [], "entities": [{"text": "NER detection task", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.9652525583902994}]}, {"text": "It seems the bootstrapped transducer's advantage is relative to the proportion of correct transliteration pairs to the total number of candidates.", "labels": [], "entities": []}, {"text": "As this proportion becomes smaller the transducer is given more opportunities to corrupt its training data and performance is affected accordingly.", "labels": [], "entities": []}, {"text": "Nevertheless, the transducer is able to perform as well as the language-specific fuzzy matching algorithm on this task, despite the greater challenge posed by selecting candidates from entire documents.", "labels": [], "entities": [{"text": "language-specific fuzzy matching", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.5801013012727102}]}, {"text": "A sample of errors made by the bootstrapped transducer and fuzzy matching algorithms is shown in.", "labels": [], "entities": []}, {"text": "Error 1 was due to the fact that names are sometimes split differently in Arabic and English.", "labels": [], "entities": []}, {"text": "The Arabic \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \u00c2 \u00ba \ud97b\udf59 (2 words) is generally written as Abdallah in English, leading to partial matches with part of the Arabic name.", "labels": [], "entities": [{"text": "\ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \u00c2 \u00ba", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.8188748478889465}]}, {"text": "Error 2 shows an issue with the one-to-one nature of the transducer.", "labels": [], "entities": []}, {"text": "The deleted h can be learned in mappings such as sh/ \ud97b\udf59 \ud97b\udf59 or ph/ \u00a9 \ud97b\udf59, but it is generally inappropriate to delete an hon its own.", "labels": [], "entities": [{"text": "\u00a9", "start_pos": 64, "end_pos": 65, "type": "METRIC", "confidence": 0.9277629256248474}]}, {"text": "Error 3 again shows that the fuzzy matching algorithm's letter transformations are too general.", "labels": [], "entities": []}, {"text": "The vowel removals lead to a 0 cost match in this case.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Precision of the various algorithms on the  NER detection task.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.959104597568512}, {"text": "NER detection task", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.9737772742907206}]}, {"text": " Table 6. Error 1 was due to the fact that names are  sometimes split differently in Arabic and English.  The Arabic \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \u00c2", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9966046810150146}, {"text": "Arabic \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \u00c2", "start_pos": 110, "end_pos": 124, "type": "DATASET", "confidence": 0.9721076130867005}]}]}