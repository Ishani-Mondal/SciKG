{"title": [{"text": "PERSONAGE: Personality Generation for Dialogue", "labels": [], "entities": [{"text": "Personality Generation", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.8447975218296051}]}], "abstractContent": [{"text": "Over the last fifty years, the \"Big Five\" model of personality traits has become a standard in psychology, and research has systematically documented correlations between a wide range of linguistic variables and the Big Five traits.", "labels": [], "entities": []}, {"text": "A distinct line of research has explored methods for automatically generating language that varies along personality dimensions.", "labels": [], "entities": []}, {"text": "We present PER-SONAGE (PERSONAlity GEnerator), the first highly parametrizable language generator for extraversion, an important aspect of personality.", "labels": [], "entities": [{"text": "PER-SONAGE", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.847388505935669}]}, {"text": "We evaluate two personality generation methods: (1) direct generation with particular parameter settings suggested by the psychology literature; and (2) overgeneration and selection using statistical models trained from judge's ratings.", "labels": [], "entities": [{"text": "personality generation", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.7367779165506363}, {"text": "direct generation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.716047465801239}]}, {"text": "Results show that both methods reliably generate utterances that vary along the extraversion dimension , according to human judges.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the last fifty years, the \"Big Five\" model of personality traits has become a standard in psychology (extraversion, neuroticism, agreeableness, conscientiousness, and openness to experience), and research has systematically documented correlations between a wide range of linguistic variables and the Big Five traits (.", "labels": [], "entities": []}, {"text": "A distinct line of research has explored methods for automatically generating language that varies along personality dimensions, targeting applications such as computer gaming and educational virtual worlds (;) inter alia.", "labels": [], "entities": []}, {"text": "Other work suggests a clear utility for generating language manifesting personality.", "labels": [], "entities": []}, {"text": "However, to date, (1) research in generation has not systematically exploited the psycholinguistic findings; and (2) there has been little evaluation showing that automatic generators can produce language with recognizable personality variation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our primary hypothesis is that language generated by varying parameters suggested by psycholinguistic research can be recognized as extravert or introvert.", "labels": [], "entities": []}, {"text": "To test this hypothesis, three expert judges evaluated a set of generated utterances as if they had been uttered by a friend responding in a dialogue to a request to recommend restaurants.", "labels": [], "entities": []}, {"text": "These utterances had been generated to systematically manipulate extraversion/introversion parameters.", "labels": [], "entities": []}, {"text": "The judges rated each utterance for perceived extraversion, by answering the two questions measur-500 ing that trait from the Ten-Item Personality Inventory, as this instrument was shown to be psychometrically superior to a 'single item per trait' questionnaire ().", "labels": [], "entities": []}, {"text": "The answers are averaged to produce an extraversion rating ranging from 1 (highly introvert) to 7 (highly extravert).", "labels": [], "entities": [{"text": "extraversion rating", "start_pos": 39, "end_pos": 58, "type": "METRIC", "confidence": 0.9721240699291229}]}, {"text": "Because it was unclear whether the generation parameters in would produce natural sounding utterances, the judges also evaluated the naturalness of each utterance on the same scale.", "labels": [], "entities": []}, {"text": "The judges rated 240 utterances, grouped into 20 sets of 12 utterances generated from the same content plan.", "labels": [], "entities": []}, {"text": "They rated one randomly ordered set at a time, but viewed all 12 utterances in that set before rating them.", "labels": [], "entities": []}, {"text": "The utterances were generated to meet two experimental goals.", "labels": [], "entities": []}, {"text": "First, to test the direct control of the perception of extraversion.", "labels": [], "entities": []}, {"text": "2 introvert utterances and 2 extravert utterances were generated for each content plan (80 in total) using the parameter values in.", "labels": [], "entities": []}, {"text": "Multiple outputs were generated with both parameter settings normally distributed with a 15% standard deviation.", "labels": [], "entities": []}, {"text": "Second, 8 utterances for each content plan (160 in total) were generated with random parameter values.", "labels": [], "entities": []}, {"text": "These random utterances make it possible to: (1) improve PERSONAGE's direct output by calibrating its parameters more precisely; and (2) build a statistical model that selects utterances matching input personality values after an overgeneration phase (see Section 6.2).", "labels": [], "entities": []}, {"text": "The interrater agreement for extraversion between the judges overall 240 utterances (average Pearson's correlation of 0.57) shows that the magnitude of the differences of perception between judges is almost constant (\u03c3 = .037).", "labels": [], "entities": [{"text": "interrater agreement", "start_pos": 4, "end_pos": 24, "type": "METRIC", "confidence": 0.9602614641189575}, {"text": "Pearson's correlation", "start_pos": 93, "end_pos": 114, "type": "METRIC", "confidence": 0.9735510150591532}]}, {"text": "A low agreement can yield a high correlation (e.g. if all values differ by a constant factor), so we also compute the intraclass correlation coefficient r based on a two-way random effect model.", "labels": [], "entities": [{"text": "intraclass correlation coefficient r", "start_pos": 118, "end_pos": 154, "type": "METRIC", "confidence": 0.7299988567829132}]}, {"text": "We obtain a r of 0.79, which is significant at the p < .001 level (reliability of average measures, identical to Cronbach's alpha).", "labels": [], "entities": [{"text": "reliability", "start_pos": 67, "end_pos": 78, "type": "METRIC", "confidence": 0.9863622784614563}]}, {"text": "This is comparable to the agreement of judgments of personality in (mean r = 0.84).", "labels": [], "entities": [{"text": "agreement", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9593168497085571}]}, {"text": "provides examples of PERSONAGE's output and extraversion ratings.", "labels": [], "entities": [{"text": "extraversion", "start_pos": 44, "end_pos": 56, "type": "METRIC", "confidence": 0.9844743013381958}]}, {"text": "To assess whether PERSONAGE generates language that can be recognized as introvert and extravert, we did a independent sample t-test between the average ratings of the 40 introvert and 40 extravert utterances (parameters with 15% standard deviation as in).: Average extraversion and naturalness ratings for the utterances generated with introvert, extravert, and random parameters.", "labels": [], "entities": []}, {"text": "We also investigate a second approach: overgeneration with random parameter settings, followed by ranking via a statistical model trained on the judges' feedback.", "labels": [], "entities": []}, {"text": "This approach supports generating utterances for any input extraversion value, as well as determining which parameters affect the judges' perception.", "labels": [], "entities": []}, {"text": "We model perceived personality ratings (1 . .", "labels": [], "entities": []}, {"text": "7) with regression models from the Weka toolbox).", "labels": [], "entities": [{"text": "Weka toolbox", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.9646914899349213}]}, {"text": "We used the full dataset of 160 averaged ratings for the random parameter utterances.", "labels": [], "entities": []}, {"text": "Each utterance was associated with a feature vector with the generation decisions for each parameter in Section 2.", "labels": [], "entities": []}, {"text": "To reduce data sparsity, we select features that correlate significantly with the ratings (p < .10) with a coefficient higher than 0.1.", "labels": [], "entities": []}, {"text": "Regression models are evaluated using the mean absolute error and the correlation between the predicted score and the actual average rating.", "labels": [], "entities": [{"text": "Regression", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9334400296211243}, {"text": "mean absolute error", "start_pos": 42, "end_pos": 61, "type": "METRIC", "confidence": 0.7326035896937052}]}, {"text": "shows the mean absolute error on a scale from 1 to 7 over ten 10-fold cross-validations for the 4 best regression models: Linear Regression (LR), M5' model tree (M5), and Support Vector Machines (i.e. SMOreg) with linear kernels (SMO 1 ) and radial-501 basis function kernels (SMO r ).", "labels": [], "entities": [{"text": "mean absolute error", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.73686882853508}]}, {"text": "All models significantly outperform the baseline (0.83 mean absolute error, p < .05), but surprisingly the linear model performs the best with a mean absolute error of 0.65.", "labels": [], "entities": [{"text": "mean absolute error", "start_pos": 55, "end_pos": 74, "type": "METRIC", "confidence": 0.74813312292099}]}, {"text": "The best model produces a correlation coefficient of 0.59 with the judges' ratings, which is higher than the correlations between pairs of judges, suggesting that the model performs as well as a human judge.: Mean absolute regression errors (scale from 1 to 7) and correlation coefficients over ten 10-fold cross-validations, for 4 models: Linear Regression (LR), M5' model tree (M5), Support Vector Machines with linear kernels (SMO 1 ) and radial-basis function kernels (SMO r ).", "labels": [], "entities": [{"text": "radial-basis function kernels (SMO r )", "start_pos": 442, "end_pos": 480, "type": "METRIC", "confidence": 0.6630619253431048}]}, {"text": "All models significantly outperform the mean baseline (0.83 error, p < .05).", "labels": [], "entities": [{"text": "mean baseline", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.9494260847568512}]}], "tableCaptions": [{"text": " Table 1: Recommendations along the extraver- sion dimension, with the average extraversion rating  from human judges on a scale from 1 to 7. Alt-2 and  3 are from the extravert set, Alt-4 and 5 are from the  introvert set, and others were randomly generated.", "labels": [], "entities": []}, {"text": " Table 3: Average extraversion and naturalness rat- ings for the utterances generated with introvert, ex- travert, and random parameters.", "labels": [], "entities": [{"text": "extraversion", "start_pos": 18, "end_pos": 30, "type": "METRIC", "confidence": 0.9110036492347717}, {"text": "naturalness rat- ings", "start_pos": 35, "end_pos": 56, "type": "METRIC", "confidence": 0.7149884253740311}]}, {"text": " Table 4: Mean absolute regression errors (scale from  1 to 7) and correlation coefficients over ten 10-fold  cross-validations, for 4 models: Linear Regression  (LR), M5' model tree (M5), Support Vector Ma- chines with linear kernels (SMO 1 ) and radial-basis  function kernels (SMO r ). All models significantly  outperform the mean baseline (0.83 error, p < .05).", "labels": [], "entities": [{"text": "Mean absolute regression errors", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.855524331331253}, {"text": "correlation", "start_pos": 67, "end_pos": 78, "type": "METRIC", "confidence": 0.9594175815582275}, {"text": "Linear Regression  (LR)", "start_pos": 143, "end_pos": 166, "type": "METRIC", "confidence": 0.8026314020156861}]}]}