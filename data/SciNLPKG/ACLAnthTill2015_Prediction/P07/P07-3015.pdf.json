{"title": [{"text": "Computing Lexical Chains with Graph Clustering", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes anew method for computing lexical chains.", "labels": [], "entities": []}, {"text": "These are sequences of semantically related words that reflect a text's cohesive structure.", "labels": [], "entities": []}, {"text": "In contrast to previous methods, we are able to select chains based on their cohesive strength.", "labels": [], "entities": []}, {"text": "This is achieved by analyzing the connectivity in graphs representing the lexical chains.", "labels": [], "entities": []}, {"text": "We show that the generated chains significantly improve performance of automatic text summarization and keyphrase indexing.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.675549253821373}, {"text": "keyphrase indexing", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.760460764169693}]}], "introductionContent": [{"text": "Text understanding tasks such as topic detection, automatic summarization, discourse analysis and question answering require deep understanding of the text's meaning.", "labels": [], "entities": [{"text": "Text understanding", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.813298761844635}, {"text": "topic detection", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.8995020091533661}, {"text": "summarization", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.8351995348930359}, {"text": "discourse analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7121508121490479}, {"text": "question answering", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.8558014929294586}]}, {"text": "The first step in determining this meaning is the analysis of the text's concepts and their inter-relations.", "labels": [], "entities": []}, {"text": "Lexical chains provide a framework for such an analysis.", "labels": [], "entities": []}, {"text": "They combine semantically related words across sentences into meaningful sequences that reflect the cohesive structure of the text.", "labels": [], "entities": []}, {"text": "Lexical chains, introduced by, have been studied extensively in the last decade, since large lexical databases are available in digital form.", "labels": [], "entities": []}, {"text": "Most approaches use WordNet or Roget's thesaurus for computing the chains and apply the results for text summarization.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.9470446705818176}, {"text": "text summarization", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7311659157276154}]}, {"text": "We present anew approach for computing lexical chains by treating them as graphs, where nodes are document terms and edges reflect semantic relations between them.", "labels": [], "entities": []}, {"text": "In contrast to previous methods, we analyze the cohesive strength within a chain by computing the diameter of the chain graph.", "labels": [], "entities": []}, {"text": "Weakly cohesive chains with a high graph diameter are decomposed by a graph clustering algorithm into several highly cohesive chains.", "labels": [], "entities": []}, {"text": "We use WordNet and alternatively a domain-specific thesaurus for obtaining semantic relations between the terms.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9491205215454102}]}, {"text": "We first give an overview of existing methods for computing lexical chains and related areas.", "labels": [], "entities": []}, {"text": "Then we discuss the motivation behind the new approach and describe the algorithm in detail.", "labels": [], "entities": []}, {"text": "Our evaluation demonstrates the advantages of using extracted lexical chains for the task of automatic text summarization and keyphrase indexing, compared to a simple baseline approach.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.6818619966506958}, {"text": "keyphrase indexing", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7320433557033539}]}, {"text": "The results are compared to annotations produced by a group of humans.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation we used a subset of a manually annotated corpus specifically created to evaluate text summarization systems ().", "labels": [], "entities": [{"text": "text summarization", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.6802591532468796}]}, {"text": "We concentrate only on documents with at least two manually produced summaries: 11 science and 29 newswire articles with two summaries each, and 7 articles additionally annotated by a third person.", "labels": [], "entities": []}, {"text": "This data allows us to compare the consistency of the system with humans to their consistency with each other.", "labels": [], "entities": []}, {"text": "The results are evaluated with the Kappa statistic \uf06b, defined for as follows: It takes into account the probability of chance agreement and is widely used to measure interrater agreement).", "labels": [], "entities": []}, {"text": "The ideal automatic summarization algorithm should have as high agreement with human subjects as they have with each other.", "labels": [], "entities": [{"text": "summarization", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.8660296201705933}]}, {"text": "We also use a baseline approach (BL) to estimate the advantage of using the proposed lexical chaining algorithm (LCA).", "labels": [], "entities": [{"text": "BL", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9099838733673096}]}, {"text": "It extracts text summaries in exactly the manner described in Section 4.1, with the exception of the lexical chaining stage.", "labels": [], "entities": []}, {"text": "Thus, when weighting sentences, the frequencies of all WordNet mappings are taken into account without the implicit word sense disambiguation provided by lexical chains.", "labels": [], "entities": []}, {"text": "compares the agreement among the human annotators and their agreement with the baseline approach BL and the lexical chain algorithm LCA.", "labels": [], "entities": []}, {"text": "The agreement between humans is low, which confirms that sentence extraction is a highly subjective task.", "labels": [], "entities": [{"text": "agreement", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9675540924072266}, {"text": "sentence extraction", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7777644693851471}]}, {"text": "The lexical chain approach LCA significantly outperforms the baseline BL, particularly on the science articles.", "labels": [], "entities": [{"text": "BL", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9869957566261292}]}, {"text": "While the average agreement of the LCA with humans is still low, the picture changes when we look at the agreement on individual documents.", "labels": [], "entities": [{"text": "agreement", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9555864334106445}]}, {"text": "Human agreement varies a lot (stdev = 0.24), while results produced by LCA are more consistent (stdev = 0.18).", "labels": [], "entities": [{"text": "agreement", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9515733122825623}]}, {"text": "In fact, for over 50% of documents LCA has greater or the same agreement with one or both human annotators than they with each other.", "labels": [], "entities": []}, {"text": "The overall superior performance of humans is due to exceptionally high agreement on a few documents, whereas on another couple of documents LCA failed to produce a consistent summary with both subjects.", "labels": [], "entities": [{"text": "agreement", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.981726348400116}]}, {"text": "This finding is similar to the one mentioned by. shows the agreement values for 7 newswire articles that were summarized by three human annotators.", "labels": [], "entities": []}, {"text": "Again, LCA clearly outperforms the baseline BL.", "labels": [], "entities": [{"text": "LCA", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.9123439192771912}, {"text": "BL", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9595662951469421}]}, {"text": "Interestingly, both systems have a greater agreement with the first subject than the first and the third human subjects with each other.", "labels": [], "entities": []}, {"text": "This approach is evaluated on 30 documents indexed each by 6 professional indexers from the UN's Food and Agriculture Organization.", "labels": [], "entities": []}, {"text": "The keyphrases are driven from the agricultural thesaurus Agrovoc 3 with around 40,000 terms and 30,000 semantic relations between them.", "labels": [], "entities": []}, {"text": "The effectiveness of the lexical chains is shown in comparison to a baseline approach, which given a document simply defines keyphrases as Agrovoc terms with top TFIDF values.", "labels": [], "entities": []}, {"text": "Indexing consistency is computed with the FMeasure F, which can be expressed in terms of The overlap between two keyphrase sets a is usually computed by exact matching of keyphrases.", "labels": [], "entities": [{"text": "consistency", "start_pos": 9, "end_pos": 20, "type": "METRIC", "confidence": 0.5680059790611267}, {"text": "FMeasure F", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.93315190076828}]}, {"text": "However, discrepancies between professional human indexers show that there are no \"correct\" keyphrases.", "labels": [], "entities": []}, {"text": "Capturing main topics rather than exact term choices is more important.", "labels": [], "entities": [{"text": "Capturing main topics", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8987909555435181}]}, {"text": "Lexical chains provide away of measuring this so called topical consistency.", "labels": [], "entities": []}, {"text": "Given a set of lexical chains extracted from a document, we first compute chains that are covered in its keyphrase set and then compute consistency in the usual manner.", "labels": [], "entities": [{"text": "consistency", "start_pos": 136, "end_pos": 147, "type": "METRIC", "confidence": 0.9871802926063538}]}, {"text": "shows topical consistency between each pair of professional human indexers, as well as between the indexers and the two automatic approaches, baseline BL and the lexical chain algorithm LCA, averaged over 30 documents.", "labels": [], "entities": [{"text": "BL", "start_pos": 151, "end_pos": 153, "type": "METRIC", "confidence": 0.5830134153366089}]}], "tableCaptions": [{"text": " Table 2. Kappa agreement on 40 summaries", "labels": [], "entities": [{"text": "Kappa", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.5694466233253479}, {"text": "summaries", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.7134349346160889}]}, {"text": " Table 3. Kappa agreement on 7 newswire articles", "labels": [], "entities": [{"text": "Kappa agreement", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7332081198692322}]}]}