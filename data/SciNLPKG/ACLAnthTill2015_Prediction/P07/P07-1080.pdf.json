{"title": [{"text": "Constituent Parsing with Incremental Sigmoid Belief Networks", "labels": [], "entities": [{"text": "Constituent Parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8322273790836334}]}], "abstractContent": [{"text": "We introduce a framework for syntactic parsing with latent variables based on a form of dynamic Sigmoid Belief Networks called Incremental Sigmoid Belief Networks.", "labels": [], "entities": [{"text": "syntactic parsing with latent variables", "start_pos": 29, "end_pos": 68, "type": "TASK", "confidence": 0.8029019117355347}]}, {"text": "We demonstrate that a previous feed-forward neural network parsing model can be viewed as a coarse approximation to inference with this class of graphical model.", "labels": [], "entities": [{"text": "feed-forward neural network parsing", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.68800388276577}]}, {"text": "By constructing a more accurate but still tractable approximation , we significantly improve parsing accuracy, suggesting that ISBNs provide a good idealization for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 93, "end_pos": 100, "type": "TASK", "confidence": 0.9779062867164612}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9697648882865906}, {"text": "parsing", "start_pos": 165, "end_pos": 172, "type": "TASK", "confidence": 0.9733012318611145}]}, {"text": "This gener-ative model of parsing achieves state-of-the-art results on WSJ text and 8% error reduction over the baseline neural network parser.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9783243536949158}, {"text": "error reduction", "start_pos": 87, "end_pos": 102, "type": "METRIC", "confidence": 0.9553276002407074}]}], "introductionContent": [{"text": "Latent variable models have recently been of increasing interest in Natural Language Processing, and in parsing in particular (e.g. ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 104, "end_pos": 111, "type": "TASK", "confidence": 0.9721607565879822}]}, {"text": "Latent variables provide a principled way to include features in a probability model without needing to have data labeled with those features in advance.", "labels": [], "entities": []}, {"text": "Instead, a labeling with these features can be induced as part of the training process.", "labels": [], "entities": []}, {"text": "The difficulty with latent variable models is that even small numbers of latent variables can lead to computationally intractable inference (a.k.a. decoding, parsing).", "labels": [], "entities": []}, {"text": "In this paper we propose a solution to this problem based on dynamic Sigmoid Belief Networks (SBNs).", "labels": [], "entities": []}, {"text": "The dynamic SBNs which we peopose, called Incremental Sigmoid Belief Networks (ISBNs) have large numbers of latent variables, which makes exact inference intractable.", "labels": [], "entities": []}, {"text": "However, they can be approximated sufficiently well to build fast and accurate statistical parsers which induce features during training.", "labels": [], "entities": []}, {"text": "We use SBNs in a generative history-based model of constituent structure parsing.", "labels": [], "entities": [{"text": "constituent structure parsing", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.6502803464730581}]}, {"text": "The probability of an unbounded structure is decomposed into a sequence of probabilities for individual derivation decisions, each decision conditioned on the unbounded history of previous decisions.", "labels": [], "entities": []}, {"text": "The most common approach to handling the unbounded nature of the histories is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g.).", "labels": [], "entities": []}, {"text": "Decision probabilities are then assumed to be independent of all information not represented by this finite set of features.", "labels": [], "entities": []}, {"text": "Another previous approach is to use neural networks to compute a compressed representation of the history and condition decisions on this representation).", "labels": [], "entities": []}, {"text": "It is possible that an unbounded amount of information is encoded in the compressed representation via its continuous values, but it is not clear whether this is actually happening due to the lack of any principled interpretation for these continuous values.", "labels": [], "entities": []}, {"text": "Like the former approach, we assume that there area finite set of features which encode the relevant information about the parse history.", "labels": [], "entities": []}, {"text": "But unlike that approach, we allow feature values to be ambiguous, and represent each feature as a distribution over (binary) values.", "labels": [], "entities": []}, {"text": "In other words, these history features are treated as latent variables.", "labels": [], "entities": []}, {"text": "Unfortunately, inter-632 preting the history representations as distributions over discrete values of latent variables makes the exact computation of decision probabilities intractable.", "labels": [], "entities": []}, {"text": "Exact computation requires marginalizing out the latent variables, which involves summing overall possible vectors of discrete values, which is exponential in the length of the vector.", "labels": [], "entities": []}, {"text": "We propose two forms of approximation for dynamic SBNs, a neural network approximation and a form of mean field approximation ().", "labels": [], "entities": []}, {"text": "We first show that the previous neural network model of can be viewed as a coarse approximation to inference with ISBNs.", "labels": [], "entities": [{"text": "ISBNs", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.969940721988678}]}, {"text": "We then propose an incremental mean field method, which results in an improved approximation over the neural network but remains tractable.", "labels": [], "entities": []}, {"text": "The resulting parser achieves significantly higher accuracy than the neural network parser (90.0% F-measure vs 89.1%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.999221920967102}, {"text": "F-measure", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9969601631164551}]}, {"text": "We argue that this correlation between better approximation and better accuracy suggests that dynamic SBNs area good abstract model for natural language parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.998221218585968}, {"text": "natural language parsing", "start_pos": 136, "end_pos": 160, "type": "TASK", "confidence": 0.6290927529335022}]}], "datasetContent": [{"text": "In this section we evaluate the two approximations to dynamic SBNs discussed in the previous section, the feed-forward method equivalent to the neural network of  90.0 90.2 90.1: Percentage labeled constituent recall (R), precision (P), combination of both (F 1 ) on the testing set.", "labels": [], "entities": [{"text": "Percentage labeled constituent recall (R)", "start_pos": 179, "end_pos": 220, "type": "METRIC", "confidence": 0.7357650782380786}, {"text": "precision (P)", "start_pos": 222, "end_pos": 235, "type": "METRIC", "confidence": 0.9518841058015823}]}, {"text": "the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in () and ().", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.9738783240318298}]}, {"text": "The standard split of the corpus into training (sections 2-22, 9,753 sentences), validation (section 24, 321 sentences), and testing (section 23, 603 sentences) was performed.", "labels": [], "entities": [{"text": "validation", "start_pos": 81, "end_pos": 91, "type": "TASK", "confidence": 0.962504506111145}]}, {"text": "As in) we used a publicly available tagger) to provide the part-of-speech tag for each word in the sentence.", "labels": [], "entities": []}, {"text": "For each tag, there is an unknown-word vocabulary item which is used for all those words which are not sufficiently frequent with that tag to be included individually in the vocabulary.", "labels": [], "entities": []}, {"text": "We only included a specific tag-word pair in the vocabulary if it occurred at least 20 time in the training set, which (with tag-unknown-word pairs) led to the very small vocabulary of 567 tag-word pairs.", "labels": [], "entities": []}, {"text": "During parsing with both the NN method and the MF method, we used beam search with a post-word beam of 10.", "labels": [], "entities": [{"text": "parsing", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.9778085947036743}]}, {"text": "Increasing the beam size beyond this value did not significantly effect parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 72, "end_pos": 79, "type": "TASK", "confidence": 0.9863193035125732}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9634812474250793}]}, {"text": "For both of the models, the state vector size of 40 was used.", "labels": [], "entities": []}, {"text": "All the parameters for both the NN and MF models were tuned on the validation set.", "labels": [], "entities": []}, {"text": "A single best model of each type was then applied to the final testing set.", "labels": [], "entities": []}, {"text": "lists the results of the NN approximation and the MF approximation, along with results of dif-ferent generative and discriminative parsing methods) evaluated in the same experimental setup.", "labels": [], "entities": [{"text": "MF approximation", "start_pos": 50, "end_pos": 66, "type": "METRIC", "confidence": 0.6035105735063553}]}, {"text": "The MF model improves over the baseline NN approximation, with an error reduction in F-measure exceeding 8%.", "labels": [], "entities": [{"text": "MF", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8431141376495361}, {"text": "error reduction", "start_pos": 66, "end_pos": 81, "type": "METRIC", "confidence": 0.9646326303482056}, {"text": "F-measure", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9872283935546875}]}, {"text": "This improvement is statically significant.", "labels": [], "entities": []}, {"text": "The MF model achieves results which do not appear to be significantly different from the results of the best model in the list.", "labels": [], "entities": [{"text": "MF", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8754146099090576}]}, {"text": "It should also be noted that the model) is the most accurate generative model on the standard WSJ parsing benchmark, which confirms the viability of our generative model.", "labels": [], "entities": [{"text": "generative", "start_pos": 61, "end_pos": 71, "type": "TASK", "confidence": 0.9714780449867249}, {"text": "WSJ parsing", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.7950499355792999}]}, {"text": "These experimental results suggest that Incremental Sigmoid Belief Networks are an appropriate model for natural language parsing.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.636993020772934}]}, {"text": "Even approximations such as those tested here, with a very strong factorisability assumption, allow us to build quite accurate parsing models.", "labels": [], "entities": [{"text": "parsing", "start_pos": 127, "end_pos": 134, "type": "TASK", "confidence": 0.9685752987861633}]}, {"text": "The main drawback of our proposed mean field approach is the relative computational complexity of the numerical procedure used to maximize L t,k V . But this approximation has succeeded in showing that a more accurate approximation of ISBNs results in a more accurate parser.", "labels": [], "entities": []}, {"text": "We believe this provides strong justification for more accurate approximations of ISBNs for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.9622204303741455}]}], "tableCaptions": []}