{"title": [{"text": "Text Analysis for Automatic Image Annotation", "labels": [], "entities": [{"text": "Text Analysis", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6695675849914551}, {"text": "Automatic Image Annotation", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.5961036284764608}]}], "abstractContent": [{"text": "We present a novel approach to automatically annotate images using associated text.", "labels": [], "entities": []}, {"text": "We detect and classify all entities (persons and objects) in the text after which we determine the salience (the importance of an entity in a text) and visualness (the extent to which an entity can be perceived visually) of these entities.", "labels": [], "entities": []}, {"text": "We combine these measures to compute the probability that an entity is present in the image.", "labels": [], "entities": []}, {"text": "The suitability of our approach was successfully tested on 100 image-text pairs of Yahoo!", "labels": [], "entities": []}], "introductionContent": [{"text": "Our society deals with a growing bulk of unstructured information such as text, images and video, a situation witnessed in many domains (news, biomedical information, intelligence information, business documents, etc.).", "labels": [], "entities": []}, {"text": "This growth comes along with the demand for more effective tools to search and summarize this information.", "labels": [], "entities": [{"text": "summarize this information", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.880564550558726}]}, {"text": "Moreover, there is the need to mine information from texts and images when they contribute to decision making by governments, businesses and other institutions.", "labels": [], "entities": []}, {"text": "The capability to accurately recognize content in these sources would largely contribute to improved indexing, classification, filtering, mining and interrogation.", "labels": [], "entities": [{"text": "classification", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.9016124606132507}]}, {"text": "Algorithms and techniques for the disclosure of information from the different media have been developed for every medium independently during the last decennium, but only recently the interplay between these different media has become a topic of interest.", "labels": [], "entities": []}, {"text": "One of the possible applications is to help analysis in one medium by employing information from another medium.", "labels": [], "entities": []}, {"text": "In this paper we study text that is associated with an image, such as for instance image captions, video transcripts or surrounding text in a web page.", "labels": [], "entities": []}, {"text": "We develop techniques that extract information from these texts to help with the difficult task of accurate object recognition in images.", "labels": [], "entities": [{"text": "accurate object recognition", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.5916188855965933}]}, {"text": "Although images and associated texts never contain precisely the same information, in many situations the associated text offers valuable information that helps to interpret the image.", "labels": [], "entities": []}, {"text": "The central objective of the CLASS project 1 is to develop advanced learning methods that allow images, video and associated text to be automatically analyzed and structured.", "labels": [], "entities": []}, {"text": "In this paper we test the feasibility of automatically annotating images by using textual information in near-parallel image-text pairs, in which most of the content of the image corresponds to content of the text and vice versa.", "labels": [], "entities": []}, {"text": "We will focus on entities such as persons and objects.", "labels": [], "entities": []}, {"text": "We will hereby take into account the text's discourse structure and semantics, which allow a more fine-grained identification of what content might be present in the image, and will enrich our model with world knowledge that is not present in the text.", "labels": [], "entities": []}, {"text": "We will first discuss the corpus on which we apply and test our techniques in section 2, after which we outline what techniques we have developed: we start with a baseline system to annotate images with person names (section 3) and improve this by computing the importance of the persons in the text (section 4).", "labels": [], "entities": []}, {"text": "We will then extend the model to include all  types of objects (section 5) and improve it by defining and computing the visualness measure (section 6).", "labels": [], "entities": []}, {"text": "Finally we will combine these different techniques in one probabilistic model in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "To determine the visualness, we first assign the correct WordNet synset to every entity, after which we compute a visualness score for these synsets.", "labels": [], "entities": []}, {"text": "Since these scores are floating point numbers, they are hard to evaluate manually.", "labels": [], "entities": []}, {"text": "During evaluation, we make the simplifying assumption that all entities with a visualness below a certain threshold are not visual, and all entities above this threshold are visual.", "labels": [], "entities": []}, {"text": "We choose this threshold to be 0.5.", "labels": [], "entities": []}, {"text": "This results in an accuracy of 79.56%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9997665286064148}]}, {"text": "Errors are mainly caused by erroneous entity detection and classification (63.10%) but also because of an incorrect assignment of the visualness (36.90%) by the method described above.", "labels": [], "entities": [{"text": "Errors", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.98136305809021}, {"text": "entity detection and classification", "start_pos": 38, "end_pos": 73, "type": "TASK", "confidence": 0.7257657498121262}]}], "tableCaptions": [{"text": " Table 1: Comparison of methods to predict what per- sons described in the text will appear in the image,  using Named Entity Recognition (NER), and the  salience measure with dynamic cut-off (DYN).", "labels": [], "entities": []}, {"text": " Table 2: Comparison of methods to predict the en- tities that appear in the image, using entity detec- tion (Ent), and the visualness (Vis) and salience (Sal)  measures.", "labels": [], "entities": [{"text": "visualness (Vis) and salience (Sal)", "start_pos": 124, "end_pos": 159, "type": "METRIC", "confidence": 0.6600414150291019}]}]}