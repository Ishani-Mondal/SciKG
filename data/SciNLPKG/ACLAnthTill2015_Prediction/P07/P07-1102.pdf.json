{"title": [], "abstractContent": [{"text": "Task-solving in dialogue depends on the linguistic alignment of the interlocutors, which Pickering & Garrod (2004) have suggested to be based on mechanistic repetition effects.", "labels": [], "entities": []}, {"text": "In this paper, we seek confirmation of this hypothesis by looking at repetition in corpora, and whether repetition is correlated with task success.", "labels": [], "entities": [{"text": "repetition", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9575096368789673}, {"text": "repetition", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9412317276000977}]}, {"text": "We show that the relevant repetition tendency is based on slow adaptation rather than short-term priming and demonstrate that lexical and syntactic repetition is a reliable predictor of task success given the first five minutes of a task-oriented dialogue.", "labels": [], "entities": []}], "introductionContent": [{"text": "While humans are remarkably efficient, flexible and reliable communicators, we are far from perfect.", "labels": [], "entities": []}, {"text": "Our dialogues differ in how successfully information is conveyed.", "labels": [], "entities": []}, {"text": "In task-oriented dialogue, where the interlocutors are communicating to solve a problem, task success is a crucial indicator of the success of the communication.", "labels": [], "entities": []}, {"text": "An automatic measure of task success would be useful for evaluating conversations among humans, e.g., for evaluating agents in a call center.", "labels": [], "entities": []}, {"text": "In humancomputer dialogues, predicting the task success after just a first few turns of the conversation could avoid disappointment: if the conversation isn't going well, a caller maybe passed onto a human operator, or the system may switch dialogue strategies.", "labels": [], "entities": []}, {"text": "As a first step, we focus on human-human dialogue, since current spoken dialogue systems do not yet yield long, syntactically complex conversations.", "labels": [], "entities": []}, {"text": "In this paper, we use syntactic and lexical features to predict task success in an environment where we assume no speaker model, no semantic information and no information typical fora human-computer dialogue system, e.g., ASR confidence.", "labels": [], "entities": [{"text": "ASR", "start_pos": 223, "end_pos": 226, "type": "TASK", "confidence": 0.8795503973960876}]}, {"text": "The features we use are based on a psychological theory, linking alignment between dialogue participants to low-level syntactic priming.", "labels": [], "entities": []}, {"text": "An examination of this priming reveals differences between short-term and long-term effects.", "labels": [], "entities": []}], "datasetContent": [{"text": "We cast the task as a regression problem.", "labels": [], "entities": []}, {"text": "To predict a dialogue's score, we apply the SVM to its data points.", "labels": [], "entities": []}, {"text": "The mean outcome is the estimated score.", "labels": [], "entities": []}, {"text": "A suitable evaluation measure, the classical r 2 , indicates the proportion of the variance in the actual task success score that can be predicted by the model.", "labels": [], "entities": []}, {"text": "All results reported here are produced from 10-fold cross-validated 90% training / 10% test splits of the dialogues.", "labels": [], "entities": []}, {"text": "No full dialogue was included in both test and training sets.", "labels": [], "entities": []}, {"text": "Task 1 was evaluated with all data, the Task 2 model was trained and tested on data points sampled from the first 5 minutes of the dialogue.", "labels": [], "entities": []}, {"text": "For Task 1 (full dialogues), the results indicate that ALL repetition features together with the LENGTH of the conversation, account for about 17% of the total score variance.", "labels": [], "entities": [{"text": "ALL", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9286419749259949}, {"text": "repetition", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.5290542244911194}, {"text": "LENGTH", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9988211989402771}]}, {"text": "The repetition features improve on the performance achieved from dialogue length alone (about 9%).", "labels": [], "entities": [{"text": "repetition", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9802087545394897}]}, {"text": "For the more difficult Task 2, ALL features together achieve 14% of the variance.", "labels": [], "entities": [{"text": "variance", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9788347482681274}]}, {"text": "(Note that LENGTH is not available.)", "labels": [], "entities": [{"text": "LENGTH", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9889312386512756}]}, {"text": "When the syntactic repetition feature is taken out and only lexical (LEXREP) and character repetition (CHARREP) are used, we achieve 6% in explained variance.", "labels": [], "entities": [{"text": "character repetition (CHARREP)", "start_pos": 81, "end_pos": 111, "type": "METRIC", "confidence": 0.7190236210823059}]}, {"text": "The baseline is implemented as a model that always estimates the mean score.", "labels": [], "entities": []}, {"text": "It should, theoretically, be close to 0.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Portion of variance explained (r 2 )", "labels": [], "entities": [{"text": "variance", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.5715696215629578}]}]}