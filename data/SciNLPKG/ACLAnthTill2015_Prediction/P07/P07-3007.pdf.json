{"title": [{"text": "Kinds of Features for Chinese Opinionated Information Retrieval", "labels": [], "entities": [{"text": "Chinese Opinionated Information Retrieval", "start_pos": 22, "end_pos": 63, "type": "TASK", "confidence": 0.4809080883860588}]}], "abstractContent": [{"text": "This paper presents the results of experiments in which we tested different kinds of features for retrieval of Chinese opinionated texts.", "labels": [], "entities": [{"text": "retrieval of Chinese opinionated texts", "start_pos": 98, "end_pos": 136, "type": "TASK", "confidence": 0.863053274154663}]}, {"text": "We assume that the task of retrieval of opinionated texts (OIR) can be regarded as a subtask of general IR, but with some distinct features.", "labels": [], "entities": [{"text": "retrieval of opinionated texts (OIR)", "start_pos": 27, "end_pos": 63, "type": "TASK", "confidence": 0.8752896445138114}, {"text": "IR", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.9560171961784363}]}, {"text": "The experiments showed that the best results were obtained from the combination of character-based processing, dictionary lookup (maximum matching) and a negation check.", "labels": [], "entities": []}], "introductionContent": [{"text": "The extraction of opinionated information has recently become an important research topic.", "labels": [], "entities": [{"text": "extraction of opinionated information", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.8719052374362946}]}, {"text": "Business and governmental institutions often need to have information about how their products or actions are perceived by people.", "labels": [], "entities": []}, {"text": "Individuals maybe interested in other people's opinions on various topics ranging from political events to consumer products.", "labels": [], "entities": []}, {"text": "At the same time globalization has made the whole world smaller, and a notion of the world as a 'global village' does not surprise people nowadays.", "labels": [], "entities": []}, {"text": "In this context we assume information in Chinese to be of particular interest as the Chinese world (the mainland China, Taiwan, Hong Kong, Singapore and numerous Chinese communities allover the world) is getting more and more influential over the world economy and politics.", "labels": [], "entities": []}, {"text": "We therefore believe that a system capable of providing access to opinionated information in other languages (especially in Chinese) might be of great use for individuals as well as for institutions involved in international trade or international relations.", "labels": [], "entities": []}, {"text": "The sentiment classification experiments presented in this paper were done in the context of Opinionated Information Retrieval which is planned to be a module in a Cross-Language Opinion Extraction system (CLOE).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.9405083656311035}, {"text": "Opinionated Information Retrieval", "start_pos": 93, "end_pos": 126, "type": "TASK", "confidence": 0.6171853840351105}]}, {"text": "The main goal of this system is to provide access to opinionated information on any topic ad-hoc in a language different to the language of a query.", "labels": [], "entities": []}, {"text": "To implement the idea the CLOE system which is the context for the experiments described in the paper will consist of four main modules: The OIR module will process complex queries consisting of a word sequence indicating a topic and sentiment information.", "labels": [], "entities": []}, {"text": "An example of such a query is: \"Asus laptop + OPINIONS\", another, more detailed query, might be \"Asus laptop + POSITIVE OPINIONS\".", "labels": [], "entities": []}, {"text": "Another possible approach to the architecture of the CLOE system would be to implement the processing as a pipeline consisting, first, of using IR to retrieve certain articles relevant to the topic followed by second stage of classifying them according to sentiment polarity.", "labels": [], "entities": []}, {"text": "But such an approach probably would be too inefficient, as the search will produce a lot of irrelevant results (containing no opinionated information).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper we present the results of sentiment classification experiments in which we tested different kinds of features for retrieval of Chinese opinionated information.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.9242940843105316}, {"text": "retrieval of Chinese opinionated information", "start_pos": 128, "end_pos": 172, "type": "TASK", "confidence": 0.8096769094467163}]}, {"text": "As stated earlier (see Section 1), we assume that the task of retrieval of opinionated texts (OIR) can be regarded as a subtask of general IR with a query consisting of two parts: (1) words indicating topic and (2) a semantic class indicating sentiment (OPIN-IONS).", "labels": [], "entities": [{"text": "retrieval of opinionated texts (OIR)", "start_pos": 62, "end_pos": 98, "type": "TASK", "confidence": 0.7872886572565351}, {"text": "IR", "start_pos": 139, "end_pos": 141, "type": "TASK", "confidence": 0.8834419250488281}]}, {"text": "The latter part of the query cannot be specified in terms that can be instantly used in the process of retrieval.", "labels": [], "entities": []}, {"text": "The sentiment part of the query can be further detailed into subcategories such as POSITIVE OPIN-IONS, NEGATIVE OPINIONS, NEUTRAL OPIN-IONS each of which can be split according to sentiment intensity (HIGHLY POSITIVE OPINIONS, SLIGHTLY NEGATIVE OPINIONS etc.).", "labels": [], "entities": []}, {"text": "But whatever level of categorisation we use, the query is still too abstract and cannot be used in practice.", "labels": [], "entities": []}, {"text": "It therefore needs to be put into words and most probably expanded.", "labels": [], "entities": []}, {"text": "The texts should also be indexed with appropriate sentiment tags which in the context of sentiment processing implies classification of the texts according to presence / absence of a sentiment and, if the texts are opinionated, according to their sentiment polarity.", "labels": [], "entities": [{"text": "sentiment processing", "start_pos": 89, "end_pos": 109, "type": "TASK", "confidence": 0.8119547069072723}]}, {"text": "To test the proposed approach we designed two experiments.", "labels": [], "entities": []}, {"text": "The purpose of the first experiment was to find the most effective kind of features for sentiment polar-ity discrimination (detection) which can be used for OIR 2 . found that for Chinese IR the most effective kinds of features were a combination of dictionary lookup (longest-match algorithm) together with unigrams (single characters).", "labels": [], "entities": [{"text": "sentiment polar-ity discrimination (detection", "start_pos": 88, "end_pos": 133, "type": "TASK", "confidence": 0.7553390085697174}, {"text": "OIR 2", "start_pos": 157, "end_pos": 162, "type": "TASK", "confidence": 0.809484988451004}]}, {"text": "The approach was tested in the first experiment.", "labels": [], "entities": []}, {"text": "The second experiment was designed to test the found set of features for text classification (indexing) for an OIR query of the first level (finds opinionated information) and for an OIR query of the second level (finds opinionated information with sentiment direction detection), thus the classifier should 1) detect opinionated texts and 2) classify the found items either as positive or as negative.", "labels": [], "entities": [{"text": "text classification", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7427660524845123}, {"text": "sentiment direction detection", "start_pos": 249, "end_pos": 278, "type": "TASK", "confidence": 0.7262879808743795}]}, {"text": "As training corpus for the second experiment we use the NTU sentiment dictionary (NTUSD) (by) as well as a list of sentiment scores of Chinese characters obtained from processing of the same dictionary.", "labels": [], "entities": [{"text": "NTU sentiment dictionary (NTUSD)", "start_pos": 56, "end_pos": 88, "type": "DATASET", "confidence": 0.8357598880926768}]}, {"text": "Dictionary lookup used the longest-match algorithm.", "labels": [], "entities": []}, {"text": "The dictionary has 2809 items in the \"positive\" part and 8273 items in the \"negative\".", "labels": [], "entities": []}, {"text": "The same dictionary was also used as a corpus for calculating the sentiment scores of Chinese characters.", "labels": [], "entities": []}, {"text": "The use of the dictionary as a training corpus for obtaining the sentiment scores of characters is justified by two reasons: 1) it is domain-independent and 2) it contains only relevant (sentiment-related) information.", "labels": [], "entities": []}, {"text": "The above mentioned parts of the dictionary used as the corpus comprised 24308 characters in the \"negative\" part and 7898 characters in the \"positive\" part.", "labels": [], "entities": []}, {"text": "A corpus of E-Bay 4 customers' reviews of products and services was used as a test corpus.", "labels": [], "entities": []}, {"text": "The total number of reviews is 128, of which 37 are negative (average length 64 characters) and 91 are positive (average length 18 characters), all of the reviews were tagged as 'positive' or 'negative' by the reviewers . We computed two scores for each item (a review): one for positive sentiment, another for negative sentiment.", "labels": [], "entities": []}, {"text": "The decision about an item's sentiment polarity was made every time by finding the biggest score of the two.", "labels": [], "entities": []}, {"text": "For every phrase (a chunk of characters between punctuation marks) a score was calculated as: where Sc dictionary is a dictionary based score calculated using following formula: where L d -length of a dictionary item, L s -length of a phrase.", "labels": [], "entities": []}, {"text": "The constant value 100 is used to weight the score, obtained by a series of preliminary tests as a value that most significantly improved the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9988419413566589}]}, {"text": "The sentiment scores for characters were obtained by the formula: where Sc i is the sentiment score fora character fora given class i, F i -the character's relative frequency in a class i, F (i+j) -the character's relative frequency in both classes i and j taken as one unit.", "labels": [], "entities": [{"text": "F", "start_pos": 189, "end_pos": 190, "type": "METRIC", "confidence": 0.9734292030334473}]}, {"text": "The relative frequency of character c is calculated as where N c is a number of the character's occurrences in the corpus, and N (1...n) is the number of all characters in the same corpus.", "labels": [], "entities": []}, {"text": "Preliminary tests showed that inverting all the characters for which Sc i \u2264 1 improves accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9980789422988892}]}, {"text": "The inverting is calculated as follows: We compute scores rather than probabilities since we are combining information from two distinct sources (characters and words).", "labels": [], "entities": []}, {"text": "In addition to the features specified (characters and dictionary items) we also used a simple negation check.", "labels": [], "entities": []}, {"text": "The system checked two most widely used negations in Chinese: bu and mei.", "labels": [], "entities": []}, {"text": "Every phrase was compared with the following pattern: negation+ 0-2 characters+ phrase.", "labels": [], "entities": []}, {"text": "The scores of all the unigrams in the phrase that matched the pattern were multiplied by -1.", "labels": [], "entities": []}, {"text": "Finally, the score was calculated for an item as the sum of the phrases' scores modified by the negation check: For sentiment polarity detection the item scores for each of the two polarities were compared to each other: the polarity with bigger score was assigned to the item.", "labels": [], "entities": [{"text": "sentiment polarity detection", "start_pos": 116, "end_pos": 144, "type": "TASK", "confidence": 0.8516938885052999}]}, {"text": "where Sc i is an item score for one polarity and Sc j is an item score for the other.", "labels": [], "entities": []}, {"text": "The main evaluation measure was accuracy of sentiment identification, expressed in percent.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9997344613075256}, {"text": "sentiment identification", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.9543135762214661}]}, {"text": "To find out which kinds of features perform best for sentiment polarity detection the system was run several times with different settings.", "labels": [], "entities": [{"text": "sentiment polarity detection", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.9413348038991293}]}, {"text": "Running without character scores (with dictionary longest-match only) gave the following results: almost 64% of positive and near 65% for negative reviews were detected correctly, which is 64% accuracy for the whole corpus (note that a baseline classifier tagging all items as positive achieves an accuracy of 71.1%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.9989890456199646}, {"text": "accuracy", "start_pos": 298, "end_pos": 306, "type": "METRIC", "confidence": 0.9989510774612427}]}, {"text": "Characters with sentiment scores alone performed much better on negative reviews (84% accuracy) rather than on positive (65%), but overall performance was still better: 70%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9793788194656372}]}, {"text": "Both methods combined gave a significant increase on positive reviews (73%) and no improvement on negative (84%), giving 77% overall.", "labels": [], "entities": []}, {"text": "The last run was with the dictionary lookup, the characters and the negation check.", "labels": [], "entities": []}, {"text": "The results were: 77% for positive and 89% for negative, 80% corpus-wide (see).", "labels": [], "entities": []}, {"text": "Judging from the results it is possible to suggest that both the word-based dictionary lookup method The negation check increased the performance by 3% overall, up to 80%.", "labels": [], "entities": []}, {"text": "Although the performance gain is not very high, the computational cost of this feature is very low.", "labels": [], "entities": []}, {"text": "As we used a non-balanced corpus (71% of the reviews are positive), it is quite difficult to compare the results with the results obtained by other authors.", "labels": [], "entities": []}, {"text": "But the proposed classifier outperformed some standart classifiers on the same data set: a Naive Bayes (multinomial) classifier gained only 49.6 % of accuracy (63 items tagged correctly) while a Support vector machine classifier got 64.5 % of accuracy (82 items).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9985755681991577}, {"text": "accuracy", "start_pos": 243, "end_pos": 251, "type": "METRIC", "confidence": 0.9984275102615356}]}, {"text": "The second experiment included two parts: determining whether texts are opinionated which is a precondition for the processing of the OPINION part of the query; and tagging found texts with relevant sentiment for processing a more detailed form of this query POSITIVE/NEGATIVE OPINION.", "labels": [], "entities": []}, {"text": "For this experiment we used the features that showed the best performance as described in section 4.1: the dictionary items and the characters with the sentiment scores.", "labels": [], "entities": []}, {"text": "The test corpus for this experiment consisted of 282 items, where every item is a paragraph.", "labels": [], "entities": []}, {"text": "We used paragraphs as basic items in this experiment because of two reasons: 1.", "labels": [], "entities": []}, {"text": "opinionated texts (reviews) are usually quite short (in our corpus all of them are one paragraph), while texts of other genres are usually much longer; and 2.", "labels": [], "entities": []}, {"text": "for IR tasks it is more usual to retrieve units longer then a sentence.", "labels": [], "entities": [{"text": "IR", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9785411953926086}]}, {"text": "The test corpus has following structure: 128 items are opinionated, of which 91 are positive and 37 are negative (all the items are the reviews used in the first experiment, see 4.1).", "labels": [], "entities": []}, {"text": "154 items are not opinionated, of which 97 are paragraphs taken from a scientific book on Chinese linguistics and 57 items are from articles taken form a Chinese on-line encyclopedia Baidu Baike 7 . For the first task we used the following technique: every item was assigned a score (a sum of the characters' scores and dictionary scores described in 4.1).", "labels": [], "entities": []}, {"text": "The score was divided by the number of characters in the item to obtain the average score: where Sc item is the item score, and L item is the length of an item (number of characters in it).", "labels": [], "entities": []}, {"text": "A positive and a negative average score is computed for each item.", "labels": [], "entities": []}, {"text": "To determine whether an item is opinionated (for OPINION query), the maximum of the two scores was compared to a threshold value.", "labels": [], "entities": []}, {"text": "The best performance was achieved with the threshold value of 1.6 -more than 85% of accuracy 8 (see).", "labels": [], "entities": [{"text": "accuracy 8", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9846426248550415}]}, {"text": "Next task (NEGATIVE/POSITIVE OPINIONS) was processed by comparing the negative and positive scores for each found item (see).", "labels": [], "entities": [{"text": "POSITIVE OPINIONS)", "start_pos": 20, "end_pos": 38, "type": "METRIC", "confidence": 0.7096381584803263}]}, {"text": "Although the unopinionated texts are very different from the opinionated ones in terms of genre and topic, the standard classifiers (Naive Bayes (multinomial) and SVM) failed to identify any nonopinionated texts.", "labels": [], "entities": [{"text": "SVM", "start_pos": 163, "end_pos": 166, "type": "DATASET", "confidence": 0.8992012739181519}]}, {"text": "The most probable explanation for this is that there were no items tagged 'unopinionated' in the training corpus (the sentiment dictionary) and there were only words and phrases with predominant sentiment meaning rather then topicrelated.", "labels": [], "entities": []}, {"text": "It is worth noting that we observed the same relation between subjectivity detection and polarity classification accuracy as described by and.", "labels": [], "entities": [{"text": "subjectivity detection", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.7801550626754761}, {"text": "polarity classification", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.7369132339954376}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.8576797246932983}]}, {"text": "The accuracy of the sentiment detection of opinionated texts (excluding erroneously detected unopinionated texts) in Experiment 2 has increased by 13% for positive reviews and by 6% for negative reviews (see).: Accuracy of sentiment polarity detection of opinionated texts (in percent).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994509816169739}, {"text": "sentiment detection of opinionated texts", "start_pos": 20, "end_pos": 60, "type": "TASK", "confidence": 0.8484181523323059}, {"text": "Accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9965565204620361}, {"text": "sentiment polarity detection", "start_pos": 223, "end_pos": 251, "type": "TASK", "confidence": 0.7192007601261139}]}], "tableCaptions": [{"text": " Table 1: Results of Experiment 1 (accuracy in per- cent).", "labels": [], "entities": [{"text": "accuracy in per- cent)", "start_pos": 35, "end_pos": 57, "type": "METRIC", "confidence": 0.8372699320316315}]}, {"text": " Table 2: Results of Experiment 2 (in percent).", "labels": [], "entities": []}, {"text": " Table 3: Accuracy of sentiment polarity detection of  opinionated texts (in percent).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9914450645446777}, {"text": "sentiment polarity detection", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.7826903959115347}]}]}