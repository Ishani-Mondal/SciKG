{"title": [{"text": "Annotating and Learning Compound Noun Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "There is little consensus on a standard experimental design for the compound interpretation task.", "labels": [], "entities": [{"text": "compound interpretation task", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.9115246931711832}]}, {"text": "This paper introduces well-motivated general desiderata for semantic annotation schemes, and describes such a scheme for in-context compound annotation accompanied by detailed publicly available guidelines.", "labels": [], "entities": []}, {"text": "Classification experiments on an open-text dataset compare favourably with previously reported results and provide a solid baseline for future research.", "labels": [], "entities": []}], "introductionContent": [{"text": "There area number of reasons why the interpretation of noun-noun compounds has long been a topic of interest for NLP researchers.", "labels": [], "entities": []}, {"text": "Compounds occur very frequently in English and many other languages, so they cannot be avoided by a robust semantic processing system.", "labels": [], "entities": []}, {"text": "Compounding is a very productive process with a highly skewed type frequency spectrum, and corpus information maybe very sparse.", "labels": [], "entities": []}, {"text": "Compounds are often highly ambiguous and a large degree of \"world knowledge\" seems necessary to understand them.", "labels": [], "entities": []}, {"text": "For example, knowing that a cheese knife is (probably) a knife for cutting cheese and (probably) not a knife made of cheese (cf. plastic knife) does not just require an ability to identify the senses of cheese and knife but also knowledge about what one usually does with cheese and knives.", "labels": [], "entities": []}, {"text": "These factors combine to yield a difficult problem that exhibits many of the challenges characteristic of lexical semantic processing in general.", "labels": [], "entities": [{"text": "lexical semantic processing", "start_pos": 106, "end_pos": 133, "type": "TASK", "confidence": 0.6930860877037048}]}, {"text": "Recent research has made significant progress on solving the problem with statistical methods and often without the need for manually created lexical resources).", "labels": [], "entities": []}, {"text": "The work presented here is part of an ongoing project that treats compound interpretation as a classification problem to be solved using machine learning.", "labels": [], "entities": [{"text": "compound interpretation", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.8692231178283691}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2.  The simple word-counting conditions w5 and w10  perform relatively well, but the highest accuracy is  achieved by Rconj. The general effect of the log- likelihood transformation cannot be stated categor- ically, as it causes some conditions to improve and  others to worsen, but the G 2 -transformed Rconj fea- tures give the best results of all with 54.95% ac- curacy (53.42% macro-average). Analysis of per- formance across categories shows that in all cases  accuracy is lower (usually below 30%) on the BE  and HAVE relations than on the others (often above  50%). These two relations are least common in the  dataset, which is why the macro-averaged figures are  slightly lower than the micro-averaged accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9985685348510742}, {"text": "ac- curacy", "start_pos": 369, "end_pos": 379, "type": "METRIC", "confidence": 0.9445431232452393}, {"text": "accuracy", "start_pos": 473, "end_pos": 481, "type": "METRIC", "confidence": 0.9989516735076904}, {"text": "BE", "start_pos": 518, "end_pos": 520, "type": "METRIC", "confidence": 0.9930549263954163}]}, {"text": " Table 2: Performance of BNC co-occurrence data", "labels": [], "entities": []}]}