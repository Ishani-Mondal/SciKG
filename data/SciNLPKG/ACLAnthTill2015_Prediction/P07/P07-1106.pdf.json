{"title": [{"text": "Chinese Segmentation with a Word-Based Perceptron Algorithm", "labels": [], "entities": [{"text": "Chinese Segmentation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5036017596721649}]}], "abstractContent": [{"text": "Standard approaches to Chinese word seg-mentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary.", "labels": [], "entities": []}, {"text": "Discrimina-tively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation.", "labels": [], "entities": [{"text": "tagging", "start_pos": 88, "end_pos": 95, "type": "TASK", "confidence": 0.9705466032028198}]}, {"text": "In this paper we propose an alternative, word-based seg-mentor, which uses features based on complete words and word sequences.", "labels": [], "entities": []}, {"text": "The generalized perceptron algorithm is used for dis-criminative training, and we use a beam-search decoder.", "labels": [], "entities": [{"text": "dis-criminative training", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.8622029721736908}]}, {"text": "Closed tests on the first and second SIGHAN bakeoffs show that our system is competitive with the best in the literature , achieving the highest reported F-scores fora number of corpora.", "labels": [], "entities": [{"text": "SIGHAN bakeoffs", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.5607752501964569}, {"text": "F-scores", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9927791953086853}]}], "introductionContent": [{"text": "Words are the basic units to process for most NLP tasks.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 46, "end_pos": 55, "type": "TASK", "confidence": 0.8988429009914398}]}, {"text": "The problem of Chinese word segmentation (CWS) is to find these basic units fora given sentence, which is written as a continuous sequence of characters.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.780461718638738}]}, {"text": "It is the initial step for most Chinese processing applications.", "labels": [], "entities": []}, {"text": "Chinese character sequences are ambiguous, often requiring knowledge from a variety of sources for disambiguation.", "labels": [], "entities": []}, {"text": "Out-of-vocabulary (OOV) words area major source of ambiguity.", "labels": [], "entities": []}, {"text": "For example, a difficult case occurs when an OOV word consists of characters which have themselves been seen as words; here an automatic segmentor may split the OOV word into individual single-character words.", "labels": [], "entities": []}, {"text": "Typical examples of unseen words include Chinese names, translated foreign names and idioms.", "labels": [], "entities": []}, {"text": "The segmentation of known words can also be ambiguous.", "labels": [], "entities": []}, {"text": "For example, \"\ud97b\udf59\ud97b\udf59\ud97b\udf59\" should be \"\ud97b\udf59 \ud97b\udf59 (here) \ud97b\udf59 (flour)\" in the sentence \"\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\u00d7\ud97b\udf59 \ud97b\udf59\" (flour and rice are expensive here) or \"\ud97b\udf59 (here) \ud97b\udf59\ud97b\udf59 (inside)\" in the sentence \"\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\u00f3\" (it's cold inside here).", "labels": [], "entities": []}, {"text": "The ambiguity can be resolved with information about the neighboring words.", "labels": [], "entities": []}, {"text": "In comparison, for the sentences \"\ud97b\udf59\u00a8\ud97b\udf59\ud97b\udf59\ud97b\udf59\u00ef\ud97b\udf59\u00a8\ud97b\udf59\ud97b\udf59\ud97b\udf59\u00ef\", possible segmentations include \"\ud97b\udf59\u00a8(\ud97b\udf59\u00a8(the discussion) \ud97b\udf59 (will) \ud97b\udf59 (very) \ud97b\udf59\u00ef (be successful)\" and \"\ud97b\udf59\u00a8\ud97b\udf59 (the discussion meeting) \ud97b\udf59(very)\ud97b\udf59\u00ef(be successful)\".", "labels": [], "entities": []}, {"text": "The ambiguity can only be resolved with contextual information outside the sentence.", "labels": [], "entities": []}, {"text": "Human readers often use semantics, contextual information about the document and world knowledge to resolve segmentation ambiguities.", "labels": [], "entities": []}, {"text": "There is no fixed standard for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.6052068769931793}]}, {"text": "Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.749016523361206}]}, {"text": "Also, specific NLP tasks may require different segmentation criteria.", "labels": [], "entities": []}, {"text": "For example, \"\ud97b\udf59\ud97b\udf59\ud97b\udf59 \u00c4\" could be treated as a single word (Bank of Beijing) for machine translation, while it is more naturally segmented into \"\ud97b\udf59\ud97b\udf59 (Beijing) \ud97b\udf59\u00c4 (bank)\" for tasks such as text-to-speech synthesis.", "labels": [], "entities": [{"text": "Bank of Beijing)", "start_pos": 56, "end_pos": 72, "type": "DATASET", "confidence": 0.7798249125480652}, {"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7316453456878662}, {"text": "text-to-speech synthesis", "start_pos": 183, "end_pos": 207, "type": "TASK", "confidence": 0.7242903709411621}]}, {"text": "Therefore, supervised learning with specifically defined training data has become the dominant approach.", "labels": [], "entities": []}, {"text": "Following, the standard approach tosupervised learning for CWS is to treat it as a tagging task.", "labels": [], "entities": [{"text": "CWS", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9124857187271118}]}, {"text": "Tags are assigned to each character in the sentence, indicating whether the character is a singlecharacter word or the start, middle or end of a multicharacter word.", "labels": [], "entities": []}, {"text": "The features are usually confined to a five-character window with the current character in the middle.", "labels": [], "entities": []}, {"text": "In this way, dynamic programming algorithms such as the Viterbi algorithm can be used for decoding.", "labels": [], "entities": []}, {"text": "Several discriminatively trained models have recently been applied to the CWS problem.", "labels": [], "entities": [{"text": "CWS problem", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.9546404182910919}]}, {"text": "Examples include Xue (2003), and; these use maximum entropy (ME) and conditional random field (CRF) models).", "labels": [], "entities": []}, {"text": "An advantage of these models is their flexibility in allowing knowledge from various sources to be encoded as features.", "labels": [], "entities": []}, {"text": "Contextual information plays an important role in word segmentation decisions; especially useful is information about surrounding words.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7677123546600342}]}, {"text": "Consider the sentence \"\u00b9\ud97b\udf59\ud97b\udf59\u00a1\ud97b\udf59\", which can be from \"\u00da\u00b9 (among which) \ud97b\udf59\ud97b\udf59 (foreign) \u00a1\ud97b\udf59 (companies)\", or \"\u00b9\ud97b\udf59 (in China) \ud97b\udf59\u00a1 (foreign companies) \ud97b\udf59 \ud97b\udf59 (business)\".", "labels": [], "entities": []}, {"text": "Note that the five-character window surrounding \"\ud97b\udf59\" is the same in both cases, making the tagging decision for that character difficult given the local window.", "labels": [], "entities": []}, {"text": "However, the correct decision can be made by comparison of the two three-word windows containing this character.", "labels": [], "entities": []}, {"text": "In order to explore the potential of word-based models, we adapt the perceptron discriminative learning algorithm to the CWS problem.", "labels": [], "entities": []}, {"text": "proposed the perceptron as an alternative to the CRF method for HMM-style taggers.", "labels": [], "entities": [{"text": "HMM-style taggers", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7571925520896912}]}, {"text": "However, our model does not map the segmentation problem to a tag sequence learning problem, but defines features on segmented sentences directly.", "labels": [], "entities": []}, {"text": "Hence we use a beam-search decoder during training and testing; our idea is similar to that of who used a beam-search decoder as part of a perceptron parsing model.", "labels": [], "entities": [{"text": "perceptron parsing", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.6633386015892029}]}, {"text": "Our work can also be seen as part of the recent move towards search-based learning methods which do not rely on dynamic programming and are thus able to exploit larger parts of the context for making decisions).", "labels": [], "entities": []}, {"text": "We study several factors that influence the performance of the perceptron word segmentor, including the averaged perceptron method, the size of the beam and the importance of word-based features.", "labels": [], "entities": [{"text": "perceptron word segmentor", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.6063626309235891}]}, {"text": "We compare the accuracy of our final system to the state-of-the-art CWS systems in the literature using the first and second SIGHAN bakeoff data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9996060729026794}, {"text": "SIGHAN bakeoff data", "start_pos": 125, "end_pos": 144, "type": "DATASET", "confidence": 0.7312421003977457}]}, {"text": "Our system is competitive with the best systems, obtaining the highest reported F-scores on a number of the bakeoff corpora.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9835085272789001}]}, {"text": "These results demonstrate the importance of word-based features for CWS.", "labels": [], "entities": [{"text": "CWS", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.932506263256073}]}, {"text": "Furthermore, our approach provides an example of the potential of search-based discriminative training methods for NLP tasks.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 115, "end_pos": 124, "type": "TASK", "confidence": 0.875887930393219}]}], "datasetContent": [{"text": "Two sets of experiments were conducted.", "labels": [], "entities": []}, {"text": "The first, used for development, was based on the part of Chinese Treebank 4 that is not in Chinese Treebank 3 (since CTB3 was used as part of the first bakeoff).", "labels": [], "entities": [{"text": "Chinese Treebank 4", "start_pos": 58, "end_pos": 76, "type": "DATASET", "confidence": 0.9739877184232076}, {"text": "Chinese Treebank 3", "start_pos": 92, "end_pos": 110, "type": "DATASET", "confidence": 0.9685353835423788}]}, {"text": "This corpus contains 240K characters (150K words and 4798 sentences).", "labels": [], "entities": []}, {"text": "80% of the sentences (3813) were randomly chosen for training and the rest (985 sentences) were used as development testing data.", "labels": [], "entities": []}, {"text": "The accuracies and learning curves for the non-averaged and averaged perceptron were compared.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9848232269287109}]}, {"text": "The influence of particular features and the agenda size were also studied.", "labels": [], "entities": []}, {"text": "The second set of experiments used training and testing sets from the first and second international Chinese word segmentation bakeoffs).", "labels": [], "entities": []}, {"text": "The accuracies are compared to other models in the literature.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9873813390731812}]}, {"text": "F-measure is used as the accuracy measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9523347020149231}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9992808699607849}]}, {"text": "Define precision p as the percentage of words in the decoder output that are segmented correctly, and recall r as the percentage of gold standard output words that are correctly segmented by the decoder.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.997309684753418}, {"text": "recall r", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.988731175661087}]}, {"text": "The (balanced) F-measure is 2pr/(p + r).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9741583466529846}]}, {"text": "Open tests measure a model's capability to utilize extra information and domain knowledge, which can lead to improved performance, but since this extra information is not standardized, direct comparison between open test results is less informative.", "labels": [], "entities": []}, {"text": "In this paper, we focus only on the closed test.", "labels": [], "entities": []}, {"text": "However, the perceptron model allows a wide range of features, and so future work will consider how to integrate open resources into our system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: accuracy using non-averaged and averaged perceptron.  P -precision (%), R -recall (%), F -F-measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994152784347534}, {"text": "P -precision", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.9228123227755228}, {"text": "R -recall", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.861346165339152}, {"text": "F -F-measure", "start_pos": 97, "end_pos": 109, "type": "METRIC", "confidence": 0.9166610638300577}]}, {"text": " Table 3: the influence of agenda size.  B -agenda size, Tr -training time (seconds), Seg -testing time (seconds), F -F-measure.", "labels": [], "entities": [{"text": "B -agenda size", "start_pos": 41, "end_pos": 55, "type": "METRIC", "confidence": 0.9483786076307297}, {"text": "Tr -training time", "start_pos": 57, "end_pos": 74, "type": "METRIC", "confidence": 0.9617694318294525}, {"text": "Seg -testing time (seconds)", "start_pos": 86, "end_pos": 113, "type": "METRIC", "confidence": 0.8351606258324215}, {"text": "F -F-measure", "start_pos": 115, "end_pos": 127, "type": "METRIC", "confidence": 0.9239187041918436}]}, {"text": " Table 4: the influence of features. (F: F-measure.  Feature numbers are from Table 1)", "labels": [], "entities": [{"text": "F", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.9677422046661377}]}, {"text": " Table 5. We follow the  format from", "labels": [], "entities": []}, {"text": " Table 5: the accuracies over the first SIGHAN bake- off data.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9917418956756592}, {"text": "SIGHAN bake- off data", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.6297119081020355}]}, {"text": " Table 6: the accuracies over the second SIGHAN  bakeoff data.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.99293053150177}, {"text": "SIGHAN  bakeoff data", "start_pos": 41, "end_pos": 61, "type": "DATASET", "confidence": 0.7602240641911825}]}]}