{"title": [{"text": "Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.9074251353740692}]}], "abstractContent": [{"text": "Automatic sentiment classification has been extensively studied and applied in recent years.", "labels": [], "entities": [{"text": "Automatic sentiment classification", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8099526961644491}]}, {"text": "However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical.", "labels": [], "entities": []}, {"text": "We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7513726353645325}, {"text": "sentiment classifiers", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.8888469040393829}]}, {"text": "First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.974061906337738}, {"text": "structural correspondence learning (SCL)", "start_pos": 67, "end_pos": 107, "type": "TASK", "confidence": 0.7318103412787119}]}, {"text": "Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another.", "labels": [], "entities": []}, {"text": "This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment detection and classification has received considerable attention recently ().", "labels": [], "entities": [{"text": "Sentiment detection and classification", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8495753556489944}]}, {"text": "While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.9686605930328369}]}, {"text": "Research results have been deployed industrially in systems that gauge market reaction and summarize opinion from Web pages, discussion boards, and blogs.", "labels": [], "entities": [{"text": "summarize opinion from Web pages, discussion boards", "start_pos": 91, "end_pos": 142, "type": "TASK", "confidence": 0.8175335973501205}]}, {"text": "With such widely-varying domains, researchers and engineers who build sentiment classification systems need to collect and curate data for each new domain they encounter.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.8870890438556671}]}, {"text": "Even in the case of market analysis, if automatic sentiment classification were to be used across a wide range of domains, the effort to annotate corpora for each domain may become prohibitive, especially since product features changeover time.", "labels": [], "entities": [{"text": "market analysis", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.752391904592514}, {"text": "sentiment classification", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.8192070126533508}]}, {"text": "We envision a scenario in which developers annotate corpora fora small number of domains, train classifiers on those corpora, and then apply them to other similar corpora.", "labels": [], "entities": []}, {"text": "However, this approach raises two important questions.", "labels": [], "entities": []}, {"text": "First, it is well known that trained classifiers lose accuracy when the test data distribution is significantly different from the training data distribution . Second, it is not clear which notion of domain similarity should be used to select domains to annotate that would be good proxies for many other domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9981145858764648}]}, {"text": "We propose solutions to these two questions and evaluate them on a corpus of reviews for four different types of products from Amazon: books, DVDs, electronics, and kitchen appliances 2 . First, we show how to extend the recently proposed structural cor- The dataset will be made available by the authors at publication time.", "labels": [], "entities": []}, {"text": "respondence learning (SCL) domain adaptation algorithm ) for use in sentiment classification.", "labels": [], "entities": [{"text": "respondence learning (SCL) domain adaptation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8021198511123657}, {"text": "sentiment classification", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.9517892599105835}]}, {"text": "A key step in SCL is the selection of pivot features that are used to link the source and target domains.", "labels": [], "entities": [{"text": "SCL", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9809015393257141}]}, {"text": "We suggest selecting pivots based not only on their common frequency but also according to their mutual information with the source labels.", "labels": [], "entities": []}, {"text": "For data as diverse as product reviews, SCL can sometimes misalign features, resulting in degradation when we adapt between domains.", "labels": [], "entities": [{"text": "SCL", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9560180306434631}]}, {"text": "In our second extension we show how to correct misalignments using a very small number of labeled instances.", "labels": [], "entities": []}, {"text": "Second, we evaluate the A-distance) between domains as measure of the loss due to adaptation from one to the other.", "labels": [], "entities": [{"text": "A-distance", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9970752000808716}]}, {"text": "The Adistance can be measured from unlabeled data, and it was designed to take into account only divergences which affect classification accuracy.", "labels": [], "entities": [{"text": "Adistance", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.8123033046722412}, {"text": "classification", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.9472469687461853}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.8812000751495361}]}, {"text": "We show that it correlates well with adaptation loss, indicating that we can use the A-distance to select a subset of domains to label as sources.", "labels": [], "entities": [{"text": "A-distance", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9876707792282104}]}, {"text": "In the next section we briefly review SCL and introduce our new pivot selection method.", "labels": [], "entities": [{"text": "SCL", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.8038524389266968}]}, {"text": "Section 3 describes datasets and experimental method.", "labels": [], "entities": []}, {"text": "Section 4 gives results for SCL and the mutual information method for selecting pivot features.", "labels": [], "entities": [{"text": "SCL", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9801086783409119}]}, {"text": "Section 5 shows how to correct feature misalignments using a small amount of labeled target domain data.", "labels": [], "entities": []}, {"text": "Section 6 motivates the A-distance and shows that it correlates well with adaptability.", "labels": [], "entities": [{"text": "A-distance", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9967505931854248}]}, {"text": "We discuss related work in Section 7 and conclude in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We constructed anew dataset for sentiment domain adaptation by selecting Amazon product reviews for four different product types: books, DVDs, electronics and kitchen appliances.", "labels": [], "entities": [{"text": "sentiment domain adaptation", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.7983950972557068}]}, {"text": "Each review consists of a rating (0-5 stars), a reviewer name and location, a product name, a review title and date, and the review text.", "labels": [], "entities": []}, {"text": "Reviews with rating > 3 were labeled positive, those with rating < 3 were labeled negative, and the rest discarded because their polarity was ambiguous.", "labels": [], "entities": []}, {"text": "After this conversion, we had 1000 positive and 1000 negative examples for each domain, the same balanced composition as the polarity dataset ().", "labels": [], "entities": []}, {"text": "In addition to the labeled data, we included between 3685 (DVDs) and 5945 (kitchen) instances of unlabeled data.", "labels": [], "entities": []}, {"text": "The size of the unlabeled data was limited primarily by the number of reviews we could crawl and download from the Amazon website.", "labels": [], "entities": []}, {"text": "Since we were able to obtain labels for all of the reviews, we also ensured that they were balanced between positive and negative examples, as well.", "labels": [], "entities": []}, {"text": "While the polarity dataset is a popular choice in the literature, we were unable to use it for our task.", "labels": [], "entities": []}, {"text": "Our method requires many unlabeled reviews and despite a large number of IMDB reviews available online, the extensive curation requirements made preparing a large amount of data difficult . For classification, we use linear predictors on unigram and bigram features, trained to minimize the Huber loss with stochastic gradient descent 4 Experiments with SCL and SCL-MI Each labeled dataset was split into a training set of 1600 instances and a test set of 400 instances.", "labels": [], "entities": []}, {"text": "All the experiments use a classifier trained on the training set of one domain and tested on the test set of a possibly different domain.", "labels": [], "entities": []}, {"text": "The baseline is a linear classifier trained without adaptation, while the gold standard is an in-domain classifier trained on the same domain as it is tested.", "labels": [], "entities": []}, {"text": "gives accuracies for all pairs of domain adaptation.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9595708847045898}]}, {"text": "The domains are ordered clockwise from the top left: books, DVDs, electronics, and kitchen.", "labels": [], "entities": []}, {"text": "For each set of bars, the first letter is the source domain and the second letter is the target domain.", "labels": [], "entities": []}, {"text": "The thick horizontal bars are the accuracies of the in-domain classifiers for these domains.", "labels": [], "entities": []}, {"text": "Thus the first set of bars shows that the baseline achieves 72.8% accuracy adapting from DVDs to books.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9990348815917969}]}, {"text": "SCL-MI achieves 79.7% and the in-domain gold standard is 80.4%.", "labels": [], "entities": [{"text": "SCL-MI", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7646876573562622}, {"text": "in-domain gold standard", "start_pos": 30, "end_pos": 53, "type": "METRIC", "confidence": 0.7193520863850912}]}, {"text": "We say that the adaptation loss for the baseline model is 7.6% and the adaptation loss for the SCL-MI model is 0.7%.", "labels": [], "entities": [{"text": "adaptation", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9891757369041443}, {"text": "adaptation", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9898446798324585}]}, {"text": "The relative reduction in error due to adaptation of SCL-MI for this testis 90.8%.", "labels": [], "entities": [{"text": "error", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9963374137878418}]}, {"text": "We can observe from these results that there is a rough grouping of our domains.", "labels": [], "entities": []}, {"text": "Books and DVDs are similar, as are kitchen appliances and electronics, but the two groups are different from one another.", "labels": [], "entities": []}, {"text": "Adapting classifiers from books to DVDs, for instance, is easier than adapting them from books to kitchen appliances.", "labels": [], "entities": [{"text": "Adapting classifiers from books to DVDs", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.9003177483876547}]}, {"text": "We note that when transferring from kitchen to electronics, SCL-MI actually outperforms the in-domain classifier.", "labels": [], "entities": []}, {"text": "This is possible since the unlabeled data may contain information that the in-domain classifier does not have access to.", "labels": [], "entities": []}, {"text": "At illustrates one row of the projection matrix \u03b8 for adapting from books to kitchen appliances; the features on each row appear only in the corresponding domain.", "labels": [], "entities": []}, {"text": "A supervised classifier trained on book reviews cannot assign weight to the kitchen features in the second row of table 2.", "labels": [], "entities": []}, {"text": "In contrast, SCL assigns weight to these features indirectly through the projection matrix.", "labels": [], "entities": []}, {"text": "When we observe the feature \"predictable\" with a negative book review, we update parameters corresponding to the entire projection, including the kitchen-specific features \"poorly designed\" and \"awkward to\".", "labels": [], "entities": []}, {"text": "While some rows of the projection matrix \u03b8 areuseful for classification, SCL can also misalign features.", "labels": [], "entities": [{"text": "classification", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.9663428664207458}]}, {"text": "This causes problems when a projection is discriminative in the source domain but not in the target.", "labels": [], "entities": []}, {"text": "This is the case for adapting from kitchen appliances to books.", "labels": [], "entities": []}, {"text": "Since the book domain is quite broad, many projections in books model topic distinctions such as between religious and political books.", "labels": [], "entities": []}, {"text": "These projections, which are uninformative as to the target label, are put into correspondence with the fewer discriminating projections in the much narrower kitchen domain.", "labels": [], "entities": []}, {"text": "When we adapt from kitchen to books, we assign weight to these uninformative projections, degrading target classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9513052701950073}]}], "tableCaptions": [{"text": " Table 3: For each domain, we show the loss due to transfer", "labels": [], "entities": []}]}