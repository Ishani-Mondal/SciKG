{"title": [{"text": "The utility of parse-derived features for automatic discourse segmentation", "labels": [], "entities": [{"text": "automatic discourse segmentation", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.6676603555679321}]}], "abstractContent": [{"text": "We investigate different feature sets for performing automatic sentence-level discourse segmentation within a general machine learning approach, including features derived from either finite-state or context-free annotations.", "labels": [], "entities": [{"text": "sentence-level discourse segmentation", "start_pos": 63, "end_pos": 100, "type": "TASK", "confidence": 0.6600770453612009}]}, {"text": "We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9961393475532532}]}, {"text": "This counters recent results suggesting that purely finite-state approaches can perform competitively.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse structure annotations have been demonstrated to be of high utility fora number of NLP applications, including automatic text summarization), sentence compression), natural language generation () and question answering ().", "labels": [], "entities": [{"text": "text summarization", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.6789346784353256}, {"text": "sentence compression", "start_pos": 151, "end_pos": 171, "type": "TASK", "confidence": 0.7939191162586212}, {"text": "natural language generation", "start_pos": 174, "end_pos": 201, "type": "TASK", "confidence": 0.64036692182223}, {"text": "question answering", "start_pos": 209, "end_pos": 227, "type": "TASK", "confidence": 0.9216175377368927}]}, {"text": "These annotations include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7259547710418701}]}, {"text": "For example, the tree in shows a sentence-level discourse tree for the string \"Prices have dropped but remain quite high, according to CEO Smith,\" which has three discourse segments, each labeled with either \"Nucleus\" or \"Satellite\" depending on how central the segment is to the coherence of the text.", "labels": [], "entities": []}, {"text": "There area number of corpora annotated with discourse structure, including the well-known RST); the Discourse GraphBank (; and the Penn Discourse Treebank ().", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 131, "end_pos": 154, "type": "DATASET", "confidence": 0.9728244543075562}]}, {"text": "While the annotation approaches differ across these corpora, the requirement of sentence segmentation into  sub-sentential discourse units is shared across all approaches.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.7322345077991486}]}, {"text": "These resources have facilitated research into stochastic models and algorithms for automatic discourse structure annotation in recent years.", "labels": [], "entities": [{"text": "automatic discourse structure annotation", "start_pos": 84, "end_pos": 124, "type": "TASK", "confidence": 0.6356254741549492}]}, {"text": "Using the RST Treebank as training and evaluation data, demonstrated that their automatic sentence-level discourse parsing system could achieve near-human levels of accuracy, if it was provided with manual segmentations and manual parse trees.", "labels": [], "entities": [{"text": "RST Treebank", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.9050076007843018}, {"text": "sentence-level discourse parsing", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.6291484435399374}, {"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9953798055648804}]}, {"text": "Manual segmentation was primarily responsible for this performance boost over their fully automatic system, thus making the case that automatic discourse segmentation is the primary impediment to high accuracy automatic sentence-level discourse structure annotation.", "labels": [], "entities": [{"text": "Manual segmentation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6585253775119781}, {"text": "discourse segmentation", "start_pos": 144, "end_pos": 166, "type": "TASK", "confidence": 0.7506565749645233}, {"text": "sentence-level discourse structure annotation", "start_pos": 220, "end_pos": 265, "type": "TASK", "confidence": 0.551916092634201}]}, {"text": "Their models and algorithm -subsequently packaged together into the publicly available SPADE discourse parser 1 -make use of the output of the Charniak (2000) parser to derive syntactic indicator features for segmentation and discourse parsing.", "labels": [], "entities": [{"text": "SPADE discourse parser", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.6849854191144308}, {"text": "discourse parsing", "start_pos": 226, "end_pos": 243, "type": "TASK", "confidence": 0.6772141009569168}]}, {"text": "Sporleder and also used the RST Treebank as training data for data-driven discourse parsing algorithms, though their focus, in contrast to, was to avoid contextfree parsing and rely exclusively on features in their model that could be derived via finite-state chunkers and taggers.", "labels": [], "entities": [{"text": "RST Treebank", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.8471200168132782}, {"text": "discourse parsing", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7280776500701904}, {"text": "contextfree parsing", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.6736424267292023}]}, {"text": "The annotations that they derive are dis-course \"chunks\", i.e., sentence-level segmentation and non-hierarchical nucleus/span labeling of segments.", "labels": [], "entities": []}, {"text": "They demonstrate that their models achieve comparable results to SPADE without the use of any context-free features.", "labels": [], "entities": []}, {"text": "Once again, segmentation is the part of the process where the automatic algorithms most seriously underperform.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.9841585159301758}]}, {"text": "In this paper we take up the question posed by the results of Sporleder and: how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses?", "labels": [], "entities": [{"text": "accuracy reduction", "start_pos": 95, "end_pos": 113, "type": "METRIC", "confidence": 0.9646323025226593}]}, {"text": "If little accuracy is lost, as their results suggest, then it would make sense to avoid relatively expensive context-free parsing, particularly if the amount of text to be processed is large or if there are real-time processing constraints on the system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988049268722534}, {"text": "context-free parsing", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.4828367829322815}]}, {"text": "If, however, the accuracy loss is substantial, one might choose to avoid context-free parsing only in the most time-constrained scenarios.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9989235997200012}, {"text": "context-free parsing", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.5030100643634796}]}, {"text": "While Sporleder and Lapata (2005) demonstrated that their finite-state system could perform as well as the SPADE system, which uses context-free parse trees, this does not directly answer the question of the utility of context-free derived features for this task.", "labels": [], "entities": []}, {"text": "SPADE makes use of a particular kind of feature from the parse trees, and does not train a general classifier making use of other features beyond the parse-derived indicator features.", "labels": [], "entities": [{"text": "SPADE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5257423520088196}]}, {"text": "As we shall show, its performance is not the highest that can be achieved via context-free parser derived features.", "labels": [], "entities": []}, {"text": "In this paper, we train a classifier using a general machine learning approach and a range of finitestate and context-free derived features.", "labels": [], "entities": []}, {"text": "We investigate the impact on discourse segmentation performance when one feature set is used versus another, in such away establishing the utility of features derived from context-free parses.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.7079743444919586}]}, {"text": "In the course of so doing, we achieve the best reported performance on this task, an absolute F-score improvement of 5.0% over SPADE, which represents a more than 34% relative error rate reduction.", "labels": [], "entities": [{"text": "F-score", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.959180474281311}, {"text": "relative error rate reduction", "start_pos": 167, "end_pos": 196, "type": "METRIC", "confidence": 0.8188070207834244}]}, {"text": "By focusing on segmentation, we provide an approach that is generally applicable to all of the various annotation approaches, given the similarities between the various sentence-level segmentation guidelines.", "labels": [], "entities": []}, {"text": "Given that segmentation has been shown to be a primary impediment to high accuracy sentence-level discourse structure annotation, this represents a large step forward in our ability to automatically parse the discourse structure of text, whatever annotation approach we choose.", "labels": [], "entities": [{"text": "parse the discourse structure of text", "start_pos": 199, "end_pos": 236, "type": "TASK", "confidence": 0.8273126383622488}]}], "datasetContent": [{"text": "Previous research into RST-DT segmentation and parsing has focused on subsets of the 991 sentence test set during evaluation.", "labels": [], "entities": [{"text": "RST-DT segmentation and parsing", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.7621597126126289}, {"text": "991 sentence test set", "start_pos": 85, "end_pos": 106, "type": "DATASET", "confidence": 0.6614275127649307}]}, {"text": "omitted sentences that were not exactly spanned by a subtree of the treebank, so that they could focus on sentence-level discourse parsing.", "labels": [], "entities": [{"text": "sentence-level discourse parsing", "start_pos": 106, "end_pos": 138, "type": "TASK", "confidence": 0.6720723410447439}]}, {"text": "By our count, this eliminates 40 of the 991 sentences in the test set from consideration.", "labels": [], "entities": []}, {"text": "went further and established a smaller subset of 608 sentences, which omitted sentences with only one segment, i.e., sentences which themselves are atomic edus.", "labels": [], "entities": []}, {"text": "Since the primary focus of this paper is on segmentation, there is no strong reason to omit any sentences from the test set, hence our results will evaluate on all 991 test sentences, with two exceptions.", "labels": [], "entities": []}, {"text": "First, in Section 2.3, we compare SPADE results under our configuration with results from Sporleder and in order to establish comparability, and this is done on their 608 sentence subset.", "labels": [], "entities": [{"text": "SPADE", "start_pos": 34, "end_pos": 39, "type": "TASK", "confidence": 0.9196685552597046}]}, {"text": "Second, in Section 3.2, we investigate feeding our segmentation into the SPADE system, in order to evaluate the impact of segmentation improvements on their sentence-level discourse parsing performance.", "labels": [], "entities": [{"text": "sentence-level discourse parsing", "start_pos": 157, "end_pos": 189, "type": "TASK", "confidence": 0.6137561003367106}]}, {"text": "For those trials, the 951 sentence subset from Soricut and Marcu is used.", "labels": [], "entities": [{"text": "Soricut and Marcu", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.7204817533493042}]}, {"text": "All other trials use the full 991 sentence test set.", "labels": [], "entities": [{"text": "991 sentence test set", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.5836574882268906}]}, {"text": "Segmentation evaluation is done with precision, recall and F1-score of segmentation boundaries.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9997348189353943}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9996304512023926}, {"text": "F1-score", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9994223117828369}]}, {"text": "Given a word string w 1 . .", "labels": [], "entities": []}, {"text": "wk , we can index word boundaries from 0 to k, so that each word w i falls between boundaries i\u22121 and i.", "labels": [], "entities": []}, {"text": "For sentence-based segmentation, indices 0 and k, representing the beginning and end of the string, are known to be segment boundaries.", "labels": [], "entities": [{"text": "sentence-based segmentation", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.7229238450527191}]}, {"text": "Hence Soricut and Marcu evaluate with respect to sentence internal segmentation boundaries, i.e., with indices j such that 0<j<k fora sentence of length k.", "labels": [], "entities": []}, {"text": "Let g be the number of sentence-internal segmentation boundaries in the gold standard, t the number of sentence-internal segmentation boundaries in the system output, and m the number of correct sentence-internal segmentation boundaries in the system output.", "labels": [], "entities": []}, {"text": "Then In Sporleder and Lapata (2005), they were primarily interested in labeled segmentation, where the segment initial boundary was labeled with the segment type.", "labels": [], "entities": [{"text": "labeled segmentation", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.785124808549881}]}, {"text": "In such a scenario, the boundary at index 0 is no longer known, hence their evaluation included those boundaries, even when reporting unlabeled results.", "labels": [], "entities": []}, {"text": "Thus, in section 2.3, for comparison with reported results in, our F1-score is defined accordingly, i.e., seg- mentation boundaries j such that 0 \u2264 j < k.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9988501071929932}]}, {"text": "In addition, we will report unlabeled bracketing precision, recall and F1-score, as defined in the PARSEVAL metrics and evaluated via the widely used evalb package.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9536810517311096}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9995157718658447}, {"text": "F1-score", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9991443157196045}, {"text": "PARSEVAL", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.615607500076294}]}, {"text": "We also use evalb when reporting labeled and unlabeled discourse parsing results in Section 3.2.", "labels": [], "entities": [{"text": "reporting labeled and unlabeled discourse parsing", "start_pos": 23, "end_pos": 72, "type": "TASK", "confidence": 0.5801982879638672}]}, {"text": "We performed a number of experiments to determine the relative utility of features derived from full context-free syntactic parses and those derived solely from shallow finite-state tagging.", "labels": [], "entities": []}, {"text": "Our primary concern is with intra-sentential discourse segmentation, but we are also interested in how much the improved segmentation helps discourse parsing.", "labels": [], "entities": [{"text": "intra-sentential discourse segmentation", "start_pos": 28, "end_pos": 67, "type": "TASK", "confidence": 0.7639349102973938}, {"text": "discourse parsing", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.6923497468233109}]}, {"text": "The syntactic parser we use for all context-free syntactic parses used in either SPADE or our classifier is the Charniak parser with reranking, as described in.", "labels": [], "entities": []}, {"text": "The Charniak parser and reranker were trained on the sections of the Penn Treebank not included in the RST-DT test set.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.9948231875896454}, {"text": "RST-DT test set", "start_pos": 103, "end_pos": 118, "type": "DATASET", "confidence": 0.9485073288281759}]}, {"text": "All statistical significance testing is done via the stratified shuffling test).", "labels": [], "entities": []}, {"text": "presents segmentation results for SPADE and four versions of our classifier.", "labels": [], "entities": []}, {"text": "The \"Basic finitestate\" system uses only finite-state sequence features as defined in Section 2.5.1, while the \"Full finite-state\" also includes the finite-state approximation features from Section 2.5.3.", "labels": [], "entities": []}, {"text": "The \"Context-free\" system uses the SPADE-inspired features detailed in Section 2.5.2, but none of the features from Sections 2.5.1 or 2.5.3.", "labels": [], "entities": []}, {"text": "Finally, the \"All features\" section includes features from all three sections.", "labels": [], "entities": []}, {"text": "Note that the full finite-state system is considerably better than the basic finite-state system, demonstrating the utility of these approximations of the SPADE-like context-free features.", "labels": [], "entities": []}, {"text": "The performance of the resulting \"Full\" finite-state system is not statistically significantly different from that of SPADE (p=0.193), despite no reliance on features derived from context-free parses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Segmentation results on all 991 sentences in the RST-DT test set. Segment boundary accuracy is for sentence internal", "labels": [], "entities": [{"text": "RST-DT test set", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.8417523503303528}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.7325565814971924}]}, {"text": " Table 3: Discourse parsing results on the 951 sentence Sori-", "labels": [], "entities": [{"text": "Discourse parsing", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8586834967136383}]}]}