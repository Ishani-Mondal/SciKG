{"title": [{"text": "Conditional Modality Fusion for Coreference Resolution", "labels": [], "entities": [{"text": "Conditional Modality Fusion", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8263987501462301}, {"text": "Coreference Resolution", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.9737018942832947}]}], "abstractContent": [{"text": "Non-verbal modalities such as gesture can improve processing of spontaneous spoken language.", "labels": [], "entities": [{"text": "processing of spontaneous spoken language", "start_pos": 50, "end_pos": 91, "type": "TASK", "confidence": 0.7511423230171204}]}, {"text": "For example, similar hand gestures tend to predict semantic similarity, so features that quantify gestural similarity can improve semantic tasks such as coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 153, "end_pos": 175, "type": "TASK", "confidence": 0.9477841258049011}]}, {"text": "However, not all hand movements are informative gestures; psychological research has shown that speakers are more likely to gesture meaningfully when their speech is ambiguous.", "labels": [], "entities": []}, {"text": "Ideally, one would attend to gesture only in such circumstances , and ignore other hand movements.", "labels": [], "entities": []}, {"text": "We present conditional modality fusion, which formalizes this intuition by treating the informativeness of gesture as a hidden variable to be learned jointly with the class label.", "labels": [], "entities": [{"text": "conditional modality fusion", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.664020965496699}]}, {"text": "Applied to coreference resolution, conditional modality fusion significantly outperforms both early and late modality fusion, which are current techniques for modality combination.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.9737115502357483}, {"text": "conditional modality fusion", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.6524141728878021}]}], "introductionContent": [{"text": "Non-verbal modalities such as gesture and prosody can increase the robustness of NLP systems to the inevitable disfluency of spontaneous speech.", "labels": [], "entities": []}, {"text": "For example, consider the following excerpt from a dialogue in which the speaker describes a mechanical device: \"So this moves up, and it -everything moves up.", "labels": [], "entities": []}, {"text": "And this top one clears this area here, and goes all the way up to the top.\"", "labels": [], "entities": []}, {"text": "The references in this passage are difficult to disambiguate, but the gestures shown in make the meaning more clear.", "labels": [], "entities": []}, {"text": "However, non-verbal modalities are often noisy, and their interactions with speech are complex.", "labels": [], "entities": []}, {"text": "Gesture, for example, is sometimes communicative, but other times merely distracting.", "labels": [], "entities": [{"text": "Gesture", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.8762105703353882}]}, {"text": "While people have little difficulty distinguishing between meaningful gestures and irrelevant hand motions (e.g., selftouching, adjusting glasses), NLP systems maybe confused by such seemingly random movements.", "labels": [], "entities": []}, {"text": "Our goal is to include non-verbal features only in the specific cases when they are helpful and necessary.", "labels": [], "entities": []}, {"text": "We present a model that learns in an unsupervised fashion when non-verbal features are useful, allowing it to gate the contribution of those features.", "labels": [], "entities": []}, {"text": "The relevance of the non-verbal features is treated as a hidden variable, which is learned jointly with the class label in a conditional model.", "labels": [], "entities": []}, {"text": "We demonstrate that this improves performance on binary coreference resolution, the task of determining whether a noun phrases refers to a single semantic entity.", "labels": [], "entities": [{"text": "binary coreference resolution", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.6622726221879324}]}, {"text": "Conditional modality fusion yields a relative increase of 73% in the contribution of hand-gesture features.", "labels": [], "entities": []}, {"text": "The model is not specifically tailored to gesturespeech integration, and may also be applicable to other non-verbal modalities.", "labels": [], "entities": [{"text": "gesturespeech integration", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.8004243969917297}]}], "datasetContent": [{"text": "Dataset Our dataset consists of sixteen short dialogues, in which participants explained the behavior 356 of mechanical devices to a friend.", "labels": [], "entities": []}, {"text": "There are nine different pairs of participants; each contributed two dialogues, with two thrown out due to recording errors.", "labels": [], "entities": []}, {"text": "One participant, the \"speaker,\" saw a short video describing the function of the device prior to the dialogue; the other participant was tested on comprehension of the device's behavior after the dialogue.", "labels": [], "entities": []}, {"text": "The speaker was given a pre-printed diagram to aid in the discussion.", "labels": [], "entities": []}, {"text": "For simplicity, only the speaker's utterances were included in these experiments.", "labels": [], "entities": []}, {"text": "The dialogues were limited to three minutes in duration, and most of the participants used the entire allotted time.", "labels": [], "entities": []}, {"text": "\"Markable\" noun phrases -those that are permitted to participate in coreference relations -were annotated by the first author, in accordance with the MUC task definition.", "labels": [], "entities": [{"text": "MUC task definition", "start_pos": 150, "end_pos": 169, "type": "DATASET", "confidence": 0.6927566727002462}]}, {"text": "A total of 1141 \"markable\" NPs were transcribed, roughly half the size of the MUC6 development set, which includes 2072 markable NPs over 30 documents.", "labels": [], "entities": [{"text": "MUC6 development set", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.9613240957260132}]}, {"text": "Evaluation metric Coreference resolution is often performed in two phases: a binary classification phase, in which the likelihood of coreference for each pair of noun phrases is assessed; and a partitioning phase, in which the clusters of mutually-coreferring NPs are formed, maximizing some global criterion).", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.8847785592079163}]}, {"text": "Our model does not address the formation of nounphrase clusters, but only the question of whether each pair of noun phrases in the document corefer.", "labels": [], "entities": []}, {"text": "Consequently, we evaluate only the binary classification phase, and report results in terms of the area under the ROC curve (AUC).", "labels": [], "entities": [{"text": "ROC curve (AUC)", "start_pos": 114, "end_pos": 129, "type": "METRIC", "confidence": 0.9211007237434388}]}, {"text": "As the small size of the corpus did not permit dedicated test and development sets, results are computed using leave-oneout cross-validation, with one fold for each of the sixteen documents in the corpus.", "labels": [], "entities": []}, {"text": "Baselines Three types of baselines were compared to our conditional modality fusion (CMF) technique: \u2022 Early fusion.", "labels": [], "entities": [{"text": "conditional modality fusion (CMF)", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.8004322548707327}, {"text": "Early fusion", "start_pos": 103, "end_pos": 115, "type": "TASK", "confidence": 0.7005141079425812}]}, {"text": "The early fusion baseline includes all features in a single vector, ignoring modality.", "labels": [], "entities": []}, {"text": "This is equivalent to standard maximum-entropy classification.", "labels": [], "entities": []}, {"text": "Early fusion is implemented with a conditionally-trained linear classifier; it uses the same code as the CMF model, but always includes all features.", "labels": [], "entities": []}, {"text": "The late fusion baselines train separate classifiers for gesture and speech, and then combine their posteriors.", "labels": [], "entities": []}, {"text": "The modalityspecific classifiers are conditionally-trained linear models, and again use the same code as the CMF model.", "labels": [], "entities": []}, {"text": "For simplicity, a parameter sweep identifies the interpolation weights that maximize performance on the test set.", "labels": [], "entities": []}, {"text": "Thus, it is likely that these results somewhat overestimate the performance of these baseline models.", "labels": [], "entities": []}, {"text": "We report results for both additive and multiplicative combination of posteriors.", "labels": [], "entities": []}, {"text": "These baselines include the features from only a single modality, and again build a conditionally-trained linear classifier.", "labels": [], "entities": []}, {"text": "Implementation uses the same code as the CMF model, but weights on features outside the target modality are forced to zero.", "labels": [], "entities": []}, {"text": "Although a comparison with existing state-of-theart coreference systems would be ideal, all such available systems use verbal features that are inapplicable to our dataset, such as punctuation, capitalization, and gazetteers.", "labels": [], "entities": []}, {"text": "The verbal features that we have included area representative sample from the literature (e.g.,).", "labels": [], "entities": []}, {"text": "The \"no fusion, verbal features only\" baseline thus provides a reasonable representation of prior work on coreference, by applying a maximum-entropy classifier to this set of typical verbal features.", "labels": [], "entities": [{"text": "coreference", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.9639152884483337}]}, {"text": "Parameter tuning Continuous features are binned separately for each cross-validation fold, using only the training data.", "labels": [], "entities": [{"text": "Parameter", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8636747598648071}]}, {"text": "The regularization constant is selected by cross-validation within each training subset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results, in terms of areas under the ROC  curve", "labels": [], "entities": [{"text": "ROC", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.8121793270111084}]}]}