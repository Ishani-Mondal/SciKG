{"title": [{"text": "Transductive learning for statistical machine translation", "labels": [], "entities": [{"text": "Transductive learning", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8802003562450409}, {"text": "statistical machine translation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.7426693836847941}]}], "abstractContent": [{"text": "Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text in the target language.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6259691715240479}]}, {"text": "In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality.", "labels": [], "entities": []}, {"text": "We propose several algorithms with this aim, and present the strengths and weaknesses of each one.", "labels": [], "entities": []}, {"text": "We present detailed experimental evaluations on the French-English EuroParl data set and on data from the NIST Chinese-English large-data track.", "labels": [], "entities": [{"text": "French-English EuroParl data set", "start_pos": 52, "end_pos": 84, "type": "DATASET", "confidence": 0.802030399441719}, {"text": "NIST Chinese-English large-data track", "start_pos": 106, "end_pos": 143, "type": "DATASET", "confidence": 0.9457718580961227}]}, {"text": "We show a significant improvement in translation quality on both tasks.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9719916582107544}]}], "introductionContent": [{"text": "In statistical machine translation (SMT), translation is modeled as a decision process.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.7982759972413381}]}, {"text": "The goal is to find the translation t of source sentence s which maximizes the posterior probability: This decomposition of the probability yields two different statistical models which can be trained independently of each other: the translation model p(s | t) and the target language model p(t).", "labels": [], "entities": []}, {"text": "State-of-the-art SMT systems are trained on large collections of text which consist of bilingual corpora (to learn the parameters of p(s | t)), and of monolingual target language corpora (for p(t)).", "labels": [], "entities": [{"text": "SMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9925899505615234}]}, {"text": "It has been shown that adding large amounts of target language text improves translation quality considerably.", "labels": [], "entities": [{"text": "translation", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.961542546749115}]}, {"text": "However, the availability of monolingual corpora in the source language does not help improve the system's performance.", "labels": [], "entities": []}, {"text": "We will show how such corpora can be used to achieve higher translation quality.", "labels": [], "entities": []}, {"text": "Even if large amounts of bilingual text are given, the training of the statistical models usually suffers from sparse data.", "labels": [], "entities": []}, {"text": "The number of possible events, i.e. phrase pairs or pairs of subtrees in the two languages, is too big to reliably estimate a probability distribution over such pairs.", "labels": [], "entities": []}, {"text": "Another problem is that for many language pairs the amount of available bilingual text is very limited.", "labels": [], "entities": []}, {"text": "In this work, we will address this problem and propose a general framework to solve it.", "labels": [], "entities": []}, {"text": "Our hypothesis is that adding information from source language text can also provide improvements.", "labels": [], "entities": []}, {"text": "Unlike adding target language text, this hypothesis is a natural semi-supervised learning problem.", "labels": [], "entities": []}, {"text": "To tackle this problem, we propose algorithms for transductive semi-supervised learning.", "labels": [], "entities": [{"text": "transductive semi-supervised learning", "start_pos": 50, "end_pos": 87, "type": "TASK", "confidence": 0.6878110667069753}]}, {"text": "By transductive, we mean that we repeatedly translate sentences from the development set or test set and use the generated translations to improve the performance of the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 170, "end_pos": 173, "type": "TASK", "confidence": 0.9913122057914734}]}, {"text": "Note that the evaluation step is still done just once at the end of our learning process.", "labels": [], "entities": []}, {"text": "In this paper, we show that such an approach can lead to better translations despite the fact that the development and test data are typically much smaller in size than typical training data for SMT systems.", "labels": [], "entities": [{"text": "translations", "start_pos": 64, "end_pos": 76, "type": "TASK", "confidence": 0.970422089099884}, {"text": "SMT", "start_pos": 195, "end_pos": 198, "type": "TASK", "confidence": 0.9932411909103394}]}, {"text": "Transductive learning can be seen as a means to adapt the SMT system to anew type of text.", "labels": [], "entities": [{"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.990013837814331}]}, {"text": "Say a system trained on newswire is used to translate weblog texts.", "labels": [], "entities": [{"text": "translate weblog texts", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.871078372001648}]}, {"text": "The proposed method adapts the trained models to the style and domain of the new input.", "labels": [], "entities": []}, {"text": "to Canadian universities for research and education purposes.", "labels": [], "entities": []}, {"text": "We provide a basic description here; fora detailed description see ( ).", "labels": [], "entities": []}, {"text": "The models (or features) which are employed by the decoder are: (a) one or several phrase table(s), which model the translation direction p(s | t), (b) one or several n-gram language model(s) trained with the SRILM toolkit; in the experiments reported here, we used 4-gram models on the NIST data, and a trigram model on EuroParl, (c) a distortion model which assigns a penalty based on the number of source words which are skipped when generating anew target phrase, and (d) a word penalty.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 209, "end_pos": 222, "type": "DATASET", "confidence": 0.840951144695282}, {"text": "NIST data", "start_pos": 287, "end_pos": 296, "type": "DATASET", "confidence": 0.9894606471061707}, {"text": "EuroParl", "start_pos": 321, "end_pos": 329, "type": "DATASET", "confidence": 0.9898255467414856}]}, {"text": "These different models are combined loglinearly.", "labels": [], "entities": []}, {"text": "Their weights are optimized w.r.t.", "labels": [], "entities": []}, {"text": "BLEU score using the algorithm described in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.983325719833374}]}, {"text": "This is done on a development corpus which we will call dev1 in this paper.", "labels": [], "entities": []}, {"text": "The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm.", "labels": [], "entities": []}, {"text": "After the main decoding step, rescoring with additional models is performed.", "labels": [], "entities": []}, {"text": "The system generates a 5,000-best list of alternative translations for each source sentence.", "labels": [], "entities": []}, {"text": "These lists are rescored with the following models: (a) the different models used in the decoder which are described above, (b) two different features based on IBM Model 1 (, (c) posterior probabilities for words, phrases, n-grams, and sentence length (, all calculated over the Nbest list and using the sentence probabilities which the baseline system assigns to the translation hypotheses.", "labels": [], "entities": []}, {"text": "The weights of these additional models and of the decoder models are again optimized to maximize BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9827663600444794}]}, {"text": "This is performed on a second development corpus, dev2.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the generated translations using three different evaluation metrics: BLEU score (), mWER (multi-reference word error rate), and mPER (multi-reference positionindependent word error rate) ().", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9859561920166016}, {"text": "multi-reference word error rate)", "start_pos": 103, "end_pos": 135, "type": "METRIC", "confidence": 0.6686177432537079}, {"text": "mPER (multi-reference positionindependent word error rate)", "start_pos": 141, "end_pos": 199, "type": "METRIC", "confidence": 0.6348478011786938}]}, {"text": "Note that BLEU score measures quality, whereas mWER and mPER measure translation errors.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9830208420753479}, {"text": "quality", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9589082598686218}]}, {"text": "We will present 95%-confidence intervals for the baseline system which are calculated using bootstrap resampling.", "labels": [], "entities": []}, {"text": "The metrics are calculated w.r.t. one and four English references: the EuroParl data comes with one reference, the NIST 2004 evaluation set and the NIST section of the 2006 evaluation set are provided with four references each, whereas the GALE section of the 2006 evaluation set comes with one reference only.", "labels": [], "entities": [{"text": "EuroParl data", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9941686391830444}, {"text": "NIST 2004 evaluation set", "start_pos": 115, "end_pos": 139, "type": "DATASET", "confidence": 0.9760794937610626}, {"text": "NIST section of the 2006 evaluation set", "start_pos": 148, "end_pos": 187, "type": "DATASET", "confidence": 0.7314900628158024}, {"text": "GALE section of the 2006 evaluation set", "start_pos": 240, "end_pos": 279, "type": "DATASET", "confidence": 0.692818147795541}]}, {"text": "This results in much lower BLEU scores and higher error rates for the translations of the GALE set (see Section 4.2).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9985052347183228}, {"text": "error", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9667996168136597}, {"text": "GALE set", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.7823566794395447}]}, {"text": "Note that these values do not indicate lower translation quality, but are simply a result of using only one reference.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Translation quality using an additional  adapted phrase table trained on the dev/test sets.  Different selection and scoring methods. NIST  Chinese-English, best results printed in boldface.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9576928615570068}, {"text": "NIST  Chinese-English", "start_pos": 144, "end_pos": 165, "type": "DATASET", "confidence": 0.931291788816452}]}, {"text": " Table 5: Translation quality using an additional  phrase table trained on monolingual Chinese news  data. Selection step using threshold on confidence  scores. NIST Chinese-English.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9643594622612}, {"text": "NIST Chinese-English", "start_pos": 161, "end_pos": 181, "type": "DATASET", "confidence": 0.9523537755012512}]}]}