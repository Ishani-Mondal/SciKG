{"title": [{"text": "Sparse Information Extraction: Unsupervised Language Models to the Rescue", "labels": [], "entities": [{"text": "Sparse Information Extraction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7028942505518595}]}], "abstractContent": [{"text": "Even in a massive corpus such as the Web, a substantial fraction of extractions appear infrequently.", "labels": [], "entities": []}, {"text": "This paper shows how to assess the correctness of sparse extractions by utilizing unsupervised language models.", "labels": [], "entities": []}, {"text": "The REALM system, which combines HMM-based and n-gram-based language models, ranks candidate extractions by the likelihood that they are correct.", "labels": [], "entities": [{"text": "REALM", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.8402700424194336}]}, {"text": "Our experiments show that REALM reduces extraction error by 39%, on average, when compared with previous work.", "labels": [], "entities": [{"text": "REALM", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.91158127784729}, {"text": "error", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.5956497192382812}]}, {"text": "Because REALM pre-computes language models based on its corpus and does not require any hand-tagged seeds, it is far more scalable than approaches that learn models for each individual relation from hand-tagged data.", "labels": [], "entities": []}, {"text": "Thus, REALM is ideally suited for open information extraction where the relations of interest are not specified in advance and their number is potentially vast.", "labels": [], "entities": [{"text": "open information extraction", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6214416027069092}]}], "introductionContent": [{"text": "Information Extraction (IE) from text is far from infallible.", "labels": [], "entities": [{"text": "Information Extraction (IE) from text", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8461867698601314}]}, {"text": "In response, researchers have begun to exploit the redundancy in massive corpora such as the Web in order to assess the veracity of extractions (e.g.,).", "labels": [], "entities": []}, {"text": "In essence, such methods utilize extraction patterns to generate candidate extractions (e.g., \"Istanbul\") and then assess each candidate by computing co-occurrence statistics between the extraction and words or phrases indicative of class membership (e.g., \"cities such as\").", "labels": [], "entities": []}, {"text": "However, Zipf's Law governs the distribution of extractions.", "labels": [], "entities": []}, {"text": "Thus, even the Web has limited redundancy for less prominent instances of relations.", "labels": [], "entities": []}, {"text": "Indeed, 50% of the extractions in the data sets employed by ) appeared only once.", "labels": [], "entities": []}, {"text": "As a result,s model, and related methods, had noway of assessing which extraction is more likely to be correct for fully half of the extractions.", "labels": [], "entities": []}, {"text": "This problem is particularly acute when moving beyond unary relations.", "labels": [], "entities": []}, {"text": "We refer to this challenge as the task of assessing sparse extractions.", "labels": [], "entities": [{"text": "assessing sparse extractions", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.6111825108528137}]}, {"text": "This paper introduces the idea that language modeling techniques such as n-gram statistics) and HMMs can be used to effectively assess sparse extractions.", "labels": [], "entities": []}, {"text": "The paper introduces the REALM system, and highlights its unique properties.", "labels": [], "entities": [{"text": "REALM", "start_pos": 25, "end_pos": 30, "type": "TASK", "confidence": 0.7419591546058655}]}, {"text": "Notably, REALM does not require any hand-tagged seeds, which enables it to scale to Open IE-extraction where the relations of interest are not specified in advance, and their number is potentially vast (.", "labels": [], "entities": []}, {"text": "REALM is based on two key hypotheses.", "labels": [], "entities": [{"text": "REALM", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.8575648665428162}]}, {"text": "The KnowItAll hypothesis is that extractions that occur more frequently in distinct sentences in the corpus are more likely to be correct.", "labels": [], "entities": []}, {"text": "For example, the hypothesis suggests that the argument pair (Giuliani, New York) is relatively likely to be appropriate for the Mayor relation, simply because this pair is extracted for the Mayor relation relatively frequently.", "labels": [], "entities": []}, {"text": "Second, we employ an instance of the distributional hypothesis, which 696 can be phrased as follows: different instances of the same semantic relation tend to appear in similar textual contexts.", "labels": [], "entities": []}, {"text": "We assess sparse extractions by comparing the contexts in which they appear to those of more common extractions.", "labels": [], "entities": []}, {"text": "Sparse extractions whose contexts are more similar to those of common extractions are judged more likely to be correct based on the conjunction of the KnowItAll and the distributional hypotheses.", "labels": [], "entities": []}, {"text": "The contributions of the paper are as follows: \u2022 The paper introduces the insight that the subfield of language modeling provides unsupervised methods that can be leveraged to assess sparse extractions.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.7303130626678467}]}, {"text": "These methods are more scalable than previous assessment techniques, and require no hand tagging whatsoever.", "labels": [], "entities": [{"text": "hand tagging", "start_pos": 84, "end_pos": 96, "type": "TASK", "confidence": 0.6959463506937027}]}, {"text": "\u2022 The paper introduces an HMM-based technique for checking whether two arguments are of the proper type fora relation.", "labels": [], "entities": []}, {"text": "\u2022 The paper introduces a relational n-gram model for the purpose of determining whether a sentence that mentions multiple arguments actually expresses a particular relationship between them.", "labels": [], "entities": []}, {"text": "\u2022 The paper introduces a novel languagemodeling system called REALM that combines both HMM-based models and relational ngram models, and shows that REALM reduces error by an average of 39% over previous methods, when applied to sparse extraction data.", "labels": [], "entities": [{"text": "error", "start_pos": 162, "end_pos": 167, "type": "METRIC", "confidence": 0.9808541536331177}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the IE assessment task, and describes the REALM system in detail.", "labels": [], "entities": [{"text": "IE assessment task", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.907082219918569}, {"text": "REALM", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.50376957654953}]}, {"text": "Section 3 reports on our experimental results followed by a discussion of related work in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we conclude with a discussion of scalability and with directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes our experiments on IE assessment for sparse data.", "labels": [], "entities": [{"text": "IE assessment", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9887528717517853}]}, {"text": "We start by describing our experimental methodology, and then present our results.", "labels": [], "entities": []}, {"text": "The first experiment tests the hypothesis that HMM-T outperforms an n-gram-based method on the task of type checking.", "labels": [], "entities": [{"text": "type checking", "start_pos": 103, "end_pos": 116, "type": "TASK", "confidence": 0.9360478222370148}]}, {"text": "The second experiment tests the hypothesis that REALM outperforms multiple approaches from previous work, and also outperforms each of its HMM-T and REL-GRAMS components taken in isolation.", "labels": [], "entities": [{"text": "REALM", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.829940676689148}]}, {"text": "The corpus used for our experiments consisted of a sample of sentences taken from Web pages.", "labels": [], "entities": []}, {"text": "From an initial crawl of nine million Web pages, we selected sentences containing relations between proper nouns.", "labels": [], "entities": []}, {"text": "The resulting text corpus consisted of about three million sentences, and was tokenized as described in Section 2.", "labels": [], "entities": []}, {"text": "For tractability, before and after performing tokenization, we replaced each token occurring fewer than five times in the corpus with one of two \"unknown word\" markers (one for capitalized words, and one for uncapitalized words).", "labels": [], "entities": []}, {"text": "This preprocessing resulted in a corpus containing about sixty-five million total tokens, and 214,787 unique tokens.", "labels": [], "entities": []}, {"text": "We evaluated performance on four relations: Conquered, Founded, Headquartered, and Merged.", "labels": [], "entities": []}, {"text": "These four relations were chosen because they typically take proper nouns as arguments, and included a large number of sparse extractions.", "labels": [], "entities": []}, {"text": "For each relation R, the candidate extraction list ER was obtained using TEXTRUNNER (.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9962992072105408}]}, {"text": "TEXTRUNNER is an IE system that computes an index of all extracted relationships it recognizes, in the form of (object, predicate, object) triples.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.7446833252906799}]}, {"text": "For each of our target relations, we executed a single query to the TEXTRUNNER index for extractions whose predicate contained a phrase indicative of the relation (e.g., \"founded by\", \"headquartered in\"), and the results formed our extraction list.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.963405191898346}]}, {"text": "For each relation, the 10 most frequent extractions served as bootstrapped seeds.", "labels": [], "entities": []}, {"text": "All of the non-seed extractions were sparse (no argument pairs were extracted more than twice fora given relation).", "labels": [], "entities": []}, {"text": "These test sets contained a total of 361 extractions.", "labels": [], "entities": []}, {"text": "As discussed in Section 2.2, on sparse data HMM-T has the potential to outperform type checking methods that rely on textual similarities of context vectors.", "labels": [], "entities": [{"text": "type checking", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.9346539974212646}]}, {"text": "To evaluate this claim, we tested the HMM-T system against an N-GRAMS type checking method on the task of type-checking the arguments to a relation.", "labels": [], "entities": []}, {"text": "The N-GRAMS method compares the context vectors of extractions in the same way as the REL-GRAMS method described in Section 2.3, but is not relational (N-GRAMS considers the distribution of each extraction argument independently, similar to HMM-T).", "labels": [], "entities": []}, {"text": "We tagged an extraction as type correct iff both arguments were valid for the relation, ignoring whether the relation held between the arguments.", "labels": [], "entities": []}, {"text": "The results of our type checking experiments are shown insured in missing area under the precision/recall curve) by 46%.", "labels": [], "entities": [{"text": "type checking", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.9432169198989868}, {"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9992403984069824}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.7867393493652344}]}, {"text": "The performance difference on each relation is statistically significant (p < 0.01, twosampled t-test), using the methodology for measuring the standard deviation of area under the precision/recall curve given in ().", "labels": [], "entities": [{"text": "precision", "start_pos": 181, "end_pos": 190, "type": "METRIC", "confidence": 0.9983484745025635}, {"text": "recall", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.7906737923622131}]}, {"text": "N-GRAMS, like REL-GRAMS, employs the BM-25 metric to measure distributional similarity between extractions and seeds.", "labels": [], "entities": [{"text": "BM-25 metric", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.7299267053604126}]}, {"text": "Replacing BM-25 with cosine distance cuts HMM-T's advantage over N-GRAMS, but HMM-T's error rate is still 23% lower on average.", "labels": [], "entities": [{"text": "error rate", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9775680005550385}]}, {"text": "The REALM system combines the type checking and relation assessment components to assess extractions.", "labels": [], "entities": [{"text": "REALM", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.4742944538593292}, {"text": "type checking", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.8930430710315704}]}, {"text": "Here, we test the ability of REALM to improve the ranking of a state of the art IE system, TEXTRUNNER.", "labels": [], "entities": [{"text": "REALM", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9197263717651367}, {"text": "TEXTRUNNER", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.8428337574005127}]}, {"text": "For these experiments, we evaluate REALM against the TEXTRUNNER frequencybased ordering, a pattern-learning approach, and the HMM-T and REL-GRAMS components taken in isolation.", "labels": [], "entities": [{"text": "REALM", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9880567789077759}, {"text": "TEXTRUNNER frequencybased ordering", "start_pos": 53, "end_pos": 87, "type": "METRIC", "confidence": 0.9223911364873251}, {"text": "REL-GRAMS", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.948932945728302}]}, {"text": "The TEXTRUNNER frequency-based ordering ranks extractions in decreasing order of their extraction frequency, and importantly, for our task this ordering is essentially equivalent to that produced by the \"Urns\" ( ) and Pointwise Mutual Information ( ) approaches employed in previous work.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9545786380767822}]}, {"text": "The pattern-learning approach, denoted as PL, is modeled after Snowball).", "labels": [], "entities": [{"text": "Snowball", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9906417727470398}]}, {"text": "The algorithm and parameter settings for PL were those manually tuned for the Headquartered relation in previous work.", "labels": [], "entities": [{"text": "Headquartered relation", "start_pos": 78, "end_pos": 100, "type": "DATASET", "confidence": 0.8854439556598663}]}, {"text": "A sensitivity analysis of these parameters indicated that the re-701  sults are sensitive to the parameter settings.", "labels": [], "entities": []}, {"text": "However, we found no parameter settings that performed significantly better, and many settings performed significantly worse.", "labels": [], "entities": []}, {"text": "As such, we believe our results reasonably reflect the performance of a pattern learning system on this task.", "labels": [], "entities": []}, {"text": "Because PL performs relation assessment, we also attempted combining PL with HMM-T in a hybrid method (PL+ HMM-T) analogous to REALM.", "labels": [], "entities": [{"text": "relation assessment", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.9252225756645203}]}, {"text": "The results of these experiments are shown in Table 2.", "labels": [], "entities": []}, {"text": "REALM outperforms the TEXTRUNNER and PL baselines for all relations, and reduces the missing area under the curve by an average of 39% relative to the strongest baseline.", "labels": [], "entities": [{"text": "REALM", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9916583895683289}, {"text": "TEXTRUNNER", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9901286959648132}, {"text": "PL", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.8708101511001587}]}, {"text": "The performance differences between REALM and TEXTRUNNER are statistically significant for all relations, as are differences between REALM and PL for all relations except Conquered (p < 0.01, two-sampled t-test).", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.886044442653656}]}, {"text": "The hybrid REALM system also outperforms each of its components in isolation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Type Checking Performance. Listed is area  under the precision/recall curve. HMM-T outper- forms N-GRAMS for all relations, and reduces the  error in terms of missing area under the curve by  46% on average.", "labels": [], "entities": [{"text": "Type Checking", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.7766069769859314}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9989905953407288}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.8995010256767273}]}, {"text": " Table 2: Performance of REALM for assessment of sparse extractions. Listed is area under the preci- sion/recall curve for each method. In parentheses is the percentage reduction in error over the strongest  baseline method (TEXTRUNNER or PL) for each relation. \"Avg. Prec.\" denotes the fraction of correct  examples in the test set for each relation. REALM outperforms its REL-GRAMS and HMM-T components  taken in isolation, as well as the TEXTRUNNER and PL systems from previous work.", "labels": [], "entities": [{"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9082779288291931}, {"text": "Avg. Prec.\"", "start_pos": 263, "end_pos": 274, "type": "METRIC", "confidence": 0.9425718635320663}]}]}