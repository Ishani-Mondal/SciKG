{"title": [{"text": "Improved Word-Level System Combination for Machine Translation", "labels": [], "entities": [{"text": "Improved Word-Level System Combination", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8036637306213379}, {"text": "Machine Translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7454183995723724}]}], "abstractContent": [{"text": "Recently, confusion network decoding has been applied in machine translation system combination.", "labels": [], "entities": [{"text": "confusion network decoding", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7149269978205363}, {"text": "machine translation system combination", "start_pos": 57, "end_pos": 95, "type": "TASK", "confidence": 0.8660202622413635}]}, {"text": "Due to errors in the hypothesis alignment, decoding may result in un-grammatical combination outputs.", "labels": [], "entities": []}, {"text": "This paper describes an improved confusion network based method to combine outputs from multiple MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9668582081794739}]}, {"text": "In this approach, arbitrary features maybe added log-linearly into the objective function, thus allowing language model expansion and re-scoring.", "labels": [], "entities": [{"text": "language model expansion", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.6328832606474558}]}, {"text": "Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed.", "labels": [], "entities": []}, {"text": "A generic weight tuning algorithm maybe used to optimize various automatic evaluation metrics including TER, BLEU and METEOR.", "labels": [], "entities": [{"text": "TER", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9985709190368652}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9981691837310791}, {"text": "METEOR", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9834163188934326}]}, {"text": "The experiments using the 2005 Arabic to En-glish and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods.", "labels": [], "entities": [{"text": "NIST MT evaluation tasks", "start_pos": 73, "end_pos": 97, "type": "DATASET", "confidence": 0.6583304554224014}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9990703463554382}]}], "introductionContent": [{"text": "System combination has been shown to improve classification performance in various tasks.", "labels": [], "entities": []}, {"text": "There are several approaches for combining classifiers.", "labels": [], "entities": []}, {"text": "In ensemble learning, a collection of simple classifiers is used to yield better performance than any single classifier; for example boosting.", "labels": [], "entities": []}, {"text": "Another approach is to combine outputs from a few highly specialized classifiers.", "labels": [], "entities": []}, {"text": "The classifiers maybe based on the same basic modeling techniques but differ by, for example, alternative feature representations.", "labels": [], "entities": []}, {"text": "Combination of speech recognition outputs is an example of this approach.", "labels": [], "entities": [{"text": "speech recognition outputs", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.7512905597686768}]}, {"text": "In speech recognition, confusion network decoding () has become widely used in system combination.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8454981446266174}, {"text": "confusion network decoding", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.6145345568656921}]}, {"text": "Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7356008142232895}, {"text": "statistical machine translation (MT)", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.794895718495051}]}, {"text": "The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around fora while.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9505544900894165}]}, {"text": "Recently, confusion network decoding for MT system combination has been proposed ().", "labels": [], "entities": [{"text": "confusion network decoding", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6846130092938741}, {"text": "MT system combination", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.908957282702128}]}, {"text": "To generate confusion networks, hypotheses have to be aligned against each other.", "labels": [], "entities": []}, {"text": "In (), Levenshtein alignment was used to generate the network.", "labels": [], "entities": [{"text": "Levenshtein alignment", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.6267087459564209}]}, {"text": "As opposed to speech recognition, the word order between two correct MT outputs maybe different and the Levenshtein alignment may not be able to align shifted words in the hypotheses.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7586250007152557}, {"text": "MT outputs", "start_pos": 69, "end_pos": 79, "type": "TASK", "confidence": 0.8758992552757263}]}, {"text": "In (), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++.", "labels": [], "entities": []}, {"text": "The size of the test set may influence the quality of these alignments.", "labels": [], "entities": []}, {"text": "Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments.", "labels": [], "entities": []}, {"text": "A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER)) was used to align hy-312 potheses in).", "labels": [], "entities": [{"text": "translation edit rate (TER))", "start_pos": 74, "end_pos": 102, "type": "METRIC", "confidence": 0.8086778223514557}]}, {"text": "The alignments from TER are consistent as they do not depend on the test set size.", "labels": [], "entities": [{"text": "TER", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.5826771259307861}]}, {"text": "Also, a more heuristic alignment method has been proposed in a different system combination approach).", "labels": [], "entities": []}, {"text": "A full comparison of different alignment methods would be difficult as many approaches require a significant amount of engineering.", "labels": [], "entities": []}, {"text": "Confusion networks are generated by choosing one hypothesis as the \"skeleton\", and other hypotheses are aligned against it.", "labels": [], "entities": []}, {"text": "The skeleton defines the word order of the combination output.", "labels": [], "entities": []}, {"text": "Minimum Bayes risk (MBR) was used to choose the skeleton in ().", "labels": [], "entities": [{"text": "Minimum Bayes risk (MBR)", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.9233274857203165}]}, {"text": "The average TER score was computed between each system's -best hypothesis and all other hypotheses.", "labels": [], "entities": [{"text": "TER score", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9814091324806213}]}, {"text": "The MBR hypothesis is the one with the minimum average TER and thus, maybe viewed as the closest to all other hypotheses in terms of TER.", "labels": [], "entities": [{"text": "MBR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.5147736668586731}, {"text": "TER", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9970645308494568}, {"text": "TER", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9954198598861694}]}, {"text": "This work was extended in () by introducing system weights for word confidences.", "labels": [], "entities": []}, {"text": "However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton.", "labels": [], "entities": []}, {"text": "In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.", "labels": [], "entities": [{"text": "TER", "start_pos": 179, "end_pos": 182, "type": "METRIC", "confidence": 0.9976752400398254}]}, {"text": "All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights.", "labels": [], "entities": []}, {"text": "The combination outputs from confusion network decoding maybe ungrammatical due to alignment errors.", "labels": [], "entities": []}, {"text": "Also the word-level decoding may break coherent phrases produced by the individual systems.", "labels": [], "entities": []}, {"text": "In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences.", "labels": [], "entities": []}, {"text": "This allows a log-linear addition of arbitrary features such as language model (LM) scores.", "labels": [], "entities": []}, {"text": "The LM scores should increase the total log-posterior of more grammatical hypotheses.", "labels": [], "entities": []}, {"text": "Powell's method) is used to tune the system and feature weights simultaneously so as to optimize various automatic evaluation metrics on a development set.", "labels": [], "entities": []}, {"text": "Tuning is fully automatic, as opposed to () where global system weights were set manually.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Three evaluation metrics used in weights tuning and reporting the test set results are reviewed in Section 2.", "labels": [], "entities": [{"text": "weights tuning", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.8891885280609131}]}, {"text": "Section 3 describes confusion network decoding for MT system combination.", "labels": [], "entities": [{"text": "MT system combination", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.906923770904541}]}, {"text": "The extensions to add features log-linearly and improve the skeleton selection are presented in Sections 4 and 5, respectively.", "labels": [], "entities": []}, {"text": "Section 6 details the weights optimization algorithm and the experimental results are reported in Section 7.", "labels": [], "entities": [{"text": "weights optimization", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.7115773558616638}]}, {"text": "Conclusions and future work are discussed in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "Currently, the most widely used automatic MT evaluation metric is the NIST BLEU-4 ( . By default, METEOR script counts the words that match exactly, and words that match after a simple Porter stemmer.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9495240747928619}, {"text": "NIST", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.8205714821815491}, {"text": "BLEU-4", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9738727807998657}, {"text": "METEOR", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.891958475112915}]}, {"text": "Additional matching modules including WordNet stemming and synonymy may also be used.", "labels": [], "entities": [{"text": "WordNet stemming", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.8379769921302795}]}, {"text": "When multiple references are provided, the lowest score is reported.", "labels": [], "entities": []}, {"text": "Full test set scores are obtained by accumulating statistics overall test sentences.", "labels": [], "entities": []}, {"text": "The METEOR scores are also between Sand , higher being better.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9961250424385071}]}, {"text": "The scores in the results section are reported as percentages.", "labels": [], "entities": []}, {"text": "Translation edit rate (TER) () has been proposed as more intuitive evaluation metric since it is based on the rate of edits required to transform the hypothesis into the reference.", "labels": [], "entities": [{"text": "Translation edit rate (TER)", "start_pos": 0, "end_pos": 27, "type": "METRIC", "confidence": 0.7895672370990118}]}, {"text": "The TER score is computed as follows where c \u00a4 is the reference length.", "labels": [], "entities": [{"text": "TER score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9747858345508575}]}, {"text": "The only difference to word error rate is that the TER allows shifts.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 23, "end_pos": 38, "type": "METRIC", "confidence": 0.6343753238519033}, {"text": "TER", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.8977223634719849}]}, {"text": "A shift of a sequence of words is counted as a single edit.", "labels": [], "entities": []}, {"text": "The minimum translation edit alignment is usually found through abeam search.", "labels": [], "entities": []}, {"text": "When multiple references are provided, the edits from the closest reference are divided by the average reference length.", "labels": [], "entities": []}, {"text": "Full test set scores are obtained by accumulating the edits and the average reference lengths.", "labels": [], "entities": []}, {"text": "The perfect TER score is 0, and otherwise higher than zero.", "labels": [], "entities": [{"text": "perfect", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9835636615753174}, {"text": "TER score", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9458056390285492}]}, {"text": "The TER score may also be higher than 1 due to insertions.", "labels": [], "entities": [{"text": "TER score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9847634434700012}]}, {"text": "Also TER is reported as a percentage in the results section.", "labels": [], "entities": [{"text": "TER", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.9997056126594543}]}], "tableCaptions": [{"text": " Table 1: Mixed-case TER and BLEU, and  lower-case METEOR scores on Arabic NIST  MT03+MT04.", "labels": [], "entities": [{"text": "TER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9958036541938782}, {"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9607328176498413}, {"text": "METEOR", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9968464970588684}, {"text": "Arabic NIST  MT03+MT04", "start_pos": 68, "end_pos": 90, "type": "DATASET", "confidence": 0.6786312937736512}]}, {"text": " Table 2: Mixed-case TER and BLEU, and lower- case METEOR scores on Arabic NIST MT05.", "labels": [], "entities": [{"text": "TER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9960901141166687}, {"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9668737053871155}, {"text": "METEOR", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9953339695930481}, {"text": "Arabic NIST MT05", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.796826958656311}]}, {"text": " Table 3: Mixed-case TER and BLEU, and  lower-case METEOR scores on Chinese NIST  MT03+MT04.", "labels": [], "entities": [{"text": "TER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9964838027954102}, {"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9744370579719543}, {"text": "METEOR", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9981381893157959}, {"text": "Chinese NIST  MT03+MT04", "start_pos": 68, "end_pos": 91, "type": "DATASET", "confidence": 0.7544886708259583}]}, {"text": " Table 4: Mixed-case TER and BLEU, and lower- case METEOR scores on Chinese NIST MT05.", "labels": [], "entities": [{"text": "TER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9966527819633484}, {"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9780358076095581}, {"text": "METEOR", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9960229396820068}, {"text": "Chinese NIST MT05", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.8156690796216329}]}]}