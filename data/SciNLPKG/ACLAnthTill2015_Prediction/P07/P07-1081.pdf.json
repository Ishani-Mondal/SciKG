{"title": [{"text": "Corpus Effects on the Evaluation of Automated Transliteration Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "Most current machine transliteration systems employ a corpus of known source-target word pairs to train their system, and typically evaluate their systems on a similar corpus.", "labels": [], "entities": []}, {"text": "In this paper we explore the performance of transliteration systems on corpora that are varied in a controlled way.", "labels": [], "entities": []}, {"text": "In particular , we control the number, and prior language knowledge of human transliterators used to construct the corpora, and the origin of the source words that makeup the corpora.", "labels": [], "entities": []}, {"text": "We find that the word accuracy of automated transliteration systems can vary by up to 30% (in absolute terms) depending on the corpus on which they are run.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.967821478843689}]}, {"text": "We conclude that at least four human transliterators should be used to construct corpora for evaluating automated transliteration systems; and that although absolute word accuracy metrics may not translate across corpora, the relative rankings of system performance remains stable across differing corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.5533795356750488}]}], "introductionContent": [{"text": "Machine transliteration is the process of transforming a word written in a source language into a word in a target language without the aid of a bilingual dictionary.", "labels": [], "entities": [{"text": "Machine transliteration", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7360331416130066}]}, {"text": "Word pronunciation is preserved, as far as possible, but the script used to render the target word is different from that of the source language.", "labels": [], "entities": [{"text": "Word pronunciation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6620131283998489}]}, {"text": "Transliteration is applied to proper nouns and outof-vocabulary terms as part of machine translation and cross-lingual information retrieval (CLIR)).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7124291956424713}, {"text": "cross-lingual information retrieval (CLIR))", "start_pos": 105, "end_pos": 148, "type": "TASK", "confidence": 0.7474191139141718}]}, {"text": "Several transliteration methods are reported in the literature fora variety of languages, with their performance being evaluated on multilingual corpora.", "labels": [], "entities": []}, {"text": "Source-target pairs are either extracted from bilingual documents or dictionaries (;), or gathered explicitly from human transliterators).", "labels": [], "entities": []}, {"text": "Some evaluations of transliteration methods depend on a single unique transliteration for each source word, while others take multiple target words fora single source word into account.", "labels": [], "entities": []}, {"text": "In their work on transliterating English to Persian, observed that the content of the corpus used for evaluating systems could have dramatic affects on the reported accuracy of methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.990369975566864}]}, {"text": "The effects of corpus composition on the evaluation of transliteration systems has not been specifically studied, with only implicit experiments or claims made in the literature such as introducing the effects of different transliteration models (, language families or application based (CLIR) evaluation ().", "labels": [], "entities": []}, {"text": "In this paper, we report our experiments designed to explicitly examine the effect that varying the underlying corpus used in both training and testing systems has on transliteration accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.8349891901016235}]}, {"text": "Specifically, we vary the number of human transliterators that are used to construct the corpus; and the origin of the English words used in the corpus.", "labels": [], "entities": []}, {"text": "Our experiments show that the word accuracy of automated transliteration systems can vary by up to 30% (in absolute terms), depending on the corpus used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9717556834220886}]}, {"text": "Despite the wide range of absolute values 640 in performance, the ranking of our two transliteration systems was preserved on all corpora.", "labels": [], "entities": []}, {"text": "We also find that a human's confidence in the language from which they are transliterating can affect the corpus in such away that word accuracy rates are altered.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9196345210075378}]}], "datasetContent": [{"text": "In order to evaluate the list Li of target words produced by a transliteration system for source word s i , a test corpus is constructed.", "labels": [], "entities": []}, {"text": "The test corpus consists of a source word, s i , and a list of possible target words {t i j }, where 1 \u2264 j \u2264 d i , the number of distinct target words for source word s i . Associated with each ti j is a count n i j which is the number of human transliterators who transliterated s i into ti j . Often the test corpus is a proportion of a larger corpus, the remainder of which has been used for training the system's rule base.", "labels": [], "entities": []}, {"text": "In this work we adopt the standard ten-fold cross validation technique for all of our results, where 90% of a corpus is used for training and 10% for testing.", "labels": [], "entities": []}, {"text": "The process is repeated ten times, and the mean result taken.", "labels": [], "entities": []}, {"text": "Forthwith, we use the term corpus to refer to the single corpus from which both training and test sets are drawn in this fashion.", "labels": [], "entities": []}, {"text": "Once the corpus is decided upon, a metric to measure the system's accuracy is required.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9986298084259033}]}, {"text": "The appropriate metric depends on the scenario in which the transliteration system is to be used.", "labels": [], "entities": []}, {"text": "For example, in a machine translation application where only one target word can be inserted in the text to represent a source word, it is important that the word at the top of the system generated list of target words (by definition the most probable) is one of the words generated by a human in the corpus.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7143470644950867}]}, {"text": "More formally, the first word generated for source word s i , Li 1 , must be one oft i j , 1 \u2264 j \u2264 d i . It may even be desirable that this is the target word most commonly used for this source word; that is, Li 1 = ti j such that n i j \u2265 n ik , for all 1 \u2264 k \u2264 d i . Alternately, in a CLIR application, all variants of a source word might be required.", "labels": [], "entities": []}, {"text": "For example, if a user searches for an English term \"Tom\" in Persian documents, the search engine should try and locate documents that contain both \" \ud97b\udf59 \u00c3\" (3 letters: \ud97b\udf59 \u00c0--) and \"\ud97b\udf59\ud97b\udf59 \ud97b\udf59 \u00b3\"(2 letters: \ud97b\udf59 \u00c0-), two possible transliterations of \"Tom\" that would be generated by human transliterators.", "labels": [], "entities": [{"text": "\ud97b\udf59 \u00c0--) and \"\ud97b\udf59\ud97b\udf59 \ud97b\udf59 \u00b3\"(2 letters: \ud97b\udf59 \u00c0-)", "start_pos": 167, "end_pos": 203, "type": "METRIC", "confidence": 0.6934354557440832}]}, {"text": "In this case, a metric that counts the number oft i j that appear in the top d i elements of the system generated list, Li , might be appropriate.", "labels": [], "entities": []}, {"text": "In this paper we focus on the \"Top-1\" case, where it is important for the most probable target word generated by the system, Li 1 to be either the most pop-641 ular ti j (labeled the Majority, with ties broken arbitrarily), or just one of the ti j 's (labeled Uniform because all possible transliterations are equally rewarded).", "labels": [], "entities": []}, {"text": "A third scheme (labeled Weighted) is also possible where the reward fort i j appearing as Li 1 is n i j / \u2211 d i j=1 n i j ; here, each target word is given a weight proportional to how often a human transliterator chose that target word.", "labels": [], "entities": []}, {"text": "Due to space considerations, we focus on the first two variants only.", "labels": [], "entities": []}, {"text": "In general, there are two commonly used metrics for transliteration evaluation: word accuracy (WA) and character accuracy (CA).", "labels": [], "entities": [{"text": "transliteration evaluation", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.9087854325771332}, {"text": "word accuracy (WA)", "start_pos": 80, "end_pos": 98, "type": "METRIC", "confidence": 0.7793066024780273}, {"text": "character accuracy (CA)", "start_pos": 103, "end_pos": 126, "type": "METRIC", "confidence": 0.829771089553833}]}, {"text": "In all of our experiments, CA based metrics closely mirrored WA based metrics, and so conclusions drawn from the data would be the same whether WA metrics or CA metrics were used.", "labels": [], "entities": []}, {"text": "Hence we only discuss and report WA based metrics in this paper.", "labels": [], "entities": [{"text": "WA based metrics", "start_pos": 33, "end_pos": 49, "type": "METRIC", "confidence": 0.8865421414375305}]}, {"text": "For each source word in the test corpus of K words, word accuracy calculates the percentage of correctly transliterated terms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9388744235038757}]}, {"text": "Hence for the majority case, where every source word in the corpus only has one target word, the word accuracy is defined as , and for the Uniform case, where every target variant is included with equal weight in the corpus, the word accuracy is defined as  To evaluate the level of agreement between transliterators, we use an agreement measure based on.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.8193653225898743}, {"text": "accuracy", "start_pos": 234, "end_pos": 242, "type": "METRIC", "confidence": 0.7343331575393677}]}, {"text": "For any source word s i , there are d i different transliterations made by then i human transliterators (n i = \u2211 d i j=1 n i j , where n i j is the number of times source word s i was transliterated into target word ti j ).", "labels": [], "entities": []}, {"text": "When any two transliterators agree on the same target word, there are two agreements being made: transliterator one agrees with transliterator two, and vice versa.", "labels": [], "entities": []}, {"text": "In general, therefore, the total number of agreements made on source word s i is \u2211 d i j=1 n i j (n i j \u2212 1).", "labels": [], "entities": []}, {"text": "Hence the total number of actual agreements made on the entire corpus of K words is The total number of possible agreements (that is, when all human transliterators agree on a single target word for each source word), is The proportion of overall agreement is therefore", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Transliterator's language knowledge (0=not familiar to 3=excellent knowledge), perception of  difficulty (1=hard to 3=easy) and confidence (1=no confidence to 3=quite confident) in creating the corpus.", "labels": [], "entities": []}, {"text": " Table 2: Number of characters used and rules generated using SYS-2, per transliterator.", "labels": [], "entities": []}, {"text": " Table 3: System performance when A 7 is split into sub-corpora based on transliterators perception of the  task (Easy or Medium).", "labels": [], "entities": []}]}