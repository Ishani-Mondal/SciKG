{"title": [{"text": "Using Corpus Statistics on Entities to Improve Semi-supervised Relation Extraction from the Web", "labels": [], "entities": [{"text": "Improve Semi-supervised Relation Extraction from the Web", "start_pos": 39, "end_pos": 95, "type": "TASK", "confidence": 0.794065317937306}]}], "abstractContent": [{"text": "Many errors produced by unsupervised and semi-supervised relation extraction (RE) systems occur because of wrong recognition of entities that participate in the relations.", "labels": [], "entities": [{"text": "relation extraction (RE)", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.8693686723709106}]}, {"text": "This is especially true for systems that do not use separate named-entity recognition components, instead relying on general-purpose shallow parsing.", "labels": [], "entities": []}, {"text": "Such systems have greater applicability, because they are able to extract relations that contain attributes of unknown types.", "labels": [], "entities": []}, {"text": "However, this generality comes with the cost inaccuracy.", "labels": [], "entities": []}, {"text": "In this paper we show how to use corpus statistics to validate and correct the arguments of extracted relation instances, improving the overall RE performance.", "labels": [], "entities": [{"text": "RE", "start_pos": 144, "end_pos": 146, "type": "TASK", "confidence": 0.7518727779388428}]}, {"text": "We test the methods on SRES-a self-supervised Web relation extraction system.", "labels": [], "entities": [{"text": "SRES-a self-supervised Web relation extraction", "start_pos": 23, "end_pos": 69, "type": "TASK", "confidence": 0.6429234981536865}]}, {"text": "We also compare the performance of corpus-based methods to the performance of validation and correction methods based on supervised NER components.", "labels": [], "entities": [{"text": "validation and correction", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.6635140180587769}]}], "introductionContent": [{"text": "Information Extraction (IE) is the task of extracting factual assertions from text.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8447979152202606}]}, {"text": "Most IE systems rely on knowledge engineering or on machine learning to generate the \"task model\" that is subsequently used for extracting instances of entities and relations from new text.", "labels": [], "entities": [{"text": "IE", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.9830800294876099}, {"text": "extracting instances of entities and relations from new text", "start_pos": 128, "end_pos": 188, "type": "TASK", "confidence": 0.7571982542673746}]}, {"text": "In the knowledge engineering approach the model (usually in the form of extraction rules) is created manually, and in the machine learning approach the model is learned automatically from a manually labeled training set of documents.", "labels": [], "entities": []}, {"text": "Both approaches require substantial human effort, particularly when applied to the broad range of documents, entities, and relations on the Web.", "labels": [], "entities": []}, {"text": "In order to minimize the manual effort necessary to build Web IE systems, semisupervised and completely unsupervised systems are being developed by many researchers.", "labels": [], "entities": []}, {"text": "The task of extracting facts from the Web has significantly different aims than the regular information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.7883788645267487}]}, {"text": "The goal of regular IE is to identify and label all mentions of all instances of the given relation type inside a document or inside a collection of documents.", "labels": [], "entities": [{"text": "IE", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.7445080280303955}]}, {"text": "Whereas, in the Web Extraction (WE) tasks we are only interested in extracting relation instances and not interested in particular mentions.", "labels": [], "entities": [{"text": "Web Extraction (WE)", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.75330091714859}]}, {"text": "This difference in goals leads to a difference in the methods of performance evaluation.", "labels": [], "entities": []}, {"text": "The usual measures of performance of regular IE systems are precision, recall, and their combinations -the breakeven point and F-measure.", "labels": [], "entities": [{"text": "IE", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9672018885612488}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9996360540390015}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9994781613349915}, {"text": "breakeven point", "start_pos": 107, "end_pos": 122, "type": "METRIC", "confidence": 0.9823035895824432}, {"text": "F-measure", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9454688429832458}]}, {"text": "Unfortunately, the true recall usually cannot be known for WE tasks.", "labels": [], "entities": [{"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9952278137207031}, {"text": "WE tasks", "start_pos": 59, "end_pos": 67, "type": "TASK", "confidence": 0.8817881643772125}]}, {"text": "Consequently, for evaluating the performance of WE systems, the recall is substituted by the number of extracted instances.", "labels": [], "entities": [{"text": "WE", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9374352097511292}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9991173148155212}]}, {"text": "WE systems usually order the extracted instances by the system's confidence in their correctness.", "labels": [], "entities": [{"text": "WE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8959387540817261}]}, {"text": "The precision of top-confidence extractions is usually very high, but it gets progressively lower when lower-confidence candidates are considered.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995717406272888}, {"text": "top-confidence extractions", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.6555937975645065}]}, {"text": "The curve that plots the number of extractions against precision level is the best indicator of system's quality.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9990853071212769}]}, {"text": "Naturally, fora comparision be-tween different systems to be meaningful, the evaluations must be performed on the same corpus.", "labels": [], "entities": []}, {"text": "In this paper we are concerned with Web RE systems that extract binary relations between named entities.", "labels": [], "entities": [{"text": "Web RE", "start_pos": 36, "end_pos": 42, "type": "TASK", "confidence": 0.5001992881298065}]}, {"text": "Most of such systems utilize separate named entity recognition (NER) components, which are usualy trained in a supervised way on a separate set of manually labeled documents.", "labels": [], "entities": [{"text": "separate named entity recognition (NER)", "start_pos": 29, "end_pos": 68, "type": "TASK", "confidence": 0.8300986886024475}]}, {"text": "The NER components recognize and extract the values of relation attributes (also called arguments, or slots), while the RE systems are concerned with patterns of contexts in which the slots appear.", "labels": [], "entities": []}, {"text": "However, good NER components only exist for common and very general entity types, such as Person, Organization, and Location.", "labels": [], "entities": []}, {"text": "For some relations, the types of attributes are less common, and no ready NER components (or ready labeled training sets) exist for them.", "labels": [], "entities": []}, {"text": "Also, some Web RE systems (e.g.,) do not use separate NER components even for known entity types, because such components are usually domain-specific and may perform poorly on cross-domain text collections extracted from the Web.", "labels": [], "entities": []}, {"text": "In such cases, the values for relation attributes must be extracted by generic methods -shallow parsing (extracting noun phrases), or even simple substring extraction.", "labels": [], "entities": []}, {"text": "Such methods are naturally much less precise and produce many entityrecognition errors . In this paper we propose several methods of using corpus statistics to improve Web RE precision by validating and correcting the entities extracted by generic methods.", "labels": [], "entities": [{"text": "precision", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.6876099705696106}]}, {"text": "The task of Web Extraction is particularly suited for the corpus statistics-based methods because of very large size of the corpora involved, and because the system is not required to identify individual mentions of the relations.", "labels": [], "entities": [{"text": "Web Extraction", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.5577452927827835}]}, {"text": "Our methods of entity validation and correction are based on the following two observations: First, the entities that appear in target relations will often also appear in many other contexts, some of which may strongly discriminate in favor of entities of specific type.", "labels": [], "entities": [{"text": "entity validation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.6798669695854187}]}, {"text": "For example, assume the system encounters a sentence \"Oracle bought PeopleSoft.\"", "labels": [], "entities": [{"text": "Oracle bought PeopleSoft", "start_pos": 54, "end_pos": 78, "type": "DATASET", "confidence": 0.623571773370107}]}, {"text": "If the system works without a NER component, it only knows that \"Oracle\" and \"PeopleSoft\" are proper noun phrases, and its confidence in correctness of a candidate relation instance Acquisition(Oracle, PeopleSoft) cannot be very high.", "labels": [], "entities": [{"text": "Acquisition", "start_pos": 182, "end_pos": 193, "type": "METRIC", "confidence": 0.9682102799415588}]}, {"text": "However, both entities occur many times elsewhere in the corpus, sometimes in strongly discriminating contexts, such as \"Oracle is a company that\u2026\" or \"PeopleSoft Inc.\"", "labels": [], "entities": []}, {"text": "If the system somehow learned that such contexts indicate entities of the correct type for the Acquisition relation (i.e., companies), then the system would be able to boost its confidence in both entities (\"Oracle\" and \"PeopleSoft\") being of correct types and, consequently, in (Oracle, PeopleSoft) being a correct instance of the Acquisition relation.", "labels": [], "entities": []}, {"text": "Another observation that we can use is the fact that the entities, in which we are interested, usually have sufficient frequency in the corpus for statistical term extraction methods to perform reasonably well.", "labels": [], "entities": [{"text": "statistical term extraction", "start_pos": 147, "end_pos": 174, "type": "TASK", "confidence": 0.6876801451047262}]}, {"text": "These methods may often correct a wrongly placed entity boundary, which is a common mistake of general-purpose shallow parsers.", "labels": [], "entities": []}, {"text": "In this paper we show how to use these observations to supplement a Web RE system with an entity validation and correction component, which is able to significantly improve the system's accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.996225118637085}]}, {"text": "We evaluate the methods using SRES ) -a Web RE system, designed to extend and improve KnowItAll).", "labels": [], "entities": [{"text": "SRES", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.7307397723197937}]}, {"text": "The contributions of this paper are as follows: \u2022 We show how to automatically generate the validating patterns for the target relation arguments, and how to integrate the results produced by the validating patterns into the whole relation extraction system.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 231, "end_pos": 250, "type": "TASK", "confidence": 0.7109042257070541}]}, {"text": "\u2022 We show how to use corpus statistics and term extraction methods to correct the boundaries of relation arguments.", "labels": [], "entities": [{"text": "term extraction", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7073489725589752}]}, {"text": "\u2022 We experimentally compare the improvement produced by the corpus-based entity validation and correction methods with the improvements produced by two alternative validators -a CRF-based NER system trained on a separate labeled corpus, and a small manually-built rule-based NER component.", "labels": [], "entities": [{"text": "corpus-based entity validation and correction", "start_pos": 60, "end_pos": 105, "type": "TASK", "confidence": 0.6606155753135681}]}, {"text": "The rest of the paper is organized as follows: Section 2 describes previous work.", "labels": [], "entities": []}, {"text": "Section 3 outlines the general design principles of SRES and briefly describes its components.", "labels": [], "entities": [{"text": "SRES", "start_pos": 52, "end_pos": 56, "type": "TASK", "confidence": 0.9013470411300659}]}, {"text": "Section 4 describes in detail the different entity validation and correction methods, and Section 5 presents their experimental evaluation.", "labels": [], "entities": [{"text": "entity validation and correction", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.6886197626590729}]}, {"text": "Section 6 contains conclusions and directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments described in this paper aim to confirm the effectiveness of the proposed corpusbased relation argument validation and correction method, and to compare its performance with the classical knowledge-engineering-based and supervised-training-based methods.", "labels": [], "entities": [{"text": "relation argument validation and correction", "start_pos": 101, "end_pos": 144, "type": "TASK", "confidence": 0.6969102919101715}]}, {"text": "The experiments were performed with five relations: The data for the experiments were collected by the KnowItAll crawler.", "labels": [], "entities": []}, {"text": "The data for the Acquisition and Merger consist of about 900,000 sentences for each of the two relations.", "labels": [], "entities": [{"text": "Acquisition and Merger", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.6052141586939493}]}, {"text": "The data for the bound relations consist of sentences, such that each contains one of a hundred values of the first (bound) attribute.", "labels": [], "entities": []}, {"text": "Half of the hundred are frequent entities (>100,000 search engine hits), and another half are rare (<10,000 hits).", "labels": [], "entities": []}, {"text": "For evaluating the validators we randomly selected a set of 10000 sentences from the corpora for each of the relations, and manually evaluated the SRES results generated from these sentences.", "labels": [], "entities": [{"text": "SRES", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.703141450881958}]}, {"text": "Four sets of results were evaluated: the baseline results produced without any NER validator, and three sets of results produced using three different NER validators.", "labels": [], "entities": []}, {"text": "For the InventorOf relation, only the corpus-based validator results can be produced, since the other two NER components cannot be adapted to validate/correct entities of type Invention.", "labels": [], "entities": []}, {"text": "The results for the five relations are shown in the.", "labels": [], "entities": []}, {"text": "Several conclusions can be drawn from the graphs.", "labels": [], "entities": []}, {"text": "First, all of the NER validators improve over the baseline SRES, sometimes as much as doubling the recall at the same level of precision.", "labels": [], "entities": [{"text": "NER validators", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.8447454869747162}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9987242817878723}, {"text": "precision", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9967626333236694}]}, {"text": "In most cases the three validators show roughly similar levels of performance.", "labels": [], "entities": []}, {"text": "A notable difference is the CEO_Of relation, where the simple rule-based component performs much better than CRF, which performs yet better than the corpus-based component.", "labels": [], "entities": [{"text": "CEO_Of", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.4436998764673869}]}, {"text": "The CEO_Of relation is tested as bound, which means that only the second relation argument, of type Person, is validated.", "labels": [], "entities": [{"text": "CEO_Of relation", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7636099010705948}]}, {"text": "The Person entities have much more rigid internal structure than the other entities -Companies and Inventions.", "labels": [], "entities": []}, {"text": "Consequently, the best performing of the three validators is the rule-based, which directly tests this internal structure.", "labels": [], "entities": []}, {"text": "The CRF-based validator is also able to take advantage of the structure, although in a weaker manner.", "labels": [], "entities": []}, {"text": "The Corpusbased validator, however, works purely on the basis of context, entirely disregarding the internal structure of entities, and thus performs worst of all in this case.", "labels": [], "entities": []}, {"text": "On the other hand, the Corpus-based validator is able to improve the results for the Inventor relation, which the other two validators are completely unable to do.", "labels": [], "entities": []}, {"text": "It is also of interest to compare the performance of CRF-based and the rule-based NER components in other cases.", "labels": [], "entities": []}, {"text": "As can be seen, inmost cases the rule-based component, despite its simplicity, outperforms the CRF-based one.", "labels": [], "entities": []}, {"text": "The possible reason for this is that relation extraction setting is significantly different from the classical named entity recognition setting.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8781644403934479}, {"text": "named entity recognition", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.709879477818807}]}, {"text": "A classical NER system is set to maximize the F 1 measure of all mentions of all entities in the corpus.", "labels": [], "entities": [{"text": "NER", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9489621520042419}, {"text": "F 1 measure", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9772865176200867}]}, {"text": "A relation argument extractor, on the other hand, should maximize its performance on relation arguments, and apparently their statistical properties are often significantly different.", "labels": [], "entities": [{"text": "relation argument extractor", "start_pos": 2, "end_pos": 29, "type": "TASK", "confidence": 0.8341676592826843}]}], "tableCaptions": []}