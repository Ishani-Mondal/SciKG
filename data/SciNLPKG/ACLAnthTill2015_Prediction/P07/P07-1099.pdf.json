{"title": [{"text": "Language-independent Probabilistic Answer Ranking for Question Answering", "labels": [], "entities": [{"text": "Probabilistic Answer Ranking", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.6676641901334127}, {"text": "Question Answering", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7674095630645752}]}], "abstractContent": [{"text": "This paper presents a language-independent probabilistic answer ranking framework for question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.9020738005638123}]}, {"text": "The framework estimates the probability of an individual answer candidate given the degree of answer relevance and the amount of supporting evidence provided in the set of answer candidates for the question.", "labels": [], "entities": []}, {"text": "Our approach was evaluated by comparing the candidate answer sets generated by Chinese and Japanese answer extractors with the re-ranked answer sets produced by the answer ranking framework.", "labels": [], "entities": []}, {"text": "Empirical results from testing on NT-CIR factoid questions show a 40% performance improvement in Chinese answer selection and a 45% improvement in Japanese answer selection.", "labels": [], "entities": [{"text": "NT-CIR factoid questions", "start_pos": 34, "end_pos": 58, "type": "DATASET", "confidence": 0.857526977856954}, {"text": "Chinese answer selection", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.60341810186704}]}], "introductionContent": [{"text": "Question answering (QA) systems aim at finding precise answers to natural language questions from large document collections.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9111684322357178}]}, {"text": "Typical QA systems (;) adopt a pipeline architecture that incorporates four major steps: (1) question analysis, (2) document retrieval, (3) answer extraction and (4) answer selection.", "labels": [], "entities": [{"text": "question analysis", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7917309105396271}, {"text": "document retrieval", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.6835345327854156}, {"text": "answer extraction", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.8143284618854523}, {"text": "answer selection", "start_pos": 166, "end_pos": 182, "type": "TASK", "confidence": 0.8415286540985107}]}, {"text": "Question analysis is a process which analyzes a question and produces a list of keywords.", "labels": [], "entities": [{"text": "Question analysis", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8295613527297974}]}, {"text": "Document retrieval is a step that searches for relevant documents or passages.", "labels": [], "entities": [{"text": "Document retrieval", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8968715667724609}]}, {"text": "Answer extraction extracts a list of answer candidates from the retrieved documents.", "labels": [], "entities": [{"text": "Answer extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8938915133476257}]}, {"text": "Answer selection is a process which pinpoints correct answer(s) from the extracted candidate answers.", "labels": [], "entities": [{"text": "Answer selection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9185548424720764}]}, {"text": "Since the first three steps in the QA pipeline may produce erroneous outputs, the final answer selection step often entails identifying correct answer(s) amongst many incorrect ones.", "labels": [], "entities": []}, {"text": "For example, given the question \"Which Chinese city has the largest number of foreign financial companies?\", the answer extraction component produces a ranked list of five answer candidates: Beijing (AP880603-0268) , Hong Kong (WSJ920110-0013), Shanghai (FBIS3-58), Taiwan (FT942-2016) and Shanghai (FBIS3-45320).", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.7003399729728699}, {"text": "WSJ920110-0013", "start_pos": 228, "end_pos": 242, "type": "DATASET", "confidence": 0.8396972417831421}, {"text": "FT942-2016", "start_pos": 274, "end_pos": 284, "type": "DATASET", "confidence": 0.9520605206489563}]}, {"text": "Due to imprecision in answer extraction, an incorrect answer (\"Beijing\") can be ranked in the first position, and the correct answer (\"Shanghai\") was extracted from two different documents and ranked in the third and the fifth positions.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.8209918737411499}]}, {"text": "In order to rank \"Shanghai\" in the top position, we have to address two interesting challenges: \u2022 Answer Similarity.", "labels": [], "entities": [{"text": "Answer Similarity", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.8061490952968597}]}, {"text": "How do we exploit similarity among answer candidates?", "labels": [], "entities": []}, {"text": "For example, when the candidates list contains redundant answers (e.g., \"Shanghai\" as above) or several answers which represent a single instance (e.g. \"U.S.A.\" and \"the United States\"), how much should we boost the rank of the redundant answers?", "labels": [], "entities": []}, {"text": "How do we identify relevant answer(s) amongst irrelevant ones?", "labels": [], "entities": []}, {"text": "This task may involve searching for evidence of a relationship between the answer and the answer type or a question keyword.", "labels": [], "entities": []}, {"text": "For example, we might wish to query a knowledge base to determine if \"Shanghai\" is a city (IS-A(Shanghai, city)), or to determine if Shanghai is in China).", "labels": [], "entities": []}, {"text": "The first challenge is to exploit redundancy in the set of answer candidates.", "labels": [], "entities": []}, {"text": "As answer candidates are extracted from different documents, they may contain identical, similar or complementary text snippets.", "labels": [], "entities": []}, {"text": "For example, \"U.S.\" can appear as \"United States\" or \"USA\" in different documents.", "labels": [], "entities": []}, {"text": "It is important to detect redundant information and boost answer confidence, especially for list questions that require a set of unique answers.", "labels": [], "entities": []}, {"text": "One approach is to perform answer clustering ().", "labels": [], "entities": [{"text": "answer clustering", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.9097833335399628}]}, {"text": "However, the use of clustering raises additional questions: how to calculate the score of the clustered answers, and how to select the cluster label.", "labels": [], "entities": []}, {"text": "To address the second question, several answer selection approaches have used external knowledge resources such as WordNet, CYC and gazetteers for answer validation or answer reranking.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.888496458530426}, {"text": "WordNet", "start_pos": 115, "end_pos": 122, "type": "DATASET", "confidence": 0.9831289649009705}, {"text": "CYC", "start_pos": 124, "end_pos": 127, "type": "DATASET", "confidence": 0.8329646587371826}, {"text": "answer validation", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.7754156291484833}, {"text": "answer reranking", "start_pos": 168, "end_pos": 184, "type": "TASK", "confidence": 0.7897458374500275}]}, {"text": "Answer candidates are either removed or discounted if they are not of the expected answer type ().", "labels": [], "entities": []}, {"text": "The Web also has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords ().", "labels": [], "entities": [{"text": "answer reranking", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.9525847434997559}]}, {"text": "This approach has been used in various languages for answer validation.", "labels": [], "entities": [{"text": "answer validation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.918440192937851}]}, {"text": "Wikipedia's structured information was used for Spanish answer type checking ().", "labels": [], "entities": [{"text": "Wikipedia's structured information", "start_pos": 0, "end_pos": 34, "type": "DATASET", "confidence": 0.8967621475458145}, {"text": "Spanish answer type checking", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.8272359818220139}]}, {"text": "Although many QA systems have incorporated individual features and/or resources for answer selection in a single language, there has been little research on a generalized probabilistic framework that supports answer ranking in multiple languages using any answer relevance and answer similarity features that are appropriate for the language in question.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.8343049585819244}, {"text": "answer ranking", "start_pos": 209, "end_pos": 223, "type": "TASK", "confidence": 0.8382367491722107}]}, {"text": "In this paper, we describe a probabilistic answer ranking framework for multiple languages.", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.741210550069809}]}, {"text": "The framework uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer relevance features and answer similarity features.", "labels": [], "entities": []}, {"text": "An existing framework which was originally developed for English ( was extended for Chinese and Japanese answer ranking by incorporating language-specific features.", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 105, "end_pos": 119, "type": "TASK", "confidence": 0.8988928496837616}]}, {"text": "Empirical results on NTCIR Chinese and Japanese factoid questions show that the framework significantly improved answer selection performance; Chinese performance improved by 40% over the baseline, and Japanese performance improved by 45% over the baseline.", "labels": [], "entities": [{"text": "NTCIR Chinese and Japanese factoid questions", "start_pos": 21, "end_pos": 65, "type": "DATASET", "confidence": 0.7495775918165842}, {"text": "answer selection", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.8406533598899841}]}, {"text": "The remainder of this paper is organized as follows: Section 2 contains an overview of the answer ranking task.", "labels": [], "entities": [{"text": "answer ranking task", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.9208284417788187}]}, {"text": "Section 3 summarizes the answer ranking framework.", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.9035874903202057}]}, {"text": "In Section 4, we explain how we extended the framework by incorporating languagespecific features.", "labels": [], "entities": []}, {"text": "Section 5 describes the experimental methodology and results.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes with suggestions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the experiments to evaluate the extended answer ranking framework for Chinese and Japanese QA.", "labels": [], "entities": []}, {"text": "We used Chinese and Japanese questions provided by the NTCIR (NII Test Collection for IR Systems), which focuses on evaluating cross-lingual and monolingual QA tasks for Chinese, Japanese and English.", "labels": [], "entities": [{"text": "NTCIR (NII Test Collection", "start_pos": 55, "end_pos": 81, "type": "DATASET", "confidence": 0.8789617657661438}]}, {"text": "For Chinese, a total of 550 factoid questions from the NTCIR5-6 QA evaluations served as the dataset.", "labels": [], "entities": [{"text": "NTCIR5-6 QA evaluations", "start_pos": 55, "end_pos": 78, "type": "DATASET", "confidence": 0.9477590719858805}]}, {"text": "Among them, 200 questions were used to train the Chinese answer extractor and 350 questions were used to evaluate our answer ranking framework.", "labels": [], "entities": [{"text": "Chinese answer extractor", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.6586378812789917}]}, {"text": "For Japanese, 700 questions from the NTCIR5-6 QA evaluations served as the dataset.", "labels": [], "entities": [{"text": "NTCIR5-6 QA evaluations", "start_pos": 37, "end_pos": 60, "type": "DATASET", "confidence": 0.9232605695724487}]}, {"text": "Among them, 300 questions were used to train the Japanese answer extractor and 400 questions were used to evaluate our framework.", "labels": [], "entities": [{"text": "Japanese answer extractor", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.6198396980762482}]}, {"text": "Both the Chinese and Japanese answer extractors use maximum-entropy to extract answer candidates based on multiple features such as named entity, dependency structures and some language-dependent features.", "labels": [], "entities": [{"text": "answer extractors", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7307492047548294}]}, {"text": "Performance of the answer ranking framework was measured by average answer accuracy: the number of correct top answers divided by the number of questions whereat least one correct answer exists in the candidate list provided by an extractor.", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.8805898129940033}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.8047693967819214}]}, {"text": "Mean Reciprocal Rank (MRR5) was also used to calculate the average reciprocal rank of the first correct answer in the top 5 answers.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR5)", "start_pos": 0, "end_pos": 27, "type": "METRIC", "confidence": 0.9475977619489034}]}, {"text": "The baseline for average answer accuracy was calculated using the answer candidate likelihood scores provided by each individual extractor; the 788 answer with the best extractor score was chosen, and no validation or similarity processing was performed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9461103677749634}]}, {"text": "3-fold cross-validation was performed, and we used aversion of Wikipedia downloaded in Aug 2006.", "labels": [], "entities": [{"text": "Wikipedia downloaded in Aug 2006", "start_pos": 63, "end_pos": 95, "type": "DATASET", "confidence": 0.9673319697380066}]}], "tableCaptions": [{"text": " Table 1: Articles in Wikipedia for different lan- guages", "labels": [], "entities": []}, {"text": " Table 2: Average top answer accuracy of individ- ual features (Rel: merging relevance features, Sim:  merging similarity features, ALL: merging all fea- tures).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.8079751133918762}]}, {"text": " Table 3: Average accuracy using individual similar- ity features under different thresholds: 0.3 and 0.5  (\"All\": combination of all similarity metrics)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9213259220123291}]}]}