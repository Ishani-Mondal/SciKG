{"title": [{"text": "A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation", "labels": [], "entities": [{"text": "Syntax-based Reordering", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.8070752024650574}, {"text": "Statistical Machine Translation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.829102615515391}]}], "abstractContent": [{"text": "Inspired by previous preprocessing approaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9926927089691162}, {"text": "SMT", "start_pos": 173, "end_pos": 176, "type": "TASK", "confidence": 0.7072501182556152}]}, {"text": "Given a source sentence and its parse tree, our method generates, by tree operations, an n-best list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation.", "labels": [], "entities": []}, {"text": "Experiments show that, for the NIST MT-05 task of Chinese-to-English translation, the proposal leads to BLEU improvement of 1.56%.", "labels": [], "entities": [{"text": "NIST MT-05 task", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.6096482872962952}, {"text": "Chinese-to-English translation", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.602555051445961}, {"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9996379613876343}]}], "introductionContent": [{"text": "The phrase-based approach has been considered the default strategy to Statistical Machine Translation (SMT) in recent years.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 70, "end_pos": 107, "type": "TASK", "confidence": 0.8875574072202047}]}, {"text": "It is widely known that the phrase-based approach is powerful in local lexical choice and word reordering within short distance.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.6975409537553787}]}, {"text": "However, long-distance reordering is problematic in phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.7469719648361206}]}, {"text": "For example, the distancebased reordering model () allows a decoder to translate in non-monotonous order, under the constraint that the distance between two phrases translated consecutively does not exceed a limit known as distortion limit.", "labels": [], "entities": []}, {"text": "In theory the distortion limit can be assigned a very large value so that all possible reorderings are allowed, yet in practise it is observed that too high a distortion limit not only harms efficiency but also translation performance ( ).", "labels": [], "entities": [{"text": "distortion", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9476361870765686}]}, {"text": "In our own experiment setting, the best distortion limit for ChineseEnglish translation is 4.", "labels": [], "entities": [{"text": "ChineseEnglish translation", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.6361278742551804}]}, {"text": "However, some ideal translations exhibit reorderings longer than such distortion limit.", "labels": [], "entities": []}, {"text": "Consider the sentence pair in NIST MT-2005 test set shown in figure 1(a): after translating the word \"/mend\", the decoder should 'jump' across six words and translate the last phrase \" /fissures in the relationship\".", "labels": [], "entities": [{"text": "NIST MT-2005 test set", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.8370706140995026}]}, {"text": "Therefore, while short-distance reordering is under the scope of the distance-based model, long-distance reordering is simply out of the question.", "labels": [], "entities": []}, {"text": "A terminological remark: In the rest of the paper, we will use the terms global reordering and local reordering in place of long-distance reordering and short-distance reordering respectively.", "labels": [], "entities": []}, {"text": "The distinction between long and short distance reordering is solely defined by distortion limit.", "labels": [], "entities": []}, {"text": "Syntax 1 is certainly a potential solution to global reordering.", "labels": [], "entities": [{"text": "global reordering", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7151307463645935}]}, {"text": "For example, for the last two Chinese phrases in figure 1(a), simply swapping the two children of the NP node will produce the correct word order on the English side.", "labels": [], "entities": []}, {"text": "However, there are also reorderings which do not agree with syntactic analysis.(b) shows how our phrase-based decoder 2 obtains a good English translation by reordering two blocks.", "labels": [], "entities": []}, {"text": "It should be noted that the second Chinese block \" \" and its English counterpart \"at the end of\" are not constituents at all.", "labels": [], "entities": []}, {"text": "In this paper, our interest is the value of syntax in reordering, and the major statement is that syntactic information is useful in handling global reordering and it achieves better MT performance on the basis of the standard phrase-based model.", "labels": [], "entities": [{"text": "MT", "start_pos": 183, "end_pos": 185, "type": "TASK", "confidence": 0.9883865714073181}]}, {"text": "To prove it, we developed a hybrid approach which preserves the strength of phrase-based SMT in local reordering as well as the strength of syntax in global reordering.", "labels": [], "entities": [{"text": "SMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.8026528358459473}]}, {"text": "Our method is inspired by previous preprocessing approaches like (,), and), which split translation into two stages: where a sentence of the source language (SL), S, is first reordered with respect to the word order of the target language (TL), and then the reordered SL sentence S is translated as a TL sentence T by monotonous translation.", "labels": [], "entities": []}, {"text": "Our first contribution is anew translation model as represented by formula 2: where an n-best list of S , instead of only one S , is generated.", "labels": [], "entities": []}, {"text": "The reason of such change will be given in section 2.", "labels": [], "entities": []}, {"text": "Note also that the translation process S \u2192 T is not monotonous, since the distance-based model is needed for local reordering.", "labels": [], "entities": []}, {"text": "Our second contribution is our definition of the best translation: where F i are the features in the standard phrasebased model and Pr (S \u2192 S ) is our new feature, viz.", "labels": [], "entities": []}, {"text": "the probability of reordering S as S . The details of this model are elaborated in sections 3 to 6.", "labels": [], "entities": []}, {"text": "The settings and results of experiments on this new model are given in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The baseline of our experiments is the standard phrase-based model, which achieves, as shown by   shows that the combination of phrase labels and lexical features is even worse than using either phrase labels or lexical features only.", "labels": [], "entities": []}, {"text": "Apart from quantitative evaluation, let us consider the translation example of test 6 shown in table 4.", "labels": [], "entities": []}, {"text": "To generate the correct translation, a phrasebased decoder should, after translating the word \" \" as \"increase\", jump to the last word \" (investment)\".", "labels": [], "entities": []}, {"text": "This is obviously out of the capability of the baseline model, and our approach can accomplish the desired reordering as expected.", "labels": [], "entities": []}, {"text": "By and large, the experiment results show that no matter what kind of reordering knowledge is used, the preprocessing of syntax-based reordering does greatly improve translation performance, and that the reordering of 3-ary nodes is crucial.", "labels": [], "entities": [{"text": "translation", "start_pos": 166, "end_pos": 177, "type": "TASK", "confidence": 0.9625042676925659}]}, {"text": "The second set of experiments test the effect of some constraints.", "labels": [], "entities": []}, {"text": "The basic setting is the same as that of test 6 in the first experiment set, and reordering is applied to both binary and 3-ary nodes.", "labels": [], "entities": []}, {"text": "The results are shown in table 5.", "labels": [], "entities": []}, {"text": "In test (a), the constraint is that the module does not consider any reordering of anode if the yield of this node contains not more than four words.", "labels": [], "entities": []}, {"text": "The underlying rationale is that reordering within distortion limit should be left to the distance-based model during decoding, and syntax-based reordering should focus on global reordering only.", "labels": [], "entities": []}, {"text": "The result shows that this hypothesis does not hold.", "labels": [], "entities": []}, {"text": "In practice syntax-based reordering also helps local reordering.", "labels": [], "entities": []}, {"text": "Consider the translation example of test (a) shown in table 6.", "labels": [], "entities": []}, {"text": "Both the baseline model and our model translate in the same way up to the word \"\" (which is incorrectly translated as \"and\").", "labels": [], "entities": []}, {"text": "From this point, the proposed preprocessing model correctly jump to the last phrase \" /discussed\", while the baseline model fail to do so for the best translation.", "labels": [], "entities": []}, {"text": "It should be noted, however, that there are only four words between \"\" and the last phrase, and the desired order of decoding is within the capability of the baseline system.", "labels": [], "entities": []}, {"text": "With the feature of syntax-based global reordering, a phrase-based decoder performs better even with respect to local reordering.", "labels": [], "entities": []}, {"text": "It is because syntaxbased reordering adds more weight to a hypothesis that moves words across longer distance, which is penalized by the distance-based model.", "labels": [], "entities": []}, {"text": "In test (b) distortion limit is set as 0; i.e. reordering is done merely by syntax-based preprocessing.", "labels": [], "entities": [{"text": "distortion limit", "start_pos": 12, "end_pos": 28, "type": "METRIC", "confidence": 0.9617620408535004}]}, {"text": "The worse result is not surprising since, after all, preprocessing discards many possibilities and thus reduce the search space of the decoder.", "labels": [], "entities": []}, {"text": "Some local reordering model is still needed during decoding.", "labels": [], "entities": []}, {"text": "Finally, test (c) shows that translation performance does not improve significantly by raising the number of reorderings.", "labels": [], "entities": [{"text": "translation", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9638552665710449}]}, {"text": "This implies that our approach is very efficient in that only a small value of n is capable of capturing the most important global reordering patterns.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of Parse Tree Nodes with Dif- ferent Branching Factors Note that nodes with only one", "labels": [], "entities": []}, {"text": " Table 3: Tests on Various Reordering Models", "labels": [], "entities": []}, {"text": " Table 4: Translation Example 1", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9776105284690857}]}]}