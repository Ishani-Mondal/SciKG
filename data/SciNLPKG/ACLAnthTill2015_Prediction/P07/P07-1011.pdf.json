{"title": [{"text": "Detecting Erroneous Sentences using Automatically Mined Sequential Patterns", "labels": [], "entities": [{"text": "Detecting Erroneous Sentences", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8978611429532369}]}], "abstractContent": [{"text": "This paper studies the problem of identifying erroneous/correct sentences.", "labels": [], "entities": [{"text": "identifying erroneous/correct sentences", "start_pos": 34, "end_pos": 73, "type": "TASK", "confidence": 0.7921938300132751}]}, {"text": "The problem has important applications, e.g., providing feedback for writers of English as a Second Language, controlling the quality of parallel bilingual sentences mined from the Web, and evaluating machine translation results.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 201, "end_pos": 220, "type": "TASK", "confidence": 0.7100004553794861}]}, {"text": "In this paper, we propose anew approach to detecting erroneous sentences by integrating pattern discovery with supervised learning models.", "labels": [], "entities": [{"text": "pattern discovery", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7596473097801208}]}, {"text": "Experimental results show that our techniques are promising.", "labels": [], "entities": []}], "introductionContent": [{"text": "Detecting erroneous/correct sentences has the following applications.", "labels": [], "entities": [{"text": "Detecting erroneous/correct sentences", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8920857310295105}]}, {"text": "First, it can provide feedback for writers of English as a Second Language (ESL) as to whether a sentence contains errors.", "labels": [], "entities": []}, {"text": "Second, it can be applied to control the quality of parallel bilingual sentences mined from the Web, which are critical sources fora wide range of applications, such as statistical machine translation ( and cross-lingual information retrieval ().", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 169, "end_pos": 200, "type": "TASK", "confidence": 0.7481959859530131}, {"text": "cross-lingual information retrieval", "start_pos": 207, "end_pos": 242, "type": "TASK", "confidence": 0.6470782657464346}]}, {"text": "Third, it can be used to evaluate machine translation results.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.786531537771225}]}, {"text": "As demonstrated in;), the better human reference translations can be distinguished from machine translations by a classification model, the worse the machine translation system is.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.7339422106742859}]}, {"text": "* Work done while the author was a visiting student at MSRA \u2020 Work done while the author was a visiting student at MSRA The previous work on identifying erroneous sentences mainly aims to find errors from the writing of ESL learners.", "labels": [], "entities": [{"text": "MSRA", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9372760057449341}, {"text": "MSRA", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.9746884107589722}]}, {"text": "The common mistakes () made by ESL learners include spelling, lexical collocation, sentence structure, tense, agreement, verb formation, wrong PartOf-Speech (POS), article usage, etc.", "labels": [], "entities": [{"text": "verb formation", "start_pos": 121, "end_pos": 135, "type": "TASK", "confidence": 0.7573812901973724}, {"text": "article usage", "start_pos": 164, "end_pos": 177, "type": "TASK", "confidence": 0.7491602301597595}]}, {"text": "The previous work focuses on grammar errors, including tense, agreement, verb formation, article usage, etc.", "labels": [], "entities": [{"text": "verb formation", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.7015252858400345}]}, {"text": "However, little work has been done to detect sentence structure and lexical collocation errors.", "labels": [], "entities": []}, {"text": "Some methods of detecting erroneous sentences are based on manual rules.", "labels": [], "entities": []}, {"text": "These methods;) have been shown to be effective in detecting certain kinds of grammatical errors in the writing of English learners.", "labels": [], "entities": []}, {"text": "However, it could be expensive to write rules manually.", "labels": [], "entities": []}, {"text": "Linguistic experts are needed to write rules of high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors.", "labels": [], "entities": []}, {"text": "Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules maybe required.", "labels": [], "entities": []}, {"text": "Worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles and singular plural usage ().", "labels": [], "entities": []}, {"text": "Instead of asking experts to write hand-crafted rules, statistical approaches) build statistical models to identify sentences containing errors.", "labels": [], "entities": []}, {"text": "However, existing statistical approaches focus on some pre-defined errors and the reported results are not attractive.", "labels": [], "entities": []}, {"text": "Moreover, these approaches, e.g.,) usually need errors to be specified and tagged in the training sentences, which requires expert help to be recruited and is time consuming and labor intensive.", "labels": [], "entities": []}, {"text": "Considering the limitations of the previous work, in this paper we propose a novel approach that is based on pattern discovery and supervised learning to successfully identify erroneous/correct sentences.", "labels": [], "entities": [{"text": "pattern discovery", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.717718243598938}]}, {"text": "The basic idea of our approach is to build a machine learning model to automatically classify each sentence into one of the two classes, \"erroneous\" and \"correct.\"", "labels": [], "entities": []}, {"text": "To build the learning model, we automatically extract labeled sequential patterns (LSPs) from both erroneous sentences and correct sentences, and use them as input features for classification models.", "labels": [], "entities": []}, {"text": "Our main contributions are: \u2022 We mine labeled sequential patterns from the preprocessed training data to build leaning models.", "labels": [], "entities": []}, {"text": "Note that LSPs are also very different from N-gram language models that only consider continuous sequences.", "labels": [], "entities": []}, {"text": "\u2022 We also enrich the LSP features with other automatically computed linguistic features, including lexical collocation, language model, syntactic score, and function word density.", "labels": [], "entities": []}, {"text": "In contrast with previous work focusing on (a specific type of) grammatical errors, our model can handle a wide range of errors, including grammar, sentence structure, and lexical choice.", "labels": [], "entities": []}, {"text": "\u2022 We empirically evaluate our methods on two datasets consisting of sentences written by Japanese and Chinese, respectively.", "labels": [], "entities": []}, {"text": "Experimental results show that labeled sequential patterns are highly useful for the classification results, and greatly outperform other features.", "labels": [], "entities": [{"text": "classification", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.9713869094848633}]}, {"text": "Our method outperforms Microsoft Word03 and ALEK) from Educational Testing Service (ETS) in some cases.", "labels": [], "entities": [{"text": "Educational Testing Service (ETS)", "start_pos": 55, "end_pos": 88, "type": "DATASET", "confidence": 0.7909506956736246}]}, {"text": "We also apply our learning model to machine translation (MT) data as a complementary measure to evaluate MT results.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.8291165351867675}, {"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9878972768783569}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section discusses related work.", "labels": [], "entities": []}, {"text": "Section 3 presents the proposed technique.", "labels": [], "entities": []}, {"text": "We evaluate our proposed technique in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 concludes this paper and discusses future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the performance of our techniques with support vector machine (SVM) and Naive Bayesian (NB) classification models.", "labels": [], "entities": []}, {"text": "We also compared the effectiveness of various features.", "labels": [], "entities": []}, {"text": "In addition, we compared our technique with two other methods of checking errors, Microsoft Word03 and ALEK method).", "labels": [], "entities": [{"text": "Microsoft Word03", "start_pos": 82, "end_pos": 98, "type": "DATASET", "confidence": 0.7237597405910492}]}, {"text": "Finally, we also applied our technique to evaluate the Machine Translation outputs.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8112303614616394}]}, {"text": "We used two classification models, SVM and NB classification model.", "labels": [], "entities": [{"text": "NB classification", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.5196671932935715}]}, {"text": "We collected two datasets from different domains, Japanese Corpus (JC) and Chinese Corpus (CC).", "labels": [], "entities": [{"text": "Japanese Corpus (JC)", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.9017025589942932}, {"text": "Chinese Corpus (CC)", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.8922372698783875}]}, {"text": "gives the details of our corpora.", "labels": [], "entities": []}, {"text": "In the learner's corpora, all of the sentences are erroneous.", "labels": [], "entities": []}, {"text": "Note that our data does not consist of parallel pairs of sentences (one error sentence and its correction).", "labels": [], "entities": []}, {"text": "The erroneous sentences includes grammar, sentence structure and lexical choice errors, but not spelling errors.", "labels": [], "entities": []}, {"text": "For each sentence, we generated five kinds of features as presented in Section 3.", "labels": [], "entities": []}, {"text": "For a non-binary feature X, its value x is normalized by z-score, , where mean(x) is the empirical mean of X and var(X) is the variance of X.", "labels": [], "entities": []}, {"text": "Thus each sentence is represented by a vector.", "labels": [], "entities": []}, {"text": "Metrics We calculated the precision, recall, and F-score for correct and erroneous sentences, respectively, and also report the overall accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9997676014900208}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9982977509498596}, {"text": "F-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9994303584098816}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.998806357383728}]}, {"text": "sitions and adverbs, auxiliary verbs, and conjunctions.", "labels": [], "entities": []}, {"text": "6 http://svmlight.joachims.org/ All the experimental results are obtained thorough 10-fold cross-validation.", "labels": [], "entities": []}, {"text": "The Effectiveness of Various Features.", "labels": [], "entities": []}, {"text": "The experiment is to evaluate the contribution of each feature to the classification.", "labels": [], "entities": []}, {"text": "The results of SVM are given in.", "labels": [], "entities": [{"text": "SVM", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.7077016234397888}]}, {"text": "We can see that the performance of labeled sequential patterns (LSP) feature consistently outperforms those of all the other individual features.", "labels": [], "entities": []}, {"text": "It also performs better even if we use all the other features together.", "labels": [], "entities": []}, {"text": "This is because other features only provide some relatively abstract and simple linguistic information, whereas the discovered LSP s characterize significant linguistic features as discussed before.", "labels": [], "entities": []}, {"text": "We also found that the results of NB area little worse than those of SVM.", "labels": [], "entities": [{"text": "NB", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.5601179599761963}]}, {"text": "However, all the features perform consistently on the two classification models and we can observe the same trend.", "labels": [], "entities": []}, {"text": "Due to space limitation, we do not give results of NB.", "labels": [], "entities": [{"text": "NB", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.7160065174102783}]}, {"text": "In addition, the discovered LSPs themselves are intuitive and meaningful since they are intuitive features that can distinguish correct sentences from erroneous sentences.", "labels": [], "entities": []}, {"text": "We discovered 6309 LSPs in JC data and 3742 LSPs in CC data.", "labels": [], "entities": [{"text": "JC data", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.8556945621967316}, {"text": "CC data", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.8187265694141388}]}, {"text": "Some example LSPs discovered from erroneous sentences are <a, NNS> (support:0.39%, confidence:85.71%), <to, VBD> (support:0.11%, confidence:84.21%), and <the, more, the, JJ> (support:0.19%, confidence:0.93%) 7 ; Similarly, we also give some example LSPs mined from correct sentences: <NN, VBZ> (support:2.29%, confidence:75.23%), and <have, VBN, since> (support:0.11%, confidence:85.71%) 8 . However, other features are abstract and it is hard to derive some intuitive knowledge from the opaque statistical values of these features.", "labels": [], "entities": []}, {"text": "As shown in, our technique achieves the highest accuracy, e.g. 81.75% on the Japanese dataset, when we use all the features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9993521571159363}, {"text": "Japanese dataset", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.9431763291358948}]}, {"text": "However, we also notice that the improvement is not very significant compared with using LSP feature individually (e.g. 79.63% on the Japanese dataset).", "labels": [], "entities": [{"text": "Japanese dataset", "start_pos": 134, "end_pos": 150, "type": "DATASET", "confidence": 0.9192758798599243}]}, {"text": "The similar results are observed when we combined the features PLM, SC, FWD, and LC.", "labels": [], "entities": [{"text": "FWD", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.7864004373550415}]}, {"text": "This could be explained 7 a + plural noun; to + past tense format; the more + the + base form of adjective 8 singular or mass noun + the 3 rd person singular present format; have + past participle format + since by two reasons: (1) A sentence may contain several kinds of errors.", "labels": [], "entities": []}, {"text": "A sentence detected to be erroneous by one feature may also be detected by another feature; and (2) Various features give conflicting results.", "labels": [], "entities": []}, {"text": "The two aspects suggest the directions of our future efforts to improve the performance of our models.", "labels": [], "entities": []}, {"text": "It is difficult to find benchmark methods to compare with our technique because, as discussed in Section 2, existing methods often require error tagged corpora or parallel corpora, or focus on a specific type of errors.", "labels": [], "entities": []}, {"text": "In this paper, we compare our technique with the grammar checker of Microsoft Word03 and the ALEK) method used by ETS.", "labels": [], "entities": [{"text": "Microsoft Word03", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.7948747873306274}, {"text": "ETS", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.9347857236862183}]}, {"text": "ALEK is used to detect inappropriate usage of specific vocabulary words.", "labels": [], "entities": [{"text": "ALEK", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6137210726737976}]}, {"text": "Note that we do not consider spelling errors.", "labels": [], "entities": []}, {"text": "Due to space limitation, we only report the precision, recall, F-score for erroneous sentences, and the overall accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9997804760932922}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9995570778846741}, {"text": "F-score", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9994537234306335}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9987114667892456}]}, {"text": "As can be seen from, our method outperforms the other two methods in terms of overall accuracy, F-score, and recall, while the three methods achieve comparable precision.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9968186616897583}, {"text": "F-score", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.9987381100654602}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9997839331626892}, {"text": "precision", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.9984169006347656}]}, {"text": "We realize that the grammar checker of Word is a general tool and the performance of ALEK () can be improved if larger training data is used.", "labels": [], "entities": [{"text": "ALEK", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9232784509658813}]}, {"text": "We found that Word and ALEK usually cannot find sentence structure and lexical collocation errors, e.g., \"The more you listen to English, the easy it becomes.\" contains the discovered LSP <the, more, the, JJ> \u2192 Error.", "labels": [], "entities": []}, {"text": "To study the performance of our method on cross-domain data from writers of the same first-language background, we collected two datasets from Japanese writers, one is composed of 694 parallel sentences (+:347, -:347), and the other 1,671 non-parallel sentences (+:795, -:876).", "labels": [], "entities": []}, {"text": "The two datasets are used as test data while we use JC dataset for training.", "labels": [], "entities": [{"text": "JC dataset", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.9403261542320251}]}, {"text": "Note that the test sentences come from different domains from the JC data.", "labels": [], "entities": [{"text": "JC data", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9091970920562744}]}, {"text": "The results are given in the first two rows of.", "labels": [], "entities": []}, {"text": "This experiment shows that our leaning model trained for one domain can be effectively applied to independent data in the other domains from the writes of the same first-language background, no matter whether the test data is parallel or not.", "labels": [], "entities": []}, {"text": "We also noticed that   To further seethe performance of our method on data written by writers with different firstlanguage backgrounds, we conducted two experiments.", "labels": [], "entities": []}, {"text": "(1) We merge the JC dataset and CC dataset.", "labels": [], "entities": [{"text": "JC dataset", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.9217697381973267}, {"text": "CC dataset", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.7473523020744324}]}, {"text": "The 10-fold cross-validation results on the merged dataset are given in the third row of.", "labels": [], "entities": []}, {"text": "The results demonstrate that our models work well when the training data and test data contain sentences from different first-language backgrounds.", "labels": [], "entities": []}, {"text": "(2) We use the JC dataset (resp. CC dataset) for training while the CC dataset (resp. JC dataset) is used as test data.", "labels": [], "entities": [{"text": "JC dataset (resp. CC dataset", "start_pos": 15, "end_pos": 43, "type": "DATASET", "confidence": 0.8302419980367025}, {"text": "JC dataset", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.7609263360500336}]}, {"text": "As shown in the fourth (resp. fifth) row of, the results are worse than their corresponding results of Word given in.", "labels": [], "entities": []}, {"text": "The reason is that the mistakes made by Japanese and Chinese are different, thus the learning model trained on one data does notwork well on the other data.", "labels": [], "entities": []}, {"text": "Note that our method is not designed to work in this scenario.", "labels": [], "entities": []}, {"text": "Our learning models could be used to evaluate the MT results as an complementary measure.", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.978735625743866}]}, {"text": "This is based on the assumption that if the MT results can be accurately distinguished from human references  The experiment was conducted using 10-fold cross validation on two LDC data, low-ranked and high-ranked data . The results using SVM as classification model are given in.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9770094752311707}, {"text": "LDC data", "start_pos": 177, "end_pos": 185, "type": "DATASET", "confidence": 0.8213547170162201}]}, {"text": "As expected, the classification accuracy on low-ranked data is higher than that on high-ranked data since low-ranked MT results are more different from human references than high-ranked MT results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9720184803009033}, {"text": "MT", "start_pos": 117, "end_pos": 119, "type": "TASK", "confidence": 0.9793948531150818}]}, {"text": "We also found that LSPs are the most effective features.", "labels": [], "entities": [{"text": "LSPs", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.805757999420166}]}, {"text": "In addition, our discovered LSPs could indicate the common errors made by the MT systems and provide some suggestions for improving machine translation results.", "labels": [], "entities": [{"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.8734215497970581}, {"text": "machine translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7401266396045685}]}, {"text": "As a summary, the mined LSPs are indeed effective for the classification models and our proposed technique is effective.: The Results on Machine Translation Data our techniques.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.7333859205245972}]}, {"text": "Moreover, we proposed to mine LSPs as the input of classification models from a set of data containing correct and erroneous sentences.", "labels": [], "entities": []}, {"text": "The LSPs were shown to be much more effective than the other linguistic features although the other features were also beneficial.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The Experimental Results (A: overall accuracy; (-): erroneous sentences; (+): correct sentences; F:  F-score; R: recall; P: precision)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9293310046195984}, {"text": "F-score", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9148455858230591}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9118006825447083}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9322493672370911}]}, {"text": " Table 3: The Comparison Results", "labels": [], "entities": []}, {"text": " Table 4: The Cross-domain Results of our Method", "labels": [], "entities": []}]}