{"title": [{"text": "Boosting Statistical Machine Translation by Lemmatization and Linear Interpolation", "labels": [], "entities": [{"text": "Boosting Statistical Machine Translation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6951633542776108}]}], "abstractContent": [{"text": "Data sparseness is one of the factors that degrade statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 51, "end_pos": 88, "type": "TASK", "confidence": 0.7977601240078608}]}, {"text": "Existing work has shown that using morpho-syntactic information is an effective solution to data sparseness.", "labels": [], "entities": []}, {"text": "However, fewer efforts have been made for Chinese-to-English SMT with using English morpho-syntactic analysis.", "labels": [], "entities": [{"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.8129405379295349}]}, {"text": "We found that while English is a language with less inflection, using En-glish lemmas in training can significantly improve the quality of word alignment that leads to yield better translation performance.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 139, "end_pos": 153, "type": "TASK", "confidence": 0.717750757932663}]}, {"text": "We carried out comprehensive experiments on multiple training data of varied sizes to prove this.", "labels": [], "entities": []}, {"text": "We also proposed anew effective linear interpolation method to integrate multiple homologous features of translation models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Raw parallel data need to be preprocessed in the modern phrase-based SMT before they are aligned by alignment algorithms, one of which is the wellknown tool, GIZA++, for training IBM models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.7722479104995728}]}, {"text": "Morphological analysis (MA) is used in data preprocessing, by which the surface words of the raw data are converted into anew format.", "labels": [], "entities": [{"text": "Morphological analysis (MA)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8649030208587647}]}, {"text": "This new format can be lemmas, stems, parts-of-speech and morphemes or mixes of these.", "labels": [], "entities": []}, {"text": "One benefit of using MA is to ease data sparseness that can reduce the translation quality significantly, especially for tasks with small amounts of training data.", "labels": [], "entities": []}, {"text": "Some published work has shown that applying morphological analysis improved the quality of SMT).", "labels": [], "entities": [{"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9956954717636108}]}, {"text": "We found that all this earlier work involved experiments conducted on translations from highly inflected languages, such as Czech, Arabic, and Spanish, to English.", "labels": [], "entities": []}, {"text": "These researchers also provided detailed descriptions of the effects of foreign language morpho-syntactic analysis but presented no specific results to show the effect of English morphological analysis.", "labels": [], "entities": [{"text": "foreign language morpho-syntactic analysis", "start_pos": 72, "end_pos": 114, "type": "TASK", "confidence": 0.6204417422413826}, {"text": "English morphological analysis", "start_pos": 171, "end_pos": 201, "type": "TASK", "confidence": 0.5496677160263062}]}, {"text": "To the best of our knowledge, there have been no papers related to English morphological analysis for Chinese-to-English (CE) translations even though the CE translation has been the main track for many evaluation campaigns including NIST MT, IWSLT and TC-STAR, where only simple tokenization or lower-case capitalization has been applied to English preprocessing.", "labels": [], "entities": [{"text": "English morphological analysis for Chinese-to-English (CE) translations", "start_pos": 67, "end_pos": 138, "type": "TASK", "confidence": 0.6827796830071343}, {"text": "NIST MT", "start_pos": 234, "end_pos": 241, "type": "DATASET", "confidence": 0.8259492814540863}, {"text": "IWSLT", "start_pos": 243, "end_pos": 248, "type": "DATASET", "confidence": 0.751676082611084}, {"text": "TC-STAR", "start_pos": 253, "end_pos": 260, "type": "DATASET", "confidence": 0.7160316705703735}]}, {"text": "One possible reason why English morphological analysis has been neglected maybe that English is less inflected to the extent that MA may not be effective.", "labels": [], "entities": [{"text": "English morphological analysis", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.6912858585516611}]}, {"text": "However, we found this assumption should not be takenfor-granted.", "labels": [], "entities": []}, {"text": "We studied what effect English lemmatization had on CE translation.", "labels": [], "entities": [{"text": "CE translation", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.9111317694187164}]}, {"text": "Lemmatization is shallow morphological analysis, which uses a lexical entry to replace inflected words.", "labels": [], "entities": [{"text": "shallow morphological analysis", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.6729004482428232}]}, {"text": "For example, the three words, doing, did and done, are replaced by one word, do.", "labels": [], "entities": []}, {"text": "They are all mapped to the same Chinese translations.", "labels": [], "entities": []}, {"text": "As a result, it eases the problem with sparse data, and retains word meanings unchanged.", "labels": [], "entities": []}, {"text": "It is not impossible to improve word alignment by using English lemmatization.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7610868215560913}]}, {"text": "We determined what effect lemmatization had in experiments using data from the BTEC) CSTAR track.", "labels": [], "entities": [{"text": "BTEC) CSTAR track", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.9526936411857605}]}, {"text": "We collected a relatively large corpus of more than 678,000 sentences.", "labels": [], "entities": []}, {"text": "We conducted comprehensive evaluations and used multiple translation metrics to evaluate the results.", "labels": [], "entities": []}, {"text": "We found that our approach of using lemmatization improved both the word alignment and the quality of SMT with a small amounts of training data, and, while much work indicates that MA is useless in training large amounts of data), our intensive experiments proved that the chance to get a better MT quality using lemmatization is higher than that without it for large amounts of training data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.7367284893989563}, {"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.993423581123352}, {"text": "MT", "start_pos": 296, "end_pos": 298, "type": "TASK", "confidence": 0.9863090515136719}]}, {"text": "On the basis of successful use of lemmatization translation, we propose anew linear interpolation method by which we integrate the homologous features of translation models of the lemmatization and non-lemmatization system.", "labels": [], "entities": []}, {"text": "We found the integrated model improved all the components' performance in the translation.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Translation results as increasing amount of training  data in IWSLT06 CSTAR track", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9270655512809753}, {"text": "IWSLT06 CSTAR", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.9020783305168152}]}, {"text": " Table 2: Statistical significance test in terms of BLEU:  sys1=non-lemma, sys2=lemma", "labels": [], "entities": [{"text": "Statistical significance test", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.6677826642990112}, {"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9975219368934631}]}, {"text": " Table 3: Competitive scores (BLEU) for non-lemmatization and  lemmatization using randomly extracted corpora", "labels": [], "entities": [{"text": "Competitive scores (BLEU)", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.7134234845638275}]}, {"text": " Table 4: Effect of linear interpolation", "labels": [], "entities": []}, {"text": " Table 4. An improvement over both of  the systems were observed.", "labels": [], "entities": []}]}