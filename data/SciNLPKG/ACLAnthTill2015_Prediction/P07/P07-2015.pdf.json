{"title": [{"text": "Support Vector Machines for Query-focused Summarization trained and evaluated on Pyramid data", "labels": [], "entities": [{"text": "Query-focused Summarization", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.5953416228294373}]}], "abstractContent": [{"text": "This paper presents the use of Support Vector Machines (SVM) to detect relevant information to be included in a query-focused summary.", "labels": [], "entities": []}, {"text": "Several SVMs are trained using information from pyramids of summary content units.", "labels": [], "entities": []}, {"text": "Their performance is compared with the best performing systems in DUC-2005, using both ROUGE and autoPan, an automatic scoring method for pyramid evaluation.", "labels": [], "entities": [{"text": "DUC-2005", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.952091634273529}, {"text": "ROUGE", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.99140465259552}, {"text": "autoPan", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9103314876556396}]}], "introductionContent": [{"text": "Multi-Document Summarization (MDS) is the task of condensing the most relevant information from several documents in a single one.", "labels": [], "entities": [{"text": "Multi-Document Summarization (MDS)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8476777791976928}]}, {"text": "In terms of the DUC contests 1 , a query-focused summary has to provide a \"brief, well-organized, fluent answer to a need for information\", described by a short query (two or three sentences).", "labels": [], "entities": [{"text": "DUC contests 1", "start_pos": 16, "end_pos": 30, "type": "DATASET", "confidence": 0.8981784184773763}]}, {"text": "DUC participants have to synthesize 250-word sized summaries for fifty sets of 25-50 documents in answer to some queries.", "labels": [], "entities": []}, {"text": "In previous DUC contests, from 2001 to 2004, the manual evaluation was based on a comparison with a single human-written model.", "labels": [], "entities": []}, {"text": "Much information in the evaluated summaries (both human and automatic) was marked as \"related to the topic, but not directly expressed in the model summary\".", "labels": [], "entities": []}, {"text": "Ideally, this relevant information should be scored during the evaluation.", "labels": [], "entities": []}, {"text": "The pyramid method) addresses the problem by using multiple human summaries to create a gold-standard, http://www-nlpir.nist.gov/projects/duc/ and by exploiting the frequency of information in the human summaries in order to assign importance to different facts.", "labels": [], "entities": []}, {"text": "However, the pyramid method requires to manually matching fragments of automatic summaries (peers) to the Semantic Content Units (SCUs) in the pyramids.", "labels": [], "entities": []}, {"text": "AutoPan (), a proposal to automate this matching process, and ROUGE are the evaluation metrics used.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9980467557907104}]}, {"text": "As proposed by, the availability of human-annotated pyramids constitutes a gold-standard that can be exploited in order to train extraction models for the summary automatic construction.", "labels": [], "entities": []}, {"text": "This paper describes several models trained from the information in the DUC-2006 manual pyramid annotations using Support Vector Machines (SVM).", "labels": [], "entities": [{"text": "DUC-2006", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.912668764591217}]}, {"text": "The evaluation, performed on the DUC-2005 data, has allowed us to discover the best configuration for training the SVMs.", "labels": [], "entities": [{"text": "DUC-2005 data", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9884188175201416}]}, {"text": "One of the first applications of supervised Machine Learning techniques in summarization was in Single-Document Summarization ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.9895901083946228}, {"text": "Single-Document Summarization", "start_pos": 96, "end_pos": 125, "type": "TASK", "confidence": 0.7521326243877411}]}, {"text": "used a similar approach for MDS.'s MDS system is based on perceptrons trained on previous DUC data.", "labels": [], "entities": [{"text": "DUC data", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.8416505753993988}]}], "datasetContent": [{"text": "The SVMs, trained on DUC-2006 data, have been tested on the DUC-2005 corpus, using the 20 clusters manually evaluated with the pyramid method.", "labels": [], "entities": [{"text": "DUC-2006 data", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.9797005653381348}, {"text": "DUC-2005 corpus", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.9903574287891388}]}, {"text": "The sentence features were computed as described before.", "labels": [], "entities": []}, {"text": "Finally, the performance of each system has been evaluated automatically using two different measures: ROUGE and autoPan.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.9975045323371887}, {"text": "autoPan", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.9090628027915955}]}, {"text": "ROUGE, the automatic procedure used in DUC, is based on n-gram co-occurrences.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9780468344688416}, {"text": "DUC", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.7026970386505127}]}, {"text": "Both ROUGE-2 (henceforward R-2) and ROUGE-SU4 (R-SU4) has been used to rank automatic summaries.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 5, "end_pos": 12, "type": "METRIC", "confidence": 0.995327353477478}, {"text": "ROUGE-SU4", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9818026423454285}]}, {"text": "AutoPan is a procedure for automatically matching fragments of text summaries to SCUs in pyramids, in the following way: first, the text in the SCU label and all its contributors is stemmed and stop words are removed, obtaining a set of stem vectors for each SCU.", "labels": [], "entities": [{"text": "matching fragments of text summaries", "start_pos": 41, "end_pos": 77, "type": "TASK", "confidence": 0.5740461230278016}]}, {"text": "The system summary text is also stemmed and freed from stop words.", "labels": [], "entities": []}, {"text": "Next, a search for non-overlapping windows of text which can match SCUs is carried.", "labels": [], "entities": []}, {"text": "Each match is scored taking into account the score of the SCU as well as the number of matching stems.", "labels": [], "entities": []}, {"text": "The solution which globally maximizes the sum of scores of all matches is found using dynamic programming techniques.", "labels": [], "entities": []}, {"text": "According to, autoPan scores are highly correlated to the manual pyramid scores.", "labels": [], "entities": [{"text": "autoPan", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.8645326495170593}]}, {"text": "Furthermore, autoPan also correlates well with manual responsiveness and both ROUGE metrics.", "labels": [], "entities": [{"text": "autoPan", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.8511380553245544}, {"text": "ROUGE", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9643771052360535}]}, {"text": "3 shows the results obtained, from which some trends can be found: firstly, the SVMs trained using the set of positive examples obtained from peer summaries consistently outperform SVMs trained using the examples obtained from the manual summaries.", "labels": [], "entities": []}, {"text": "This maybe due to the fact that the number of positive examples is much higher in the first case (on average 48,9 vs. 12,75 examples per cluster).", "labels": [], "entities": []}, {"text": "Secondly, generating automatically a set with seed negative examples for the M-C algorithm, as indicated by, usually performs worse than choosing the strong negative examples from the SCU annotation.", "labels": [], "entities": []}, {"text": "This maybe due to the fact that its quality is better, even though the amount of seed negative examples is one order of magnitude smaller in this case (11.9 examples in average).", "labels": [], "entities": []}, {"text": "Finally, the best results are obtained when using a RBF kernel, while previous summarization work () uses polynomial kernels.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE and autoPan results using different SVMs.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9458652138710022}, {"text": "autoPan", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.7195513844490051}]}]}