{"title": [{"text": "What to be? -Electronic Career Guidance Based on Semantic Relatedness", "labels": [], "entities": [{"text": "Electronic Career Guidance Based on Semantic Relatedness", "start_pos": 13, "end_pos": 69, "type": "TASK", "confidence": 0.6045356265136174}]}], "abstractContent": [{"text": "We present a study aimed at investigating the use of semantic information in a novel NLP application, Electronic Career Guidance (ECG), in German.", "labels": [], "entities": [{"text": "Electronic Career Guidance (ECG)", "start_pos": 102, "end_pos": 134, "type": "TASK", "confidence": 0.6907205432653427}]}, {"text": "ECG is formulated as an information retrieval (IR) task, whereby textual descriptions of professions (documents) are ranked for their relevance to natural language descriptions of a per-son's professional interests (the topic).", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.8523109436035157}]}, {"text": "We compare the performance of two semantic IR models: (IR-1) utilizing semantic relat-edness (SR) measures based on either word-net or Wikipedia and a set of heuristics, and (IR-2) measuring the similarity between the topic and documents based on Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007).", "labels": [], "entities": []}, {"text": "We evaluate the performance of SR measures intrinsically on the tasks of (T-1) computing SR, and (T-2) solving Reader's Digest Word Power (RDWP) questions.", "labels": [], "entities": [{"text": "SR", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9820899963378906}, {"text": "computing SR", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.6626409888267517}, {"text": "solving Reader's Digest Word Power (RDWP) questions", "start_pos": 103, "end_pos": 154, "type": "TASK", "confidence": 0.5768288046121597}]}], "introductionContent": [], "datasetContent": [{"text": "Semantic relatedness datasets for German employed in our study are presented in. conducted experiments with two datasets: i) a German translation of the English dataset by, and ii) a larger dataset containing 350 word pairs (Gur350).", "labels": [], "entities": [{"text": "Gur350", "start_pos": 225, "end_pos": 231, "type": "METRIC", "confidence": 0.4951169490814209}]}, {"text": "created a third dataset from domain-specific corpora using a semi-automatic process (ZG222).", "labels": [], "entities": [{"text": "ZG222", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.924860954284668}]}, {"text": "Gur65 is rather small and contains only noun-noun pairs connected by either synonymy or hypernymy.", "labels": [], "entities": [{"text": "Gur65", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9515136480331421}]}, {"text": "Gur350 contains nouns, verbs and adjectives that are connected by classical and non-classical relations).", "labels": [], "entities": [{"text": "Gur350", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9359841346740723}]}, {"text": "We discarded 44 questions that had more than one correct answer, and 20 questions that used a phrase instead of a single term as query.", "labels": [], "entities": []}, {"text": "The resulting 1008 questions form our evaluation dataset.", "labels": [], "entities": []}, {"text": "An example question is given below: The task is to find the correct choice -'a)' in this case.", "labels": [], "entities": []}, {"text": "This dataset is significantly larger than any of the previous datasets employed in this type of evaluation.", "labels": [], "entities": []}, {"text": "Also, it is not restricted to synonym questions, as in the work by, but also includes hypernymy/hyponymy, and few non-classical relations.", "labels": [], "entities": []}, {"text": "gives the results of evaluation on the task of correlating the results of an SR measure with human judgments using Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 115, "end_pos": 134, "type": "METRIC", "confidence": 0.8880107402801514}]}, {"text": "The GermaNet based LIN measure outperforms ESA on the Gur65 dataset.", "labels": [], "entities": [{"text": "ESA", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.7333336472511292}, {"text": "Gur65 dataset", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.9900896847248077}]}, {"text": "On the other datasets, ESA is better than LIN.", "labels": [], "entities": [{"text": "ESA", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9915114045143127}, {"text": "LIN", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9618232846260071}]}, {"text": "This is clearly due to the fact, that Gur65 contains only noun-noun word pairs connected by classical semantic relations, while the other datasets also contain cross part-of-speech pairs connected by non-classical relations.", "labels": [], "entities": []}, {"text": "The Wikipedia based ESA measure can better capture such relations.", "labels": [], "entities": [{"text": "ESA measure", "start_pos": 20, "end_pos": 31, "type": "DATASET", "confidence": 0.6406393200159073}]}, {"text": "Additionally, shows that ESA also covers almost all  word pairs in each dataset, while GermaNet is much lower for Gur350 and ZG222.", "labels": [], "entities": [{"text": "ZG222", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.840999960899353}]}, {"text": "ESA performs even better on the Reader's Digest task (see).", "labels": [], "entities": [{"text": "ESA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8566256761550903}, {"text": "Reader's Digest task", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.6976303160190582}]}, {"text": "It shows high coverage and near human performance regarding the relative number of correctly solved questions.", "labels": [], "entities": [{"text": "coverage", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9947559833526611}]}, {"text": "Given the high performance and coverage of the Wikipedia based ESA measure, we expect it to yield better IR results than LIN.", "labels": [], "entities": [{"text": "coverage", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9811826348304749}, {"text": "Wikipedia based ESA measure", "start_pos": 47, "end_pos": 74, "type": "DATASET", "confidence": 0.7086845338344574}, {"text": "IR", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.5775696635246277}]}], "tableCaptions": [{"text": " Table 1: Comparison of datasets used for evaluating semantic relatedness in German.", "labels": [], "entities": []}, {"text": " Table 2: Pearson correlation r of human judgments  with SR measures on word pairs covered by Ger- maNet and Wikipedia.", "labels": [], "entities": [{"text": "Pearson correlation r", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.945975124835968}, {"text": "SR", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.7760902047157288}, {"text": "Ger- maNet", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.8614990909894308}]}, {"text": " Table 3: Number of covered word pairs based on Lin  or ESA measure on different datasets.", "labels": [], "entities": [{"text": "Lin", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9930394291877747}, {"text": "ESA", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.8931024074554443}]}, {"text": " Table 4: Evaluation results on multiple-choice word  analogy questions.", "labels": [], "entities": [{"text": "multiple-choice word  analogy", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.6385385096073151}]}, {"text": " Table 5: Information Retrieval performance on the BERUFEnet dataset.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8194323778152466}, {"text": "BERUFEnet dataset", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.8393685221672058}]}]}