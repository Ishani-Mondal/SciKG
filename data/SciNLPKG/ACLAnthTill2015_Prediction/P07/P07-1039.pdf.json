{"title": [{"text": "Bootstrapping Word Alignment via Word Packing", "labels": [], "entities": [{"text": "Bootstrapping Word Alignment", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.618667076031367}]}], "abstractContent": [{"text": "We introduce a simple method to pack words for statistical word alignment.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.6948759059111277}]}, {"text": "Our goal is to simplify the task of automatic word alignment by packing several consecutive words together when we believe they correspond to a single word in the opposite language.", "labels": [], "entities": [{"text": "automatic word alignment", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.60722087820371}]}, {"text": "This is done using the word aligner itself, i.e. by bootstrapping on its output.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our approach on a Chinese-to-English machine translation task, and report a 12.2% relative increase in BLEU score over a state-of-the art phrase-based SMT system.", "labels": [], "entities": [{"text": "Chinese-to-English machine translation task", "start_pos": 49, "end_pos": 92, "type": "TASK", "confidence": 0.6714130491018295}, {"text": "BLEU score", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.9808879792690277}, {"text": "SMT", "start_pos": 182, "end_pos": 185, "type": "TASK", "confidence": 0.8915743231773376}]}], "introductionContent": [{"text": "Automatic word alignment can be defined as the problem of determining a translational correspondence at word level given a parallel corpus of aligned sentences.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6847670674324036}]}, {"text": "Most current statistical models) treat the aligned sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target words.", "labels": [], "entities": []}, {"text": "Before applying such aligners, we thus need to segment the sentences into words -a task which can be quite hard for languages such as Chinese for which word boundaries are not orthographically marked.", "labels": [], "entities": []}, {"text": "More importantly, however, this segmentation is often performed in a monolingual context, which makes the word alignment task more difficult since different languages may realize the same concept using varying numbers of words (see e.g. ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7593606412410736}]}, {"text": "Moreover, a segmentation considered to be \"good\" from a monolingual point of view maybe unadapted for training alignment models.", "labels": [], "entities": []}, {"text": "Although some statistical alignment models allow for 1-to-n word alignments for those reasons, they rarely question the monolingual tokenization and the basic unit of the alignment process remains the word.", "labels": [], "entities": []}, {"text": "In this paper, we focus on 1-to-n alignments with the goal of simplifying the task of automatic word aligners by packing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 332, "end_pos": 346, "type": "TASK", "confidence": 0.7703952193260193}]}, {"text": "Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidates for word packing.", "labels": [], "entities": [{"text": "word packing", "start_pos": 118, "end_pos": 130, "type": "TASK", "confidence": 0.7566298246383667}]}, {"text": "We evaluate the reliability of these candidates, using simple metrics based on co-occurence frequencies, similar to those used in associative approaches to word alignment (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 156, "end_pos": 170, "type": "TASK", "confidence": 0.7711178362369537}]}, {"text": "We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments.", "labels": [], "entities": []}, {"text": "We evaluate the validity of our approach by measuring the influence of the alignment process on a Chinese-to-English Machine Translation (MT) task.", "labels": [], "entities": [{"text": "Chinese-to-English Machine Translation (MT) task", "start_pos": 98, "end_pos": 146, "type": "TASK", "confidence": 0.7794514511312757}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we study the case of 1-ton word alignment.", "labels": [], "entities": [{"text": "1-ton word alignment", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.6768479744593302}]}, {"text": "Section 3 introduces an automatic method to pack together groups of consecutive 304 1: 0 words based on the output from a word aligner.", "labels": [], "entities": []}, {"text": "In Section 4, the experimental setting is described.", "labels": [], "entities": []}, {"text": "In Section 5, we evaluate the influence of our method on the alignment process on a Chinese to English MT task, and experimental results are presented.", "labels": [], "entities": [{"text": "alignment process", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.888493686914444}, {"text": "MT task", "start_pos": 103, "end_pos": 110, "type": "TASK", "confidence": 0.7482779026031494}]}, {"text": "Section 6 concludes the paper and gives avenues for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The intrinsic quality of word alignment can be assessed using the Alignment Error Rate (AER) metric (, that compares a system's alignment output to a set of gold-standard alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7217262089252472}, {"text": "Alignment Error Rate (AER) metric", "start_pos": 66, "end_pos": 99, "type": "METRIC", "confidence": 0.9598038026264736}]}, {"text": "While this method gives a direct evaluation of the quality of word alignment, it is faced with several limitations.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.726840540766716}]}, {"text": "First, it is really difficult to build a reliable and objective gold-standard set, especially for languages as different as Chinese and English.", "labels": [], "entities": []}, {"text": "Second, an increase in AER does not necessarily imply an improvement in translation quality () and vice-versa ().", "labels": [], "entities": [{"text": "AER", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9993517994880676}]}, {"text": "The 307 relationship between word alignments and their impact on MT is also investigated in (;).", "labels": [], "entities": [{"text": "word alignments", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.704630970954895}, {"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9897899031639099}]}, {"text": "Consequently, we chose to extrinsically evaluate the performance of our approach via the translation task, i.e. we measure the influence of the alignment process on the final translation output.", "labels": [], "entities": []}, {"text": "The quality of the translation output is evaluated using BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9974539875984192}]}], "tableCaptions": [{"text": " Table 1: Distribution of alignment types for different language pairs (%)", "labels": [], "entities": []}, {"text": " Table 4: Distribution of alignment types (%)", "labels": [], "entities": []}, {"text": " Table 5: Influence of Chinese segmentation", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.595352828502655}]}, {"text": " Table 5. As  expected, the automatic segmenter leads to slightly  lower results than the human-corrected segmenta- tion. However, the proposed method seems to be  beneficial irrespective of the choice of segmentation.  Indeed, we can also observe an improvement in the  new setting: 2.6 points absolute increase in BLEU  (17.4% relative). 11", "labels": [], "entities": [{"text": "absolute", "start_pos": 295, "end_pos": 303, "type": "METRIC", "confidence": 0.9623542428016663}, {"text": "BLEU", "start_pos": 316, "end_pos": 320, "type": "METRIC", "confidence": 0.9994727969169617}]}]}