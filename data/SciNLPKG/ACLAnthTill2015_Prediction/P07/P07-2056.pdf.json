{"title": [{"text": "Automatic Part-of-Speech Tagging for Bengali: An Approach for Morphologically Rich Languages in a Poor Resource Scenario", "labels": [], "entities": [{"text": "Automatic Part-of-Speech Tagging", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.572484532992045}]}], "abstractContent": [{"text": "This paper describes our work on building Part-of-Speech (POS) tagger for Bengali.", "labels": [], "entities": [{"text": "Part-of-Speech (POS) tagger", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6034072756767273}]}, {"text": "We have use Hidden Markov Model (HMM) and Maximum Entropy (ME) based stochastic taggers.", "labels": [], "entities": []}, {"text": "Bengali is a morphologically rich language and our taggers make use of morphological and contextual information of the words.", "labels": [], "entities": []}, {"text": "Since only a small labeled training set is available (45,000 words), simple stochas-tic approach does not yield very good results.", "labels": [], "entities": []}, {"text": "In this work, we have studied the effect of using a morphological analyzer to improve the performance of the tagger.", "labels": [], "entities": []}, {"text": "We find that the use of morphology helps improve the accuracy of the tagger especially when less amount of tagged corpora are available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.999261200428009}]}], "introductionContent": [{"text": "Part-of-Speech (POS) taggers for natural language texts have been developed using linguistic rules, stochastic models as well as a combination of both (hybrid taggers).", "labels": [], "entities": [{"text": "Part-of-Speech (POS) taggers", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5671428143978119}]}, {"text": "Stochastic models) have been widely used in POS tagging for simplicity and language independence of the models.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.8400633335113525}]}, {"text": "Among stochastic models, bi-gram and tri-gram Hidden Markov Model (HMM) are quite popular.", "labels": [], "entities": []}, {"text": "Development of a high accuracy stochastic tagger requires a large amount of annotated text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9922571182250977}, {"text": "stochastic tagger", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.5198644399642944}]}, {"text": "Stochastic taggers with more than 95% word-level accuracy have been developed for English, German and other European Languages, for which large labeled data is available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9676014184951782}]}, {"text": "Our aim here is to develop a stochastic POS tagger for Bengali but we are limited by lack of a large annotated corpus for Bengali.", "labels": [], "entities": []}, {"text": "Simple HMM models do not achieve high accuracy when the training set is small.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9988090991973877}]}, {"text": "In such cases, additional information maybe coded into the HMM model to achieve higher accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9977892637252808}]}, {"text": "The semi-supervised model described in, makes use of both labeled training text and some amount of unlabeled text.", "labels": [], "entities": []}, {"text": "Incorporating a diverse set of overlapping features in a HMM-based tagger is difficult and complicates the smoothing typically used for such taggers.", "labels": [], "entities": []}, {"text": "In contrast, methods based on Maximum Entropy, Conditional Random Field  etc.", "labels": [], "entities": []}, {"text": "can deal with diverse, overlapping features.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have a total of 12 models as described in subsection 2.1 under different stochastic tagging schemes.", "labels": [], "entities": []}, {"text": "The same training text has been used to estimate the parameters for all the models.", "labels": [], "entities": []}, {"text": "The model parameters for supervised HMM and ME models are estimated from the annotated text corpus.", "labels": [], "entities": []}, {"text": "For semi-supervised learning, the HMM learned through supervised training is considered as the initial model.", "labels": [], "entities": []}, {"text": "Further, a larger unlabelled training data has been used to re-estimate the model parameters of the semi-supervised HMM.", "labels": [], "entities": []}, {"text": "The experiments were conducted with three different sizes (10K, 20K and 40K words) of the training data to understand the relative performance of the models as we keep on increasing the size of the annotated data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tagging accuracies (in %) of different  models with 10K, 20K and 40K training data.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9642168879508972}, {"text": "accuracies", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.6028051376342773}]}, {"text": " Table 2: Five most common types of errors  Almost all the confusions are wrong assignment  due to less number of instances in the training  corpora, including errors due to long distance  phenomena.", "labels": [], "entities": []}]}