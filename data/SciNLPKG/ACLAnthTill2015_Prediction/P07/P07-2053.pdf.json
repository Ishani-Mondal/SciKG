{"title": [{"text": "HunPos -an open source trigram tagger", "labels": [], "entities": []}], "abstractContent": [{"text": "In the world of non-proprietary NLP software the standard, and perhaps the best, HMM-based POS tagger is TnT (Brants, 2000).", "labels": [], "entities": [{"text": "HMM-based POS tagger", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.548030843337377}, {"text": "TnT (Brants, 2000)", "start_pos": 105, "end_pos": 123, "type": "DATASET", "confidence": 0.6507479200760523}]}, {"text": "We argue here that some of the criticism aimed at HMM performance on languages with rich morphology should more properly be directed at TnT's peculiar license , free but not open source, since it is those details of the implementation which are hidden from the user that hold the key for improved POS tagging across a wider variety of languages.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 297, "end_pos": 308, "type": "TASK", "confidence": 0.8922100365161896}]}, {"text": "We present HunPos 1 , a free and open source (LGPL-licensed) alternative , which can be tuned by the user to fully utilize the potential of HMM architec-tures, offering performance comparable to more complex models, but preserving the ease and speed of the training and tagging process.", "labels": [], "entities": []}], "introductionContent": [{"text": "Even without a formal survey it is clear that TnT) is used widely in research labs throughout the world: Google Scholar shows over 400 citations.", "labels": [], "entities": []}, {"text": "For research purposes TnT is freely available, but only in executable form (closed source).", "labels": [], "entities": []}, {"text": "Its greatest advantage is its speed, important both fora fast tuning cycle and when dealing with large corpora, especially when the POS tagger is but one component in a larger information retrieval, information extraction, or question answer-1 http://mokk.bme.hu/resources/hunpos/ ing system.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 199, "end_pos": 221, "type": "TASK", "confidence": 0.7412781268358231}]}, {"text": "Though taggers based on dependency networks (, SVM (, MaxEnt, CRF (), and other methods may reach slightly better results, their train/test cycle is orders of magnitude longer.", "labels": [], "entities": []}, {"text": "A ubiquitous problem in HMM tagging originates from the standard way of calculating lexical probabilities by means of a lexicon generated during training.", "labels": [], "entities": [{"text": "HMM tagging", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9655096232891083}]}, {"text": "In highly inflecting languages considerably more unseen words will be present in the test data than in more isolating languages, which largely accounts for the drop in the performance of n-gram taggers when moving away from English.", "labels": [], "entities": []}, {"text": "To mitigate the effect one needs a morphological dictionary) or a morphological analyzer), but if the implementation source is closed there is no handy way to incorporate morphological knowledge in the tagger.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 1 we present our own system, HunPos, while in Section 2 we describe some of the implementation details of TnT that we believe influence the performance of a HMM based tagging system.", "labels": [], "entities": [{"text": "HunPos", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.6095216870307922}, {"text": "HMM based tagging", "start_pos": 168, "end_pos": 185, "type": "TASK", "confidence": 0.7188595533370972}]}, {"text": "We evaluate the system and compare it to TnT on a variety of tasks in Section 3.", "labels": [], "entities": []}, {"text": "We don't necessarily consider HunPos to be significantly better than TnT, but we argue that we could reach better results, and so could others coming after us, because the system is open to explore all kinds of fine-tuning strategies.", "labels": [], "entities": []}, {"text": "Some concluding remarks close the paper in Section 4. 209", "labels": [], "entities": [{"text": "Section 4. 209", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.8230216304461161}]}], "datasetContent": [{"text": "English For the English evaluation we used the WSJ data from Penn Treebank II.", "labels": [], "entities": [{"text": "WSJ data from Penn Treebank II", "start_pos": 47, "end_pos": 77, "type": "DATASET", "confidence": 0.9198626478513082}]}, {"text": "We extracted sentences from the parse trees.", "labels": [], "entities": []}, {"text": "We split data into training and test set in the standard way.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data set splits used for English", "labels": [], "entities": []}, {"text": " Table 2: WSJ tagging accuracy, HunPos with first  and second order emission/lexicon probabilities", "labels": [], "entities": [{"text": "WSJ tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7360180616378784}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9744324088096619}]}, {"text": " Table 3: Data set splits used for Hungarian.", "labels": [], "entities": []}, {"text": " Table 4: Tagging accuracy for Hungarian of HunPos  with and without morphological lexicon and with  first and second order emission/lexicon probabili- ties.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9305548071861267}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9786152839660645}, {"text": "Hungarian of HunPos", "start_pos": 31, "end_pos": 50, "type": "DATASET", "confidence": 0.8188290397326151}]}]}