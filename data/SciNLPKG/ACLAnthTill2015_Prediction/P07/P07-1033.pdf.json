{"title": [], "abstractContent": [{"text": "We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough \"target\" data to do slightly better than just using only \"source\" data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7737337648868561}]}, {"text": "Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets.", "labels": [], "entities": []}, {"text": "Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.", "labels": [], "entities": [{"text": "multi-domain adaptation problem", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.7827149828275045}]}], "introductionContent": [{"text": "The task of domain adaptation is to develop learning algorithms that can be easily ported from one domain to another-say, from newswire to biomedical documents.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7586501836776733}]}, {"text": "This problem is particularly interesting in NLP because we are often in the situation that we have a large collection of labeled data in one \"source\" domain (say, newswire) but truly desire a model that performs well in a second \"target\" domain.", "labels": [], "entities": []}, {"text": "The approach we present in this paper is based on the idea of transforming the domain adaptation learning problem into a standard supervised learning problem to which any standard algorithm maybe applied (eg., maxent, SVMs, etc.).", "labels": [], "entities": [{"text": "domain adaptation learning problem", "start_pos": 79, "end_pos": 113, "type": "TASK", "confidence": 0.7850498855113983}]}, {"text": "Our transformation is incredibly simple: we augment the feature space of both the source and target data and use the result as input to a standard learning algorithm.", "labels": [], "entities": []}, {"text": "There are roughly two varieties of the domain adaptation problem that have been addressed in the literature: the fully supervised case and the semisupervised case.", "labels": [], "entities": [{"text": "domain adaptation problem", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.788441022237142}]}, {"text": "The fully supervised case models the following scenario.", "labels": [], "entities": []}, {"text": "We have access to a large, annotated corpus of data from a source domain.", "labels": [], "entities": []}, {"text": "In addition, we spend a little money to annotate a small corpus in the target domain.", "labels": [], "entities": []}, {"text": "We want to leverage both annotated datasets to obtain a model that performs well on the target domain.", "labels": [], "entities": []}, {"text": "The semisupervised case is similar, but instead of having a small annotated target corpus, we have a large but unannotated target corpus.", "labels": [], "entities": []}, {"text": "In this paper, we focus exclusively on the fully supervised case.", "labels": [], "entities": []}, {"text": "One particularly nice property of our approach is that it is incredibly easy to implement: the Appendix provides a 10 line, 194 character Perl script for performing the complete transformation (available at http://hal3.name/easyadapt.pl.gz).", "labels": [], "entities": []}, {"text": "In addition to this simplicity, our algorithm performs as well as (or, in some cases, better than) current state of the art techniques.", "labels": [], "entities": []}], "datasetContent": [{"text": "The full-somewhat daunting-table of results is presented in.", "labels": [], "entities": []}, {"text": "The first two columns specify the task and domain.", "labels": [], "entities": []}, {"text": "For the tasks with only a single source and target, we simply report results on the target.", "labels": [], "entities": []}, {"text": "For the multi-domain adaptation tasks, we report results for each setting of the target (where all other data-sets are used as different \"source\" domains).", "labels": [], "entities": [{"text": "multi-domain adaptation", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.7032475471496582}]}, {"text": "The next set of eight columns are the error rates for the task, using one of the different techniques (\"AUGMENT\" is our proposed technique).", "labels": [], "entities": [{"text": "AUGMENT", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.8275744318962097}]}, {"text": "For each row, the error rate of the best performing technique is bolded (as are all techniques whose performance is not statistically significantly different at the 95% level).", "labels": [], "entities": [{"text": "error rate", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9828140139579773}]}, {"text": "The \"T<S\" column is contains a \"+\" whenever TGTONLY outperforms SRCONLY (this will become important shortly).", "labels": [], "entities": [{"text": "TGTONLY outperforms SRCONLY", "start_pos": 44, "end_pos": 71, "type": "METRIC", "confidence": 0.7319867114226023}]}, {"text": "The final column indicates when AUGMENT comes in first.", "labels": [], "entities": [{"text": "AUGMENT", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.6002877950668335}]}, {"text": "There are several trends to note in the results.", "labels": [], "entities": []}, {"text": "Excluding fora moment the \"br-*\" domains on the Treebank-Chunk task, our technique always performs best.", "labels": [], "entities": []}, {"text": "Still excluding \"br-*\", the clear secondplace contestant is the PRIOR model, a finding consistent with prior research.", "labels": [], "entities": []}, {"text": "When we repeat the Treebank-Chunk task, but lumping all of the \"br-*\" data together into a single \"brown\" domain, the story reverts to what we expected before: our algorithm performs best, followed by the PRIOR method.", "labels": [], "entities": []}, {"text": "Importantly, this simple story breaks down on the Treebank-Chunk task for the eight sections of the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 100, "end_pos": 112, "type": "DATASET", "confidence": 0.9328778386116028}]}, {"text": "For these, our AUGMENT technique performs rather poorly.", "labels": [], "entities": [{"text": "AUGMENT", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.5284741520881653}]}, {"text": "Moreover, there is no clear winning approach on this task.", "labels": [], "entities": []}, {"text": "Our hypothesis is that the common feature of these examples is that these are exactly the tasks for which SRCONLY outperforms TGTONLY (with one exception: CoNLL).", "labels": [], "entities": []}, {"text": "This seems like a plausible explanation, since it implies that the source and target domains may not be that different.", "labels": [], "entities": []}, {"text": "If the domains are so similar that a large amount of source data outperforms a small amount of target data, then it is unlikely that blow-  ing up the feature space will help.", "labels": [], "entities": []}, {"text": "We additionally ran the MEGAM model) on these data (though not in the multi-conditional case; for this, we considered the single source as the union of all sources).", "labels": [], "entities": []}, {"text": "The results are not displayed in to save space.", "labels": [], "entities": []}, {"text": "For the majority of results, MEGAM performed roughly comparably to the best of the systems in the table.", "labels": [], "entities": []}, {"text": "In particular, it was not statistically significantly different that AUGMENT on: ACE-NER, CoNLL, PubMed, Treebank-chunk-wsj, Treebank-chunk-swbd3, CNN and Treebank-brown.", "labels": [], "entities": [{"text": "AUGMENT", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.5546762347221375}, {"text": "ACE-NER", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9614042639732361}, {"text": "CoNLL", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.9031172394752502}, {"text": "PubMed", "start_pos": 97, "end_pos": 103, "type": "DATASET", "confidence": 0.929040253162384}, {"text": "CNN", "start_pos": 147, "end_pos": 150, "type": "DATASET", "confidence": 0.9561249613761902}]}, {"text": "It did outperform AUGMENT on the Treebank-chunk on the Treebank-chunk-br-* data sets, but only outperformed the best other model on these data sets for br-cg, br-cm and br-cp.", "labels": [], "entities": [{"text": "AUGMENT", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9737170934677124}, {"text": "Treebank-chunk", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.9693498015403748}, {"text": "Treebank-chunk-br-* data sets", "start_pos": 55, "end_pos": 84, "type": "DATASET", "confidence": 0.8969477564096451}]}, {"text": "However, despite its advantages on these data sets, it was quite significantly slower to train: a single run required about ten times longer than any of the other models (including AUGMENT), and also required five-to-ten iterations of cross-validation to tune its hyperparameters so as to achieve these results.", "labels": [], "entities": [{"text": "AUGMENT", "start_pos": 181, "end_pos": 188, "type": "DATASET", "confidence": 0.7647730112075806}]}], "tableCaptions": [{"text": " Table 1: Task statistics; columns are task, domain,  size of the training, development and test sets, and  the number of unique features in the training set.", "labels": [], "entities": []}]}