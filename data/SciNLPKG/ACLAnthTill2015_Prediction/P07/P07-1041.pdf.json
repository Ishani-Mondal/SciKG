{"title": [{"text": "Generating Constituent Order in German Clauses", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate the factors which determine constituent order in German clauses and propose an algorithm which performs the task in two steps: First, the best candidate for the initial sentence position is chosen.", "labels": [], "entities": []}, {"text": "Then, the order for the remaining constituents is determined.", "labels": [], "entities": []}, {"text": "The first task is more difficult than the second one because of properties of the German sentence-initial position.", "labels": [], "entities": []}, {"text": "Experiments show a significant improvement over competing approaches.", "labels": [], "entities": []}, {"text": "Our algorithm is also more efficient than these.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many natural languages allow variation in the word order.", "labels": [], "entities": []}, {"text": "This is a challenge for natural language generation and machine translation systems, or for text summarizers.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6762319604555765}, {"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.717296913266182}, {"text": "text summarizers", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.7290723919868469}]}, {"text": "E.g., in text-to-text generation), new sentences are fused from dependency structures of input sentences.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.7733505368232727}]}, {"text": "The last step of sentence fusion is linearization of the resulting parse.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.7514322996139526}]}, {"text": "Even for English, which is a language with fixed word order, this is not a trivial task.", "labels": [], "entities": []}, {"text": "German has a relatively free word order.", "labels": [], "entities": []}, {"text": "This concerns the order of constituents 1 within sentences while the order of words within constituents is relatively rigid.", "labels": [], "entities": []}, {"text": "The grammar only partially prescribes how constituents dependent on the verb should be ordered, and for many clauses each of the n!", "labels": [], "entities": []}, {"text": "possible permutations of n constituents is grammatical.", "labels": [], "entities": []}, {"text": "In spite of the permanent interest in German word order in the linguistics community, most studies have limited their scope to the order of verb arguments and few researchers have implemented -and even less evaluated -a generation algorithm.", "labels": [], "entities": [{"text": "German word order", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.632848858833313}]}, {"text": "In this paper, we present an algorithm, which orders not only verb arguments but all kinds of constituents, and evaluate it on a corpus of biographies.", "labels": [], "entities": []}, {"text": "For each parsed sentence in the test set, our maximumentropy-based algorithm aims at reproducing the order found in the original text.", "labels": [], "entities": []}, {"text": "We investigate the importance of different linguistic factors and suggest an algorithm to constituent ordering which first determines the sentence initial constituent and then orders the remaining ones.", "labels": [], "entities": [{"text": "constituent ordering", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.7144700288772583}]}, {"text": "We provide evidence that the task requires language-specific knowledge to achieve better results and point to the most difficult part of it.", "labels": [], "entities": []}, {"text": "Similar to we utilize statistical methods.", "labels": [], "entities": []}, {"text": "Unlike overgeneration approaches, inter alia) which select the best of all possible outputs ours is more efficient, because we do not need to generate every permutation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use several metrics to evaluate our systems and the baselines.", "labels": [], "entities": []}, {"text": "The first is per-sentence accuracy (acc) which is the proportion of correctly regenerated sentences.", "labels": [], "entities": [{"text": "accuracy (acc)", "start_pos": 26, "end_pos": 40, "type": "METRIC", "confidence": 0.9236975759267807}]}, {"text": "Kendall's \u03c4 , which has been used for evaluating sentence ordering tasks, is the second metric we use.", "labels": [], "entities": [{"text": "evaluating sentence ordering tasks", "start_pos": 38, "end_pos": 72, "type": "TASK", "confidence": 0.7030181139707565}]}, {"text": "\u03c4 is calculated as 1 \u2212 4 t N (N \u22121) , where t is the number of interchanges of consecutive elements to arrange N elements in the right order.", "labels": [], "entities": []}, {"text": "\u03c4 is sensitive to near misses and assigns abdc (almost correct order) a score of 0.66 while dcba (inverse order) gets \u22121.", "labels": [], "entities": []}, {"text": "Note that it is questionable whether this metric is as appropriate for word ordering tasks as for sentence ordering ones because a near miss might turnout to be ungrammatical whereas a more different order stays acceptable.", "labels": [], "entities": [{"text": "word ordering tasks", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.8218472599983215}, {"text": "sentence ordering", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.7214026153087616}]}, {"text": "Apart from acc and \u03c4 , we also adopt the metrics used by and.", "labels": [], "entities": [{"text": "acc", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9788903594017029}]}, {"text": "The former use agreement rate (agr) calculated as 2p N (N \u22121) : the number of correctly ordered pairs of constituents over the total number of all possible pairs, as well as complete agreement which is basically per-sentence accuracy.", "labels": [], "entities": [{"text": "agreement rate (agr)", "start_pos": 15, "end_pos": 35, "type": "METRIC", "confidence": 0.9445796728134155}, {"text": "accuracy", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.9441764950752258}]}, {"text": "Unlike \u03c4 , which has \u22121 as the lowest score, agr ranges from 0 to 1.", "labels": [], "entities": [{"text": "agr", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.999210000038147}]}, {"text": "evaluate the performance only in terms of per-constituent edit distance calculated as m N , where m is the minimum number of moves 11 needed to arrange N constituents in the right order.", "labels": [], "entities": []}, {"text": "This measure seems less appropriate than \u03c4 or agr because it does not take the distance of the move into account and scores abced and eabcd equally (0.2).", "labels": [], "entities": [{"text": "agr", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9509130716323853}]}, {"text": "Since \u03c4 and agr, unlike edit distance, give higher scores to better orders, we compute inverse distance: inv = 1 -edit distance instead.", "labels": [], "entities": [{"text": "inverse distance", "start_pos": 87, "end_pos": 103, "type": "METRIC", "confidence": 0.9615026712417603}]}, {"text": "Thus, all three metrics give the maximum of 1 if constituents are ordered correctly.", "labels": [], "entities": []}, {"text": "However, like \u03c4 , agr and inv can give a positive score to an ungrammatical order.", "labels": [], "entities": []}, {"text": "Hence, none of the evaluation metrics describes the performance perfectly.", "labels": [], "entities": []}, {"text": "Human evaluation which reliably distinguishes between appropriate, acceptable, grammatical and ingrammatical orders was out of choice because of its high cost.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of the data sets in clauses", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9744364023208618}]}, {"text": " Table 2: Proportion of clauses with certain lengths", "labels": [], "entities": []}, {"text": " Table  3. The performance of TWO-STEP is significantly  better than any other method (\u03c7 2 , p < 0.01). The  performance of MAXENT does not significantly dif- fer from UCHIMOTO. BIGRAM performed about as  good as UCHIMOTO and MAXENT. We also checked  how well TWO-STEP performs on each of the two  sub-tasks (", "labels": [], "entities": [{"text": "UCHIMOTO", "start_pos": 168, "end_pos": 176, "type": "DATASET", "confidence": 0.9226682186126709}, {"text": "BIGRAM", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.9913362264633179}]}, {"text": " Table 3: Per-clause mean of the results", "labels": [], "entities": [{"text": "Per-clause mean", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9083931744098663}]}]}