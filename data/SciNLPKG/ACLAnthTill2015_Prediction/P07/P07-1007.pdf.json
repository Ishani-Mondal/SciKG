{"title": [{"text": "Domain Adaptation with Active Learning for Word Sense Disambiguation", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7065511643886566}, {"text": "Word Sense Disambiguation", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.6634194056193033}]}], "abstractContent": [{"text": "When a word sense disambiguation (WSD) system is trained on one domain but applied to a different domain, a drop inaccuracy is frequently observed.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 7, "end_pos": 38, "type": "TASK", "confidence": 0.8208521952231725}]}, {"text": "This highlights the importance of domain adaptation for word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.7323494156201681}]}, {"text": "In this paper , we first show that an active learning approach can be successfully used to perform domain adaptation of WSD systems.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.7282073795795441}, {"text": "WSD", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.7995392680168152}]}, {"text": "Then, by using the predominant sense predicted by expectation-maximization (EM) and adopting a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learning approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "In natural language, a word often assumes different meanings, and the task of determining the correct meaning, or sense, of a word in different contexts is known as word sense disambiguation (WSD).", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 165, "end_pos": 196, "type": "TASK", "confidence": 0.7664699504772822}]}, {"text": "To date, the best performing systems in WSD use a corpus-based, supervised learning approach.", "labels": [], "entities": [{"text": "WSD", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9437696933746338}]}, {"text": "With this approach, one would need to collect a text corpus, in which each ambiguous word occurrence is first tagged with its correct sense to serve as training data.", "labels": [], "entities": []}, {"text": "The reliance of supervised WSD systems on annotated corpus raises the important issue of domain dependence.", "labels": [], "entities": [{"text": "WSD", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9437499046325684}]}, {"text": "To investigate this, and conducted experiments using the DSO corpus, which contains sentences from two different corpora, namely Brown Corpus (BC) and Wall Street Journal (WSJ).", "labels": [], "entities": [{"text": "DSO corpus", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.9687963724136353}, {"text": "Brown Corpus (BC)", "start_pos": 129, "end_pos": 146, "type": "DATASET", "confidence": 0.963656771183014}, {"text": "Wall Street Journal (WSJ)", "start_pos": 151, "end_pos": 176, "type": "DATASET", "confidence": 0.9532740612824758}]}, {"text": "They found that training a WSD system on one part (BC or WSJ) of the DSO corpus, and applying it to the other, can result in an accuracy drop of more than 10%, highlighting the need to perform domain adaptation of WSD systems to new domains.", "labels": [], "entities": [{"text": "WSD", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9558047652244568}, {"text": "WSJ) of the DSO corpus", "start_pos": 57, "end_pos": 79, "type": "DATASET", "confidence": 0.6075563331445059}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9990943670272827}]}, {"text": "pointed out that one of the reasons for the drop inaccuracy is the difference in sense priors (i.e., the proportions of the different senses of a word) between BC and WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 167, "end_pos": 170, "type": "DATASET", "confidence": 0.8813011646270752}]}, {"text": "When the authors assumed they knew the sense priors of each word in BC and WSJ, and adjusted these two datasets such that the proportions of the different senses of each word were the same between BC and WSJ, accuracy improved by 9%.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.8482866287231445}, {"text": "WSJ", "start_pos": 204, "end_pos": 207, "type": "DATASET", "confidence": 0.8653156757354736}, {"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.9995417594909668}]}, {"text": "In this paper, we explore domain adaptation of WSD systems, by adding training examples from the new domain as additional training data to a WSD system.", "labels": [], "entities": [{"text": "WSD", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8453412652015686}]}, {"text": "To reduce the effort required to adapt a WSD system to anew domain, we employ an active learning strategy ( to select examples to annotate from the new domain of interest.", "labels": [], "entities": [{"text": "WSD", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9473406672477722}]}, {"text": "To our knowledge, our work is the first to use active learning for domain adaptation for WSD.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7164522856473923}, {"text": "WSD", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9627821445465088}]}, {"text": "A similar work is the recent research by, where active learning was used successfully to reduce the annotation effort for WSD of 5 English verbs using coarse-grained evaluation.", "labels": [], "entities": [{"text": "WSD", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.8699856400489807}]}, {"text": "In that work, the authors only used active learning to reduce the annotation effort and did not deal with the porting of a WSD system to anew domain.", "labels": [], "entities": []}, {"text": "Domain adaptation is necessary when the training and target domains are different.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8040491938591003}]}, {"text": "In this paper,we perform domain adaptation for WSD of a set of nouns using fine-grained evaluation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7163888961076736}, {"text": "WSD", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.912652313709259}]}, {"text": "The contribution of our work is not only in showing that active learning can be successfully employed to reduce the annotation effort required for domain adaptation in a fine-grained WSD setting.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.7077944725751877}]}, {"text": "More importantly, our main focus and contribution is in showing how we can improve the effectiveness of a basic active learning approach when it is used for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.729093536734581}]}, {"text": "In particular, we explore the issue of different sense priors across different domains.", "labels": [], "entities": []}, {"text": "Using the sense priors estimated by expectation-maximization (EM), the predominant sense in the new domain is predicted.", "labels": [], "entities": []}, {"text": "Using this predicted predominant sense and adopting a count-merging technique, we improve the effectiveness of the adaptation process.", "labels": [], "entities": []}, {"text": "In the next section, we discuss the choice of corpus and nouns used in our experiments.", "labels": [], "entities": []}, {"text": "We then introduce active learning for domain adaptation, followed by count-merging.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7442562878131866}]}, {"text": "Next, we describe an EMbased algorithm to estimate the sense priors in the new domain.", "labels": [], "entities": []}, {"text": "Performance of domain adaptation using active learning and count-merging is then presented.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7294883728027344}]}, {"text": "Next, we show that by using the predominant sense of the target domain as predicted by the EM-based algorithm, we improve the effectiveness of the adaptation process.", "labels": [], "entities": []}, {"text": "Our empirical results show that for the set of nouns which have different predominant senses between the training and target domains, we are able to reduce the annotation effort by 71%.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we discuss the motivations for choosing the particular corpus and the set of nouns to conduct our domain adaptation experiments.", "labels": [], "entities": []}, {"text": "For each adaptation experiment, we start off with a classifier built from an initial training set consisting 52  To obtain these curves, we first calculate for each noun, the WSD accuracy when different percentages of adaptation examples are added.", "labels": [], "entities": [{"text": "WSD", "start_pos": 175, "end_pos": 178, "type": "METRIC", "confidence": 0.8790993690490723}, {"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.5183652639389038}]}, {"text": "Then, for each percentage, we calculate the macro-average WSD accuracy overall the nouns to obtain a single learning curve representing all the nouns.", "labels": [], "entities": [{"text": "macro-average WSD accuracy", "start_pos": 44, "end_pos": 70, "type": "METRIC", "confidence": 0.5537081857522329}]}], "tableCaptions": [{"text": " Table 1: The average number of senses in BC and  WSJ, average MFS accuracy, average number of BC  training, and WSJ adaptation examples per noun.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.682534396648407}, {"text": "MFS", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.7240686416625977}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.6559224724769592}, {"text": "WSJ adaptation", "start_pos": 113, "end_pos": 127, "type": "TASK", "confidence": 0.7205495536327362}]}, {"text": " Table 2: Annotation savings and percentage of adap- tation examples needed to reach various accuracies.", "labels": [], "entities": [{"text": "Annotation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9850581884384155}]}]}