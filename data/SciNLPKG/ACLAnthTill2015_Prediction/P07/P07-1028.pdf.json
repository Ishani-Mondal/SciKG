{"title": [{"text": "A Simple, Similarity-based Model for Selectional Preferences", "labels": [], "entities": [{"text": "Selectional Preferences", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.8826961815357208}]}], "abstractContent": [{"text": "We propose anew, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.", "labels": [], "entities": []}, {"text": "Fo-cusing on the task of semantic role labeling, we compute selectional preferences for semantic roles.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.653519203265508}]}, {"text": "In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems.", "labels": [], "entities": [{"text": "coverage", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9704549908638}]}], "introductionContent": [{"text": "Selectional preferences, which characterize typical arguments of predicates, area very useful and versatile knowledge source.", "labels": [], "entities": []}, {"text": "They have been used for example for syntactic disambiguation, word sense disambiguation (WSD) and semantic role labeling (SRL) ().", "labels": [], "entities": [{"text": "syntactic disambiguation", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7809710502624512}, {"text": "word sense disambiguation (WSD)", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.7465823193391165}, {"text": "semantic role labeling (SRL)", "start_pos": 98, "end_pos": 126, "type": "TASK", "confidence": 0.7724656015634537}]}, {"text": "The corpus-based induction of selectional preferences was first proposed by.", "labels": [], "entities": []}, {"text": "All later approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing to other, similar words.", "labels": [], "entities": []}, {"text": "Some approaches have used WordNet for the generalization step, others EM-based clustering (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.941577672958374}, {"text": "generalization", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.9701308608055115}, {"text": "EM-based clustering", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.6236744821071625}]}, {"text": "In this paper we propose anew, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or mutual informationbased metric, for the generalization step.", "labels": [], "entities": [{"text": "selectional preference induction", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.7578110297520956}]}, {"text": "This model does not require any manually created lexical resources.", "labels": [], "entities": []}, {"text": "In addition, the corpus for computing the similarity metrics can be freely chosen, allowing greater variation in the domain of generalization than a fixed lexical resource.", "labels": [], "entities": []}, {"text": "We focus on one application of selectional preferences: semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7175341447194418}]}, {"text": "The argument positions for which we compute selectional preferences will be semantic roles in the FrameNet () paradigm, and the predicates we consider will be semantic classes of words rather than individual words (which means that different preferences will be learned for different senses of a predicate word).", "labels": [], "entities": []}, {"text": "In SRL, the two most pressing issues today are (1) the development of strong semantic features to complement the current mostly syntacticallybased systems, and (2) the problem of the domain dependence).", "labels": [], "entities": [{"text": "SRL", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9522489905357361}]}, {"text": "In the CoNLL-05 shared task, participating systems showed about 10 points F-score difference between in-domain and out-of-domain test data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9990509152412415}]}, {"text": "Concerning (1), we focus on selectional preferences as the strongest candidate for informative semantic features.", "labels": [], "entities": []}, {"text": "Concerning (2), the corpusbased similarity metrics that we use for selectional preference induction open up interesting possibilities of mixing domains.", "labels": [], "entities": [{"text": "selectional preference induction", "start_pos": 67, "end_pos": 99, "type": "TASK", "confidence": 0.6926228801409403}]}, {"text": "We evaluate the similarity-based model against Resnik's WordNet-based model as well as the EM-based clustering approach.", "labels": [], "entities": [{"text": "EM-based clustering", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.6753106117248535}]}, {"text": "In the evaluation, the similarity-model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model.", "labels": [], "entities": []}, {"text": "However, the EM-based clustering model has higher coverage than both other paradigms.", "labels": [], "entities": [{"text": "EM-based clustering", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.781946450471878}]}, {"text": "After discussing previ-216 ous approaches to selectional preference induction in Section 2, we introduce the similaritybased model in Section 3.", "labels": [], "entities": [{"text": "selectional preference induction", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.7476688623428345}]}, {"text": "Section 4 describes the data used for the experiments reported in Section 5, and Section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe experiments comparing the similarity-based model for selectional preferences to Resnik's WordNet-based model and to an EM-based clustering model 3 . For the similarity-based model we test the five similarity metrics and three weighting schemes listed in section 3.", "labels": [], "entities": []}, {"text": "(1999) we evaluate selectional preference induction approaches in a pseudodisambiguation task.", "labels": [], "entities": [{"text": "selectional preference induction", "start_pos": 19, "end_pos": 51, "type": "TASK", "confidence": 0.7267629305521647}]}, {"text": "Ina test set of pairs (r p , w), each headword w is paired with a confounder w chosen randomly from the BNC according to its frequency 4 . Noun headwords are paired with noun confounders in order not to disadvantage Resnik's model, which only works with nouns.", "labels": [], "entities": [{"text": "BNC", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.8572916388511658}]}, {"text": "The headword/confounder pairs are only computed once and reused in all crossvalidation runs.", "labels": [], "entities": []}, {"text": "The task is to choose the more likely role headword from the pair (w, w ).", "labels": [], "entities": []}, {"text": "In the main part of the experiment, we count a pair as covered if both wand ware assigned some level of preference by a model (\"full coverage\").", "labels": [], "entities": []}, {"text": "We contrast this with another condition, where we count a pair as covered if at least one of the two words w, w is assigned a level of preference by a model (\"half coverage\").", "labels": [], "entities": []}, {"text": "If only one is assigned a preference, that word is counted as chosen.", "labels": [], "entities": []}, {"text": "To test the performance difference between models for significance, we use Dietterich's We are grateful to Carsten Brockmann and Detlef Prescher for the use of their software.", "labels": [], "entities": []}, {"text": "We exclude potential confounders that occur less than 30 or more than 3,000 times.", "labels": [], "entities": []}, {"text": "The test involves five 2-fold cross-validation runs.", "labels": [], "entities": []}, {"text": "Let d i,j (i \u2208 {1, 2}, j \u2208 {1, . .", "labels": [], "entities": []}, {"text": ", 5}) be the difference in error rates between the two models when using split i of cross-validation run j as training data.", "labels": [], "entities": []}, {"text": "Let . Then the 5x2cv\u02dct5x2cv\u02dc 5x2cv\u02dct statistic is defined as Under the null hypothesis, the\u02dctthe\u02dc the\u02dct statistic has approximately at distribution with 5 degrees of freedom.", "labels": [], "entities": []}, {"text": "5 Results and discussion Error rates.), this has been attributed to the difference in coverage.", "labels": [], "entities": [{"text": "Error", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9785539507865906}, {"text": "coverage", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9967430233955383}]}, {"text": "In addition to the full coverage condition, we also computed error rate and coverage for the half coverage case.", "labels": [], "entities": [{"text": "coverage", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9621486067771912}, {"text": "error rate", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9876458644866943}, {"text": "coverage", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9946186542510986}]}, {"text": "In this condition, the error rates of the EM-based models are unchanged, while the error rates for all similarity-based models as well as Resnik's model rise to values between 0.4 and 0.6.", "labels": [], "entities": []}, {"text": "So the EM-based model tends to have preferences only for the \"right\" words.", "labels": [], "entities": []}, {"text": "Why this is so is not clear.", "labels": [], "entities": []}, {"text": "It maybe a genuine property, or an artifact of the FrameNet data, which only contains chosen, illustrative sentences for each frame.", "labels": [], "entities": [{"text": "FrameNet data", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9452065527439117}]}, {"text": "It is possible that these sentences have fewer occurrences of highly frequent but semantically less informative role headwords like \"it\" or \"that\" exactly because of their illustrative purpose.", "labels": [], "entities": []}, {"text": "inspects differences between error rates using Dietterich's 5x2cv, basically confirming.", "labels": [], "entities": [{"text": "confirming", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9294601082801819}]}, {"text": "Each cell shows the wins minus losses for the method listed in the row when compared against the method in the column.", "labels": [], "entities": []}, {"text": "The number of cases that did not reach significance is given in brackets.", "labels": [], "entities": []}, {"text": "The coverage rates of the similarity-based models, while comparable to Resnik's model, are considerably lower than for EM-based clustering, which achieves good coverage with 30 and almost perfect coverage with 40 clusters).", "labels": [], "entities": [{"text": "coverage", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9865908622741699}]}, {"text": "While peculiarities of the FrameNet data may have influenced the results in the EM-based model's favor (see the discussion of the half coverage condition above), the low coverage of the similarity-based models is still surprising.", "labels": [], "entities": [{"text": "FrameNet data", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.8925516307353973}, {"text": "half coverage", "start_pos": 130, "end_pos": 143, "type": "METRIC", "confidence": 0.7213166356086731}]}, {"text": "After all, the generalization corpus of the similarity-based models is far larger than the corpus used for clustering.", "labels": [], "entities": []}, {"text": "Given the learning curve in it is unlikely that the reason for the lower coverage is data sparseness.", "labels": [], "entities": [{"text": "coverage", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9600494503974915}]}, {"text": "However, EM-based clustering is a soft clustering method, which relates every predicate and every headword to every cluster, if only with a very low probability.", "labels": [], "entities": [{"text": "EM-based clustering", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7765622437000275}]}, {"text": "In similarity-based models, on the other hand, two words that have never been seen in the same argument slot in the generalization corpus will have zero similarity.", "labels": [], "entities": []}, {"text": "That is, a similarity-based model can assign a level of preference for an argument r p and word w 0 only if R(w 0 ) \u2229 R(Seen(r p )) is nonempty.", "labels": [], "entities": []}, {"text": "Since the flexibility of similarity-based models extends to the vector space for computing similarities, one obvious remedy to the coverage problem would be the use of a less sparse vector space.", "labels": [], "entities": []}, {"text": "Given the low error rates of similarity-based models, it may even be advisable to use two vector spaces, backing off to the denser one for words not covered by the sparse but highly accurate space used in this paper.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Error rate and coverage (micro- average), similarity-based models with uniform  weights.", "labels": [], "entities": [{"text": "Error rate", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9705656468868256}, {"text": "coverage", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.49992823600769043}]}, {"text": " Table 3: Comparing similarity measures: number of wins minus losses (in brackets non-significant  cases) using Dietterich's 5x2cv; uniform weights; condition (1): both members of a pair must be  covered", "labels": [], "entities": []}, {"text": " Table 4: Error rates for similarity-based mod- els, by semantic role frequency band. Micro- averages, uniform weights", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9684814810752869}, {"text": "Micro- averages", "start_pos": 86, "end_pos": 101, "type": "METRIC", "confidence": 0.9366870721181234}]}, {"text": " Table 5: Comparing sim. metrics: (a) avg. freq.  of similar words; (b) % of times the more fre- quent word won; (c) number of distinct similar  words per seen headword; (d) avg. size of inter- section between roles", "labels": [], "entities": []}]}