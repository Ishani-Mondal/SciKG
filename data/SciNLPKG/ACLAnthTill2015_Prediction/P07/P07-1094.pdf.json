{"title": [{"text": "A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging *", "labels": [], "entities": [{"text": "Part-of-Speech Tagging", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.6937312036752701}]}], "abstractContent": [{"text": "Unsupervised learning of linguistic structure is a difficult problem.", "labels": [], "entities": []}, {"text": "A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data.", "labels": [], "entities": []}, {"text": "Typically, this is done using maximum-likelihood estimation (MLE) of the model parameters.", "labels": [], "entities": []}, {"text": "We show using part-of-speech tagging that a fully Bayesian approach can greatly improve performance.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.7124638557434082}]}, {"text": "Rather than estimating a single set of parameters, the Bayesian approach integrates overall possible parameter values.", "labels": [], "entities": []}, {"text": "This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language.", "labels": [], "entities": []}, {"text": "Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discrimi-native model (Smith and Eisner, 2005), up to 14 percentage points better than MLE.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9996411800384521}, {"text": "MLE", "start_pos": 199, "end_pos": 202, "type": "METRIC", "confidence": 0.6746428608894348}]}, {"text": "We find improvements both when training from data alone, and using a tagging dictionary.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised learning of linguistic structure is a difficult problem.", "labels": [], "entities": []}, {"text": "Recently, several new model-based approaches have improved performance on a variety of tasks (; Smith and * This work was supported by grants NSF 0631518 and ONR MURI N000140510388.", "labels": [], "entities": [{"text": "NSF 0631518", "start_pos": 142, "end_pos": 153, "type": "DATASET", "confidence": 0.8759685456752777}, {"text": "ONR", "start_pos": 158, "end_pos": 161, "type": "DATASET", "confidence": 0.585889995098114}, {"text": "MURI", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.6002894639968872}]}, {"text": "We would also like to thank Noah Smith for providing us with his data sets..", "labels": [], "entities": []}, {"text": "Nearly all of these approaches have one aspect in common: the goal of learning is to identify the set of model parameters that maximizes some objective function.", "labels": [], "entities": []}, {"text": "Values for the hidden variables in the model are then chosen based on the learned parameterization.", "labels": [], "entities": []}, {"text": "Here, we propose a different approach based on Bayesian statistical principles: rather than searching for an optimal set of parameter values, we seek to directly maximize the probability of the hidden variables given the observed data, integrating overall possible parameter values.", "labels": [], "entities": []}, {"text": "Using part-of-speech (POS) tagging as an example application, we show that the Bayesian approach provides large performance improvements over maximum-likelihood estimation (MLE) for the same model structure.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 6, "end_pos": 34, "type": "TASK", "confidence": 0.6730586171150208}]}, {"text": "Two factors can explain the improvement.", "labels": [], "entities": []}, {"text": "First, integrating over parameter values leads to greater robustness in the choice of tag sequence, since it must have high probability over a range of parameters.", "labels": [], "entities": []}, {"text": "Second, integration permits the use of priors favoring sparse distributions, which are typical of natural language.", "labels": [], "entities": []}, {"text": "These kinds of priors can lead to degenerate solutions if the parameters are estimated directly.", "labels": [], "entities": []}, {"text": "Before describing our approach in more detail, we briefly review previous work on unsupervised POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.8153951466083527}]}, {"text": "Perhaps the most well-known is that of, who used MLE to train a trigram hidden Markov model (HMM).", "labels": [], "entities": []}, {"text": "More recent work has shown that improvements can be made by modifying the basic HMM structure (), using better smoothing techniques or added constraints (), or using a discriminative model rather than an HMM 744).", "labels": [], "entities": []}, {"text": "Non-model-based approaches have also been proposed (; see also discussion in).", "labels": [], "entities": []}, {"text": "All of this work is really POS disambiguation: learning is strongly constrained by a dictionary listing the allowable tags for each word in the text.", "labels": [], "entities": [{"text": "POS disambiguation", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7219594717025757}]}, {"text": "also present results using a diluted dictionary, where infrequent words may have any tag.", "labels": [], "entities": []}, {"text": "use a small list of labeled prototypes and no dictionary.", "labels": [], "entities": []}, {"text": "A different tradition treats the identification of syntactic classes as a knowledge-free clustering problem.", "labels": [], "entities": []}, {"text": "Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired; probabilistic models have been used to find classes that can improve smoothing and reduce perplexity.", "labels": [], "entities": [{"text": "Distributional clustering", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7384533882141113}, {"text": "dimensionality reduction", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6976313441991806}]}, {"text": "Unfortunately, due to alack of standard and informative evaluation techniques, it is difficult to compare the effectiveness of different clustering methods.", "labels": [], "entities": []}, {"text": "In this paper, we hope to unify the problems of POS disambiguation and syntactic clustering by presenting results for conditions ranging from a full tag dictionary to no dictionary at all.", "labels": [], "entities": [{"text": "POS disambiguation", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.9307045340538025}, {"text": "syntactic clustering", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.7017119377851486}]}, {"text": "We introduce the use of anew information-theoretic criterion, variation of information), which can be used to compare a gold standard clustering to the clustering induced from a tagger's output, regardless of the cluster labels.", "labels": [], "entities": []}, {"text": "We also evaluate using tag accuracy when possible.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8361685276031494}]}, {"text": "Our system outperforms an HMM trained with MLE on both metrics in all circumstances tested, often by a wide margin.", "labels": [], "entities": [{"text": "MLE", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9476998448371887}]}, {"text": "Its accuracy in some cases is close to that of discriminative model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.99951171875}]}, {"text": "Our results show that the Bayesian approach is particularly useful when learning is less constrained, either because less evidence is available (corpus size is small) or because the dictionary contains less information.", "labels": [], "entities": []}, {"text": "In the following section, we discuss the motivation fora Bayesian approach and present our model and search procedure.", "labels": [], "entities": []}, {"text": "Section 3 gives results illustrating how the parameters of the prior affect results, and Section 4 describes how to infer a good choice of parameters from unlabeled data.", "labels": [], "entities": []}, {"text": "Section 5 presents results fora range of corpus sizes and dictionary information, and Section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Percentage of words tagged correctly by  BHMM as a function of the hyperparameters \u03b1 and  \u03b2. Results are averaged over 5 runs on the 24k cor- pus with full tag dictionary. Standard deviations in  most cases are less than .5.", "labels": [], "entities": [{"text": "BHMM", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.7979347109794617}, {"text": "Standard deviations", "start_pos": 182, "end_pos": 201, "type": "METRIC", "confidence": 0.9594417214393616}]}, {"text": " Table 3: Percentage of words tagged correctly and  variation of information between clusterings in- duced by the assigned and gold standard tags as the  amount of information in the dictionary is varied.  Standard deviations (\u03c3) for the BHMM results fell  below those shown in each column. The percentage  of ambiguous tokens and average number of tags per  token for each value of d is also shown.", "labels": [], "entities": [{"text": "Standard deviations (\u03c3)", "start_pos": 206, "end_pos": 229, "type": "METRIC", "confidence": 0.9574121832847595}, {"text": "BHMM", "start_pos": 238, "end_pos": 242, "type": "DATASET", "confidence": 0.6064675450325012}]}]}