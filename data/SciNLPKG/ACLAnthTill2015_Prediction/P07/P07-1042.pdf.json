{"title": [{"text": "A Symbolic Approach to Near-Deterministic Surface Realisation using Tree Adjoining Grammar", "labels": [], "entities": [{"text": "Near-Deterministic Surface Realisation", "start_pos": 23, "end_pos": 61, "type": "TASK", "confidence": 0.598896861076355}]}], "abstractContent": [{"text": "Surface realisers divide into those used in generation (NLG geared realisers) and those mirroring the parsing process (Reversible re-alisers).", "labels": [], "entities": [{"text": "Surface realisers", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.555486410856247}]}, {"text": "While the first rely on grammars not easily usable for parsing, it is unclear how the second type of realisers could be param-eterised to yield from among the set of possible paraphrases, the paraphrase appropriate to a given generation context.", "labels": [], "entities": []}, {"text": "In this paper , we present a surface realiser which combines a reversible grammar (used for parsing and doing semantic construction) with a symbolic means of selecting paraphrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "In generation, the surface realisation task consists in mapping a semantic representation into a grammatical sentence.", "labels": [], "entities": [{"text": "surface realisation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.758612871170044}]}, {"text": "Depending on their use, on their degree of nondeterminism and on the type of grammar they assume, existing surface realisers can be divided into two main categories namely, NLG (Natural Language Generation) geared realisers and reversible realisers.", "labels": [], "entities": [{"text": "Natural Language Generation) geared realisers", "start_pos": 178, "end_pos": 223, "type": "TASK", "confidence": 0.5971839278936386}]}, {"text": "NLG geared realisers are meant as modules in a full-blown generation system and as such, they are constrained to be deterministic: a generation system must output exactly one text, no less, no more.", "labels": [], "entities": [{"text": "NLG geared realisers", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.7969296971956888}]}, {"text": "In order to ensure this determinism, NLG geared realisers generally rely on theories of grammar which systematically link form to function such as systemic functional grammar) and, to a lesser extent, Meaning Text Theory (MTT,).", "labels": [], "entities": [{"text": "NLG geared realisers", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.7643224994341532}, {"text": "Meaning Text Theory (MTT", "start_pos": 201, "end_pos": 225, "type": "TASK", "confidence": 0.6408028066158294}]}, {"text": "In these theories, a sentence is associated not just with a semantic representation but with a semantic representation enriched with additional syntactic, pragmatic and/or discourse information.", "labels": [], "entities": []}, {"text": "This additional information is then used to constrain the realiser output.", "labels": [], "entities": []}, {"text": "One drawback of these NLG geared realisers however, is that the grammar used is not usually reversible i.e., cannot be used both for parsing and for generation.", "labels": [], "entities": []}, {"text": "Given the time and expertise involved in developing a grammar, this is a non-trivial drawback.", "labels": [], "entities": []}, {"text": "Reversible realisers on the other hand, are meant to mirror the parsing process.", "labels": [], "entities": [{"text": "parsing", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.9676342606544495}]}, {"text": "They are used on a grammar developed for parsing and equipped with a compositional semantics.", "labels": [], "entities": []}, {"text": "Given a string and such a grammar, a parser will assign the input string all the semantic representations associated with that string by the grammar.", "labels": [], "entities": []}, {"text": "Conversely, given a semantic representation and the same grammar, a realiser will assign the input semantics all the strings associated with that semantics by the grammar.", "labels": [], "entities": []}, {"text": "In such approaches, non-determinism is usually handled by statistical filtering: treebank induced probabilities are used to select from among the possible paraphrases, the most probable one.", "labels": [], "entities": []}, {"text": "Since the most probable paraphrase is not necessarily the most appropriate one in a given context, it is unclear however, how such realisers could be integrated into a generation system.", "labels": [], "entities": []}, {"text": "In this paper, we present a surface realiser which combines reversibility with a symbolic approach to determinism.", "labels": [], "entities": []}, {"text": "The grammar used is fully reversible (it is used for parsing) and the realisation algorithm can be constrained by the input so as to ensure a unique output conforming to the requirement of a given (generation) context.", "labels": [], "entities": []}, {"text": "We show both that the grammar used has a good paraphrastic power (it is designed in such away that grammatical paraphrases are assigned the same semantic representations) and that the realisation algorithm can be used either to generate all the grammatical paraphrases of a given input or just one provided the input is adequately constrained.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the grammar used namely, a Feature Based Lexicalised Tree Adjoining Grammar enriched with a compositional semantics.", "labels": [], "entities": []}, {"text": "Importantly, this grammar is compiled from a more abstract specification (a so-called \"meta-grammar\") and as we shall see, it is this feature which permits a natural and systematic coupling of semantic literals with syntactic annotations.", "labels": [], "entities": []}, {"text": "Section 3 defines the surface realisation algorithm used to generate sentences from semantic formulae.", "labels": [], "entities": []}, {"text": "This algorithm is non-deterministic and produces all paraphrases associated by the grammar with the input semantics.", "labels": [], "entities": []}, {"text": "We then goon to show (section 4) how this algorithm can be used on a semantic input enriched with syntactic or more abstract control annotations and further, how these annotations can be used to select from among the set of admissible paraphrases precisely these which obey the constraints expressed in the added annotations.", "labels": [], "entities": []}, {"text": "Section 5 reports on a quantitative evaluation based on the use of a core tree adjoining grammar for French.", "labels": [], "entities": []}, {"text": "The evaluation gives an indication of the paraphrasing power of the grammar used as well as some evidence of the deterministic nature of the realiser.", "labels": [], "entities": []}, {"text": "Section 6 relates the proposed approach to existing work and section 7 concludes with pointers for further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate both the paraphrastic power of the realiser and the impact of the control annotations on non-determinism, we used a graduated test-suite which was built by (i) parsing a set of sentences, (ii) selecting the correct meaning representations from the parser output and (iii) generating from these meaning representations.", "labels": [], "entities": []}, {"text": "The gradation in the test suite complexity was obtained by partitioning the input into sentences containing one, two or three finite verbs and by choosing cases allowing for different paraphrasing patterns.", "labels": [], "entities": []}, {"text": "More specifically, the test suite includes cases involving the following types of paraphrases: \u2022 Grammatical variations in the realisations of the arguments (cleft, cliticisation, question, relativisation, subject-inversion, etc.) or of the verb (active/passive, impersonal) \u2022 Variations in the realisation of modifiers (e.g., relative clause vs adjective, predicative vs nonpredicative adjective) \u2022 Variations in the position of modifiers (e.g., pre-vs post-nominal adjective) \u2022 Variations licensed by a morpho-derivational link (e.g., to arrive/arrival) On a test set of 80 cases, the paraphrastic level varies between 1 and over 50 with an average of 18 paraphrases per input (taking 36 as uppercut off point in the paraphrases count).", "labels": [], "entities": []}, {"text": "gives a more detailed description of the distribution of the paraphrastic variation.", "labels": [], "entities": []}, {"text": "In essence, 42% of the sentences with one finite verb accept 1 to 3 paraphrases (cases of intransitive verbs), 44% accept 4 to 28 paraphrases (verbs of arity 2) and 13% yield more than 29 paraphrases (ditransitives).", "labels": [], "entities": []}, {"text": "For sentences containing two finite verbs, the ratio is 5% for 1 to 3 paraphrases, 36% for 4 to 14 paraphrases and 59% for more than 14 paraphrases.", "labels": [], "entities": []}, {"text": "Finally, sentences containing 3 finite verbs all accept more than 29 paraphrases.", "labels": [], "entities": []}, {"text": "Two things are worth noting here.", "labels": [], "entities": []}, {"text": "First, the paraphrase figures might seem low wrt to e.g., work by) which mentions several thousand outputs for one given input and an average number of realisations per input varying between 85.7 and 102.2.", "labels": [], "entities": []}, {"text": "Admittedly, the French grammar we are using has a much more limited coverage than the ERG (the grammar used by) and it is possible that its paraphrastic power is lower.", "labels": [], "entities": []}, {"text": "However, the counts we give only take into account valid paraphrases of the input.", "labels": [], "entities": []}, {"text": "In other words, overgeneration and spurious derivations are excluded from the toll.", "labels": [], "entities": []}, {"text": "This does not seem to be the casein ()'s approach where the count seems to include all sentences associated by the grammar with the input semantics.", "labels": [], "entities": []}, {"text": "Second, although the test set may seem small it is important to keep in mind that it represents 80 inputs 333 with distinct grammatical and paraphrastic properties.", "labels": [], "entities": []}, {"text": "In effect, these 80 test cases yields 1 528 distinct well-formed sentences.", "labels": [], "entities": []}, {"text": "This figure compares favourably with the size of the largest regression test suite used by a symbolic NLG realiser namely, the SURGE test suite which contains 500 input each corresponding to a single sentence.", "labels": [], "entities": [{"text": "SURGE test suite", "start_pos": 127, "end_pos": 143, "type": "DATASET", "confidence": 0.7923398514588674}]}, {"text": "It also compares reasonably with other more recent evaluations) which derive their input data from the Penn Treebank by transforming each sentence tree into a format suitable for the realiser.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.9955897331237793}]}, {"text": "For these approaches, the test set size varies between roughly 1 000 and almost 3 000 sentences.", "labels": [], "entities": []}, {"text": "But again, it is worth stressing that these evaluations aim at assessing coverage and correctness (does the realiser find the sentence used to derive the input by parsing it?) rather than the paraphrastic power of the grammar.", "labels": [], "entities": [{"text": "coverage", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9859089255332947}]}, {"text": "They fail to provide a systematic assessment of how many distinct grammatical paraphrases are associated with each given input.", "labels": [], "entities": []}, {"text": "To verify the claim that tree properties can be used to ensure determinism (cf. footnote 4), we started by eliminating from the output all ill-formed sentences.", "labels": [], "entities": []}, {"text": "We then automatically associated each wellformed output with its set of tree properties.", "labels": [], "entities": []}, {"text": "Finally, for each input semantics, we did a systematic pairwise comparison of the tree property sets associated with the input realisations and we checked whether for any given input, there were two (or more) distinct paraphrases whose tree properties were the same.", "labels": [], "entities": []}, {"text": "We found that such cases represented slightly over 2% of the total number of (input,realisations) pairs.", "labels": [], "entities": []}, {"text": "Closer investigation of the faulty data indicates two main reasons for non-determinism namely, trees with alternating order of arguments and derivations with distinct modifier adjunctions.", "labels": [], "entities": []}, {"text": "Both cases can be handled by modifying the grammar in such away that those differences are reflected in the tree properties.", "labels": [], "entities": []}], "tableCaptions": []}