{"title": [{"text": "A Computational Model of Text Reuse in Ancient Literary Texts", "labels": [], "entities": [{"text": "Text Reuse in Ancient Literary Texts", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.7354229837656021}]}], "abstractContent": [{"text": "We propose a computational model of text reuse tailored for ancient literary texts, available to us often only in small and noisy samples.", "labels": [], "entities": []}, {"text": "The model takes into account source alternation patterns, so as to be able to align even sentences with low surface similarity.", "labels": [], "entities": []}, {"text": "We demonstrate its ability to characterize text reuse in the Greek New Testament.", "labels": [], "entities": [{"text": "characterize text reuse", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.8491909305254618}]}], "introductionContent": [{"text": "Text reuse is the transformation of a source text into a target text in order to serve a different purpose.", "labels": [], "entities": [{"text": "Text reuse", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.7418972253799438}]}, {"text": "Past research has addressed a variety of text-reuse applications, including: journalists turning a news agency text into a newspaper story (); editors adapting an encyclopedia entry to an abridged version (; and plagiarizers disguising their sources by removing surface similarities (.", "labels": [], "entities": [{"text": "journalists turning a news agency text into a newspaper story", "start_pos": 77, "end_pos": 138, "type": "TASK", "confidence": 0.7299036622047425}]}, {"text": "A common assumption in the recovery of text reuse is the conservation of some degree of lexical similarity from the source sentence to the derived sentence.", "labels": [], "entities": []}, {"text": "A simple approach, then, is to define a lexical similarity measure and estimate a score threshold; given a sentence in the target text, if the highest-scoring sentence in the source text is above the threshold, then the former is considered to be derived from the latter.", "labels": [], "entities": []}, {"text": "Obviously, the effectiveness of this basic approach depends on the degree of lexical similarity: source sentences that are quoted verbatim are easier to identify than those that have been transformed by a skillful plagiarizer.", "labels": [], "entities": []}, {"text": "The crux of the question, therefore, is how to identify source sentences despite their lack of surface similarity to the derived sentences.", "labels": [], "entities": []}, {"text": "Ancient literary texts, which are the focus of this paper, present some distinctive challenges in this respect.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model can make two types of errors: source error, when it predicts a non-derived target verse to be derived, or vice versa; and alignment error, when it correctly predicts a target verse to be derived, but aligns it to the wrong source verse.", "labels": [], "entities": [{"text": "alignment error", "start_pos": 132, "end_pos": 147, "type": "METRIC", "confidence": 0.954646497964859}]}, {"text": "Correspondingly, we interpret the output of our model at two levels: as a binary output, i.e., the target verse is either \"derived\" or \"non-derived\"; or, as an alignment of the target verse to a source verse.", "labels": [], "entities": []}, {"text": "We measure the precision and recall of the target verses at both levels, yielding two F-measures, F source and F align 7 . Literary dependencies in the Synoptics are typically expressed as pairs of pericopes (short, coherent passages), for example, \"Luke 22:47-53 // Mark 14:43-52\".", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9985827207565308}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9969865679740906}]}, {"text": "Likewise, for F align , we consider the output correct if the hypothesized source verse lies within the pericope 8 .  This section presents experiments for evaluating our text-reuse model.", "labels": [], "entities": []}, {"text": "\u00a75.1 gives some implementation details.", "labels": [], "entities": []}, {"text": "\u00a75.2 describes the training process, which uses text-reuse hypotheses of two different researchers (L train.B and L train.J ) on the same training text.", "labels": [], "entities": []}, {"text": "The two resulting models thus represent two different opinions on how Luke re-used Mark; they then produce two hypotheses on the test text ( \u02c6 L test.B and\u02c6Land\u02c6 and\u02c6L test.J ).", "labels": [], "entities": []}, {"text": "Evaluations of these hypotheses follow.", "labels": [], "entities": []}, {"text": "In \u00a75.3, we compare them with the hypotheses of the same two researchers on the test text (L test.B and L test.J ).", "labels": [], "entities": []}, {"text": "In \u00a75.3, we compare them with the hypotheses of seven other representative researchers.", "labels": [], "entities": []}, {"text": "Ideally, when the model is trained on a particular researcher's hypothesis on the train text, its hypothesis on the test text should be closest to the one proposed by the same researcher.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Performance on the training text, L train .  The features are accumulative; All refers to the full  feature set.", "labels": [], "entities": []}, {"text": " Table 5: Performance on the test text, L test .", "labels": [], "entities": [{"text": "L", "start_pos": 40, "end_pos": 41, "type": "METRIC", "confidence": 0.9725947380065918}]}]}