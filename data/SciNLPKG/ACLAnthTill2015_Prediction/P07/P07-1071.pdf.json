{"title": [{"text": "Fast Semantic Extraction Using a Novel Neural Network Architecture", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a novel neural network architecture for the problem of semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.7549031178156534}]}, {"text": "Many current solutions are complicated , consist of several stages and hand-built features, and are too slow to be applied as part of real applications that require such semantic labels, partly because of their use of a syntactic parser (Pradhan et al., 2004; Gildea and Jurafsky, 2002).", "labels": [], "entities": []}, {"text": "Our method instead learns a direct mapping from source sentence to semantic tags fora given predicate without the aid of a parser or a chun-ker.", "labels": [], "entities": []}, {"text": "Our resulting system obtains accuracies comparable to the current state-of-the-art at a fraction of the computational cost.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic understanding plays an important role in many end-user applications involving text: for information extraction, web-crawling systems, question and answer based systems, as well as machine translation, summarization and search.", "labels": [], "entities": [{"text": "Semantic understanding", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8104435801506042}, {"text": "information extraction", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.731877788901329}, {"text": "machine translation", "start_pos": 189, "end_pos": 208, "type": "TASK", "confidence": 0.7955238521099091}, {"text": "summarization", "start_pos": 210, "end_pos": 223, "type": "TASK", "confidence": 0.9833499193191528}]}, {"text": "Such applications typically have to be computationally cheap to deal with an enormous quantity of data, e.g. web-based systems process large numbers of documents, whilst interactive human-machine applications require almost instant response.", "labels": [], "entities": []}, {"text": "Another issue is the cost of producing labeled training data required for statistical models, which is exacerbated when those models also depend on syntactic features which must themselves be learnt.", "labels": [], "entities": []}, {"text": "To achieve the goal of semantic understanding, the current consensus is to divide and conquer the).", "labels": [], "entities": [{"text": "semantic understanding", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.8766348958015442}]}, {"text": "ARG0 is typically an actor, REL an action, ARG1 an object, and ARGM describe various modifiers such as location (LOC) and purpose (PNC). problem.", "labels": [], "entities": [{"text": "ARG0", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.859004557132721}, {"text": "ARGM", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.8664791584014893}]}, {"text": "Researchers tackle several layers of processing tasks ranging from the syntactic, such as part-of-speech labeling and parsing, to the semantic: word-sense disambiguation, semantic role-labeling, named entity extraction, co-reference resolution and entailment.", "labels": [], "entities": [{"text": "part-of-speech labeling and parsing", "start_pos": 90, "end_pos": 125, "type": "TASK", "confidence": 0.6434353888034821}, {"text": "word-sense disambiguation", "start_pos": 144, "end_pos": 169, "type": "TASK", "confidence": 0.7027314752340317}, {"text": "named entity extraction", "start_pos": 195, "end_pos": 218, "type": "TASK", "confidence": 0.6610608696937561}, {"text": "co-reference resolution", "start_pos": 220, "end_pos": 243, "type": "TASK", "confidence": 0.7192119657993317}]}, {"text": "None of these tasks are end goals in themselves but can be seen as layers of feature extraction that can help in a language-based end application, such as the ones described above.", "labels": [], "entities": []}, {"text": "Unfortunately, the state-of-the-art solutions of many of these tasks are simply too slow to be used in the applications previously described.", "labels": [], "entities": []}, {"text": "For example, stateof-the-art syntactic parsers theoretically have cubic complexity in the sentence length and several semantic extraction algorithms use the parse tree as an initial feature.", "labels": [], "entities": [{"text": "stateof-the-art syntactic parsers", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.6496994396050771}, {"text": "semantic extraction", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7616367340087891}]}, {"text": "In this work, we describe a novel type of neural network architecture that could help to solve some of these issues.", "labels": [], "entities": []}, {"text": "We focus our experimental study on the semantic role labeling problem (): being able to give a semantic role to a syn-tactic constituent of a sentence, i.e. annotating the predicate argument structure in text (see for example).", "labels": [], "entities": [{"text": "semantic role labeling problem", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.706900417804718}]}, {"text": "Because of its nature, role labeling seems to require the syntactic analysis of a sentence before attributing semantic labels.", "labels": [], "entities": [{"text": "role labeling", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.7772998511791229}]}, {"text": "Using this intuition, state-of-the-art systems first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system, e.g. the ASSERT system ().", "labels": [], "entities": []}, {"text": "This is rather slow, taking a few seconds per sentence attest time, partly because of the parse tree component, and partly because of the use of Support Vector Machines (, which have linear complexity in testing time with respect to the number of training examples.", "labels": [], "entities": []}, {"text": "This makes it hard to apply this method to interesting end user applications.", "labels": [], "entities": []}, {"text": "Here, we propose a radically different approach that avoids the more complex task of building a full parse tree.", "labels": [], "entities": []}, {"text": "From a machine learning point of view, a human does not need to be taught about parse trees to talk.", "labels": [], "entities": []}, {"text": "It is possible, however, that our brains may implicitly learn features highly correlated with those extracted from a parse tree.", "labels": [], "entities": []}, {"text": "We propose to develop an architecture that implements this kind of implicit learning, rather than using explicitly engineered features.", "labels": [], "entities": []}, {"text": "In practice, our system also provides semantic tags at a fraction of the computational cost of other methods, taking on average 0.02 seconds to label a sentence from the Penn Treebank, with almost no loss inaccuracy.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 170, "end_pos": 183, "type": "DATASET", "confidence": 0.9949312508106232}]}, {"text": "The rest of the article is as follows.", "labels": [], "entities": []}, {"text": "First, we describe the problem of shallow semantic parsing in more detail, as well as existing solutions to this problem.", "labels": [], "entities": [{"text": "shallow semantic parsing", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.6440996925036112}]}, {"text": "We then detail our algorithmic approach -the neural network architecture we employ -followed by experiments that evaluate our method.", "labels": [], "entities": []}, {"text": "Finally, we conclude with a summary and discussion of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used Sections 02-21 of the PropBank dataset version 1 for training and validation and Section 23 for testing as standard in all our experiments.", "labels": [], "entities": [{"text": "PropBank dataset version 1", "start_pos": 30, "end_pos": 56, "type": "DATASET", "confidence": 0.9579035490751266}]}, {"text": "We first describe the part-of-speech tagger we employ, and then describe our semantic role labeling experiments.", "labels": [], "entities": [{"text": "part-of-speech tagger", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.6979072093963623}, {"text": "semantic role labeling", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.644522100687027}]}, {"text": "Software for our method, SENNA (Semantic Extraction using a Neural Network Architecture), more details on its implementation, an online applet and test set predictions of our system in comparison to ASSERT can be found at http: //ml.nec-labs.com/software/senna.", "labels": [], "entities": []}, {"text": "Part-Of-Speech Tagger The part-of-speech classifier we employ is a neural network architecture of the same type as in Section 4, where the function Ci = C(i \u2212 pos w ) depends now only on the word position, and not on a verb.", "labels": [], "entities": [{"text": "Part-Of-Speech Tagger", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7475012242794037}]}, {"text": "More precisely: where W k \u2208 Rn hu \u00d7d and wsz is a window size.", "labels": [], "entities": []}, {"text": "We chose wsz = 5 in our experiments.", "labels": [], "entities": []}, {"text": "The d-dimensional vectors learnt take into account the capitalization of a word, and the prefix and suffix calculated using Porter-Stemmer.", "labels": [], "entities": []}, {"text": "See http: //ml.nec-labs.com/software/senna for more details.", "labels": [], "entities": []}, {"text": "We trained on the training set of PropBank supplemented with the Brown corpus, resulting in a test accuracy on the test set of PropBank of 96.85% which compares to 96.66% using the Brill tagger.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9266778826713562}, {"text": "Brown corpus", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.9698772728443146}, {"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9830440878868103}, {"text": "PropBank", "start_pos": 127, "end_pos": 135, "type": "DATASET", "confidence": 0.9319198727607727}, {"text": "Brill tagger", "start_pos": 181, "end_pos": 193, "type": "DATASET", "confidence": 0.937608003616333}]}], "tableCaptions": []}