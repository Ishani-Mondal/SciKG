{"title": [{"text": "Generalizing Tree Transformations for Inductive Dependency Parsing", "labels": [], "entities": [{"text": "Generalizing Tree Transformations", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9060006936391195}, {"text": "Inductive Dependency Parsing", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.580134520928065}]}], "abstractContent": [{"text": "Previous studies in data-driven dependency parsing have shown that tree transformations can improve parsing accuracy for specific parsers and data sets.", "labels": [], "entities": [{"text": "data-driven dependency parsing", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.6469466884930929}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.8960000872612}]}, {"text": "We investigate to what extent this can be generalized across languages/treebanks and parsers, focusing on pseudo-projective parsing, as away of capturing non-projective dependencies, and transformations used to facilitate parsing of coordinate structures and verb groups.", "labels": [], "entities": [{"text": "parsing of coordinate structures and verb groups", "start_pos": 222, "end_pos": 270, "type": "TASK", "confidence": 0.8122201647077288}]}, {"text": "The results indicate that the beneficial effect of pseudo-projective parsing is independent of parsing strategy but sensitive to language or treebank specific properties.", "labels": [], "entities": [{"text": "pseudo-projective parsing", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.5903833210468292}]}, {"text": "By contrast, the construction specific transformations appear to be more sensitive to parsing strategy but have a constant positive effect over several languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Treebank parsers are trained on syntactically annotated sentences and a major part of their success can be attributed to extensive manipulations of the training data as well as the output of the parser, usually in the form of various tree transformations.", "labels": [], "entities": []}, {"text": "This can be seen in state-of-the-art constituency-based parsers such as,, and, and the effects of different transformations have been studied by,.", "labels": [], "entities": []}, {"text": "Corresponding manipulations in the form of tree transformations for dependency-based parsers have recently gained more interest) but are still less studied, partly because constituency-based parsing has dominated the field fora longtime, and partly because dependency structures have less structure to manipulate than constituent structures.", "labels": [], "entities": []}, {"text": "Most of the studies in this tradition focus on a particular parsing model and a particular data set, which means that it is difficult to say whether the effect of a given transformation is dependent on a particular parsing strategy or on properties of a particular language or treebank, or both.", "labels": [], "entities": []}, {"text": "The aim of this study is to further investigate some tree transformation techniques previously proposed for data-driven dependency parsing, with the specific aim of trying to generalize results across languages/treebanks and parsers.", "labels": [], "entities": [{"text": "tree transformation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7791106998920441}, {"text": "dependency parsing", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.6907611638307571}]}, {"text": "More precisely, we want to establish, first of all, whether the transformation as such makes specific assumptions about the language, treebank or parser and, secondly, whether the improved parsing accuracy that is due to a given transformation is constant across different languages, treebanks, and parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.7699717879295349}]}, {"text": "The three types of syntactic phenomena that will be studied here are non-projectivity, coordination and verb groups, which in different ways pose problems for dependency parsers.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.7389447391033173}]}, {"text": "We will focus on tree transformations that combine preprocessing with post-processing, and where the parser is treated as a black box, such as the pseudo-projective parsing technique proposed by and the tree transformations investigated in.", "labels": [], "entities": []}, {"text": "To study the influence of lan-968 guage and treebank specific properties we will use data from Arabic, Czech, Dutch, and Slovene, taken from the CoNLL-X shared task on multilingual dependency parsing).", "labels": [], "entities": [{"text": "multilingual dependency parsing", "start_pos": 168, "end_pos": 199, "type": "TASK", "confidence": 0.5814658304055532}]}, {"text": "To study the influence of parsing methodology, we will compare two different parsers: MaltParser () and MSTParser ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9749277830123901}, {"text": "MSTParser", "start_pos": 104, "end_pos": 113, "type": "DATASET", "confidence": 0.8766282200813293}]}, {"text": "Note that, while it is possible in principle to distinguish between syntactic properties of a language as such and properties of a particular syntactic annotation of the language in question, it will be impossible to tease these apart in the experiments reported here, since this would require having not only multiple languages but also multiple treebanks for each language.", "labels": [], "entities": []}, {"text": "In the following, we will therefore speak about the properties of treebanks (rather than languages), but it should be understood that these properties in general depend both on properties of the language and of the particular syntactic annotation adopted in the treebank.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 surveys tree transformations used in dependency parsing and discusses dependencies between transformations, on the one hand, and treebanks and parsers, on the other.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.8189828097820282}]}, {"text": "Section 3 introduces the four treebanks used in this study, and section 4 briefly describes the two parsers.", "labels": [], "entities": []}, {"text": "Experimental results are presented in section 5 and conclusions in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments reported in section 5.1-5.2 below are based on the training sets from the CoNLL-X shared task, except where noted.", "labels": [], "entities": [{"text": "CoNLL-X shared task", "start_pos": 90, "end_pos": 109, "type": "DATASET", "confidence": 0.7559736172358195}]}, {"text": "The results reported are obtained by a ten-fold cross-validation (with a pseudo-randomized split) for all treebanks except PDT, where 80% of the data was used for training and 20% for development testing (again with a pseudo-randomized split).", "labels": [], "entities": [{"text": "PDT", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.8330036997795105}]}, {"text": "In section 5.3, we give results for the final evaluation on the CoNLL-X test sets using all three transformations together with MaltParser.", "labels": [], "entities": [{"text": "CoNLL-X test sets", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.9442012508710226}]}, {"text": "Parsing accuracy is primarily measured by the unlabeled attachment score (AS U ), i.e., the proportion of tokens that are assigned the correct head, as computed by the official CoNLL-X evaluation script with default settings (thus excluding all punctuation tokens).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.712882936000824}, {"text": "unlabeled attachment score (AS U )", "start_pos": 46, "end_pos": 80, "type": "METRIC", "confidence": 0.8171214972223554}]}, {"text": "In section 5.3 we also include the labeled attachment score (AS L ) (where a token must have both the correct head and the correct dependency label to be counted as correct), which was the official evaluation metric in the CoNLL-X shared task.", "labels": [], "entities": [{"text": "labeled attachment score (AS L )", "start_pos": 35, "end_pos": 67, "type": "METRIC", "confidence": 0.841221741267613}, {"text": "CoNLL-X shared task", "start_pos": 223, "end_pos": 242, "type": "TASK", "confidence": 0.5462756951649984}]}], "tableCaptions": [{"text": " Table 1: Overview of the data sets (ordered by size),  where # S * 1000 = number of sentences, # T * 1000  = number of tokens, %-NPS = percentage of non- projective sentences, %-NPA = percentage of non- projective arcs, %-C = percentage of conjuncts, %-A  = percentage of auxiliary verbs.", "labels": [], "entities": [{"text": "A", "start_pos": 254, "end_pos": 255, "type": "METRIC", "confidence": 0.9461717009544373}]}, {"text": " Table 2: AS U for pseudo-projective parsing with  MaltParser. McNemar's test:  *  = p < .05 and   *  *  = p < 0.01 compared to N-Proj.", "labels": [], "entities": [{"text": "McNemar", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.7150312066078186}]}, {"text": " Table 3: The number of lifts for non-projective arcs.", "labels": [], "entities": []}, {"text": " Table 4: AS U for coordination and verb group trans- formations with MaltParser (None = N-Proj). Mc- Nemar's test:  *  *  = p < .01 compared to None.", "labels": [], "entities": [{"text": "AS U", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9044045507907867}, {"text": "verb group trans- formations", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.5896941483020782}]}, {"text": " Table 5: Pseudo-projective parsing results (AS U ) for  Alpino with MSTParser.", "labels": [], "entities": [{"text": "Pseudo-projective parsing", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7619759440422058}, {"text": "AS U )", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.94650266567866}, {"text": "MSTParser", "start_pos": 69, "end_pos": 78, "type": "DATASET", "confidence": 0.9133976101875305}]}, {"text": " Table 6: Coordination and verb group transforma- tions for PDT with the CLE algorithm.", "labels": [], "entities": []}, {"text": " Table 7: Evaluation on CoNLL-X test data; Malt- Parser with all transformations (Dev = development,  Eval = CoNLL test set, Niv = Nivre et al. (2006),  McD = McDonald et al. (2006))", "labels": [], "entities": [{"text": "CoNLL-X test data", "start_pos": 24, "end_pos": 41, "type": "DATASET", "confidence": 0.823185384273529}, {"text": "CoNLL test set", "start_pos": 109, "end_pos": 123, "type": "DATASET", "confidence": 0.9087278644243876}]}]}