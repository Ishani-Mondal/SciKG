{"title": [{"text": "WordNet-based Semantic Relatedness Measures in Automatic Speech Recognition for Meetings", "labels": [], "entities": [{"text": "WordNet-based Semantic Relatedness Measures", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7994682341814041}, {"text": "Automatic Speech Recognition", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.6216129163901011}]}], "abstractContent": [{"text": "This paper presents the application of WordNet-based semantic relatedness measures to Automatic Speech Recognition (ASR) in multi-party meetings.", "labels": [], "entities": [{"text": "WordNet-based semantic relatedness", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.6342567900816599}, {"text": "Automatic Speech Recognition (ASR)", "start_pos": 86, "end_pos": 120, "type": "TASK", "confidence": 0.7721207489569982}]}, {"text": "Different word-utterance context relatedness measures and utterance-coherence measures are defined and applied to the rescoring of N-best lists.", "labels": [], "entities": []}, {"text": "No significant improvements in terms of Word-Error-Rate (WER) are achieved compared to a large word-based n-gram baseline model.", "labels": [], "entities": [{"text": "Word-Error-Rate (WER)", "start_pos": 40, "end_pos": 61, "type": "METRIC", "confidence": 0.8605071604251862}]}, {"text": "We discuss our results and the relation to other work that achieved an improvement with such models for simpler tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "As () has shown different WordNetbased measures and contexts are best for word prediction in conversational speech.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.7198412269353867}]}, {"text": "The JCN (Section 2.1) measure performs best for nouns using the noun-context.", "labels": [], "entities": [{"text": "JCN (Section 2.1) measure", "start_pos": 4, "end_pos": 29, "type": "DATASET", "confidence": 0.8604838053385416}]}, {"text": "The LESK (Section 2.1) measure performs best for verbs and adjectives using a mixed word-context.", "labels": [], "entities": [{"text": "LESK", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9842656850814819}]}, {"text": "Text-based semantic relatedness measures can improve word prediction on simulated speech recognition hypotheses as () have shown.", "labels": [], "entities": [{"text": "Text-based semantic relatedness", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6452092528343201}, {"text": "word prediction", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.8180750906467438}]}, {"text": "() generated N -best lists from phoneme confusion data acquired from a speech recognizer, and a pronunciation lexicon.", "labels": [], "entities": []}, {"text": "Then sentence hypotheses of varying Word-ErrorRate (WER) were generated based on sentences from different genres from the British National Corpus (BNC).", "labels": [], "entities": [{"text": "Word-ErrorRate (WER)", "start_pos": 36, "end_pos": 56, "type": "METRIC", "confidence": 0.7730048298835754}, {"text": "British National Corpus (BNC)", "start_pos": 122, "end_pos": 151, "type": "DATASET", "confidence": 0.9727927247683207}]}, {"text": "It was shown by them that the semantic model can improve recognition, where the amount of improvement varies with context length and sentence length.", "labels": [], "entities": []}, {"text": "Thereby it was shown that these models can make use of long-term information.", "labels": [], "entities": []}, {"text": "In this paper the best performing measures from, which outperform baseline models on word prediction for conversational telephone speech are used for Automatic Speech Recognition (ASR) in multi-party meetings.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.7251904308795929}, {"text": "Automatic Speech Recognition (ASR)", "start_pos": 150, "end_pos": 184, "type": "TASK", "confidence": 0.7771503080924352}]}, {"text": "Thereby we want to investigate if WordNet-based models can be used for rescoring of 'real' N -best lists in a difficult task.", "labels": [], "entities": [{"text": "rescoring of 'real' N -best lists", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.8236355053053962}]}], "datasetContent": [{"text": "In this first group of experiments Definitions 8 and 9 are applied to the rescoring task.", "labels": [], "entities": [{"text": "rescoring task", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.8826768398284912}]}, {"text": "Similarity scores for each element in an N -best list are derived according to the definitions.", "labels": [], "entities": []}, {"text": "The first-best element of the last list is always added to the context.", "labels": [], "entities": []}, {"text": "The context size is constrained to the last 20 words.", "labels": [], "entities": []}, {"text": "Definition 8 includes context apart from the utterance context, Definition 9 only uses the utterance context.", "labels": [], "entities": []}, {"text": "No improvement over the n-gram baseline is achieved for these two measures.", "labels": [], "entities": []}, {"text": "Neither with the log-linearly interpolated models nor with the WordNet scores alone.", "labels": [], "entities": [{"text": "WordNet scores", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.9523462951183319}]}, {"text": "The differences between the methods in terms of WER are not significant.", "labels": [], "entities": [{"text": "WER", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.7112687826156616}]}, {"text": "In the second group of experiments Definitions 10 and 11 are applied to the rescoring task.", "labels": [], "entities": [{"text": "rescoring task", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.8769518435001373}]}, {"text": "There is again one measure that uses dialog context (10) and one that only uses utterance context (11).", "labels": [], "entities": []}, {"text": "Also for these experiments no improvement over the n-gram baseline is achieved.", "labels": [], "entities": []}, {"text": "Neither with thelog-linearly interpolated models nor with the WordNet scores alone.", "labels": [], "entities": [{"text": "WordNet scores", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.9502169787883759}]}, {"text": "The differences between the methods in terms of WER are also not significant.", "labels": [], "entities": [{"text": "WER", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.7185528874397278}]}, {"text": "There are also no significant differences in performance between the second group and the first group of experiments.", "labels": [], "entities": []}], "tableCaptions": []}