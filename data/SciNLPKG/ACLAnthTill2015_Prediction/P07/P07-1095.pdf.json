{"title": [{"text": "Computationally Efficient M-Estimation of Log-Linear Structure Models *", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe anew loss function, due to Jeon and Lin (2006), for estimating structured log-linear models on arbitrary features.", "labels": [], "entities": []}, {"text": "The loss function can be seen as a (generative) alternative to maximum likelihood estimation with an interesting information-theoretic interpretation , and it is statistically consistent.", "labels": [], "entities": []}, {"text": "It is substantially faster than maximum (conditional) likelihood estimation of conditional random fields (Lafferty et al., 2001; an order of magnitude or more).", "labels": [], "entities": []}, {"text": "We compare its performance and training time to an HMM, a CRF, an MEMM, and pseudolike-lihood on a shallow parsing task.", "labels": [], "entities": []}, {"text": "These experiments help tease apart the contributions of rich features and discriminative training, which are shown to be more than additive.", "labels": [], "entities": []}], "introductionContent": [{"text": "Log-linear models area very popular tool in natural language processing, and are often lauded for permitting the use of \"arbitrary\" and \"correlated\" features of the data by a model.", "labels": [], "entities": []}, {"text": "Users of log-linear models know, however, that this claim requires some qualification: any feature is permitted in principle, but training log-linear models (and decoding under them) is tractable only when the model's independence assumptions permit efficient inference procedures.", "labels": [], "entities": []}, {"text": "For example, in the original conditional random fields (), features were con- * This work was supported by NSF grant IIS-0427206 and the DARPA CALO project.", "labels": [], "entities": [{"text": "NSF grant IIS-0427206", "start_pos": 107, "end_pos": 128, "type": "DATASET", "confidence": 0.7087672154108683}]}, {"text": "The authors are grateful for feedback from David Smith and from three anonymous ACL reviewers, and helpful discussions with Charles Sutton.", "labels": [], "entities": []}, {"text": "fined to locally-factored indicators on label bigrams and label unigrams (with any of the observation).", "labels": [], "entities": []}, {"text": "Even in cases where inference in log-linear models is tractable, it requires the computation of a partition function.", "labels": [], "entities": []}, {"text": "More formally, a log-linear model for random variables X and Y over X, Y defines: p w (x, y) = e w f (x,y) x ,y \u2208X\u00d7Y e w f (x ,y ) = e w f Z(w) (1) where f : X\u00d7Y \u2192 R m is the feature vector-function and w \u2208 R m is a weight vector that parameterizes the model.", "labels": [], "entities": []}, {"text": "In NLP, we rarely train this model by maximizing likelihood, because the partition function Z(w) is expensive to compute exactly.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9545994997024536}]}, {"text": "Z(w) can be approximated (e.g., using Gibbs sampling;.", "labels": [], "entities": []}, {"text": "In this paper, we propose the use of anew loss function that is computationally efficient and statistically consistent ( \u00a72).", "labels": [], "entities": []}, {"text": "Notably, repeated inference is not required during estimation.", "labels": [], "entities": []}, {"text": "This loss function can be seen as a case of M-estimation 1 that was originally developed by for nonparametric density estimation.", "labels": [], "entities": []}, {"text": "This paper gives an information-theoretic motivation that helps elucidate the objective function ( \u00a73), shows how to apply the new estimator to structured models used in NLP ( \u00a74), and compares it to a state-of-the-art noun phrase chunker ( \u00a75).", "labels": [], "entities": []}, {"text": "We discuss implications and future directions in \u00a76.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: NP chunking accuracy on test data us- ing different training methods. The effects of dis- criminative training (CRF) and extended feature sets  (lower section) are more than additive.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8809787034988403}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9597197771072388}]}, {"text": " Table 2: NP chunking accuracy on test data using  different base models for the M-estimator. The \"se- lection\" column shows which accuracy measure was  optimized when selecting the hyperparameter c.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8587384521961212}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9682978391647339}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9964428544044495}]}]}