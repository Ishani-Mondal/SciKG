{"title": [{"text": "Words and Echoes: Assessing and Mitigating the Non-Randomness Problem in Word Frequency Distribution Modeling", "labels": [], "entities": [{"text": "Word Frequency Distribution Modeling", "start_pos": 73, "end_pos": 109, "type": "TASK", "confidence": 0.6912722289562225}]}], "abstractContent": [{"text": "Frequency distribution models tuned to words and other linguistic events can predict the number of distinct types and their frequency distribution in samples of arbitrary sizes.", "labels": [], "entities": []}, {"text": "We conduct, for the first time, a rigorous evaluation of these models based on cross-validation and separation of training and test data.", "labels": [], "entities": []}, {"text": "Our experiments reveal that the prediction accuracy of the models is marred by serious overfitting problems, due to violations of the random sampling assumption in corpus data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.8955954313278198}]}, {"text": "We then propose a simple pre-processing method to alleviate such non-randomness problems.", "labels": [], "entities": []}, {"text": "Further evaluation confirms the effectiveness of the method, which compares favourably to more complex correction techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "Large-Number-of-Rare-Events (LNRE) models) area class of specialized statistical models that allow us to estimate the characteristics of the distribution of type probabilities in type-rich linguistic populations (such as words) from limited samples (our corpora).", "labels": [], "entities": []}, {"text": "They also allow us to extrapolate quantities such as vocabulary size (the number of distinct types) and the number of hapaxes (types occurring just once) beyond a given corpus or make predictions for completely unseen data from the same underlying population.", "labels": [], "entities": []}, {"text": "LNRE models have applications in theoretical linguistics, e.g. for comparing the type richness of morphological or syntactic processes that are attested to different degrees in the data.", "labels": [], "entities": []}, {"text": "Consider for example a very common prefix such as reand a rather rare prefix such as meta-.", "labels": [], "entities": []}, {"text": "With LNRE models we can answer questions such as: If we could obtain as many tokens of meta-as we have of re-, would we also see as many distinct types?", "labels": [], "entities": []}, {"text": "In other words, is the prefix meta-as productive as the prefix re-?", "labels": [], "entities": []}, {"text": "Practical NLP applications, on the other hand, include estimating how many out-ofvocabulary words we will encounter given a lexicon of a certain size, or making informed guesses about type counts in very large data sets (e.g., how many typos are thereon the Internet?)", "labels": [], "entities": []}, {"text": "In this paper, after introducing LNRE models (Section 2), we present an evaluation of their performance based on separate training and test data as well as cross-validation (Section 3).", "labels": [], "entities": []}, {"text": "As far as we know, this is the first time that such a rigorous evaluation has been conducted.", "labels": [], "entities": []}, {"text": "The results show how evaluating on the training set, a common strategy in LNRE research, favours models that overfit the training data and perform poorly on unseen data.", "labels": [], "entities": [{"text": "LNRE research", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.8310087323188782}]}, {"text": "They also confirm the observation by Evert and that current LNRE models achieve only unsatisfactory prediction accuracy, and this is the issue we turn to in the second part of the paper (Section 4).", "labels": [], "entities": [{"text": "Evert", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.686849057674408}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9610622525215149}]}, {"text": "Having identified the violation of the random sampling assumption by real-world data as one of the main factors affecting the quality of the models, we present anew approach to alleviating nonrandomness problems.", "labels": [], "entities": []}, {"text": "Further evaluation shows our solution to outperform partitionadjustment method, the former state-of-the-art in non-randomness correction.", "labels": [], "entities": []}, {"text": "Section 5 concludes by 904 pointing out directions for future work.", "labels": [], "entities": []}, {"text": "introduces a family of models for Zipf-like frequency distributions of linguistic populations, referred to as LNRE models.", "labels": [], "entities": []}, {"text": "Such a linguistic population is formally described by a finite or countably infinite set of types \u03c9 i and their occurrence probabilities \u03c0 i . Word frequency models are not concerned with the probabilities (i.e., relative frequencies) of specific individual types, but rather the overall distribution of these probabilities.", "labels": [], "entities": []}], "datasetContent": [{"text": "LNRE models are traditionally evaluated by looking at how well expected values generated by them fit empirical counts extracted from the same dataset used for parameter estimation, often by visual inspection of differences between observed and predicted data in plots.", "labels": [], "entities": []}, {"text": "More rigorously, and compare the frequency distribution observed in the training set to the one predicted by the model with a multivariate chi-squared test.", "labels": [], "entities": []}, {"text": "As we will show below, evaluating standard LNRE models on the same data that were used to estimate their parameters favours overfitting, which results in poor performance on unseen data.", "labels": [], "entities": []}, {"text": "Evert and Baroni attempt, for the first time, to evaluate LNRE models on unseen data.", "labels": [], "entities": []}, {"text": "However, rather than splitting the data into separate training and test sets, they evaluate the models in an extrapolation setting, where the parameters of the model are estimated on a subset of the data used for testing.", "labels": [], "entities": []}, {"text": "Evert and Baroni do not attempt to cross-validate the results, and they do not provide a quantitative evaluation, relying instead on visual inspection of empirical and observed vocabulary growth curves.", "labels": [], "entities": []}, {"text": "Using the same training and test sets as in Section 3.1, we train the partition-adjusted GIGP model implemented in the LEXSTATS toolkit.", "labels": [], "entities": []}, {"text": "We estimate the parameters of echo-adjusted ZM, fZM and GIGP models on versions of the training corpora that have been pre-processed as described above.", "labels": [], "entities": [{"text": "fZM", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.5977668166160583}, {"text": "GIGP", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9739013910293579}]}, {"text": "The performance of the models is evaluated with the same measures as in Section 3.1 (for prediction of V 1 , echo-adjusted versions of the test data are used).", "labels": [], "entities": []}, {"text": "reports the performance of the echoadjusted fZM and GIGP models and of partitionadjusted GIGP (echo-adjusted ZM performed systematically much worse than the other echo-adjusted models and typically worse than uncorrected ZM, and it is not reported in the.", "labels": [], "entities": [{"text": "fZM", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.6821117997169495}]}, {"text": "Both correction methods lead to a dramatic improvement, bringing the prediction performance of fZM and GIGP to levels comparable to ZM (with the latter outperforming the corrected models on the BNC, but being outperformed on la Repubblica).", "labels": [], "entities": [{"text": "fZM", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.8690764307975769}, {"text": "GIGP", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9565585255622864}, {"text": "BNC", "start_pos": 194, "end_pos": 197, "type": "DATASET", "confidence": 0.9469115734100342}, {"text": "Repubblica", "start_pos": 228, "end_pos": 238, "type": "DATASET", "confidence": 0.4867219030857086}]}, {"text": "Moreover, echo-adjusted GIGP is as good as partitioned GIGP on la Repubblica, and better on both BNC and deWaC, suggesting that the much simpler echo-adjustment method is at least as good and probably better than Baayen's partitioning.", "labels": [], "entities": []}, {"text": "The mean error and confidence interval plots in show that the echo-adjusted models have a much weaker underestimation bias than the corresponding unadjusted models, and are comparable to, if not better than, ZM (although they might have a tendency to display more variance, as clearly illustrated by the performance of echo-adjusted fZM on la Repubblica at 3N 0 prediction size).", "labels": [], "entities": [{"text": "mean error", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.8955457806587219}, {"text": "Repubblica", "start_pos": 343, "end_pos": 353, "type": "DATASET", "confidence": 0.5214686989784241}]}, {"text": "Finally, the echo-adjusted models clearly standout with respect to ZM when it comes to V 1 prediction), indicating that echo-adjusted versions of the more sophisticated fZM and GIGP models should be the focus of future work on improving prediction of the full frequency distribution, rather than plain ZM.", "labels": [], "entities": [{"text": "fZM", "start_pos": 169, "end_pos": 172, "type": "DATASET", "confidence": 0.9226841926574707}]}, {"text": "Moreover, echo-adjusted GIGP is outperforming partitioned GIGP, and emerging as the best model overall.", "labels": [], "entities": []}, {"text": "Reassuringly, for the echoed models there is a very strong positive correlation between goodness-of-fit on the training set and quality of prediction, as illustrated for V prediction at 3N 0 by the triangles in (again, the patterns in this In looking at the V1 data, it must be kept in mind, however, that V1 has a different interpretation when predicted by echo-adjusted models, i.e., it is the number of document-based hapaxes, the number of types that occur in one document only.", "labels": [], "entities": []}, {"text": "figure represent the general trend for echo-adjusted models found in all settings).", "labels": [], "entities": []}, {"text": "11 This indicates that the over-fitting problem has been resolved, and for echo-adjusted models goodness-of-fit on the training set is a reliable indicator of prediction accuracy.", "labels": [], "entities": [{"text": "goodness-of-fit", "start_pos": 96, "end_pos": 111, "type": "METRIC", "confidence": 0.9778938293457031}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.8821409344673157}]}], "tableCaptions": []}