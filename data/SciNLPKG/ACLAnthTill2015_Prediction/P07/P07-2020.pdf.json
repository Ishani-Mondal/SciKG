{"title": [{"text": "Ensemble Document Clustering Using Weighted Hypergraph Generated by NMF", "labels": [], "entities": [{"text": "Ensemble Document Clustering", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7274537980556488}, {"text": "NMF", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.8760718703269958}]}], "abstractContent": [{"text": "In this paper, we propose anew ensemble document clustering method.", "labels": [], "entities": [{"text": "ensemble document clustering", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.6245736181735992}]}, {"text": "The novelty of our method is the use of Non-negative Matrix Factorization (NMF) in the generation phase and a weighted hypergraph in the integration phase.", "labels": [], "entities": [{"text": "Non-negative Matrix Factorization (NMF)", "start_pos": 40, "end_pos": 79, "type": "METRIC", "confidence": 0.7249647130568823}]}, {"text": "In our experiment, we compared our method with some clustering methods.", "labels": [], "entities": []}, {"text": "Our method achieved the best results .", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we propose anew ensemble document clustering method using Non-negative Matrix Factorization (NMF) in the generation phase and a weighted hypergraph in the integration phase.", "labels": [], "entities": [{"text": "ensemble document clustering", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.6329010725021362}]}, {"text": "Document clustering is the task of dividing a document's data set into groups based on document similarity.", "labels": [], "entities": [{"text": "Document clustering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.876261979341507}]}, {"text": "This is the basic intelligent procedure, and is important in text mining systems (M. W..", "labels": [], "entities": [{"text": "text mining", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.8206353783607483}, {"text": "M. W..", "start_pos": 82, "end_pos": 88, "type": "DATASET", "confidence": 0.7012359102567037}]}, {"text": "As the specific application, relevant feedback in IR, where retrieved documents are clustered, is actively researched)().", "labels": [], "entities": [{"text": "IR", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9673780202865601}]}, {"text": "In document clustering, the document is represented as a vector, which typically uses the \"bag of word\" model and the TF-IDF term weight.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.703479528427124}]}, {"text": "A vector represented in this manner is highly dimensional and sparse.", "labels": [], "entities": []}, {"text": "Thus, in document clustering, a dimensional reduction method such as PCA or SVD is applied before actual clustering ().", "labels": [], "entities": [{"text": "document clustering", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7462559342384338}]}, {"text": "Dimensional reduction maps data in a high-dimensional space into a low-dimensional space, and improves both clustering accuracy and speed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9743391275405884}]}, {"text": "NMF is a dimensional reduction method () that is based on the \"aspect model\" used in the Probabilistic Latent Semantic Indexing).", "labels": [], "entities": [{"text": "NMF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8328216075897217}]}, {"text": "Because the axis in the reduced space by NMF corresponds to a topic, the reduced vector represents the clustering result.", "labels": [], "entities": []}, {"text": "For a given termdocument matrix and cluster number, we can obtain the NMF result with an iterative procedure).", "labels": [], "entities": []}, {"text": "However, this iteration does not always converge to a global optimum solution.", "labels": [], "entities": []}, {"text": "That is, NMF results depend on the initial value.", "labels": [], "entities": []}, {"text": "The standard countermeasure for this problem is to generate multiple clustering results by changing the initial value, and then select the best clustering result estimated by an object function.", "labels": [], "entities": []}, {"text": "However, this selection often fails because the object function does not always measure clustering accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9527957439422607}]}, {"text": "To overcome this problem, we use ensemble clustering, which combines multiple clustering results to obtain an accurate clustering result.", "labels": [], "entities": []}, {"text": "Ensemble clustering consists of generation and integration phases.", "labels": [], "entities": [{"text": "Ensemble clustering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7276841104030609}]}, {"text": "The generation phase produces multiple clustering results.", "labels": [], "entities": []}, {"text": "Many strategies have been proposed to achieve this goal, including random initialization), feature extraction based on random projection) and the combination of sets of \"weak\" partitions (.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7832047343254089}]}, {"text": "The integration phase, as the name implies, integrates multiple clustering results to improve the accuracy of the final clustering result.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9987332224845886}]}, {"text": "This phase primarily relies on two methods.", "labels": [], "entities": []}, {"text": "The first method constructs anew simi-77 larity matrix from multiple clustering results).", "labels": [], "entities": []}, {"text": "The second method constructs new vectors for each instance data using multiple clustering results).", "labels": [], "entities": []}, {"text": "Both methods apply the clustering procedure to the new object to obtain the final clustering result.", "labels": [], "entities": []}, {"text": "Our method generates multiple clustering results by random initialization of the NMF, and integrates them with a weighted hypergraph instead of the standard hypergraph).", "labels": [], "entities": []}, {"text": "An advantage of our method is that the weighted hypergraph can be directly obtained from the NMF result.", "labels": [], "entities": []}, {"text": "In our experiment, we compared the k-means, NMF, the ensemble method using a standard hypergraph and the ensemble method using a weighted hypergraph.", "labels": [], "entities": []}, {"text": "Our method achieved the best results.", "labels": [], "entities": []}, {"text": "In NMF, the clustering result depends on the initial values.", "labels": [], "entities": []}, {"text": "Generally, we conduct NMF several times with random initialization, and then select the clustering result with the smallest value of Eq.4.", "labels": [], "entities": [{"text": "Eq.4", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9653211236000061}]}, {"text": "The value of Eq.4 represents the NMF decomposition error and not the clustering error.", "labels": [], "entities": [{"text": "Eq.4", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9490755200386047}, {"text": "NMF decomposition error", "start_pos": 33, "end_pos": 56, "type": "METRIC", "confidence": 0.5223177870114645}]}, {"text": "Thus, we cannot alway select the best result.", "labels": [], "entities": []}], "datasetContent": [{"text": "To confirm the effectiveness of our method, we compared the k-means, NMF, the ensemble method using a standard hypergraph and the ensemble method using a weighted hypergraph.", "labels": [], "entities": []}, {"text": "In our experiment, we use 18 document data sets provided at http://glaros.dtc.umn.edu/ gkhome/cluto/cluto/download.", "labels": [], "entities": []}, {"text": "The document vector is not normalized for each data set.", "labels": [], "entities": []}, {"text": "We normalize them using TF-IDF.", "labels": [], "entities": [{"text": "normalize", "start_pos": 3, "end_pos": 12, "type": "TASK", "confidence": 0.9658077359199524}, {"text": "TF-IDF", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.7545212507247925}]}, {"text": "shows the result of the experiment . The value in the table represents entropy, and the smaller it is, the better the clustering result.", "labels": [], "entities": []}, {"text": "In NMF, we generated 20 clustering results using random initialization, and selected the cluster- We used the clustering toolkit CLUTO for clustering the hypergraph.", "labels": [], "entities": []}, {"text": "ing result with the smallest decomposition error.", "labels": [], "entities": []}, {"text": "The selected clustering result is shown as \"NMF\" in.", "labels": [], "entities": []}, {"text": "\"NMF means\" in is the average of 20 entropy values for 20 clustering results.", "labels": [], "entities": []}, {"text": "The \"standard hypergraph\" and \"weighted hypergraph\" in show the results of the ensemble method obtained using the two hypergraph types.", "labels": [], "entities": []}, {"text": "shows the effectiveness of our method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Document data sets and Experiment results", "labels": [], "entities": []}]}