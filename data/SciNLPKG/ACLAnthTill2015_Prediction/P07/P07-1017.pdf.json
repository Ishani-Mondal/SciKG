{"title": [{"text": "Generating Complex Morphology for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.8003536760807037}]}], "abstractContent": [{"text": "We present a novel method for predicting inflected word forms for generating morphologically rich languages in machine translation.", "labels": [], "entities": [{"text": "predicting inflected word forms", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.8504353761672974}, {"text": "machine translation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7196927815675735}]}, {"text": "We utilize a rich set of syntactic and morphological knowledge sources from both source and target sentences in a prob-abilistic model, and evaluate their contribution in generating Russian and Arabic sentences.", "labels": [], "entities": []}, {"text": "Our results show that the proposed model substantially outperforms the commonly used baseline of a trigram target language model; in particular, the use of morphological and syntactic features leads to large gains in prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 228, "end_pos": 236, "type": "METRIC", "confidence": 0.9058883786201477}]}, {"text": "We also show that the proposed method is effective with a relatively small amount of data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine Translation (MT) quality has improved substantially in recent years due to applying data intensive statistical techniques.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8831377148628234}]}, {"text": "However, state-ofthe-art approaches are essentially lexical, considering every surface word or phrase in both the source sentence and the corresponding translation as an independent entity.", "labels": [], "entities": []}, {"text": "A shortcoming of this word-based approach is that it is sensitive to data sparsity.", "labels": [], "entities": []}, {"text": "This is an issue of importance as aligned corpora are an expensive resource, which is not abundantly available for many language pairs.", "labels": [], "entities": []}, {"text": "This is particularly problematic for morphologically rich languages, where word stems are realized in many different surface forms, which exacerbates the sparsity problem.", "labels": [], "entities": []}, {"text": "In this paper, we explore an approach in which words are represented as a collection of morphological entities, and use this information to aid in MT for morphologically rich languages.", "labels": [], "entities": [{"text": "MT", "start_pos": 147, "end_pos": 149, "type": "TASK", "confidence": 0.9969372749328613}]}, {"text": "Our goal is twofold: first, to allow generalization over morphology to alleviate the data sparsity problem in morphology generation.", "labels": [], "entities": [{"text": "morphology generation", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.7216560542583466}]}, {"text": "Second, to model syntactic coherence in the form of morphological agreement in the target language to improve the generation of morphologically rich languages.", "labels": [], "entities": []}, {"text": "So far, this problem has been addressed in a very limited manner in MT, most typically by using a target language model.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9893866777420044}]}, {"text": "In the framework suggested in this paper, we train a model that predicts the inflected forms of a sequence of word stems in a target sentence, given the corresponding source sentence.", "labels": [], "entities": []}, {"text": "We use word and word alignment information, as well as lexical resources that provide morphological information about the words on both the source and target sides.", "labels": [], "entities": [{"text": "word and word alignment", "start_pos": 7, "end_pos": 30, "type": "TASK", "confidence": 0.6024123430252075}]}, {"text": "Given a sentence pair, we also obtain syntactic analysis information for both the source and translated sentences.", "labels": [], "entities": []}, {"text": "We generate the inflected forms of words in the target sentence using all of the available information, using a log-linear model that learns the relevant mapping functions.", "labels": [], "entities": []}, {"text": "As a case study, we focus on the English-Russian and English-Arabic language pairs.", "labels": [], "entities": []}, {"text": "Unlike English, Russian and Arabic have very rich systems of morphology, each with distinct characteristics.", "labels": [], "entities": []}, {"text": "Translating from a morphology-poor to a morphologyrich language is especially challenging since detailed morphological information needs to be decoded from a language that does not encode this information or does so only implicitly).", "labels": [], "entities": []}, {"text": "We believe that these language pairs are represen-128 tative in this respect and therefore demonstrate the generality of our approach.", "labels": [], "entities": []}, {"text": "There are several contributions of this work.", "labels": [], "entities": []}, {"text": "First, we propose a general approach that shows promise in addressing the challenges of MT into morphologically rich languages.", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.995422899723053}]}, {"text": "We show that the use of both syntactic and morphological information improves translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.9600094556808472}]}, {"text": "We also show the utility of source language information in predicting the word forms of the target language.", "labels": [], "entities": [{"text": "predicting the word forms of the target language", "start_pos": 59, "end_pos": 107, "type": "TASK", "confidence": 0.7514224350452423}]}, {"text": "Finally, we achieve these results with limited morphological resources and training data, suggesting that the approach is generally useful for resource-scarce language pairs.", "labels": [], "entities": []}, {"text": "describes the morphological features relevant to Russian and Arabic, along with their possible values.", "labels": [], "entities": []}, {"text": "The rightmost column in the table refers to the morphological features that are shared by Russian and Arabic, including person, number, gender and tense.", "labels": [], "entities": []}, {"text": "While these features are fairly generic (they are also present in English), note that Russian includes an additional gender (neuter) and Arabic has a distinct number notion for two (dual).", "labels": [], "entities": []}, {"text": "A central dimension of Russian morphology is case marking, realized as suffixation on nouns and nominal modifiers . The Russian case feature includes six possible values, representing the notions of subject, direct object, location, etc.", "labels": [], "entities": [{"text": "case marking", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.7237631231546402}]}, {"text": "In Arabic, like other Semitic languages, word surface forms may include proclitics and enclitics (or prefixes and suffixes as we refer to them in this paper), concatenated to inflected stems.", "labels": [], "entities": []}, {"text": "For nouns, prefixes include conjunctions (wa: \"and\", fa: \"and, so\"), prepositions (bi: \"by, with\", ka: \"like, such as\", li: \"for, to\") and a determiner, and suffixes include possessive pronouns.", "labels": [], "entities": []}, {"text": "Verbal prefixes include conjunction and negation, and suffixes include object pronouns.", "labels": [], "entities": []}, {"text": "Both object and possessive pronouns are captured by an indicator function for its presence or absence, as well as by the features that indicate their person, number and gender.", "labels": [], "entities": []}, {"text": "As can be observed from the table, a large number of surface inflected forms can be generated by the combination of these features, making the morphological generation of these languages a non-trivial task.", "labels": [], "entities": [{"text": "morphological generation", "start_pos": 143, "end_pos": 167, "type": "TASK", "confidence": 0.7567059695720673}]}], "datasetContent": [{"text": "In order to evaluate the effectiveness of the suggested approach, we performed reference experiments, that is, using the aligned sentence pairs of 132  reference translations rather than the output of an MT system as input.", "labels": [], "entities": []}, {"text": "This allows us to evaluate our method with a reduced noise level, as the words and word order are perfect in reference translations.", "labels": [], "entities": []}, {"text": "These experiments thus constitute a preliminary step for tackling the real task of inflecting words in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.9287721514701843}]}, {"text": "In the experiments, our primary goal is to evaluate the effectiveness of the proposed model using all features available to us.", "labels": [], "entities": []}, {"text": "Additionally, we are interested in knowing the contribution of each information source, namely of morpho-syntactic and bilingual features.", "labels": [], "entities": []}, {"text": "Therefore, we study the performanceof models including the full feature schemata as well as models that are restricted to feature subsets according to the feature types as described in Section 5.2.", "labels": [], "entities": []}, {"text": "The models are as follows: Monolingual-Word, including LM-like and stem n-gram features only; Bilingual-Word, which also includes bilingual lexical features; 7 Monolingual-All, which has access to all the information available in the target language, including morphological and syntactic features; and finally, Bilingual-All, which includes all feature types from.", "labels": [], "entities": [{"text": "Bilingual-Word", "start_pos": 94, "end_pos": 108, "type": "METRIC", "confidence": 0.8179665207862854}]}, {"text": "For each model and language, we perform feature selection in the following manner.", "labels": [], "entities": []}, {"text": "The features are represented as feature templates, such as \"POS=X\", which generate a set of binary features corresponding to different instantiations of the template, as in \"POS=NOUN\".", "labels": [], "entities": []}, {"text": "In addition to individual features, conjunctions of up to three features are also considered for selection (e.g., \"POS=NOUN & Number=plural\").", "labels": [], "entities": [{"text": "POS=NOUN", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.7366177638371786}]}, {"text": "Every conjunction of feature templates considered contains at least one predicate on the prediction y t , and up to two predicates on the context.", "labels": [], "entities": []}, {"text": "The feature selection algorithm performs a greedy forward stepwise feature selection on the feature templates so as to maximize development set accuracy.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7644859850406647}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9642748832702637}]}, {"text": "The algorithm is similar to the one described in.", "labels": [], "entities": []}, {"text": "After this process, we performed some manual inspection of the selected templates, and finally obtained 11 and 36 templates for the MonolingualAll and Bilingual-All settings for Russian, respectively.", "labels": [], "entities": []}, {"text": "These templates generated 7.9 million and 9.3 million binary feature instantiations in the final model, respectively.", "labels": [], "entities": []}, {"text": "The corresponding numbers for Arabic were 27 feature templates (0.7 million binary instantiations) and 39 feature templates (2.3 million binary instantiations) for MonolingualAll and Bilingual-All, respectively.", "labels": [], "entities": []}, {"text": "shows the accuracy of predicting word forms for the baseline and proposed models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995585083961487}, {"text": "predicting word forms", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.8504235943158468}]}, {"text": "We report accuracy only on words that appear in our lexicons.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992534518241882}]}, {"text": "Thus, punctuation, English words occurring in the target sentence, and words with unknown lemmas are excluded from the evaluation.", "labels": [], "entities": []}, {"text": "The reported accuracy measure therefore abstracts away from the is- Overall, this feature set approximates the information that is available to a state-of-the-art statistical MT system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9870936274528503}, {"text": "MT", "start_pos": 175, "end_pos": 177, "type": "TASK", "confidence": 0.9182168841362}]}], "tableCaptions": [{"text": " Table 3: Data set statistics: corpus size and average  sentence length (in words)", "labels": [], "entities": []}, {"text": " Table 5: Accuracy (%) results by model", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999652624130249}]}]}