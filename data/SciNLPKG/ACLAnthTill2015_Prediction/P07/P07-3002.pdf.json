{"title": [{"text": "Inducing Combinatory Categorial Grammars with Genetic Algorithms", "labels": [], "entities": [{"text": "Inducing Combinatory Categorial Grammars", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.830968052148819}]}], "abstractContent": [{"text": "This paper proposes a novel approach to the induction of Combinatory Categorial Grammars (CCGs) by their potential affinity with the Genetic Algorithms (GAs).", "labels": [], "entities": [{"text": "induction of Combinatory Categorial Grammars (CCGs)", "start_pos": 44, "end_pos": 95, "type": "TASK", "confidence": 0.8348793908953667}]}, {"text": "Specifically, CCGs utilize a rich yet compact notation for lexical categories, which combine with relatively few grammatical rules, presumed universal.", "labels": [], "entities": []}, {"text": "Thus, the search fora CCG consists in large part in a search for the appropriate categories for the data-set's lexical items.", "labels": [], "entities": []}, {"text": "We present and evaluates a system utilizing a simple GA to successively search and improve on such assignments.", "labels": [], "entities": []}, {"text": "The fitness of categorial-assignments is approximated by the coverage of the resulting grammar on the data-set itself, and candidate solutions are updated via the standard GA techniques of reproduction, crossover and mutation.", "labels": [], "entities": []}], "introductionContent": [{"text": "The discovery of grammars from unannotated material is an important problem which has received much recent research.", "labels": [], "entities": [{"text": "discovery of grammars from unannotated material", "start_pos": 4, "end_pos": 51, "type": "TASK", "confidence": 0.8654928803443909}]}, {"text": "We propose a novel approach to this effort by leveraging the theoretical insights of Combinatory Categorial Grammars (CCG)), and their potential affinity with Genetic Algorithms (GA).", "labels": [], "entities": []}, {"text": "Specifically, CCGs utilize an extremely small set of grammatical rules, presumed near-universal, which operate over a rich set of grammatical categories, which are themselves simple and straightforward data structures.", "labels": [], "entities": []}, {"text": "A search fora CCG grammar fora language can be construed as a search for accurate category assignments to the words of that language, albeit over a large landscape of potential solutions.", "labels": [], "entities": []}, {"text": "GAs are biologically-inspired general purpose search/optimization methods that have succeeded in these kinds of environments: wherein solutions are straightforwardly coded, yet nevertheless the solution space is complex and difficult.", "labels": [], "entities": [{"text": "general purpose search/optimization", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.6251432240009308}]}, {"text": "We evaluate a system that uses a GA to successively refine a population of categorial lexicons given a collection of unannotated training material.", "labels": [], "entities": []}, {"text": "This is an important problem for several reasons.", "labels": [], "entities": []}, {"text": "First of all, the development of annotated training material is expensive and difficult, and so schemes to discover linguistic patterns from unannotated text may help cut down the cost of corpora development.", "labels": [], "entities": [{"text": "corpora development", "start_pos": 188, "end_pos": 207, "type": "TASK", "confidence": 0.7705958485603333}]}, {"text": "Also, this project is closely related to the problem of resolving lexical gaps in parsing, which is a dogged problem for statistical parsing systems in CCG, even trained in a supervised manner.", "labels": [], "entities": [{"text": "parsing", "start_pos": 82, "end_pos": 89, "type": "TASK", "confidence": 0.7527176141738892}, {"text": "statistical parsing", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.6400211751461029}]}, {"text": "Carrying over techniques from this project to that could help solve a major problem in CCG parsing technology.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.8075404763221741}]}, {"text": "Statistical parsing with CCGs is an active area of research.", "labels": [], "entities": [{"text": "Statistical parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9436306059360504}]}, {"text": "The development of CCGbank) based on the Penn Treebank has allowed for the development of widecoverage statistical parsers.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.9954886138439178}, {"text": "widecoverage statistical parsers", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.529531459013621}]}, {"text": "In particular, Hockenmaier and Steedman (2001) report a generative model for CCG parsing roughly akin to the Collins parser) specific to CCG.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.743237167596817}]}, {"text": "Whereas Hockenmaier's parser is trained on (normal-form) CCG derivations, present a CCG parser trained on the dependency structures within parsed sentences, as well as the possible derivations for them, using a log-linear (MaximumEntropy) model.", "labels": [], "entities": []}, {"text": "This is one of the most accurate parsers for producing deep dependencies currently available.", "labels": [], "entities": []}, {"text": "Both systems, however, suffer from gaps in lexical coverage.", "labels": [], "entities": []}, {"text": "The system proposed here was evaluated against a small corpus of unannotated English with the goal of inducing a categorial lexicon for the fragment.", "labels": [], "entities": []}, {"text": "The system is not ultimately successful and fails to achieve the baseline category assignment accuracy, however it does suggest directions for improvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9312777519226074}]}], "datasetContent": [{"text": "The system was evaluated on a small data-set of examples taken from the World Cup test-bed included with the OpenCCG grammar development system 1 and simplified considerably.", "labels": [], "entities": [{"text": "World Cup test-bed", "start_pos": 72, "end_pos": 90, "type": "DATASET", "confidence": 0.9505653778711954}, {"text": "OpenCCG grammar development system 1", "start_pos": 109, "end_pos": 145, "type": "DATASET", "confidence": 0.8907488346099853}]}, {"text": "This included 19 example sentences with a total of 105 word-types and 613 tokens from.", "labels": [], "entities": []}, {"text": "In spite of the simplifying assumption that an individual candidate only assigns a single category to a lexical item, one can derive a multi-assignment of categories to lexemes from the population by choosing the top category elected by the candidates.", "labels": [], "entities": []}, {"text": "It is on the basis of these derived assignments that the system was evaluated.", "labels": [], "entities": []}, {"text": "The examples chosen require only 1-to-1 category assignment, hence the relevant category from the test-bed constitutes the gold standard (minus Baldridge (2002)'s modalities).", "labels": [], "entities": []}, {"text": "The baseline for this dataset, assigning np to all lexical items, was 28.6%.", "labels": [], "entities": []}, {"text": "The hypothesis is that optimizing Fitness Metric Accuracy COUNT 18.5 RELATIVE 22.0 WEIGHTED 20.4: Final accuracy of the metrics parsing coverage with a GA scheme would correlate with improved category-accuracy.", "labels": [], "entities": [{"text": "Accuracy COUNT 18.5 RELATIVE 22.0", "start_pos": 49, "end_pos": 82, "type": "METRIC", "confidence": 0.7474734663963318}, {"text": "WEIGHTED", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.5264612436294556}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9675772190093994}]}, {"text": "The end-conditions apply if the parsing coverage for the derived grammar exceeds 90%.", "labels": [], "entities": []}, {"text": "Such endconditions generally were not met; otherwise, experiments ran for 100 generations, with a population of 50 candidates.", "labels": [], "entities": []}, {"text": "Because of the heavy reliance of GAs on pseudo-random number generation, individual experiments can show idiosyncratic successor failure.", "labels": [], "entities": []}, {"text": "To control for this, the experiments were replicated 100 times each.", "labels": [], "entities": []}, {"text": "The results presented here are averages over the runs.", "labels": [], "entities": []}], "tableCaptions": []}