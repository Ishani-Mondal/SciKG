{"title": [{"text": "Bilingual-LSA Based LM Adaptation for Spoken Language Translation", "labels": [], "entities": [{"text": "Spoken Language Translation", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.7953844467798868}]}], "abstractContent": [{"text": "We propose a novel approach to crosslingual language model (LM) adaptation based on bilingual Latent Semantic Analysis (bLSA).", "labels": [], "entities": [{"text": "crosslingual language model (LM) adaptation", "start_pos": 31, "end_pos": 74, "type": "TASK", "confidence": 0.7477160479341235}]}, {"text": "A bLSA model is introduced which enables latent topic distributions to be efficiently transferred across languages by enforcing a one-to-one topic correspondence during training.", "labels": [], "entities": []}, {"text": "Using the proposed bLSA framework crosslingual LM adaptation can be performed by, first, inferring the topic posterior distribution of the source text and then applying the inferred distribution to the target language N-gram LM via marginal adaptation.", "labels": [], "entities": [{"text": "crosslingual LM adaptation", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.5892227590084076}]}, {"text": "The proposed framework also enables rapid bootstrapping of LSA models for new languages based on a source LSA model from another language.", "labels": [], "entities": []}, {"text": "On Chinese to English speech and text translation the proposed bLSA framework successfully reduced word perplexity of the English LM by over 27% fora unigram LM and up to 13.6% fora 4-gram LM.", "labels": [], "entities": [{"text": "Chinese to English speech and text translation", "start_pos": 3, "end_pos": 49, "type": "TASK", "confidence": 0.5762313817228589}]}, {"text": "Furthermore, the proposed approach consistently improved machine translation quality on both speech and text based adaptation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7827005982398987}, {"text": "text based adaptation", "start_pos": 104, "end_pos": 125, "type": "TASK", "confidence": 0.6753580967585245}]}], "introductionContent": [{"text": "Language model adaptation is crucial to numerous speech and translation tasks as it enables higherlevel contextual information to be effectively incorporated into a background LM improving recognition or translation performance.", "labels": [], "entities": [{"text": "Language model adaptation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.611991673707962}]}, {"text": "One approach is to employ Latent Semantic Analysis (LSA) to capture in-domain word unigram distributions which are then integrated into the background N-gram LM.", "labels": [], "entities": []}, {"text": "This approach has been successfully applied in automatic speech recognition (ASR)) using the Latent Dirichlet Allocation (LDA) ().", "labels": [], "entities": [{"text": "automatic speech recognition (ASR))", "start_pos": 47, "end_pos": 82, "type": "TASK", "confidence": 0.8036133646965027}, {"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 93, "end_pos": 126, "type": "METRIC", "confidence": 0.9508903920650482}]}, {"text": "The LDA model can be viewed as a Bayesian topic mixture model with the topic mixture weights drawn from a Dirichlet distribution.", "labels": [], "entities": []}, {"text": "For LM adaptation, the topic mixture weights are estimated based on in-domain adaptation text (e.g. ASR hypotheses).", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9674595892429352}]}, {"text": "The adapted mixture weights are then used to interpolate a topicdependent unigram LM, which is finally integrated into the background N-gram LM using marginal adaptation ( In this paper, we propose a framework to perform LM adaptation across languages, enabling the adaptation of a LM from one language based on the adaptation text of another language.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 221, "end_pos": 234, "type": "TASK", "confidence": 0.8918590843677521}]}, {"text": "In statistical machine translation (SMT), one approach is to apply LM adaptation on the target language based on an initial translation of input references).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.8072506040334702}, {"text": "LM adaptation", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.86813023686409}]}, {"text": "This scheme is limited by the coverage of the translation model, and overall by the quality of translation.", "labels": [], "entities": []}, {"text": "Since this approach only allows to apply LM adaptation after translation, available knowledge cannot be applied to extend the coverage.", "labels": [], "entities": [{"text": "LM adaptation after translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.7958614081144333}]}, {"text": "We propose a bilingual LSA model (bLSA) for crosslingual LM adaptation that can be applied before translation.", "labels": [], "entities": [{"text": "crosslingual LM adaptation", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.6748789648214976}]}, {"text": "The bLSA model consists of two LSA models: one for each side of the language trained on parallel document corpora.", "labels": [], "entities": []}, {"text": "The key property of the bLSA model is that 520 the latent topic of the source and target LSA models can be assumed to be a one-to-one correspondence and thus share a common latent topic space since the training corpora consist of bilingual parallel data.", "labels": [], "entities": []}, {"text": "For instance, say topic 10 of the Chinese LSA model is about politics.", "labels": [], "entities": [{"text": "Chinese LSA model", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.8129321734110514}]}, {"text": "Then topic 10 of the English LSA model is set to also correspond to politics and so forth.", "labels": [], "entities": [{"text": "English LSA model", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.8883222540219625}]}, {"text": "During LM adaptation, we first infer the topic mixture weights from the source text using the source LSA model.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 7, "end_pos": 20, "type": "TASK", "confidence": 0.9778628647327423}]}, {"text": "Then we transfer the inferred mixture weights to the target LSA model and thus obtain the target LSA marginals.", "labels": [], "entities": []}, {"text": "The challenge is to enforce the one-to-one topic correspondence.", "labels": [], "entities": []}, {"text": "Our proposal is to share common variational Dirichlet posteriors over the topic mixture weights of a document pair in the LDA-style model.", "labels": [], "entities": [{"text": "variational Dirichlet posteriors", "start_pos": 32, "end_pos": 64, "type": "METRIC", "confidence": 0.8185370763142904}]}, {"text": "The beauty of the bLSA framework is that the model searches fora common latent topic space in an unsupervised fashion, rather than to require manual interaction.", "labels": [], "entities": []}, {"text": "Since the topic space is language independent, our approach supports topic transfer in multiple language pairs in O(N) where N is the number of languages.", "labels": [], "entities": [{"text": "topic transfer", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.8195203840732574}]}, {"text": "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.7847836911678314}]}, {"text": "Basically, the BiTAM model consists of topic-dependent translation lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", "labels": [], "entities": []}, {"text": "On the other hand, the bLSA framework models P r(c|k) and P r(e|k) which is different from the BiTAM model.", "labels": [], "entities": []}, {"text": "By their different modeling nature, the bLSA model usually supports more topics than the BiTAM model.", "labels": [], "entities": []}, {"text": "Another work by) employed crosslingual LSA using singular value decomposition which concatenates bilingual documents into a single input supervector before projection.", "labels": [], "entities": []}, {"text": "We organize the paper as follows: In Section 2, we introduce the bLSA framework including Latent Dirichlet-Tree Allocation (LDTA) as a correlated LSA model, bLSA training and crosslingual LM adaptation.", "labels": [], "entities": [{"text": "Latent Dirichlet-Tree Allocation (LDTA)", "start_pos": 90, "end_pos": 129, "type": "METRIC", "confidence": 0.86308487256368}, {"text": "LM adaptation", "start_pos": 188, "end_pos": 201, "type": "TASK", "confidence": 0.7943578064441681}]}, {"text": "In Section 3, we present the effect of LM adaptation on word perplexity, followed by SMT experiments reported in BLEU on both speech and text input in Section 3.3.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.8963499665260315}, {"text": "word perplexity", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7088625282049179}, {"text": "SMT", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9473941922187805}, {"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9620205163955688}]}, {"text": "Section 4 describes conclusions and fu-", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our bLSA model using the Chinese- With an increasing interest in the ASR-SMT coupling for spoken language translation, we also evaluated our approach with Chinese ASR hypotheses and compared with Chinese manual transcriptions.", "labels": [], "entities": [{"text": "spoken language translation", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.755152960618337}]}, {"text": "We are interested to seethe impact due to recognition errors on the ASR hypotheses compared to the manual transcriptions.", "labels": [], "entities": [{"text": "ASR", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9026291966438293}]}, {"text": "We employed the CMUInterACT ASR system developed for the GALE 2006 evaluation.", "labels": [], "entities": [{"text": "CMUInterACT ASR", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.6937143504619598}, {"text": "GALE 2006 evaluation", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.8509506781895956}]}, {"text": "We trained acoustic models with over 500 hours of quickly transcribed speech data released by the GALE program and the LM with over 800M-word Chinese corpora.", "labels": [], "entities": [{"text": "GALE program", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.8679699599742889}]}, {"text": "The character error rates on the CCTV, RFA and NTDTV shows in the RT04 test set are 7.4%, 25.5% and 13.1% respectively.", "labels": [], "entities": [{"text": "error", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.5049902200698853}, {"text": "CCTV", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.9131709933280945}, {"text": "RFA", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.6238059997558594}, {"text": "NTDTV shows", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.9289242327213287}, {"text": "RT04 test set", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9623274008433024}]}], "tableCaptions": [{"text": " Table 1: Parallel topics extracted by the bLSA  model. Top words on the Chinese side are translated  into English for illustration purpose.", "labels": [], "entities": []}, {"text": " Table 2: English word perplexity (PPL) on the RT04  test set using a unigram LM.", "labels": [], "entities": [{"text": "English word perplexity (PPL)", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.5821486214796702}, {"text": "RT04  test set", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9793665210405985}]}, {"text": " Table 3: English word perplexity (PPL) on the RT04  test set using a 4-gram LM.", "labels": [], "entities": [{"text": "English word perplexity (PPL)", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.574944814046224}, {"text": "RT04  test set", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9813332358996073}]}, {"text": " Table 4: Translation performance of baseline and bLSA-Adapted Chinese-English SMT systems on manual  transcriptions and 1-best ASR hypotheses", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9744002819061279}, {"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.6377768516540527}]}]}