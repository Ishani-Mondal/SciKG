{"title": [{"text": "Statistical Machine Translation for Query Expansion in Answer Retrieval", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.713076134522756}, {"text": "Answer Retrieval", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.7517856657505035}]}], "abstractContent": [{"text": "We present an approach to query expansion in answer retrieval that uses Statistical Machine Translation (SMT) techniques to bridge the lexical gap between questions and answers.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7984038293361664}, {"text": "answer retrieval", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7731671333312988}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 72, "end_pos": 109, "type": "TASK", "confidence": 0.7926688889662424}]}, {"text": "SMT-based query expansion is done by i) using a full-sentence paraphraser to introduce synonyms in context of the entire query, and ii) by translating query terms into answer terms using a full-sentence SMT model trained on question-answer pairs.", "labels": [], "entities": [{"text": "SMT-based query expansion", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9341512322425842}, {"text": "SMT", "start_pos": 203, "end_pos": 206, "type": "TASK", "confidence": 0.9378021359443665}]}, {"text": "We evaluate these global, context-aware query expansion techniques on tfidf retrieval from 10 million question-answer pairs extracted from FAQ pages.", "labels": [], "entities": [{"text": "context-aware query expansion", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.6144181688626608}, {"text": "FAQ pages", "start_pos": 139, "end_pos": 148, "type": "DATASET", "confidence": 0.9034200310707092}]}, {"text": "Experimental results show that SMT-based expansion improves retrieval performance over local expansion and over retrieval without expansion.", "labels": [], "entities": [{"text": "SMT-based expansion", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.9489060640335083}]}], "introductionContent": [{"text": "One of the fundamental problems in Question Answering (QA) has been recognized to be the \"lexical chasm\" () between question strings and answer strings.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.9079702854156494}]}, {"text": "This problem is manifested in a mismatch between question and answer vocabularies, and is aggravated by the inherent ambiguity of natural language.", "labels": [], "entities": []}, {"text": "Several approaches have been presented that apply natural language processing technology to close this gap.", "labels": [], "entities": []}, {"text": "For example, syntactic information has been deployed to reformulate questions) or to replace questions by syntactically similar ones); lexical ontologies such as Wordnet have been used to find synonyms for question words (), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities ().", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 162, "end_pos": 169, "type": "DATASET", "confidence": 0.9607496857643127}, {"text": "statistical machine translation (SMT)", "start_pos": 229, "end_pos": 266, "type": "TASK", "confidence": 0.7805953770875931}]}, {"text": "Information retrieval (IR) is faced by a similar fundamental problem of \"term mismatch\" between queries and documents.", "labels": [], "entities": [{"text": "Information retrieval (IR)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8795765042304993}]}, {"text": "A standard IR solution, query expansion, attempts to increase the chances of matching words in relevant documents by adding terms with similar statistical properties to those in the original query.", "labels": [], "entities": [{"text": "IR", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.967268705368042}, {"text": "query expansion", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.8775362968444824}]}, {"text": "In this paper we will concentrate on the task of answer retrieval from FAQ pages, i.e., an IR problem where user queries are matched against documents consisting of question-answer pairs found in FAQ pages.", "labels": [], "entities": [{"text": "answer retrieval from FAQ pages", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.7340278387069702}, {"text": "IR", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.9697391390800476}]}, {"text": "Equivalently, this is a QA problem that concentrates on finding answers given FAQ documents that are known to contain the answers.", "labels": [], "entities": [{"text": "FAQ documents", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9073168933391571}]}, {"text": "Our approach to close the lexical gap in this setting attempts to marry QA and IR technology by deploying SMT methods for query expansion in answer retrieval.", "labels": [], "entities": [{"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9795855283737183}, {"text": "query expansion", "start_pos": 122, "end_pos": 137, "type": "TASK", "confidence": 0.7192391902208328}, {"text": "answer retrieval", "start_pos": 141, "end_pos": 157, "type": "TASK", "confidence": 0.8137527704238892}]}, {"text": "We present two approaches to SMT-based query expansion, both of which are implemented in the framework of phrase-based SMT (.", "labels": [], "entities": [{"text": "SMT-based query expansion", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.927757998307546}, {"text": "SMT", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.6300898790359497}]}, {"text": "Our first query expansion model trains an endto-end phrase-based SMT model on 10 million question-answer pairs extracted from FAQ pages.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9391583204269409}, {"text": "FAQ pages", "start_pos": 126, "end_pos": 135, "type": "DATASET", "confidence": 0.8416281640529633}]}, {"text": "The goal of this system is to learn lexical correlations between words and phrases in questions and answers, for example by allowing for multiple unaligned words in automatic word alignment, and disregarding issues such as word order.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 175, "end_pos": 189, "type": "TASK", "confidence": 0.7279072105884552}]}, {"text": "The ability to translate phrases instead of words and the use of a large language model serve as rich context to make precise decisions in the case of ambiguous translations.", "labels": [], "entities": []}, {"text": "Query expansion is performed by adding content words that have not been seen in the original query from the n-best translations of the query.", "labels": [], "entities": [{"text": "Query expansion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9241524338722229}]}, {"text": "Our second query expansion model is based on the use of SMT technology for full-sentence paraphrasing.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.7652390599250793}, {"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9782940149307251}, {"text": "full-sentence paraphrasing", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.7326355278491974}]}, {"text": "A phrase table of paraphrases is extracted from bilingual phrase tables, and paraphrasing quality is improved by additional discriminative training on manually created paraphrases.", "labels": [], "entities": []}, {"text": "This approach utilizes large bilingual phrase tables as information source to extract a table of para-phrases.", "labels": [], "entities": []}, {"text": "Synonyms for query expansion are read off from the n-best paraphrases of full queries instead of from paraphrases of separate words or phrases.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.8420591652393341}]}, {"text": "This allows the model to take advantage of the rich context of a large n-gram language model when adding terms from the n-best paraphrases to the original query.", "labels": [], "entities": []}, {"text": "In our experimental evaluation we deploy a database of question-answer pairs extracted from FAQ pages for both training a question-answer translation model, and fora comparative evaluation of different systems on the task of answer retrieval.", "labels": [], "entities": [{"text": "FAQ pages", "start_pos": 92, "end_pos": 101, "type": "DATASET", "confidence": 0.9449415802955627}, {"text": "question-answer translation", "start_pos": 122, "end_pos": 149, "type": "TASK", "confidence": 0.7245885580778122}, {"text": "answer retrieval", "start_pos": 225, "end_pos": 241, "type": "TASK", "confidence": 0.7901652157306671}]}, {"text": "Retrieval is based on the tfidf framework of Jijkoun and de, and query expansion is done straightforwardly by adding expansion terms to the query fora second retrieval cycle.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7952585220336914}]}, {"text": "We compare our global, context-aware query expansion techniques with Jijkoun and de tfidf model for answer retrieval and a local query expansion technique (.", "labels": [], "entities": [{"text": "context-aware query expansion", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.6237330834070841}, {"text": "answer retrieval", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.8415579795837402}]}, {"text": "Experimental results show a significant improvement of SMTbased query expansion over both baselines.", "labels": [], "entities": [{"text": "SMTbased query expansion", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.8985661665598551}]}], "datasetContent": [{"text": "Our baseline answer retrieval system is modeled after the tfidf retrieval model of Jijkoun and de.", "labels": [], "entities": [{"text": "answer retrieval", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7254919707775116}]}, {"text": "Their model calculates a linear combination of vector similarity scores between the user query and several fields in the question-answer pair.", "labels": [], "entities": []}, {"text": "We used the cosine similarity metric with logarithmically weighted term and document frequency weights in order to reproduce the Lucene 3 model used in Jijkoun and de.", "labels": [], "entities": []}, {"text": "For indexing of fields, we adopted the settings that were reported to be optimal in Jijkoun and de each of the above without stopwords.", "labels": [], "entities": [{"text": "Jijkoun", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.9442169070243835}]}, {"text": "The second field thus takes takes wh-words, which would typically be filtered out, into account.", "labels": [], "entities": []}, {"text": "All other fields are matched without stopwords, with higher weight assigned to document and question than to answer and title fields.", "labels": [], "entities": []}, {"text": "We did not use phrase-matching or stemming in our experiments, similar to Jijkoun and de, who could not find positive effects for these features in their experiments.", "labels": [], "entities": []}, {"text": "Expansion terms are taken from those terms in the n-best translations of the query that have not been seen in the original query string.", "labels": [], "entities": []}, {"text": "For paraphrasing-based query expansion, a 50-best list of paraphrases of the original query was used.", "labels": [], "entities": [{"text": "paraphrasing-based query expansion", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.7380023698012034}]}, {"text": "For the noisier question-answer translation, expansion terms and phrases were extracted from a 10-: Success rate at 10 or 20 results for retrieval of adequate  The local expansion technique used in our experiments follows in taking expansion terms from the top n answers that were retrieved by the baseline tfidf system, and by incorporating cooccurrence information with query terms.", "labels": [], "entities": [{"text": "question-answer translation", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6744191199541092}]}, {"text": "This is done by calculating term frequencies for expansion terms by summing up the tfidf weights of the answers in which they occur, thus giving higher weight to terms that occur in answers that receive a higher similarity score to the original query.", "labels": [], "entities": []}, {"text": "In our experiments, expansion terms are ranked according to this modified tfidf calculation over the top 20 answers retrieved by the baseline retrieval run, and matched a second time with the field weight vector 0.0, 1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3 that prefers answer fields over question fields.", "labels": [], "entities": []}, {"text": "After stopword removal, the average number of expansion terms produced by the local expansion technique was 9.25.", "labels": [], "entities": [{"text": "stopword removal", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.7138459980487823}]}, {"text": "The test queries we used for retrieval are taken from query logs of the MetaCrawler search engine and were provided to us by Valentin Jijkoun.", "labels": [], "entities": []}, {"text": "In order to maximize recall for the comparative evaluation of systems, we selected 60 queries that were well-formed natural language questions without metacharacters and spelling errors.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9984254837036133}]}, {"text": "However, for one third of these well-formed queries none of the five compared systems could retrieve an answer.", "labels": [], "entities": []}, {"text": "Examples are \"how do you make a cornhusk doll\", Evaluation was performed by manual labeling of top 20 answers retrieved for each of 60 queries for each system by two independent judges.", "labels": [], "entities": []}, {"text": "For the sake of consistency, we chose not to use the assessments provided by Jijkoun and de Rijke.", "labels": [], "entities": [{"text": "consistency", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9641903042793274}]}, {"text": "Instead, the judges were asked to find agreement on the examples on which they disagreed after each evaluation round.", "labels": [], "entities": []}, {"text": "The ratings together with the question-answer pair id were stored and merged into the retrieval results for the next system evaluation.", "labels": [], "entities": []}, {"text": "In this way consistency across system evaluations could be ensured, and the effort of manual labeling could be substantially reduced.", "labels": [], "entities": []}, {"text": "The quality of retrieval results was assessed according to Jijkoun and de three point scale: \u2022 adequate (2): answer is contained \u2022 material (1): no exact answer, but important information given \u2022 unsatisfactory (0): user's information need is not addressed The evaluation measure used in Jijkoun and de is the success rate at 10 or 20 answers, i.e., S 2 @n is the percentage of queries with at least one adequate answer in the top n retrieved question-answer pairs, and S 1,2 @n is the percentage of queries with at least one adequate or material answer in the top n results.", "labels": [], "entities": []}, {"text": "This evaluation measure accounts for improvements in coverage, i.e., it rewards cases where answers are found for queries that did not have an adequate or material answer before.", "labels": [], "entities": [{"text": "coverage", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.988878071308136}]}, {"text": "In contrast, the mean reciprocal rank (MRR) measure standardly used in QA can have the effect of preferring systems that find answers only fora small set of queries, but rank them higher than systems with 469 what is dna hybridization local expansion (-): instructions individual blueprint characteristics chromosomes deoxyribonucleic information biological genetic molecule qa-translation (+): slides clone cdna sitting sequences paraphrasing (+): hibridization hybrids hybridation anything hibridacion hybridising adn hybridisation nothing (4) query: how to enhance competitiveness of indian industries local expansion (+): resources production quality processing established investment development facilities institutional qa-translation (+): increase industry paraphrasing (+): promote raise improve increase industry strengthen (5) query: how to induce labour local expansion (-): experience induction practice imagination concentration information consciousness different meditation relaxation qa-translation (-): birth industrial induced induces paraphrasing (-): way workers inducing employment ways labor working child work job action unions: Examples for queries and expansion terms yielding improved (+), decreased (-), or unchanged (0) retrieval performance compared to retrieval without expansion.", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 17, "end_pos": 43, "type": "METRIC", "confidence": 0.6626321226358414}]}, {"text": "This makes MRR less adequate for the low-recall setup of FAQ retrieval.", "labels": [], "entities": [{"text": "MRR", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.5671527981758118}, {"text": "FAQ retrieval", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.811626136302948}]}, {"text": "shows success rates at 10 and 20 retrieved question-answer pairs for five different systems.", "labels": [], "entities": []}, {"text": "The results for the baseline tfidf system, following Jijkoun and de, are shown in row 2.", "labels": [], "entities": []}, {"text": "Row 3 presents results for our variant of local expansion by pseudo-relevance feedback (.", "labels": [], "entities": []}, {"text": "Results for SMT-based expansion are given in row 4.", "labels": [], "entities": [{"text": "SMT-based expansion", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.9862166047096252}]}, {"text": "A comparison of success rates for retrieving at least one adequate answer in the top 10 results shows relative improvements over the baseline of 11.1% for local query expansion, and of 40.7% for combined SMT-based expansion.", "labels": [], "entities": [{"text": "local query expansion", "start_pos": 155, "end_pos": 176, "type": "TASK", "confidence": 0.6437815129756927}, {"text": "SMT-based expansion", "start_pos": 204, "end_pos": 223, "type": "TASK", "confidence": 0.9876241385936737}]}, {"text": "Success rates at top 20 results show similar relative improvements of 14.2% for local query expansion, and of 22.8% for combined SMT-based expansion.", "labels": [], "entities": [{"text": "local query expansion", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.6162406901518503}, {"text": "SMT-based expansion", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.9807532727718353}]}, {"text": "On the easier task of retrieving a material or adequate answer, success rates drop by a small amount for local expansion, and stay unchanged for SMT-based expansion.", "labels": [], "entities": [{"text": "SMT-based expansion", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.9836515188217163}]}, {"text": "These results can be explained by inspecting a few sample query expansions.", "labels": [], "entities": []}, {"text": "Examples (1)-(3) in Table 4 illustrate cases where SMT-based query expansion improves results over baseline performance, but local expansion decreases performance by introducing irrelevant terms.", "labels": [], "entities": [{"text": "SMT-based query expansion", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.9244888027509054}]}, {"text": "In (4) retrieval performance is improved over the baseline for both expansion techniques.", "labels": [], "entities": []}, {"text": "In (5) both local and SMT-based expansion introduce terms that decrease retrieval performance compared to retrieval without expansion.", "labels": [], "entities": [{"text": "SMT-based expansion", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.9555082619190216}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics of QA pair data", "labels": [], "entities": []}]}