{"title": [{"text": "SVM Model Tampering and Anchored Learning: A Case Study in Hebrew NP Chunking", "labels": [], "entities": [{"text": "NP Chunking", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.5066765546798706}]}], "abstractContent": [{"text": "We study the issue of porting a known NLP method to a language with little existing NLP resources, specifically Hebrew SVM-based chunking.", "labels": [], "entities": []}, {"text": "We introduce two SVM-based methods-Model Tampering and Anchored Learning.", "labels": [], "entities": []}, {"text": "These allow fine grained analysis of the learned SVM models, which provides guidance to identify errors in the training corpus , distinguish the role and interaction of lexical features and eventually construct a model with \u223c10% error reduction.", "labels": [], "entities": []}, {"text": "The resulting chunker is shown to be robust in the presence of noise in the training corpus, relies on less lexical features than was previously understood and achieves an F-measure performance of 92.2 on automatically PoS-tagged text.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9982221722602844}]}, {"text": "The SVM analysis methods also provide general insight on SVM-based chunking.", "labels": [], "entities": [{"text": "SVM analysis", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.8023741245269775}, {"text": "SVM-based chunking", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7233473360538483}]}], "introductionContent": [{"text": "While high-quality NLP corpora and tools are available in English, such resources are difficult to obtain inmost other languages.", "labels": [], "entities": []}, {"text": "Three challenges must be met when adapting results established in English to another language: (1) acquiring high quality annotated data; (2) adapting the English task definition to the nature of a different language, and (3) adapting the algorithm to the new language.", "labels": [], "entities": []}, {"text": "This paper presents a case study in the adaptation of a well known task to a language with few NLP resources available.", "labels": [], "entities": []}, {"text": "Specifically, we deal with SVM based Hebrew NP chunking.", "labels": [], "entities": [{"text": "SVM based Hebrew NP chunking", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.587817931175232}]}, {"text": "In (), we established that the task is not trivially transferable to Hebrew, but reported that SVM based chunking) performs well.", "labels": [], "entities": [{"text": "SVM based chunking", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.6567821403344473}]}, {"text": "We extend that work and study the problem from 3 angles: (1) how to deal with a corpus that is smaller and with a higher level of noise than is available in English; we propose techniques that help identify 'suspicious' data points in the corpus, and identify how robust the model is in the presence of noise; (2) we compare the task definition in English and in Hebrew through quantitative evaluation of the differences between the two languages by analyzing the relative importance of features in the learned SVM models; and (3) we analyze the structure of learned SVM models to better understand the characteristics of the chunking problem in Hebrew.", "labels": [], "entities": []}, {"text": "While most work on chunking with machine learning techniques tend to treat the classification engine as a black-box, we try to investigate the resulting classification model in order to understand its inner working, strengths and weaknesses.", "labels": [], "entities": []}, {"text": "We introduce two SVM-based methods -Model Tampering and Anchored Learning -and demonstrate how a fine-grained analysis of SVM models provides insights on all three accounts.", "labels": [], "entities": []}, {"text": "The understanding of the relative contribution of each feature in the model helps us construct a better model, which achieves \u223c10% error reduction in Hebrew chunking, as well as identify corpus errors.", "labels": [], "entities": [{"text": "Hebrew chunking", "start_pos": 150, "end_pos": 165, "type": "TASK", "confidence": 0.630680724978447}]}, {"text": "The methods also provide general insight on SVM-based chunking.", "labels": [], "entities": [{"text": "SVM-based chunking", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.8315470218658447}]}], "datasetContent": [{"text": "We conducted experiments both for English and Hebrew chunking.", "labels": [], "entities": [{"text": "Hebrew chunking", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.647327795624733}]}, {"text": "For the Hebrew experiments, we use the corpora of ().", "labels": [], "entities": []}, {"text": "The first one is derived from the original Treebank by projecting the full syntactic tree, constructed manually, onto a set of NP chunks according to the SimpleNP rules.", "labels": [], "entities": []}, {"text": "We refer to the resulting corpus as HEB Gold since PoS tags are fully reliable.", "labels": [], "entities": [{"text": "HEB Gold", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.8852396309375763}]}, {"text": "The HEB Err version of the corpus is obtained by projecting the chunk boundaries on the sequence of PoS and morphology tags obtained by the automatic PoS tagger of . This corpus includes an error rate of about 8% on PoS tags.", "labels": [], "entities": [{"text": "HEB Err version of the corpus", "start_pos": 4, "end_pos": 33, "type": "DATASET", "confidence": 0.7945598165194193}, {"text": "error rate", "start_pos": 190, "end_pos": 200, "type": "METRIC", "confidence": 0.9695432186126709}]}, {"text": "The first 500 sentences are used for testing, and the rest for training.", "labels": [], "entities": []}, {"text": "The corpus contains 27K NP chunks.", "labels": [], "entities": []}, {"text": "For the English experiments, we use the now-standard training and test sets that were introduced in (Marcus and Ramshaw, 1995) 2 . Training was done using Kudo's YAMCHA toolkit . Both Hebrew and English models were trained using a polynomial kernel of degree 2, with C = 1.", "labels": [], "entities": []}, {"text": "For English, the features used were: w \u22122 . .", "labels": [], "entities": []}, {"text": "w 2 , p \u22122 . .", "labels": [], "entities": []}, {"text": "p 2 , t \u22122 . .", "labels": [], "entities": []}, {"text": "t \u22121 . The same features were used for Hebrew, with the addition of m \u22122 . .", "labels": [], "entities": []}, {"text": "m 2 . These are the same settings as in).", "labels": [], "entities": []}, {"text": "A linear SVM model (M full ) was trained on the training subset of the anchored, punctuationnormalized, HEB Gold corpus, with the same features as in the previous experiments, and a C value of 9,999.", "labels": [], "entities": [{"text": "HEB Gold corpus", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.9827393293380737}]}, {"text": "Corpus locations corresponding to anchors with weights >1 were inspected.", "labels": [], "entities": []}, {"text": "There were about 120 such locations out of 4,500 sentences used in the training set.", "labels": [], "entities": []}, {"text": "Decreasing the threshold t would result in more cases.", "labels": [], "entities": []}, {"text": "We analyzed these locations into 3 categories: corpus errors, cases that challenge the SimpleNP definition, and cases where the chunking decision is genuinely difficult to make in the absence of global syntactic context or world knowledge.", "labels": [], "entities": [{"text": "SimpleNP definition", "start_pos": 87, "end_pos": 106, "type": "DATASET", "confidence": 0.8457252383232117}]}, {"text": "Corpus Errors: The analysis revealed the following corpus errors: we identified 29 hard cases related to conjunction and apposition (is the comma, colon or slash inside an NP or separating two distinct NPs).", "labels": [], "entities": []}, {"text": "14 of these hard cases were indeed mistakes in the corpus.", "labels": [], "entities": []}, {"text": "This was anticipated, as we distinguished appositions and conjunctive commas using heuristics, since the Treebank marking of conjunctions is somewhat inconsistent.", "labels": [], "entities": []}, {"text": "In order to build the Chunk NP corpus, the syntactic trees of the Treebank were processed to derive chunks according to the SimpleNP definition.", "labels": [], "entities": [{"text": "Chunk NP corpus", "start_pos": 22, "end_pos": 37, "type": "DATASET", "confidence": 0.9374805490175883}]}, {"text": "The hard cases analysis identified 18 instances where this transformation results in erroneous chunks.", "labels": [], "entities": []}, {"text": "For example, null elements result in improper chunks, such as chunks containing only adverbs or only adjectives.", "labels": [], "entities": []}, {"text": "We also found 3 invalid sentences, 6 inconsistencies in the tagging of interrogatives with respect to chunk boundaries, as well as 34 other specific mistakes.", "labels": [], "entities": []}, {"text": "Overall, more than half of the locations identified by the anchors were corpus errors.", "labels": [], "entities": []}, {"text": "Looking for cases similar to the errors identified by anchors, we found 99 more locations, 77 of which were errors.", "labels": [], "entities": []}, {"text": "Refining Because 2 prepositions are involved in this NP, '\u202b'\u05e9\u05dc\u202c (of) and '\u202b'\u05dc\u202c (for), the '\u202b'\u05e9\u05dc\u202c part cannot be attached unambiguously to its head ('court').", "labels": [], "entities": []}, {"text": "It is unclear whether the '\u202b'\u05dc\u202c preposition should be given special treatment to allow it to enter simple NPs in certain contexts, or whether the inconsistent handling of the '\u202b'\u05e9\u05dc\u202c that results from the '\u202b'\u05dc\u202c inter-position is preferable.", "labels": [], "entities": []}, {"text": "Complex determiners and quantifiers: In many cases, complex determiners in Hebrew are multiword expressions that include nouns.", "labels": [], "entities": []}, {"text": "The inclusion of such determiners inside the SimpleNPs is not consistent.", "labels": [], "entities": [{"text": "SimpleNPs", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.8802109956741333}]}, {"text": "Genuinely hard cases were also identified.", "labels": [], "entities": [{"text": "Genuinely", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9401240944862366}]}, {"text": "These include prepositions, conjunctions and multiword idioms (most of them are adjectives and prepositions which are made up of nouns and determiners, e.g., as the word unanimously is expressed in Hebrew as the multi-word expression 'one mouth').", "labels": [], "entities": []}, {"text": "Also, some adverbials and adjectives are impossible to distinguish using only local context.", "labels": [], "entities": []}, {"text": "The anchors analysis helped us improve the chunking method on two accounts: (1) it identified corpus errors with high precision; (2) it made us focus on hard cases that challenge the linguistic definition of chunks we have adopted.", "labels": [], "entities": [{"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9900218844413757}]}, {"text": "Following these findings, we intend to refine the Hebrew SimpleNP definition, and create anew version of the Hebrew chunking corpus.", "labels": [], "entities": [{"text": "Hebrew SimpleNP definition", "start_pos": 50, "end_pos": 76, "type": "DATASET", "confidence": 0.7429743607838949}, {"text": "Hebrew chunking corpus", "start_pos": 109, "end_pos": 131, "type": "DATASET", "confidence": 0.5931594967842102}]}, {"text": "The intent of this experiment is to understand the role of the contextual lexical features (w i , i \ud97b\udf59 = 0).", "labels": [], "entities": []}, {"text": "This is done by training 2 additional anchored linear SVM models, M no\u2212cont and M near . These are the same as M full except for the lexical features used during training.", "labels": [], "entities": []}, {"text": "M no\u2212cont uses only w 0 , while M near uses w 0 ,w \u22121 ,w +1 . Anchors are again used to locate the hard examples for each classifier, and the differences are examined.", "labels": [], "entities": [{"text": "Anchors", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9715894460678101}]}, {"text": "The examples that are hard for M near but not for M full are those solved by w \u22122 ,w +2 . Similarly, the examples that are hard for M no\u2212cont but not for M near are those solved by w \u22121 ,w +1 . indicates the number of hard cases identified by the anchor method for each model.", "labels": [], "entities": []}, {"text": "One way to interpret these figures, is that the introduction of features w \u22121,+1 solves 5 times more hard cases than w \u22122,+2 .  Qualitative analysis of the hard cases solved by the contextual lexical features shows that they contribute mostly to the identification of chunk boundaries in cases of conjunction, apposition, attachment of adverbs and adjectives, and some multi-word expressions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of TopN Tampering.", "labels": [], "entities": [{"text": "TopN Tampering", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.9264160990715027}]}, {"text": " Table 2: Results of Hebrew NoPOS Tampering.  Other scores are \u2265 93.3(HEB G ), \u2265 92.2(HEB E ).", "labels": [], "entities": [{"text": "Hebrew NoPOS Tampering", "start_pos": 21, "end_pos": 43, "type": "DATASET", "confidence": 0.7961175839106241}, {"text": "HEB G )", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9756017327308655}, {"text": "HEB E )", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9092518289883932}]}, {"text": " Table 3: Results of Loc Tamperings.", "labels": [], "entities": [{"text": "Loc Tamperings", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.8529133200645447}]}, {"text": " Table 4: Number of hard cases per model type.", "labels": [], "entities": []}]}