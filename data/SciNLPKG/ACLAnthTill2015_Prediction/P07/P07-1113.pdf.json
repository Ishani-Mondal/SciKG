{"title": [{"text": "A Sequencing Model for Situation Entity Classification", "labels": [], "entities": [{"text": "Situation Entity Classification", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.7467775146166483}]}], "abstractContent": [{"text": "Situation entities (SEs) are the events, states, generic statements, and embedded facts and propositions introduced to a discourse by clauses of text.", "labels": [], "entities": [{"text": "Situation entities (SEs) are the events, states, generic statements, and embedded facts and propositions introduced to a discourse by clauses of text", "start_pos": 0, "end_pos": 149, "type": "Description", "confidence": 0.7472214312465103}]}, {"text": "We report on the first data-driven models for labeling clauses according to the type of SE they introduce.", "labels": [], "entities": []}, {"text": "SE classification is important for discourse mode identification and for tracking the temporal progression of a discourse.", "labels": [], "entities": [{"text": "SE classification", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9345350861549377}, {"text": "discourse mode identification", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6445989310741425}]}, {"text": "We show that (a) linguistically-motivated cooccurrence features and grammatical relation information from deep syntactic analysis improve classification accuracy and (b) using a sequenc-ing model provides improvements over assigning labels based on the utterance alone.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.8925877213478088}]}, {"text": "We report on genre effects which support the analysis of discourse modes having characteristic distributions and sequences of SEs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding discourse requires identifying the participants in the discourse, the situations they participate in, and the various relationships between and among both participants and situations.", "labels": [], "entities": []}, {"text": "Coreference resolution, for example, is concerned with understanding the relationships between references to discourse participants.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9156092703342438}]}, {"text": "This paper addresses the problem of identifying and classifying references to situations expressed in written English texts.", "labels": [], "entities": [{"text": "classifying references to situations expressed in written English texts", "start_pos": 52, "end_pos": 123, "type": "TASK", "confidence": 0.8243647747569613}]}, {"text": "Situation entities (SEs) are the events, states, generic statements, and embedded facts and propositions which clauses introduce.", "labels": [], "entities": [{"text": "Situation entities (SEs) are the events, states, generic statements, and embedded facts and propositions which clauses introduce", "start_pos": 0, "end_pos": 128, "type": "Description", "confidence": 0.7039278257976879}]}, {"text": "Consider the text passage below, which introduces an event-type entity in (1), a report-type entity in, and a statetype entity in (3).", "labels": [], "entities": []}, {"text": "(1) Sony Corp. has heavily promoted the Video Walkman since the product's introduction last summer , SE classification is a fundamental component in determining the discourse mode of texts and, along with aspectual classification, for temporal interpretation.", "labels": [], "entities": [{"text": "Video Walkman", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.8879990875720978}, {"text": "SE classification", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7834959030151367}, {"text": "aspectual classification", "start_pos": 205, "end_pos": 229, "type": "TASK", "confidence": 0.7354707419872284}, {"text": "temporal interpretation", "start_pos": 235, "end_pos": 258, "type": "TASK", "confidence": 0.7595961093902588}]}, {"text": "It maybe useful for discourse relation projection and discourse parsing.", "labels": [], "entities": [{"text": "discourse relation projection", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.7502349615097046}, {"text": "discourse parsing", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7506082057952881}]}, {"text": "Though situation entities are well-studied in linguistics, they have received very little computational treatment.", "labels": [], "entities": []}, {"text": "This paper presents the first data-driven models for SE classification.", "labels": [], "entities": [{"text": "SE classification", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.9896220564842224}]}, {"text": "Our two main strategies are (a) the use of linguistically-motivated features and (b) the implementation of SE classification as a sequencing task.", "labels": [], "entities": [{"text": "SE classification", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.9808299541473389}]}, {"text": "Our results also provide empirical support for the very notion of discourse modes, as we see clear genre effects in SE classification.", "labels": [], "entities": [{"text": "SE classification", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.9481944143772125}]}, {"text": "We begin by discussing SEs in more detail.", "labels": [], "entities": [{"text": "SEs", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9015690684318542}]}, {"text": "Section 3 describes our two annotated data sets and provides examples of each SE type.", "labels": [], "entities": []}, {"text": "Section 4 discusses feature sets, and sections 5 and 6 present models, experiments, and results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we give results for testing on Brown data.", "labels": [], "entities": [{"text": "Brown data", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.7654910981655121}]}, {"text": "All results are reported in terms of accuracy, defined as the percentage of correctly-labeled clauses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9994338154792786}]}, {"text": "Standard 10-fold cross-validation on the training data was used to develop models and feature sets.", "labels": [], "entities": []}, {"text": "The optimized models were then tested on the held-out Brown and MUC data.", "labels": [], "entities": [{"text": "Brown and MUC data", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.8085013255476952}]}, {"text": "The baseline was determined by assigning S (state), the most frequent label in both training sets, to each clause.", "labels": [], "entities": []}, {"text": "Baseline accuracy was 38.5% and 36.2% for Brown and MUC, respectively.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9943742752075195}, {"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9424830675125122}, {"text": "Brown", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.9748315811157227}, {"text": "MUC", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.9208475351333618}]}, {"text": "In general, accuracy figures for MUC are much higher than for Brown.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9997584223747253}, {"text": "MUC", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.557413637638092}, {"text": "Brown", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.9644312858581543}]}, {"text": "This is likely due to the fact that the MUC texts are more consistent: they are all newswire texts of a fairly consistent tone and genre.", "labels": [], "entities": [{"text": "MUC texts", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.9156751930713654}]}, {"text": "The Brown texts, in contrast, are from the 'popular lore' section of the corpus and span a wide range of topics and text types.", "labels": [], "entities": [{"text": "Brown texts", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8834135830402374}]}, {"text": "Nonetheless, the patterns between the feature sets and use of sequence prediction hold across both data sets; here, we focus our discussion on the results for the Brown data.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.6729180067777634}]}], "tableCaptions": [{"text": " Table 3: SE classification results with sequencing  on Brown test set. Bold cell indicates accuracy at- tained by model parameters that performed best on  development data.", "labels": [], "entities": [{"text": "SE classification", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7878982722759247}, {"text": "Brown test set", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.9184731841087341}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9995014667510986}]}, {"text": " Table 4: Confusion matrix for Brown held-out test  data, WTLG feature set, lookback n = 2. Numbers  in parentheses indicate how many clauses have the  associated gold standard label.", "labels": [], "entities": [{"text": "Confusion", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9546536803245544}, {"text": "Brown held-out test  data", "start_pos": 31, "end_pos": 56, "type": "DATASET", "confidence": 0.7302711308002472}, {"text": "WTLG feature set", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.5811377366383871}]}]}