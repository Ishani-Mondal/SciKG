{"title": [{"text": "Guiding Statistical Word Alignment Models With Prior Knowledge", "labels": [], "entities": [{"text": "Guiding Statistical Word Alignment", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6916922479867935}]}], "abstractContent": [{"text": "We present a general framework to incorporate prior knowledge such as heuristics or linguistic features in statistical generative word alignment models.", "labels": [], "entities": [{"text": "statistical generative word alignment", "start_pos": 107, "end_pos": 144, "type": "TASK", "confidence": 0.7040043324232101}]}, {"text": "Prior knowledge plays a role of probabilistic soft constraints between bilingual word pairs that shall be used to guide word alignment model training.", "labels": [], "entities": [{"text": "word alignment model training", "start_pos": 120, "end_pos": 149, "type": "TASK", "confidence": 0.8572714924812317}]}, {"text": "We investigate knowledge that can be derived automatically from entropy principle and bilingual latent semantic analysis and show how they can be applied to improve translation performance.", "labels": [], "entities": [{"text": "bilingual latent semantic analysis", "start_pos": 86, "end_pos": 120, "type": "TASK", "confidence": 0.6975118219852448}]}], "introductionContent": [{"text": "Statistical word alignment models learn word associations between parallel sentences from statistics.", "labels": [], "entities": [{"text": "Statistical word alignment", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6052215993404388}]}, {"text": "Most models are trained from corpora in an unsupervised manner whose success is heavily dependent on the quality and quantity of the training data.", "labels": [], "entities": []}, {"text": "It has been shown that human knowledge, in the form of a small amount of manually annotated parallel data to be used to seed or guide model training, can significantly improve word alignment F-measure and translation performance).", "labels": [], "entities": [{"text": "word alignment F-measure", "start_pos": 176, "end_pos": 200, "type": "TASK", "confidence": 0.7689624428749084}, {"text": "translation", "start_pos": 205, "end_pos": 216, "type": "TASK", "confidence": 0.9644387364387512}]}, {"text": "As formulated in the competitive linking algorithm), the problem of word alignment can be regarded as a process of word linkage disambiguation, that is, choosing correct associations among all competing hypothesis.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.7762685418128967}, {"text": "word linkage disambiguation", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.6471408804257711}]}, {"text": "The more reasonable constraints are imposed on this process, the easier the task would become.", "labels": [], "entities": []}, {"text": "For instance, the most relaxed IBM Model-1, which assumes that any source word can be generated by any target word equally regardless of distance, can be improved by demanding a Markov process of alignments as in HMM-based models (, or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models.", "labels": [], "entities": []}, {"text": "Following the path, we shall put more constraints on word alignment models and investigate ways of implementing them in a statistical framework.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.7760429084300995}]}, {"text": "We have seen examples showing that names tend to align to names and function words are likely to be linked to function words.", "labels": [], "entities": []}, {"text": "These observations are independent of language and can be understood by commonsense.", "labels": [], "entities": []}, {"text": "Moreover, there are other linguistically motivated constraints.", "labels": [], "entities": []}, {"text": "For instance, words aligned to each other presumably are semantically consistent; and likely to be, they are syntactically agreeable.", "labels": [], "entities": []}, {"text": "In these paper, we shall exploit some of these constraints in building better word alignments in the application of statistical machine translation.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.7062601000070572}, {"text": "statistical machine translation", "start_pos": 116, "end_pos": 147, "type": "TASK", "confidence": 0.7053477565447489}]}, {"text": "We propose a simple framework that can integrate prior knowledge into statistical word alignment model training.", "labels": [], "entities": [{"text": "statistical word alignment model training", "start_pos": 70, "end_pos": 111, "type": "TASK", "confidence": 0.7924266040325165}]}, {"text": "In the framework, prior knowledge serves as probabilistic soft constraints that will guide word alignment model training.", "labels": [], "entities": [{"text": "word alignment model training", "start_pos": 91, "end_pos": 120, "type": "TASK", "confidence": 0.8846057206392288}]}, {"text": "We present two types of constraints that are derived in an unsupervised way: one is based on the entropy principle, the other comes from bilingual latent semantic analysis.", "labels": [], "entities": []}, {"text": "We investigate their impact on word alignments and show their effectiveness in improving translation performance.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.7654077410697937}]}], "datasetContent": [{"text": "We test our framework on the task of large vocabulary translation from dialectical (Iraqi) Arabic utterances into English.", "labels": [], "entities": [{"text": "large vocabulary translation from dialectical (Iraqi) Arabic utterances", "start_pos": 37, "end_pos": 108, "type": "TASK", "confidence": 0.8283493638038635}]}, {"text": "The task covers multiple domains including travel, emergency medical diagnosis, defense-oriented force protection, security and etc.", "labels": [], "entities": [{"text": "emergency medical diagnosis", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.7171101371447245}, {"text": "defense-oriented force protection", "start_pos": 80, "end_pos": 113, "type": "TASK", "confidence": 0.6211093763510386}]}, {"text": "To avoid impacts of speech recognition errors, we only report experiments from text to text translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7033185660839081}, {"text": "text to text translation", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.7381168156862259}]}, {"text": "The training corpus consists of 390K sentence pairs, with total 2.43M Arabic words and 3.38M English words.", "labels": [], "entities": []}, {"text": "These sentences are in typical spoken transcription form, i.e., spelling errors, disfluencies, such as word or phrase repetition, and ungrammatical utterances are commonly observed.", "labels": [], "entities": [{"text": "word or phrase repetition", "start_pos": 103, "end_pos": 128, "type": "TASK", "confidence": 0.6172359138727188}]}, {"text": "Arabic utterance length ranges from 3 to 70 words with the average of 6 words.", "labels": [], "entities": []}, {"text": "There are 25K entries in the English vocabulary and 90K in Arabic side.", "labels": [], "entities": []}, {"text": "Data sparseness severely challenges word alignment model and consequently automatic phrase translation induction.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.823776513338089}, {"text": "phrase translation induction", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.837265153725942}]}, {"text": "There are 42K singletons in Arabic vocabulary, and 14K Arabic words with occurrence of twice each in the corpus.", "labels": [], "entities": []}, {"text": "Since Arabic is a morphologically rich language where affixes are attached to stem words to indicate gender, tense, case and etc, in order to reduce vocabulary size and address out-of-vocabulary words, we split Arabic words into affix and root according to a rule-based segmentation scheme () with the help from the Buckwalter analyzer) output.", "labels": [], "entities": []}, {"text": "This reduces the size of Arabic vocabulary to 52K.", "labels": [], "entities": []}, {"text": "Our test data consists of 1294 sentence pairs.", "labels": [], "entities": []}, {"text": "They are split into two parts: half of them is used as the development set, on which training parameters and decoding feature weights are tuned, the other half is for test.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Translation Results with different word  alignments.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9616053104400635}]}, {"text": " Table 2: Word pair constraint values", "labels": [], "entities": []}]}