{"title": [{"text": "Generalizing Semantic Role Annotations Across Syntactically Similar Verbs", "labels": [], "entities": [{"text": "Generalizing Semantic Role Annotations", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7540288418531418}]}], "abstractContent": [{"text": "Large corpora of parsed sentences with semantic role labels (e.g. PropBank) provide training data for use in the creation of high-performance automatic semantic role labeling systems.", "labels": [], "entities": []}, {"text": "Despite the size of these corpora, individual verbs (or role-sets) often have only a handful of instances in these corpora, and only a fraction of English verbs have even a single annotation.", "labels": [], "entities": []}, {"text": "In this paper, we describe an approach for dealing with this sparse data problem, enabling accurate semantic role labeling for novel verbs (rolesets) with only a single training example.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.6241746246814728}]}, {"text": "Our approach involves the identification of syntactically similar verbs found in Prop-Bank, the alignment of arguments in their corresponding rolesets, and the use of their corresponding annotations in Prop-Bank as surrogate training data.", "labels": [], "entities": [{"text": "Prop-Bank", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.8792686462402344}]}], "introductionContent": [], "datasetContent": [{"text": "We conducted a large-scale evaluation to determine the performance of our semantic role labeling algorithm when using variable amounts of surrogate training data, and compared these results to the performance that could be obtained using various amounts of real training data (as described in section 3).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.6282000541687012}]}, {"text": "Our hypothesis was that learning-curves for surrogate-trained labelers would be somewhat less steep, but that the availability of large-amounts of surrogate training data would more than makeup for the gap.", "labels": [], "entities": []}, {"text": "To test this hypothesis, we conducted an evaluation using the PropBank corpus as our testing data as well as our source for surrogate training data.", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.9824775755405426}]}, {"text": "As described in section 5, our approach requires the availability of at least one fully-annotated sentence fora given roleset.", "labels": [], "entities": []}, {"text": "Only 28.5% of the PropBank annotations assign labels for each of the numbered arguments in their given roleset, and only 2,858 of the 4,250 rolesets used in PropBank annotations (66.5%) have at least one fully-annotated sentence.", "labels": [], "entities": []}, {"text": "Of these, 2,807 rolesets were for verbs that appeared at least once in our analysis of the GigaWord corpus (Section 4).", "labels": [], "entities": [{"text": "GigaWord corpus", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.9425039887428284}]}, {"text": "Accordingly, we evaluated our approach using the annotations for this set of 2,807 rolesets as test data.", "labels": [], "entities": []}, {"text": "For each of these rolesets, various amounts of surrogate training data were gathered from all 4,250 rolesets represented in PropBank, leaving out the data for whichever roleset was being tested.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 124, "end_pos": 132, "type": "DATASET", "confidence": 0.9529549479484558}]}, {"text": "For each of the target 2,807 rolesets, we generated a list of semantic role mappings ordered by syntactic similarity, using the methods described in sections 4 and 5.", "labels": [], "entities": []}, {"text": "In aligning arguments, only a single training example from the target roleset was used, namely the first annotation within the PropBank corpus where all of the rolesets arguments were assigned.", "labels": [], "entities": [{"text": "PropBank corpus", "start_pos": 127, "end_pos": 142, "type": "DATASET", "confidence": 0.9422947466373444}]}, {"text": "Our approach failed to identify any argument mappings for 41 of the target rolesets, leaving them without any surrogate training data to utilize.", "labels": [], "entities": []}, {"text": "Of the remaining 2,766 rolesets, the number of mapped rolesets fora given target ranged from 1,041 to 1 (mean = 608, stdev = 297).", "labels": [], "entities": []}, {"text": "For each of the 2,766 target rolesets with alignable roles, we gathered increasingly larger amounts of surrogate training data by descending the ordered list of mappings translating the PropBank data for each entry according to its argument mappings.", "labels": [], "entities": [{"text": "PropBank data", "start_pos": 186, "end_pos": 199, "type": "DATASET", "confidence": 0.9585655629634857}]}, {"text": "Then each of these incrementally larger sets of training data was then used to build a semantic role labeler as described in section 3.", "labels": [], "entities": []}, {"text": "The performance of each of the resulting labelers was then evaluated by applying it to all of the test data available for target roleset in PropBank, using the same scoring methods described in section 3.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 140, "end_pos": 148, "type": "DATASET", "confidence": 0.9693856835365295}]}, {"text": "The performance scores for each labeler were recorded along with the total number of surrogate training examples used to build the labeler.", "labels": [], "entities": []}, {"text": "presents the performance result of our semantic role labeling approach using various amounts of surrogate training data.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6042412320772806}]}, {"text": "Along with precision, recall, and F-score data, also graphs the percentage of PropBank rolesets for which a given amount of training data had been identified using our approach, of the 2,858 rolesets with at least one fully-annotated training example.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9995930790901184}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.998491644859314}, {"text": "F-score", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.999313473701477}]}, {"text": "For instance, with 120 surrogate annotations our system achieves an F-score above 0.5, and we identified this much surrogate training data for 96% of PropBank rolesets with at least one fullyannotated sentence.", "labels": [], "entities": [{"text": "F-score", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.99888676404953}]}, {"text": "This represents 64% of all PropBank rolesets that are used for annotation.", "labels": [], "entities": []}, {"text": "Beyond 120 surrogate training examples, F-scores remain around 0.6 before slowly declining after around 700 examples.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9983589053153992}]}, {"text": "Several interesting comparisons can be made between the results presented in and those in, where actual PropBank training data is used instead of surrogate training data.", "labels": [], "entities": [{"text": "PropBank training data", "start_pos": 104, "end_pos": 126, "type": "DATASET", "confidence": 0.8920846184094747}]}, {"text": "First, the precision obtained with surrogate training data is roughly 10% lower than with real data.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9996992349624634}]}, {"text": "Second, the recall performance of surrogate data performs similar to real data at first, but is consistently 10% lower than with real data after the first 50 training examples.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9991933703422546}]}, {"text": "Accordingly, F-scores for surrogate training data are 10% lower overall.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9982302784919739}]}, {"text": "Even though the performance obtained using surrogate training data is less than with actual data, there is abundant amounts of it available for most PropBank rolesets.", "labels": [], "entities": []}, {"text": "Comparing the \"% of rolesets\" plots in, the real value of surrogate training data is apparent.", "labels": [], "entities": []}, {"text": "suggests that over 20 real training examples are needed to achieve F-scores that are consistently above 0.5, but that less than 20% of PropBank rolesets have this much data available.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9909735321998596}, {"text": "PropBank rolesets", "start_pos": 135, "end_pos": 152, "type": "DATASET", "confidence": 0.924738734960556}]}, {"text": "In contrast, 64% of all PropBank rolesets can achieve this F-score performance with the use of surrogate training data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9963765740394592}]}, {"text": "This percentage increases to 96% if every PropBank roleset is given at least one fully annotated sentence, where all of its numbered arguments are assigned to constituents.", "labels": [], "entities": [{"text": "PropBank roleset", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.9374949634075165}]}, {"text": "In addition to supplementing the real training data available for existing PropBank rolesets, these results predict the labeling performance that can be obtained by applying this technique to a novel roleset with one fully-annotated training example, e.g. for the verb yearn.", "labels": [], "entities": []}, {"text": "Using the first 120 surrogate training examples and our simple semantic role labeling approach, we would expect F-scores that are above 0.5, and that using the first 700 would yield F-scores around 0.6.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.6056714554627737}, {"text": "F-scores", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9882643222808838}, {"text": "F-scores", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9781166315078735}]}], "tableCaptions": []}