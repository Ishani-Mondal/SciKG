{"title": [{"text": "A Discriminative Language Model with Pseudo-Negative Samples", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose a novel discrim-inative language model, which can be applied quite generally.", "labels": [], "entities": []}, {"text": "Compared to the well known N-gram language models, dis-criminative language models can achieve more accurate discrimination because they can employ overlapping features and non-local information.", "labels": [], "entities": []}, {"text": "However, discriminative language models have been used only for re-ranking in specific applications because negative examples are not available.", "labels": [], "entities": []}, {"text": "We propose sampling pseudo-negative examples taken from probabilistic language models.", "labels": [], "entities": []}, {"text": "However, this approach requires prohibitive computational cost if we are dealing with quite a few features and training samples.", "labels": [], "entities": []}, {"text": "We tackle the problem by estimating the latent information in sentences using a semi-Markov class model, and then extracting features from them.", "labels": [], "entities": []}, {"text": "We also use an on-line margin-based algorithm with efficient kernel computation.", "labels": [], "entities": []}, {"text": "Experimental results show that pseudo-negative examples can be treated as real negative examples and our model can classify these sentences correctly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models (LMs) are fundamental tools for many applications, such as speech recognition, machine translation and spelling correction.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.774910032749176}, {"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.8111258149147034}, {"text": "spelling correction", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.9395739436149597}]}, {"text": "The goal of LMs is to determine whether a sentence is corrector incorrect in terms of grammars and pragmatics.", "labels": [], "entities": []}, {"text": "The most widely used LM is a probabilistic language model (PLM), which assigns a probability to a sentence or a word sequence.", "labels": [], "entities": []}, {"text": "In particular, Ngrams with maximum likelihood estimation (NLMs) are often used.", "labels": [], "entities": [{"text": "maximum likelihood estimation (NLMs", "start_pos": 27, "end_pos": 62, "type": "METRIC", "confidence": 0.7092860519886017}]}, {"text": "Although NLMs are simple, they are effective for many applications.", "labels": [], "entities": []}, {"text": "However, NLMs cannot determine correctness of a sentence independently because the probability depends on the length of the sentence and the global frequencies of each word in it.", "labels": [], "entities": []}, {"text": "For example, \u00d4\u00b4\u00cb \u00bd \u00b5 \ud97b\udf59 \u00d4\u00b4\u00cb \u00be \u00b5, where \u00d4\u00b4\u00cb\u00b5 is the probability of a sentence \u00cb given by an NLM, does not always mean that \u00cb \u00be is more correct, but instead could occur when \u00cb \u00be is shorter than \u00cb \u00bd , or if \u00cb \u00be has more common words than \u00cb \u00bd . Another problem is that NLMs cannot handle overlapping information or non-local information easily, which is important for more accurate sentence classification.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 377, "end_pos": 400, "type": "TASK", "confidence": 0.7246744334697723}]}, {"text": "For example, a NLM could assign a high probability to a sentence even if it does not have a verb.", "labels": [], "entities": []}, {"text": "Discriminative language models (DLMs) have been proposed to classify sentences directly as corrector incorrect (, and these models can handle both non-local and overlapping information.", "labels": [], "entities": []}, {"text": "However DLMs in previous studies have been restricted to specific applications.", "labels": [], "entities": []}, {"text": "Therefore the model cannot be used for other applications.", "labels": [], "entities": []}, {"text": "If we had negative examples available, the models could be trained directly by discriminating between correct and incorrect sentences.", "labels": [], "entities": []}, {"text": "In this paper, we propose a generic DLM, which can be used not only for specific applications, but also more generally, similar to PLMs.", "labels": [], "entities": []}, {"text": "To achievethis goal, we need to solve two problems.", "labels": [], "entities": []}, {"text": "The first is that since we cannot obtain negative examples (incorrect sentences), we need to generate them.", "labels": [], "entities": []}, {"text": "The second is the prohibitive computational cost because the number of features and examples is very large.", "labels": [], "entities": []}, {"text": "In previous studies this problem did not arise because the amount of training data was limited and they did not use a combination of features, and thus the computational cost was negligible.", "labels": [], "entities": []}, {"text": "To solve the first problem, we propose sampling incorrect sentences taken from a PLM and then training a model to discriminate between correct and incorrect sentences.", "labels": [], "entities": []}, {"text": "We call these examples PseudoNegative because they are not actually negative sentences.", "labels": [], "entities": []}, {"text": "We call this method DLM-PN (DLM with Pseudo-Negative samples).", "labels": [], "entities": []}, {"text": "To deal with the second problem, we employ an online margin-based learning algorithm with fast kernel computation.", "labels": [], "entities": []}, {"text": "This enables us to employ combinations of features, which are important for discrimination between correct and incorrect sentences.", "labels": [], "entities": []}, {"text": "We also estimate the latent information in sentences by using a semi-Markov class model to extract features.", "labels": [], "entities": []}, {"text": "Although there are substantially fewer latent features than explicit features such as words or phrases, latent features contain essential information for sentence classification.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 154, "end_pos": 177, "type": "TASK", "confidence": 0.7114894837141037}]}, {"text": "Experimental results show that these pseudonegative samples can be treated as incorrect examples, and that DLM-PN can learn to correctly discriminate between correct and incorrect sentences and can therefore classify these sentences correctly.", "labels": [], "entities": []}, {"text": "The parameters can be estimated using the maximum likelihood method.", "labels": [], "entities": []}], "datasetContent": [{"text": "We partitioned a BNC-corpus into model-train, DLM-train-positive, and DLM-test-positive sets.", "labels": [], "entities": [{"text": "BNC-corpus", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.8471410274505615}]}, {"text": "The numbers of sentences in model-train, DLMtrain-positive and DLM-test-positive were \ud97b\udf59\ud97b\udf59\u00bc\u00bck, \u00be\u00be\u00bck, and \u00bd\u00bck respectively.", "labels": [], "entities": [{"text": "model-train", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.9337161779403687}]}, {"text": "An NLM was built using model-train and Pseudo-Negative examples (\u00be\u00be\u00bck sentences) were sampled from it.", "labels": [], "entities": []}, {"text": "We mixed sentences from DLM-train-positive and the PseudoNegative examples and then shuffled the order of these sentences to make DLM-train.", "labels": [], "entities": []}, {"text": "We also constructed DLM-test by mixing DLM-test-positive and \u00bd\u00bck new (not already used) sentences from the Pseudo-Negative examples.", "labels": [], "entities": []}, {"text": "We call the sentences from DLM-train-positive \"positive\" examples and the sentences from the Pseudo-Negative examples \"negative\" examples in the following.", "labels": [], "entities": []}, {"text": "From these sentences the ones with less than \ud97b\udf59 words were excluded beforehand because it was difficult to decide whether these sentences were corrector not (e.g.  We examined the property of a sentence being Pseudo-Negative, in order to justify our framework.", "labels": [], "entities": []}, {"text": "A native English speaker and two non-native English speaker were asked to assign correct/incorrect labels to \u00bd\u00bc\u00bc sentences in DLM-train 1 . The result for an native English speaker was that all positive sentences were labeled as correct and all negative sentences except for one were labeled as incorrect.", "labels": [], "entities": []}, {"text": "On the other hand, the results for non-native English speakers are 67\u00b1 and 70\u00b1.", "labels": [], "entities": []}, {"text": "From this result, we can say that the sampling method was able to generate incorrect sentences and if a classifier can discriminate them, the classifier can also discriminate between correct and incorrect sentences.", "labels": [], "entities": []}, {"text": "Note that it takes an average of 25 seconds for the native English speaker to assign the label, which suggests that it is difficult even fora human to determine the correctness of a sentence.", "labels": [], "entities": []}, {"text": "We then examined whether it was possible to discriminate between correct and incorrect sentences using parsing methods, since if so, we could have used parsing as a classification tool.", "labels": [], "entities": []}, {"text": "We examined \u00bd\u00bc\u00bc sentences using a phrase structure parser) and an HPSG parser Since the PLM also made use of the BNC-corpus for positive examples, we were notable to classify sentences based on word occurrences).", "labels": [], "entities": [{"text": "HPSG", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.9106611609458923}]}, {"text": "All sentences were parsed correctly except for one positive example.", "labels": [], "entities": []}, {"text": "This result indicates that correct sentences and pseudonegative examples cannot be differentiated syntactically.", "labels": [], "entities": []}, {"text": "We investigated the performance of classifiers and the effect of different sets of features.", "labels": [], "entities": []}, {"text": "For N-grams and Part of Speech (POS), we used tri-gram features.", "labels": [], "entities": []}, {"text": "For SMCM, we used bi-gram features.", "labels": [], "entities": [{"text": "SMCM", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.9733278751373291}]}, {"text": "We used DLM-train as a training set.", "labels": [], "entities": [{"text": "DLM-train", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.9144517779350281}]}, {"text": "In all experiments, we set \ud97b\udf59 \u00bc \ud97b\udf59\u00bc where \ud97b\udf59 is a parameter in the classification (Section 4).", "labels": [], "entities": [{"text": "\ud97b\udf59 \u00bc \ud97b\udf59\u00bc", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.794232984383901}]}, {"text": "In all kernel experiments, a \u00bfrd order polynomial kernel was used and values were computed using PKI (the inverted indexing method).", "labels": [], "entities": []}, {"text": "shows the accuracy results with different features, or in the case of the SMCMs, different numbers of classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994587302207947}]}, {"text": "This result shows that the kernel method is important in achieving high performance.", "labels": [], "entities": []}, {"text": "Note that the classifier with SMCM features performs as well as the one with word.", "labels": [], "entities": []}, {"text": "shows the number of features in each method.", "labels": [], "entities": []}, {"text": "Note that anew feature is added only if the classifier needs to update its parameters.", "labels": [], "entities": []}, {"text": "These numbers are therefore smaller than the possible number of all candidate features.", "labels": [], "entities": []}, {"text": "This result and the previous result indicate that SMCM achieves high performance with very few features.", "labels": [], "entities": [{"text": "SMCM", "start_pos": 50, "end_pos": 54, "type": "TASK", "confidence": 0.9807522296905518}]}, {"text": "We then examined the effect of PKI.    were used for both experiments because training using all the training data would have required a much longer time than was possible with our experimental setup.", "labels": [], "entities": []}, {"text": "shows the margin distribution for positive and negative examples using SMCM bi-gram features.", "labels": [], "entities": [{"text": "SMCM", "start_pos": 71, "end_pos": 75, "type": "TASK", "confidence": 0.8432797789573669}]}, {"text": "Although many examples are close to the borderline (margin \ud97b\udf59 \u00bc), positive and negative examples are distributed on either side of \u00bc.", "labels": [], "entities": [{"text": "margin \ud97b\udf59 \u00bc)", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.9577489296595255}]}, {"text": "Therefore higher recall or precision could be achieved by using a pre-defined margin threshold other than \u00bc.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9993699193000793}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9936866164207458}]}, {"text": "Finally, we generated learning curves to examine the effect of the size of training data on performance.", "labels": [], "entities": []}, {"text": "shows the result of the classification task using SMCM-bi-gram features.", "labels": [], "entities": []}, {"text": "The result suggests that the performance could be further improved by enlarging the training data set.", "labels": [], "entities": [{"text": "training data set", "start_pos": 84, "end_pos": 101, "type": "DATASET", "confidence": 0.7703196903069814}]}, {"text": "The accuracy is the percentage of sentences in the evaluation set classified correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999630331993103}]}], "tableCaptions": [{"text": " Table 1: Performance on the evaluation data.", "labels": [], "entities": []}, {"text": " Table 2: The number of features.", "labels": [], "entities": []}, {"text": " Table 3: Comparison between classification perfor- mance with/without index", "labels": [], "entities": []}]}