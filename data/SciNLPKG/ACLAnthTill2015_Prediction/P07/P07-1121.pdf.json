{"title": [{"text": "Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.6725294142961502}]}], "abstractContent": [{"text": "This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms.", "labels": [], "entities": []}, {"text": "Using statistical machine translation techniques, a semantic parser based on asynchronous context-free grammar augmented with \u03bb-operators is learned given a set of training sentences and their correct logical forms.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 6, "end_pos": 37, "type": "TASK", "confidence": 0.6665796041488647}]}, {"text": "The resulting parser is shown to be the best-performing system so far in a database query domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Originally developed as a theory of compiling programming languages, synchronous grammars have seen a surge of interest recently in the statistical machine translation (SMT) community as away of formalizing syntax-based translation models between natural languages (NL).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 136, "end_pos": 173, "type": "TASK", "confidence": 0.7821851968765259}, {"text": "formalizing syntax-based translation models between natural languages (NL)", "start_pos": 195, "end_pos": 269, "type": "TASK", "confidence": 0.7605458974838257}]}, {"text": "In generating multiple parse trees in a single derivation, synchronous grammars are ideal for modeling syntax-based translation because they describe not only the hierarchical structures of a sentence and its translation, but also the exact correspondence between their sub-parts.", "labels": [], "entities": []}, {"text": "Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars (SCFG) ( and synchronous treesubstitution grammars (STSG)).", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.6788330078125}]}, {"text": "Both formalisms have led to SMT systems whose performance is state-of-the-art).", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9942139983177185}]}, {"text": "Synchronous grammars have also been used in other NLP tasks, most notably semantic parsing, which is the construction of a complete, formal meaning representation (MR) of an NL sentence.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.7148427367210388}]}, {"text": "In our previous work), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7786124050617218}, {"text": "machine translation task", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.8045756022135416}]}, {"text": "Our algorithm, WASP, uses statistical models developed for syntax-based SMT for lexical learning and parse disambiguation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.8711923956871033}, {"text": "parse disambiguation", "start_pos": 101, "end_pos": 121, "type": "TASK", "confidence": 0.88893923163414}]}, {"text": "The result is a robust semantic parser that gives good performance in various domains.", "labels": [], "entities": [{"text": "semantic parser", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7069958597421646}]}, {"text": "More recently, we show that our SCFG-based parser can be inverted to produce a state-of-the-art NL generator, where a formal MRL is translated into an NL.", "labels": [], "entities": []}, {"text": "Currently, the use of learned synchronous grammars in semantic parsing and NL generation is limited to simple MRLs that are free of logical variables.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7233627885580063}, {"text": "NL generation", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.9016174972057343}]}, {"text": "This is because grammar formalisms such as SCFG do not have a principled mechanism for handling logical variables.", "labels": [], "entities": []}, {"text": "This is unfortunate because most existing work on computational semantics is based on predicate logic, where logical variables play an important role).", "labels": [], "entities": []}, {"text": "For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in).", "labels": [], "entities": []}, {"text": "However, development of such a functional language is non-trivial, and as we will see, logical languages can be more appropriate for certain domains.", "labels": [], "entities": []}, {"text": "On the other hand, most existing methods for mapping NL sentences to logical forms involve substantial hand-written components that are difficult to maintain.", "labels": [], "entities": [{"text": "mapping NL sentences to logical forms", "start_pos": 45, "end_pos": 82, "type": "TASK", "confidence": 0.8155392011006674}]}, {"text": "present a statistical method that is considerably more robust, but it still relies on hand-written rules for lexical acquisition, which can create a performance bottleneck.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.7367508113384247}]}, {"text": "In this work, we show that methods developed for SMT can be brought to bear on tasks where logical forms are involved, such as semantic parsing.", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9957188963890076}, {"text": "semantic parsing", "start_pos": 127, "end_pos": 143, "type": "TASK", "confidence": 0.7101991772651672}]}, {"text": "In particular, we extend the WASP semantic parsing algorithm by adding variable-binding \u03bb-operators to the underlying SCFG.", "labels": [], "entities": [{"text": "WASP semantic parsing", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.9113445281982422}]}, {"text": "The resulting synchronous grammar generates logical forms using \u03bb-calculus.", "labels": [], "entities": []}, {"text": "A semantic parser is learned given a set of sentences and their correct logical forms using SMT methods.", "labels": [], "entities": [{"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9661911725997925}]}, {"text": "The new algorithm is called \u03bb-WASP, and is shown to be the best-performing system so far in the GEOQUERY domain.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.8785408735275269}]}], "datasetContent": [{"text": "We evaluated the \u03bb-WASP algorithm in the GEO-QUERY domain.", "labels": [], "entities": [{"text": "GEO-QUERY domain", "start_pos": 41, "end_pos": 57, "type": "DATASET", "confidence": 0.9373796880245209}]}, {"text": "The larger GEOQUERY corpus consists of 880 English questions gathered from various sources ().", "labels": [], "entities": [{"text": "GEOQUERY corpus", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.9584085941314697}]}, {"text": "The questions were manually translated into Prolog logical forms.", "labels": [], "entities": [{"text": "Prolog", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.9221501350402832}]}, {"text": "The average length of a sentence is 7.57 words.", "labels": [], "entities": []}, {"text": "We performed a single run of 10-fold cross validation, and measured the performance of the learned parsers using precision (percentage of translations that were correct), recall (percentage of test sentences that were correctly translated), and Fmeasure (harmonic mean of precision and recall).", "labels": [], "entities": [{"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9991335272789001}, {"text": "recall", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9993826150894165}, {"text": "Fmeasure", "start_pos": 245, "end_pos": 253, "type": "METRIC", "confidence": 0.9992989301681519}, {"text": "precision", "start_pos": 272, "end_pos": 281, "type": "METRIC", "confidence": 0.8141737580299377}, {"text": "recall", "start_pos": 286, "end_pos": 292, "type": "METRIC", "confidence": 0.9744516611099243}]}, {"text": "A translation is considered correct if it retrieves the same answer as the correct logical form.", "labels": [], "entities": []}, {"text": "shows the learning curves for the \u03bb-965  WASP algorithm compared to: (1) the original WASP algorithm which uses a functional query language (FunQL); (2) SCISSOR (Ge and), a fully-supervised, combined syntacticsemantic parsing algorithm which also uses FunQL; and (3) Zettlemoyer and Collins (2005) (Z&C), a CCG-based algorithm which uses Prolog logical forms.", "labels": [], "entities": []}, {"text": "summarizes the results at the end of the learning curves (792 training examples for \u03bb-WASP, WASP and SCISSOR, 600 for Z&C).", "labels": [], "entities": [{"text": "WASP", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.6838194727897644}]}, {"text": "A few observations can be made.", "labels": [], "entities": []}, {"text": "First, algorithms that use Prolog logical forms as the target MRL generally show better recall than those using FunQL.", "labels": [], "entities": [{"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9993289709091187}, {"text": "FunQL", "start_pos": 112, "end_pos": 117, "type": "DATASET", "confidence": 0.9770654439926147}]}, {"text": "In particular, \u03bb-WASP has the best recall by far.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9988149404525757}]}, {"text": "One reason is that it allows lexical items to be combined in ways not allowed by FunQL or the hand-written templates in Z&C, e.g. [smallest [by area]] in.", "labels": [], "entities": [{"text": "FunQL", "start_pos": 81, "end_pos": 86, "type": "DATASET", "confidence": 0.9696282744407654}]}, {"text": "Second, Z&C has the best precision, although their results are based on 280 test examples only, whereas our results are based on 10-fold cross validation.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9992478489875793}]}, {"text": "Third, \u03bb-WASP has the best F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9978526830673218}]}, {"text": "To seethe relative importance of each component of the \u03bb-WASP algorithm, we performed two ablation studies.", "labels": [], "entities": []}, {"text": "First, we compared the performance of \u03bb-WASP with and without conjunct regrouping (Section 4).", "labels": [], "entities": []}, {"text": "Second, we compared the performance of \u03bb-WASP with and without language modeling for the MRL (Section 5).", "labels": [], "entities": [{"text": "MRL", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.853130578994751}]}, {"text": "It is found that conjunct regrouping improves recall (p < 0.01 based on the paired t-test), and the use of two-level rules in the maximum-entropy model improves precision and recall (p < 0.05).", "labels": [], "entities": [{"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9990999698638916}, {"text": "precision", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9992050528526306}, {"text": "recall", "start_pos": 175, "end_pos": 181, "type": "METRIC", "confidence": 0.9993793964385986}]}, {"text": "Type checking also significantly improves precision and recall.", "labels": [], "entities": [{"text": "Type checking", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.914958119392395}, {"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9995388984680176}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9989498257637024}]}, {"text": "A major advantage of \u03bb-WASP over SCISSOR and Z&C is that it does not require any prior knowledge of the NL syntax.", "labels": [], "entities": []}, {"text": "shows the performance of \u03bb-WASP on the multilingual GEOQUERY data set.", "labels": [], "entities": [{"text": "GEOQUERY data set", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.9521240989367167}]}, {"text": "The 250-example data set is a subset of the larger GEOQUERY corpus.", "labels": [], "entities": [{"text": "250-example data set", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7942944963773092}, {"text": "GEOQUERY corpus", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.9788787066936493}]}, {"text": "All English questions in this data set were manually translated into Spanish, Japanese and Turkish, while the corresponding Prolog queries remain unchanged.", "labels": [], "entities": [{"text": "Prolog", "start_pos": 124, "end_pos": 130, "type": "DATASET", "confidence": 0.9268022775650024}]}, {"text": "shows that \u03bb-WASP performed comparably for all NLs.", "labels": [], "entities": []}, {"text": "In contrast, SCISSOR cannot be used directly on the nonEnglish data, because syntactic annotations are only available in English.", "labels": [], "entities": []}, {"text": "Z&C cannot be used directly either, because it requires NL-specific templates for building CCG grammars.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of various parsing algorithms on the larger GEOQUERY corpus.", "labels": [], "entities": [{"text": "GEOQUERY corpus", "start_pos": 66, "end_pos": 81, "type": "DATASET", "confidence": 0.982817530632019}]}, {"text": " Table 2: Performance of \u03bb-WASP with certain components of the algorithm removed.", "labels": [], "entities": []}]}