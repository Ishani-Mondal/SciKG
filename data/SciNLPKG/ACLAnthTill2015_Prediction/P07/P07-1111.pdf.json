{"title": [{"text": "A Re-examination of Machine Learning Approaches for Sentence-Level MT Evaluation", "labels": [], "entities": [{"text": "Sentence-Level MT", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.6965986788272858}]}], "abstractContent": [{"text": "Recent studies suggest that machine learning can be applied to develop good automatic evaluation metrics for machine translated sentences.", "labels": [], "entities": []}, {"text": "This paper further analyzes aspects of learning that impact performance.", "labels": [], "entities": []}, {"text": "We argue that previously proposed approaches of training a Human-Likeness classifier is not as well correlated with human judgments of translation quality , but that regression-based learning produces more reliable metrics.", "labels": [], "entities": []}, {"text": "We demonstrate the feasibility of regression-based metrics through empirical analysis of learning curves and generalization studies and show that they can achieve higher correlations with human judgments than standard automatic metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "As machine translation (MT) research advances, the importance of its evaluation also grows.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.8756806373596191}]}, {"text": "Efficient evaluation methodologies are needed both for facilitating the system development cycle and for providing an unbiased comparison between systems.", "labels": [], "entities": []}, {"text": "To this end, a number of automatic evaluation metrics have been proposed to approximate human judgments of MT output quality.", "labels": [], "entities": [{"text": "MT output", "start_pos": 107, "end_pos": 116, "type": "TASK", "confidence": 0.9025853276252747}]}, {"text": "Although studies have shown them to correlate with human judgments at the document level, they are not sensitive enough to provide reliable evaluations at the sentence level (.", "labels": [], "entities": []}, {"text": "This suggests that current metrics do not fully reflect the set of criteria that people use in judging sentential translation quality.", "labels": [], "entities": [{"text": "sentential translation quality", "start_pos": 103, "end_pos": 133, "type": "TASK", "confidence": 0.6509868999322256}]}, {"text": "A recent direction in the development of metrics for sentence-level evaluation is to apply machine learning to create an improved composite metric out of less indicative ones).", "labels": [], "entities": []}, {"text": "Under the assumption that good machine translation will produce \"human-like\" sentences, classifiers are trained to predict whether a sentence is authored by a human or by a machine based on features of that sentence, which maybe the sentence's scores from individual automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "The confidence of the classifier's prediction can then be interpreted as a judgment on the translation quality of the sentence.", "labels": [], "entities": []}, {"text": "Thus, the composite metric is encoded in the confidence scores of the classification labels.", "labels": [], "entities": []}, {"text": "While the learning approach to metric design offers the promise of ease of combining multiple metrics and the potential for improved performance, several salient questions should be addressed more fully.", "labels": [], "entities": [{"text": "metric design", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9152252972126007}]}, {"text": "First, is learning a \"Human Likeness\" classifier the most suitable approach for framing the MTevaluation question?", "labels": [], "entities": [{"text": "framing", "start_pos": 80, "end_pos": 87, "type": "TASK", "confidence": 0.9723653197288513}, {"text": "MTevaluation question", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.8322613835334778}]}, {"text": "An alternative is regression, in which the composite metric is explicitly learned as a function that approximates humans' quantitative judgments, based on a set of human evaluated training sentences.", "labels": [], "entities": []}, {"text": "Although regression has been considered on a small scale fora single system as confidence estimation, this approach has not been studied as extensively due to scalability and generalization concerns.", "labels": [], "entities": []}, {"text": "Second, how does the diversity of the model features impact the learned metric?", "labels": [], "entities": []}, {"text": "Third, how well do learning-based metrics generalize beyond their training examples?", "labels": [], "entities": []}, {"text": "In particular, how well can a metric that was developed based 880 on one group of MT systems evaluate the translation qualities of new systems?", "labels": [], "entities": []}, {"text": "In this paper, we argue for the viability of a regression-based framework for sentence-level MTevaluation.", "labels": [], "entities": [{"text": "MTevaluation", "start_pos": 93, "end_pos": 105, "type": "TASK", "confidence": 0.822555422782898}]}, {"text": "Through empirical studies, we first show that having an accurate Human-Likeness classifier does not necessarily imply having a good MTevaluation metric.", "labels": [], "entities": [{"text": "MTevaluation", "start_pos": 132, "end_pos": 144, "type": "TASK", "confidence": 0.8885805010795593}]}, {"text": "Second, we analyze the resource requirement for regression models for different sizes of feature sets through learning curves.", "labels": [], "entities": []}, {"text": "Finally, we show that SVM-regression metrics generalize better than SVM-classification metrics in their evaluation of systems that are different from those in the training set (by languages and by years), and their correlations with human assessment are higher than standard automatic evaluation metrics.", "labels": [], "entities": []}], "datasetContent": [{"text": "Recent automatic evaluation metrics typically frame the evaluation problem as a comparison task: how similar is the machine-produced output to a set of human-produced reference translations for the same source text?", "labels": [], "entities": []}, {"text": "However, as the notion of similarity is itself underspecified, several different families of metrics have been developed.", "labels": [], "entities": []}, {"text": "First, similarity can be expressed in terms of string edit distances.", "labels": [], "entities": [{"text": "similarity", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.9625574350357056}]}, {"text": "In addition to the well-known word error rate (WER), more sophisticated modifications have been proposed ().", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 30, "end_pos": 51, "type": "METRIC", "confidence": 0.8431870937347412}]}, {"text": "Second, similarity can be expressed in terms of common word sequences.", "labels": [], "entities": [{"text": "similarity", "start_pos": 8, "end_pos": 18, "type": "METRIC", "confidence": 0.9718824625015259}]}, {"text": "Since the introduction of BLEU () the basic n-gram precision idea has been augmented in a number of ways.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9938765168190002}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.97122722864151}]}, {"text": "Metrics in the Rouge family allow for skip n-grams (; take paraphrasing into account; metrics such as METEOR () and GTM () calculate both recall and precision; ME-TEOR is also similar to SIA () in that word class information is used.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.8944756984710693}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.998609185218811}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9961119294166565}]}, {"text": "Finally, researchers have begun to look for similarities at a deeper structural level.", "labels": [], "entities": []}, {"text": "For example, developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees.", "labels": [], "entities": []}, {"text": "With this wide array of metrics to choose from, MT developers need away to evaluate them.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9718909859657288}]}, {"text": "One possibility is to examine whether the automatic metric ranks the human reference translations highly with respect to machine translations ().", "labels": [], "entities": []}, {"text": "The reliability of a metric can also be more directly assessed by determining how well it correlates with human judgments of the same data.", "labels": [], "entities": [{"text": "reliability", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9731044769287109}]}, {"text": "For instance, as apart of the recent NIST sponsored MT Evaluation, each translated sentence by participating systems is evaluated by two (non-reference) human judges on a five point scale for its adequacy (does the translation retain the meaning of the original source text?) and fluency (does the translation sound natural in the target language?).", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.8741481304168701}]}, {"text": "These human assessment data are an invaluable resource for measuring the reliability of automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "In this paper, we show that they are also informative in developing better metrics.", "labels": [], "entities": []}, {"text": "A good automatic evaluation metric can be seen as a computational model that captures a human's decision process in making judgments about the adequacy and fluency of translation outputs.", "labels": [], "entities": []}, {"text": "Inferring a cognitive model of human judgments is a challenging problem because the ultimate judgment encompasses a multitude of fine-grained decisions, and the decision process may differ slightly from person to person.", "labels": [], "entities": []}, {"text": "The metrics cited in the previous section aim to capture certain aspects of human judgments.", "labels": [], "entities": []}, {"text": "One way to combine these metrics in a uniform and principled manner is through a learning framework.", "labels": [], "entities": []}, {"text": "The individual metrics participate as input features, from which the learning algorithm infers a composite metric that is optimized on training examples.", "labels": [], "entities": []}, {"text": "Reframing sentence-level translation evaluation as a classification task was first proposed by.", "labels": [], "entities": [{"text": "sentence-level translation evaluation", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.7390361825625101}]}, {"text": "Interestingly, instead of recasting the classification problem as a \"Human Acceptability\" test (distinguishing good translations outputs from bad one), they chose to develop a Human-Likeness classifier (distinguishing outputs seem human-produced from machine-produced ones) to avoid the necessity of obtaining manually labeled training examples.", "labels": [], "entities": []}, {"text": "Later, noted that if a classifier provides a 881 confidence score for its output, that value can be interpreted as a quantitative estimate of the input instance's translation quality.", "labels": [], "entities": [{"text": "881 confidence score", "start_pos": 45, "end_pos": 65, "type": "METRIC", "confidence": 0.7993213931719462}]}, {"text": "In particular, they trained an SVM classifier that makes its decisions based on a set of input features computed from the sentence to be evaluated; the distance between input feature vector and the separating hyperplane then serves as the evaluation score.", "labels": [], "entities": []}, {"text": "The underlying assumption for both is that improving the accuracy of the classifier on the Human-Likeness test will also improve the implicit MT evaluation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9990559220314026}, {"text": "MT evaluation", "start_pos": 142, "end_pos": 155, "type": "TASK", "confidence": 0.8625660836696625}]}, {"text": "A more direct alternative to the classification approach is to learn via regression and explicitly optimize fora function (i.e. MT evaluation metric) that approximates human judgments in training examples.", "labels": [], "entities": []}, {"text": "raised two main objections against regression for MT evaluations.", "labels": [], "entities": [{"text": "MT evaluations", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.9292742609977722}]}, {"text": "One is that regression requires a large set of labeled training examples.", "labels": [], "entities": [{"text": "regression", "start_pos": 12, "end_pos": 22, "type": "TASK", "confidence": 0.9833595752716064}]}, {"text": "Another is that regression may not generalize well overtime, and re-training may become necessary, which would require collecting additional human assessment data.", "labels": [], "entities": []}, {"text": "While these are legitimate concerns, we show through empirical studies (in Section 4.2) that the additional resource requirement is not impractically high, and that a regression-based metric has higher correlations with human judgments and generalizes better than a metric derived from a Human-Likeness classifier.", "labels": [], "entities": []}, {"text": "A concern in using a metric derived from a HumanLikeness classifier is whether it would be predictive for MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.945320725440979}]}, {"text": "tried to demonstrate a positive correlation between the Human-Likeness classification task and the MT evaluation task empirically.", "labels": [], "entities": [{"text": "Human-Likeness classification task", "start_pos": 56, "end_pos": 90, "type": "TASK", "confidence": 0.7416745026906332}, {"text": "MT evaluation", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.9044630229473114}]}, {"text": "They plotted the classification accuracy and evaluation reliability fora number of classifiers, which were generated as apart of a greedy search for kernel parameters and found some linear correlation between the two.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9100850820541382}]}, {"text": "This proof of concept is a little misleading, however, because the population of the sampled classifiers was biased toward those from the same neighborhood as the local optimal classifier (so accuracy and correlation may only exhibit linear relationship locally).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 192, "end_pos": 200, "type": "METRIC", "confidence": 0.9986799359321594}]}, {"text": "Here, we perform a similar study except that we sampled the kernel parameter more uniformly (on a log scale).", "labels": [], "entities": []}, {"text": "As confirms, having an accurate Human-Likeness classifier does not necessarily entail having a good MT evaluation metric.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 100, "end_pos": 113, "type": "TASK", "confidence": 0.8213521838188171}]}, {"text": "Although the two tasks do seem to be positively related, and in the limit there maybe a system that is good at both tasks, one may improve classification without improving MT evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 172, "end_pos": 174, "type": "TASK", "confidence": 0.9856491684913635}]}, {"text": "For this set of heldout data, at the near 80% accuracy range, a derived metric might have an MT evaluation correlation coefficient anywhere between 0.25 (on par with 883 unsmoothed BLEU, which is known to be unsuitable for sentence-level evaluation) and 0.35 (competitive with standard metrics).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9981223940849304}, {"text": "MT evaluation correlation coefficient", "start_pos": 93, "end_pos": 130, "type": "METRIC", "confidence": 0.8047153353691101}, {"text": "BLEU", "start_pos": 181, "end_pos": 185, "type": "METRIC", "confidence": 0.9645002484321594}]}], "tableCaptions": [{"text": " Table 1: Correlations for cross-year generalization.  Learning-based metrics are developed from NIST  2003 Chinese data. All metrics are tested on datasets  from 2003 Arabic, 2002 Chinese and 2004 Chinese.", "labels": [], "entities": [{"text": "cross-year generalization", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.6278095245361328}, {"text": "NIST  2003 Chinese data", "start_pos": 97, "end_pos": 120, "type": "DATASET", "confidence": 0.979692131280899}]}, {"text": " Table 2: Metric correlations within each system. The columns specify which metric is used. The rows  specify which MT system is under evaluation; they are ordered by human-judged system quality, from best  to worst. For each evaluated MT system (row), the highest coefficient in bold font, and those that are  statistically comparable to the highest are shown in italics.", "labels": [], "entities": [{"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9275164604187012}]}]}