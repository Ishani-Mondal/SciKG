{"title": [{"text": "Guided Learning for Bidirectional Sequence Classification", "labels": [], "entities": [{"text": "Bidirectional Sequence Classification", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.7899659276008606}]}], "abstractContent": [{"text": "In this paper, we propose guided learning, anew learning framework for bidirectional sequence classification.", "labels": [], "entities": [{"text": "bidirectional sequence classification", "start_pos": 71, "end_pos": 108, "type": "TASK", "confidence": 0.6747990846633911}]}, {"text": "The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm.", "labels": [], "entities": []}, {"text": "We apply this novel learning algorithm to POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.7694260776042938}]}, {"text": "It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features.", "labels": [], "entities": [{"text": "error rate", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9902075231075287}, {"text": "PTB test set", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.9773851831754049}, {"text": "error reduction", "start_pos": 95, "end_pos": 110, "type": "METRIC", "confidence": 0.8494231402873993}]}], "introductionContent": [{"text": "Many NLP tasks can be modeled as a sequence classification problem, such as POS tagging, chunking, and incremental parsing.", "labels": [], "entities": [{"text": "sequence classification", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7092783004045486}, {"text": "POS tagging", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.8229431211948395}]}, {"text": "A traditional method to solve this problem is to decompose the whole task into a set of individual tasks for each token in the input sequence, and solve these small tasks in a fixed order, usually from left to right.", "labels": [], "entities": []}, {"text": "In this way, the output of the previous small tasks can be used as the input of the later tasks.", "labels": [], "entities": []}, {"text": "HMM and MaxEnt Markov Model are examples of this method.", "labels": [], "entities": [{"text": "HMM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8733925819396973}]}, {"text": "showed that this approach suffered from the so called label bias problem.", "labels": [], "entities": []}, {"text": "They proposed Conditional Random Fields (CRF) as a general solution for sequence classification.", "labels": [], "entities": [{"text": "Conditional Random Fields (CRF)", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.5833080460627874}, {"text": "sequence classification", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.8562513887882233}]}, {"text": "CRF models a sequence as an undirected graph, which means that all the individual tasks are solved simultaneously.", "labels": [], "entities": []}, {"text": "improved the CRF method by employing the large margin method to separate the gold standard sequence labeling from incorrect labellings.", "labels": [], "entities": [{"text": "CRF", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.6494004726409912}]}, {"text": "However, the complexity of quadratic programming for the large margin approach prevented it from being used in large scale NLP tasks.", "labels": [], "entities": []}, {"text": "Collins proposed a Perceptron like learning algorithm to solve sequence classification in the traditional left-to-right order.", "labels": [], "entities": [{"text": "sequence classification", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.7172188609838486}]}, {"text": "This solution does not suffer from the label bias problem.", "labels": [], "entities": []}, {"text": "Compared to the undirected methods, the Perceptron like algorithm is faster in training.", "labels": [], "entities": []}, {"text": "In this paper, we will improve upon Collins' algorithm by introducing a bidirectional searching strategy, so as to effectively utilize more context information at little extra cost.", "labels": [], "entities": []}, {"text": "When a bidirectional strategy is used, the main problem is how to select the order of inference.", "labels": [], "entities": []}, {"text": "Tsuruoka and proposed the easiest-first approach which greatly reduced the computation complexity of inference while maintaining the accuracy on labeling.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9988420605659485}]}, {"text": "However, the easiest-first approach only serves as a heuristic rule.", "labels": [], "entities": []}, {"text": "The order of inference is not incorporated into the training of the MaxEnt classifier for individual labeling.", "labels": [], "entities": []}, {"text": "Here, we will propose a novel learning framework, namely guided learning, to integrate classification of individual tokens and inference order selection into a single learning task.", "labels": [], "entities": []}, {"text": "We proposed a Perceptron like learning algorithm () for guided learning.", "labels": [], "entities": []}, {"text": "We apply this algorithm to POS tagging, a classic sequence learning problem.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.8217325806617737}]}, {"text": "Our system reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system () by using fewer features.", "labels": [], "entities": [{"text": "error rate", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9891111850738525}, {"text": "PTB test set", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9691997567812601}]}, {"text": "By using deterministic search, it obtains an error rate of 2.73%, a 5.9% relative error reduction 760 over the previous best deterministic algorithm).", "labels": [], "entities": [{"text": "error rate", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.986561506986618}, {"text": "relative error reduction", "start_pos": 73, "end_pos": 97, "type": "METRIC", "confidence": 0.6604767441749573}]}, {"text": "The new POS tagger is similar to () in the way that we employ context features.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.676020622253418}]}, {"text": "We use a bidirectional search strategy, and our algorithm is based on Perceptron learning.", "labels": [], "entities": []}, {"text": "A unique contribution of our work is on the integration of individual classification and inference order selection, which are learned simultaneously.", "labels": [], "entities": [{"text": "individual classification", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.683251678943634}, {"text": "inference order selection", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.611739863952001}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Data set splits", "labels": [], "entities": []}, {"text": " Table 2: Experiments on the development data with beam width of 3  we cut the PTB into the training, development and  test sets as shown in", "labels": [], "entities": [{"text": "PTB", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.7547327280044556}]}, {"text": " Table 1. We use tools provided  by", "labels": [], "entities": []}, {"text": " Table 4: Comparison with the previous works", "labels": [], "entities": []}]}