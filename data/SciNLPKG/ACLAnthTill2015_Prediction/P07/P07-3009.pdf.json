{"title": [{"text": "Logistic Online Learning Methods and Their Application to Incremental Dependency Parsing", "labels": [], "entities": [{"text": "Incremental Dependency Parsing", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.6450052261352539}]}], "abstractContent": [{"text": "We investigate a family of update methods for online machine learning algorithms for cost-sensitive multiclass and structured classification problems.", "labels": [], "entities": []}, {"text": "The update rules are based on multinomial logistic models.", "labels": [], "entities": []}, {"text": "The most interesting question for such an approach is how to integrate the cost function into the learning paradigm.", "labels": [], "entities": []}, {"text": "We propose a number of solutions to this problem.", "labels": [], "entities": []}, {"text": "To demonstrate the applicability of the algorithms , we evaluated them on a number of classification tasks related to incremental dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.6806414872407913}]}, {"text": "These tasks were conventional multiclass classification, hiearchi-cal classification, and a structured classification task: complete labeled dependency tree prediction.", "labels": [], "entities": [{"text": "multiclass classification", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.7085254341363907}, {"text": "hiearchi-cal classification", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.7348076403141022}, {"text": "dependency tree prediction", "start_pos": 141, "end_pos": 167, "type": "TASK", "confidence": 0.6338655153910319}]}, {"text": "The performance figures of the logistic algorithms range from slightly lower to slightly higher than margin-based online algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language consists of complex structures, such as sequences of phonemes, parse trees, and discourse or temporal graphs.", "labels": [], "entities": []}, {"text": "Researchers in NLP have started to realize that this complexity should be reflected in their statistical models.", "labels": [], "entities": []}, {"text": "This intuition has spurred a growing interest of related research in the machine learning community, which in turn has led to improved results in a wide range of applications in NLP, including sequence labeling (), constituent and dependency parsing), and logical form extraction).", "labels": [], "entities": [{"text": "constituent and dependency parsing", "start_pos": 215, "end_pos": 249, "type": "TASK", "confidence": 0.5861508697271347}, {"text": "logical form extraction", "start_pos": 256, "end_pos": 279, "type": "TASK", "confidence": 0.628998855749766}]}, {"text": "Machine learning research for structured problems have generally used margin-based formulations.", "labels": [], "entities": []}, {"text": "These include global batch methods such as Max-margin Markov Networks (M 3 N) () and SVM struct) as well as online methods such as Margin Infused Relaxed Algorithm (MIRA) and the Online Passive-Aggressive Algorithm (OPA)).", "labels": [], "entities": []}, {"text": "Although the batch methods are formulated very elegantly, they do not seem to scale well to the large training sets prevalent in NLP contexts.", "labels": [], "entities": []}, {"text": "The online methods on the other hand, although less theoretically appealing, can handle realistically sized data sets.", "labels": [], "entities": []}, {"text": "In this work, we investigate whether logistic online learning performs as well as margin-based methods.", "labels": [], "entities": []}, {"text": "Logistic models are easily extended to using kernels; that this is theoretically well-justified was shown by, who also made an elegant argument that margin-based methods are in fact related to regularized logistic models.", "labels": [], "entities": []}, {"text": "For batch learning, there exist several learning algorithms in a logistic framework for conventional multiclass classification but few for structured problems.", "labels": [], "entities": [{"text": "batch learning", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.91773721575737}, {"text": "multiclass classification", "start_pos": 101, "end_pos": 126, "type": "TASK", "confidence": 0.7068609595298767}]}, {"text": "Prediction of complex structures is conventionally treated as a cost-sensitive multiclass classification problem, although special care has to betaken to handle the large space of possible outputs.", "labels": [], "entities": [{"text": "Prediction of complex structures", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8326098620891571}, {"text": "multiclass classification", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.7303895354270935}]}, {"text": "The integration of the cost function into the logistic framework leads to two distinct (although related) update methods: the Scaled Prior Variance (SPV) and the Minimum Expected Cost (MEC) updates.", "labels": [], "entities": [{"text": "Scaled Prior Variance (SPV)", "start_pos": 126, "end_pos": 153, "type": "METRIC", "confidence": 0.8750071326891581}]}, {"text": "Apart from its use in structured prediction, costsensitive classification is useful for hierachical classification, which we briefly consider herein an experiment.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.6953327357769012}, {"text": "hierachical classification", "start_pos": 88, "end_pos": 114, "type": "TASK", "confidence": 0.8283958733081818}]}, {"text": "This type of classification has useful ap-plications in NLP.", "labels": [], "entities": []}, {"text": "Apart from the obvious use in classification of concepts in an ontology, it is also useful for prediction of complex morphological or named-entity tags.", "labels": [], "entities": [{"text": "prediction of complex morphological or named-entity tags", "start_pos": 95, "end_pos": 151, "type": "TASK", "confidence": 0.8132411071232387}]}, {"text": "Cost-sensitive learning is also required in the SEARN algorithm), which is a method to decompose the prediction problem of a complex structure into a sequence of actions, and train the search in the space of action sequences to maximize global performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "To compare the logistic online algorithms against other learning algorithms, we performed a set of experiments in incremental dependency parsing using the Nivre algorithm.", "labels": [], "entities": []}, {"text": "The algorithm is a variant of the shift-reduce algorithm and creates a projective and acyclic graph.", "labels": [], "entities": []}, {"text": "As with the regular shift-reduce, it uses a stack Sand a list of input words W , and builds the parse tree incrementally using a set of parsing actions (see).", "labels": [], "entities": []}, {"text": "However, instead of finding constituents, it builds a set of arcs representing the graph of dependencies.", "labels": [], "entities": []}, {"text": "It can be shown that every projective dependency graph can be produced by a sequence of parser actions, and that the worst-case number of actions is linear with respect to the number of words in the sentence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Results for dependency tree prediction.", "labels": [], "entities": [{"text": "dependency tree prediction", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.8423219124476115}]}]}