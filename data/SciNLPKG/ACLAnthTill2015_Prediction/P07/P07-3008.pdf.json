{"title": [{"text": "Limitations of Current Grammar Induction Algorithms", "labels": [], "entities": []}], "abstractContent": [{"text": "I review a number of grammar induction algorithms (ABL, Emile, Adios), and test them on the Eindhoven corpus, resulting in disappointing results, compared to the usually tested corpora (ATIS, OVIS).", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7054403424263}, {"text": "ABL", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.917522668838501}, {"text": "Emile", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.8960157632827759}, {"text": "Eindhoven corpus", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.9428261816501617}]}, {"text": "Also, I show that using neither POS-tags induced from Biemann's unsupervised POS-tagging algorithm nor hand-corrected POS-tags as input improves this situation.", "labels": [], "entities": []}, {"text": "Last, I argue for the development of entirely incremental grammar induction algorithms instead of the approaches of the systems discussed before.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammar induction is a task within the field of natural language processing that attempts to construct a grammar of a given language solely on the basis of positive examples of this language.", "labels": [], "entities": [{"text": "Grammar induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7882643043994904}, {"text": "natural language processing", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.5773782630761465}]}, {"text": "If a successful method is found, this will have both practical applications and considerable theoretical implications.", "labels": [], "entities": []}, {"text": "Concerning the practical side, this will make the engineering of NLP systems easier, especially for less widely studied languages.", "labels": [], "entities": []}, {"text": "One can conceive successful GI algorithms as an inspiration for statistical machine translation systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.6417931715647379}]}, {"text": "Theoretically, grammar induction is important as well.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.8724124431610107}]}, {"text": "One of the main assertions in the nativist's position is the Poverty of the Stimulus argument, which means that the child does not perceive enough positive examples of language throughout his early youth to have learned the grammar from his parents, without the help of innate knowledge (or: Universal Grammar), that severely constrains the number of hypotheses (i.e. grammars) that he can learn.", "labels": [], "entities": []}, {"text": "Proved more strictly for formal grammars, work showed that one cannot learn any type of superfinite grammar (e.g. regular languages, contextfree languages), if one only perceives (an unlimited amount of) positive examples.", "labels": [], "entities": []}, {"text": "After, say, n examples, there is always more than 1 grammar that would be able to explain the seen examples, thus these grammar might give different judgments on an n + 1 th example, of which it is impossible to say in advance which judgment is the correct one.", "labels": [], "entities": []}, {"text": "But, given this is true, isn't the grammar induction pursuit deemed to fail?", "labels": [], "entities": [{"text": "grammar induction pursuit", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.7867500384648641}]}, {"text": "First, there are hints that children do receive negative information, and that they use it for grammar acquisition.", "labels": [], "entities": [{"text": "grammar acquisition", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.8248835802078247}]}, {"text": "Also, the strictness required by Gold is not needed, and an approximation in the framework of PAC (Probably Approximately Correct) or VC (Vapnis and Chervonenkis) could then suffice.", "labels": [], "entities": [{"text": "PAC", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9447225332260132}, {"text": "VC", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.8971011638641357}]}, {"text": "This, and other arguments favouring the use of machine learning techniques in linguistic theory testing, are very well reviewed in.", "labels": [], "entities": [{"text": "linguistic theory testing", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.8247578740119934}]}, {"text": "Several attempts have been made to create such systems.", "labels": [], "entities": []}, {"text": "The authors of these systems reported promising results on the ATIS and OVIS treebanks.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.7951141595840454}, {"text": "OVIS treebanks", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.9427209496498108}]}, {"text": "I tried to replicate these findings on the more complicated Eindhoven treebank, which turned out to yield disappointing results, even inferior to very simple baselines.", "labels": [], "entities": [{"text": "Eindhoven treebank", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.9645004570484161}]}, {"text": "As an attempt to ameliorate this, and as an attempt to confirm and thesis that good enough unsupervised POS-taggers exist to justify using POS-tags instead of words in evaluating GI systems, I pre-sented the algorithms with both POS-tags that were induced from Biemann's unsupervised POS-tagging algorithm and hand-corrected POS-tags.", "labels": [], "entities": []}, {"text": "This did not lead to improvement.", "labels": [], "entities": []}], "datasetContent": [{"text": "Different methods of evaluation are used in GI.", "labels": [], "entities": [{"text": "GI", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9791475534439087}]}, {"text": "One of them is visual inspection).", "labels": [], "entities": []}, {"text": "This is not a reproducible and independent evaluation measure, and it does certainly not suffice as an assessment of the quality of the results.", "labels": [], "entities": []}, {"text": "However, argue that this evaluation should still be included in GI discussions.", "labels": [], "entities": [{"text": "GI", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.8977376818656921}]}, {"text": "A second evaluation method is shown by, in which Adios had to carryout a test that is available on the Internet: English as a Second Language (ESL).", "labels": [], "entities": []}, {"text": "This test shows three sentences, of which the examinee has to say which sentence is the grammatical one.", "labels": [], "entities": []}, {"text": "Adios answers around 60% correct on these questions, which is considered as intermediate fora person who has had 6 years of English lessons.", "labels": [], "entities": []}, {"text": "Although this sounds impressive, no examples of test sentences are given, and the website is not available anymore, so we are notable to assess this result.", "labels": [], "entities": []}, {"text": "A third option is to have sentences generated by the induced grammar judged on their naturalness, and compare this average with the average of the sentences of the original corpus.", "labels": [], "entities": []}, {"text": "showed that the judgments of Adios generated sentences were comparable to the sentences in their corpus.", "labels": [], "entities": []}, {"text": "However, the algorithm might just generates overly simple utterances, and will receive relatively high scores that it doesn't deserve.", "labels": [], "entities": []}, {"text": "The last option for evaluation is to compare the parses with hand-annotated treebanks.", "labels": [], "entities": []}, {"text": "This gives the most quantifiable and detailed view on the performance of a GI system.", "labels": [], "entities": []}, {"text": "An interesting comparative study between Emile and ABL using this evaluation method is available in van Zaanen and Adriaans (2001) where F-scores of 41.4% (Emile) and 61.7% (ABL) are reported on the OVIS (Openbaar Vervoer Informatie Systeem 3 ; Dutch) corpus, and 25.4% and 39.2% on the ATIS (Air Traffic Information System; English) corpus.", "labels": [], "entities": [{"text": "ABL", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.48819512128829956}, {"text": "F-scores", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9970544576644897}, {"text": "ABL", "start_pos": 174, "end_pos": 177, "type": "METRIC", "confidence": 0.9717488288879395}, {"text": "OVIS (Openbaar Vervoer Informatie Systeem 3 ; Dutch) corpus", "start_pos": 199, "end_pos": 258, "type": "DATASET", "confidence": 0.6567015051841736}, {"text": "ATIS (Air Traffic Information System; English) corpus", "start_pos": 287, "end_pos": 340, "type": "DATASET", "confidence": 0.6842764645814896}]}, {"text": "A major choice in evaluating GI systems is to decide which corpus to train the algorithm on.", "labels": [], "entities": []}, {"text": "The creators of ABL and Emile chose to test on the ATIS and OVIS corpus, which is, I believe, an unfortunate choice.", "labels": [], "entities": [{"text": "ABL", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.7532148361206055}, {"text": "Emile", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.6973427534103394}, {"text": "ATIS and OVIS corpus", "start_pos": 51, "end_pos": 71, "type": "DATASET", "confidence": 0.6285737752914429}]}, {"text": "These corpora contain sentences that are spoken to a computer, and represent a very limited subset of language.", "labels": [], "entities": []}, {"text": "Deep recursion, one of the aspects that is hard to catch in grammar induction, does not occur often.", "labels": [], "entities": [{"text": "Deep recursion", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7324356436729431}, {"text": "grammar induction", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7240778654813766}]}, {"text": "The average sentence lengths are 7.5 (ATIS) and 4.4 (OVIS).", "labels": [], "entities": [{"text": "ATIS", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9852299094200134}, {"text": "OVIS", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.8414656519889832}]}, {"text": "If we want to know whether a system is truly capable of bootstrapping knowledge about language, there is only one way to test it: by using natural language that is unlimited in its expressive power.", "labels": [], "entities": []}, {"text": "Therefore, I will test ABL, Adios and Emile on the Eindhoven corpus, that contains 7K sentences, with an average length of approximately 20 tokens.", "labels": [], "entities": [{"text": "ABL", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.8214851021766663}, {"text": "Emile", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.9088122844696045}, {"text": "Eindhoven corpus", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.9695894122123718}]}, {"text": "This is, as far as I know, the first attempt to train and test word-based GI algorithms on such a complicated corpus.", "labels": [], "entities": []}, {"text": "The second experiment deals with the difference between tag-based and word-based systems.", "labels": [], "entities": []}, {"text": "Intuitively, the latter task seems to be more challenging.", "labels": [], "entities": []}, {"text": "Still, and stick to tag-based models.", "labels": [], "entities": []}, {"text": "First, Bod assumes that unsupervised POStagging can be done successfully, without explicitly showing results that can confirm this.", "labels": [], "entities": [{"text": "Bod", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.9438640475273132}]}, {"text": "Klein and Manning did tag their text using a simple unsupervised POS-tagging algorithm, and this mod-erately harmed their performance: their ContextConstituent Model's F-score on Wall Street Journal text fell from 71.1% to 63.2%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 168, "end_pos": 175, "type": "METRIC", "confidence": 0.9889680743217468}, {"text": "Wall Street Journal text", "start_pos": 179, "end_pos": 203, "type": "DATASET", "confidence": 0.9572272151708603}]}, {"text": "Second, Klein and Manning created context vectors fora number of non-terminals (NP, VP, PP), and extracted the two principal components from these vectors.", "labels": [], "entities": []}, {"text": "They did the same with contexts of constituents and distituents.", "labels": [], "entities": []}, {"text": "The distribution of these vectors suggest that the non-terminals were easier to distinguish from each other than the constituents from the distituents, suggesting that POS-tagging is easier than finding syntactic rules.", "labels": [], "entities": []}, {"text": "However, this result would be more convincing if this is true for POS-tags as well.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: This table shows the results of experiment 1. Left, Right and Random are baseline scores. The two  variants of ABL differ in the selection phase. 62.9K facts were found in the Alpino treebank.", "labels": [], "entities": [{"text": "ABL", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.7384642362594604}, {"text": "Alpino treebank", "start_pos": 186, "end_pos": 201, "type": "DATASET", "confidence": 0.9645435214042664}]}, {"text": " Table 3: This table shows the results of experiment 2. The baseline scores are identical to the ones in  experiment 1.", "labels": [], "entities": []}]}