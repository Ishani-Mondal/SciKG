{"title": [{"text": "Unsupervised Language Model Adaptation Incorporating Named Entity Information", "labels": [], "entities": [{"text": "Unsupervised Language Model Adaptation Incorporating Named Entity Information", "start_pos": 0, "end_pos": 77, "type": "TASK", "confidence": 0.738301869481802}]}], "abstractContent": [{"text": "Language model (LM) adaptation is important for both speech and language processing.", "labels": [], "entities": [{"text": "Language model (LM) adaptation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6463930308818817}]}, {"text": "It is often achieved by combining a generic LM with a topic-specific model that is more relevant to the target document.", "labels": [], "entities": []}, {"text": "Unlike previous work on un-supervised LM adaptation, this paper investigates how effectively using named entity (NE) information, instead of considering all the words, helps LM adaptation.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.8922513127326965}, {"text": "LM adaptation", "start_pos": 174, "end_pos": 187, "type": "TASK", "confidence": 0.9729960858821869}]}, {"text": "We evaluate two latent topic analysis approaches in this paper, namely, clustering and Latent Dirichlet Allocation (LDA).", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA", "start_pos": 87, "end_pos": 119, "type": "METRIC", "confidence": 0.952135956287384}]}, {"text": "In addition, anew dynamically adapted weighting scheme for topic mixture models is proposed based on LDA topic analysis.", "labels": [], "entities": []}, {"text": "Our experimental results show that the NE-driven LM adaptation framework outperforms the baseline generic LM.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.774236649274826}]}, {"text": "The best result is obtained using the LDA-based approach by expanding the named entities with syntactically filtered words, together with using a large number of topics, which yields a perplexity reduction of 14.23% compared to the baseline generic LM.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language model (LM) adaptation plays an important role in speech recognition and many natural language processing tasks, such as machine translation and information retrieval.", "labels": [], "entities": [{"text": "Language model (LM) adaptation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5956850250562032}, {"text": "speech recognition", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7818325161933899}, {"text": "machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7894860208034515}, {"text": "information retrieval", "start_pos": 153, "end_pos": 174, "type": "TASK", "confidence": 0.7727651596069336}]}, {"text": "Statistical N-gram LMs have been widely used; however, they capture only local contextual information.", "labels": [], "entities": []}, {"text": "In addition, even with the increasing amount of LM training data, there is often a mismatch problem because of differences in domain, topics, or styles.", "labels": [], "entities": []}, {"text": "Adaptation of LM, therefore, is very important in order to better deal with a variety of topics and styles.", "labels": [], "entities": [{"text": "Adaptation of LM", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.837153971195221}]}, {"text": "Many studies have been conducted for LM adaptation.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9795540571212769}]}, {"text": "One method is supervised LM adaptation, where topic information is typically available and a topic specific LM is interpolated with the generic LM ().", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.9277107417583466}]}, {"text": "In contrast, various unsupervised approaches perform latent topic analysis for LM adaptation.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.9646539986133575}]}, {"text": "To identify implicit topics from the unlabeled corpus, one simple technique is to group the documents into topic clusters by assigning only one topic label to a document.", "labels": [], "entities": []}, {"text": "Recently several other methods in the line of latent semantic analysis have been proposed and used in LM adaptation, such as latent semantic analysis (LSA), probabilistic latent semantic analysis (PLSA)), and LDA ().", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.7004610399405161}, {"text": "LM adaptation", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.9867759346961975}, {"text": "latent semantic analysis (LSA)", "start_pos": 125, "end_pos": 155, "type": "TASK", "confidence": 0.7123154054085413}, {"text": "probabilistic latent semantic analysis (PLSA))", "start_pos": 157, "end_pos": 203, "type": "TASK", "confidence": 0.7291183088506971}]}, {"text": "Most of these existing approaches are based on the \"bag of words\" model to represent documents, where all the words are treated equally and no relation or association between words is considered.", "labels": [], "entities": []}, {"text": "Unlike prior work in LM adaptation, this paper investigates how to effectively leverage named entity information for latent topic analysis.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9896120131015778}, {"text": "latent topic analysis", "start_pos": 117, "end_pos": 138, "type": "TASK", "confidence": 0.619011640548706}]}, {"text": "Named entities are very common in domains such as newswire or broadcast news, and carry valuable information, which we hypothesize is topic indicative and useful for latent topic analysis.", "labels": [], "entities": [{"text": "latent topic analysis", "start_pos": 166, "end_pos": 187, "type": "TASK", "confidence": 0.6268250942230225}]}, {"text": "We compare different latent topic generation approaches as well as model adaptation methods, and propose an LDA based dynamic weighting method for the topic mixture model.", "labels": [], "entities": [{"text": "topic generation", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7144523859024048}]}, {"text": "Furthermore, we expand named entities by incorporating other content words, in order to capture more topic information.", "labels": [], "entities": []}, {"text": "Our experimental results show that the proposed method of incorporating named information in LM adaptation is effective.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 93, "end_pos": 106, "type": "TASK", "confidence": 0.9641840755939484}]}, {"text": "In addition, we find that for the LDA based adaptation scheme, adding more content words and increasing the number of topics can further improve the performance significantly.", "labels": [], "entities": [{"text": "LDA based adaptation", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.8297499815622965}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we review some related work.", "labels": [], "entities": []}, {"text": "Section 3 describes in detail our unsupervised LM adaptation approach using named entities.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9571252465248108}]}, {"text": "Experimental results are presented and discussed in Section 4.", "labels": [], "entities": []}, {"text": "Conclusion and future work appear in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Statistics of our experimental data.", "labels": [], "entities": []}, {"text": " Table 3. Perplexity results using the baseline LM  vs. the single topic LMs.", "labels": [], "entities": []}, {"text": " Table 4. Perplexity results using the adapted topic  model (single vs. mixture) for clustering and LDA  based approaches.", "labels": [], "entities": []}]}