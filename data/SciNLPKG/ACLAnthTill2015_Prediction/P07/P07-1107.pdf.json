{"title": [{"text": "Unsupervised Coreference Resolution in a Nonparametric Bayesian Model", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.873602420091629}]}], "abstractContent": [{"text": "We present an unsupervised, nonparamet-ric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.9606966376304626}]}, {"text": "While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state.", "labels": [], "entities": []}, {"text": "Despite being unsupervised, our system achieves a 70.3 MUC F 1 measure on the MUC-6 test set, broadly in the range of some recent supervised results.", "labels": [], "entities": [{"text": "MUC F 1 measure", "start_pos": 55, "end_pos": 70, "type": "METRIC", "confidence": 0.77728521078825}, {"text": "MUC-6 test set", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9817981322606405}]}], "introductionContent": [{"text": "Referring to an entity in natural language can broadly be decomposed into two processes.", "labels": [], "entities": [{"text": "Referring to an entity in natural language", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.861631555216653}]}, {"text": "First, speakers directly introduce new entities into discourse, entities which maybe shared across discourses.", "labels": [], "entities": []}, {"text": "This initial reference is typically accomplished with proper or nominal expressions.", "labels": [], "entities": []}, {"text": "Second, speakers refer back to entities already introduced.", "labels": [], "entities": []}, {"text": "This anaphoric reference is canonically, though of course not always, accomplished with pronouns, and is governed by linguistic and cognitive constraints.", "labels": [], "entities": []}, {"text": "In this paper, we present a nonparametric generative model of a document corpus which naturally connects these two processes.", "labels": [], "entities": []}, {"text": "Most recent coreference resolution work has focused on the task of deciding which mentions (noun phrases) in a document are coreferent.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.9720920026302338}, {"text": "deciding which mentions (noun phrases) in a document are coreferent", "start_pos": 67, "end_pos": 134, "type": "TASK", "confidence": 0.6772835229833921}]}, {"text": "The dominant approach is to decompose the task into a collection of pairwise coreference decisions.", "labels": [], "entities": []}, {"text": "One then applies discriminative learning methods to pairs of mentions, using features which encode properties such as distance, syntactic environment, and soon ().", "labels": [], "entities": []}, {"text": "Although such approaches have been successful, they have several liabilities.", "labels": [], "entities": []}, {"text": "First, rich features require plentiful labeled data, which we do not have for coreference tasks inmost domains and languages.", "labels": [], "entities": []}, {"text": "Second, coreference is inherently a clustering or partitioning task.", "labels": [], "entities": [{"text": "coreference", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9737505912780762}]}, {"text": "Naive pairwise methods can and do fail to produce coherent partitions.", "labels": [], "entities": []}, {"text": "One classic solution is to make greedy left-to-right linkage decisions.", "labels": [], "entities": []}, {"text": "Recent work has addressed this issue in more global ways.", "labels": [], "entities": []}, {"text": "use graph partioning in order to reconcile pairwise scores into a final coherent clustering.", "labels": [], "entities": []}, {"text": "Nonetheless, all these systems crucially rely on pairwise models because clusterlevel models are much harder to work with, combinatorially, in discriminative approaches.", "labels": [], "entities": []}, {"text": "Another thread of coreference work has focused on the problem of identifying matches between documents ().", "labels": [], "entities": []}, {"text": "These methods ignore the sequential anaphoric structure inside documents, but construct models of how and when entities are shared between them.", "labels": [], "entities": []}, {"text": "1 These models, as ours, are generative ones, since the focus is on cluster discovery and the data is generally unlabeled.", "labels": [], "entities": [{"text": "cluster discovery", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.8280941545963287}]}, {"text": "In this paper, we present a novel, fully generative, nonparametric Bayesian model of mentions in a document corpus.", "labels": [], "entities": []}, {"text": "Our model captures both withinand cross-document coreference.", "labels": [], "entities": []}, {"text": "At the top, a hierarchical Dirichlet process () cap-tures cross-document entity (and parameter) sharing, while, at the bottom, a sequential model of salience captures within-document sequential structure.", "labels": [], "entities": []}, {"text": "As a joint model of several kinds of discourse variables, it can be used to make predictions about either kind of coreference, though we focus experimentally on within-document measures.", "labels": [], "entities": []}, {"text": "To the best of our ability to compare, our model achieves the best unsupervised coreference performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We adopt the terminology of the Automatic Context Extraction (ACE) task.", "labels": [], "entities": [{"text": "Automatic Context Extraction (ACE) task", "start_pos": 32, "end_pos": 71, "type": "TASK", "confidence": 0.7998954313141959}]}, {"text": "For this paper, we assume that each document in a corpus consists of a set of mentions, typically noun phrases.", "labels": [], "entities": []}, {"text": "Each mention is a reference to some entity in the domain of discourse.", "labels": [], "entities": []}, {"text": "The coreference resolution task is to partition the mentions according to referent.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9287084639072418}]}, {"text": "Mentions can be divided into three categories, proper mentions (names), nominal mentions (descriptions), and pronominal mentions (pronouns).", "labels": [], "entities": []}, {"text": "In section 3, we present a sequence of increasingly enriched models, motivating each from shortcomings of the previous.", "labels": [], "entities": []}, {"text": "As we go, we will indicate the performance of each model on data from ACE 2004).", "labels": [], "entities": [{"text": "ACE 2004", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.944604218006134}]}, {"text": "In particular, we used as our development corpus the English translations of the Arabic and Chinese treebanks, comprising 95 documents and about 3,905 mentions.", "labels": [], "entities": []}, {"text": "This data was used heavily for model design and hyperparameter selection.", "labels": [], "entities": [{"text": "model design", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.7621317803859711}, {"text": "hyperparameter selection", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.7786598801612854}]}, {"text": "In section 5, we present final results for new test data from MUC-6 on which no tuning or development was performed.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.9230909943580627}]}, {"text": "This test data will form our basis for comparison to previous work.", "labels": [], "entities": []}, {"text": "In all experiments, as is common, we will assume that we have been given as part of our input the true mention boundaries, the headword of each mention and the mention type (proper, nominal, or pronominal).", "labels": [], "entities": []}, {"text": "For the ACE data sets, the head and mention type are given as part of the mention annotation.", "labels": [], "entities": [{"text": "ACE data sets", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9789790511131287}]}, {"text": "For the MUC data, the head was crudely chosen to be the rightmost mention token, and the mention type was automatically detected.", "labels": [], "entities": [{"text": "MUC data", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.8116294145584106}]}, {"text": "We will not assume any other information to be present in the data beyond the text itself.", "labels": [], "entities": []}, {"text": "In particular, unlike much related work, we do not assume gold named entity recognition (NER) labels; indeed we do not assume observed NER labels or POS tags at all.", "labels": [], "entities": [{"text": "gold named entity recognition (NER) labels", "start_pos": 58, "end_pos": 100, "type": "TASK", "confidence": 0.7864456027746201}]}, {"text": "Our pri- Figure 1: Graphical model depiction of document level entity models described in sections 3.1 and 3.2 respectively.", "labels": [], "entities": []}, {"text": "The shaded nodes indicate observed variables.", "labels": [], "entities": []}, {"text": "mary performance metric will be the MUC F 1 measure (, commonly used to evaluate coreference systems on a within-document basis.", "labels": [], "entities": [{"text": "MUC F 1 measure", "start_pos": 36, "end_pos": 51, "type": "METRIC", "confidence": 0.8388841301202774}]}, {"text": "Since our system relies on sampling, all results are averaged over five random runs.", "labels": [], "entities": []}, {"text": "We present our final experiments using the full model developed in section 3.", "labels": [], "entities": []}, {"text": "As in section 3, we use true mention boundaries and evaluate using the MUC F 1 measure (.", "labels": [], "entities": [{"text": "MUC F 1 measure", "start_pos": 71, "end_pos": 86, "type": "METRIC", "confidence": 0.7176867201924324}]}, {"text": "All hyperparameters were tuned on the development set only.", "labels": [], "entities": []}, {"text": "The document concentration parameter \u03b1 was set by taking a constant proportion of the average number of mentions in a document across the corpus.", "labels": [], "entities": [{"text": "document concentration parameter \u03b1", "start_pos": 4, "end_pos": 38, "type": "METRIC", "confidence": 0.5401016920804977}]}, {"text": "This number was chosen to minimize the squared error between the number of proposed entities and true entities in a document.", "labels": [], "entities": []}, {"text": "It was not tuned to maximize the F 1 measure.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9859600861867269}]}, {"text": "A coefficient of 0.4 was chosen.", "labels": [], "entities": []}, {"text": "The global concentration coefficient \u03b3 was chosen to be a constant proportion of \u03b1M , where M is the number of documents in the corpus.", "labels": [], "entities": [{"text": "global concentration coefficient \u03b3", "start_pos": 4, "end_pos": 38, "type": "METRIC", "confidence": 0.6735819429159164}]}, {"text": "We found 0.15 to be a good value using the same least-square procedure.", "labels": [], "entities": []}, {"text": "The values for these coefficients were not changed for the experiments on the test sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Posterior distribution of mention type given salience  by bucketing entity activation rank. Pronouns are preferred for  entities which have high salience and non-pronominal mentions  are preferred for inactive entities.", "labels": [], "entities": []}, {"text": " Table 2: Formal Results: Our system evaluated using the MUC model theoretic measure Vilain et al. (1995). The table in (a) is  our performance on the thirty document MUC-6 formal test set with increasing amounts of training data. In all cases for the table,  we are evaluating on the same thirty document test set which is included in our training set, since our system in unsupervised. The  table in (b) is our performance on the ACE 2004 training sets.", "labels": [], "entities": [{"text": "MUC model theoretic measure Vilain et al. (1995)", "start_pos": 57, "end_pos": 105, "type": "DATASET", "confidence": 0.8843839276920665}, {"text": "MUC-6 formal test set", "start_pos": 167, "end_pos": 188, "type": "DATASET", "confidence": 0.9045962691307068}, {"text": "ACE 2004 training sets", "start_pos": 432, "end_pos": 454, "type": "DATASET", "confidence": 0.9824718683958054}]}]}