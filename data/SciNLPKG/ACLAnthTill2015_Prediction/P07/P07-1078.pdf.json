{"title": [{"text": "Self-Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7245780229568481}]}], "abstractContent": [{"text": "Creating large amounts of annotated data to train statistical PCFG parsers is expensive, and the performance of such parsers declines when training and test data are taken from different domains.", "labels": [], "entities": [{"text": "PCFG parsers", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.67120660841465}]}, {"text": "In this paper we use self-training in order to improve the quality of a parser and to adapt it to a different domain , using only small amounts of manually annotated seed data.", "labels": [], "entities": []}, {"text": "We report significant improvement both when the seed and test data are in the same domain and in the out-of-domain adaptation scenario.", "labels": [], "entities": []}, {"text": "In particular , we achieve 50% reduction in annotation cost for the in-domain case, yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case.", "labels": [], "entities": []}, {"text": "This is the first time that self-training with small labeled datasets is applied successfully to these tasks.", "labels": [], "entities": []}, {"text": "We were also able to formulate a characterization of when self-training is valuable.", "labels": [], "entities": []}], "introductionContent": [{"text": "State of the art statistical parsers) are trained on manually annotated treebanks that are highly expensive to create.", "labels": [], "entities": []}, {"text": "Furthermore, the performance of these parsers decreases as the distance between the genres of their training and test data increases.", "labels": [], "entities": []}, {"text": "Therefore, enhancing the performance of parsers when trained on small manually annotated datasets is of great importance, both when the seed and test data are taken from the same domain (the in-domain scenario) and when they are taken from different domains (the outof-domain or parser adaptation scenario).", "labels": [], "entities": []}, {"text": "Since the problem is the expense in manual annotation, we define 'small' to be 100-2,000 sentences, which are the sizes of sentence sets that can be manually annotated by constituent structure in a few hours . Self-training is a method for using unannotated data when training supervised models.", "labels": [], "entities": []}, {"text": "The model is first trained using manually annotated ('seed') data, then the model is used to automatically annotate a pool of unannotated ('self-training') data, and then the manually and automatically annotated datasets are combined to create the training data for the final model.", "labels": [], "entities": []}, {"text": "Self-training of parsers trained on small datasets is of enormous potential practical importance, due to the huge amounts of unannotated data that are becoming available today and to the high cost of manual annotation.", "labels": [], "entities": []}, {"text": "In this paper we use self-training to enhance the performance of a generative statistical PCFG parser) for both the in-domain and the parser adaptation scenarios, using only small amounts of manually annotated data.", "labels": [], "entities": [{"text": "generative statistical PCFG parser", "start_pos": 67, "end_pos": 101, "type": "TASK", "confidence": 0.8215489834547043}, {"text": "parser adaptation", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.8399665057659149}]}, {"text": "We perform four experiments, examining all combinations of in-domain and out-of-domain seed and self-training data.", "labels": [], "entities": []}, {"text": "Our results show that self-training is of substantial benefit for the problem.", "labels": [], "entities": []}, {"text": "In particular, we present: \u2022 50% reduction in annotation cost when the seed and test data are taken from the same domain, which is 66% higher than any previous result with small manually annotated datasets.", "labels": [], "entities": []}, {"text": "\u2022 The first time that self-training improves a generative parser when the seed and test data are from the same domain.", "labels": [], "entities": [{"text": "generative parser", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.9591482877731323}]}, {"text": "\u2022 20-33% reduction in annotation cost when the seed and test data are from different domains.", "labels": [], "entities": []}, {"text": "\u2022 The first time that self-training succeeds in adapting a generative parser between domains using a small manually annotated dataset.", "labels": [], "entities": [{"text": "generative parser", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.8481535017490387}]}, {"text": "\u2022 The first formulation (related to the number of unknown words in a sentence) of when selftraining is valuable.", "labels": [], "entities": []}, {"text": "Section 2 discusses previous work, and Section 3 compares in-depth our protocol to a previous one.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 present the experimental setup and our results, and Section 6 analyzes the results in an attempt to shed light on the phenomenon of selftraining.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used a reimplementation of Collins' parsing model 2).", "labels": [], "entities": []}, {"text": "We performed four experiments, II, IO, OI, and OO, two with in-domain seed (II, IO) and two with out-of-domain seed (OI, OO), examining in-domain self-training (II, OI) and outof-domain self-training (IO, OO).", "labels": [], "entities": [{"text": "OO", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9652004241943359}]}, {"text": "Note that being 'in' or 'out' of domain is determined by the test data.", "labels": [], "entities": []}, {"text": "Each experiment contained 19 runs.", "labels": [], "entities": []}, {"text": "In each run a different seed size was used, from 100 sentences onwards, in steps of 100.", "labels": [], "entities": []}, {"text": "For statistical significance, we repeated each experiment five times, in each repetition randomly sampling different manually annotated sentences to form the seed dataset 3 . The seed data were taken from WSJ sections 2-21.", "labels": [], "entities": [{"text": "WSJ sections 2-21", "start_pos": 205, "end_pos": 222, "type": "DATASET", "confidence": 0.9578491846720377}]}, {"text": "For II and IO, the test data is WSJ section 23 (2416 sentences) and the self-training data are either WSJ sections 2-21 (in II, excluding the seed sentences) or the Brown training section (in IO).", "labels": [], "entities": [{"text": "II and IO", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.5744302372137705}, {"text": "WSJ section 23", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.9256885250409445}, {"text": "WSJ", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.9513357877731323}]}, {"text": "For OI and OO, the test data is the Brown test section (2424 sentences), and the self-training data is either the Brown training section (in OI) or WSJ sections 2-21 (in OO).", "labels": [], "entities": [{"text": "OI", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8051262497901917}, {"text": "OO", "start_pos": 11, "end_pos": 13, "type": "DATASET", "confidence": 0.5469548106193542}, {"text": "Brown test section", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.7668203910191854}, {"text": "WSJ", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.8414172530174255}]}, {"text": "We removed the manual annotations from the self-training sections before using them.", "labels": [], "entities": []}, {"text": "For the Brown corpus, we based our division on ().", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.9605921506881714}]}, {"text": "The test and training sections consist of sentences from all of the genres that form the corpus.", "labels": [], "entities": []}, {"text": "The training division consists of 90% (9 of each 10 consecutive sentences) of the data, and the test section are the remaining 10% (We did not use any held out data).", "labels": [], "entities": []}, {"text": "Parsing performance is measured by f-score, f = 2\u00d7P \u00d7R P +R , where P, R are labeled precision and recall.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.8461186289787292}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9991316199302673}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9969838261604309}]}, {"text": "To further demonstrate our results for parser adaptation, we also performed the OI experiment where seed data is taken from WSJ sections 2-21 and both self-training and test data are taken from the Switchboard corpus.", "labels": [], "entities": [{"text": "parser adaptation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.9759346842765808}, {"text": "WSJ sections 2-21", "start_pos": 124, "end_pos": 141, "type": "DATASET", "confidence": 0.9242030580838522}, {"text": "Switchboard corpus", "start_pos": 198, "end_pos": 216, "type": "DATASET", "confidence": 0.9106037616729736}]}, {"text": "The distance between the domains of these corpora is much greater than the distance between the domains of WSJ and Brown.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.9354971051216125}]}, {"text": "The Brown and Switchboard corpora were divided to sections in the same way.", "labels": [], "entities": []}, {"text": "We have also performed all four experiments with the seed data taken from the Brown training section.) used the first 500 sentences of WSJ training section as seed data.", "labels": [], "entities": [{"text": "Brown training section.", "start_pos": 78, "end_pos": 101, "type": "DATASET", "confidence": 0.8651025493939718}, {"text": "WSJ training section", "start_pos": 135, "end_pos": 155, "type": "DATASET", "confidence": 0.8974617123603821}]}, {"text": "For direct comparison, we performed our protocol in the II scenario using the first 500 or 1000 sentences of WSJ training section as seed data and got similar results to those reported below for our protocol with random selection.", "labels": [], "entities": [{"text": "WSJ training section", "start_pos": 109, "end_pos": 129, "type": "DATASET", "confidence": 0.813240130742391}]}, {"text": "We also applied the protocol of Steedman et alto scenario II with 500 randomly selected sentences, getting no improvement over the random baseline.", "labels": [], "entities": []}, {"text": "The results were very similar and will not be detailed here due to space constraints.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of in-domain seed sentences  needed for achieving certain f-scores. Reductions  compared to no self-training (line 1) are given in  parentheses.", "labels": [], "entities": []}, {"text": " Table 2: F-scores of our in-domain-seed self- training vs. self-training (ST) and co-training (CT)  of (Steedman et al, 20003a; 2003b).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985080361366272}, {"text": "co-training (CT)", "start_pos": 83, "end_pos": 99, "type": "METRIC", "confidence": 0.6979038566350937}]}, {"text": " Table 3: Reduction of the number of manually anno- tated constituents needed for achieving f score value  of 80 on section 23 of the WSJ. In all cases the seed  and additional sentences selected to train the parser  are taken from sections 02-21 of WSJ.", "labels": [], "entities": [{"text": "f score value", "start_pos": 92, "end_pos": 105, "type": "METRIC", "confidence": 0.9771655400594076}, {"text": "WSJ", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.9493377208709717}, {"text": "WSJ", "start_pos": 250, "end_pos": 253, "type": "DATASET", "confidence": 0.9855653643608093}]}, {"text": " Table 4: Number of manually annotated seed sen- tences needed for achieving certain f-score values.  The first two lines show the out-of-domain and in- domain seed baselines. The reductions compared to  the baselines is given as ID, OD.", "labels": [], "entities": [{"text": "ID", "start_pos": 230, "end_pos": 232, "type": "METRIC", "confidence": 0.9962956309318542}, {"text": "OD", "start_pos": 234, "end_pos": 236, "type": "METRIC", "confidence": 0.9959793090820312}]}]}