{"title": [], "abstractContent": [{"text": "This paper presents pipeline iteration, an approach that uses output from later stages of a pipeline to constrain earlier stages of the same pipeline.", "labels": [], "entities": []}, {"text": "We demonstrate significant improvements in a state-of-the-art PCFG parsing pipeline using base-phrase constraints, derived either from later stages of the parsing pipeline or from a finite-state shallow parser.", "labels": [], "entities": [{"text": "PCFG parsing pipeline", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.7234665552775065}]}, {"text": "The best performance is achieved by reranking the union of un-constrained parses and relatively heavily-constrained parses.", "labels": [], "entities": []}], "introductionContent": [{"text": "A \"pipeline\" system consists of a sequence of processing stages such that the output from one stage provides the input to the next.", "labels": [], "entities": []}, {"text": "Each stage in such a pipeline identifies a subset of the possible solutions, and later stages are constrained to find solutions within that subset.", "labels": [], "entities": []}, {"text": "For example, a part-of-speech tagger could constrain a \"base phrase\" chunker), or the n-best output of a parser could constrain a reranker.", "labels": [], "entities": []}, {"text": "A pipeline is typically used to reduce search complexity for rich models used in later stages, usually at the risk that the best solutions maybe pruned in early stages.", "labels": [], "entities": []}, {"text": "Pipeline systems are ubiquitous in natural language processing, used not only in parsing), but also machine translation and speech recognition (Fiscus, 1997;), among others.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7726382911205292}, {"text": "speech recognition", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7176244109869003}]}, {"text": "Despite the widespread use of pipelines, they have been understudied, with very little work on general techniques for designing and improving pipeline systems (although cf.).", "labels": [], "entities": []}, {"text": "This paper presents one such general technique, here applied to stochastic parsing, whereby output from later stages of a pipeline is used to constrain earlier stages of the same pipeline.", "labels": [], "entities": [{"text": "stochastic parsing", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.6215908229351044}]}, {"text": "To our knowledge, this is the first time such a pipeline architecture has been proposed.", "labels": [], "entities": []}, {"text": "It may seem surprising that later stages of a pipeline, already constrained to be consistent with the output of earlier stages, can profitably inform the earlier stages in a second pass.", "labels": [], "entities": []}, {"text": "However, the richer models used in later stages of a pipeline provide a better distribution over the subset of possible solutions produced by the early stages, effectively resolving some of the ambiguities that account for much of the original variation.", "labels": [], "entities": []}, {"text": "If an earlier stage is then constrained in a second pass not to vary with respect to these resolved ambiguities, it will be forced to find other variations, which may include better solutions than were originally provided.", "labels": [], "entities": []}, {"text": "To give a rough illustration, consider the Venn diagram in.", "labels": [], "entities": []}, {"text": "Set A represents the original subset of possible solutions passed along by the earlier stage, and the dark shaded region represents highprobability solutions according to later stages.", "labels": [], "entities": []}, {"text": "If some constraints are then extracted from these highprobability solutions, defining a subset of solutions (S) that rule out some of A, the early stage will be forced to produce a different set (B).", "labels": [], "entities": []}, {"text": "Constraints derived from later stages of the pipeline focus the search in an area believed to contain high-quality candidates.", "labels": [], "entities": []}, {"text": "Another scenario is to use a different model altogether to constrain the pipeline.", "labels": [], "entities": []}, {"text": "In this scenario, derived from later stages of an iterated pipelined system; and (ii) constraints derived from a different model.", "labels": [], "entities": []}, {"text": "represented in(ii), the other model constrains the early stage to be consistent with some subset of solutions (S), which maybe largely or completely disjoint from the original set A.", "labels": [], "entities": []}, {"text": "Again, a different set (B) results, which may include better results than A.", "labels": [], "entities": []}, {"text": "Whereas when iterating we are guaranteed that the new subset S will overlap at least partially with the original subset A, that is not the case when making use of constraints from a separately trained model.", "labels": [], "entities": []}, {"text": "In this paper, we investigate pipeline iteration within the context of the parsing pipeline, by constraining parses to be consistent with a base-phrase tree.", "labels": [], "entities": []}, {"text": "We derive these base-phrase constraints from three sources: the reranking stage of the parsing pipeline; a finite-state shallow parser); and a combination of the output from these two sources.", "labels": [], "entities": [{"text": "parsing pipeline", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.9006526172161102}]}, {"text": "We compare the relative performance of these three sources and find the best performance improvements using constraints derived from a weighted combination of shallow parser output and reranker output.", "labels": [], "entities": []}, {"text": "The Charniak parsing pipeline has been extensively studied over the past decade, with a number of papers focused on improving early stages of the pipeline) as well as many focused on optimizing final parse accuracy).", "labels": [], "entities": [{"text": "Charniak parsing", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.5634652674198151}, {"text": "accuracy", "start_pos": 206, "end_pos": 214, "type": "METRIC", "confidence": 0.7196653485298157}]}, {"text": "This focus on optimization has made system improvements very difficult to achieve; yet our relatively simple architecture yields statistically significant improvements, making pipeline iteration a promising approach for other tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we constructed a simple parsing pipeline, shown in.", "labels": [], "entities": [{"text": "parsing pipeline", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.8968847692012787}]}, {"text": "At the core of the pipeline is the coarse-to-fine parser and MaxEnt reranker, described in Sec.", "labels": [], "entities": []}, {"text": "2. The parser constitutes the first and second stages of our pipeline, and the reranker the final stage.", "labels": [], "entities": []}, {"text": "Following, we set the parser to output 50-best parses for all experiments described here.", "labels": [], "entities": []}, {"text": "We constrain only the first stage of the parser: during chart construction, we disallow any constituents that conflict with the constraints, as described in detail in the next section.", "labels": [], "entities": []}, {"text": "Our experiments will demonstrate the effects of constraining the Charniak parser under several different conditions.", "labels": [], "entities": []}, {"text": "The baseline system places no constraints on the parser.", "labels": [], "entities": []}, {"text": "The remaining experimental conditions each consider one of three possible sources of the base phrase constraints: (1) the base phrases output by the finite-state shallow parser; (2) the base phrases extracted from output of the reranker; and (3) a combination of the output from the shallow parser and the reranker, which is produced using the techniques outlined in Sec.", "labels": [], "entities": []}, {"text": "Constraints are enforced as described in Sec.", "labels": [], "entities": []}, {"text": "Unconstrained For our baseline system, we run the Charniak and Johnson (2005) parser and reranker with default parameters.", "labels": [], "entities": []}, {"text": "The parser is provided with treebank-tokenized text and, as mentioned previously, outputs 50-best parse candidates to the reranker.", "labels": [], "entities": []}, {"text": "FS-constrained The FS-constrained condition provides a comparison point of non-iterated constraints.", "labels": [], "entities": [{"text": "FS-constrained", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.6808118224143982}]}, {"text": "Under this condition, the one-best base-  phrase tree output by the finite-state shallow parser is input as a constraint to the Charniak parser.", "labels": [], "entities": []}, {"text": "We run the parser and reranker as before, under constraints from the shallow parser.", "labels": [], "entities": []}, {"text": "The accuracy of the constraints used under this condition is shown in the first row of.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993947744369507}]}, {"text": "Note that this condition is not an instance of pipeline iteration, but is included to show the performance levels that can be achieved without iteration.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F-scores on WSJ section 24 of output from two", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9877413511276245}, {"text": "WSJ section 24", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.8407585422197977}]}, {"text": " Table 2: Labeled recall (LR), precision (LP), and F-scores", "labels": [], "entities": [{"text": "recall (LR)", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9435320943593979}, {"text": "precision (LP)", "start_pos": 31, "end_pos": 45, "type": "METRIC", "confidence": 0.9537418633699417}, {"text": "F-scores", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9920944571495056}]}, {"text": " Table 3: Full-parse F-scores on WSJ section 24. The unconstrained search (first row) provides a baseline comparison for the", "labels": [], "entities": [{"text": "F-scores", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.8049968481063843}, {"text": "WSJ section 24", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.90168164173762}]}, {"text": " Table 4: Full-parse F-scores on WSJ section 24 after taking the set union of unconstrained and constrained parser output under", "labels": [], "entities": [{"text": "WSJ section 24", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.8262474139531454}]}]}