{"title": [{"text": "Making Sense of Sound: Unsupervised Topic Segmentation over Acoustic Input", "labels": [], "entities": [{"text": "Topic Segmentation", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7534387707710266}]}], "abstractContent": [{"text": "We address the task of unsupervised topic segmentation of speech data operating over raw acoustic information.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7441765666007996}]}, {"text": "In contrast to existing algorithms for topic segmentation of speech, our approach does not require input transcripts.", "labels": [], "entities": [{"text": "topic segmentation of speech", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.8266829177737236}]}, {"text": "Our method predicts topic changes by analyzing the distribution of re-occurring acoustic patterns in the speech signal corresponding to a single speaker.", "labels": [], "entities": []}, {"text": "The algorithm robustly handles noise inherent in acoustic matching by intelligently aggregat-ing information about the similarity profile from multiple local comparisons.", "labels": [], "entities": [{"text": "acoustic matching", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7379712760448456}]}, {"text": "Our experiments show that audio-based segmen-tation compares favorably with transcript-based segmentation computed over noisy transcripts.", "labels": [], "entities": []}, {"text": "These results demonstrate the desirability of our method for applications where a speech recognizer is not available, or its output has a high word error rate.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7191705107688904}]}], "introductionContent": [{"text": "An important practical application of topic segmentation is the analysis of spoken data.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7335014194250107}]}, {"text": "Paragraph breaks, section markers and other structural cues common in written documents are entirely missing in spoken data.", "labels": [], "entities": []}, {"text": "Insertion of these structural markers can benefit multiple speech processing applications, including audio browsing, retrieval, and summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 132, "end_pos": 145, "type": "TASK", "confidence": 0.9873346090316772}]}, {"text": "Not surprisingly, a variety of methods for topic segmentation have been developed in the past ().", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.8331489861011505}]}, {"text": "These methods typically assume that a segmentation algorithm has access not only to acoustic input, but also to its transcript.", "labels": [], "entities": []}, {"text": "This assumption is natural for applications where the transcript has to be computed as part of the system output, or it is readily available from other system components.", "labels": [], "entities": []}, {"text": "However, for some domains and languages, the transcripts may not be available, or the recognition performance may not be adequate to achieve reliable segmentation.", "labels": [], "entities": []}, {"text": "In order to process such data, we need a method for topic segmentation that does not require transcribed input.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7969104647636414}]}, {"text": "In this paper, we explore a method for topic segmentation that operates directly on a raw acoustic speech signal, without using any input transcripts.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7555625736713409}]}, {"text": "This method predicts topic changes by analyzing the distribution of reoccurring acoustic patterns in the speech signal corresponding to a single speaker.", "labels": [], "entities": []}, {"text": "In the same way that unsupervised segmentation algorithms predict boundaries based on changes in lexical distribution, our algorithm is driven by changes in the distribution of acoustic patterns.", "labels": [], "entities": []}, {"text": "The central hypothesis here is that similar sounding acoustic sequences produced by the same speaker correspond to similar lexicographic sequences.", "labels": [], "entities": []}, {"text": "Thus, by analyzing the distribution of acoustic patterns we could approximate a traditional content analysis based on the lexical distribution of words in a transcript.", "labels": [], "entities": []}, {"text": "Analyzing high-level content structure based on low-level acoustic features poses interesting computational and linguistic challenges.", "labels": [], "entities": []}, {"text": "For instance, we need to handle the noise inherent in matching based on acoustic similarity, because of possible varia-504 tions in speaking rate or pronunciation.", "labels": [], "entities": []}, {"text": "Moreover, in the absence of higher-level knowledge, information about word boundaries is not always discernible from the raw acoustic input.", "labels": [], "entities": []}, {"text": "This causes problems because we have no obvious unit of comparison.", "labels": [], "entities": []}, {"text": "Finally, noise inherent in the acoustic matching procedure complicates the detection of distributional changes in the comparison matrix.", "labels": [], "entities": [{"text": "acoustic matching", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7347873449325562}]}, {"text": "The algorithm presented in this paper demonstrates the feasibility of topic segmentation over raw acoustic input corresponding to a single speaker.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7336902320384979}]}, {"text": "We first apply a variant of the dynamic time warping algorithm to find similar fragments in the speech input through alignment.", "labels": [], "entities": []}, {"text": "Next, we construct a comparison matrix that aggregates the output of the alignment stage.", "labels": [], "entities": []}, {"text": "Since aligned utterances are separated by gaps and differ in duration, this representation gives rise to sparse and irregular input.", "labels": [], "entities": []}, {"text": "To obtain robust similarity change detection, we invoke a series of transformations to smooth and refine the comparison matrix.", "labels": [], "entities": [{"text": "similarity change detection", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.7659831047058105}]}, {"text": "Finally, we apply the minimum-cut segmentation algorithm to the transformed comparison matrix to detect topic boundaries.", "labels": [], "entities": []}, {"text": "We compare the performance of our method against traditional transcript-based segmentation algorithms.", "labels": [], "entities": []}, {"text": "As expected, the performance of the latter depends on the accuracy of the input transcript.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9989084005355835}]}, {"text": "When a manual transcription is available, the gap between audio-based segmentation and transcriptbased segmentation is substantial.", "labels": [], "entities": []}, {"text": "However, in a more realistic scenario when the transcripts are fraught with recognition errors, the two approaches exhibit similar performance.", "labels": [], "entities": []}, {"text": "These results demonstrate that audio-based algorithms are an effective and efficient solution for applications where transcripts are unavailable or highly errorful.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We use a publicly available 3 corpus of introductory Physics lectures described in our previous work).", "labels": [], "entities": []}, {"text": "This material is a particularly appealing application area for an audio-based segmentation algorithm -many academic subjects lack transcribed data for training, while a high ratio of in-domain technical terms limits the use of out-of-domain transcripts.", "labels": [], "entities": []}, {"text": "This corpus is also challenging from the segmentation perspective because the lectures are long and transitions between topics are subtle.", "labels": [], "entities": []}, {"text": "The corpus consists of 33 lectures, with an average length of 8500 words and an average duration of 50 minutes.", "labels": [], "entities": []}, {"text": "On average, a lecture was annotated with six segments, and atypical segment corresponds to two pages of a transcript.", "labels": [], "entities": []}, {"text": "Three lectures from this set were used for development, and 30 lectures were used for testing.", "labels": [], "entities": []}, {"text": "The lectures were delivered by the same speaker.", "labels": [], "entities": []}, {"text": "To evaluate the performance of traditional transcript-based segmentation algorithms on this corpus, we also use several types of transcripts at different levels of recognition accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.8791432976722717}]}, {"text": "In addition to manual transcripts, our corpus contains two types of automatic transcripts, one obtained using speaker-dependent (SD) models and the other obtained using speaker-independent (SI) models.", "labels": [], "entities": []}, {"text": "The speaker-independent model was trained on 85 hours of out-of-domain general lecture material and contained no speech from the speaker in the test set.", "labels": [], "entities": []}, {"text": "The speaker-dependent model was trained by using 38 hours of audio data from other lectures given by the speaker.", "labels": [], "entities": []}, {"text": "Both recognizers incorporated word statistics from the accompanying class textbook into the language model.", "labels": [], "entities": []}, {"text": "The word error rates for the speaker-independent and speaker-dependent models are 44.9% and 19.4%, respectively.", "labels": [], "entities": []}, {"text": "We use the P k and WindowDiff measures to evaluate our system).", "labels": [], "entities": []}, {"text": "The P k measure estimates the probability that a randomly chosen pair of words within a window of length k words is inconsistently classified.", "labels": [], "entities": []}, {"text": "The WindowDiff metric is a variant of the P k measure, which penalizes false positives and near misses equally.", "labels": [], "entities": []}, {"text": "For both of these metrics, lower scores indicate better segmentation accuracy.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.9625004529953003}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9374794960021973}]}, {"text": "Baseline We use the state-of-the-art mincut segmentation system by as our point of comparison.", "labels": [], "entities": [{"text": "mincut segmentation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.661689043045044}]}, {"text": "This model is an appropriate baseline, because it has been shown to compare favorably with other top-performing segmentation systems ().", "labels": [], "entities": []}, {"text": "We use the publicly available implementation of the system.", "labels": [], "entities": []}, {"text": "As additional points of comparison, we test the uniform and random baselines.", "labels": [], "entities": []}, {"text": "These correspond to segmentations obtained by uniformly placing 509: Segmentation accuracy for audio-based segmentor (AUDIO), random (RAND), uniform (UNI) and three transcript-based segmentation algorithms that use manual (MAN), speaker-dependent (SD) and speaker-independent (SI) transcripts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9779900312423706}]}, {"text": "For all of the algorithms, the target number of segments is set to the reference number of segments.", "labels": [], "entities": []}, {"text": "boundaries along the span of the lecture and selecting random boundaries, respectively.", "labels": [], "entities": []}, {"text": "To control for segmentation granularity, we specify the number of segments in the reference segmentation for both our system and the baselines.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.9721828699111938}]}, {"text": "Parameter Tuning We tuned the number of quantized blocks, the edge cutoff parameter of the minimum cut algorithm, and the anisotropic diffusion parameters on a heldout set of three development lectures.", "labels": [], "entities": []}, {"text": "We used the same development set for the baseline segmentation systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Segmentation accuracy for audio-based  segmentor (AUDIO), random (RAND), uniform  (UNI) and three transcript-based segmentation algo- rithms that use manual (MAN), speaker-dependent  (SD) and speaker-independent (SI) transcripts. For  all of the algorithms, the target number of segments  is set to the reference number of segments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.959800660610199}]}]}