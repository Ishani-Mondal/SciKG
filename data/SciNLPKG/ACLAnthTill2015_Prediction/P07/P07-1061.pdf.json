{"title": [{"text": "Finding document topics for improving topic segmentation", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7296932190656662}]}], "abstractContent": [{"text": "Topic segmentation and identification are often tackled as separate problems whereas they are both part of topic analysis.", "labels": [], "entities": [{"text": "Topic segmentation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9019323587417603}]}, {"text": "In this article, we study how topic identification can help to improve a topic segmenter based on word reiteration.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.8639587759971619}, {"text": "word reiteration", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.6985166072845459}]}, {"text": "We first present an unsu-pervised method for discovering the topics of a text.", "labels": [], "entities": []}, {"text": "Then, we detail how these topics are used by segmentation for finding topical similarities between text segments.", "labels": [], "entities": []}, {"text": "Finally, we show through the results of an evaluation done both for French and English the interest of the method we propose.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this article, we address the problem of linear topic segmentation, which consists in segmenting documents into topically homogeneous segments that does not overlap each other.", "labels": [], "entities": [{"text": "linear topic segmentation", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.6383785903453827}]}, {"text": "This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as.", "labels": [], "entities": [{"text": "Discourse Analysis", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.9093848168849945}]}, {"text": "One criterion for classifying topic segmentation systems is the kind of knowledge they depend on.", "labels": [], "entities": [{"text": "classifying topic segmentation", "start_pos": 18, "end_pos": 48, "type": "TASK", "confidence": 0.8622549176216125}]}, {"text": "Most of them only rely on surface features of documents: word reiteration in) or discourse cues in (Passonneau and Litman, 1997;.", "labels": [], "entities": []}, {"text": "As such systems do not require external knowledge, they are not sensitive to domains but they are limited by the type of documents they can be applied to: lexical reiteration is reliable only if concepts are not too frequently expressed by several means (synonyms, etc.) and discourse cues are often rare and corpus-specific.", "labels": [], "entities": []}, {"text": "To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in; a thesaurus in; a large set of lexical cooccurrences collected from a corpus in ().", "labels": [], "entities": []}, {"text": "To a certain extent, these lexical networks enable topic segmenters to exploit a sort of concept reiteration.", "labels": [], "entities": []}, {"text": "However, their lack of any explicit topical structure makes this kind of knowledge difficult to use when lexical ambiguity is high.", "labels": [], "entities": []}, {"text": "The most simple solution to this problem is to exploit knowledge about the topics that may occur in documents.", "labels": [], "entities": []}, {"text": "Such topic models are generally built from a large set of example documents as in,) or in one component of).", "labels": [], "entities": []}, {"text": "These statistical topic models enable segmenters to improve their precision but they also restrict their scope.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9981690645217896}]}, {"text": "Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination:) combined word recurrence, co-occurrences and a thesaurus;) relied on both lexical modeling and discourse cues; () made use of word reiteration through lexical chains and discourse cues.", "labels": [], "entities": []}, {"text": "The work we report in this article takes place in the first category we have presented.", "labels": [], "entities": []}, {"text": "It does not rely on any a priori knowledge and exploits word usage rather than discourse cues.", "labels": [], "entities": []}, {"text": "More precisely, we present anew method for enhancing the results 480 of segmentation systems based on word reiteration without relying on any external knowledge.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main objective of our evaluation was to verify that taking into account text topics discovered without relying on external knowledge can actually improve a topic segmentation algorithm that is initially based on word reiteration.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.7163788676261902}]}, {"text": "Since the work of Choi), the evaluation framework he proposed has become a kind of standard for the evaluation of topic segmentation algorithms.", "labels": [], "entities": [{"text": "topic segmentation algorithms", "start_pos": 114, "end_pos": 143, "type": "TASK", "confidence": 0.7553276916344961}]}, {"text": "This framework is based on the building of artificial texts made of segments extracted from different documents.", "labels": [], "entities": []}, {"text": "It has at least two advantages: the reference corpus is easy to build as it does not require human annotations; parameters such as the size of the documents or the segments can be precisely controlled.", "labels": [], "entities": []}, {"text": "But it has also an obvious drawback: its texts are artificial.", "labels": [], "entities": []}, {"text": "This is a problem in our case as our algorithm for discovering text topics exploits the fact that the words of a topic tend to co-occur at the document scale.", "labels": [], "entities": []}, {"text": "This hypothesis is no longer valid for documents built according to the procedure of Choi.", "labels": [], "entities": []}, {"text": "It is why we adapted his framework for having more realistic documents without losing its advantages.", "labels": [], "entities": []}, {"text": "This adaptation con- Instead of taking each segment from a different document, we only use two source documents.", "labels": [], "entities": []}, {"text": "Each of them is split into a set of segments whose size is between 3 and 11 sentences, as for Choi, and an evaluation document is built by concatenating these segments in an alternate way from the beginning of the source documents, i.e. one segment from a source document and the following from the other one, until 10 segments are extracted.", "labels": [], "entities": []}, {"text": "Moreover, in order to be sure that the boundary between two adjacent segments of an evaluation document actually corresponds to a topic shift, the source documents are selected in such away that they refer to different topics.", "labels": [], "entities": []}, {"text": "This point was controlled in our case by taking documents from the corpus of the CLEF 2003 evaluation for crosslingual information retrieval: each evaluation document was built from two source documents that had been judged as relevant for two different CLEF 2003 topics.", "labels": [], "entities": [{"text": "CLEF 2003 evaluation", "start_pos": 81, "end_pos": 101, "type": "DATASET", "confidence": 0.9226564367612203}, {"text": "crosslingual information retrieval", "start_pos": 106, "end_pos": 140, "type": "TASK", "confidence": 0.6959381103515625}, {"text": "CLEF 2003 topics", "start_pos": 254, "end_pos": 270, "type": "DATASET", "confidence": 0.8910420735677084}]}, {"text": "Two evaluation corpora made of 100 documents each, one in French and one in English, were built following this procedure.", "labels": [], "entities": []}, {"text": "Table 1 shows their main characteristics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data about our evaluation corpora", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of topic segmentation for the  French corpus (P k and WD as percentages)", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7158838510513306}, {"text": "French corpus", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9207449853420258}]}, {"text": " Table 4: Evaluation of topic segmentation for the  English corpus (P k and WD as percentages)", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7373277246952057}]}]}