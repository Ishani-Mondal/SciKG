{"title": [{"text": "A Comparative Study of Parameter Estimation Methods for Statistical Natural Language Processing", "labels": [], "entities": [{"text": "Parameter Estimation", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.886923611164093}, {"text": "Statistical Natural Language Processing", "start_pos": 56, "end_pos": 95, "type": "TASK", "confidence": 0.6832466274499893}]}], "abstractContent": [{"text": "This paper presents a comparative study of five parameter estimation algorithms on four NLP tasks.", "labels": [], "entities": []}, {"text": "Three of the five algorithms are well-known in the computational linguistics community: Maximum Entropy (ME) estimation with L 2 regularization, the Averaged Perceptron (AP), and Boosting.", "labels": [], "entities": [{"text": "Averaged Perceptron (AP)", "start_pos": 149, "end_pos": 173, "type": "METRIC", "confidence": 0.9054670214653016}]}, {"text": "We also investigate ME estimation with L 1 regularization using a novel optimization algorithm, and BLasso, which is aversion of Boosting with Lasso (L 1) regularization.", "labels": [], "entities": [{"text": "ME estimation", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.8541156947612762}, {"text": "BLasso", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9952699542045593}]}, {"text": "We first investigate all of our estimators on two re-ranking tasks: a parse selection task and a language model (LM) adaptation task.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.8678906261920929}, {"text": "language model (LM) adaptation", "start_pos": 97, "end_pos": 127, "type": "TASK", "confidence": 0.6666426857312521}]}, {"text": "Then we apply the best of these estimators to two additional tasks involving conditional sequence models: a Conditional Markov Model (CMM) for part of speech tagging and a Conditional Random Field (CRF) for Chinese word segmentation.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 151, "end_pos": 165, "type": "TASK", "confidence": 0.6944495141506195}, {"text": "Chinese word segmentation", "start_pos": 207, "end_pos": 232, "type": "TASK", "confidence": 0.5841895441214243}]}, {"text": "Our experiments show that across tasks, three of the estimators-ME estimation with L 1 or L 2 regularization, and AP-are in a near statistical tie for first place.", "labels": [], "entities": [{"text": "AP-are", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9986674785614014}]}], "introductionContent": [{"text": "Parameter estimation is fundamental to many statistical approaches to NLP.", "labels": [], "entities": [{"text": "Parameter estimation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8432227671146393}]}, {"text": "Because of the high-dimensional nature of natural language, it is often easy to generate an extremely large number of features.", "labels": [], "entities": []}, {"text": "The challenge of parameter estimation is to find a combination of the typically noisy, redundant features that accurately predicts the target output variable and avoids overfitting.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.6903422623872757}]}, {"text": "Intuitively, this can be achieved either by selecting a small number of highly-effective features and ignoring the others, or by averaging over a large number of weakly informative features.", "labels": [], "entities": []}, {"text": "The first intuition motivates feature selection methods such as Boosting and BLasso (e.g.,), which usually work best when many features are completely irrelevant.", "labels": [], "entities": [{"text": "BLasso", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.7742325663566589}]}, {"text": "L 1 or Lasso regularization of linear models, introduced by, embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003;.", "labels": [], "entities": []}, {"text": "If on the other hand most features are noisy but at least weakly correlated with the target, it maybe reasonable to attempt to reduce noise by averaging overall of the features.", "labels": [], "entities": []}, {"text": "ME estimators with L 2 regularization, which have been widely used in NLP tasks (e.g.,), tend to produce models that have this property.", "labels": [], "entities": []}, {"text": "In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g.,.", "labels": [], "entities": []}, {"text": "While recent studies claim advantages for L 1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks.", "labels": [], "entities": [{"text": "L 1 regularization", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.5106264154116312}]}, {"text": "showed that BLasso, due to its explicit use of L 1 regularization, outperformed Boosting in the LM adaptation task.", "labels": [], "entities": [{"text": "BLasso", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.7662192583084106}, {"text": "LM adaptation task", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.9158798456192017}]}, {"text": "showed that for logistic regression, L 1 regularization outperforms L 2 regularization on artificial datasets which contain many completely irrelevant features.", "labels": [], "entities": []}, {"text": "showed that in two out of three tasks, an ME estimator with a one-sided Laplacian prior (i.e., L 1 regularization with the constraint that all feature weights are positive) outperformed a comparable estimator using a Gaussian prior (i.e., L 2 regularization).", "labels": [], "entities": []}, {"text": "showed that an L 1 -regularized ME estimator outperformed an L 2 -regularized estimator for ranking the parses of a stochastic unification-based grammar.", "labels": [], "entities": []}, {"text": "While these individual estimators are well described in the literature, little is known about the relative performance of these methods because the published results are generally not directly comparable.", "labels": [], "entities": []}, {"text": "For example, in the parse re-ranking task, one cannot tell whether the L 2 -regularized ME approach used by significantly outperforms the Boosting method by because different feature sets and n-best parses were used in the evaluations of these methods.", "labels": [], "entities": [{"text": "parse re-ranking task", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.947128156820933}]}, {"text": "This paper conducts a much-needed comparative study of these five parameter estimation algorithms on four NLP tasks: ME estimation with L 1 and L 2 regularization, the Averaged Perceptron (AP), Boosting, and BLasso, aversion of Boosting with Lasso (L 1 ) regularization.", "labels": [], "entities": [{"text": "ME estimation", "start_pos": 117, "end_pos": 130, "type": "TASK", "confidence": 0.6714017391204834}, {"text": "Averaged Perceptron (AP)", "start_pos": 168, "end_pos": 192, "type": "METRIC", "confidence": 0.9271882176399231}, {"text": "BLasso", "start_pos": 208, "end_pos": 214, "type": "METRIC", "confidence": 0.996637225151062}]}, {"text": "We first investigate all of our estimators on two re-ranking tasks: a parse selection task and a language model adaptation task.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.904589056968689}, {"text": "language model adaptation", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.6750021378199259}]}, {"text": "Then we apply the best of these estimators to two additional tasks involving conditional sequence models: a CMM for POS tagging and a CRF for Chinese word segmentation.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 116, "end_pos": 127, "type": "TASK", "confidence": 0.7874201238155365}, {"text": "Chinese word segmentation", "start_pos": 142, "end_pos": 167, "type": "TASK", "confidence": 0.5817291339238485}]}, {"text": "Our results show that ME estimation with L 2 regularization achieves the best performing estimators in all of the tasks, and AP achieves almost as well and requires much less training time.", "labels": [], "entities": [{"text": "ME estimation", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.741841584444046}, {"text": "AP", "start_pos": 125, "end_pos": 127, "type": "METRIC", "confidence": 0.9765971302986145}]}, {"text": "L 1 (Lasso) regularization also performs well and leads to sparser models.", "labels": [], "entities": []}], "datasetContent": [{"text": "From the four tasks we consider, parsing and language model adaptation are both examples of re-ranking.", "labels": [], "entities": [{"text": "parsing", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9835636615753174}, {"text": "language model adaptation", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.6694077352682749}]}, {"text": "In these tasks, we assume that we have been given a list of candidates \u00ed \u00b5\u00ed\u00b1\u00ae\u00ed \u00b5\u00ed\u00b1\u00ac\u00ed \u00b5\u00ed\u00b1\u00b5(\u00ed \u00b5\u00ed\u00b1\u00a5) for each training or test sample \u00ed \u00b5\u00ed\u00b1\u00a5, \u00ed \u00b5\u00ed\u00b1\u00a6 , generated using a baseline model.", "labels": [], "entities": []}, {"text": "Then, a linear model of the form in Equation (1) is used to discriminatively re-rank the candidate list using additional features which mayor may not be included in the baseline model.", "labels": [], "entities": [{"text": "Equation", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.8408564925193787}]}, {"text": "Since the mapping from \u00ed \u00b5\u00ed\u00b1\u00a5 to \u00ed \u00b5\u00ed\u00b1\u00a6 by the linear model may make use of arbitrary global features of the output and is performed \"all at once\", we call such a linear model a global model.", "labels": [], "entities": []}, {"text": "In the other two tasks (i.e., Chinese word segmentation and POS tagging), there is no explicit enumeration of \u00ed \u00b5\u00ed\u00b1\u00ae\u00ed \u00b5\u00ed\u00b1\u00ac\u00ed \u00b5\u00ed\u00b1\u00b5(\u00ed \u00b5\u00ed\u00b1\u00a5).", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.6022470494111379}, {"text": "POS tagging", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.7649520933628082}]}, {"text": "The mapping from \u00ed \u00b5\u00ed\u00b1\u00a5 to \u00ed \u00b5\u00ed\u00b1\u00a6 is determined by a sequence model which aggregates the decisions of local linear models via a dynamic program.", "labels": [], "entities": []}, {"text": "In the CMM, the local linear models are trained independently, while in the CRF model, the local models are trained jointly.", "labels": [], "entities": [{"text": "CMM", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.834632396697998}]}, {"text": "We call these two linear models local models because they dynamically combine the output of models that use only local features.", "labels": [], "entities": []}, {"text": "While it is straightforward to apply the five estimators to global models in the re-ranking framework, the application of some estimators to the local models is problematic.", "labels": [], "entities": []}, {"text": "Boosting and BLasso are too computationally expensive to be applied to CRF training and we compared the other three better performing estimation methods for this model.", "labels": [], "entities": [{"text": "Boosting", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9375083446502686}, {"text": "BLasso", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9953088164329529}, {"text": "CRF training", "start_pos": 71, "end_pos": 83, "type": "TASK", "confidence": 0.9019741714000702}]}, {"text": "The CMM is a probabilistic sequence model and the log-loss used by ME estimation is most natural for it; thus we limit the comparison to the two kinds of ME models for CMMs.", "labels": [], "entities": [{"text": "ME estimation", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.7296415865421295}]}, {"text": "Note that our goal is not to compare locally trained models to globally trained ones; fora study which focuses on this issue, see.", "labels": [], "entities": []}, {"text": "In each task we compared the performance of different estimators using task-specific measures.", "labels": [], "entities": []}, {"text": "We used the Wilcoxon signed rank test to test the statistical significance of the difference among the competing estimators.", "labels": [], "entities": [{"text": "Wilcoxon signed rank test", "start_pos": 12, "end_pos": 37, "type": "METRIC", "confidence": 0.5567957013845444}]}, {"text": "We also report other results such as number of non-zero features after estimation, number of training iterations, and computation time (in minutes of elapsed time on an XEON TM MP 3.6GHz machine).", "labels": [], "entities": [{"text": "XEON TM MP 3.6GHz machine", "start_pos": 169, "end_pos": 194, "type": "DATASET", "confidence": 0.8177528262138367}]}], "tableCaptions": [{"text": " Table 3. Performance summary of estimators  (lower is better) on language model adaptation", "labels": [], "entities": []}, {"text": " Table 1: Performance summary of estimators on  parsing re-ranking (ME/L2: ME with L 2 regulari- zation; ME/L1: ME with L 1 regularization)", "labels": [], "entities": []}, {"text": " Table 5. Performance summary of estimators on  CWS", "labels": [], "entities": [{"text": "CWS", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.5307711958885193}]}]}