{"title": [{"text": "Regression for Sentence-Level MT Evaluation with Pseudo References", "labels": [], "entities": [{"text": "Sentence-Level MT Evaluation", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.7796870271364847}]}], "abstractContent": [{"text": "Many automatic evaluation metrics for machine translation (MT) rely on making comparisons to human translations, a resource that may not always be available.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8561266481876373}]}, {"text": "We present a method for developing sentence-level MT evaluation metrics that do not directly rely on human reference translations.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9191987216472626}]}, {"text": "Our met-rics are developed using regression learning and are based on a set of weaker indicators of fluency and adequacy (pseudo references).", "labels": [], "entities": []}, {"text": "Experimental results suggest that they rival standard reference-based metrics in terms of correlations with human judgments on new test instances.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic assessment of translation quality is a challenging problem because the evaluation task, at its core, is based on subjective human judgments.", "labels": [], "entities": [{"text": "Automatic assessment of translation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6156620234251022}]}, {"text": "Reference-based metrics such as BLEU () have rephrased this subjective task as a somewhat more objective question: how closely does the translation resemble sentences that are known to be good translations for the same source?", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9950231313705444}]}, {"text": "This approach requires the participation of human translators, who provide the \"gold standard\" reference sentences.", "labels": [], "entities": []}, {"text": "However, keeping humans in the evaluation loop represents a significant expenditure both in terms of time and resources; therefore it is worthwhile to explore ways of reducing the degree of human involvement.", "labels": [], "entities": []}, {"text": "To this end, proposed a learning-based evaluation metric that does not compare against reference translations.", "labels": [], "entities": []}, {"text": "Under a learning framework, the input (i.e., the sentence to be evaluated) is represented as a set of features.", "labels": [], "entities": []}, {"text": "These are measurements that can be extracted from the input sentence (and maybe individual metrics themselves).", "labels": [], "entities": []}, {"text": "The learning algorithm combines the features to form a model (a composite evaluation metric) that produces the final score for the input.", "labels": [], "entities": []}, {"text": "Without human references, the features in the model proposed by were primarily language model features and linguistic indicators that could be directly derived from the input sentence alone.", "labels": [], "entities": []}, {"text": "Although their initial results were not competitive with standard reference-based metrics, their studies suggested that a referenceless metric may still provide useful information about translation fluency.", "labels": [], "entities": [{"text": "translation fluency", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.9229903519153595}]}, {"text": "However, a potential pitfall is that systems might \"game the metric\" by producing fluent outputs that are not adequate translations of the source.", "labels": [], "entities": []}, {"text": "This paper proposes an alternative approach to evaluate MT outputs without comparing against human references.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9777013063430786}]}, {"text": "While our metrics are also trained, our model consists of different features and is trained under a different learning regime.", "labels": [], "entities": []}, {"text": "Crucially, our model includes features that capture some notions of adequacy by comparing the input against pseudo references: sentences from other MT systems (such as commercial off-the-shelf systems or open sourced research systems).", "labels": [], "entities": [{"text": "MT", "start_pos": 148, "end_pos": 150, "type": "TASK", "confidence": 0.9547420144081116}]}, {"text": "To improve fluency judgments, the model also includes features that compare the input against target-language \"references\" such as large text corpora and treebanks.", "labels": [], "entities": []}, {"text": "Unlike human translations used by standard reference-based metrics, pseudo references are not\"gold standards\" and can be worse than the sentences being evaluated; therefore, these \"references\" in-and-of themselves are not necessarily informative enough for MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 257, "end_pos": 270, "type": "TASK", "confidence": 0.9423766732215881}]}, {"text": "The main insight of our approach is that through regression, the trained metrics can make more nuanced comparisons between the input and pseudo references.", "labels": [], "entities": []}, {"text": "More specifically, our regression objective is to infer a function that maps a feature vector (which measures an input's similarity to the pseudo references) to a score that indicates the quality of the input.", "labels": [], "entities": []}, {"text": "This is achieved by optimizing the model's output to correlate against a set of training examples, which are translation sentences labeled with quantitative assessments of their quality by human judges.", "labels": [], "entities": []}, {"text": "Although this approach does incur some human effort, it is primarily for the development of training data, which, ideally, can be amortized over along period of time.", "labels": [], "entities": []}, {"text": "To determine the feasibility of the proposed approach, we conducted empirical studies that compare our trained metrics against standard referencebased metrics.", "labels": [], "entities": []}, {"text": "We report three main findings.", "labels": [], "entities": []}, {"text": "First, pseudo references are informative comparison points.", "labels": [], "entities": []}, {"text": "Experimental results suggest that a regression-trained metric that compares against pseudo references can have higher correlations with human judgments than applying standard metrics with multiple human references.", "labels": [], "entities": []}, {"text": "Second, the learning model that uses both adequacy and fluency features performed the best, with adequacy being the more important factor.", "labels": [], "entities": []}, {"text": "Third, when the pseudo references are multiple MT systems, the regressiontrained metric is predictive even when the input is from a better MT system than those providing the references.", "labels": [], "entities": []}, {"text": "We conjecture that comparing MT outputs against other imperfect translations allows fora more nuanced discrimination of quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9842739701271057}]}], "datasetContent": [{"text": "Reference-based metrics are typically thought of as measurements of \"similarity to good translations\" because human translations are used as references, but in more general terms, they are distance measurements between two sentences.", "labels": [], "entities": []}, {"text": "The distance between a translation hypothesis and an imperfect reference is still somewhat informative.", "labels": [], "entities": [{"text": "translation", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.9745827317237854}]}, {"text": "As a toy example, consider a one-dimensional line segment.", "labels": [], "entities": []}, {"text": "A distance from the end-point uniquely determines the position of a point.", "labels": [], "entities": []}, {"text": "When the reference location is anywhere else on the line segment, a relative distance to the reference does not uniquely specify a location on the line segment.", "labels": [], "entities": []}, {"text": "However, the position of a point can be uniquely determined if we are given its relative distances to two reference locations.", "labels": [], "entities": []}, {"text": "The problem space for MT evaluation, though more complex, is not dissimilar to the toy scenario.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.980693906545639}]}, {"text": "There are two main differences.", "labels": [], "entities": []}, {"text": "First, we do not know the actual distance function -this is what we are trying to learn.", "labels": [], "entities": []}, {"text": "The distance functions we have at our disposal are all heuristic approximations to the true translational distance function.", "labels": [], "entities": []}, {"text": "Second, unlike human references, whose quality value is assumed to be maximum, the quality of a pseudo reference sentence is not known.", "labels": [], "entities": []}, {"text": "In fact, prior to training, we do not even know the quality of the reference systems.", "labels": [], "entities": []}, {"text": "Although the direct way to calibrate a reference system is to evaluate its outputs, this is not practically ideal, since human judgments would be needed each time we wish to incorporate anew reference system.", "labels": [], "entities": []}, {"text": "Our proposed alternative is to calibrate the reference systems against an existing set of human judgments fora range of outputs from different MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 143, "end_pos": 145, "type": "TASK", "confidence": 0.9429001212120056}]}, {"text": "That is, if many of the reference system's outputs are similar to those MT outputs that received low assessments, we conclude this reference system may not be of high quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9083566069602966}]}, {"text": "Thus, if anew translation is found to be similar with this reference system's output, it is more likely for the new translation to also be bad.", "labels": [], "entities": []}, {"text": "Both issues of combining evidences from heuristic distances and calibrating the quality of pseudo reference systems can be addressed by a probabilistic learning model.", "labels": [], "entities": []}, {"text": "In particular, we use regression because its problem formulation fits naturally with the objective of MT evaluations.", "labels": [], "entities": [{"text": "MT evaluations", "start_pos": 102, "end_pos": 116, "type": "TASK", "confidence": 0.8991328477859497}]}, {"text": "In regression learning, we are interested in approximating a function f that maps a multi-dimensional input vector, x, to a continuous real value, y, such that the error over a set of m training examples, {(x 1 , y 1 ), . .", "labels": [], "entities": [{"text": "regression learning", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.9530276954174042}]}, {"text": ", (x m , y m )}, is minimized according to a loss function.", "labels": [], "entities": []}, {"text": "In the context of MT evaluation, y is the \"true\" quantitative measure of translation quality for an input sentence . The function f represents a mathematical model of human judgments of translations; an input sentence is represented as a feature vector, x, which contains the information that can be extracted from the input sentence (possibly including comparisons against some reference sentences) that are relevant to computing y.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.9736632406711578}]}, {"text": "Determining the set of relevant features for this modeling is on-going re-search.", "labels": [], "entities": []}, {"text": "In this work, we consider some of the more widely used metrics as features.", "labels": [], "entities": []}, {"text": "Our full feature vector consists of r \u00d7 18 adequacy features, where r is the number of reference systems used, and 26 fluency features: Adequacy features: These include features derived from BLEU (e.g., n-gram precision, where 1 \u2264 n \u2264 5, length ratios), PER, WER, features derived from METEOR (precision, recall, fragmentation), and ROUGE-related features (nonconsecutive bigrams with a gap size of g, where 1 \u2264 g \u2264 5 and longest common subsequence).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 191, "end_pos": 195, "type": "METRIC", "confidence": 0.9980823993682861}, {"text": "precision", "start_pos": 210, "end_pos": 219, "type": "METRIC", "confidence": 0.9241165518760681}, {"text": "PER", "start_pos": 254, "end_pos": 257, "type": "METRIC", "confidence": 0.9783951044082642}, {"text": "WER", "start_pos": 259, "end_pos": 262, "type": "METRIC", "confidence": 0.8271726965904236}, {"text": "METEOR", "start_pos": 286, "end_pos": 292, "type": "METRIC", "confidence": 0.8426980972290039}, {"text": "precision", "start_pos": 294, "end_pos": 303, "type": "METRIC", "confidence": 0.9339160919189453}, {"text": "recall", "start_pos": 305, "end_pos": 311, "type": "METRIC", "confidence": 0.9690591096878052}, {"text": "ROUGE-related", "start_pos": 333, "end_pos": 346, "type": "METRIC", "confidence": 0.9479711651802063}]}, {"text": "Fluency features: We consider both string-level features such as computing n-gram precision against a target-language corpus as well as several syntaxbased features.", "labels": [], "entities": []}, {"text": "We parse each input sentence into a dependency tree and compared aspects of it against a large target-language dependency treebank.", "labels": [], "entities": []}, {"text": "In addition to adapting the idea of Head Word Chains (Liu and), we also compared the input sentence's argument structures against the treebank for certain syntactic categories.", "labels": [], "entities": []}, {"text": "Due to the large feature space to explore, we chose to work with support vector regression as the learning algorithm.", "labels": [], "entities": []}, {"text": "As its loss function, support vector regression uses an -insensitive error function, which allows for errors within a margin of a small positive value, , to be considered as having zero error (cf., pp.339-344).", "labels": [], "entities": [{"text": "support vector regression", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6724479993184408}]}, {"text": "Like its classification counterpart, this is a kernel-based algorithm that finds sparse solutions so that scores for new test instances are efficiently computed based on a subset of the most informative training examples.", "labels": [], "entities": []}, {"text": "In this work, Gaussian kernels are used.", "labels": [], "entities": []}, {"text": "The cost of regression learning is that it requires training examples that are manually assessed by human judges.", "labels": [], "entities": [{"text": "regression learning", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.9760292768478394}]}, {"text": "However, compared to the cost of creating new references whenever new (test) sentences are evaluated, the effort of creating human assessment training data is a limited (ideally, one-time) cost.", "labels": [], "entities": []}, {"text": "Moreover, there is already a sizable collection of human assessed data fora range of MT systems through multiple years of the NIST MT Eval efforts.", "labels": [], "entities": [{"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.9874550104141235}, {"text": "NIST MT Eval", "start_pos": 126, "end_pos": 138, "type": "DATASET", "confidence": 0.7316598494847616}]}, {"text": "Our experiments suggest that there is enough assessed data to train the proposed regression model.", "labels": [], "entities": []}, {"text": "Aside from reducing the cost of developing human reference translations, the proposed metric also provides an alternative perspective on automatic MT evaluation that maybe informative in its own right.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.9609906077384949}]}, {"text": "We conjecture that a metric that compares inputs against a diverse population of differently imperfect sentences maybe more discriminative in judging translation systems than solely comparing against gold standards.", "labels": [], "entities": []}, {"text": "That is, two sentences maybe considered equally bad from the perspective of a gold standard, but subtle differences between them may become more prominent if they are compared against sentences in their peer group.", "labels": [], "entities": []}, {"text": "We conducted experiments to determine the feasibility of the proposed approach and to address the following questions: (1) How informative are pseudo references in-and-of themselves?", "labels": [], "entities": []}, {"text": "Does varying the number and/or the quality of the references have an impact on the metrics?", "labels": [], "entities": []}, {"text": "(2) What are the contributions of the adequacy features versus the fluency features to the learning-based metric?", "labels": [], "entities": []}, {"text": "(3) How do the quality and distribution of the training examples, together with the quality of the pseudo references, impact the metric training?", "labels": [], "entities": []}, {"text": "(4) Do these factors impact the metric's ability in assessing sentences produced within a single MT system?", "labels": [], "entities": []}, {"text": "How does that system's quality affect metric performance?", "labels": [], "entities": []}, {"text": "The implementation of support vector regression used for these experiments is SVM-Light).", "labels": [], "entities": [{"text": "SVM-Light", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.8770901560783386}]}, {"text": "We performed all experiments using the 2004 NIST Chinese MT Eval dataset.", "labels": [], "entities": [{"text": "NIST Chinese MT Eval dataset", "start_pos": 44, "end_pos": 72, "type": "DATASET", "confidence": 0.8692832827568054}]}, {"text": "It consists of 447 source sentences that were translated by four human translators as well as ten MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9453959465026855}]}, {"text": "Each machine translated sentence was evaluated by two human judges for their fluency and adequacy on a 5-point scale 2 . To remove the bias in the distributions of scores between different judges, we follow the normalization procedure described by.", "labels": [], "entities": []}, {"text": "The two judge's total scores (i.e., sum of the normalized fluency and adequacy scores) are then averaged.", "labels": [], "entities": []}, {"text": "We chose to work with this NIST dataset because it contains numerous systems that span over a range of performance levels (see fora ranking of the systems and their averaged human assessment scores).", "labels": [], "entities": [{"text": "NIST dataset", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.9525404572486877}]}, {"text": "This allows us to have control over the variability of the experiments while answering the questions we posed above (such as the quality of the systems providing the pseudo references, the quality of MT systems being evaluated, and the diversity over the distribution of training examples).", "labels": [], "entities": [{"text": "MT", "start_pos": 200, "end_pos": 202, "type": "TASK", "confidence": 0.9621198177337646}]}, {"text": "Specifically, we reserved four systems (MT2, MT5, MT6, and MT9) for the role of pseudo references.", "labels": [], "entities": [{"text": "MT2", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.7989259362220764}, {"text": "MT5", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.761346697807312}, {"text": "MT6", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.7577061653137207}]}, {"text": "Sentences produced by the remaining six systems are used as evaluative data.", "labels": [], "entities": []}, {"text": "This set includes the best and worst systems so that we can see how well the metrics performs on sentences that are better (or worse) than the pseudo references.", "labels": [], "entities": []}, {"text": "Metrics that require no learning are directly applied onto all sentences of the evaluative set.", "labels": [], "entities": []}, {"text": "For the learningbased metrics, we perform six-fold cross validation on the evaluative dataset.", "labels": [], "entities": []}, {"text": "Each fold consists of sentences from one MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9612367153167725}]}, {"text": "Ina round robin fashion, each fold serves as the test set while the other five are used for training and heldout.", "labels": [], "entities": []}, {"text": "Thus, the trained models have seen neither the test instances nor other instances from the MT system that produced them.", "labels": [], "entities": [{"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.9568524956703186}]}, {"text": "A metric is evaluated based on its Spearman rank correlation coefficient between the scores it gave to the evaluative dataset and human assessments for the same data.", "labels": [], "entities": [{"text": "Spearman rank correlation coefficient", "start_pos": 35, "end_pos": 72, "type": "METRIC", "confidence": 0.7354686036705971}]}, {"text": "The correlation coefficient is areal number between -1, indicating perfect negative correlations, and +1, indicating perfect positive correlations.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.9797755479812622}, {"text": "areal number", "start_pos": 31, "end_pos": 43, "type": "METRIC", "confidence": 0.9521516263484955}]}, {"text": "To compare the relative quality of different metrics, we apply bootstrapping re-sampling on the data, and then use paired t-test to determine the statistical significance of the correlation differences).", "labels": [], "entities": []}, {"text": "For the results we report, unless explicitly mentioned, all stated comparisons are statistically significant with 99.8% confidence.", "labels": [], "entities": []}, {"text": "We include two standard reference-based metrics, BLEU and METEOR, as baseline comparisons.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9987877011299133}, {"text": "METEOR", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9730821847915649}]}, {"text": "BLEU is smoothed (, and it considers only matching up to bigrams because this has higher correlations with human judgments than when higher-ordered n-grams are included.: The human-judged quality often participating systems in the NIST 2004 Chinese MT Evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9909192323684692}, {"text": "NIST 2004 Chinese MT Evaluation", "start_pos": 231, "end_pos": 262, "type": "DATASET", "confidence": 0.8640724897384644}]}, {"text": "We used four systems as references (highlighted in boldface) and the data from the remaining six for training and evaluation.", "labels": [], "entities": []}, {"text": "To explore the interaction between the quality of the reference MT systems and that of the test MT systems, we further study the following pseudo reference configurations: all four systems, a highquality system with a medium quality system, two systems of medium-quality, one medium with one poor system, and only the high-quality system.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9643665552139282}]}, {"text": "For each pseudo reference configuration, we consider three metrics: BLEU, METEOR, and the regressiontrained metric (using the full feature set).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9987272620201111}, {"text": "METEOR", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9886031150817871}]}, {"text": "Each metric evaluates sentences from four test systems of varying quality: the best system in the dataset (MT1), the worst in the set (MT10), and two midranged systems (MT4 and MT7).", "labels": [], "entities": [{"text": "MT4", "start_pos": 169, "end_pos": 172, "type": "DATASET", "confidence": 0.9204191565513611}, {"text": "MT7", "start_pos": 177, "end_pos": 180, "type": "DATASET", "confidence": 0.774627685546875}]}, {"text": "The correlation coefficients are summarized in.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9750323295593262}]}, {"text": "Each row specifies a metric/reference-type combination; each column specifies an MT system being evaluated (using sentences from all other systems as training examples).", "labels": [], "entities": [{"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.8962408304214478}]}, {"text": "The fluency-only metric and standard metrics using four human references are baselines.", "labels": [], "entities": []}, {"text": "The overall trends at the dataset level generally also hold for the per-system comparisons.", "labels": [], "entities": []}, {"text": "With the exception of the evaluation of MT10, regressionbased metrics always has higher correlations than standard metrics that use the same reference configuration (comparing correlation coefficients within each cell).", "labels": [], "entities": [{"text": "MT10", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.649491548538208}]}, {"text": "When the best MT reference system (MT2) is included as pseudo references, regressionbased metrics are typically better than or not statistically different from standard applications of BLEU and METEOR with 4 human references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 185, "end_pos": 189, "type": "METRIC", "confidence": 0.9789613485336304}]}, {"text": "Using the two mid-quality MT systems as references (MT5 and MT6), regression metrics yield correlations that are only slightly lower than standard metrics with human references.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9266935586929321}, {"text": "MT5", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.882705569267273}, {"text": "MT6", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.8535634279251099}]}, {"text": "These results support our conjecture that comparing against multiple systems is informative.", "labels": [], "entities": []}, {"text": "The poorer performances of the regression-based metrics on MT10 point out an asymmetry in the learning approach.", "labels": [], "entities": [{"text": "MT10", "start_pos": 59, "end_pos": 63, "type": "TASK", "confidence": 0.5590589046478271}]}, {"text": "The regression model aims to learn a function that approximates human judgments of translated sentences through training examples.", "labels": [], "entities": []}, {"text": "In the space of all possible MT outputs, the neighborhood of good translations is much smaller than that of bad translations.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 29, "end_pos": 39, "type": "TASK", "confidence": 0.9288648366928101}]}, {"text": "Thus, as long as the regression models sees some examples of sentences with high assessment scores during training, it should have a much better estimation of the characteristics of good translations.", "labels": [], "entities": []}, {"text": "This idea is supported by the experimental data.", "labels": [], "entities": []}, {"text": "Consider the scenario of evaluating MT1 while using two mid-quality MT systems as references.", "labels": [], "entities": [{"text": "MT1", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.8905633687973022}]}, {"text": "Although the reference systems are not as high quality as the system under evaluation, and although the training examples shown to the regression model were also generated by systems whose overall quality was rated lower, the trained metric was reasonably good at ranking sentences produced by MT1.", "labels": [], "entities": [{"text": "MT1", "start_pos": 294, "end_pos": 297, "type": "DATASET", "confidence": 0.6129796504974365}]}, {"text": "In contrast, the task of evaluating sentences from MT10 is more difficult for the learning approach, perhaps because it is sufficiently different from all training and reference systems.", "labels": [], "entities": []}, {"text": "Correlations might be improved with additional reference systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The human-judged quality of ten partici- pating systems in the NIST 2004 Chinese MT Eval- uation. We used four systems as references (high- lighted in boldface) and the data from the remaining  six for training and evaluation.", "labels": [], "entities": [{"text": "NIST 2004 Chinese MT Eval- uation", "start_pos": 73, "end_pos": 106, "type": "DATASET", "confidence": 0.9184954421860831}]}, {"text": " Table 2: Comparisons of metrics (columns) using different types of references (rows). The full regression- trained metric has the highest correlation (shown in boldface) when four human references are used; it has  the second highest correlation rate (shown in italic) when four MT system references are used instead. A  regression-trained metric with only fluency features has a correlation coefficient of 0.459.", "labels": [], "entities": []}, {"text": " Table 3: Correlation comparisons of metrics by test systems. For each test system (columns) the overall  highest correlations is distinguished by an asterisk (*); correlations higher than standard metrics using  human-references are highlighted in boldface; those that are statistically comparable to them are italicized.", "labels": [], "entities": []}]}