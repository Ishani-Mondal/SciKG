{"title": [{"text": "Semantic Class Induction and Coreference Resolution", "labels": [], "entities": [{"text": "Semantic Class Induction", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7862398624420166}, {"text": "Coreference Resolution", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.9548218548297882}]}], "abstractContent": [{"text": "This paper examines whether a learning-based coreference resolver can be improved using semantic class knowledge that is automatically acquired from aversion of the Penn Treebank in which the noun phrases are labeled with their semantic classes.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.801692008972168}, {"text": "Penn Treebank", "start_pos": 165, "end_pos": 178, "type": "DATASET", "confidence": 0.9952515661716461}]}, {"text": "Experiments on the ACE test data show that a resolver that employs such induced semantic class knowledge yields a statistically significant improvement of 2% in F-measure over one that exploits heuristically computed semantic class knowledge.", "labels": [], "entities": [{"text": "ACE test data", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.9635488589604696}, {"text": "F-measure", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9923182725906372}]}, {"text": "In addition, the induced knowledge improves the accuracy of common noun resolution by 2-6%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9996273517608643}, {"text": "common noun resolution", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.5794660151004791}]}], "introductionContent": [{"text": "In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.", "labels": [], "entities": [{"text": "noun phrase (NP) coreference resolution", "start_pos": 88, "end_pos": 127, "type": "TASK", "confidence": 0.6811469537871224}]}, {"text": "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g.,,).", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.932704895734787}]}, {"text": "While these approaches have been reasonably successful (see), speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.", "labels": [], "entities": []}, {"text": "In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.", "labels": [], "entities": [{"text": "resolution of common NPs", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.8223840147256851}, {"text": "coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations)", "start_pos": 113, "end_pos": 210, "type": "TASK", "confidence": 0.5552886128425598}]}, {"text": "As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g.,), their semantic similarity as computed using WordNet (e.g.,) or Wikipedia (), and the contextual role played by an NP (see).", "labels": [], "entities": [{"text": "common noun resolution", "start_pos": 141, "end_pos": 163, "type": "TASK", "confidence": 0.6120220919450124}]}, {"text": "Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.9282229840755463}]}, {"text": "However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun the first (i.e., most frequent) WordNet sense as its SC (e.g.,,).", "labels": [], "entities": []}, {"text": "It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used bys decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9994617104530334}]}, {"text": "Motivated in part by this observation, we examine whether automatically induced semantic class knowledge can improve the performance of a learning-based coreference resolver, reporting evaluation results on the commonly-used ACE corefer-ence corpus.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 153, "end_pos": 173, "type": "TASK", "confidence": 0.7637265920639038}, {"text": "ACE corefer-ence corpus", "start_pos": 225, "end_pos": 248, "type": "DATASET", "confidence": 0.9084834059079488}]}, {"text": "Our investigation proceeds as follows.", "labels": [], "entities": []}], "datasetContent": [{"text": "As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.", "labels": [], "entities": [{"text": "SC induction", "start_pos": 6, "end_pos": 18, "type": "TASK", "confidence": 0.951004683971405}, {"text": "ACE Phase 2 coreference corpus", "start_pos": 31, "end_pos": 61, "type": "DATASET", "confidence": 0.8883868813514709}]}, {"text": "We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (, and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.", "labels": [], "entities": [{"text": "Fmeasure score", "start_pos": 55, "end_pos": 69, "type": "METRIC", "confidence": 0.9876207411289215}, {"text": "MUC scorer", "start_pos": 103, "end_pos": 113, "type": "DATASET", "confidence": 0.8485125005245209}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9992375373840332}]}, {"text": "Following, we consider an anaphoric reference, NP i , correctly resolved if NP i and its closest antecedent are in the same coreference chain in the resulting partition.", "labels": [], "entities": []}, {"text": "In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder.", "labels": [], "entities": [{"text": "IdentiFinder", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.8170441389083862}]}], "tableCaptions": [{"text": " Table 3: SC classification accuracies of different methods for the ACE training set and test set.", "labels": [], "entities": [{"text": "SC classification", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.841817170381546}, {"text": "ACE training set", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.8753928343454996}]}, {"text": " Table 4: Results for feature ablation experiments.", "labels": [], "entities": []}, {"text": " Table 5: Accuracies of single-feature classifiers.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.945307731628418}]}, {"text": " Table 5. As we can  see, NEIGHBOR has the largest contribution. This  again demonstrates the effectiveness of a distribu- tional approach to semantic similarity. Its superior  performance to WORD, the second largest contribu- tor, could be attributed to its ability to combat data", "labels": [], "entities": [{"text": "NEIGHBOR", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.4572315216064453}]}, {"text": " Table 6: Coreference results obtained via the MUC scoring program for the ACE test set.", "labels": [], "entities": [{"text": "MUC", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.7317254543304443}, {"text": "ACE test set", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.950788676738739}]}, {"text": " Table 7: Resolution accuracies for the ACE test set.", "labels": [], "entities": [{"text": "Resolution accuracies", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.9112941920757294}, {"text": "ACE test set", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.8409400383631388}]}]}