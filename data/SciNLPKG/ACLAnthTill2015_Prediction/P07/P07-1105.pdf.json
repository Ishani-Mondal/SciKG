{"title": [{"text": "Grammar Approximation by Representative Sublanguage: A New Model for Language Learning", "labels": [], "entities": [{"text": "Grammar Approximation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7576367855072021}]}], "abstractContent": [{"text": "We propose anew language learning model that learns a syntactic-semantic grammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax.", "labels": [], "entities": []}, {"text": "We show that the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.737093061208725}]}], "introductionContent": [{"text": "There is considerable interest in learning computational grammars.", "labels": [], "entities": []}, {"text": "While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well ().", "labels": [], "entities": []}, {"text": "Learning both syntax and semantics is arguably more difficult than learning syntax alone.", "labels": [], "entities": []}, {"text": "In formal grammar learning theory it has been shown that learning from \"good examples,\" or representative examples, is more powerful than learning from all the examples (.", "labels": [], "entities": [{"text": "formal grammar learning theory", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.6984260156750679}]}, {"text": "show that using a handful of \"proto-types\" significantly improves over a fully unsupervised PCFG induction model (their prototypes were formed by sequences of POS tags; for example, prototypical NPs were DT NN, JJ NN).", "labels": [], "entities": []}, {"text": "In this paper, we present anew grammar formalism and anew learning method which together address the problem of learning a syntactic-semantic grammar in the presence of a representative sample of strings annotated with their semantics, along with minimal assumptions about syntax (such as syntactic categories).", "labels": [], "entities": []}, {"text": "The semantic representation is an ontology-based semantic representation.", "labels": [], "entities": []}, {"text": "The annotation of the representative examples does not include the entire derivation, unlike most of the existing syntactic treebanks.", "labels": [], "entities": []}, {"text": "The aim of the paper is to present the formal aspects of our grammar induction model.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.6768936812877655}]}, {"text": "In Section 2, we present anew grammar formalism, called Lexicalized Well-Founded Grammars, a type of constraint-based grammars that combine syntax and semantics.", "labels": [], "entities": []}, {"text": "We then turn to the two main results of this paper.", "labels": [], "entities": []}, {"text": "In Section 3 we show that our grammars can always be learned from a set of positive representative examples (with no negative examples), and the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar.", "labels": [], "entities": []}, {"text": "In Section 4, we propose anew computationally efficient model for grammar induction from pairs of utterances and their semantic representations, called Grammar Approximation by Representative Sublanguage (GARS).", "labels": [], "entities": [{"text": "grammar induction from pairs of utterances", "start_pos": 66, "end_pos": 108, "type": "TASK", "confidence": 0.8125052452087402}]}, {"text": "Section 5 discusses the practical use of our model and Section 6 states our conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}