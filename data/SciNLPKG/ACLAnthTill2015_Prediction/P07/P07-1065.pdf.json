{"title": [{"text": "Randomised Language Modelling for Statistical Machine Translation", "labels": [], "entities": [{"text": "Randomised Language Modelling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7702323198318481}, {"text": "Statistical Machine Translation", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.81020055214564}]}], "abstractContent": [{"text": "A Bloom filter (BF) is a randomised data structure for set membership queries.", "labels": [], "entities": []}, {"text": "Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability.", "labels": [], "entities": []}, {"text": "Here we explore the use of BFs for language modelling in statistical machine translation.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7217370867729187}, {"text": "statistical machine translation", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.7122379740079244}]}, {"text": "We show how a BF containing n-grams can enable us to use much larger corpora and higher-order models complementing a conventional n-gram LM within an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 150, "end_pos": 153, "type": "TASK", "confidence": 0.9833055138587952}]}, {"text": "We also consider (i) how to include approximate frequency information efficiently within a BF and (ii) how to reduce the error rate of these models by first checking for lower-order sub-sequences in candidate n-grams.", "labels": [], "entities": [{"text": "error rate", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9724434018135071}]}, {"text": "Our solutions in both cases retain the one-sided error guarantees of the BF while taking advantage of the Zipf-like distribution of word frequencies to reduce the space requirements .", "labels": [], "entities": [{"text": "BF", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.7839391231536865}]}], "introductionContent": [{"text": "Language modelling (LM) is a crucial component in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Language modelling (LM)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8306907713413239}, {"text": "statistical machine translation (SMT)", "start_pos": 50, "end_pos": 87, "type": "TASK", "confidence": 0.8188611219326655}]}, {"text": "Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models, e.g. (. Although it is well-known that higher-order LMs and models trained on additional monolingual corpora can yield better translation performance, the challenges in deploying large LMs are not trivial.", "labels": [], "entities": []}, {"text": "Increasing the order of an n-gram model can result in an exponential increase in the number of parameters; for corpora such as the English Gigaword corpus, for instance, there are 300 million distinct trigrams and over 1.2 billion 5-grams.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 131, "end_pos": 154, "type": "DATASET", "confidence": 0.7686598499615988}]}, {"text": "Since a LM maybe queried millions of times per sentence, it should ideally reside locally in memory to avoid time-consuming remote or disk-based look-ups.", "labels": [], "entities": []}, {"text": "Against this background, we consider a radically different approach to language modelling: instead of explicitly storing all distinct n-grams, we store a randomised representation.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7621656656265259}]}, {"text": "In particular, we show that the Bloom filter; BF), a simple space-efficient randomised data structure for representing sets, maybe used to represent statistics from larger corpora and for higher-order n-grams to complement a conventional smoothed trigram model within an SMT decoder.", "labels": [], "entities": [{"text": "Bloom filter; BF", "start_pos": 32, "end_pos": 48, "type": "METRIC", "confidence": 0.7198468744754791}, {"text": "SMT decoder", "start_pos": 271, "end_pos": 282, "type": "TASK", "confidence": 0.9155394434928894}]}, {"text": "The space requirements of a Bloom filter are quite spectacular, falling significantly below informationtheoretic error-free lower bounds while query times are constant.", "labels": [], "entities": []}, {"text": "This efficiency, however, comes at the price of false positives: the filter may erroneously report that an item not in the set is a member.", "labels": [], "entities": []}, {"text": "False negatives, on the other hand, will never occur: the error is said to be one-sided.", "labels": [], "entities": []}, {"text": "In this paper, we show that a Bloom filter can be used effectively for language modelling within an SMT decoder and present the log-frequency Bloom filter, an extension of the standard Boolean BF that takes advantage of the Zipf-like distribution of corpus statistics to allow frequency information to be associated with n-grams in the filter in a spaceefficient manner.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7157085537910461}, {"text": "SMT decoder", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.8521805703639984}]}, {"text": "We then propose a mechanism, sub-sequence filtering, for reducing the error rates of these models by using the fact that an n-gram's frequency is bound from above by the frequency of its least frequent sub-sequence.", "labels": [], "entities": [{"text": "sub-sequence filtering", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.7150978446006775}]}, {"text": "We present machine translation experiments using these models to represent information regarding higher-order n-grams and additional larger monolingual corpora in combination with conventional smoothed trigram models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7120973467826843}]}, {"text": "We also run experiments with these models in isolation to highlight the impact of different order n-grams on the translation process.", "labels": [], "entities": []}, {"text": "Finally we provide some empirical analysis of the effectiveness of both the log frequency Bloom filter and sub-sequence filtering.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted a range of experiments to explore the effectiveness and the error-space trade-off of Bloom filters for language modelling in SMT.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.7174124419689178}, {"text": "SMT", "start_pos": 138, "end_pos": 141, "type": "TASK", "confidence": 0.9136545658111572}]}, {"text": "The spaceefficiency of these models also allows us to investigate the impact of using much larger corpora and higher-order n-grams on translation quality.", "labels": [], "entities": []}, {"text": "While our main experiments use the Bloom filter models in conjunction with a conventional smoothed trigram model, we also present experiments with these models in isolation to highlight the impact of different order n-grams on the translation process.", "labels": [], "entities": []}, {"text": "Finally, we present some empirical analysis of both the logfrequency Bloom filter and the sub-sequence filtering technique which maybe of independent interest..", "labels": [], "entities": []}, {"text": "We holdout 500 test sentences and 250 development sentences from the parallel text for evaluation purposes.", "labels": [], "entities": []}, {"text": "The feature functions in our models are optimised using minimum error rate training and evaluation is performed using the BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.998404324054718}]}, {"text": "Adding 4-grams in the form of a Boolean BF or a log-frequency BF (see) improves on the 3-gram baseline with little additional memory (around 4MBs) while performing on a par with or above the Europarl 4-gram model with around 10MBs; this suggests that a lossy representation of the unpruned set of 4-grams contains more useful information than a lossless representation of the pruned set.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 191, "end_pos": 199, "type": "DATASET", "confidence": 0.9752094149589539}]}, {"text": "3 As the false positive rate exceeds 0.20 the performance is severly degraded.", "labels": [], "entities": [{"text": "false positive rate", "start_pos": 9, "end_pos": 28, "type": "METRIC", "confidence": 0.8697998722394308}]}, {"text": "Adding 3-grams drawn from the whole of the Gigaword corpus rather than simply the Agence France Press section results in slightly improved performance with signficantly less memory than the AFP-KN-3 model (see).", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.9464682638645172}, {"text": "Agence France Press section", "start_pos": 82, "end_pos": 109, "type": "DATASET", "confidence": 0.9629231542348862}]}, {"text": "shows the results of adding 5-grams drawn from the Gigaword corpus to the baseline.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.9525155127048492}]}, {"text": "It also contrasts the Boolean BF and the log-frequency BF suggesting in this case that the log-frequency BF can provide useful information when the quantisation base is relatively fine-grained (base 2).", "labels": [], "entities": [{"text": "Boolean BF", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.5957984030246735}, {"text": "BF", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.6394622325897217}]}, {"text": "The Boolean BF and the base 5 (coarse-grained quantisation) log-frequency BF perform approximately the same.", "labels": [], "entities": [{"text": "Boolean", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.7540087103843689}, {"text": "BF", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.47562292218208313}, {"text": "BF", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.6618167757987976}]}, {"text": "The base 2 quantisation performs worse for smaller amounts of memory, possibly due to the larger set of events it is required to store.", "labels": [], "entities": []}, {"text": "shows sub-sequence filtering resulting in a small increase in performance when false positive rates are high (i.e. less memory is allocated).", "labels": [], "entities": [{"text": "sub-sequence filtering", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.7039762735366821}]}, {"text": "We believe this to be the result of an increased a priori membership probability for n-grams presented to the filter under the sub-sequence filtering scheme.", "labels": [], "entities": []}, {"text": "shows that for this task the most useful n-gram sizes are between 3 and 6.", "labels": [], "entities": []}, {"text": "compares the memory requirements of the log-frequencey BF (base 2) and the Boolean 517 BF for various order n-gram sets from the Gigaword Corpus with the same underlying false positive rate (0.125).", "labels": [], "entities": [{"text": "Boolean 517 BF", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.9363981088002523}, {"text": "Gigaword Corpus", "start_pos": 129, "end_pos": 144, "type": "DATASET", "confidence": 0.9400914013385773}, {"text": "false positive rate", "start_pos": 170, "end_pos": 189, "type": "METRIC", "confidence": 0.7722894350687662}]}, {"text": "The additional space required by our scheme for storing frequency information is less than a factor of 2 compared to the standard BF.", "labels": [], "entities": []}, {"text": "shows the number and size of frequency estimation errors made by our log-frequency BF scheme and a non-redundant scheme that stores only the exact quantised count.", "labels": [], "entities": []}, {"text": "We presented 500K negatives to the filter and recorded the frequency of overestimation errors of each size.", "labels": [], "entities": []}, {"text": "As shown in Section 3.1, the probability of overestimating an item's frequency under the log-frequency BF scheme decays exponentially in the size of this overestimation error.", "labels": [], "entities": [{"text": "BF", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9401676058769226}]}, {"text": "Although the non-redundant scheme requires fewer items be stored in the filter and, therefore, has a lower underlying false positive rate (0.076 versus 0.159), in practice it incurs a much higher error rate (0.717) with many large errors.", "labels": [], "entities": [{"text": "false positive rate", "start_pos": 118, "end_pos": 137, "type": "METRIC", "confidence": 0.810237447420756}]}, {"text": "shows the impact of sub-sequence filtering on the actual error rate.", "labels": [], "entities": []}, {"text": "Although, the false positive rate for the BF containing 2-grams, in addition, to 3-grams (filtered) is higher than the false positive rate of the unfiltered BF containing only 3-grams, the actual error rate of the former is lower for models with less memory.", "labels": [], "entities": [{"text": "error", "start_pos": 196, "end_pos": 201, "type": "METRIC", "confidence": 0.9654766917228699}]}, {"text": "By testing for 2-grams prior to querying for the 3-grams, we can avoid performing some queries that may otherwise have incurred errors using the fact that a 3-gram cannot be present if one of its constituent 2-grams is absent.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Baseline and Comparison Models", "labels": [], "entities": []}]}