{"title": [{"text": "Different Structures for Evaluating Answers to Complex Questions: Pyramids Won't Topple, and Neither Will Human Assessors", "labels": [], "entities": [{"text": "Evaluating Answers to Complex Questions", "start_pos": 25, "end_pos": 64, "type": "TASK", "confidence": 0.8221535444259643}]}], "abstractContent": [{"text": "The idea of \"nugget pyramids\" has recently been introduced as a refinement to the nugget-based methodology used to evaluate answers to complex questions in the TREC QA tracks.", "labels": [], "entities": [{"text": "TREC QA tracks", "start_pos": 160, "end_pos": 174, "type": "DATASET", "confidence": 0.8147089083989462}]}, {"text": "This paper examines data from the 2006 evaluation, the first large-scale deployment of the nugget pyramids scheme.", "labels": [], "entities": []}, {"text": "We show that this method of combining judgments of nugget importance from multiple assessors increases the stability and dis-criminative power of the evaluation while introducing only a small additional burden in terms of manual assessment.", "labels": [], "entities": []}, {"text": "We also consider an alternative method for combining assessor opinions, which yields a distinction similar to micro-and macro-averaging in the context of classification tasks.", "labels": [], "entities": []}, {"text": "While the two approaches differ in terms of underlying assumptions, their results are nevertheless highly correlated.", "labels": [], "entities": []}], "introductionContent": [{"text": "The emergence of question answering (QA) systems for addressing complex information needs has necessitated the development and refinement of new methodologies for evaluating and comparing systems.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.8507578492164611}]}, {"text": "In the Text REtrieval Conference (TREC) QA tracks organized by the U.S. National Institute of Standards and Technology (NIST), improvements in evaluation processes have kept pace with the evolution of QA tasks.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC) QA", "start_pos": 7, "end_pos": 42, "type": "TASK", "confidence": 0.8514922048364367}]}, {"text": "For the past several years, NIST has implemented an evaluation methodology based on the notion of \"information nuggets\" to assess answers to complex questions.", "labels": [], "entities": [{"text": "NIST", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.8843560814857483}]}, {"text": "As it has become the de facto standard for evaluating such systems, the research community stands to benefit from a better understanding of the characteristics of this evaluation methodology.", "labels": [], "entities": []}, {"text": "This paper explores recent refinements to the nugget-based evaluation methodology developed by NIST.", "labels": [], "entities": [{"text": "NIST", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.9014872312545776}]}, {"text": "In particular, we examine the recent so-called \"pyramid extension\" that incorporates relevance judgments from multiple assessors to improve evaluation stability).", "labels": [], "entities": []}, {"text": "We organize our discussion as follows: The next section begins by providing a brief overview of nugget-based evaluations and the pyramid extension.", "labels": [], "entities": []}, {"text": "Section 3 presents results from the first largescale implementation of nugget pyramids for QA evaluation in TREC 2006.", "labels": [], "entities": [{"text": "QA evaluation in TREC 2006", "start_pos": 91, "end_pos": 117, "type": "TASK", "confidence": 0.6679443120956421}]}, {"text": "Analysis shows that this extension improves both stability and discriminative power.", "labels": [], "entities": []}, {"text": "In Section 4, we discuss an alternative for combining multiple judgments that parallels the distinction between micro-and macro-averaging often seen in classification tasks.", "labels": [], "entities": []}, {"text": "Experiments reveal that the methods yield almost exactly the same results, despite operating on different granularities (individual nuggets vs. individual users).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Pearson's correlation of F-scores, by ques- tion and by run.", "labels": [], "entities": [{"text": "Pearson's correlation of F-scores", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.5932436347007751}, {"text": "ques- tion", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9345691601435343}]}]}