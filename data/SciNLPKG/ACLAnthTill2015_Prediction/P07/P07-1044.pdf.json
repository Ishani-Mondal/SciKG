{"title": [{"text": "GLEU: Automatic Evaluation of Sentence-Level Fluency", "labels": [], "entities": [{"text": "GLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9224771857261658}]}], "abstractContent": [{"text": "In evaluating the output of language technology applications-MT, natural language generation, summarisation-automatic evaluation techniques generally conflate measurement of faithfulness to source content with fluency of the resulting text.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 65, "end_pos": 92, "type": "TASK", "confidence": 0.6956035296122233}, {"text": "summarisation-automatic evaluation", "start_pos": 94, "end_pos": 128, "type": "TASK", "confidence": 0.9470601081848145}]}, {"text": "In this paper we develop an automatic evaluation metric to estimate fluency alone, by examining the use of parser outputs as metrics, and show that they correlate with human judgements of generated text fluency.", "labels": [], "entities": []}, {"text": "We then develop a machine learner based on these, and show that this performs better than the individual parser metrics, approaching a lower bound on human performance.", "labels": [], "entities": []}, {"text": "We finally look at different language models for generating sentences, and show that while individual parser metrics can be 'fooled' depending on generation method, the machine learner provides a consistent estimator of fluency.", "labels": [], "entities": []}], "introductionContent": [{"text": "Intrinsic evaluation of the output of many language technologies can be characterised as having at least two aspects: how well the generated text reflects the source data, whether it be text in another language for machine translation (MT), a natural language generation (NLG) input representation, a document to be summarised, and soon; and how well it conforms to normal human language usage.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 215, "end_pos": 239, "type": "TASK", "confidence": 0.8681810617446899}]}, {"text": "These two aspects are often made explicit in approaches to creating the text.", "labels": [], "entities": []}, {"text": "For example, in statistical MT the translation model and the language model are treated separately, characterised as faithfulness and fluency respectively (as in the treatment in).", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9033390879631042}, {"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9479949474334717}]}, {"text": "Similarly, the ultrasummarisation model of consists of a content model, modelling the probability that a word in the source text will be in the summary, and a language model.", "labels": [], "entities": []}, {"text": "Evaluation methods can be said to fall into two categories: a comparison to gold reference, or an appeal to human judgements.", "labels": [], "entities": []}, {"text": "Automatic evaluation methods carrying out a comparison to gold reference tend to conflate the two aspects of faithfulness and fluency in giving a goodness score for generated output.", "labels": [], "entities": []}, {"text": "BLEU () is a canonical example: in matching n-grams in a candidate translation text with those in a reference text, the metric measures faithfulness by counting the matches, and fluency by implicitly using the reference n-grams as a language model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9777839779853821}]}, {"text": "Often we are interested in knowing the quality of the two aspects separately; many human judgement frameworks ask specifically for separate judgements on elements of the task that correspond to faithfulness and to fluency.", "labels": [], "entities": []}, {"text": "In addition, the need for reference texts for an evaluation metric can be problematic, and intuitively seems unnecessary for characterising an aspect of text quality that is not related to its content source but to the use of language itself.", "labels": [], "entities": []}, {"text": "It is a goal of this paper to provide an automatic evaluation method for fluency alone, without the use of a reference text.", "labels": [], "entities": []}, {"text": "One might consider using a metric based on language model probabilities for sentences: in eval-uating a language model on (already existing) test data, a higher probability fora sentence (and lower perplexity over a whole test corpus) indicates better language modelling; perhaps a higher probability might indicate a better sentence.", "labels": [], "entities": []}, {"text": "However, here we are looking at generated sentences, which have been generated using their own language model, rather than human-authored sentences already existing in a test corpus; and so it is not obvious what language model would bean objective assessment of sentence naturalness.", "labels": [], "entities": []}, {"text": "In the case of evaluating a single system, using the language model that generated the sentence will only confirm that the sentence does fit the language model; in situations such as comparing two systems which each generate text using a different language model, it is not obvious that there is a principled way of deciding on a fair language model.", "labels": [], "entities": []}, {"text": "Quite a different idea was suggested in, of using the grammatical judgement of a parser to assess fluency, giving a measure independent of the language model used to generate the text.", "labels": [], "entities": []}, {"text": "The idea is that, assuming the parser has been trained on an appropriate corpus, the poor performance of the parser on one sentence relative to another might bean indicator of some degree of ungrammaticality and possibly disfluency.", "labels": [], "entities": []}, {"text": "In that work, however, correlation with human judgements was left uninvestigated.", "labels": [], "entities": [{"text": "correlation with human judgements", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.7253174930810928}]}, {"text": "The goal of this paper is to take this idea and develop it.", "labels": [], "entities": []}, {"text": "In Section 2 we look at some related work on metrics, in particular for NLG.", "labels": [], "entities": [{"text": "NLG", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.8803044557571411}]}, {"text": "In Section 3, we verify whether parser outputs can be used as estimators of generated sentence fluency by correlating them with human judgements.", "labels": [], "entities": []}, {"text": "In Section 4, we propose an SVM-based metric using parser outputs as features, and compare its correlation against human judgements with that of the individual parsers.", "labels": [], "entities": []}, {"text": "In Section 5, we investigate the effects on the various metrics from different types of language model for the generated text.", "labels": [], "entities": []}, {"text": "Then in Section 6 we conclude.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Parser vs sequence size for original data set", "labels": [], "entities": [{"text": "Parser", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.861525297164917}]}, {"text": " Table 3: Data on correlation between humans", "labels": [], "entities": [{"text": "correlation", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9331510663032532}]}, {"text": " Table 4: Correlation with sequence size for human  trial data set", "labels": [], "entities": []}, {"text": " Table 6: Mean normalised scores per sentence type", "labels": [], "entities": [{"text": "Mean normalised scores", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.921239991982778}]}, {"text": " Table 8. This confirms the very vari- able performance of the Collins parser, which has  dropped significantly. GLEU performs quite consis- tently here, this time a little behind the Link Gram- mar (nulled tokens) result, but still with a better  correlation with human judgement than at least two", "labels": [], "entities": [{"text": "Collins", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9544950127601624}, {"text": "GLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.8730876445770264}]}, {"text": " Table 7: Data on correlation between humans", "labels": [], "entities": [{"text": "correlation", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9222269058227539}]}, {"text": " Table 8: Correlation between parsers and human  evaluators on new human trial data", "labels": [], "entities": []}]}