{"title": [{"text": "Predicting Evidence of Understanding by Monitoring User's Task Manipulation in Multimodal Conversations", "labels": [], "entities": [{"text": "Predicting Evidence of Understanding", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8603544235229492}]}], "abstractContent": [{"text": "The aim of this paper is to develop animated agents that can control multimodal instruction dialogues by monitoring user's behaviors.", "labels": [], "entities": []}, {"text": "First, this paper reports on our Wizard-of-Oz experiments, and then, using the collected corpus, proposes a probabilis-tic model of fine-grained timing dependencies among multimodal communication behaviors: speech, gestures, and mouse manipulations.", "labels": [], "entities": []}, {"text": "A preliminary evaluation revealed that our model can predict a in-structor's grounding judgment and a lis-tener's successful mouse manipulation quite accurately, suggesting that the model is useful in estimating the user's understanding , and can be applied to determining the agent's next action.", "labels": [], "entities": []}], "introductionContent": [{"text": "In face-to-face conversation, speakers adjust their utterances in progress according to the listener's feedback expressed in multimodal manners, such as speech, facial expression, and eye-gaze.", "labels": [], "entities": []}, {"text": "In taskmanipulation situations where the listener manipulates objects by following the speaker's instructions, correct task manipulation by the listener serves as more direct evidence of understanding (, and affects the speaker's dialogue control strategies.", "labels": [], "entities": []}, {"text": "shows an example of a software instruction dialogue in a video-mediated situation (originally in Japanese).", "labels": [], "entities": []}, {"text": "While the learner says nothing, the instructor gives the instruction in small pieces, simultaneously modifying her gestures and utterances according to the learner's mouse movements.", "labels": [], "entities": []}, {"text": "To accomplish such interaction between human users and animated help agents, and to assist the user through natural conversational interaction, this paper proposes a probabilistic model that computes timing dependencies among different types of behaviors in different modalities: speech, gestures, and mouse events.", "labels": [], "entities": []}, {"text": "The model predicts (a) whether the instructor's current utterance will be successfully understood by the learner and grounded, and (b) whether the learner will successfully manipulate the object in the near future.", "labels": [], "entities": []}, {"text": "These predictions can be used as constraints in determining agent actions.", "labels": [], "entities": []}, {"text": "For example, if the current utterance will not be grounded, then the help agent must add more information.", "labels": [], "entities": []}, {"text": "In the following sections, first, we collect human-agent conversations by employing a Wizardof-Oz method, and annotate verbal and nonverbal behaviors.", "labels": [], "entities": []}, {"text": "The annotated corpus is used to build a Bayesian network model for the multimodal instruction dialogues.", "labels": [], "entities": []}, {"text": "Finally, we will evaluate how Instructor: Learner: \"at the most\" (395ms pause) \"left-hand side\" Instructor: Learner: Instructor: Mouse move \"That\" (204ms pause) Pointing gesture <preparation> <stroke> Mouse move Instructor: Learner: \"at the most\" (395ms pause) \"left-hand side\" Instructor: Learner: Instructor: Mouse move Figure 1: Example of task manipulation dialogue accurately the model can predict the events in (a) and (b) mentioned above.", "labels": [], "entities": [{"text": "Instructor", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.961432158946991}]}], "datasetContent": [], "tableCaptions": []}