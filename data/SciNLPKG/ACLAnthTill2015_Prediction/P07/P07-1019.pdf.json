{"title": [{"text": "Forest Rescoring: Faster Decoding with Integrated Language Models *", "labels": [], "entities": [{"text": "Forest Rescoring", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6547176837921143}]}], "abstractContent": [{"text": "Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7904865741729736}]}, {"text": "We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 155, "end_pos": 157, "type": "TASK", "confidence": 0.9100272059440613}]}, {"text": "In both cases, our methods achieve significant speed improvements, often by more than a factor often, over the conventional beam-search method at the same levels of search error and translation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.8240553140640259}]}], "introductionContent": [{"text": "Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality, especially the phrase-based models () and syntax-based models).", "labels": [], "entities": [{"text": "statistical machine translation (MT)", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.8098116268714269}]}, {"text": "However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem.", "labels": [], "entities": []}, {"text": "Part of the complexity arises from the expressive power of the translation model: for example, a phrase-or word-based model with full reordering has exponential complexity).", "labels": [], "entities": []}, {"text": "The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming).", "labels": [], "entities": []}, {"text": "In practice, one must prune the search space aggressively to reduce it to a reasonable size.", "labels": [], "entities": []}, {"text": "A much simpler alternative method to incorporate the LM is rescoring: we first decode without the LM (henceforth \u2212LM decoding) to produce a k-best list of candidate translations, and then rerank the k-best list using the LM.", "labels": [], "entities": []}, {"text": "This method runs much faster in practice but often produces a considerable number of search errors since the true best translation (taking LM into account) is often outside of the k-best list.", "labels": [], "entities": []}, {"text": "Cube pruning) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring.", "labels": [], "entities": []}, {"text": "By adapting the k-best parsing Algorithm 2 of Huang and, it achieves significant speed-up over full-integration on Chiang's Hiero system.", "labels": [], "entities": [{"text": "Chiang's Hiero system", "start_pos": 115, "end_pos": 136, "type": "DATASET", "confidence": 0.6966076642274857}]}, {"text": "We push the idea behind this method further and make the following contributions in this paper: \u2022 We generalize cube pruning and adapt it to two systems very different from Hiero: a phrasebased system similar to Pharaoh) and a tree-to-string system ().", "labels": [], "entities": []}, {"text": "\u2022 We also devise a faster variant of cube pruning, called cube growing, which uses a lazy version of k-best parsing (Huang and) that tries to reduce k to the minimum needed at each node to obtain the desired number of hypotheses at the root.", "labels": [], "entities": [{"text": "cube growing", "start_pos": 58, "end_pos": 70, "type": "TASK", "confidence": 0.739106073975563}]}, {"text": "Cube pruning and cube growing are collectively called forest rescoring since they both approximately rescore the packed forest of derivations from \u2212LM decoding.", "labels": [], "entities": [{"text": "cube growing", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.7684354782104492}]}, {"text": "In practice they run an order ofmagnitude faster than full-integration with beam search, at the same level of search errors and translation accuracy as measured by BLEU.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.7926962971687317}, {"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9726375937461853}]}], "datasetContent": [{"text": "We test our methods on two large-scale English-toChinese translation systems: a phrase-based system and our tree-to-string system).", "labels": [], "entities": []}, {"text": "w it h S ha r on an d S ha r on w it h Ar i e l S ha r on ...: A hyperedge bundle represents all +LM deductions that derives an item in the current bin from the same coverage vector (see).", "labels": [], "entities": [{"text": "Ar", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9534910321235657}]}, {"text": "The phrases on the top denote the target-sides of applicable phrase-pairs sharing the same source-side.", "labels": [], "entities": []}], "tableCaptions": []}