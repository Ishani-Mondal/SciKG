{"title": [], "abstractContent": [{"text": "This paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text.", "labels": [], "entities": []}, {"text": "The parser uses a representation for syntactic structure similar to dependency links which is well-suited for incremental parsing.", "labels": [], "entities": []}, {"text": "In contrast to previous unsupervised parsers, the parser does not use part-of-speech tags and both learning and parsing are local and fast, requiring no explicit clustering or global optimization.", "labels": [], "entities": []}, {"text": "The parser is evaluated by converting its output into equivalent bracketing and improves on previously published results for unsupervised parsing from plain text.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammar induction, the learning of the grammar of a language from unannotated example sentences, has long been of interest to linguists because of its relevance to language acquisition by children.", "labels": [], "entities": [{"text": "Grammar induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8025084137916565}]}, {"text": "In recent years, interest in unsupervised learning of grammar has also increased among computational linguists, as the difficulty and cost of constructing annotated corpora led researchers to look for ways to train parsers on unannotated text.", "labels": [], "entities": []}, {"text": "This can either be semi-supervised parsing, using both annotated and unannotated data () or unsupervised parsing, training entirely on unannotated text.", "labels": [], "entities": []}, {"text": "The past few years have seen considerable improvement in the performance of unsupervised parsers () and, for the first time, unsupervised parsers have been able to improve on the right-branching heuristic for parsing English.", "labels": [], "entities": [{"text": "parsing English", "start_pos": 209, "end_pos": 224, "type": "TASK", "confidence": 0.8840334117412567}]}, {"text": "All these parsers learn and parse from sequences of part-of-speech tags and select, for each sentence, the binary parse tree which maximizes some objective function.", "labels": [], "entities": []}, {"text": "Learning is based on global maximization of this objective function over the whole corpus.", "labels": [], "entities": []}, {"text": "In this paper I present an unsupervised parser from plain text which does not use parts-of-speech.", "labels": [], "entities": []}, {"text": "Learning is local and parsing is (locally) greedy.", "labels": [], "entities": []}, {"text": "As a result, both learning and parsing are fast.", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9811192750930786}]}, {"text": "The parser is incremental, using anew link representation for syntactic structure.", "labels": [], "entities": []}, {"text": "Incremental parsing was chosen because it considerably restricts the search space for both learning and parsing.", "labels": [], "entities": [{"text": "Incremental parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8108154833316803}]}, {"text": "The representation the parser uses is designed for incremental parsing and allows a prefix of an utterance to be parsed before the full utterance has been read (see section 3).", "labels": [], "entities": []}, {"text": "The representation the parser outputs can be converted into bracketing, thus allowing evaluation of the parser on standard treebanks.", "labels": [], "entities": []}, {"text": "To achieve completely unsupervised parsing, standard unsupervised parsers, working from partof-speech sequences, need first to induce the partsof-speech for the plain text they need to parse.", "labels": [], "entities": []}, {"text": "There are several algorithms for doing so), which cluster words into classes based on the most frequent neighbors of each word.", "labels": [], "entities": []}, {"text": "This step becomes superfluous in the algorithm I present here: the algorithm collects lists of labels for each word, based on neighboring words, and then directly 384 uses these labels to parse.", "labels": [], "entities": []}, {"text": "No clustering is performed, but due to the Zipfian distribution of words, high frequency words dominate these lists and parsing decisions for words of similar distribution are guided by the same labels.", "labels": [], "entities": []}, {"text": "Section 2 describes the syntactic representation used, section 3 describes the general parser algorithm and sections 4 and 5 complete the details by describing the learning algorithm, the lexicon it constructs and the way the parser uses this lexicon.", "labels": [], "entities": []}, {"text": "Section 6 gives experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The incremental parser was tested on the Wall Street Journal and Negra Corpora.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.9642754197120667}, {"text": "Negra Corpora", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.8820191621780396}]}, {"text": "Parsing accuracy was evaluated on the subsets WSJX and NegraX of these corpora containing sentences of length at most X (excluding punctuation).", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9162157773971558}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9031927585601807}, {"text": "WSJX", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9149694442749023}]}, {"text": "Some of these subsets were used for scoring in ().", "labels": [], "entities": []}, {"text": "I also use the same precision and recall measures used in those papers: multiple brackets and brackets covering a single word were not counted, but the top bracket was.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9991724491119385}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9976800084114075}]}, {"text": "The incremental parser learns while parsing, and it could, in principle, simply be evaluated fora single pass of the data.", "labels": [], "entities": []}, {"text": "But, because the quality of the parses of the first sentences would below, I first trained on the full corpus and then measured parsing accuracy on the corpus subset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.979744553565979}]}, {"text": "By training on the full corpus, the procedure differs from that of Klein, Manning and Bod who only train on the subset of bounded length sentences.", "labels": [], "entities": []}, {"text": "However, this excludes the induction of parts-of-speech for parsing from plain text.", "labels": [], "entities": []}, {"text": "When Klein and Manning induce the parts-of-speech, they do so from a much larger corpus containing the full WSJ treebank together with additional WSJ newswire ().", "labels": [], "entities": [{"text": "WSJ treebank", "start_pos": 108, "end_pos": 120, "type": "DATASET", "confidence": 0.9839881062507629}, {"text": "WSJ newswire", "start_pos": 146, "end_pos": 158, "type": "DATASET", "confidence": 0.9681310057640076}]}, {"text": "The comparison between the algorithms remains, therefore, valid.", "labels": [], "entities": []}, {"text": "gives two baselines and the parsing results for WSJ10, WSJ40, Negra10 and Negra40 for recent unsupervised parsing algorithms: CCM and DMV+CCM (), U-DOP (Bod, 2006b) and UML-DOP.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.9528027772903442}, {"text": "WSJ40", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.9343695044517517}, {"text": "Negra10", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.8606842160224915}, {"text": "Negra40", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9027025699615479}, {"text": "Bod, 2006b)", "start_pos": 153, "end_pos": 164, "type": "DATASET", "confidence": 0.916855126619339}]}, {"text": "The middle part of the table gives results for parsing from part-of-speech sequences extracted from the treebank while the bottom part of the table given results for parsing from plain text.", "labels": [], "entities": [{"text": "parsing from part-of-speech sequences extracted", "start_pos": 47, "end_pos": 94, "type": "TASK", "confidence": 0.823207414150238}, {"text": "parsing from plain text", "start_pos": 166, "end_pos": 189, "type": "TASK", "confidence": 0.7892578393220901}]}, {"text": "Results for the incremental parser are given for learning and parsing from left to right and from right to left.", "labels": [], "entities": []}, {"text": "The first baseline is the standard right-branching baseline.", "labels": [], "entities": []}, {"text": "The second baseline modifies rightbranching by using punctuation in the same way as the incremental parser: brackets (except the top one) are not allowed to contain stopping punctuation.", "labels": [], "entities": []}, {"text": "It can be seen that punctuation accounts for merely a small part of the incremental parser's improvement over the right-branching heuristic.", "labels": [], "entities": []}, {"text": "Comparing the two algorithms parsing from plain text (of WSJ10), it can be seen that the incremental parser has a somewhat higher combined F 1 score, with better precision but worse recall.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.9273105263710022}, {"text": "F 1 score", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9893358945846558}, {"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9985530972480774}, {"text": "recall", "start_pos": 182, "end_pos": 188, "type": "METRIC", "confidence": 0.9982579350471497}]}, {"text": "This is because Klein and Manning's algorithms (as well as Bod's) always generate binary parse trees, while here no such condition is imposed.", "labels": [], "entities": []}, {"text": "The small difference between the recall (76.2) and precision (75.6) of the incremental parser shows that the number of brackets induced by the parser is very close to that of the corpus and that the parser captures the same depth of syntactic structure as that which was used by the corpus annotators.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9991210103034973}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9994988441467285}]}, {"text": "Incremental parsing from right to left achieves results close to those of parsing from left to right.", "labels": [], "entities": [{"text": "Incremental parsing from right to left", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7526667565107346}, {"text": "parsing from left to right", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.8456401824951172}]}, {"text": "This shows that the incremental parser has no built-in bias for right branching structures.", "labels": [], "entities": []}, {"text": "The slight degradation in performance may suggest that language should not, after all, be processed backwards.", "labels": [], "entities": []}, {"text": "While achieving state of the art accuracy, the algorithm also proved to be fast, parsing (on a 1.86GHz Centrino laptop) at a rate of around 4000 words/sec. and learning (including parsing) at a rate of 3200 -3600 words/sec.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9986909031867981}, {"text": "parsing", "start_pos": 81, "end_pos": 88, "type": "TASK", "confidence": 0.9728164672851562}]}, {"text": "The effect of sentence length on parsing speed is small: the full WSJ corpus was parsed at 3900 words/sec.", "labels": [], "entities": [{"text": "parsing", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9727118015289307}, {"text": "WSJ corpus", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.9225290417671204}]}, {"text": "while WSJ10 was parsed at 4300 words/sec.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.9298562407493591}]}, {"text": "The algorithm produced 35588 brackets compared with 35302 brackets in the corpus.", "labels": [], "entities": []}, {"text": "I would like to thank Alexander Clark for suggesting this test.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parsing results on WSJ10, WSJ40, Negra10 and Negra40.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.9713423848152161}, {"text": "WSJ40", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.9395826458930969}, {"text": "Negra10", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9134233593940735}, {"text": "Negra40", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.9421147108078003}]}]}