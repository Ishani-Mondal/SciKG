{"title": [{"text": "An Ensemble Method for Selection of High Quality Parses", "labels": [], "entities": []}], "abstractContent": [{"text": "While the average performance of statistical parsers gradually improves, they still attach to many sentences annotations of rather low quality.", "labels": [], "entities": []}, {"text": "The number of such sentences grows when the training and test data are taken from different domains, which is the case for major web applications such as information retrieval and question answering.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.7837589979171753}, {"text": "question answering", "start_pos": 180, "end_pos": 198, "type": "TASK", "confidence": 0.8996375203132629}]}, {"text": "In this paper we present a Sample Ensemble Parse Assessment (SEPA) algorithm for detecting parse quality.", "labels": [], "entities": [{"text": "Sample Ensemble Parse Assessment (SEPA)", "start_pos": 27, "end_pos": 66, "type": "TASK", "confidence": 0.5668682030269078}, {"text": "detecting parse quality", "start_pos": 81, "end_pos": 104, "type": "TASK", "confidence": 0.7508326768875122}]}, {"text": "We use a function of the agreement among several copies of a parser, each of which trained on a different sample from the training data, to assess parse quality.", "labels": [], "entities": []}, {"text": "We experimented with both generative and reranking parsers (Collins, Charniak and Johnson respectively).", "labels": [], "entities": [{"text": "generative", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.963696300983429}]}, {"text": "We show superior results over several baselines, both when the training and test data are from the same domain and when they are from different domains.", "labels": [], "entities": []}, {"text": "For a test setting used by previous work, we show an error reduction of 31% as opposed to their 20%.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.9872690737247467}]}], "introductionContent": [{"text": "Many algorithms for major NLP applications such as information extraction (IE) and question answering (QA) utilize the output of statistical parsers (see).", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.8536828637123108}, {"text": "question answering (QA)", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.8566531538963318}]}, {"text": "While the average performance of statistical parsers gradually improves, the quality of many of the parses they produce is too low for applications.", "labels": [], "entities": []}, {"text": "When the training and test data are taken from different domains (the parser adaptation scenario) the ratio of such low quality parses becomes even higher.", "labels": [], "entities": [{"text": "parser adaptation", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.9282113909721375}]}, {"text": "demonstrates these phenomena for two leading models, Collins (1999) model 2, a generative model, and, a reranking model.", "labels": [], "entities": []}, {"text": "The parser adaptation scenario is the rule rather than the exception for QA and IE systems, because these usually operate over the highly variable Web, making it very difficult to create a representative corpus for manual annotation.", "labels": [], "entities": [{"text": "parser adaptation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8467922508716583}]}, {"text": "Medium quality parses may seriously harm the performance of such systems.", "labels": [], "entities": []}, {"text": "In this paper we address the problem of assessing parse quality, using a Sample Ensemble Parse Assessment (SEPA) algorithm.", "labels": [], "entities": [{"text": "parse quality", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.6806100308895111}, {"text": "Sample Ensemble Parse Assessment (SEPA)", "start_pos": 73, "end_pos": 112, "type": "METRIC", "confidence": 0.6390377921717507}]}, {"text": "We use the level of agreement among several copies of a parser, each of which trained on a different sample from the training data, to predict the quality of a parse.", "labels": [], "entities": []}, {"text": "The algorithm does not assume uniformity of training and test data, and is thus suitable to web-based applications such as QA and IE.", "labels": [], "entities": []}, {"text": "Generative statistical parsers compute a probability p(a, s) for each sentence annotation, so the immediate technique that comes to mind for assessing parse quality is to simply use p(a, s).", "labels": [], "entities": []}, {"text": "Another seemingly trivial method is to assume that shorter sentences would be parsed better than longer ones.", "labels": [], "entities": []}, {"text": "However, these techniques produce results that are far from optimal.", "labels": [], "entities": []}, {"text": "In Section 5 we show the superiority of our method over these and other baselines.", "labels": [], "entities": []}, {"text": "Surprisingly, as far as we know there is only one previous work explicitly addressing this problem ( Figure 1: F-score vs. the fraction of parses whose f-score is at least that f-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9870447516441345}]}, {"text": "For the in-domain scenario, the parsers are tested on sec 23 of the WSJ Penn Treebank.", "labels": [], "entities": [{"text": "WSJ Penn Treebank", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.9039039413134257}]}, {"text": "For the parser adaptation scenario, they are tested on the Brown test section.", "labels": [], "entities": [{"text": "parser adaptation", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.9190761148929596}, {"text": "Brown test section", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.7899182736873627}]}, {"text": "In both cases they are trained on sections 2-21 of WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.9434701204299927}]}, {"text": "The present paper provides a detailed comparison between the two algorithms, showing both that SEPA produces superior results and that it operates under less restrictive conditions.", "labels": [], "entities": [{"text": "SEPA", "start_pos": 95, "end_pos": 99, "type": "TASK", "confidence": 0.8519578576087952}]}, {"text": "We experiment with both the generative parsing model number 2 of Collins (1999) and the reranking parser of, both when the training and test data belong to the same domain (the in-domain scenario) and in the parser adaptation scenario.", "labels": [], "entities": [{"text": "generative parsing", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.9306150078773499}]}, {"text": "In all four cases, we show substantial improvement over the baselines.", "labels": [], "entities": []}, {"text": "The present paper is the first to use a reranking parser and the first to address the adaptation scenario for this problem.", "labels": [], "entities": []}, {"text": "Section 2 discusses relevant previous work, Section 3 describes the SEPA algorithm, Sections 4 and 5 present the experimental setup and results, and Section 6 discusses certain aspects of these results and compares SEPA to WOODWARD.", "labels": [], "entities": [{"text": "SEPA", "start_pos": 68, "end_pos": 72, "type": "TASK", "confidence": 0.7286073565483093}, {"text": "WOODWARD", "start_pos": 223, "end_pos": 231, "type": "DATASET", "confidence": 0.8451400399208069}]}], "datasetContent": [{"text": "We performed experiments with two parsing models, the Collins (1999) generative model number 2 and the Charniak and Johnson, the Brown test and development sections consist of 10% of Brown sentences (the 9th and 10th of each 10 consecutive sentences in the development and test sections respectively).", "labels": [], "entities": []}, {"text": "We performed experiments with many configurations of the parameters N (number of models), S (sample size) and F (agreement function).", "labels": [], "entities": [{"text": "F (agreement function)", "start_pos": 110, "end_pos": 132, "type": "METRIC", "confidence": 0.8514020085334778}]}, {"text": "Due to space limitations we describe only experiments where the values of the parameters N, Sand F are fixed (F is M F , N and S are given in Section 5) and the threshold parameter T is changed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Error reduction compared to the MR base- line, measured by filter f-score with parameter 100.  The data is the WSJ sec 23 test set usd by (Yates  et al., 2006). All three methods use Collins' model.  SEPA uses N = 20, S = 33, 000, T = 100.", "labels": [], "entities": [{"text": "Error reduction", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9221970438957214}, {"text": "MR base- line", "start_pos": 42, "end_pos": 55, "type": "METRIC", "confidence": 0.6447956413030624}, {"text": "WSJ sec 23 test set", "start_pos": 121, "end_pos": 140, "type": "DATASET", "confidence": 0.7661471962928772}, {"text": "T", "start_pos": 241, "end_pos": 242, "type": "METRIC", "confidence": 0.9852477312088013}]}, {"text": " Table 4: SEPA error reduction vs. the CB base- line in the in-domain and adaptation scenarios, us- ing the traditional f-score of the parsing literature.  N = 20, S = 13, 000, T = 100.", "labels": [], "entities": [{"text": "SEPA error reduction", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.675219734509786}, {"text": "T", "start_pos": 177, "end_pos": 178, "type": "METRIC", "confidence": 0.9754942655563354}]}]}