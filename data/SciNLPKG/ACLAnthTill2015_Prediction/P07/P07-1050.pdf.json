{"title": [], "abstractContent": [{"text": "This paper introduces a Maximum Entropy dependency parser based on an efficient k-best Maximum Spanning Tree (MST) algorithm.", "labels": [], "entities": [{"text": "Maximum Entropy dependency parser", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.5864664390683174}]}, {"text": "Although recent work suggests that the edge-factored constraints of the MST algorithm significantly inhibit parsing accuracy , we show that generating the 50-best parses according to an edge-factored model has an oracle performance well above the 1-best performance of the best dependency parsers.", "labels": [], "entities": [{"text": "MST algorithm", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.8851217031478882}, {"text": "parsing", "start_pos": 108, "end_pos": 115, "type": "TASK", "confidence": 0.974536120891571}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.8200600147247314}]}, {"text": "This motivates our parsing approach , which is based on reranking the k-best parses generated by an edge-factored model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.9748543500900269}]}, {"text": "Oracle parse accuracy results are presented for the edge-factored model and 1-best results for the reranker on eight languages (seven from CoNLL-X and English).", "labels": [], "entities": [{"text": "Oracle parse", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.5235458612442017}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.7961947321891785}, {"text": "CoNLL-X", "start_pos": 139, "end_pos": 146, "type": "DATASET", "confidence": 0.9019196629524231}]}], "introductionContent": [{"text": "The Maximum Spanning Tree algorithm 1 was recently introduced as a viable solution for nonprojective dependency parsing ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7104810774326324}]}, {"text": "The dependency parsing problem is naturally a spanning tree problem; however, efficient spanning-tree optimization algorithms assume a cost function which assigns scores independently to edges of the graph.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8083339333534241}]}, {"text": "In dependency parsing, this effectively constrains the set of models to those which independently generate parent-child pairs; these are known as edge-factored models.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8138073682785034}]}, {"text": "These models are limited to relatively simple features which exclude linguistic constructs such as verb sub-categorization/valency, lexical selectional preferences, etc.", "labels": [], "entities": []}, {"text": "In order to explore a rich set of syntactic features in the MST framework, we can either approximate the optimal non-projective solution as in, or we can use the constrained MST model to select a subset of the set of dependency parses to which we then apply lessconstrained models.", "labels": [], "entities": []}, {"text": "An efficient algorithm for generating the k-best parse trees fora constituencybased parser was presented in; a variation of that algorithm was used for generating projective dependency trees for parsing in and for training in.", "labels": [], "entities": []}, {"text": "However, prior to this paper, an efficient non-projective k-best MST dependency parser has not been proposed.", "labels": [], "entities": [{"text": "MST dependency parser", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.8853646914164225}]}, {"text": "In this paper we show that the na\u00a8\u0131vena\u00a8\u0131ve edge-factored models are effective at selecting sets of parses on which the oracle parse accuracy is high.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.8976224660873413}]}, {"text": "The oracle parse accuracy fora set of parse trees is the highest accuracy for any individual tree in the set.", "labels": [], "entities": [{"text": "oracle parse", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.721613347530365}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.6135576963424683}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9956561326980591}]}, {"text": "We show that the 1-best accuracy and oracle accuracy can differ by as much as an absolute 9% when the oracle is computed over a small set generated by edge-factored models (k = 50).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9368966221809387}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9829163551330566}]}, {"text": "Figure 1: A dependency graph for an English sentence in our development set (Penn WSJ section 24): Two share a house almost devoid of furniture.", "labels": [], "entities": [{"text": "Penn WSJ section", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.9627817471822103}]}, {"text": "The combination of two discriminatively trained models, a k-best MST parser and a parse tree reranker, results in an efficient parser that includes complex tree-based features.", "labels": [], "entities": [{"text": "MST parser", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.7506509125232697}]}, {"text": "In the remainder of the paper, we first describe the core of our parser, the k-best MST algorithm.", "labels": [], "entities": []}, {"text": "We then introduce the features that we use to compute edge-factored scores as well as tree-based scores.", "labels": [], "entities": []}, {"text": "Following, we outline the technical details of our training procedure and finally we present empirical results for the parser on seven languages from the CoNLL-X shared-task and a dependency version of the WSJ Penn Treebank.", "labels": [], "entities": [{"text": "WSJ Penn Treebank", "start_pos": 206, "end_pos": 223, "type": "DATASET", "confidence": 0.9081022342046102}]}], "datasetContent": [{"text": "The CoNLL-X shared task on dependency parsing provided data fora number of languages in a common data format.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7802774310112}]}, {"text": "We have selected seven of these languages for which the data is available to us.", "labels": [], "entities": []}, {"text": "Additionally, we have automatically generated a dependency version of the Penn WSJ treebank.", "labels": [], "entities": [{"text": "Penn WSJ treebank", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.9492008686065674}]}, {"text": "As we are only interested in the structural component of a parse in this paper, we present results for unlabeled dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.6595371216535568}]}, {"text": "A second labeling stage can be applied to get labeled dependency structures as described in ).", "labels": [], "entities": []}, {"text": "In we report the accuracy for seven of the CoNLL languages and English.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9996150732040405}, {"text": "CoNLL languages", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.9238698482513428}]}, {"text": "11 Already, at k = 50, we seethe oracle rate climb as much as 9.25% over the 1-best result (Dutch).", "labels": [], "entities": []}, {"text": "Continuing to increase the size of the k-best lists adds to the oracle accuracy, but the relative improvement appears to be increasing at a logarithmic rate.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9950788021087646}]}, {"text": "The k-best parser is used both to train the k-best reranker and, at inference time, to select a set of hypotheses to rerank.", "labels": [], "entities": []}, {"text": "It is not necessary that training is done with the same size hypothesis set as test, we explore the matched and mismatched conditions in our reranking experiments.", "labels": [], "entities": []}, {"text": "The Penn WSJ treebank was converted using the conversion program described in and available on the web at: http://nlp.cs.lth.se/ pennconverter/ The Best Reported results is from the CoNLL-X competition.", "labels": [], "entities": [{"text": "Penn WSJ treebank", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.949108878771464}, {"text": "CoNLL-X competition", "start_pos": 182, "end_pos": 201, "type": "DATASET", "confidence": 0.8636113703250885}]}, {"text": "The best result reported for English is the Charniak parser (without reranking) on Section 23 of the WSJ Treebank using the same head-finding rules as for the evaluation data.", "labels": [], "entities": [{"text": "Section 23 of the WSJ Treebank", "start_pos": 83, "end_pos": 113, "type": "DATASET", "confidence": 0.877553790807724}]}, {"text": "shows the reranking results for the set of languages.", "labels": [], "entities": []}, {"text": "For each language, we select model parameters on a development set prior to running on the test data.", "labels": [], "entities": []}, {"text": "These parameters include a feature count threshold (the minimum number of observations of a feature before it is included in a model) and a mixture weight controlling the contribution of a quadratic regularizer (used in MaxEnt training).", "labels": [], "entities": [{"text": "feature count threshold", "start_pos": 27, "end_pos": 50, "type": "METRIC", "confidence": 0.7660837968190511}]}, {"text": "For Czech, German, and English, we use the MST bagging technique with 10 bags.", "labels": [], "entities": [{"text": "MST bagging", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.5687832832336426}]}, {"text": "These test results are for the models which performed best on the development set (using 50-best parses).", "labels": [], "entities": []}, {"text": "We see minor improvements over the 1-best baseline MST output (repeated in this table for comparison).", "labels": [], "entities": [{"text": "MST", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.8456783890724182}]}, {"text": "We believe this is due to the overwhelming number of parameters in the reranking models and the relatively small amount of training data.", "labels": [], "entities": []}, {"text": "Interestingly, increasing the number of hypotheses helps for some languages and hurts the others.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: k-best MST oracle results. The 1-best results represent the performance of the parser in isolation.  Results are reported for the CoNLL test set and for English, on Section 23 of the Penn WSJ Treebank.", "labels": [], "entities": [{"text": "MST oracle", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.8886195719242096}, {"text": "CoNLL test set", "start_pos": 140, "end_pos": 154, "type": "DATASET", "confidence": 0.9505946834882101}, {"text": "Section 23 of the Penn WSJ Treebank", "start_pos": 175, "end_pos": 210, "type": "DATASET", "confidence": 0.823304295539856}]}, {"text": " Table 2: Second-stage results from the k-best parser and reranker. The Best Reported and 1-best fields are  copied from table 1. Only non-lexical features were used for the reranking models.", "labels": [], "entities": []}]}