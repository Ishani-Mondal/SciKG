{"title": [{"text": "Extending MARIE: an N -gram-based SMT decoder", "labels": [], "entities": [{"text": "MARIE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.6508066654205322}, {"text": "SMT decoder", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.900772750377655}]}], "abstractContent": [{"text": "In this paper we present several extensions of MARIE 1 , a freely available N-gram-based statistical machine translation (SMT) decoder.", "labels": [], "entities": [{"text": "MARIE", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.7317955493927002}, {"text": "statistical machine translation (SMT) decoder", "start_pos": 89, "end_pos": 134, "type": "TASK", "confidence": 0.8107827816690717}]}, {"text": "The extensions mainly consist of the ability to accept and generate word graphs and the introduction of two new N-gram models in the log-linear combination of feature functions the de-coder implements.", "labels": [], "entities": []}, {"text": "Additionally, the decoder is enhanced with a caching strategy that reduces the number of N-gram calls improving the overall search efficiency.", "labels": [], "entities": []}, {"text": "Experiments are carried out over the Eurpoean Parliament Spanish-English translation task.", "labels": [], "entities": [{"text": "Eurpoean Parliament Spanish-English translation task", "start_pos": 37, "end_pos": 89, "type": "TASK", "confidence": 0.7647819578647613}]}], "introductionContent": [{"text": "Research on SMT has been strongly boosted in the last few years, partially thanks to the relatively easy development of systems with enough competence as to achieve rather competitive results.", "labels": [], "entities": [{"text": "SMT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9951961636543274}]}, {"text": "In parallel, tools and techniques have grown in complexity, which makes it difficult to carryout state-of-the-art research without sharing some of this toolkits.", "labels": [], "entities": []}, {"text": "Without aiming at being exhaustive, GIZA++ 2 , SRILM and PHARAOH are probably the best known examples.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.766857385635376}, {"text": "SRILM", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.8849378228187561}, {"text": "PHARAOH", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.973861038684845}]}, {"text": "We introduce the recent extensions made to an Ngram-based SMT decoder (, which allowed us to tackle several translation issues (such as reordering, rescoring, modeling, etc.) successfully improving accuracy, as well as efficiency results.", "labels": [], "entities": [{"text": "Ngram-based SMT decoder", "start_pos": 46, "end_pos": 69, "type": "DATASET", "confidence": 0.6777497033278147}, {"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9991657733917236}]}, {"text": "As far as SMT can be seen as a double-sided problem (modeling and search), the decoder emerges as a key component, core module of any SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9902961850166321}, {"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9888661503791809}]}, {"text": "Mainly, http://gps-tsc.upc.es/soft/soft/marie 2 http://www.fjoch.com/GIZA++.html 3 http://www.speech.sri.com/projects/srilm/ 4 http://www.isi.edu/publications/licensed-sw/pharaoh/ any technique aiming at dealing with a translation problem needs fora decoder extension to be implemented.", "labels": [], "entities": [{"text": "translation problem", "start_pos": 219, "end_pos": 238, "type": "TASK", "confidence": 0.9203977584838867}]}, {"text": "Particularly, the reordering problem can be more efficiently (and accurate) addressed when tightly coupled with decoding.", "labels": [], "entities": []}, {"text": "In general, the competence of a decoder to make use of the maximum of information in the global search is directly connected with the likeliness of successfully improving translations.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we and briefly review the previous work on decoding with special attention to N -gram-based decoding.", "labels": [], "entities": []}, {"text": "Section 3 describes the extended log-linear combination of feature functions after introduced the two new models.", "labels": [], "entities": []}, {"text": "Section 4 details the particularities of the input and output word graph extensions.", "labels": [], "entities": []}, {"text": "Experiments are reported on section 5.", "labels": [], "entities": []}, {"text": "Finally, conclusions are drawn in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments are carried out fora Spanish-to-English translation task using the EPPS data set, corresponding to session transcriptions of the European Parliament.", "labels": [], "entities": [{"text": "Spanish-to-English translation task", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.7130495309829712}, {"text": "EPPS data set", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9910823305447897}]}, {"text": "shows translation efficiency results (measured in seconds) given two different beam search sizes.", "labels": [], "entities": [{"text": "translation", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.9315234422683716}]}, {"text": "w/cache and w/o cache indicate whether the decoder employs (or not) the cache technique (section 3.3).", "labels": [], "entities": []}, {"text": "Several system configuration have been tested: a baseline monotonous system using a 4-gram translation LM and a 5-gram target LM (base), extended with a target POStagged 5-gram LM (+tpos), further extended by allowing for reordering (+reor), and finally using a source-side POS-tagged 5-gram LM (+spos).", "labels": [], "entities": []}, {"text": "As it can be seen, the cache technique improves the efficiency of the search in terms of decoding time.", "labels": [], "entities": []}, {"text": "Time results are further decreased (reduced time is shown for the w/ cache setting) by using more N -gram LM and allowing fora larger search graph (increasing the beam size and introducing distortion).", "labels": [], "entities": [{"text": "Time", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9599027037620544}]}, {"text": "Further details on the previous experiment can be seen in, where additionally, the input word graph and extended N -gram tagged LM's are successfully used to improve accuracy at a very low computational cost.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9960533380508423}]}, {"text": "Several publications can also be found in bibliography which show the use of output graphs in rescoring tasks allowing for clear accuracy improvements.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9559914469718933}]}], "tableCaptions": [{"text": " Table 1: Translation efficiency results.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9764309525489807}]}]}