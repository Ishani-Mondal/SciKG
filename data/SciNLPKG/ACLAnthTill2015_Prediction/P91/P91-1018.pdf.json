{"title": [{"text": "LEARNING PERCEPTUALLY-GROUNDED SEMANTICS IN THE L0 PROJECT \"Above\" Figure 1: Learning to Associate Scenes with Spatial Terms", "labels": [], "entities": [{"text": "PERCEPTUALLY-GROUNDED", "start_pos": 9, "end_pos": 30, "type": "METRIC", "confidence": 0.7592071890830994}]}], "abstractContent": [{"text": "A method is presented for acquiring perceptually-grounded semantics for spatial terms in a simple visual domain, as apart of the L0 miniature language acquisition project.", "labels": [], "entities": [{"text": "L0 miniature language acquisition", "start_pos": 129, "end_pos": 162, "type": "TASK", "confidence": 0.5213689357042313}]}, {"text": "Two central problems in this learning task are (a) ensuring that the terms learned generalize well, so that they can be accurately applied to new scenes, and (b) learning in the absence of explicit negative evidence.", "labels": [], "entities": []}, {"text": "Solutions to these two problems are presented, and the results discussed.", "labels": [], "entities": []}], "introductionContent": [{"text": "The L0 language learning project at the International Computer Science Institute seeks to provide an account of language acquisition in the semantic domain of spatial relations between geometrical objects.", "labels": [], "entities": [{"text": "International Computer Science Institute", "start_pos": 40, "end_pos": 80, "type": "DATASET", "confidence": 0.9010398983955383}, {"text": "language acquisition", "start_pos": 112, "end_pos": 132, "type": "TASK", "confidence": 0.7354590892791748}]}, {"text": "Within this domain, the work reported here addresses the subtask of learning to associate scenes, containing several simple objects, with terms to describe the spatial relations among the objects in the scenes.", "labels": [], "entities": []}, {"text": "For each scene, the learning system is supplied with an indication of which object is the reference object (we call this object the landmark, or LM), and which object is the one being located relative to the reference object (this is the trajector, or TR).", "labels": [], "entities": []}, {"text": "The system is also supplied with a single spatial term that describes the spatial relation *Supported through the International Computer Science Institute.", "labels": [], "entities": [{"text": "International Computer Science Institute", "start_pos": 114, "end_pos": 154, "type": "DATASET", "confidence": 0.9346591532230377}]}, {"text": "It is to learn to associate all applicable terms to novel scenes.", "labels": [], "entities": []}, {"text": "The TR is restricted to be a single point for the time being; current work is directed at addressing the more general case of an arbitrarily shaped TR.", "labels": [], "entities": []}, {"text": "Another aspect of the task is that learning must take place in the absence of explicit negative instances.", "labels": [], "entities": []}, {"text": "This condition is imposed so that the conditions under which learning takes place will be similar in this respect to those under which children learn.", "labels": [], "entities": []}, {"text": "Given this, there are two central problems in the subtask as stated: \u2022 Ensuring that the learning will generalize to scenes which were not apart of the training set.", "labels": [], "entities": []}, {"text": "This means that the region in which a TR will be considered \"above\" a LM may have to change size, shape, and position when a novel LM is presented.", "labels": [], "entities": []}, {"text": "\u2022 Learning without explicit negative evidence.", "labels": [], "entities": []}, {"text": "This paper presents solutions to both of these problems.", "labels": [], "entities": []}, {"text": "It begins with a general discussion of each of the two problems and their solutions.", "labels": [], "entities": []}, {"text": "Results of training are then presented.", "labels": [], "entities": []}, {"text": "Then, implementation details are discussed.", "labels": [], "entities": []}, {"text": "And finally, some conclusions are presented.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}