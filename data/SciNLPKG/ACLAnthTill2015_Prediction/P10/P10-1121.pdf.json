{"title": [{"text": "Complexity Metrics in an Incremental Right-corner Parser", "labels": [], "entities": [{"text": "Parser", "start_pos": 50, "end_pos": 56, "type": "TASK", "confidence": 0.4848659038543701}]}], "abstractContent": [{"text": "Hierarchical HMM (HHMM) parsers make promising cognitive models: while they use a bounded model of working memory and pursue incremental hypotheses in parallel, they still achieve parsing accuracies competitive with chart-based techniques.", "labels": [], "entities": [{"text": "Hierarchical HMM (HHMM) parsers", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6330032646656036}]}, {"text": "This paper aims to validate that a right-corner HHMM parser is also able to produce complexity metrics, which quantify a reader's incremental difficulty in understanding a sentence.", "labels": [], "entities": []}, {"text": "Besides defining standard metrics in the HHMM framework, anew metric, embedding difference, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory.", "labels": [], "entities": []}, {"text": "Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since the introduction of a parser-based calculation for surprisal by, statistical techniques have been become common as models of reading difficulty and linguistic complexity.", "labels": [], "entities": []}, {"text": "Surprisal has received a lot of attention in recent literature due to nice mathematical properties and predictive ability on eye-tracking movements).", "labels": [], "entities": [{"text": "Surprisal", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.8309328556060791}]}, {"text": "Many other complexity metrics have been suggested as mutually contributing to reading difficulty; for example, entropy reduction), bigram probabilities, and split-syntactic/lexical versions of other metrics . A parser-derived complexity metric such as surprisal can only be as good (empirically) as the model of language from which it derives.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.6683345586061478}]}, {"text": "Ideally, a psychologically-plausible language model would produce a surprisal that would correlate better with linguistic complexity.", "labels": [], "entities": []}, {"text": "Therefore, the specification of how to encode a syntactic language model is of utmost importance to the quality of the metric.", "labels": [], "entities": []}, {"text": "However, it is difficult to quantify linguistic complexity and reading difficulty.", "labels": [], "entities": []}, {"text": "The two commonly-used empirical quantifications of reading difficulty are eye-tracking measurements and word-by-word reading times; this paper uses reading times to find the predictiveness of several parser-derived complexity metrics.", "labels": [], "entities": []}, {"text": "Various factors (i.e., from syntax, semantics, discourse) are likely necessary fora full accounting of linguistic complexity, so current computational models (with some exceptions) narrow the scope to syntactic or lexical complexity.", "labels": [], "entities": []}, {"text": "Three complexity metrics will be calculated in a Hierarchical Hidden Markov Model (HHMM) parser that recognizes trees in right-corner form (the left-right dual of left-corner form).", "labels": [], "entities": []}, {"text": "This type of parser performs competitively on standard parsing tasks (; also, it reflects plausible accounts of human language processing as incremental (), as considering hypotheses probabilistically in parallel, as bounding memory usage to short-term memory limits, and as requiring more memory storage for center-embedding structures than for right-or left-branching ones.", "labels": [], "entities": []}, {"text": "Also, unlike most other parsers, this parser preserves the arceager/arc-standard ambiguity of.", "labels": [], "entities": []}, {"text": "Typical parsing strategies are arcstandard, keeping all right-descendants open for subsequent attachment; but since there can bean unbounded number of such open constituents, this assumption is not compatible with simple models of bounded memory.", "labels": [], "entities": []}, {"text": "A consistently arc-eager strategy acknowledges memory bounds, but yields dead-end parses.", "labels": [], "entities": []}, {"text": "Both analyses are considered in right-corner HHMM parsing.", "labels": [], "entities": [{"text": "HHMM parsing", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.7113243639469147}]}, {"text": "The purpose of this paper is to determine whether the language model defined by the HHMM parser can also predict reading timesit would be strange if a psychologically plausible model did not also produce viable complexity metrics.", "labels": [], "entities": []}, {"text": "In the course of showing that the HHMM parser does, in fact, predict reading times, we will define surprisal and entropy reduction in the HHMM parser, and introduce a third metric called embedding difference.", "labels": [], "entities": []}, {"text": "hypothesized two types of syntactic processing costs: integration cost, in which incremental input is combined with existing structures; and memory cost, where unfinished syntactic constructions may incur some short-term memory usage.", "labels": [], "entities": []}, {"text": "HHMM surprisal and entropy reduction maybe considered forms of integration cost.", "labels": [], "entities": []}, {"text": "Though typical PCFG surprisal has been considered a forward-looking metric), the incremental nature of the right-corner transform causes surprisal and entropy reduction in the HHMM parser to measure the likelihood of grammatical structures that were hypothesized before evidence was observed for them.", "labels": [], "entities": []}, {"text": "Therefore, these HHMM metrics resemble an integration cost encompassing both backwardlooking and forward-looking information.", "labels": [], "entities": []}, {"text": "On the other hand, embedding difference is designed to model the cost of storing centerembedded structures in working memory.", "labels": [], "entities": []}, {"text": "showed that sentences requiring more syntactic memory during sentence processing increased reading times, and it is widely understood that center-embedding incurs significant syntactic processing costs).", "labels": [], "entities": []}, {"text": "Thus, we would expect for the usage of the center-embedding memory store in an HHMM parser to correlate with reading times (and therefore linguistic complexity).", "labels": [], "entities": []}, {"text": "The HHMM parser processes syntactic constructs using a bounded number of store states, defined to represent short-term memory elements; additional states are utilized whenever centerembedded syntactic structures are present.", "labels": [], "entities": []}, {"text": "Similar models such as  implicitly allow an infinite memory size, but showed that a right-corner HHMM parser can parse most sentences in English with 4 or fewer center-embedded-depth levels.", "labels": [], "entities": []}, {"text": "This behavior is similar to the hypothesized size of a human short-term memory store.", "labels": [], "entities": []}, {"text": "A positive result in predicting reading times will lend additional validity to the claim that the HHMM parser's bounded memory corresponds to bounded memory inhuman sentence processing.", "labels": [], "entities": [{"text": "predicting reading", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8746810853481293}]}, {"text": "The rest of this paper is organized as follows: Section 2 defines the language model of the HHMM parser, including definitions of the three complexity metrics.", "labels": [], "entities": []}, {"text": "The methodology for evaluating the complexity metrics is described in Section 3, with actual results in Section 4.", "labels": [], "entities": []}, {"text": "Further discussion on results, and comparisons to other work, are in Section 5.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 69, "end_pos": 78, "type": "DATASET", "confidence": 0.9169261157512665}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results of linear mixed-effect modeling. Significance (indicated by  *  ) is reported at p < 0.05.", "labels": [], "entities": []}, {"text": " Table 3: Correlations in the full model.", "labels": [], "entities": []}]}