{"title": [{"text": "An Exact A* Method for Deciphering Letter-Substitution Ciphers", "labels": [], "entities": []}], "abstractContent": [{"text": "Letter-substitution ciphers encode a document from a known or hypothesized language into an unknown writing system or an unknown encoding of a known writing system.", "labels": [], "entities": []}, {"text": "It is a problem that can occur in a number of practical applications, such as in the problem of determining the encod-ings of electronic documents in which the language is known, but the encoding standard is not.", "labels": [], "entities": []}, {"text": "It has also been used in relation to OCR applications.", "labels": [], "entities": [{"text": "OCR", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9241989254951477}]}, {"text": "In this paper, we introduce an exact method for deciphering messages using a generalization of the Viterbi algorithm.", "labels": [], "entities": []}, {"text": "We test this model on a set of ciphers developed from various web sites, and find that our algorithm has the potential to be a viable, practical method for efficiently solving decipherment problems .", "labels": [], "entities": []}], "introductionContent": [{"text": "Letter-substitution ciphers encode a document from a known language into an unknown writing system or an unknown encoding of a known writing system.", "labels": [], "entities": []}, {"text": "This problem has practical significance in a number of areas, such as in reading electronic documents that may use one of many different standards to encode text.", "labels": [], "entities": []}, {"text": "While this is not a problem in languages like English and Chinese, which have a small set of well known standard encodings such as ASCII, Big5 and Unicode, there are other languages such as Hindi in which there is no dominant encoding standard for the writing system.", "labels": [], "entities": []}, {"text": "In these languages, we would like to be able to automatically retrieve and display the information in electronic documents which use unknown encodings when we find them.", "labels": [], "entities": []}, {"text": "We also want to use these documents for information retrieval and data mining, in which case it is important to be able to read through them automatically, without resorting to a human annotator.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.7626659274101257}, {"text": "data mining", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.8483114540576935}]}, {"text": "The holy grail in this area would bean application to archaeological decipherment, in which the underlying language's identity is only hypothesized, and must be tested.", "labels": [], "entities": []}, {"text": "The purpose of this paper, then, is to simplify the problem of reading documents in unknown encodings by presenting anew algorithm to be used in their decipherment.", "labels": [], "entities": []}, {"text": "Our algorithm operates by running a search over the n-gram probabilities of possible solutions to the cipher, using a generalization of the Viterbi algorithm that is wrapped in an A* search, which determines at each step which partial solutions to expand.", "labels": [], "entities": []}, {"text": "It is guaranteed to converge on the language-modeloptimal solution, and does not require restarts or risk falling into local optima.", "labels": [], "entities": []}, {"text": "We specifically consider the problem of finding decodings of electronic documents drawn from the internet, and we test our algorithm on ciphers drawn from randomly selected pages of Wikipedia.", "labels": [], "entities": []}, {"text": "Our testing indicates that our algorithm will be effective in this domain.", "labels": [], "entities": []}, {"text": "It may seem at first that automatically decoding (as opposed to deciphering) a document is a simple matter, but studies have shown that simple algorithms such as letter frequency counting do not always produce optimal solutions.", "labels": [], "entities": [{"text": "letter frequency counting", "start_pos": 162, "end_pos": 187, "type": "TASK", "confidence": 0.6400261123975118}]}, {"text": "If the text from which a language model is trained is of a different genre than the plaintext of a cipher, the unigraph letter frequencies may differ substantially from those of the language model, and so frequency counting will be misleading.", "labels": [], "entities": []}, {"text": "Because of the perceived simplicity of the problem, however, little work was performed to understand its computational properties until, who developed a method that repeatedly swaps letters in a cipher to find a maximum probability solution.", "labels": [], "entities": []}, {"text": "Since then, several different approaches to this problem have been suggested, some of which use word counts in the language to arrive at a solution, and some of which treat the problem as an expectation maximization problem).", "labels": [], "entities": []}, {"text": "These later algorithms are, however, highly dependent on their initial states, and require a number of restarts in order to find the globally optimal solution.", "labels": [], "entities": []}, {"text": "A further contribution was made by, which, though published earlier, was inspired in part by the method presented here, first discovered in 2007.", "labels": [], "entities": []}, {"text": "Unlike the present method, however, treat the decipherment of letter-substitution ciphers as an integer programming problem.", "labels": [], "entities": []}, {"text": "Clever though this constraint-based encoding is, their paper does not quantify the massive running times required to decode even very short documents with this sort of approach.", "labels": [], "entities": []}, {"text": "Such inefficiency indicates that integer programming may simply be the wrong tool for the job, possibly because language model probabilities computed from empirical data are not smoothly distributed enough over the space in which a cutting-plane method would attempt to compute a linear relaxation of this problem.", "labels": [], "entities": []}, {"text": "In any case, an exact method is available with a much more efficient A* search that is linear-time in the length of the cipher (though still horribly exponential in the size of the cipher and plain text alphabets), and has the additional advantage of being massively parallelizable.", "labels": [], "entities": []}, {"text": "() also seem to believe that short cipher texts are somehow inherently more difficult to solve than long cipher texts.", "labels": [], "entities": []}, {"text": "This difference in difficulty, while real, is not inherent, but rather an artefact of the character-level n-gram language models that they (and we) use, in which preponderant evidence of differences in short character sequences is necessary for the model to clearly favour one lettersubstitution mapping over another.", "labels": [], "entities": []}, {"text": "Uniform character models equivocate regardless of the length of the cipher, and sharp character models with many zeroes can quickly converge even on short ciphers of only a few characters.", "labels": [], "entities": []}, {"text": "In the present method, the role of the language model can be acutely perceived; both the time complexity of the algorithm and the accuracy of the results depend crucially on this characteristic of the language model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9986633062362671}]}, {"text": "In fact, we must use add-one smoothing to decipher texts of even modest lengths because even one unseen plain-text letter sequence is enough to knockout the correct solution.", "labels": [], "entities": []}, {"text": "It is likely that the method of) is sensitive to this as well, but their experiments were apparently fixed on a single, well-trained model.", "labels": [], "entities": []}, {"text": "Applications of decipherment are also explored by, who uses it in the context of optical character recognition (OCR).", "labels": [], "entities": [{"text": "optical character recognition (OCR)", "start_pos": 81, "end_pos": 116, "type": "TASK", "confidence": 0.7705990473429362}]}, {"text": "The problem we consider here is cosmetically related to the \"L2P\" (letter-to-phoneme) mapping problem of text-to-speech synthesis, which also features a prominent constraint-based approach (van den), but the constraints in L2P are very different: two different instances of the same written letter may legitimately map to two different phonemes.", "labels": [], "entities": [{"text": "text-to-speech synthesis", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.7530105710029602}]}, {"text": "This is not the casein letter-substitution maps.", "labels": [], "entities": []}], "datasetContent": [{"text": "The above algorithm is designed for application to the transliteration of electronic documents, specifically, the transliteration of websites, and it has been tested with this in mind.", "labels": [], "entities": [{"text": "transliteration of electronic documents", "start_pos": 55, "end_pos": 94, "type": "TASK", "confidence": 0.8325522691011429}]}, {"text": "In order to gain realistic test data, we have operated on the assumption that Wikipedia is a good approximation of the type of language that will be found inmost internet articles.", "labels": [], "entities": []}, {"text": "We sampled a sequence of Englishlanguage articles from Wikipedia using their random page selector, and these were used to create a set of reference pages.", "labels": [], "entities": []}, {"text": "In order to minimize the common material used in each page, only the text enclosed by the paragraph tags of the main body of the pages were used.", "labels": [], "entities": []}, {"text": "A rough search over internet articles has shown that a length of 1000 to 11000 characters is a realistic length for many articles, although this can vary according to the genre of the page.", "labels": [], "entities": []}, {"text": "Wikipedia, for example, does have entries that are one sentence in length.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9375404119491577}]}, {"text": "We have run two groups of tests for our algorithm.", "labels": [], "entities": []}, {"text": "In the first set of tests, we chose the mean of the above lengths to be our sample size, and we created and decoded 10 ciphers of this size (i.e., different texts, same size).", "labels": [], "entities": []}, {"text": "We made these cipher texts by appending the contents of randomly chosen Wikipedia pages until they contained at least 6000 characters, and then using the first 6000 characters of the resulting files as the plaintexts of the cipher.", "labels": [], "entities": []}, {"text": "The text length was rounded up to the nearest word where needed.", "labels": [], "entities": []}, {"text": "In the second set of tests, we used a single long ciphertext, and measured the time required for the algorithm to finish a number of prefixes of it (i.e., same text, different sizes).", "labels": [], "entities": []}, {"text": "The plaintext for this set of tests was developed in the same way as the first set, and the input ciphertext lengths considered were 1000, 3500, 6000, 8500, 11000, and 13500 characters.", "labels": [], "entities": []}, {"text": "Each cell in the greenhouse is indexed by a plaintext letter and a character from the cipher.", "labels": [], "entities": []}, {"text": "Each cell consists of a smaller array.", "labels": [], "entities": []}, {"text": "The cells in the array give the best probabilities of any path passing through the greenhouse cell, given that the index character of the array maps to the character in column c, where c is the next ciphertext character to be fixed in the solution.", "labels": [], "entities": []}, {"text": "The probability is set to zero if no path can pass through the cell.", "labels": [], "entities": []}, {"text": "This is the case, for example, in (b) and (c), where the knowledge that \" \" maps to \" \" would tell us that the cells indicated in gray are unreachable.", "labels": [], "entities": []}, {"text": "The cell at (d) is filled using the trigram probabilities and the probability of the path at starting at (a).", "labels": [], "entities": []}, {"text": "In all of the data considered, the frequency of spaces was far higher than that of any other character, and so in any real application the character corresponding to the space can likely be guessed without difficulty.", "labels": [], "entities": []}, {"text": "The ciphers we have considered have therefore been simplified by allowing the knowledge of which character corresponds to the space.", "labels": [], "entities": []}, {"text": "It appears that did this as well.", "labels": [], "entities": []}, {"text": "Our algorithm will still work without this assumption, but would take longer.", "labels": [], "entities": []}, {"text": "In the event that a trigram or bigram would be found in the plaintext that was not counted in the language model, add one smoothing was used.", "labels": [], "entities": []}, {"text": "Our character-level language model used was developed from the first 1.5 million characters of the Wall Street Journal section of the Penn Treebank corpus.", "labels": [], "entities": [{"text": "Wall Street Journal section of the Penn Treebank corpus", "start_pos": 99, "end_pos": 154, "type": "DATASET", "confidence": 0.9481486545668708}]}, {"text": "The characters used in the language model were the upper and lowercase letters, spaces, and full stops; other characters were skipped when counting the frequencies.", "labels": [], "entities": []}, {"text": "Furthermore, the number of sequential spaces allowed was limited to one in order to maximize context and to eliminate any long stretches of white space.", "labels": [], "entities": []}, {"text": "As discussed in the previous paragraph, the space character is assumed to be known.", "labels": [], "entities": []}, {"text": "When testing our algorithm, we judged the time complexity of our algorithm by measuring the actual time taken by the algorithm to complete its runs, as well as the number of partial solutions placed onto the queue (\"enqueued\"), the number popped off the queue (\"expanded\"), and the number of zero-probability partial solutions not enqueued (\"zeros\") during these runs.", "labels": [], "entities": []}, {"text": "These latter numbers give us insight into the quality of trigram probabilities as a heuristic for the A* search.", "labels": [], "entities": []}, {"text": "We judged the quality of the decoding by measuring the percentage of characters in the cipher alphabet that were correctly guessed, and also the word error rate of the plaintext generated by our solution.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 145, "end_pos": 160, "type": "METRIC", "confidence": 0.8256545265515646}]}, {"text": "The second metric is useful because a low probability character in the ciphertext maybe guessed wrong without changing as much of the actual plaintext.", "labels": [], "entities": []}, {"text": "Counting the actual number of word errors is meant as an estimate of how useful or readable the plaintext will be.", "labels": [], "entities": []}, {"text": "We did not count the accuracy or word error rate for unfinished ciphers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9995829463005066}, {"text": "word error rate", "start_pos": 33, "end_pos": 48, "type": "METRIC", "confidence": 0.7183911403020223}]}, {"text": "We would have liked to compare our results with those of, but the method presented there was simply not feasible", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Time consumption and accuracy on a sample of 10 6000-character texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9986942410469055}]}, {"text": " Table 2: Time consumption and accuracy on prefixes of a single 13500-character ciphertext.", "labels": [], "entities": [{"text": "Time consumption", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9595246911048889}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.99835604429245}]}]}