{"title": [], "abstractContent": [{"text": "Conditional Random Fields (CRFs) area widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels.", "labels": [], "entities": []}, {"text": "Even for the simple linear-chain model, taking structure into account implies a number of parameters and a computational effort that grows quadrati-cally with the cardinality of the label set.", "labels": [], "entities": []}, {"text": "In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features.", "labels": [], "entities": []}, {"text": "Efficiency stems here from the spar-sity induced by the use of a 1 penalty term.", "labels": [], "entities": [{"text": "spar-sity", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9541149139404297}]}, {"text": "Based on our own implementation , we compare three recent proposals for implementing this regularization strategy.", "labels": [], "entities": [{"text": "regularization", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.9691333770751953}]}, {"text": "Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9992163181304932}]}], "introductionContent": [{"text": "Conditional Random Fields (CRFs) () constitute a widely-used and effective approach for supervised structure learning tasks involving the mapping between complex objects such as strings and trees.", "labels": [], "entities": []}, {"text": "An important property of CRFs is their ability to handle large and redundant feature sets and to integrate structural dependency between output labels.", "labels": [], "entities": []}, {"text": "However, even for simple linear chain CRFs, the complexity of learning and inference This work was partly supported by ANR projects CroTaL (ANR-07-MDCO-003) and MGA (ANR-07-BLAN-0311-02).", "labels": [], "entities": [{"text": "MGA", "start_pos": 161, "end_pos": 164, "type": "DATASET", "confidence": 0.5618258118629456}]}, {"text": "grows quadratically with respect to the number of output labels and so does the number of structural features, ie. features testing adjacent pairs of labels.", "labels": [], "entities": []}, {"text": "Most empirical studies on CRFs thus either consider tasks with a restricted output space (typically in the order of few dozens of output labels), heuristically reduce the use of features, especially of features that test pairs of adjacent labels 1 , and/or propose heuristics to simulate contextual dependencies, via extended tests on the observations (see discussions in, eg.,).", "labels": [], "entities": []}, {"text": "Limitating the feature set or the number of output labels is however frustrating for many NLP tasks, where the type and number of potentially relevant features are very large.", "labels": [], "entities": []}, {"text": "A number of studies have tried to alleviate this problem.", "labels": [], "entities": []}, {"text": "propose to use a \"sparse\" version of the forward-backward algorithm during training, where sparsity is enforced through beam pruning.", "labels": [], "entities": []}, {"text": "Related ideas are discussed by; by Cohn (2006), who considers \"generalized\" feature functions; and by, who use approximations to simplify the forward-backward recursions.", "labels": [], "entities": []}, {"text": "In this paper, we show that the sparsity that is induced by 1 -penalized estimation of CRFs can be used to reduce the total training time, while yielding extremely compact models.", "labels": [], "entities": []}, {"text": "The benefits of sparsity are even greater during inference: less features need to be extracted and included in the potential functions, speeding up decoding with a lesser memory footprint.", "labels": [], "entities": []}, {"text": "We study and compare three different ways to implement 1 penalty for CRFs that have been introduced recently: orthantwise Quasi Newton ( , stochastic gradient descent ( and coordinate descent (, concluding that these methods have complemen-tary strengths and weaknesses.", "labels": [], "entities": [{"text": "coordinate descent", "start_pos": 173, "end_pos": 191, "type": "TASK", "confidence": 0.6869338899850845}]}, {"text": "Based on an efficient implementation of these algorithms, we were able to train very large CRFs containing more than a hundred of output labels and up to several billion features, yielding results that are as good or better than the best reported results for two NLP benchmarks, text phonetization and part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 302, "end_pos": 324, "type": "TASK", "confidence": 0.7771291732788086}]}, {"text": "Our contribution is therefore twofold: firstly a detailed analysis of these three algorithms, discussing implementation, convergence and comparing the effect of various speed-ups.", "labels": [], "entities": []}, {"text": "This comparison is made fair and reliable thanks to the reimplementation of these techniques in the same software package.", "labels": [], "entities": []}, {"text": "Second, the experimental demonstration that using large output label sets is doable and that very large feature sets actually help improve prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.7594468593597412}]}, {"text": "In addition, we show how sparsity in structured feature sets can be used in incremental training regimes, where long-range features are progressively incorporated in the model insofar as the shorter range features have proven useful.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: we first recall the basics of CRFs in Section 2, and discuss three ways to train CRFs with a 1 penalty in Section 3.", "labels": [], "entities": [{"text": "CRFs", "start_pos": 77, "end_pos": 81, "type": "TASK", "confidence": 0.8875375390052795}]}, {"text": "We then detail several implementation issues that need to be addressed when dealing with massive feature sets in Section 4.", "labels": [], "entities": []}, {"text": "Our experiments are reported in Section 5.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.8232555985450745}]}, {"text": "The main conclusions of this study are drawn in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments use two standard NLP tasks, phonetization and part-of-speech tagging, chosen hereto illustrate two very different situations, and to allow for comparison with results reported elsewhere in the literature.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.6952308863401413}]}, {"text": "Unless otherwise mentioned, the experiments use the same protocol: 10 fold cross validation, where eight folds are used for training, one for development, and one for testing.", "labels": [], "entities": []}, {"text": "Results are reported in terms of phoneme error rates or tag error rates on the test set.", "labels": [], "entities": [{"text": "tag error rates", "start_pos": 56, "end_pos": 71, "type": "METRIC", "confidence": 0.9185842672983805}]}, {"text": "Comparing run-times can be a tricky matter, especially when different software packages are involved.", "labels": [], "entities": []}, {"text": "As discussed above, the observed runtimes depend on many small implementation details.", "labels": [], "entities": []}, {"text": "As the three algorithms share as much code as possible, we believe the comparison reported hereafter to be fair and reliable.", "labels": [], "entities": []}, {"text": "All experiments were performed on a server with 64G of memory and two Xeon processors with 4 cores at 2.27 Ghz.", "labels": [], "entities": []}, {"text": "For comparison, all measures of run-times include the cumulated activity of all cores and give very pessimistic estimates of the wall time, which can be up to 7 times smaller.", "labels": [], "entities": []}, {"text": "For OWL-QN, we use 5 past values of the gradient to approximate the inverse of the Hessian matrix: increasing this value had no effect on accuracy or convergence and was detrimental to speed; for SGD, the learning rate parameter was tuned manually.", "labels": [], "entities": [{"text": "OWL-QN", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.7659439444541931}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.999110758304596}, {"text": "SGD", "start_pos": 196, "end_pos": 199, "type": "TASK", "confidence": 0.8983625769615173}]}, {"text": "Note that we have not spent much time optimizing the values of \u03c1 1 and \u03c1 2 . Based on a pilot study on Nettalk, we found that taking \u03c1 1 = .5 and \u03c1 2 in the order of 10 \u22125 to yield nearly optimal performance, and have used these values throughout.", "labels": [], "entities": [{"text": "Nettalk", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9771570563316345}]}], "tableCaptions": [{"text": " Table 1: Features jointly testing label pairs and  the observation are useful (error rates and features  counts.)", "labels": [], "entities": []}, {"text": " Table 2: Sparse vs standard forward-backward  (training times and percentages of sparsity of M )", "labels": [], "entities": [{"text": "Sparse", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9171007871627808}]}, {"text": " Table 4: Performance on Nettalk", "labels": [], "entities": [{"text": "Nettalk", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.7884665131568909}]}]}