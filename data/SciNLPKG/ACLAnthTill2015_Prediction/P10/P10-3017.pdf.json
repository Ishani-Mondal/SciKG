{"title": [{"text": "Weakly Supervised Learning of Presupposition Relations between Verbs", "labels": [], "entities": [{"text": "Supervised Learning of Presupposition Relations between Verbs", "start_pos": 7, "end_pos": 68, "type": "TASK", "confidence": 0.7461569394384112}]}], "abstractContent": [{"text": "Presupposition relations between verbs are not very well covered in existing lexical semantic resources.", "labels": [], "entities": []}, {"text": "We propose a weakly supervised algorithm for learning presup-position relations between verbs that distinguishes five semantic relations: presup-position, entailment, temporal inclusion, antonymy and other/no relation.", "labels": [], "entities": []}, {"text": "We start with a number of seed verb pairs selected manually for each semantic relation and classify unseen verb pairs.", "labels": [], "entities": []}, {"text": "Our algorithm achieves an overall accuracy of 36% for type-based classification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9996808767318726}, {"text": "type-based classification", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.8737379312515259}]}], "introductionContent": [{"text": "A main characteristics of natural language is that significant portions of content conveyed in a message may not be overtly realized.", "labels": [], "entities": []}, {"text": "This is the case for presuppositions: e.g, the utterance Columbus didn't manage to reach India.", "labels": [], "entities": []}, {"text": "presupposes that Columbus had tried to reach India.", "labels": [], "entities": []}, {"text": "This presupposition does not need to be stated, but is implicitly understood.", "labels": [], "entities": []}, {"text": "Determining the presuppositions of events reported in texts can be exploited to improve the quality of many natural language processing applications, such as information extraction, text understanding, text summarization, question-answering or machine translation.", "labels": [], "entities": [{"text": "Determining the presuppositions of events reported in texts", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.6880564317107201}, {"text": "information extraction", "start_pos": 158, "end_pos": 180, "type": "TASK", "confidence": 0.8205711245536804}, {"text": "text understanding", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.8075952529907227}, {"text": "text summarization", "start_pos": 202, "end_pos": 220, "type": "TASK", "confidence": 0.724088042974472}, {"text": "machine translation", "start_pos": 244, "end_pos": 263, "type": "TASK", "confidence": 0.7667893469333649}]}, {"text": "The phenomenon of presupposition has been throughly investigated by philosophers and linguists (i.a..", "labels": [], "entities": []}, {"text": "There are only few attempts for practical implementations of presupposition in computational linguistics (e.g..", "labels": [], "entities": []}, {"text": "Especially, presupposition is understudied in the field of corpus-based learning of semantic relations.", "labels": [], "entities": []}, {"text": "Machine learning methods have been previously applied to determine semantic relations such as is-a and part-of, also succession, reaction and production).", "labels": [], "entities": []}, {"text": "explored classification of fine-grained verb semantic relations, such as similarity, strength, antonymy, enablement and happens-before.", "labels": [], "entities": [{"text": "classification of fine-grained verb semantic relations", "start_pos": 9, "end_pos": 63, "type": "TASK", "confidence": 0.6859045575062434}]}, {"text": "For the task of entailment recognition, learning of entailment relations was attempted.", "labels": [], "entities": [{"text": "entailment recognition", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.9724103212356567}]}, {"text": "None of the previous work investigated subclassifying semantic relations including presupposition and entailment, two relations that are closely related, but behave differently in context.", "labels": [], "entities": []}, {"text": "In particular, the inferential behaviour of presuppositions and entailments crucially differs in special semantic contexts.", "labels": [], "entities": []}, {"text": "E.g., while presuppositions are preserved under negation (as in Columbus managed/didn't manage to reach India the presupposition tried to), entailments do not survive under negation (John F. Kennedy has been/has not been killed).", "labels": [], "entities": []}, {"text": "Here the entailment died only survives in the positive sentence.", "labels": [], "entities": []}, {"text": "Such differences are crucial for both analysis and generation-oriented NLP tasks.", "labels": [], "entities": []}, {"text": "This paper presents a weakly supervised algorithm for learning presupposition relations between verbs cast as a discriminative classification problem.", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows: Section 2 reviews state of the art.", "labels": [], "entities": []}, {"text": "Section 3 introduces our task and the learning algorithm.", "labels": [], "entities": []}, {"text": "Section 4 reports on experiment organization; the results are presented in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we summarise and present objectives for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Initial Subset of Verb Pair Candidates.", "labels": [], "entities": []}, {"text": "Unlike other semi-supervised approaches, we don't use patterns for acquiring new candidates for classification.", "labels": [], "entities": []}, {"text": "Candidate verb pairs are obtained from a previously compiled list of highly associated verbs.", "labels": [], "entities": []}, {"text": "We use the DIRT Collection () from which we further extract pairs of highly associated verbs as candidates for classification.", "labels": [], "entities": [{"text": "DIRT Collection", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.9004645347595215}]}, {"text": "The advantage of this resource is that it consists of pairs of verbs which stand in a semantic relation (cf. Section 2).", "labels": [], "entities": []}, {"text": "This considerably reduces the number of verb pairs that need to be processed as candidates in our classification task.", "labels": [], "entities": []}, {"text": "DIRT contains 5,604 verb types and 808,764 verb pair types.", "labels": [], "entities": [{"text": "DIRT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.903117835521698}]}, {"text": "This still represents a huge number of verb pairs to be processed.", "labels": [], "entities": []}, {"text": "We therefore filtered the extracted set by checking verb pair frequency in the first three parts of the ukWAC corpus () (UKWAC 1.", "labels": [], "entities": [{"text": "ukWAC corpus", "start_pos": 104, "end_pos": 116, "type": "DATASET", "confidence": 0.9864627122879028}, {"text": "UKWAC 1", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.9562574923038483}]}, {"text": "3) and by applying the PMI test with threshold 2.0.", "labels": [], "entities": [{"text": "PMI test", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9157270789146423}]}, {"text": "This reduces the number of verb pairs to 199,393.", "labels": [], "entities": []}, {"text": "For each semantic relation we select three verb pairs as seeds.", "labels": [], "entities": []}, {"text": "The only exception is temporal inclusion for which we selected six verb pairs, due to the low frequency of such verb pairs within a single sentence.", "labels": [], "entities": [{"text": "temporal inclusion", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.6879841238260269}]}, {"text": "These verb pairs were used for building an initial training corpus of verb pairs in context.", "labels": [], "entities": []}, {"text": "The remaining verb pairs are used to build the corpus of unlabeled verb pairs in context in the iterative classification process.", "labels": [], "entities": []}, {"text": "Given these verb pairs, we extracted sentences for training and for unlabeled data set from the first three parts of the UKWAC corpus ().", "labels": [], "entities": [{"text": "UKWAC corpus", "start_pos": 121, "end_pos": 133, "type": "DATASET", "confidence": 0.9934636652469635}]}, {"text": "We compiled a set of CQP queries) to find sentences that contain both verbs of a verb pair and applied them on UKWAC 1.", "labels": [], "entities": [{"text": "UKWAC 1", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9907338917255402}]}, {"text": "3 to build the training and unlabeled subcorpora.", "labels": [], "entities": []}, {"text": "We filter out sentences with more than 60 words and sentences with a distance between verbs exceeding 20 words.", "labels": [], "entities": []}, {"text": "To avoid growing complexity, only sentences with exactly one occurrence of each verb pair are retained.", "labels": [], "entities": []}, {"text": "We also remove sentences that trigger wrong candidates, in which the auxiliaries have or do appear in a candidate verb pair.", "labels": [], "entities": []}, {"text": "The corpus is parsed using the XLE parser (.", "labels": [], "entities": []}, {"text": "Its output contains both the structural and functional information we need to extract the shallow and deep features used in the classification, and to generate patterns.", "labels": [], "entities": []}, {"text": "From this preprocessed corpus, we created a training corpus that contains three different components: 1.", "labels": [], "entities": []}, {"text": "All sentences containing seed verb pairs extracted from UKWAC 1 are annotated manually with two values true/false in order to separate the negative training data.", "labels": [], "entities": [{"text": "UKWAC 1", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.9671814441680908}]}, {"text": "2. Automatically annotated training set.", "labels": [], "entities": []}, {"text": "We build an extended, heuristically annotated training set for the seed verb pairs, by extracting further instances from the remaining corpora (UKWAC 2 and UKWAC 3).", "labels": [], "entities": [{"text": "UKWAC", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.9570847153663635}, {"text": "UKWAC", "start_pos": 156, "end_pos": 161, "type": "DATASET", "confidence": 0.9238560199737549}]}, {"text": "Using the manual annotations of step 1., we manually compiled a small stoplist of patterns that are used to filter out wrong instances.", "labels": [], "entities": []}, {"text": "The constructed stoplist serves as an elementary disambiguation step.", "labels": [], "entities": []}, {"text": "For example, the verbs look and see can stand in an entailment relation if look is followed by the prepositions at, on, in, but not in case of prepositions after or forward (e.g. looking forward to).", "labels": [], "entities": []}, {"text": "To further enrich the training set of data, synonyms of the verb pairs are manually selected from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9756028652191162}]}, {"text": "The corresponding verb pairs were extracted from UKWAC 1.", "labels": [], "entities": [{"text": "UKWAC 1", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.9807867705821991}]}, {"text": "3. In order to avoid adding noise, we used only synonyms of unambiguous verbs.", "labels": [], "entities": []}, {"text": "The problem of ambiguity of the target verbs wasn't considered at this step.", "labels": [], "entities": []}, {"text": "The overall size of the training set for the first classification step is 15,717 sentences from which 5,032 are manually labeled, 9,918 sentences are automatically labeled and 757 sentences contain synonymous verb pairs.", "labels": [], "entities": []}, {"text": "The distribution is unbalanced: temporal inclusion e.g. covers only 2%, while entailment covers 39% of sentences.", "labels": [], "entities": []}, {"text": "We balanced the training set by undersampling entailment and other/no by 20% and correspondingly oversampling the temporal inclusion class.", "labels": [], "entities": []}, {"text": "Similar to other pattern-based approaches we use a set of seed verb pairs to induce indicative patterns for each semantic relation.", "labels": [], "entities": []}, {"text": "We use the induced patterns to restrict the number of the verb pair candidates and to rank the labelled instances in the iterative classification step.", "labels": [], "entities": []}, {"text": "The patterns use information about the verb forms of analyzed verb pairs, modal verbs and the polarity verbs (only if they are related to the analyzed verbs) and coordinating/subordinating conjunctions connecting two verbs.", "labels": [], "entities": []}, {"text": "The analyzed verbs in the sentence are substituted with V1 and V2 placeholders in the pattern.", "labels": [], "entities": []}, {"text": "The patterns are extracted automatically from deep parses of the training corpus.", "labels": [], "entities": []}, {"text": "Examples of the best patterns we determined for semantic relations are presented in  Pattern ranks are used to compute the reliability score for instances, as proposed by.", "labels": [], "entities": [{"text": "reliability score", "start_pos": 123, "end_pos": 140, "type": "METRIC", "confidence": 0.9658584296703339}]}, {"text": "The pattern reliability is calculated as follows: where: pmi(i, p) -pointwise mutual information (PMI) between the instance i and the pattern p; max pmi -maximum PMI between all patterns and all instances; r i (i) -reliability of an instance i.", "labels": [], "entities": [{"text": "pointwise mutual information (PMI)", "start_pos": 68, "end_pos": 102, "type": "METRIC", "confidence": 0.693425049384435}]}, {"text": "For seeds r i (i) = 1 (they are selected manually), for the next iterations the instance reliability is: We also consider using the patterns as a feature for classification, in case they turnout to be sufficiently discriminative.", "labels": [], "entities": []}, {"text": "We independently train 5 binary classifiers, one for each semantic relation, using the J48 decision tree algorithm).", "labels": [], "entities": []}, {"text": "As the primary goal of this paper is to classify semantic relations on the type level, we elaborated a first gold standard dataset for typebased classification.", "labels": [], "entities": [{"text": "typebased classification", "start_pos": 135, "end_pos": 159, "type": "TASK", "confidence": 0.7995792925357819}]}, {"text": "We used a small sample of 100 verb pairs randomly selected from the automatically labeled corpus.", "labels": [], "entities": []}, {"text": "This sample was manually annotated by two judges after we had eliminated the system annotations in order not to influence the judges' decisions.", "labels": [], "entities": []}, {"text": "The judges had the possibility to select more than one annotation, if necessary.", "labels": [], "entities": []}, {"text": "We measured inter-annotator agreement was 61% (k \u2248 0.21).", "labels": [], "entities": [{"text": "agreement", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.6792152523994446}]}, {"text": "The low agreement shows the difficulty of decision in the annotation of fine-grained semantic relations.", "labels": [], "entities": []}, {"text": "While the first gold standard dataset of verb pairs was annotated out of context, we constructed a second gold standard of verb pairs annotated at the token level, i.e. in context.", "labels": [], "entities": []}, {"text": "This second data set can be used to evaluate a token-based classifier (a task not attempted in the present paper).", "labels": [], "entities": []}, {"text": "It also offers aground truth for type-based classification, in that it controls for contextual ambiguity effects.", "labels": [], "entities": [{"text": "type-based classification", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.8560155630111694}]}, {"text": "I.e., we can extract a type-based gold standard on the basis of the token-annotated data.", "labels": [], "entities": []}, {"text": "We proposed to one judge to annotate the same 100 verb pair types as in the previous annotation task, this time in context.", "labels": [], "entities": []}, {"text": "For this purpose we randomly selected 10 instances for each verb pair type (for rare verb pair types only 5).", "labels": [], "entities": []}, {"text": "We compared the gold standards elaborated by the same judge for typebased and token-based classification: \u2022 62% of verb pair types were annotated with the same labels on both levels, indicating correct annotation \u2022 10% of verb pair types were assigned conflicting labels, indicating wrong annotation \u2022 28% of verb pair types were assigned labels not present on the type level, or the type level label was not assigned in context The figures show that for the most part the typebased annotation conforms with the ground truth obtained from token-based annotation.", "labels": [], "entities": []}, {"text": "Only 10% of verb pair types were established as conflicting with the ground truth.", "labels": [], "entities": []}, {"text": "The remaining 28% can be considered as potentially correct: either the annotated data does not contain the appropriate context fora given type label or the type-level anno-tation, performed without context, does not foresee an existing relation.", "labels": [], "entities": []}, {"text": "This points to a general difficulty, namely to acquire representative data sets for token-level annotation, and also to perform type-level annotations without context for the present task.", "labels": [], "entities": []}, {"text": "Combining Classifiers in Ensemble Learning.", "labels": [], "entities": []}, {"text": "Both token-based and type-based classification starts with determining of the most confident classification for instances.", "labels": [], "entities": [{"text": "type-based classification", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.7001097500324249}]}, {"text": "Each instance of the corpus of unlabeled verb pairs is classified by the individual binary classifiers.", "labels": [], "entities": []}, {"text": "In order to select the most confident classification we compare the votes of the individual classifiers as follows: 1.", "labels": [], "entities": []}, {"text": "If an instance is classified by one of the classifiers as true with confidence less than 0.75, we discard this classification.", "labels": [], "entities": []}, {"text": "2. If an instance is classified as true by more than one classifier, we consider only the classification with the highest confidence.", "labels": [], "entities": []}, {"text": "4 In contrast to token-based classification that accepts only one semantic relation, for type-based classification we allow the existence of more than one semantic relation fora verb pair.", "labels": [], "entities": []}, {"text": "To avoid the unreliable classifications, we apply several filters: 1.", "labels": [], "entities": []}, {"text": "If less than 10% of the instances fora verb pair are classified with some specific semantic relation, this classification is considered to be unconfident and is discarded.", "labels": [], "entities": []}, {"text": "2. If a verb pair is classified as positive for more than three semantic relations, this verb pair remains unclassified.", "labels": [], "entities": []}, {"text": "3. If a verb pair is classified with up to three semantic relations and if more than 10% of the examples are classified with any of these relations, the verb pair is labeled with all of them.", "labels": [], "entities": []}, {"text": "After determining the most confident classification we rank the instances, following the ranking procedure of.", "labels": [], "entities": []}, {"text": "Instances that exceed a reliability threshold (0.3 for our experiment) are selected for the extended training set.", "labels": [], "entities": [{"text": "reliability", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9754496812820435}]}, {"text": "The remainining instances are returned to the unlabeled set.", "labels": [], "entities": []}, {"text": "The algorithm stops if the average reliability score is smaller than a threshold value.", "labels": [], "entities": [{"text": "reliability score", "start_pos": 35, "end_pos": 52, "type": "METRIC", "confidence": 0.9312165677547455}]}, {"text": "In our paper we concentrate on the first iteration.", "labels": [], "entities": []}, {"text": "Extension of the training set and re-ranking of patterns will be reported in future work.", "labels": [], "entities": []}, {"text": "Macro-Average 56% 36% Micro-Average 65% 36%: Accuracy for type-based classification  Results for type-based classification.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9984782338142395}, {"text": "type-based classification", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.7678690552711487}, {"text": "type-based classification", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.7901164293289185}]}, {"text": "We evaluate the accuracy of classification based on two alternative measures: 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9993662238121033}, {"text": "classification", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.9289659261703491}]}, {"text": "Majority -the semantic relation with which the majority of the sentences containing a verb pair have been annotated.", "labels": [], "entities": []}, {"text": "2. Without NONE -as in 1., but after removing the label NONE from all relation assignments except for those cases where NONE is the only label assigned to a verb pair.", "labels": [], "entities": []}, {"text": "We computed accuracy as the number of verb pairs which were correctly labeled by the system divided by the total number of system labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995322227478027}]}, {"text": "We compare our results against a baseline of random assignment, taking the distribution found in the manually labeled gold standard as the underlying verb relation distribution.", "labels": [], "entities": []}, {"text": "shows the accuracy results for each semantic relation . Results for token-based classification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995442032814026}, {"text": "token-based classification", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.8227419257164001}]}, {"text": "We also evaluate the accuracy of classification for tokenbased classification as the number of instances which were correctly labeled by the system divided by the total number of system labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9995988011360168}, {"text": "tokenbased classification", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.7931236922740936}]}, {"text": "As the baseline we took the relation distribution on the token level.", "labels": [], "entities": []}, {"text": "shows the accuracy results for each semantic relation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996623992919922}]}, {"text": "The results obtained for type-based classification are well above the baseline with one exception.", "labels": [], "entities": [{"text": "type-based classification", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.9405272006988525}]}, {"text": "The best performance is achieved by antonymy (72% and 42% respectively for both: Accuracy for token-based classification measures), followed by temporal inclusion, presupposition and entailment.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9972178936004639}, {"text": "token-based classification", "start_pos": 94, "end_pos": 120, "type": "TASK", "confidence": 0.7205455303192139}]}, {"text": "Accuracy scores for token-based classification (excluding NONE) are lower at 29% to 13%.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9965697526931763}, {"text": "token-based classification", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.8498255610466003}]}, {"text": "Error analysis of randomly selected false positives shows that the main reason for lower accuracy on the token level is that the context is not always significant enough to determine the correct relation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9980509281158447}]}, {"text": "Other projects such as VerbOcean () report higher accuracy: the average accuracy is 65.5% if at least one tag is correct and 53% for the correct preferred tag.", "labels": [], "entities": [{"text": "VerbOcean", "start_pos": 23, "end_pos": 32, "type": "DATASET", "confidence": 0.8630021810531616}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9990795850753784}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9988358616828918}]}, {"text": "However, we cannot objectively compare the results of VerbOcean to our system because of the difference in the set of relation classes and evaluation procedures.", "labels": [], "entities": [{"text": "VerbOcean", "start_pos": 54, "end_pos": 63, "type": "DATASET", "confidence": 0.8688527345657349}]}, {"text": "Similar to us, evaluated VerbOcean using a small sample of data which was presented to two judges for manual evaluation.", "labels": [], "entities": []}, {"text": "In contrast to our setup, they didn't remove the system annotations from the evaluation data set.", "labels": [], "entities": []}, {"text": "Given the difficulty of the classification we suspect that correction of system output relations for establishing a gold standard bears a strong risk in favouring system classifications.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Accuracy for type-based classification", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9983747005462646}, {"text": "type-based classification", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.9284100532531738}]}, {"text": " Table 4: Accuracy for token-based classification", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.998304009437561}, {"text": "token-based classification", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.8199544250965118}]}]}