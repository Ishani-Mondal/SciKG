{"title": [{"text": "Contextualizing Semantic Representations Using Syntactically Enriched Vector Models", "labels": [], "entities": [{"text": "Contextualizing Semantic Representations", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7533416152000427}]}], "abstractContent": [{"text": "We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion.", "labels": [], "entities": []}, {"text": "It employs a systematic combination of first-and second-order context vectors.", "labels": [], "entities": []}, {"text": "We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a word-sense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.", "labels": [], "entities": [{"text": "paraphrase ranking task", "start_pos": 108, "end_pos": 131, "type": "TASK", "confidence": 0.7933703263600668}]}], "introductionContent": [{"text": "In the logical paradigm of natural-language semantics originating from, semantic structure, composition and entailment have been modelled to an impressive degree of detail and formal consistency.", "labels": [], "entities": []}, {"text": "These approaches, however, lack coverage and robustness, and their impact on realistic natural-language applications is limited: The logical framework suffers from overspecificity, and is inappropriate to model the pervasive vagueness, ambivalence, and uncertainty of natural-language semantics.", "labels": [], "entities": []}, {"text": "Also, the handcrafting of resources covering the huge amounts of content which are required for deep semantic processing is highly inefficient and expensive.", "labels": [], "entities": []}, {"text": "Co-occurrence-based semantic vector models offer an attractive alternative.", "labels": [], "entities": []}, {"text": "In the standard approach, word meaning is represented by feature vectors, with large sets of context words as dimensions, and their co-occurrence frequencies as values.", "labels": [], "entities": []}, {"text": "Semantic similarity information can be acquired using unsupervised methods at virtually no cost, and the information gained is soft and gradual.", "labels": [], "entities": []}, {"text": "Many NLP tasks have been modelled successfully using vector-based models.", "labels": [], "entities": []}, {"text": "Examples include information retrieval, wordsense discrimination and disambiguation (, to name but a few.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.8458074033260345}, {"text": "wordsense discrimination", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.8161988258361816}]}, {"text": "Standard vector-space models have serious limitations, however: While semantic information is typically encoded in phrases and sentences, distributional semantics, in sharp contrast to logic-based semantics, does not offer any natural concept of compositionality that would allow the semantics of a complex expression to be computed from the meaning of its parts.", "labels": [], "entities": []}, {"text": "A different, but related problem is caused by word-sense ambiguity and contextual variation of usage.", "labels": [], "entities": []}, {"text": "Frequency counts of context words fora given target word provide invariant representations averaging overall different usages of the target word.", "labels": [], "entities": []}, {"text": "There is no obvious way to distinguish the different senses of e.g. acquire in different contexts, such as acquire knowledge or acquire shares.", "labels": [], "entities": []}, {"text": "Several approaches for word-sense disambiguation in the framework of distributional semantics have been proposed in the literature.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.7371323704719543}]}, {"text": "In contrast to these approaches, we present a method to model the mutual contextualization of words in a phrase in a compositional way, guided by syntactic structure.", "labels": [], "entities": []}, {"text": "To some extent, our method resembles the approaches proposed by and.", "labels": [], "entities": []}, {"text": "We go one step further, however, in that we employ syntactically enriched vector models as the basic meaning representations, assuming a vector space spanned by combinations of dependency relations and words.", "labels": [], "entities": []}, {"text": "This allows us to model the semantic interaction between the meaning of ahead word and its dependent at the micro-level of relation-specific cooccurrence frequencies.", "labels": [], "entities": []}, {"text": "It turns out that the benefit to precision is considerable.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9951766729354858}]}, {"text": "Using syntactically enriched vector models raises problems of different kinds: First, the use of syntax increases dimensionality and thus may cause data sparseness.", "labels": [], "entities": []}, {"text": "Second, the vectors of two syntactically related words, e.g., a target verb acquire and its direct object knowledge, typically have different syntactic environments, which implies that their vector representations encode complementary information and there is no direct way of combining the information encoded in the respective vectors.", "labels": [], "entities": []}, {"text": "To solve these problems, we build upon previous work) and propose to use syntactic second-order vector representations.", "labels": [], "entities": []}, {"text": "Second-order vector representations in a bag-ofwords setting were first used by; in a syntactic setting, they also feature in.", "labels": [], "entities": []}, {"text": "For the problem at hand, the use of second-order vectors alleviates the sparseness problem, and enables the definition of vector space transformations that make the distributional information attached to words in different syntactic positions compatible.", "labels": [], "entities": []}, {"text": "Thus, it allows vectors fora predicate and its arguments to be combined in a compositional way.", "labels": [], "entities": []}, {"text": "We conduct two experiments to assess the suitability of our method.", "labels": [], "entities": []}, {"text": "Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset.", "labels": [], "entities": [{"text": "SemEval 2007 lexical substitution task dataset", "start_pos": 43, "end_pos": 89, "type": "DATASET", "confidence": 0.720152939359347}]}, {"text": "It will show that our method significantly outperforms other unsupervised methods that have been proposed in the literature to rank words with respect to their semantic similarity in a given linguistic context.", "labels": [], "entities": []}, {"text": "Ina second experiment, we apply our model to the \"word sense similarity task\" recently proposed by, which is a refined variant of a word-sense disambiguation task.", "labels": [], "entities": [{"text": "word sense similarity task", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.7176850587129593}, {"text": "word-sense disambiguation task", "start_pos": 132, "end_pos": 162, "type": "TASK", "confidence": 0.7768774131933848}]}, {"text": "The results show a substantial positive effect.", "labels": [], "entities": []}, {"text": "We will first review related work in Section 2, before presenting our model in Section 3.", "labels": [], "entities": []}, {"text": "In Sections 4 and 5 we evaluate our model on the two different tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our model on a paraphrase ranking task.", "labels": [], "entities": []}, {"text": "We consider sentences with an occurrence of some target word wand a list of paraphrase candidates w 1 , . .", "labels": [], "entities": []}, {"text": ", wk such that each of thew i is a paraphrase of w for some sense of w.", "labels": [], "entities": []}, {"text": "The task is to decide for each of the paraphrase candidates w i how appropriate it is as a paraphrase of win the given context.", "labels": [], "entities": []}, {"text": "For instance, buy, purchase and obtain are all paraphrases of acquire, in the sense that they can be substituted for acquire in some contexts, but purchase and buy are not paraphrases of acquire in the first sentence of.", "labels": [], "entities": []}, {"text": "To evaluate the performance of our method we use generalized average precision), a variant of average precision.", "labels": [], "entities": [{"text": "generalized average precision", "start_pos": 49, "end_pos": 78, "type": "METRIC", "confidence": 0.5677559673786163}, {"text": "average precision", "start_pos": 94, "end_pos": 111, "type": "METRIC", "confidence": 0.8567257523536682}]}, {"text": "Average precision) is a measure commonly used to evaluate systems that return ranked lists of results.", "labels": [], "entities": [{"text": "Average precision)", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.7808412710825602}]}, {"text": "Generalized average precision (GAP) additionally rewards the correct order of positive cases w.r.t. their gold standard weight.", "labels": [], "entities": [{"text": "Generalized average precision (GAP)", "start_pos": 0, "end_pos": 35, "type": "METRIC", "confidence": 0.8710292180379232}]}, {"text": "We define average precision first: where xi is a binary variable indicating whether the ith item as ranked by the model is in the gold standard or not, R is the size of the gold standard, and n is the number of paraphrase candidates to be ranked.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.7294896841049194}]}, {"text": "If we take xi to be the gold standard weight of the ith item or zero if it is not in the gold standard, we can define generalized average precision as follows: where I(x i ) = 1 if xi is larger than zero, zero otherwise, and y i is the average weight of the ideal ranked list y 1 , . .", "labels": [], "entities": [{"text": "average precision", "start_pos": 130, "end_pos": 147, "type": "METRIC", "confidence": 0.7075695693492889}]}, {"text": ", y i of gold standard paraphrases.", "labels": [], "entities": [{"text": "gold standard paraphrases", "start_pos": 9, "end_pos": 34, "type": "DATASET", "confidence": 0.8571029901504517}]}, {"text": "As a second scoring method, we use precision out often (P 10 ).", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9988962411880493}]}, {"text": "The measure is less discriminative than GAP.", "labels": [], "entities": []}, {"text": "We use it because we want to compare our model with E&P.", "labels": [], "entities": []}, {"text": "P 10 measures the percentage of gold-standard paraphrases in the top-ten list of paraphrases as ranked by the system, and can be defined as follows: where M is the list of 10 paraphrase candidates topranked by the model, G is the corresponding annotated gold-standard data, and f (s) is the weight of the individual paraphrases.", "labels": [], "entities": []}, {"text": "In our first experiment, we consider verb paraphrases using the same controlled subset of the lexical substitution task data that had been used by TDP in an earlier study.", "labels": [], "entities": []}, {"text": "We compare our model to various baselines and the models of TDP and E&P, and show that our new model substantially outperforms previous work.", "labels": [], "entities": []}, {"text": "The dataset is identical to the one used by TDP and has been constructed in the same way as the dataset used by E&P: it contains those goldstandard instances of verbs that have-according to the analyses produced by the MiniPar parser-an overtly realized subject and object.", "labels": [], "entities": []}, {"text": "Gold-standard paraphrases that do not occur in the parsed British National Corpus are removed.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 58, "end_pos": 81, "type": "DATASET", "confidence": 0.9481234749158224}]}, {"text": "In total, the dataset contains 162 instances for 34 different verbs.", "labels": [], "entities": []}, {"text": "On average, target verbs have 20.5 substitution candidates; for individual instances of a target verb, an average of 3.9 of the substitution candidates are annotated as correct paraphrases.", "labels": [], "entities": []}, {"text": "Below, we will refer to this dataset as \"LST/SO.\"", "labels": [], "entities": [{"text": "LST/SO", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.5075997412204742}]}, {"text": "To compute the vector space, we consider only a subset of the complete set of dependency triples extracted from the parsed Gigaword corpus.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 123, "end_pos": 138, "type": "DATASET", "confidence": 0.9162016212940216}]}, {"text": "We experimented with various strategies, and found that models which consider all dependency triples exceeding certain pmi-and frequency thresholds perform best.", "labels": [], "entities": []}, {"text": "Since the dataset is rather small, we use a fourfold cross-validation method for parameter tuning: We divide the dataset into four subsets, test various parameter settings on one subset and use the parameters that perform best (in terms of GAP) to evaluate the model on the three other subsets.", "labels": [], "entities": []}, {"text": "We consider the following parameters: pmi-thresholds for the dependency triples used in the computation of the first-and second-order vectors, and frequency thresholds.", "labels": [], "entities": []}, {"text": "The parameters differ only slightly between the four subsets, and the general tendency is that good results are obtained if a low pmi-threshold (\u2264 2) is applied to filter dependency triples used in the computation of the second-order vectors, and a relatively high pmi-threshold (\u2265 4) to filter dependency triples in the computation of the first-order vectors.", "labels": [], "entities": []}, {"text": "Good performing frequency thresholds are 10 or 15.", "labels": [], "entities": []}, {"text": "The threshold values for context vectors are slightly different: a medium pmi-threshold between 2 and 4 and a low frequency threshold of 3.", "labels": [], "entities": []}, {"text": "To rank paraphrases in context, we compute contextualized vectors for the verb in the input sen-tence, i.e., a second order vector for the verb that is contextually constrained by the first order vectors of all its arguments, and compare them to the unconstrained (second-order) vectors of each paraphrase candidate, using cosine similarity.", "labels": [], "entities": []}, {"text": "For the first sentence in  Baselines.", "labels": [], "entities": []}, {"text": "We evaluate our model against a random baseline and two variants of our model: One variant (\"2 nd order uncontexualized\") simply uses contextually unconstrained second-order vectors to rank paraphrase candidates.", "labels": [], "entities": []}, {"text": "Comparing the full model to this variant will show how effective our method of contextualizing vectors is.", "labels": [], "entities": []}, {"text": "The second variant (\"1 st order contextualized\") represents verbs in context by their first order vectors that specify how often the verb co-occurs with its arguments in the parsed Gigaword corpus.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 181, "end_pos": 196, "type": "DATASET", "confidence": 0.8935396075248718}]}, {"text": "We compare our model to this baseline to demonstrate the benefit of (contextualized) second-order vectors.", "labels": [], "entities": []}, {"text": "As for the full model, we use pmi values rather than raw frequency counts as co-occurrence statistics.", "labels": [], "entities": []}, {"text": "For the LST/SO dataset, the generalized average precision, averaged overall instances in the dataset, is 45.94%, and the average P 10 is 73.11%.", "labels": [], "entities": [{"text": "LST/SO dataset", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.6350261270999908}, {"text": "generalized average precision", "start_pos": 28, "end_pos": 57, "type": "METRIC", "confidence": 0.8951487938563029}, {"text": "P 10", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9489724636077881}]}, {"text": "compares our model to the random baseline, the two variants of our model, and previous work.", "labels": [], "entities": []}, {"text": "As can be seen, our model improves about 8% in terms of GAP and almost 7% in terms of P 10 upon the two variants of our model, which in turn perform 10% above the random baseline.", "labels": [], "entities": [{"text": "GAP", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.8455532789230347}, {"text": "P 10", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9593041837215424}]}, {"text": "We conclude that both the use of second-order vectors, as well as the method used to contextualize them, are very effective for the task under consideration.", "labels": [], "entities": []}, {"text": "The table also compares our model to the model of TDP and two different instantiations of E&P's model.", "labels": [], "entities": []}, {"text": "The results for these three models are cited from.", "labels": [], "entities": []}, {"text": "We can observe that our model improves about 9% in terms of GAP and about 7% in terms of P 10 upon previous work.", "labels": [], "entities": [{"text": "GAP", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9473711848258972}]}, {"text": "Note that the results for the E&P models are based Note that the context information is the same for both words.", "labels": [], "entities": []}, {"text": "With our choice of pointwise multiplication for the composition operator \u00d7 we have ( Therefore the choice of which word is contextualized does not strongly influence their cosine similarity, and contextualizing both should not add any useful information.", "labels": [], "entities": []}, {"text": "On the contrary we found that it even lowers performance.", "labels": [], "entities": []}, {"text": "Although this could be repaired by appropriately modifying the operator \u00d7, for this experiment we stick with the easier solution of only contextualizing one of the words.", "labels": [], "entities": []}, {"text": "on a reimplementation of E&P's original modelthe P 10 -scores reported by range between 60.2 and 62.3, over a slightly lower random baseline.", "labels": [], "entities": [{"text": "E&P's original modelthe P 10 -scores", "start_pos": 25, "end_pos": 61, "type": "DATASET", "confidence": 0.8552125096321106}]}, {"text": "According to a paired t-test the differences are statistically significant at p < 0.01.", "labels": [], "entities": []}, {"text": "We now apply our model to parts of speech (POS) other than verbs.", "labels": [], "entities": []}, {"text": "The main difference between verbs on the one hand, and nouns, adjectives, and adverbs on the other hand, is that verbs typically come with a rich context-subject, object, and so on-while non-verbs often have either no dependents at all or only closed class dependents such as determiners which provide only limited contextual informations, if any at all.", "labels": [], "entities": []}, {"text": "While we can apply the same method as before also to non-verbs, we might expect it to work less well due to limited contextual: GAP-scores for non-verb paraphrases using two different methods.", "labels": [], "entities": [{"text": "GAP-scores", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.8750794529914856}]}, {"text": "In this section, we apply our model to a different word sense ranking task: Given a word win context, the task is to decide to what extent the different WordNet senses of w apply to this occurrence of w.", "labels": [], "entities": [{"text": "word sense ranking", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.6821517844994863}]}, {"text": "We use the dataset provided by.", "labels": [], "entities": []}, {"text": "The dataset contains ordinal judgments of the applicability of WordNet senses on a 5 point scale, ranging from completely different to identical for eight different lemmas in 50 different sentential contexts.", "labels": [], "entities": []}, {"text": "In this experiment, we concentrate on the three verbs in the dataset: ask, add and win.", "labels": [], "entities": []}, {"text": "Similar to, we represent different word senses by the words in the corresponding synsets.", "labels": [], "entities": []}, {"text": "For each word sense, we compute the centroid of the second-order vectors of its synset members.", "labels": [], "entities": []}, {"text": "Since synsets tend to be small (they even may contain only the target word itself), we additionally add the centroid of the sense's hypernyms, scaled down by the factor 10 (chosen as a rough heuristic without any attempt at optimization).", "labels": [], "entities": []}, {"text": "We apply the same method as in Section 4.3: For each instance in the dataset, we compute the second-order vector of the target verb, contextually constrain it by the first-order vectors of the verb's arguments, and compare the resulting vector to the vectors that represent the different WordNet senses of the verb.", "labels": [], "entities": []}, {"text": "The WordNet senses are then ranked according to the cosine similarity between their sense vector and the contextually constrained target verb vector.", "labels": [], "entities": []}, {"text": "To compare the predicted ranking to the goldstandard ranking, we use Spearman's \u03c1, a standard method to compare ranked lists to each other.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 69, "end_pos": 81, "type": "METRIC", "confidence": 0.6964010000228882}]}, {"text": "We compute \u03c1 between the similarity scores averaged overall three annotators and our model's predictions.", "labels": [], "entities": []}, {"text": "Based on agreement between human judges, estimate an upper bound \u03c1 of 0.544 for the dataset.", "labels": [], "entities": [{"text": "\u03c1", "start_pos": 65, "end_pos": 66, "type": "METRIC", "confidence": 0.9638739824295044}]}, {"text": "shows the results of our experiment.", "labels": [], "entities": []}, {"text": "The first column shows the correlation of our model's predictions with the human judgments from the gold-standard, averaged overall instances.", "labels": [], "entities": []}, {"text": "All correlations are significant (p < 0.001) as tested by approximate randomization.", "labels": [], "entities": []}, {"text": "The second column shows the results of a frequency-informed baseline, which predicts the ranking based on the order of the senses in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 133, "end_pos": 140, "type": "DATASET", "confidence": 0.9763869643211365}]}, {"text": "This (weakly supervised) baseline outperforms our unsupervised model for two of the three verbs.", "labels": [], "entities": []}, {"text": "As a final step, we explored the effect of  combining our rankings with those of the frequency baseline, by simply computing the average ranks of those two models.", "labels": [], "entities": []}, {"text": "The results are shown in the third column.", "labels": [], "entities": []}, {"text": "Performance is significantly higher than for both the original model and the frequencyinformed baseline.", "labels": [], "entities": []}, {"text": "This shows that our model captures an additional kind of information, and thus can be used to improve the frequency-based model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of Experiment 1", "labels": [], "entities": []}, {"text": " Table 3: GAP-scores for non-verb paraphrases us- ing two different methods.", "labels": [], "entities": []}, {"text": " Table 4: Correlation of model predictions and hu- man judgments", "labels": [], "entities": []}]}