{"title": [{"text": "Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "We show how web markup can be used to improve unsupervised dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.6886036098003387}]}, {"text": "Starting from raw bracketings of four common HTML tags (anchors, bold, ital-ics and underlines), we refine approximate partial phrase boundaries to yield accurate parsing constraints.", "labels": [], "entities": []}, {"text": "Conversion procedures fallout of our linguistic analysis of a newly available million-word hyper-text corpus.", "labels": [], "entities": []}, {"text": "We demonstrate that derived constraints aid grammar induction by training Klein and Manning's Dependency Model with Valence (DMV) on this data set: parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50.4%, beating previous state-of-the-art by more than 5%.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.8430351316928864}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9848362803459167}, {"text": "Wall Street Journal corpus", "start_pos": 202, "end_pos": 228, "type": "DATASET", "confidence": 0.9403543770313263}]}, {"text": "Web-scale experiments show that the DMV, perhaps because it is unlexicalized, does not benefit from orders of magnitude more annotated but noisier data.", "labels": [], "entities": []}, {"text": "Our model, trained on a single blog, generalizes to 53.3% accuracy out-of-domain, against the Brown corpus-nearly 10% higher than the previous published best.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9994033575057983}, {"text": "Brown corpus-nearly", "start_pos": 94, "end_pos": 113, "type": "DATASET", "confidence": 0.9783430993556976}]}, {"text": "The fact that web markup strongly correlates with syntactic structure may have broad applicability in NLP.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised learning of hierarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.8238976299762726}, {"text": "speech recognition", "start_pos": 205, "end_pos": 223, "type": "TASK", "confidence": 0.7097984105348587}, {"text": "machine translation", "start_pos": 228, "end_pos": 247, "type": "TASK", "confidence": 0.7881582379341125}]}, {"text": "A restricted version of this problem that targets dependencies and assumes partial annotation -sentence boundaries and part-of-speech (POS) tagging -has received much attention. were the first to beat a simple parsing heuristic, the right-branching baseline; today's state-of-the-art systems) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 119, "end_pos": 147, "type": "TASK", "confidence": 0.7238314032554627}]}, {"text": "outlined three major problems with classic EM, applied to a related problem, constituent parsing.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7812246680259705}]}, {"text": "They extended classic inside-outside re-estimation to respect any bracketing constraints included with a training corpus.", "labels": [], "entities": []}, {"text": "This conditioning on partial parses addressed all three problems, leading to: (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgments of sentence structure, which is underdetermined by unannotated text; (ii) fewer iterations needed to reach a good grammar, countering convergence properties that sharply deteriorate with the number of non-terminal symbols, due to a proliferation of local maxima; and (iii) better (in the best case, linear) time complexity per iteration, versus running time that is ordinarily cubic in both sentence length and the total number of non-terminals, rendering sufficiently large grammars computationally impractical.", "labels": [], "entities": []}, {"text": "Their algorithm sometimes found good solutions from bracketed corpora but not from raw text, supporting the view that purely unsupervised, selforganizing inference methods can miss the trees for the forest of distributional regularities.", "labels": [], "entities": []}, {"text": "This was a promising break-through, but the problem of whence to get partial bracketings was left open.", "labels": [], "entities": []}, {"text": "We suggest mining partial bracketings from a cheap and abundant natural language resource: the hyper-text mark-up that annotates web-pages.", "labels": [], "entities": []}, {"text": "For example, consider that anchor text can match linguistic constituents, such as verb phrases, exactly: ..., whereas McCain is secure on the topic, Obama <a>[ VP worries about winning the pro-Israel vote]</a>.", "labels": [], "entities": []}, {"text": "To validate this idea, we created anew data set, novel in combining areal blog's raw HTML with tree-bank-like constituent structure parses, gener-ated automatically.", "labels": [], "entities": []}, {"text": "Our linguistic analysis of the most prevalent tags (anchors, bold, italics and underlines) over its 1M + words reveals a strong connection between syntax and mark-up (all of our examples draw from this corpus), inspiring several simple techniques for automatically deriving parsing constraints.", "labels": [], "entities": []}, {"text": "Experiments with both hard and more flexible constraints, as well as with different styles and quantities of annotated training datathe blog, web news and the web itself, confirm that mark-up-induced constraints consistently improve (otherwise unsupervised) dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 258, "end_pos": 276, "type": "TASK", "confidence": 0.7440551221370697}]}], "datasetContent": [{"text": "The appeal of unsupervised parsing lies in its ability to learn from surface text alone; but (intrinsic) evaluation still requires parsed sentences.", "labels": [], "entities": []}, {"text": "Following, we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty subtrees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic \"head-percolation\" rules to convert the rest, as is standard practice.: Counts of sentences, tokens and (unique) bracketings for BLOG p , restricted to only those sentences having at least one bracketing no shorter than the length cutoff (but shorter than the sentence).", "labels": [], "entities": []}, {"text": "Our primary reference sets are derived from the Penn English Treebank's Wall Street Journal portion (): WSJ45 (sentences with fewer than 46 tokens) and Section 23 of WSJ \u221e (all sentence lengths).", "labels": [], "entities": [{"text": "Penn English Treebank's Wall Street Journal portion", "start_pos": 48, "end_pos": 99, "type": "DATASET", "confidence": 0.9448560625314713}, {"text": "WSJ45", "start_pos": 104, "end_pos": 109, "type": "DATASET", "confidence": 0.7409646511077881}, {"text": "WSJ \u221e", "start_pos": 166, "end_pos": 171, "type": "DATASET", "confidence": 0.8542298078536987}]}, {"text": "We also evaluate on Brown100, similarly derived from the parsed portion of the Brown corpus).", "labels": [], "entities": [{"text": "Brown100", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.7468127012252808}, {"text": "Brown corpus", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.8268602788448334}]}, {"text": "While we use WSJ45 and WSJ15 to train baseline models, the bulk of our experiments is with web data.", "labels": [], "entities": [{"text": "WSJ45", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.9601299166679382}, {"text": "WSJ15", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.90207439661026}]}, {"text": "We implemented the DMV (), consulting the details of ().", "labels": [], "entities": [{"text": "DMV", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.7378019094467163}]}, {"text": "Crucially, we swapped out inside-outside re-estimation in favor of Viterbi training.", "labels": [], "entities": [{"text": "re-estimation", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.9089528918266296}]}, {"text": "Not only is it better-suited to the general problem (see \u00a77.1), but it also admits a trivial implementation of (most of) the dependency constraints we proposed.", "labels": [], "entities": []}, {"text": "14 Six settings parameterized each run: \u2022 INIT: 0 -default, uniform initialization; or 1 -a high quality initializer, pre-trained using Ad-Hoc * (Spitkovsky et al., 2010a): we chose the Laplace-smoothed model trained at WSJ15 (the \"sweet spot\" data gradation) but initialized off WSJ8, since that ad-hoc harmonic initializer has the best cross-entropy on WSJ15 (see).", "labels": [], "entities": [{"text": "INIT", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9986030459403992}, {"text": "WSJ15", "start_pos": 220, "end_pos": 225, "type": "DATASET", "confidence": 0.9672548770904541}, {"text": "WSJ8", "start_pos": 280, "end_pos": 284, "type": "DATASET", "confidence": 0.9768437743186951}, {"text": "WSJ15", "start_pos": 355, "end_pos": 360, "type": "DATASET", "confidence": 0.9742883443832397}]}, {"text": "\u2022 GENRE: 0 -default, baseline training on WSJ; else, uses 1 -BLOG t ; 2 -NEWS; or 3 -WEB.", "labels": [], "entities": [{"text": "GENRE", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.8766342401504517}, {"text": "WSJ", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.9189942479133606}, {"text": "BLOG", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9795411229133606}, {"text": "NEWS", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9066792130470276}, {"text": "WEB", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.9348236322402954}]}, {"text": "\u2022 SCOPE: 0 -default, uses all sentences up to length 45; if 1, trains using sentences up to length 15; if 2, re-trains on sentences up to length 45, starting from the solution to sentences up to length 15, as recommended by.", "labels": [], "entities": []}, {"text": "\u2022 CONSTR: if 4, strict; if 3, loose; and if 2, sprawl.", "labels": [], "entities": [{"text": "CONSTR", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9632750749588013}]}, {"text": "We did not implement level 1, tear.", "labels": [], "entities": [{"text": "tear", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9949224591255188}]}, {"text": "Overconstrained sentences are re-attempted at successively lower levels until they become possible to parse, if necessary at the lowest (default) level 0.", "labels": [], "entities": []}, {"text": "\u2022 TRIM: if 1, discards any sentence without a single multi-token mark-up (shorter than its length).", "labels": [], "entities": [{"text": "TRIM", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9756694436073303}]}, {"text": "\u2022 ADAPT: if 1, upon convergence, initializes retraining on WSJ45 using the solution to <GENRE>, attempting domain adaptation ().", "labels": [], "entities": [{"text": "ADAPT", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9825626611709595}, {"text": "WSJ45", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.9717292189598083}, {"text": "domain adaptation", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.696452334523201}]}, {"text": "These make for 294 meaningful combinations.", "labels": [], "entities": []}, {"text": "We judged each one by its accuracy on WSJ45, using standard directed scoring -the fraction of correct dependencies over randomized \"best\" parse trees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9994388222694397}, {"text": "WSJ45", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.9699634909629822}]}, {"text": "We analyze the benefits of Viterbi training in a companion paper (, which dedicates more space to implementation and to the WSJ baselines used here.", "labels": [], "entities": [{"text": "WSJ baselines", "start_pos": 124, "end_pos": 137, "type": "DATASET", "confidence": 0.929209977388382}]}, {"text": "At level 4, <b> X<u> Y</b> Z</u> is over-constrained.", "labels": [], "entities": []}, {"text": "Evaluation on Section 23 of WSJ and Brown reveals that blog-training beats all published stateof-the-art numbers in every traditionally-reported length cutoff category, with news-training not far behind.", "labels": [], "entities": [{"text": "Section 23 of WSJ and Brown", "start_pos": 14, "end_pos": 41, "type": "DATASET", "confidence": 0.725988358259201}]}, {"text": "Here is a mini-preview of these results, for Section 23 of WSJ10 and WSJ \u221e (from): WSJ10 WSJ \u221e ( 62.0 42.2 57.1 45.0 NEWS-best 67.3 50.1 BLOGt-best 69.3 50.4 68.8: Directed accuracies on Section 23 of WSJ{10, \u221e } for three recent state-of-the-art systems and our best runs (as judged against WSJ45) for NEWS and BLOG t (more details in).", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.8580365180969238}, {"text": "WSJ \u221e", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.9040583670139313}, {"text": "WSJ10 WSJ \u221e", "start_pos": 83, "end_pos": 94, "type": "DATASET", "confidence": 0.8119870821634928}, {"text": "BLOGt-best", "start_pos": 137, "end_pos": 147, "type": "METRIC", "confidence": 0.9085525870323181}, {"text": "WSJ45", "start_pos": 292, "end_pos": 297, "type": "DATASET", "confidence": 0.9659594297409058}, {"text": "NEWS", "start_pos": 303, "end_pos": 307, "type": "DATASET", "confidence": 0.8599028587341309}, {"text": "BLOG", "start_pos": 312, "end_pos": 316, "type": "METRIC", "confidence": 0.8311237692832947}]}, {"text": "Since our experimental setup involved testing nearly three hundred models simultaneously, we must take extreme care in analyzing and interpreting these results, to avoid falling prey to any looming \"data-snooping\" biases.", "labels": [], "entities": []}, {"text": "Ina sufficiently large pool of models, where each is trained using a randomized and/or chaotic procedure (such as ours), the best may look good due to pure chance.", "labels": [], "entities": []}, {"text": "We appealed to three separate diagnostics to convince ourselves that our best results are not noise.", "labels": [], "entities": []}, {"text": "The most radical approach would be to write off WSJ as a development set and to focus only on the results from the held-out Brown corpus.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.7703443169593811}, {"text": "Brown corpus", "start_pos": 124, "end_pos": 136, "type": "DATASET", "confidence": 0.9304317235946655}]}, {"text": "It was initially intended as a test of out-of-domain generalization, but since Brown was in noway involved in selecting the best models, it also qualifies as a blind evaluation set.", "labels": [], "entities": []}, {"text": "We observe that our best models perform even better (and gain more -see) on Brown than on WSJ -a strong indication that our selection process has not overfitted.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.860539436340332}]}, {"text": "Our second diagnostic is a closer look at WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.8395119905471802}]}, {"text": "Since we cannot graph the full (six-dimensional) set of results, we begin with a simple linear regression, using accuracy on WSJ45 as the dependent variable.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9993646740913391}, {"text": "WSJ45", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.956720769405365}]}, {"text": "We prefer this full factorial design to the more traditional ablation studies because it allows us to account for and to incorporate every single experimental data point incurred along the: Counts of sentences, tokens and (unique) bracketings for web-based data sets; trimmed versions, restricted to only those sentences having at least one multi-token bracketing, are indicated by a prime ( \u2032 ). way.", "labels": [], "entities": []}, {"text": "Its output is a coarse, high-level summary of our runs, showing which factors significantly contribute to changes in error rate on WSJ45: We use a standard convention: *** for p < 0.001; ** for p < 0.01 (very signif.); and * for p < 0.05 (signif.).", "labels": [], "entities": [{"text": "WSJ45", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.9732114672660828}]}, {"text": "The default training mode (all parameters zero) is estimated to score 39.9%.", "labels": [], "entities": []}, {"text": "A good initializer gives the biggest (double-digit) gain; both domain adaptation and constraints also make a positive impact.", "labels": [], "entities": []}, {"text": "Throwing away unannotated data hurts, as does training out-of-domain (the blog is least bad; the web is worst).", "labels": [], "entities": []}, {"text": "Of course, this overview should not betaken too seriously.", "labels": [], "entities": []}, {"text": "Overly simplistic, a first order model ignores interactions between parameters.", "labels": [], "entities": []}, {"text": "Furthermore, a least squares fit aims to capture central tendencies, whereas we are more interested in outliers -the best-performing runs.", "labels": [], "entities": []}, {"text": "A major imperfection of the simple regression model is that helpful factors that require an interaction to \"kick in\" may not, on their own, appear statistically significant.", "labels": [], "entities": []}, {"text": "Our third diagnostic is to examine parameter settings that give rise to the best-performing models, looking out for combinations that consistently deliver superior results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Counts of sentences, tokens and (unique) bracketings for BLOG p , restricted to only those  sentences having at least one bracketing no shorter than the length cutoff (but shorter than the sentence).", "labels": [], "entities": []}, {"text": " Table 3: Top 50% of marked POS tag sequences.", "labels": [], "entities": [{"text": "POS tag sequences", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.47395333647727966}]}, {"text": " Table 4: Top 99% of dominating non-terminals.", "labels": [], "entities": []}, {"text": " Table 5: Top 15 marked productions, viewed as constituents (left) and as dependencies (right), after  recursively expanding any internal nodes that did not align with the bracketing (underlined). Tabulated  dependencies were collapsed, dropping any dependents that fell entirely in the same region as their parent  (i.e., both inside the bracketing, both to its left or both to its right), keeping only crossing attachments.", "labels": [], "entities": []}, {"text": " Table 6: Directed accuracies on Section 23 of  WSJ{10, \u221e } for three recent state-of-the-art sys- tems and our best runs (as judged against WSJ45)  for NEWS and BLOG t (more details in", "labels": [], "entities": [{"text": "Section 23 of  WSJ{10, \u221e }", "start_pos": 33, "end_pos": 59, "type": "DATASET", "confidence": 0.8737613095177544}, {"text": "WSJ45", "start_pos": 141, "end_pos": 146, "type": "DATASET", "confidence": 0.9671747088432312}, {"text": "NEWS", "start_pos": 153, "end_pos": 157, "type": "DATASET", "confidence": 0.882706880569458}, {"text": "BLOG t", "start_pos": 162, "end_pos": 168, "type": "DATASET", "confidence": 0.6922177076339722}]}, {"text": " Table 7: Counts of sentences, tokens and (unique) bracketings for web-based data sets; trimmed versions,  restricted to only those sentences having at least one multi-token bracketing, are indicated by a prime ( \u2032 ).", "labels": [], "entities": []}, {"text": " Table 8: Accuracies on Section 23 of WSJ{10, 20, \u221e } and Brown100 for three recent state-of-the-art  systems, our default run, and our best runs (judged by accuracy on WSJ45) for each of four training sets.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9970515966415405}, {"text": "WSJ", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.878410816192627}, {"text": "Brown100", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.7035173177719116}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9989480376243591}, {"text": "WSJ45", "start_pos": 169, "end_pos": 174, "type": "DATASET", "confidence": 0.9422407150268555}]}]}