{"title": [{"text": "Exploring Syntactic Structural Features for Sub-Tree Alignment using Bilingual Tree Kernels", "labels": [], "entities": [{"text": "Sub-Tree Alignment", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7215007096529007}]}], "abstractContent": [{"text": "We propose Bilingual Tree Kernels (BTKs) to capture the structural similarities across a pair of syntactic translational equivalences and apply BTKs to sub-tree alignment along with some plain features.", "labels": [], "entities": []}, {"text": "Our study reveals that the structural features embedded in a bilingual parse tree pair are very effective for sub-tree alignment and the bilingual tree kernels can well capture such features.", "labels": [], "entities": [{"text": "sub-tree alignment", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.6369960904121399}]}, {"text": "The experimental results show that our approach achieves a significant improvement on both gold standard tree bank and automatically parsed tree pairs against a heuris-tic similarity based method.", "labels": [], "entities": []}, {"text": "We further apply the sub-tree alignment in machine translation with two methods.", "labels": [], "entities": [{"text": "sub-tree alignment", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.720876932144165}, {"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.708606168627739}]}, {"text": "It is suggested that the sub-tree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 124, "end_pos": 138, "type": "TASK", "confidence": 0.6945972144603729}]}], "introductionContent": [{"text": "Syntax based Statistical Machine Translation (SMT) systems allow the translation process to be more grammatically performed, which provides decent reordering capability.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.8172644476095835}]}, {"text": "However, most of the syntax based systems construct the syntactic translation rules based on word alignment, which not only suffers from the pipeline errors, but also fails to effectively utilize the syntactic structural features.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.7172428667545319}]}, {"text": "To address those deficiencies, attempt to directly capture the syntactic translational equivalences by automatically conducting sub-tree alignment, which can be defined as follows: A sub-tree alignment process pairs up sub-tree pairs across bilingual parse trees whose contexts are semantically translational equivalent.", "labels": [], "entities": []}, {"text": "According to, a sub-tree aligned parse tree pair follows the following criteria: (i) anode can only be linked once; (ii) descendants of a source linked node may only link to descendants of its target linked counterpart; (iii) ancestors of a source linked node may only link to ancestors of its target linked counterpart.", "labels": [], "entities": []}, {"text": "By sub-tree alignment, translational equivalent sub-tree pairs are coupled as aligned counterparts.", "labels": [], "entities": []}, {"text": "Each pair consists of both the lexical constituents and their maximum tree structures generated over the lexical sequences in the original parse trees.", "labels": [], "entities": []}, {"text": "Due to the 1-to-1 mapping between sub-trees and tree nodes, sub-tree alignment can also be considered as node alignment by conducting multiple links across the internal nodes as shown in.", "labels": [], "entities": [{"text": "sub-tree alignment", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7268783152103424}, {"text": "node alignment", "start_pos": 105, "end_pos": 119, "type": "TASK", "confidence": 0.7032182514667511}]}, {"text": "Previous studies conduct sub-tree alignments by either using a rule based method or conducting some similarity measurement only based on lexical features.", "labels": [], "entities": []}, {"text": "conduct sub-tree alignment by using some heuristic rules, lack of extensibility and generality.", "labels": [], "entities": [{"text": "sub-tree alignment", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.6853353530168533}]}, {"text": "Figure 1: Sub-tree alignment as referred to propose some score functions based on the lexical similarity and co-occurrence.", "labels": [], "entities": [{"text": "Sub-tree alignment", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7351714372634888}]}, {"text": "These works fail to utilize the structural features, rendering the syntactic rich task of sub-tree alignment less convincing and attractive.", "labels": [], "entities": [{"text": "sub-tree alignment", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.6941124945878983}]}, {"text": "This maybe due to the fact that the syntactic structures in a parse tree pair are hard to describe using plain features.", "labels": [], "entities": []}, {"text": "In addition, explicitly utilizing syntactic tree fragments results in exponentially high dimensional feature vectors, which is hard to compute.", "labels": [], "entities": []}, {"text": "Alternatively, convolution parse tree kernels), which implicitly explore the tree structure information, have been successfully applied in many NLP tasks, such as Semantic parsing) and Relation Extraction ().", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 163, "end_pos": 179, "type": "TASK", "confidence": 0.8488050401210785}, {"text": "Relation Extraction", "start_pos": 185, "end_pos": 204, "type": "TASK", "confidence": 0.9571272730827332}]}, {"text": "However, all those studies are carried out in monolingual tasks.", "labels": [], "entities": []}, {"text": "In multilingual tasks such as machine translation, tree kernels are seldom applied.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7924554347991943}]}, {"text": "In this paper, we propose Bilingual Tree Kernels (BTKs) to model the bilingual translational equivalences, in our case, to conduct sub-tree alignment.", "labels": [], "entities": [{"text": "sub-tree alignment", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.6637845486402512}]}, {"text": "This is motivated by the decent effectiveness of tree kernels in expressing the similarity between tree structures.", "labels": [], "entities": []}, {"text": "We propose two kinds of BTKs named dependent Bilingual Tree Kernel (dBTK), which takes the sub-tree pair as a whole and independent Bilingual Tree Kernel (iBTK), which individually models the source and the target sub-trees.", "labels": [], "entities": []}, {"text": "Both kernels can be utilized within different feature spaces using various representations of the sub-structures.", "labels": [], "entities": []}, {"text": "Along with BTKs, various lexical and syntactic structural features are proposed to capture the correspondence between bilingual sub-trees using a polynomial kernel.", "labels": [], "entities": []}, {"text": "We then attempt to combine the polynomial kernel and BTKs to construct a composite kernel.", "labels": [], "entities": [{"text": "BTKs", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.7237525582313538}]}, {"text": "The sub-tree alignment task is considered as a binary classification problem.", "labels": [], "entities": [{"text": "sub-tree alignment task", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7770387530326843}]}, {"text": "We employ a kernel based classifier with the composite kernel to classify each candidate of sub-tree pair as aligned or unaligned.", "labels": [], "entities": []}, {"text": "Then a greedy search algorithm is performed according to the three criteria of sub-tree alignment within the space of candidates classified as aligned.", "labels": [], "entities": []}, {"text": "We evaluate the sub-tree alignment on both the gold standard tree bank and an automatically parsed corpus.", "labels": [], "entities": [{"text": "gold standard tree bank", "start_pos": 47, "end_pos": 70, "type": "DATASET", "confidence": 0.8657107353210449}]}, {"text": "Experimental results show that the proposed BTKs benefit sub-tree alignment on both corpora, along with the lexical features and the plain structural features.", "labels": [], "entities": []}, {"text": "Further experiments in machine translation also suggest that the obtained sub-tree alignment can improve the performance of both phrase and syntax based SMT systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7771511077880859}, {"text": "SMT", "start_pos": 153, "end_pos": 156, "type": "TASK", "confidence": 0.8201600313186646}]}], "datasetContent": [{"text": "In order to evaluate the effectiveness of the alignment model and its capability in the applications requiring syntactic translational equivalences, we employ two corpora to carryout the sub-tree alignment evaluation.", "labels": [], "entities": []}, {"text": "The first is HIT gold standard English Chinese parallel tree bank referred as HIT corpus . The other is the automatically parsed bilingual tree pairs selected from FBIS corpus (allowing minor parsing errors) with human annotated sub-tree alignment.", "labels": [], "entities": [{"text": "HIT gold standard English Chinese parallel tree bank", "start_pos": 13, "end_pos": 65, "type": "DATASET", "confidence": 0.9117485284805298}, {"text": "HIT corpus", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.9469243288040161}, {"text": "FBIS corpus", "start_pos": 164, "end_pos": 175, "type": "DATASET", "confidence": 0.9491456747055054}]}, {"text": "We use SVM with binary classes as the classifier.", "labels": [], "entities": []}, {"text": "In case of the implementation, we modify the Tree Kernel tool) and SVMLight).", "labels": [], "entities": []}, {"text": "The coefficient \ud97b\udf59 \ud97b\udf59 for the composite kernel are tuned with respect to F-measure (F) on the development set of HIT corpus.", "labels": [], "entities": [{"text": "F-measure (F)", "start_pos": 71, "end_pos": 84, "type": "METRIC", "confidence": 0.9437438249588013}, {"text": "HIT corpus", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.9430666565895081}]}, {"text": "We empirically set C=2.4 for SVM and use 0.23, the default parameter 0.4 for BTKs.", "labels": [], "entities": [{"text": "BTKs", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8968310356140137}]}, {"text": "Since the negative training instances largely overwhelm the positive instances, we prune the negative instances using the thresholds according to the lexical feature functions (\ud97b\udf59 \ud97b\udf59 , , \ud97b\udf59 , , \ud97b\udf59 , , \ud97b\udf59 ) and online structural feature functions ( \ud97b\udf59 \ud97b\udf59 , , \ud97b\udf59 , , \ud97b\udf59 ).", "labels": [], "entities": []}, {"text": "Those thresholds are also tuned on the development set of HIT corpus with respect to F-measure.", "labels": [], "entities": [{"text": "HIT corpus", "start_pos": 58, "end_pos": 68, "type": "DATASET", "confidence": 0.9096764326095581}, {"text": "F-measure", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.8513327240943909}]}, {"text": "To learn the lexical and word alignment features for both the proposed model and the baseline method, we train GIZA++ on the entire FBIS bilingual corpus (240k).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7112084031105042}, {"text": "FBIS bilingual corpus", "start_pos": 132, "end_pos": 153, "type": "DATASET", "confidence": 0.7562402685483297}]}, {"text": "The evaluation is conducted by means of Precision (P), Recall (R) and Fmeasure (F).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.9489637911319733}, {"text": "Recall (R)", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9666526317596436}, {"text": "Fmeasure (F)", "start_pos": 70, "end_pos": 82, "type": "METRIC", "confidence": 0.9723513126373291}]}, {"text": "Compared with the adoption of word alignment, translational equivalences generated from structural alignment tend to be more grammatically An elementary tree is a fragment whose leaf nodes can be either non-terminal symbols or terminal symbols.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.747198075056076}]}, {"text": "However, utilizing syntactic translational equivalences alone for machine translation loses the capability of modeling non-syntactic phrases (.", "labels": [], "entities": [{"text": "syntactic translational equivalences", "start_pos": 19, "end_pos": 55, "type": "TASK", "confidence": 0.7865501840909322}, {"text": "machine translation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7416169047355652}]}, {"text": "Consequently, instead of using phrases constraint by sub-tree alignment alone, we attempt to combine word alignment and sub-tree alignment and deploy the capability of both with two methods.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 101, "end_pos": 115, "type": "TASK", "confidence": 0.7410156279802322}]}, {"text": "\u2022 Directly Concatenate (DirC) is operated by directly concatenating the rule set genereted from sub-tree alignment and the original rule set generated from word alignment ().", "labels": [], "entities": []}, {"text": "As shown in, we gain minor improvement in the Bleu score for all configurations.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.984334796667099}]}, {"text": "\u2022 Alternatively, we proposed anew approach to generate the rule set from the scratch.", "labels": [], "entities": []}, {"text": "We constrain the bilingual phrases to be consistent with Either Word alignment or Sub-tree alignment (EWoS) instead of being originally consistent with the word alignment only.", "labels": [], "entities": []}, {"text": "The method helps tailoring the rule set decently without redundant counts for syntactic rules.", "labels": [], "entities": []}, {"text": "The performance is further improved compared to DirC in all systems.", "labels": [], "entities": [{"text": "DirC", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.5201549530029297}]}, {"text": "The findings suggest that with the modeling of non-syntactic phrases maintained, more emphasis on syntactic phrases can benefit both the phrase and syntax based SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.9633694887161255}]}, {"text": "In addition to the intrinsic alignment evaluation, we further conduct the extrinsic MT evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9543565511703491}]}, {"text": "We explore the effectiveness of sub-tree alignment for both phrase based and linguistically motivated syntax based SMT systems.", "labels": [], "entities": [{"text": "sub-tree alignment", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7218839526176453}, {"text": "SMT", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.7267003655433655}]}, {"text": "In the experiments, we train the translation model on FBIS corpus (7.2M (Chinese) + 9.2M (English) words in 240,000 sentence pairs) and train a 4-gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits).", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.9012947976589203}, {"text": "English Gigaword corpus", "start_pos": 195, "end_pos": 218, "type": "DATASET", "confidence": 0.7532206972440084}, {"text": "SRILM Toolkits", "start_pos": 242, "end_pos": 256, "type": "DATASET", "confidence": 0.9487064778804779}]}, {"text": "We use these sentences with less than 50 characters from the NIST MT-2002 test set as the development set (to speedup tuning for syntax based system) and the NIST MT-2005 test set as our test set.", "labels": [], "entities": [{"text": "NIST MT-2002 test set", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.9314297586679459}, {"text": "NIST MT-2005 test set", "start_pos": 158, "end_pos": 179, "type": "DATASET", "confidence": 0.937429279088974}]}, {"text": "We use the Stanford parser ( to parse bilingual sentences on the training set and Chinese sentences on the development and test set.", "labels": [], "entities": []}, {"text": "The evaluation metric is case-sensitive BLEU-4.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9819338917732239}]}, {"text": "For the phrase based system, we use Moses () with its default settings.", "labels": [], "entities": []}, {"text": "For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (.", "labels": [], "entities": []}, {"text": "The STSG based decoder uses a pair of elementary tree 3 as a basic translation unit.", "labels": [], "entities": []}, {"text": "Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (.", "labels": [], "entities": [{"text": "Synchronous Tree Sequence Substitution Grammar: STSSG", "start_pos": 122, "end_pos": 175, "type": "TASK", "confidence": 0.6581678220203945}]}, {"text": "We implement the STSG/STSSG based model in the Pisces decoder with the identical features and settings in.", "labels": [], "entities": [{"text": "Pisces decoder", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.8645288348197937}]}, {"text": "In the Pisces decoder, the STSSG based decoder translates each span iteratively in a bottom up manner which guarantees that when translating a source span, any of its subspans is already translated.", "labels": [], "entities": []}, {"text": "The STSG based decoding can be easily performed with the STSSG decoder by restricting the translation rule set to be elementary tree pairs only.", "labels": [], "entities": [{"text": "STSSG decoder", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.8478653430938721}]}, {"text": "As for the alignment setting, we use the word alignment trained on the entire FBIS (240k) corpus by GIZA++ with heuristic grow-diag-final for both Moses and the syntax system.", "labels": [], "entities": [{"text": "FBIS (240k) corpus", "start_pos": 78, "end_pos": 96, "type": "DATASET", "confidence": 0.9200284242630005}]}, {"text": "For sub-treealignment, we use the above word alignment to learn lexical/word alignment feature, and train with the FBIS training corpus (200) using the composite kernel of Plain+dBTK-Root+iBTK-RdSTT.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.7097770869731903}, {"text": "word alignment", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.6815403401851654}, {"text": "FBIS training corpus", "start_pos": 115, "end_pos": 135, "type": "DATASET", "confidence": 0.8788241942723592}]}], "tableCaptions": [{"text": " Table 2. Statistics of FBIS selected Corpus", "labels": [], "entities": []}, {"text": " Table 4. Structure feature contribution for FBIS test set", "labels": [], "entities": [{"text": "FBIS test set", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.8274743755658468}]}, {"text": " Table 3. Structure feature contribution for HIT test set  *Plain= Lex +Online Str", "labels": [], "entities": [{"text": "HIT test set", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.8319101134936014}, {"text": "Lex +Online Str", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.7339325398206711}]}]}