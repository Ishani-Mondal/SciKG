{"title": [], "abstractContent": [{"text": "We propose CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language , based on matrix multiplication.", "labels": [], "entities": []}, {"text": "We argue for the structural and cognitive plau-sibility of this model and show that it is able to cover and combine various common compositional NLP approaches ranging from statistical word space models to symbolic grammar formalisms.", "labels": [], "entities": []}], "introductionContent": [{"text": "In computational linguistics and information retrieval, Vector Space Models ( and its variations -such as Word Space Models, Hyperspace Analogue to Language, or Latent Semantic Analysis) -have become a mainstream paradigm for text representation.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7504906952381134}, {"text": "Latent Semantic Analysis", "start_pos": 161, "end_pos": 185, "type": "TASK", "confidence": 0.6294682423273722}, {"text": "text representation", "start_pos": 226, "end_pos": 245, "type": "TASK", "confidence": 0.7720155119895935}]}, {"text": "Vector Space Models (VSMs) have been empirically justified by results from cognitive science).", "labels": [], "entities": [{"text": "Vector Space Models (VSMs)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7635387579600016}]}, {"text": "They embody the distributional hypothesis of meaning, according to which the meaning of words is defined by contexts in which they (co-)occur.", "labels": [], "entities": []}, {"text": "Depending on the specific model employed, these contexts can be either local (the co-occurring words), or global (a sentence or a paragraph or the whole document).", "labels": [], "entities": []}, {"text": "Indeed, VSMs proved to perform well in a number of tasks requiring computation of semantic relatedness between words, such as synonymy identification, automatic thesaurus construction, semantic priming, and word sense disambiguation.", "labels": [], "entities": [{"text": "synonymy identification", "start_pos": 126, "end_pos": 149, "type": "TASK", "confidence": 0.7117878198623657}, {"text": "automatic thesaurus construction", "start_pos": 151, "end_pos": 183, "type": "TASK", "confidence": 0.625699539979299}, {"text": "word sense disambiguation", "start_pos": 207, "end_pos": 232, "type": "TASK", "confidence": 0.7105526328086853}]}, {"text": "Until recently, little attention has been paid to the task of modeling more complex conceptual structures with such models, which constitutes a crucial barrier for semantic vector models on the way to model language).", "labels": [], "entities": []}, {"text": "An emerging area of research receiving more and more attention among the advocates of distributional models addresses the methods, algorithms, and evaluation strategies for representing compositional aspects of language within a VSM framework.", "labels": [], "entities": []}, {"text": "This requires novel modeling paradigms, as most VSMs have been predominantly used for meaning representation of single words and the key problem of common bag-of-words-based VSMs is that word order information and thereby the structure of the language is lost.", "labels": [], "entities": [{"text": "meaning representation of single words", "start_pos": 86, "end_pos": 124, "type": "TASK", "confidence": 0.8239556431770325}]}, {"text": "There are approaches underway to workout a combined framework for meaning representation using both the advantages of symbolic and distributional methods.", "labels": [], "entities": [{"text": "meaning representation", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.8355514705181122}]}, {"text": "suggest a conceptual model which unites symbolic and distributional representations by means of traversing the parse tree of a sentence and applying a tensor product for combining vectors of the meanings of words with the vectors of their roles.", "labels": [], "entities": []}, {"text": "The model is further elaborated by.", "labels": [], "entities": []}, {"text": "To overcome the aforementioned difficulties with VSMs and work towards a tight integration of symbolic and distributional approaches, we propose a Compositional Matrix-Space Model (CMSM) which employs matrices instead of vectors and makes use of matrix multiplication as the one and only composition operation.", "labels": [], "entities": [{"text": "VSMs", "start_pos": 49, "end_pos": 53, "type": "TASK", "confidence": 0.9094076156616211}]}, {"text": "The paper is structured as follows: We start by providing the necessary basic notions in linear algebra in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we give a formal account of the concept of compositionality, introduce our model, and argue for the plausibility of CMSMs in the light of structural and cognitive considerations.", "labels": [], "entities": []}, {"text": "Section 4 shows how common VSM approaches to compositionality can be captured by CMSMs while Section 5 illustrates the capabilities of our model to likewise cover symbolic approaches.", "labels": [], "entities": []}, {"text": "In Section 6, we demonstrate how several CMSMs can be combined into one model.", "labels": [], "entities": []}, {"text": "We provide an overview of related work in Section 7 before we conclude and point out avenues for further research in Section 8.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}