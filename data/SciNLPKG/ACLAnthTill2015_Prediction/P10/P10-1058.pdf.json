{"title": [], "abstractContent": [{"text": "In this paper we present a joint content selection and compression model for single-document summarization.", "labels": [], "entities": [{"text": "single-document summarization", "start_pos": 77, "end_pos": 106, "type": "TASK", "confidence": 0.5312271565198898}]}, {"text": "The model operates over a phrase-based representation of the source document which we obtain by merging information from PCFG parse trees and dependency graphs.", "labels": [], "entities": []}, {"text": "Using an integer linear programming formulation , the model learns to select and combine phrases subject to length, coverage and grammar constraints.", "labels": [], "entities": []}, {"text": "We evaluate the approach on the task of generating \"story highlights\"-a small number of brief, self-contained sentences that allow readers to quickly gather information on news stories.", "labels": [], "entities": []}, {"text": "Experimental results show that the model's output is comparable to human-written highlights in terms of both grammaticality and content.", "labels": [], "entities": []}], "introductionContent": [{"text": "Summarization is the process of condensing a source text into a shorter version while preserving its information content.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9826596975326538}]}, {"text": "Humans summarize on a daily basis and effortlessly, but producing high quality summaries automatically remains a challenge.", "labels": [], "entities": [{"text": "summarize", "start_pos": 7, "end_pos": 16, "type": "TASK", "confidence": 0.9703962802886963}]}, {"text": "The difficulty lies primarily in the nature of the task which is complex, must satisfy many constraints (e.g., summary length, informativeness, coherence, grammaticality) and ultimately requires wide-coverage text understanding.", "labels": [], "entities": []}, {"text": "Since the latter is beyond the capabilities of current NLP technology, most work today focuses on extractive summarization, where a summary is created simply by identifying and subsequently concatenating the most important sentences in a document.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.6592767536640167}]}, {"text": "Without a great deal of linguistic analysis, it is possible to create summaries fora wide range of documents.", "labels": [], "entities": []}, {"text": "Unfortunately, extracts are often documents of low readability and text quality and contain much redundant information.", "labels": [], "entities": []}, {"text": "This is in marked contrast with hand-written summaries which often combine several pieces of information from the original document) and exhibit many rewrite operations such as substitutions, insertions, deletions, or reorderings.", "labels": [], "entities": []}, {"text": "Sentence compression is often regarded as a promising first step towards ameliorating some of the problems associated with extractive summarization.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9541003406047821}, {"text": "extractive summarization", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.6223710775375366}]}, {"text": "The task is commonly expressed as a word deletion problem.", "labels": [], "entities": [{"text": "word deletion problem", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7423339188098907}]}, {"text": "It involves creating a short grammatical summary of a single sentence, by removing elements that are considered extraneous, while retaining the most important information ().", "labels": [], "entities": []}, {"text": "Interfacing extractive summarization with a sentence compression module could improve the conciseness of the generated summaries and render them more informative.", "labels": [], "entities": []}, {"text": "Despite the bulk of work on sentence compression and summarization (see for overviews) only a handful of approaches attempt to do both in a joint model.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8155907988548279}, {"text": "summarization", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.9682139754295349}]}, {"text": "One reason for this might be the performance of sentence compression systems which falls short of attaining grammaticality levels of human output.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7388998717069626}]}, {"text": "For example, evaluate a range of state-of-the-art compression systems across different domains and show that machine generated compressions are consistently perceived as worse than the human gold standard.", "labels": [], "entities": []}, {"text": "Another reason is the summarization objective itself.", "labels": [], "entities": [{"text": "summarization", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9658615589141846}]}, {"text": "If our goal is to summarize news articles, then we maybe better off selecting the first n sentences of the document.", "labels": [], "entities": [{"text": "summarize news articles", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.928946336110433}]}, {"text": "This \"lead\" baseline may err on the side of verbosity but at least will be grammatical, and it has indeed proved extremely hard to outperform by more sophisticated methods.", "labels": [], "entities": []}, {"text": "In this paper we propose a model for sum-marization that incorporates compression into the task.", "labels": [], "entities": []}, {"text": "A key insight in our approach is to formulate summarization as a phrase rather than sentence extraction problem.", "labels": [], "entities": [{"text": "formulate summarization", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.8384282290935516}, {"text": "sentence extraction", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.6908237338066101}]}, {"text": "Compression falls naturally out of this formulation as only phrases deemed important should appear in the summary.", "labels": [], "entities": [{"text": "Compression", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.8302432894706726}]}, {"text": "Obviously, our output summaries must meet additional requirements such as sentence length, overall length, topic coverage and, importantly, grammaticality.", "labels": [], "entities": []}, {"text": "We combine phrase and dependency information into a single data structure, which allows us to express grammaticality as constraints across phrase dependencies.", "labels": [], "entities": []}, {"text": "We encode these constraints through the use of integer linear programming (ILP), a well-studied optimization framework that is able to search the entire solution space efficiently.", "labels": [], "entities": []}, {"text": "We apply our model to the task of generating highlights fora single document.", "labels": [], "entities": []}, {"text": "Examples of CNN news articles with human-authored highlights are shown in.", "labels": [], "entities": []}, {"text": "Highlights give a brief overview of the article to allow readers to quickly gather information on stories, and usually appear as bullet points.", "labels": [], "entities": []}, {"text": "Importantly, they represent the gist of the entire document and thus often differ substantially from the first n sentences in the article (.", "labels": [], "entities": []}, {"text": "They are also highly compressed, written in a telegraphic style and thus provide an excellent testbed for models that generate compressed summaries.", "labels": [], "entities": []}, {"text": "Experimental results show that our model's output is comparable to hand-written highlights both in terms of grammaticality and informativeness.", "labels": [], "entities": []}], "datasetContent": [{"text": "Training We obtained phrase-based salience scores using a supervised machine learning algorithm.", "labels": [], "entities": []}, {"text": "210 document-highlight pairs were chosen randomly from our corpus (see Section 3).", "labels": [], "entities": []}, {"text": "Two annotators manually aligned the highlights and document sentences.", "labels": [], "entities": []}, {"text": "Specifically, each sentence in the document was assigned one of three alignment labels: must be in the summary (1), could be in the summary, and is not in the summary (3).", "labels": [], "entities": []}, {"text": "The annotators were asked to label document sentences whose content was identical to the highlights as \"must be in the summary\", sentences with partially overlapping content as \"could be in the summary\" and the remainder as \"should not be in the summary\".", "labels": [], "entities": []}, {"text": "Inter-annotator agreement was .82 (p < 0.01, using Spearman's \u03c1 rank correlation).", "labels": [], "entities": [{"text": "\u03c1 rank correlation", "start_pos": 62, "end_pos": 80, "type": "METRIC", "confidence": 0.7563709815343221}]}, {"text": "The mapping of sentence labels to phrase labels was unsupervised: if the phrase came from a sentence labeled (1), and there was a unigram overlap (excluding stop words) between the phrase and any of the original highlights, we marked this phrase with a positive label.", "labels": [], "entities": []}, {"text": "All other phrases were marked negative.", "labels": [], "entities": []}, {"text": "Our feature set comprised surface features such as sentence and paragraph position information, POS tags, unigram and bigram overlap with the title, and whether high-scoring tf.idf words were present in the phrase (66 features in total).", "labels": [], "entities": []}, {"text": "The 210 documents produced a training set of 42,684 phrases (3,334 positive and 39,350 negative).", "labels": [], "entities": []}, {"text": "We learned the feature weights with a linear SVM, using the software SVM-OOPS.", "labels": [], "entities": []}, {"text": "This tool gave us directly the feature weights as well as support vector values, and it allowed different penalties to be applied to positive and negative misclassifications, enabling us to compensate for the unbalanced data set.", "labels": [], "entities": []}, {"text": "The penalty hyper-parameters chosen were the ones that gave the best F-scores, using 10-fold validation.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9956915974617004}]}, {"text": "Highlight generation We generated highlights fora test set of 600 documents.", "labels": [], "entities": []}, {"text": "We created and solved an ILP for each document.", "labels": [], "entities": []}, {"text": "Sentences were first tokenized to separate words and punctuation, then parsed to obtain phrases and dependencies as described in Section 4 using the Stanford parser (.", "labels": [], "entities": []}, {"text": "For each phrase, features were extracted and salience scores calculated from the feature weights determined through SVM training.", "labels": [], "entities": []}, {"text": "The distance from the SVM hyperplane represents the salience score.", "labels": [], "entities": [{"text": "SVM hyperplane", "start_pos": 22, "end_pos": 36, "type": "DATASET", "confidence": 0.8967304527759552}]}, {"text": "The ILP model (see Equation) was parametrized as follows: the maximum number of highlights NS was 4, the overall limit on length L T was 75 tokens, the length of each highlight was in the range of tokens, and the topic coverage set T contained the top 5 tf.idf words.", "labels": [], "entities": []}, {"text": "These parameters were chosen to capture the properties seen in the majority of the training set; they were also relaxed enough to allow a feasible solution of the ILP model (with hard constraints) for all the documents in the test set.", "labels": [], "entities": []}, {"text": "To solve the ILP model we used the ZIB Optimization Suite software.", "labels": [], "entities": []}, {"text": "The solution was converted into highlights by concatenating the chosen leaf nodes in order.", "labels": [], "entities": []}, {"text": "The ILP problems we created had on average 290 binary variables and 380 constraints.", "labels": [], "entities": []}, {"text": "The mean solve time was 0.03 seconds.", "labels": [], "entities": [{"text": "solve", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.9648554921150208}]}, {"text": "Summarization In order to examine the generality of our model and compare with previous work, we also evaluated our system on a vanilla summarization task.", "labels": [], "entities": [{"text": "vanilla summarization task", "start_pos": 128, "end_pos": 154, "type": "TASK", "confidence": 0.6985695163408915}]}, {"text": "Specifically, we used the same model (trained on the CNN corpus) to generate summaries for the DUC-2002 corpus 2 . We report results on the entire dataset and on a subset containing 140 documents.", "labels": [], "entities": [{"text": "CNN corpus", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.9669591784477234}, {"text": "DUC-2002 corpus 2", "start_pos": 95, "end_pos": 112, "type": "DATASET", "confidence": 0.9755973815917969}]}, {"text": "This is the same partition used by to evaluate their ILP model.", "labels": [], "entities": []}, {"text": "3 Baselines We compared the output of our model to two baselines.", "labels": [], "entities": []}, {"text": "The first one simply selects the \"leading\" three sentences from each document (without any compression).", "labels": [], "entities": []}, {"text": "The second baseline is the output of a sentence-based ILP model, similar to our own, but simpler.", "labels": [], "entities": []}, {"text": "The model is given in (2).", "labels": [], "entities": []}, {"text": "The binary decision variables x \u2208 {0, 1} |S | now represent sentences, and f i the salience score for each sentence.", "labels": [], "entities": []}, {"text": "The objective again is to maximize the total score, but now subject only to tf.idf coverage (2b) and a limit on the number of highlights (2c) which we set to 3.", "labels": [], "entities": [{"text": "coverage", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.8753695487976074}]}, {"text": "There are no sentence length or grammaticality constraints, as there is no sentence compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.7439292967319489}]}, {"text": "The SVM was trained with the same features used to obtain phrase-based salience scores, but with sentence-level labels (labels and positive, (3) negative).", "labels": [], "entities": []}, {"text": "Evaluation We evaluated summarization quality using ROUGE (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9774571657180786}, {"text": "ROUGE", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9681651592254639}]}, {"text": "For the highlight generation task, the original CNN highlights were used as the reference.", "labels": [], "entities": [{"text": "highlight generation task", "start_pos": 8, "end_pos": 33, "type": "TASK", "confidence": 0.8223509391148885}, {"text": "CNN highlights", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.9125399589538574}]}, {"text": "We report unigram overlap (ROUGE-1) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency.", "labels": [], "entities": [{"text": "unigram overlap (ROUGE-1)", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8606771111488343}, {"text": "ROUGE-L", "start_pos": 112, "end_pos": 119, "type": "METRIC", "confidence": 0.8139861822128296}]}, {"text": "In addition, we evaluated the generated highlights by eliciting human judgments.", "labels": [], "entities": []}, {"text": "Participants were presented with a news article and its corresponding highlights and were asked to rate the latter along three dimensions: informativeness (do the highlights represent the article's main topics?), grammaticality (are they fluent?), and verbosity (are they overly wordy and repetitive?).", "labels": [], "entities": []}, {"text": "The subjects used a seven point rating scale.", "labels": [], "entities": []}, {"text": "An ideal system would receive high numbers for grammaticality and informativeness and a low number for verbosity.", "labels": [], "entities": []}, {"text": "We randomly selected nine documents from the test set and generated highlights with our model and the sentence-based ILP baseline.", "labels": [], "entities": []}, {"text": "We also included the original highlights as a gold standard.", "labels": [], "entities": []}, {"text": "We thus obtained ratings for 27 (9 \u00d7 3) document-highlights pairs.", "labels": [], "entities": []}, {"text": "The study was conducted over the Internet using WebExp ( and was completed by 34 volunteers, all self reported native English speakers.", "labels": [], "entities": []}, {"text": "With regard to the summarization task, following, we used ROUGE-1 and ROUGE-2 to evaluate our system's output.", "labels": [], "entities": [{"text": "summarization", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.9791780114173889}, {"text": "ROUGE-1", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9844470024108887}, {"text": "ROUGE-2", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9577016234397888}]}, {"text": "We also report results with ROUGE-L.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9932667016983032}]}, {"text": "Each document in the DUC-2002 dataset is paired with a human-authored summary (approximately 100 words) which we used as reference.", "labels": [], "entities": [{"text": "DUC-2002 dataset", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.9765852391719818}]}], "tableCaptions": [{"text": " Table 3: Comparison of output lengths: number  of sentences, tokens per sentence, and compres- sion rate, for CNN articles, their highlights, the  ILP phrase model, and two baselines.", "labels": [], "entities": [{"text": "compres- sion rate", "start_pos": 87, "end_pos": 105, "type": "METRIC", "confidence": 0.948298305273056}]}, {"text": " Table 4: Average human ratings for original CNN  highlights, and two ILP models.", "labels": [], "entities": []}, {"text": " Table 5: Generated highlights for the stories in Ta- ble 1 using the phrase ILP model.", "labels": [], "entities": [{"text": "Ta- ble 1", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.8326008319854736}]}, {"text": " Table 6: ROUGE results on the complete  DUC-2002 corpus, including the top 5 original  participants. For all results, the 95% confidence  interval is \u00b10.008.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9914984107017517}, {"text": "DUC-2002 corpus", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.965428501367569}]}, {"text": " Table 7: ROUGE results on DUC-2002 cor- pus (140 documents). -: only ROUGE-1 and  ROUGE-2 results are given in", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9265702366828918}, {"text": "DUC-2002 cor- pus (140 documents", "start_pos": 27, "end_pos": 59, "type": "DATASET", "confidence": 0.9239164761134556}, {"text": "ROUGE-1", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9668939113616943}, {"text": "ROUGE-2", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.8425295948982239}]}]}