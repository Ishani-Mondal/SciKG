{"title": [{"text": "Learning Word-Class Lattices for Definition and Hypernym Extraction", "labels": [], "entities": [{"text": "Hypernym Extraction", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.6841427087783813}]}], "abstractContent": [{"text": "Definition extraction is the task of automatically identifying definitional sentences within texts.", "labels": [], "entities": [{"text": "Definition extraction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8967025578022003}, {"text": "automatically identifying definitional sentences within texts", "start_pos": 37, "end_pos": 98, "type": "TASK", "confidence": 0.6282690515120825}]}, {"text": "The task has proven useful in many research areas including ontology learning, relation extraction and question answering.", "labels": [], "entities": [{"text": "ontology learning", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8815828263759613}, {"text": "relation extraction", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.9356614351272583}, {"text": "question answering", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.9290428459644318}]}, {"text": "However, current approaches mostly focused on lexico-syntactic patterns-suffer from both low recall and precision, as definitional sentences occur in highly variable syntactic structures.", "labels": [], "entities": [{"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9991015195846558}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9984551668167114}]}, {"text": "In this paper, we propose Word-Class Lattices (WCLs), a generalization of word lattices that we use to model tex-tual definitions.", "labels": [], "entities": []}, {"text": "Lattices are learned from a dataset of definitions from Wikipedia.", "labels": [], "entities": []}, {"text": "Our method is applied to the task of definition and hypernym extraction and compares favorably to other pattern generalization methods proposed in the literature.", "labels": [], "entities": [{"text": "definition", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.9617452025413513}, {"text": "hypernym extraction", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7445621341466904}]}], "introductionContent": [{"text": "Textual definitions constitute a fundamental source to lookup when the meaning of a term is sought.", "labels": [], "entities": []}, {"text": "Definitions are usually collected in dictionaries and domain glossaries for consultation purposes.", "labels": [], "entities": []}, {"text": "However, manually constructing and updating glossaries requires the cooperative effort of a team of domain experts.", "labels": [], "entities": []}, {"text": "Further, in the presence of new words or usages, and -even worse -new domains, such resources are of no help.", "labels": [], "entities": []}, {"text": "Nonetheless, terms are attested in texts and some (usually few) of the sentences in which a term occurs are typically definitional, that is they provide a formal explanation for the term of interest.", "labels": [], "entities": []}, {"text": "While it is not feasible to manually search texts for definitions, this task can be automatized by means of Machine Learning (ML) and Natural Language Processing (NLP) techniques.", "labels": [], "entities": []}, {"text": "Automatic definition extraction is useful not only in the construction of glossaries, but also in many other NLP tasks.", "labels": [], "entities": [{"text": "Automatic definition extraction", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6591373781363169}]}, {"text": "In ontology learning, definitions are used to create and enrich concepts with textual information (, and extract taxonomic and non-taxonomic relations (.", "labels": [], "entities": []}, {"text": "Definitions are also harvested in Question Answering to deal with \"what is\" questions ().", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7767207622528076}]}, {"text": "In eLearning, they are used to help students assimilate knowledge, etc.", "labels": [], "entities": []}, {"text": "Much of the current literature focuses on the use of lexico-syntactic patterns, inspired by seminal work.", "labels": [], "entities": []}, {"text": "However, these methods suffer both from low recall and precision, as definitional sentences occur in highly variable syntactic structures, and because the most frequent definitional pattern -X is a Y -is inherently very noisy.", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9988947510719299}, {"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9985476136207581}]}, {"text": "In this paper we propose a generalized form of word lattices, called Word-Class Lattices (WCLs), as an alternative to lexico-syntactic pattern learning.", "labels": [], "entities": []}, {"text": "A lattice is a directed acyclic graph (DAG), a subclass of non-deterministic finite state automata (NFA).", "labels": [], "entities": []}, {"text": "The lattice structure has the purpose of preserving the salient differences among distinct sequences, while eliminating redundant information.", "labels": [], "entities": []}, {"text": "In computational linguistics, lattices have been used to model in a compact way many sequences of symbols, each representing an alternative hypothesis.", "labels": [], "entities": []}, {"text": "Lattice-based methods differ in the types of nodes (words, phonemes, concepts), the interpretation of links (representing either a sequential or hierarchical ordering between nodes), their means of creation, and the scoring method used to extract the best consensus output from the lattice ().", "labels": [], "entities": []}, {"text": "In speech processing, phoneme or word lattices) are used as an interface between speech recognition and understanding.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7546710073947906}]}, {"text": "Lat-tices are adopted also in Chinese word segmentation (, decompounding in German, and to represent classes of translation models in machine translation).", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.6804477969805399}, {"text": "machine translation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.7204811871051788}]}, {"text": "In more complex text processing tasks, such as information retrieval, information extraction and summarization, the use of word lattices has been postulated but is considered unrealistic because of the dimension of the hypothesis space.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7896930277347565}, {"text": "information extraction", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.820279061794281}, {"text": "summarization", "start_pos": 97, "end_pos": 110, "type": "TASK", "confidence": 0.9629626870155334}]}, {"text": "To reduce this problem, concept lattices have been proposed.", "labels": [], "entities": []}, {"text": "Here links represent hierarchical relations, rather than the sequential order of symbols like in word/phoneme lattices, and nodes are clusters of salient words aggregated using synonymy, similarity, or subtrees of a thesaurus.", "labels": [], "entities": []}, {"text": "However, salient word selection and aggregation is non-obvious and furthermore it falls into word sense disambiguation, a notoriously AI-hard problem.", "labels": [], "entities": [{"text": "word selection", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.730598658323288}, {"text": "word sense disambiguation", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.7117870847384135}]}, {"text": "In definition extraction, the variability of patterns is higher than for \"traditional\" applications of lattices, such as translation and speech, however not as high as in unconstrained sentences.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.903418093919754}]}, {"text": "The methodology that we propose to align patterns is based on the use of star (wildcard *) characters to facilitate sentence clustering.", "labels": [], "entities": [{"text": "sentence clustering", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7053452581167221}]}, {"text": "Each cluster of sentences is then generalized to a lattice of word classes (each class being either a frequent word or apart of speech).", "labels": [], "entities": []}, {"text": "A key feature of our approach is its inherent ability to both identify definitions and extract hypernyms.", "labels": [], "entities": []}, {"text": "The method is tested on an annotated corpus of Wikipedia sentences and a large Web corpus, in order to demonstrate the independence of the method from the annotated dataset.", "labels": [], "entities": []}, {"text": "WCLs are shown to generalize over lexico-syntactic patterns, and outperform well-known approaches to definition and hypernym extraction.", "labels": [], "entities": [{"text": "definition and hypernym extraction", "start_pos": 101, "end_pos": 135, "type": "TASK", "confidence": 0.6464032009243965}]}, {"text": "The paper is organized as follows: Section 2 discusses related work, WCLs are introduced in Section 3 and illustrated by means of an example in Section 4, experiments are presented in Section 5.", "labels": [], "entities": []}, {"text": "We conclude the paper in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on two different datasets: \u2022 A corpus of 4,619 Wikipedia sentences, that contains 1,908 definitional and 2,711 nondefinitional sentences.", "labels": [], "entities": []}, {"text": "The former were obtained from a random selection of the first sentences of Wikipedia articles 3 . The defined terms belong to different Wikipedia domain categories 4 , so as to capture a representative and cross-domain sample of lexical and syntactic patterns for definitions.", "labels": [], "entities": []}, {"text": "These sentences were manually annotated with DEFINIENDUM, DEFINITOR, DEFINIENS and REST fields by an expert annotator, who also marked the hypernyms.", "labels": [], "entities": [{"text": "DEFINIENDUM", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.6080198884010315}, {"text": "DEFINITOR", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.5898481607437134}, {"text": "DEFINIENS", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.5081701874732971}, {"text": "REST", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9811956882476807}]}, {"text": "The associated set of negative examples (\"syntactically plausible\" false definitions) was obtained by extracting from the same Wikipedia articles sentences in which the page title occurs.", "labels": [], "entities": []}, {"text": "\u2022 A subset of the ukWaC Web corpus, a large corpus of the English language constructed by crawling the .uk domain of the Web.", "labels": [], "entities": [{"text": "ukWaC Web corpus", "start_pos": 18, "end_pos": 34, "type": "DATASET", "confidence": 0.9667421181996664}]}, {"text": "The subset includes over 300,000 sentences in which occur any of 239 terms selected from the terminology of four different domains (COMPUTER SCI-The reason for using the ukWaC corpus is that, unlike the \"clean\" Wikipedia dataset, in which relatively simple patterns can achieve good results, ukWaC represents a real-world test, with many complex cases.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 170, "end_pos": 182, "type": "DATASET", "confidence": 0.9831492304801941}, {"text": "Wikipedia dataset", "start_pos": 211, "end_pos": 228, "type": "DATASET", "confidence": 0.8805144429206848}, {"text": "ukWaC", "start_pos": 292, "end_pos": 297, "type": "DATASET", "confidence": 0.9751436710357666}]}, {"text": "For example, there are sentences that should be classified as definitional according to Section 3.1 but are rather uninformative, like \"dynamic programming was the brainchild of an american mathematician\", as well as informative sentences that are not definitional (e.g., they do not have a hypernym), like \"cubism was characterised by muted colours and fragmented images\".", "labels": [], "entities": []}, {"text": "Even more frequently, the dataset includes sentences which are not definitions but have a definitional pattern (\"A Pacific Northwest tribe's saga refers to a young woman who [..]\"), or sentences with very complex definitional patterns (\"white body cells are the body's cleanup squad\" and \"joule is also an expression of electric energy\").", "labels": [], "entities": []}, {"text": "These cases can be correctly handled only with fine-grained patterns.", "labels": [], "entities": []}, {"text": "Additional details on the corpus and a more thorough linguistic analysis of complex cases can be found in.", "labels": [], "entities": []}, {"text": "For definition extraction, we experiment with the following systems: \u2022 WCL-1 and WCL-3: these two classifiers are based on our Word-Class Lattice model.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.904522180557251}]}, {"text": "WCL-1 learns from the training set a lattice for each cluster of sentences, whereas WCL-3 identifies clusters (and lattices) separately for each sentence field (DEFINIENDUM, DEFINITOR and DEFINIENS) and classifies a sentence as a definition if any combination from the three sets of lattices matches (cf. Section 3.2.4, the best combination is selected).", "labels": [], "entities": []}, {"text": "\u2022 Star patterns: a simple classifier based on the patterns learned as a result of step 1 of our WCL learning algorithm (cf. Section 3.2.1): a sentence is classified as a definition if it matches any of the star patterns in the model.", "labels": [], "entities": []}, {"text": "\u2022 Bigrams: an implementation of the bigram classifier for soft pattern matching proposed by.", "labels": [], "entities": [{"text": "soft pattern matching", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.6977164546648661}]}, {"text": "The classifier selects as definitions all the sentences whose probability is above a specific threshold.", "labels": [], "entities": []}, {"text": "The probability is calculated as a mixture of bigram and   For hypernym extraction, we compared WCL-1 and WCL-3 with Hearst's patterns, a system that extracts hypernyms from sentences based on the lexico-syntactic patterns specified in Hearst's seminal.", "labels": [], "entities": [{"text": "hypernym extraction", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.8563558757305145}]}, {"text": "These include (hypernym in italic): \"such NP as {NP ,} {(or | and)} NP\", \"NP {, NP} {,} or other NP\", \"NP {,} including { NP ,} {or | and} NP\", \"NP {,} especially { NP ,} {or | and} NP\", and variants thereof.", "labels": [], "entities": []}, {"text": "However, it should be noted that hypernym extraction methods in the literature do not extract hypernyms from definitional sentences, like we do, but rather from specific patterns like \"X such as Y\".", "labels": [], "entities": [{"text": "hypernym extraction", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7723532915115356}]}, {"text": "Therefore a direct comparison with these methods is not possible.", "labels": [], "entities": []}, {"text": "Nonetheless, we decided to implement Hearst's patterns for the sake of completeness.", "labels": [], "entities": []}, {"text": "We could not replicate the more refined approach by because it requires the annotation of a possibly very large dataset of sentence fragments.", "labels": [], "entities": []}, {"text": "In any case reported the following performance figures on a corpus of dimension and complexity comparable with ukWaC: the recall-precision graph indicates precision 85% at recall 10% and precision 25% at recall of 30% for the hypernym classifier.", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.9726282954216003}, {"text": "recall-precision", "start_pos": 122, "end_pos": 138, "type": "METRIC", "confidence": 0.9909342527389526}, {"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9969279170036316}, {"text": "precision", "start_pos": 187, "end_pos": 196, "type": "METRIC", "confidence": 0.9965125918388367}, {"text": "recall", "start_pos": 204, "end_pos": 210, "type": "METRIC", "confidence": 0.9942041039466858}]}, {"text": "A variant of the classifier that includes evidence from coordinate terms (terms with a common ancestor in a taxonomy) obtains an increased precision of 35% at recall 30%.", "labels": [], "entities": [{"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9992074370384216}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9929150342941284}]}, {"text": "We see no reasons why these figures should vary dramatically on the ukWaC.", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.7382987141609192}]}, {"text": "Finally, we compare all systems with the random baseline, that classifies a sentence as a definition with probability 1 2 .: Performance on the ukWaC dataset ( \u2020 Recall is estimated).", "labels": [], "entities": [{"text": "ukWaC dataset", "start_pos": 144, "end_pos": 157, "type": "DATASET", "confidence": 0.9950753152370453}, {"text": "Recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9674578309059143}]}, {"text": "To assess the performance of our systems, we calculated the following measures: \u2022 precision -the number of definitional sentences correctly retrieved by the system over the number of sentences marked by the system as definitional.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9995315074920654}]}, {"text": "\u2022 recall -the number of definitional sentences correctly retrieved by the system over the number of definitional sentences in the dataset.", "labels": [], "entities": [{"text": "recall", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9989797472953796}]}, {"text": "\u2022 the F 1 -measure -a harmonic mean of precision (P) and recall (R) given by 2P RP +R . \u2022 accuracy -the number of correctly classified sentences (either as definitional or nondefinitional) over the total number of sentences in the dataset.", "labels": [], "entities": [{"text": "F 1 -measure", "start_pos": 6, "end_pos": 18, "type": "METRIC", "confidence": 0.9312881082296371}, {"text": "precision (P)", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.9291213899850845}, {"text": "recall (R)", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9555702805519104}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9993525147438049}]}], "tableCaptions": [{"text": " Table 2: Performance on the Wikipedia dataset.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.9783887267112732}]}, {"text": " Table 3: Performance on the ukWaC dataset ( \u2020 Re- call is estimated).", "labels": [], "entities": [{"text": "ukWaC dataset", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9961266815662384}, {"text": "Re- call", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9536503156026205}]}, {"text": " Table 5: Precision in hypernym extraction on the  ukWaC dataset (number of hypernyms in paren- theses).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9939242005348206}, {"text": "hypernym extraction", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7517006397247314}, {"text": "ukWaC dataset", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9964366555213928}]}]}