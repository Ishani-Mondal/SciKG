{"title": [], "abstractContent": [{"text": "We present a method for automatically generating focused and accurate topic-specific subjectivity lexicons from a general purpose polarity lexicon that allow users to pinpoint subjective on-topic information in a set of relevant documents.", "labels": [], "entities": []}, {"text": "We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval.", "labels": [], "entities": [{"text": "TREC Blog track test set", "start_pos": 261, "end_pos": 285, "type": "DATASET", "confidence": 0.8194986939430237}, {"text": "opinionated blog post retrieval", "start_pos": 290, "end_pos": 321, "type": "TASK", "confidence": 0.5968999713659286}]}, {"text": "Although the generated lexicons can bean order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opinion retrieval system.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the area of media analysis, one of the key tasks is collecting detailed information about opinions and attitudes toward specific topics from various sources, both offline (traditional newspapers, archives) and online (news sites, blogs, forums).", "labels": [], "entities": []}, {"text": "Specifically, media analysis concerns the following system task: given a topic and list of documents (discussing the topic), find all instances of attitudes toward the topic (e.g., positive/negative sentiments, or, if the topic is an organization or person, support/criticism of this entity).", "labels": [], "entities": []}, {"text": "For every such instance, one should identify the source of the sentiment, the polarity and, possibly, subtopics that this attitude relates to (e.g., specific targets of criticism or support).", "labels": [], "entities": []}, {"text": "Subsequently, a (human) media analyst must be able to aggregate the extracted information by source, polarity or subtopics, allowing him to build support/criticism networks etc..", "labels": [], "entities": []}, {"text": "Recent advances in language technology, especially in sentiment analysis, promise to (partially) automate this task.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.969670444726944}]}, {"text": "Sentiment analysis is often considered in the context of the following two tasks: \u2022 sentiment extraction: given a set of textual documents, identify phrases, clauses, sentences or entire documents that express attitudes, and determine the polarity of these attitudes (); and \u2022 sentiment retrieval: given a topic (and possibly, a list of documents relevant to the topic), identify documents that express attitudes toward this topic ().", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9603822529315948}, {"text": "sentiment extraction", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.7452437877655029}, {"text": "sentiment retrieval", "start_pos": 277, "end_pos": 296, "type": "TASK", "confidence": 0.7761100232601166}]}, {"text": "How can technology developed for sentiment analysis be applied to media analysis?", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.9763137698173523}, {"text": "media analysis", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.8027518391609192}]}, {"text": "In order to use a sentiment extraction system fora media analysis problem, a system would have to be able to determine which of the extracted sentiments are actually relevant, i.e., it would not only have to identify specific targets of all extracted sentiments, but also decide which of the targets are relevant for the topic at hand.", "labels": [], "entities": [{"text": "sentiment extraction", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.7418146729469299}]}, {"text": "This is a difficult task, as the relation between a topic (e.g., a movie) and specific targets of sentiments (e.g., acting or special effects in the movie) is not always straightforward, in the face of ubiquitous complex linguistic phenomena such as referential expressions (\".", "labels": [], "entities": []}, {"text": "this beautifully shot documentary\") or bridging anaphora (\"the director did an excellent jobs\").", "labels": [], "entities": []}, {"text": "In sentiment retrieval, on the other hand, the topic is initially present in the task definition, but it is left to the user to identify sources and targets of sentiments, as systems typically return a list of documents ranked by relevance and opinionatedness.", "labels": [], "entities": [{"text": "sentiment retrieval", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8792127668857574}]}, {"text": "To use a traditional sentiment retrieval system in media analysis, one would still have to manually go through ranked lists of documents returned by the system.", "labels": [], "entities": [{"text": "sentiment retrieval", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8509968817234039}]}, {"text": "To be able to support media analysis, we need to combine the specificity of (phrase-or word-level) sentiment analysis with the topicality provided by sentiment retrieval.", "labels": [], "entities": [{"text": "phrase-or word-level) sentiment analysis", "start_pos": 77, "end_pos": 117, "type": "TASK", "confidence": 0.654613071680069}]}, {"text": "Moreover, we should be able to identify sources and specific targets of opinions.", "labels": [], "entities": []}, {"text": "Another important issue in the media analysis context is evidence fora system's decision.", "labels": [], "entities": []}, {"text": "If the output of a system is to be used to inform actions, the system should present evidence, e.g., highlighting words or phrases that indicate a specific attitude.", "labels": [], "entities": []}, {"text": "Most modern approaches to sentiment analysis, however, use various flavors of classification, where decisions (typically) come with confidence scores, but without explicit support.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.9656662344932556}]}, {"text": "In order to move towards the requirements of media analysis, in this paper we focus on two of the problems identified above: (1) pinpointing evidence fora system's decisions about the presence of sentiment in text, and (2) identifying specific targets of sentiment.", "labels": [], "entities": []}, {"text": "We address these problems by introducing a special type of lexical resource: a topic-specific subjectivity lexicon that indicates specific relevant targets for which sentiments maybe expressed; fora given topic, such a lexicon consists of pairs (syntactic clue, target).", "labels": [], "entities": []}, {"text": "We present a method for automatically generating a topic-specific lexicon fora given topic and query-biased set of documents.", "labels": [], "entities": []}, {"text": "We evaluate the quality of the lexicon both manually and in the setting of an opinionated blog post retrieval task.", "labels": [], "entities": [{"text": "opinionated blog post retrieval task", "start_pos": 78, "end_pos": 114, "type": "TASK", "confidence": 0.6818824768066406}]}, {"text": "We demonstrate that such a lexicon is highly focused, allowing one to effectively pinpoint evidence for sentiment, while being competetive with traditional subjectivity lexicons consisting of (a large number of) clue words.", "labels": [], "entities": []}, {"text": "Unlike other methods for topic-specific sentiment analysis, we do not expand a seed lexicon.", "labels": [], "entities": [{"text": "topic-specific sentiment analysis", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.7296307782332102}]}, {"text": "Instead, we make an existing lexicon more focused, so that it can be used to actually pin-point subjectivity in documents relevant to a given topic.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider two types of evaluation.", "labels": [], "entities": []}, {"text": "In the next section, we examine the quality of the lexicons we generate.", "labels": [], "entities": []}, {"text": "In the section after that we evaluate lexicons quantitatively using the TREC Blog track benchmark.", "labels": [], "entities": [{"text": "TREC Blog track benchmark", "start_pos": 72, "end_pos": 97, "type": "DATASET", "confidence": 0.8373088091611862}]}, {"text": "For extrinsic evaluation we apply our lexicon generation method to a collection of documents containing opinionated utterances: blog posts.", "labels": [], "entities": [{"text": "extrinsic evaluation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7131573855876923}]}, {"text": "The Blogs06 collection ) is a crawl of blog posts from 100,649 blogs over a period of 11 weeks (06/12/2005-21/02/2006), with 3,215,171 posts in total.", "labels": [], "entities": [{"text": "Blogs06 collection", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9377026259899139}]}, {"text": "Before indexing the collection, we perform two preprocessing steps: (i) when extracting plain text from HTML, we only keep block-level elements longer than 15 words (to remove boilerplate material), and (ii) we remove non-English posts using TextCat 2 for language detection.", "labels": [], "entities": [{"text": "language detection", "start_pos": 256, "end_pos": 274, "type": "TASK", "confidence": 0.7387188971042633}]}, {"text": "This leaves us with 2,574,356 posts with 506 words per post on average.", "labels": [], "entities": []}, {"text": "We index the collection using Indri, 3 version 2.10.", "labels": [], "entities": [{"text": "Indri, 3 version 2.10", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.8386337161064148}]}, {"text": "TREC 2006-2008 came with the task of opinionated blog post retrieval (.", "labels": [], "entities": [{"text": "TREC 2006-2008", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9325204491615295}, {"text": "opinionated blog post retrieval", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.624091625213623}]}, {"text": "For each year a set of 50 topics was created, giv-ing us 150 topics in total.", "labels": [], "entities": []}, {"text": "Every topic comes with a set of relevance judgments: Given a topic, a blog post can be either (i) nonrelevant, (ii) relevant, but not opinionated, or (iii) relevant and opinionated.", "labels": [], "entities": []}, {"text": "TREC topics consist of three fields (title, description, and narrative), of which we only use the title field: a query of 1-3 keywords.", "labels": [], "entities": []}, {"text": "We use standard TREC evaluation measures for opinion retrieval: MAP (mean average precision), R-precision (precision within the top R retrieved documents, where R is the number of known relevant documents in the collection), MRR (mean reciprocal rank), P@10 and P@100 (precision within the top 10 and 100 retrieved documents).", "labels": [], "entities": [{"text": "opinion retrieval", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7302972823381424}, {"text": "MAP (mean average precision", "start_pos": 64, "end_pos": 91, "type": "METRIC", "confidence": 0.7113639652729035}, {"text": "R-precision", "start_pos": 94, "end_pos": 105, "type": "METRIC", "confidence": 0.9751986265182495}, {"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9722055792808533}, {"text": "MRR", "start_pos": 225, "end_pos": 228, "type": "METRIC", "confidence": 0.9956398010253906}, {"text": "precision", "start_pos": 269, "end_pos": 278, "type": "METRIC", "confidence": 0.990532636642456}]}, {"text": "In the context of media analysis, recall-oriented measures such as MAP and R-precision are more meaningful than the other, early precision-oriented measures.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 34, "end_pos": 49, "type": "METRIC", "confidence": 0.992385983467102}, {"text": "MAP", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9070872664451599}, {"text": "R-precision", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.9764625430107117}]}, {"text": "Note that for the opinion retrieval task a document is considered relevant if it is on topic and contains opinions or sentiments towards the topic.", "labels": [], "entities": [{"text": "opinion retrieval", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7680439054965973}]}, {"text": "Throughout Section 6 below, we test for significant differences using a two-tailed paired t-test, and report on significant differences for \u03b1 = 0.01 ( and ), and \u03b1 = 0.05 ( and ).", "labels": [], "entities": []}, {"text": "For the quantative experiments in Section 6 we need a topical baseline: a set of blog posts potentially relevant to each topic.", "labels": [], "entities": []}, {"text": "For this, we use the Indri retrieval engine, and apply the Markov Random Fields to model term dependencies in the query) to improve topical retrieval.", "labels": [], "entities": [{"text": "topical retrieval", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.7981949746608734}]}, {"text": "We retrieve the top 1,000 posts for each query.", "labels": [], "entities": []}, {"text": "In this section we assess the quality of the generated topic-specific lexicons numerically and extrinsically.", "labels": [], "entities": []}, {"text": "To this end we deploy our lexicons to the task of opinionated blog post retrieval (.", "labels": [], "entities": [{"text": "opinionated blog post retrieval", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.5957197695970535}]}, {"text": "A commonly used approach to this task works in two stages: (1) identify topically relevant blog posts, and (2) classify these posts as being opinionated or not.", "labels": [], "entities": []}, {"text": "In stage 2 the standard approach is to rerank the results from stage 1, instead of doing actual binary classification.", "labels": [], "entities": []}, {"text": "We take this approach, as it has shown good performance in the past TREC editions ( and is fairly straightforward to implement.", "labels": [], "entities": []}, {"text": "We also explore another way of using the lexicon: as a source for query expansion (i.e., adding new terms to the original query) in Section 6.2.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7046761512756348}]}, {"text": "For all experiments we use the collection described in Section 4.", "labels": [], "entities": []}, {"text": "Our experiments have two goals: to compare the use of topic-independent and topic-specific lexicons for the opinionated post retrieval task, and to examine how different settings for the parameters of the lexicon generation affect the empirical quality.", "labels": [], "entities": [{"text": "opinionated post retrieval task", "start_pos": 108, "end_pos": 139, "type": "TASK", "confidence": 0.6904947459697723}]}], "tableCaptions": [{"text": " Table 4: Evaluation of topic-specific lexicons applied to the opinion retrieval task, compared to the topic- independent lexicon. The two rightmost columns show the number of lexicon entries (average per topic)  and the number of matches of lexicon entries in blog posts (average for top 1,000 posts).", "labels": [], "entities": [{"text": "opinion retrieval task", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7849058012167612}]}]}