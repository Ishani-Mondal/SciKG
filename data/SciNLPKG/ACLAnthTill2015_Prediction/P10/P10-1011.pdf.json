{"title": [{"text": "Bilingual Lexicon Generation Using Non-Aligned Signatures", "labels": [], "entities": [{"text": "Bilingual Lexicon Generation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7823862036069235}]}], "abstractContent": [{"text": "Bilingual lexicons are fundamental resources.", "labels": [], "entities": []}, {"text": "Modern automated lexicon generation methods usually require parallel corpora, which are not available for most language pairs.", "labels": [], "entities": [{"text": "automated lexicon generation", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.7899961868921915}]}, {"text": "Lexicons can be generated using non-parallel corpora or a pivot language, but such lexicons are noisy.", "labels": [], "entities": []}, {"text": "We present an algorithm for generating a high quality lexicon from a noisy one, which only requires an independent corpus for each language.", "labels": [], "entities": []}, {"text": "Our algorithm introduces non-aligned signatures (NAS), a cross-lingual word context similarity score that avoids the over-constrained and inefficient nature of alignment-based methods.", "labels": [], "entities": [{"text": "non-aligned signatures (NAS)", "start_pos": 25, "end_pos": 53, "type": "METRIC", "confidence": 0.6717917144298553}]}, {"text": "We use NAS to eliminate incorrect translations from the generated lexicon.", "labels": [], "entities": []}, {"text": "We evaluate our method by improving the quality of noisy Spanish-Hebrew lexicons generated from two pivot English lexicons.", "labels": [], "entities": []}, {"text": "Our algorithm substantially outperforms other lexicon generation methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bilingual lexicons are useful for both end users and computerized language processing tasks.", "labels": [], "entities": []}, {"text": "They provide, for each source language word or phrase, a set of translations in the target language, and thus they area basic component of dictionaries, which also include syntactic information, sense division, usage examples, semantic fields, usage guidelines, etc.", "labels": [], "entities": [{"text": "sense division", "start_pos": 195, "end_pos": 209, "type": "TASK", "confidence": 0.7158052325248718}]}, {"text": "Traditionally, when bilingual lexicons are not compiled manually, they are extracted from parallel corpora.", "labels": [], "entities": []}, {"text": "However, for most language pairs parallel bilingual corpora either do not exist or are at best small and unrepresentative of the general language.", "labels": [], "entities": []}, {"text": "Bilingual lexicons can be generated using nonparallel corpora or pivot language lexicons (see Section 2).", "labels": [], "entities": []}, {"text": "However, such lexicons are noisy.", "labels": [], "entities": []}, {"text": "In this paper we present a method for generating a high quality lexicon given such a noisy one.", "labels": [], "entities": []}, {"text": "Our evaluation focuses on the pivot language case.", "labels": [], "entities": []}, {"text": "Pivot language approaches deal with the scarcity of bilingual data for most language pairs by relying on the availability of bilingual data for each of the languages in question with a third, pivot, language.", "labels": [], "entities": []}, {"text": "In practice, this third language is often English.", "labels": [], "entities": []}, {"text": "A naive method for pivot-based lexicon generation goes as follows.", "labels": [], "entities": [{"text": "pivot-based lexicon generation", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.6534275114536285}]}, {"text": "For each source headword 1 , take its translations to the pivot language using the source-to-pivot lexicon, then for each such translation take its translations to the target language using the pivot-to-target lexicon.", "labels": [], "entities": []}, {"text": "This method yields highly noisy ('divergent') lexicons, because lexicons are generally intransitive.", "labels": [], "entities": []}, {"text": "This intransitivity stems from polysemy in the pivot language that does not exist in the source language.", "labels": [], "entities": []}, {"text": "For example, take French-English-Spanish.", "labels": [], "entities": []}, {"text": "The English word spring is the translation of the French word printemps, but only in the season of year sense.", "labels": [], "entities": []}, {"text": "Further translating spring into Spanish yields both the correct translation primavera and an incorrect one, resorte (the elastic object).", "labels": [], "entities": []}, {"text": "To cope with the issue of divergence due to lexical intransitivity, we present an algorithm for assessing the correctness of candidate translations.", "labels": [], "entities": []}, {"text": "The algorithm is quite simple to understand and to implement and is computationally efficient.", "labels": [], "entities": []}, {"text": "In spite of its simplicity, we are not aware of previous work applying it to our problem.", "labels": [], "entities": []}, {"text": "The algorithm utilizes two monolingual corpora, comparable in their domain but otherwise unrelated, in the source and target languages.", "labels": [], "entities": []}, {"text": "It does not need a pivot language corpus.", "labels": [], "entities": []}, {"text": "The algorithm comprises two stages: signature genera-tion and signature ranking.", "labels": [], "entities": [{"text": "signature ranking", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.8307750821113586}]}, {"text": "The signature of word w is the set of words that co-occur with w most strongly.", "labels": [], "entities": []}, {"text": "While co-occurrence scores are used to compute signatures, signatures, unlike context vectors, do not contain the score values.", "labels": [], "entities": []}, {"text": "For each given source headword we compute its signature and the signatures of all of its candidate translations.", "labels": [], "entities": []}, {"text": "We present the non-aligned signatures (NAS) similarity score for signature and use it to rank these translations.", "labels": [], "entities": [{"text": "non-aligned signatures (NAS) similarity score", "start_pos": 15, "end_pos": 60, "type": "METRIC", "confidence": 0.7265494295528957}]}, {"text": "NAS is based on the number of headword signature words that maybe translated using the input noisy lexicon into words in the signature of a candidate translation.", "labels": [], "entities": []}, {"text": "We evaluate our algorithm by generating a bilingual lexicon for Hebrew and Spanish using pivot Hebrew-English and English-Spanish lexicons compiled by a professional publishing house.", "labels": [], "entities": []}, {"text": "We show that the algorithm outperforms existing algorithms for handling divergence induced by lexical intransitivity.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our algorithm by generating bilingual lexicons for Hebrew and Spanish, using English as a pivot language.", "labels": [], "entities": []}, {"text": "We chose a language pair for which basically no parallel corpora exist 2 , and that do not share ancestry or writing system in away that can provide cues for alignment.", "labels": [], "entities": []}, {"text": "We conducted the test twice: once creating a Hebrew-Spanish lexicon, and once creating a Spanish-Hebrew one.", "labels": [], "entities": []}, {"text": "Lexicon generation, as defined in our experiment, is a relatively high standard for cross-linguistic semantic distance evaluation.", "labels": [], "entities": [{"text": "Lexicon generation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8941043615341187}, {"text": "cross-linguistic semantic distance evaluation", "start_pos": 84, "end_pos": 129, "type": "TASK", "confidence": 0.8011553287506104}]}, {"text": "This is especially cor-: Precision of score comparison experiments.", "labels": [], "entities": [{"text": "Precision of score comparison", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.5571216940879822}]}, {"text": "The percentage of cases in which each of the scoring methods was able to successfully distinguish the correct (SCE1) or possible correct (SCE2) translation from the random translation.", "labels": [], "entities": []}, {"text": "rect since our gold standard gives only a small set of translations.", "labels": [], "entities": []}, {"text": "The set of possible translations in iLex tends to include, besides the \"correct\" translation of the gold standard, other translations that are suitable in certain contexts or are semantically related.", "labels": [], "entities": []}, {"text": "For example, for one Hebrew word, kvuza, the gold standard translation was grupo (group), while our method chose equipo (team), which was at least as plausible given the amount of sports news in the corpus.", "labels": [], "entities": []}, {"text": "Thus to better compare the capability of NAS to distinguish correct and incorrect translations with that of other scores, we performed two more experiments.", "labels": [], "entities": []}, {"text": "In the first score comparison experiment (SCE1), we used the two R1 test sets, Hebrew and Spanish, from the lexicon generation test (section 4.1.4).", "labels": [], "entities": []}, {"text": "For each word in the test set, we used our method to select between one of two translations: a correct translation, from the gold standard, and a random translation, chosen randomly among all the nouns similar in frequency to the correct translation.", "labels": [], "entities": []}, {"text": "The second score comparison experiment (SCE2) was designed to test the score with a more extensive test set.", "labels": [], "entities": []}, {"text": "For each of the two languages, we randomly selected 1000 nouns, and used our method to select between a possibly correct translation, chosen randomly among the translations suggested in iLex, and a random translation, chosen randomly among nouns similar in frequency to the possibly correct translation.", "labels": [], "entities": []}, {"text": "This test, while using a more extensive test set, is less accurate because it is not guaranteed that any of the input translations is correct.", "labels": [], "entities": []}, {"text": "In both SCE1 and SCE2, cosine and city block distance were used as baselines.", "labels": [], "entities": [{"text": "SCE1", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.890572726726532}, {"text": "SCE2", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.8039368391036987}]}, {"text": "Inverse Consultation is irrelevant here because it can only score translation pairs that appear in iLex.", "labels": [], "entities": []}, {"text": "presents the results of the two score comparison experiments, each of them for each of the translation directions.", "labels": [], "entities": []}, {"text": "Recall is by definition 100% and is omitted.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9878936409950256}]}, {"text": "Again, NAS performs better than the baselines in all cases.", "labels": [], "entities": [{"text": "NAS", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.5706663727760315}]}, {"text": "With all scores, precision values in SCE1 are higher than in the lexicon generation experiment.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9996747970581055}, {"text": "SCE1", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.46647563576698303}]}, {"text": "This is consistent with the expectation that selection between a correct and a random, probably incorrect, translation is easier than selecting among the translations in iLex.", "labels": [], "entities": []}, {"text": "The precision in SCE2 is lower than that in SCE1.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.999745786190033}, {"text": "SCE2", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.7605356574058533}, {"text": "SCE1", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.913551390171051}]}, {"text": "This maybe a result of both translations in SCE2 being in some cases incorrect.", "labels": [], "entities": [{"text": "SCE2", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.7990805506706238}]}, {"text": "Yet this may also reflect a weakness of all three scores with lower-frequency words, which are represented in the 1000-word samples but not in the ones used in SCE1.", "labels": [], "entities": [{"text": "SCE1", "start_pos": 160, "end_pos": 164, "type": "DATASET", "confidence": 0.7634995579719543}]}], "tableCaptions": [{"text": " Table 2: Number of words in lexicons, and branch- ing factors (BF).", "labels": [], "entities": [{"text": "branch- ing factors (BF)", "start_pos": 43, "end_pos": 67, "type": "METRIC", "confidence": 0.8182946145534515}]}, {"text": " Table 3: Hebrew-Spanish lexicon generation:  highest-ranking translation.", "labels": [], "entities": []}, {"text": " Table 4: Spanish-Hebrew Lexicon Generation:  highest-ranking translation.", "labels": [], "entities": []}, {"text": " Table 5: Hebrew-Spanish lexicon generation: ac- curacy of 3 best translations for the R1 condition.  The table shows how many of the 2nd and 3rd  translations are correct. Note that NAS is always  a better solution, even though its numbers for 2nd  and 3rd are smaller, because its accumulative per- centage, shown in the last column, is higher.", "labels": [], "entities": [{"text": "Hebrew-Spanish lexicon generation", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.6150318284829458}, {"text": "ac- curacy", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9522793292999268}, {"text": "accumulative per- centage", "start_pos": 283, "end_pos": 308, "type": "METRIC", "confidence": 0.9556505382061005}]}, {"text": " Table 6: Spanish-Hebrew lexicon generation: ac- curacy of 3 best translations for the R1 condition.  The total exceeds 100% because Spanish words  had more than one correct translation. See also  the caption of", "labels": [], "entities": [{"text": "Spanish-Hebrew lexicon generation", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.5857969224452972}, {"text": "ac- curacy", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9594069719314575}]}, {"text": " Table 7: Precision of score comparison experi- ments. The percentage of cases in which each  of the scoring methods was able to successfully  distinguish the correct (SCE1) or possible correct  (SCE2) translation from the random translation.", "labels": [], "entities": [{"text": "Precision of score comparison experi- ments", "start_pos": 10, "end_pos": 53, "type": "METRIC", "confidence": 0.7763756002698626}]}]}