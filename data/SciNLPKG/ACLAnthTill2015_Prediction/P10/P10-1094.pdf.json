{"title": [{"text": "Cross-Language Document Summarization Based on Machine Translation Quality Prediction", "labels": [], "entities": [{"text": "Cross-Language Document Summarization", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8489771087964376}, {"text": "Machine Translation Quality Prediction", "start_pos": 47, "end_pos": 85, "type": "TASK", "confidence": 0.791389524936676}]}], "abstractContent": [{"text": "Cross-language document summarization is a task of producing a summary in one language fora document set in a different language.", "labels": [], "entities": [{"text": "Cross-language document summarization", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.5906324585278829}]}, {"text": "Existing methods simply use machine translation for document translation or summary translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7230663597583771}, {"text": "document translation", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.7840306758880615}, {"text": "summary translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.709556981921196}]}, {"text": "However, current machine translation services are far from satisfactory, which results in that the quality of the cross-language summary is usually very poor, both in read-ability and content.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7515289783477783}]}, {"text": "In this paper, we propose to consider the translation quality of each sentence in the English-to-Chinese cross-language summarization process.", "labels": [], "entities": []}, {"text": "First, the translation quality of each English sentence in the document set is predicted with the SVM regression method, and then the quality score of each sentence is incorporated into the summarization process.", "labels": [], "entities": []}, {"text": "Finally, the English sentences with high translation quality and high informative-ness are selected and translated to form the Chinese summary.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate the effectiveness and usefulness of the proposed approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given a document or document set in one source language, cross-language document summarization aims to produce a summary in a different target language.", "labels": [], "entities": [{"text": "cross-language document summarization", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.6110784808794657}]}, {"text": "In this study, we focus on English-to-Chinese document summarization for the purpose of helping Chinese readers to quickly understand the major content of an English document or document set.", "labels": [], "entities": [{"text": "English-to-Chinese document summarization", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.596260279417038}]}, {"text": "This task is very important in the field of multilingual information access.", "labels": [], "entities": [{"text": "multilingual information access", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.6857901215553284}]}, {"text": "Till now, most previous work focuses on monolingual document summarization, but cross-language document summarization has received little attention in the past years.", "labels": [], "entities": [{"text": "monolingual document summarization", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.5801159143447876}, {"text": "cross-language document summarization", "start_pos": 80, "end_pos": 117, "type": "TASK", "confidence": 0.6458976566791534}]}, {"text": "A straightforward way for cross-language document summarization is to translate the summary from the source language to the target language by using machine translation services.", "labels": [], "entities": [{"text": "cross-language document summarization", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.7301170627276102}]}, {"text": "However, though machine translation techniques have been advanced a lot, the machine translation quality is far from satisfactory, and in many cases, the translated texts are hard to understand.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7395888268947601}, {"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7086611688137054}]}, {"text": "Therefore, the translated summary is likely to be hard to understand by readers, i.e., the summary quality is likely to be very poor.", "labels": [], "entities": []}, {"text": "For example, the translated Chinese sentence for an ordinary English sentence (\"It is also Mr Baker who is making the most of presidential powers to dispense largesse.\") by using Google Translate is \"\u540c\u65f6\uff0c\u4e5f \u662f\u8d1d\u514b\u662f\u8c01\u63d0\u51fa\u4e86\u5bf9\u603b\u7edf\u6743\u529b\u514d\u9664\u6700\u6177\u6168\u3002\".", "labels": [], "entities": []}, {"text": "The translated sentence is hard to understand because it contains incorrect translations and it is very disfluent.", "labels": [], "entities": []}, {"text": "If such sentences are selected into the summary, the quality of the summary would be very poor.", "labels": [], "entities": []}, {"text": "In order to address the above problem, we propose to consider the translation quality of the English sentences in the summarization process.", "labels": [], "entities": []}, {"text": "In particular, the translation quality of each English sentence is predicted by using the SVM regression method, and then the predicted MT quality score of each sentence is incorporated into the sentence evaluation process, and finally both informative and easy-to-translate sentences are selected and translated to form the Chinese summary.", "labels": [], "entities": [{"text": "MT quality score", "start_pos": 136, "end_pos": 152, "type": "METRIC", "confidence": 0.8417638937632242}]}, {"text": "An empirical evaluation is conducted to evaluate the performance of machine translation quality prediction, and a user study is performed to evaluate the cross-language summary quality.", "labels": [], "entities": [{"text": "machine translation quality prediction", "start_pos": 68, "end_pos": 106, "type": "TASK", "confidence": 0.8754584789276123}]}, {"text": "The results demonstrate the effectiveness of the proposed approach.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 introduces related work.", "labels": [], "entities": []}, {"text": "The system is overviewed in Section 3.", "labels": [], "entities": []}, {"text": "In Sections 4 and 5, we present the detailed algorithms and evaluation results of machine translation quality prediction and cross-language summarization, respectively.", "labels": [], "entities": [{"text": "machine translation quality prediction", "start_pos": 82, "end_pos": 120, "type": "TASK", "confidence": 0.8490606844425201}, {"text": "cross-language summarization", "start_pos": 125, "end_pos": 153, "type": "TASK", "confidence": 0.7287541031837463}]}, {"text": "We discuss in Section 6 and conclude this paper in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this experiment, we used the document sets provided by DUC2001 for evaluation.", "labels": [], "entities": [{"text": "DUC2001", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9563663005828857}]}, {"text": "As mentioned in Section 4.2.1, DUC2001 provided 30 English document sets for generic multidocument summarization.", "labels": [], "entities": [{"text": "DUC2001", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9394547939300537}, {"text": "generic multidocument summarization", "start_pos": 77, "end_pos": 112, "type": "TASK", "confidence": 0.6104250152905782}]}, {"text": "The average document number per document set was 10.", "labels": [], "entities": []}, {"text": "The sentences in each article have been separated and the sentence information has been stored into files.", "labels": [], "entities": []}, {"text": "Generic reference English summaries were provided by NIST annotators for evaluation.", "labels": [], "entities": [{"text": "NIST", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.9333657622337341}]}, {"text": "In our study, we aimed to produce Chinese summaries for the English document sets.", "labels": [], "entities": [{"text": "English document sets", "start_pos": 60, "end_pos": 81, "type": "DATASET", "confidence": 0.7790833512941996}]}, {"text": "The summary length was limited to five sentences, i.e. each summary consisted of five sentences.", "labels": [], "entities": []}, {"text": "The DUC2001 dataset was divided into the following two datasets: Ideal Dataset: We have manually labeled the MT quality scores for the sentences in five document sets (d04-d11), and we directly used the manually labeled scores in the summarization process.", "labels": [], "entities": [{"text": "DUC2001 dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9796712100505829}, {"text": "Ideal Dataset", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.6614174693822861}, {"text": "MT quality", "start_pos": 109, "end_pos": 119, "type": "TASK", "confidence": 0.7810952663421631}, {"text": "summarization", "start_pos": 234, "end_pos": 247, "type": "TASK", "confidence": 0.9684439301490784}]}, {"text": "The ideal dataset contained these five document sets.", "labels": [], "entities": []}, {"text": "Real Dataset: The MT quality scores for the sentences in the remaining 25 document sets were automatically predicted by using the learned SVM regression model.", "labels": [], "entities": [{"text": "Real Dataset", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.7545287907123566}, {"text": "MT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.975033164024353}]}, {"text": "And we used the automatically predicted scores in the summarization process.", "labels": [], "entities": [{"text": "summarization", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.9755359292030334}]}, {"text": "The real dataset contained these 25 document sets.", "labels": [], "entities": []}, {"text": "We performed two evaluation procedures: one based on the ideal dataset to validate the feasibility of the proposed approach, and the other based on the real dataset to demonstrate the effectiveness of the proposed approach in real applications.", "labels": [], "entities": []}, {"text": "To date, various methods and metrics have been developed for English summary evaluation by comparing system summary with reference summary, such as the pyramid method () and the ROUGE metrics ( . However, such methods or metrics cannot be directly used for evaluating Chinese summary without reference Chinese summary.", "labels": [], "entities": [{"text": "English summary evaluation", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.6337002416451772}, {"text": "ROUGE", "start_pos": 178, "end_pos": 183, "type": "METRIC", "confidence": 0.7856066226959229}]}, {"text": "Instead, we developed an evaluation protocol as follows: The evaluation was based on human scoring.", "labels": [], "entities": []}, {"text": "Four Chinese college students participated in the evaluation as subjects.", "labels": [], "entities": []}, {"text": "We have developed a friendly tool for helping the subjects to evaluate each Chinese summary from the following three aspects: Content: This aspect indicates how much a summary reflects the major content of the document set.", "labels": [], "entities": []}, {"text": "After reading a summary, each user can select a score between 1 and 5 for the summary.", "labels": [], "entities": []}, {"text": "1 means \"very uninformative\" and 5 means \"very informative\".", "labels": [], "entities": []}, {"text": "Readability: This aspect indicates the readability level of the whole summary.", "labels": [], "entities": []}, {"text": "After reading a summary, each user can select a score between 1 and 5 for the summary.", "labels": [], "entities": []}, {"text": "1 means \"hard to read\", and 5 means \"easy to read\".", "labels": [], "entities": []}, {"text": "Overall: This aspect indicates the overall quality of a summary.", "labels": [], "entities": []}, {"text": "After reading a summary, each user can select a score between 1 and 5 for the summary.", "labels": [], "entities": []}, {"text": "1 means \"very bad\", and 5 means \"very good\".", "labels": [], "entities": []}, {"text": "We performed the evaluation procedures on the ideal dataset and the read dataset, separately.", "labels": [], "entities": []}, {"text": "During each evaluation procedure, we compared our proposed approach (\u03bb=0.3) with the baseline approach without considering the MT quality factor (\u03bb=0).", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.6904116272926331}]}, {"text": "And the two summaries produced by the two systems for the same document set were presented in the same interface, and then the four subjects assigned scores to each summary after they read and compared the two summaries.", "labels": [], "entities": []}, {"text": "And the assigned scores were finally averaged across the documents sets and across the subjects.", "labels": [], "entities": []}, {"text": "shows the evaluation results on the ideal dataset with 5 document sets.", "labels": [], "entities": []}, {"text": "We can see that based on the manually labeled MT quality scores, the Chinese summaries produced by our proposed approach are significantly better than that produced by the baseline approach overall three aspects.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.966374397277832}]}, {"text": "All subjects agree that our proposed approach can produce more informative and easyto-read Chinese summaries than the baseline approach.", "labels": [], "entities": []}, {"text": "shows the evaluation results on the real dataset with 25 document sets.", "labels": [], "entities": []}, {"text": "We can see that based on the automatically predicted MT quality scores, the Chinese summaries produced by our proposed approach are significantly better than that produced by the baseline approach over the readability aspect and the overall aspect.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9806810021400452}]}, {"text": "Almost all subjects agree that our proposed approach can produce more easy-to-read and highquality Chinese summaries than the baseline approach.", "labels": [], "entities": []}, {"text": "Comparing the evaluation results in the two tables, we can find that the performance difference between the two approaches on the ideal dataset is bigger than that on the real dataset, especially on the content aspect.", "labels": [], "entities": []}, {"text": "The results demonstrate that the more accurate the MT quality scores are, the more significant the performance improvement is.", "labels": [], "entities": [{"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9880686402320862}]}, {"text": "Overall, the proposed approach is effective to produce good-quality Chinese summaries for English document sets.", "labels": [], "entities": []}, {"text": "( * indicates the difference between the average score of the proposed approach and that of the baseline approach is statistically significant by using t-test.)", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation results on the ideal dataset (5 document sets)", "labels": [], "entities": []}, {"text": " Table 3: Evaluation results on the real dataset (25 document sets)", "labels": [], "entities": []}]}