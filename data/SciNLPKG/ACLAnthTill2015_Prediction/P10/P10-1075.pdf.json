{"title": [{"text": "Detecting Errors in Automatically-Parsed Dependency Relations", "labels": [], "entities": [{"text": "Detecting Errors", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8184741735458374}]}], "abstractContent": [{"text": "We outline different methods to detect errors in automatically-parsed dependency corpora, by comparing so-called dependency rules to their representation in the training data and flagging anomalous ones.", "labels": [], "entities": []}, {"text": "By comparing each new rule to every relevant rule from training, we can identify parts of parse trees which are likely erroneous.", "labels": [], "entities": []}, {"text": "Even the relatively simple methods of comparison we propose show promise for speeding up the annotation process.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In evaluating the methods, our main question is: how accurate are the dependencies, in terms of both attachment and labeling?", "labels": [], "entities": []}, {"text": "We therefore currently examine the scores for elements functioning as dependents in a rule.", "labels": [], "entities": []}, {"text": "In, for example, for har ('has'), we look at its score within ET \u2192 PR PA:HV and not when it functions as ahead, as in PA \u2192 SS:NN XX:XX HV OO:VN.", "labels": [], "entities": []}, {"text": "Relatedly, for each method, we are interested in whether elements with scores below a threshold have worse attachment accuracy than scores above, as we predict they do.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.7872057557106018}]}, {"text": "We can measure this by scoring each testing data position below the threshold as a 1 if it has the correct head and dependency relation and a 0 otherwise.", "labels": [], "entities": []}, {"text": "These are simply labeled attachment scores (LAS).", "labels": [], "entities": [{"text": "attachment scores (LAS)", "start_pos": 25, "end_pos": 48, "type": "METRIC", "confidence": 0.9444142937660217}]}, {"text": "Scoring separately for positions above and below a threshold views the task as one of sorting parser output into two bins, those more or less likely to be correctly parsed.", "labels": [], "entities": []}, {"text": "For development, we also report unlabeled attachement scores (UAS).", "labels": [], "entities": [{"text": "unlabeled attachement scores (UAS)", "start_pos": 32, "end_pos": 66, "type": "METRIC", "confidence": 0.8157953917980194}]}, {"text": "Since the goal is to speedup the post-editing of corpus data by flagging erroneous rules, we also report the precision and recall for error detection.", "labels": [], "entities": [{"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9997444748878479}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9993744492530823}, {"text": "error detection", "start_pos": 134, "end_pos": 149, "type": "TASK", "confidence": 0.6805683970451355}]}, {"text": "We count either attachment or labeling errors as an error, and precision and recall are measured with respect to how many errors are found below the threshold.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9996602535247803}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9995324611663818}]}, {"text": "For development, we use two Fscores to provide a measure of the settings to examine across language, corpus, and parser conditions: the balanced F 1 measure and the F 0.5 measure, weighing precision twice as much.", "labels": [], "entities": [{"text": "Fscores", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9872050285339355}, {"text": "F 1 measure", "start_pos": 145, "end_pos": 156, "type": "METRIC", "confidence": 0.8352785309155782}, {"text": "F 0.5 measure", "start_pos": 165, "end_pos": 178, "type": "METRIC", "confidence": 0.9409813086191813}, {"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9801805019378662}]}, {"text": "Precision is likely more important in this context, so as to prevent annotators from sorting through too many false positives.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9767318964004517}]}, {"text": "In practice, one way to use these methods is to start with the lowest thresholds and work upwards until there are too many non-errors.", "labels": [], "entities": []}, {"text": "To establish a basis for comparison, we compare method performance to a parser on its own.", "labels": [], "entities": []}, {"text": "By examining the parser output without any automatic assistance, how often does a correction need to be made?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: MaltParser results for Talbanken, for select values (b = below, a = above threshold (Thr.))", "labels": [], "entities": [{"text": "Talbanken", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.8487260341644287}, {"text": "Thr.))", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9565253257751465}]}, {"text": " Table 2: MaltParser results for Alpino", "labels": [], "entities": [{"text": "MaltParser", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.7846303582191467}, {"text": "Alpino", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.9189842939376831}]}, {"text": " Table 3: MaltParser results for Bosque", "labels": [], "entities": [{"text": "MaltParser", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.808978259563446}, {"text": "Bosque", "start_pos": 33, "end_pos": 39, "type": "TASK", "confidence": 0.5491355657577515}]}, {"text": " Table 4: MaltParser results for DDT", "labels": [], "entities": [{"text": "MaltParser", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.7439600825309753}, {"text": "DDT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.7093397378921509}]}, {"text": " Table 5: MSTParser results for Talbanken", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.6736161112785339}, {"text": "Talbanken", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.6299287676811218}]}, {"text": " Table 6: MSTParser results for Alpino", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.6682048439979553}, {"text": "Alpino", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.923331618309021}]}, {"text": " Table 7: MSTParser results for Bosque", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.7254081964492798}, {"text": "Bosque", "start_pos": 32, "end_pos": 38, "type": "TASK", "confidence": 0.6116053462028503}]}, {"text": " Table 8: MSTParser results for DDT", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.7417512536048889}, {"text": "DDT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.7626160383224487}]}]}