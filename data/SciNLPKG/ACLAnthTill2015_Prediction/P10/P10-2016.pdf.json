{"title": [{"text": "Tackling Sparse Data Issue in Machine Translation Evaluation *", "labels": [], "entities": [{"text": "Tackling Sparse Data Issue", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8478711545467377}, {"text": "Machine Translation Evaluation", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.8809533913930258}]}], "abstractContent": [{"text": "We illustrate and explain problems of n-grams-based machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech.", "labels": [], "entities": [{"text": "n-grams-based machine translation (MT)", "start_pos": 38, "end_pos": 76, "type": "TASK", "confidence": 0.7877898315588633}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9927065968513489}]}, {"text": "A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well.", "labels": [], "entities": [{"text": "translation", "start_pos": 131, "end_pos": 142, "type": "TASK", "confidence": 0.9647323489189148}]}], "introductionContent": [{"text": "Automatic metrics of machine translation (MT) quality are vital for research progress at a fast pace.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8406500220298767}]}, {"text": "Many automatic metrics of MT quality have been proposed and evaluated in terms of correlation with human judgments while various techniques of manual judging are being examined as well, see e.g. 1 , WMT08 and WMT09 . The contribution of this paper is twofold.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9940511584281921}, {"text": "WMT08", "start_pos": 199, "end_pos": 204, "type": "DATASET", "confidence": 0.9089107513427734}, {"text": "WMT09", "start_pos": 209, "end_pos": 214, "type": "DATASET", "confidence": 0.9422218799591064}]}, {"text": "Section 2 illustrates and explains severe problems of a widely used BLEU metric () when applied to Czech as a representative of languages with rich morphology.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 68, "end_pos": 79, "type": "METRIC", "confidence": 0.9623109698295593}]}, {"text": "We see this as an instance of the sparse data problem well known for MT itself: too much detail in the formal representation leading to low coverage of e.g. a translation dictionary.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.983109712600708}]}, {"text": "In MT evaluation, too much detail leads to the lack of comparable parts of the hypothesis and the reference.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.9573659896850586}]}, {"text": "Section 3 introduces and evaluates some new variations of SemPOS (), a metric based on the deep syntactic representation of the sentence performing very well for Czech as the target language.", "labels": [], "entities": []}, {"text": "Aside from including dependency and n-gram relations in the scoring, we also apply and evaluate SemPOS for English.", "labels": [], "entities": []}], "datasetContent": [{"text": "We measured the metric performance on data used in MetricsMATR08, WMT09 and WMT08.", "labels": [], "entities": [{"text": "MetricsMATR08", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.8324069976806641}, {"text": "WMT09", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.9638310074806213}, {"text": "WMT08", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.9728686809539795}]}, {"text": "For the evaluation of metric correlation with human judgments at the system level, we used the Pearson correlation coefficient \u03c1 applied to ranks.", "labels": [], "entities": [{"text": "Pearson correlation coefficient \u03c1", "start_pos": 95, "end_pos": 128, "type": "METRIC", "confidence": 0.9551109671592712}]}, {"text": "In case of a tie, the systems were assigned the average position.", "labels": [], "entities": []}, {"text": "For example if three systems achieved the same highest score (thus occupying the positions 1, 2 and 3 when sorted by score), each of them would obtain the average rank of 2 = 1+2+3 3 . When correlating ranks (instead of exact scores) and with this handling of ties, the Pearson coefficient is equivalent to Spearman's rank correlation coefficient.", "labels": [], "entities": [{"text": "Pearson coefficient", "start_pos": 270, "end_pos": 289, "type": "METRIC", "confidence": 0.9744510650634766}, {"text": "rank correlation coefficient", "start_pos": 318, "end_pos": 346, "type": "METRIC", "confidence": 0.7569078604380289}]}, {"text": "The MetricsMATR08 human judgments include preferences for pairs of MT systems saying which one of the two systems is better, while the WMT08 and WMT09 data contain system scores (for up to 5 systems) on the scale 1 to 5 fora given sentence.", "labels": [], "entities": [{"text": "WMT08", "start_pos": 135, "end_pos": 140, "type": "DATASET", "confidence": 0.9674865007400513}, {"text": "WMT09 data", "start_pos": 145, "end_pos": 155, "type": "DATASET", "confidence": 0.9166146218776703}]}, {"text": "We assigned a human ranking to the systems based on the percent of time that their translations were judged to be better than or equal to the translations of any other system in the manual evaluation.", "labels": [], "entities": []}, {"text": "We converted automatic metric scores to ranks.", "labels": [], "entities": []}, {"text": "Metrics' performance for translation to English and Czech was measured on the following testsets (the number of human judgments fora given source language in brackets): The MetricsMATR08 testset contained 4 reference translations for each sentence whereas the remaining testsets only one reference.", "labels": [], "entities": [{"text": "translation to English and Czech", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.8654919505119324}]}, {"text": "Correlation coefficients for English are shown in.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9612298607826233}]}, {"text": "The best metric is Void par closely followed by Void sons . The explanation is that Void compared to SemPOS or Functor does not lose points by an erroneous assignment of the POS or the functor, and that Void par profits from checking the dependency relations between autosemantic words.", "labels": [], "entities": []}, {"text": "The combination of BLEU and Sem-POS 6 outperforms both individual metrics, but in case of SemPOS only by a minimal difference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9986324906349182}]}, {"text": "Additionally, we confirm that 4-grams alone have little discriminative power both when used as a metric of their own (BLEU 4 ) as well as in a linear combination with SemPOS.", "labels": [], "entities": [{"text": "BLEU 4 )", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9751382072766622}]}, {"text": "The best metric for Czech (see) is a linear combination of SemPOS and 4-gram BLEU closely followed by other SemPOS and BLEU n combinations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.975560188293457}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9862343072891235}]}, {"text": "We assume this is because BLEU 4 can capture correctly translated fixed phrases, which is positively reflected inhuman judgments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9845902919769287}]}, {"text": "Including BLEU 1 in the combination favors translations with word forms as expected by the refer-  ence, thus allowing to spot bad word forms.", "labels": [], "entities": [{"text": "BLEU 1", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9763942360877991}]}, {"text": "In all cases, the linear combination puts more weight on SemPOS.", "labels": [], "entities": []}, {"text": "Given the negligible difference between SemPOS alone and the linear combinations, we see that word forms are not the major issue for humans interpreting the translation-most likely because the systems so far often make more important errors.", "labels": [], "entities": [{"text": "interpreting the translation-most", "start_pos": 140, "end_pos": 173, "type": "TASK", "confidence": 0.7343241572380066}]}, {"text": "This is also confirmed by the observation that using BLEU alone is rather unreliable for Czech and BLEU-1 (which judges unigrams only) is even worse.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9917375445365906}, {"text": "Czech", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.9166079163551331}, {"text": "BLEU-1", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9966577291488647}]}, {"text": "Surprisingly BLEU-2 performed better than any other n-grams for reasons that have yet to be examined.", "labels": [], "entities": [{"text": "BLEU-2", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9974581599235535}]}, {"text": "The error metrics PER and TER showed the lowest correlation with human judgments for translation to Czech.", "labels": [], "entities": [{"text": "PER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.8764875531196594}, {"text": "TER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9972167015075684}, {"text": "translation to Czech", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.914625863234202}]}], "tableCaptions": [{"text": " Table 1: n-grams confirmed by the reference and  containing error flags.", "labels": [], "entities": []}, {"text": " Table 2: Average, best and worst system-level cor- relation coefficients for translation to English from  various source languages evaluated on 10 different  testsets.", "labels": [], "entities": []}, {"text": " Table 3: System-level correlation coefficients for  English-to-Czech translation evaluated on 3 differ- ent testsets.", "labels": [], "entities": []}]}