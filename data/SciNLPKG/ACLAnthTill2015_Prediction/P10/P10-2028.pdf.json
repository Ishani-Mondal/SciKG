{"title": [{"text": "Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure", "labels": [], "entities": [{"text": "Unsupervised Discourse Segmentation of Documents", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.6771898925304413}]}], "abstractContent": [{"text": "Documents often have inherently parallel structure: they may consist of a text and commentaries, or an abstract and a body, or parts presenting alternative views on the same problem.", "labels": [], "entities": []}, {"text": "Revealing relations between the parts by jointly segmenting and predicting links between the segments, would help to visualize such documents and construct friendlier user interfaces.", "labels": [], "entities": []}, {"text": "To address this problem, we propose an un-supervised Bayesian model for joint discourse segmentation and alignment.", "labels": [], "entities": [{"text": "joint discourse segmentation", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.6248450775941213}]}, {"text": "We apply our method to the \"English as a second language\" podcast dataset where each episode is composed of two parallel parts: a story and an explanatory lecture.", "labels": [], "entities": [{"text": "English as a second language\" podcast dataset", "start_pos": 28, "end_pos": 73, "type": "DATASET", "confidence": 0.6662684120237827}]}, {"text": "The predicted topical links uncover hidden relations between the stories and the lectures.", "labels": [], "entities": []}, {"text": "In this domain, our method achieves competitive results, rivaling those of a previously proposed supervised technique.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many documents consist of parts exhibiting a high degree of parallelism: e.g., abstract and body of academic publications, summaries and detailed news stories, etc.", "labels": [], "entities": []}, {"text": "This is especially common with the emergence of the Web 2.0 technologies: many texts on the web are now accompanied with comments and discussions.", "labels": [], "entities": []}, {"text": "Segmentation of these parallel parts into coherent fragments and discovery of hidden relations between them would facilitate the development of better user interfaces and improve the performance of summarization and information retrieval systems.", "labels": [], "entities": [{"text": "summarization", "start_pos": 198, "end_pos": 211, "type": "TASK", "confidence": 0.9859948754310608}, {"text": "information retrieval", "start_pos": 216, "end_pos": 237, "type": "TASK", "confidence": 0.6743672788143158}]}, {"text": "Discourse segmentation of the documents composed of parallel parts is a novel and challenging problem, as previous research has mostly focused on the linear segmentation of isolated texts (e.g.,).", "labels": [], "entities": [{"text": "Discourse segmentation of the documents composed of parallel parts", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.8718294302622477}]}, {"text": "The most straightforward approach would be to use a pipeline strategy, where an existing segmentation algorithm finds discourse boundaries of each part independently, and then the segments are aligned.", "labels": [], "entities": []}, {"text": "Or, conversely, a sentence-alignment stage can be followed by a segmentation stage.", "labels": [], "entities": []}, {"text": "However, as we will see in our experiments, these strategies may result in poor segmentation and alignment quality.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 80, "end_pos": 92, "type": "TASK", "confidence": 0.9648319482803345}]}, {"text": "To address this problem, we construct a nonparametric Bayesian model for joint segmentation and alignment of parallel parts.", "labels": [], "entities": [{"text": "joint segmentation", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.6805571168661118}, {"text": "alignment of parallel parts", "start_pos": 96, "end_pos": 123, "type": "TASK", "confidence": 0.8046887665987015}]}, {"text": "In comparison with the discussed pipeline approaches, our method has two important advantages: (1) it leverages the lexical cohesion phenomenon) in modeling the parallel parts of documents, and (2) ensures that the effective number of segments can grow adaptively.", "labels": [], "entities": []}, {"text": "Lexical cohesion is an idea that topicallycoherent segments display compact lexical distributions.", "labels": [], "entities": []}, {"text": "We hypothesize that not only isolated fragments but also each group of linked fragments displays a compact and consistent lexical distribution, and our generative model leverages this inter-part cohesion assumption.", "labels": [], "entities": []}, {"text": "In this paper, we consider the dataset of \"English as a second language\" (ESL) podcast 1 , where each episode consists of two parallel parts: a story (an example monologue or dialogue) and an explanatory lecture discussing the meaning and usage of English expressions appearing in the story.", "labels": [], "entities": []}, {"text": "presents an example episode, consisting of two parallel parts, and their hidden topical relations.", "labels": [], "entities": []}, {"text": "From the figure we may conclude that there is a tendency of word repetition between each pair of aligned segments, illustrating our hypothesis of compactness of their joint distribution.", "labels": [], "entities": []}, {"text": "Our goal is I have a day job, but I recently started a small business on the side.", "labels": [], "entities": []}, {"text": "I didn't know anything about accounting and my friend, Roland, said that he would give me some advice.", "labels": [], "entities": [{"text": "accounting", "start_pos": 29, "end_pos": 39, "type": "TASK", "confidence": 0.9384464025497437}]}, {"text": "Roland: So, the reason that you need to do your bookkeeping is so you can manage your cash flow.", "labels": [], "entities": []}, {"text": "This podcast is all about business vocabulary related to accounting.", "labels": [], "entities": []}, {"text": "The title of the podcast is Business Bookkeeping.", "labels": [], "entities": [{"text": "Business Bookkeeping", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.8468601405620575}]}, {"text": "The story begins by Magdalena saying that she has a day job.", "labels": [], "entities": []}, {"text": "A day job is your regular job that you work at from nine in the morning 'til five in the afternoon, for example.", "labels": [], "entities": []}, {"text": "She also has a small business on the side.", "labels": [], "entities": []}, {"text": "Magdalena continues by saying that she didn't know anything about accounting and her friend, Roland, said he would give her some advice.", "labels": [], "entities": []}, {"text": "Accounting is the job of keeping correct records of the money you spend; it's very similar to bookkeeping.", "labels": [], "entities": [{"text": "Accounting", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.7609537243843079}]}, {"text": "Roland begins by saying that the reason that you need to do your bookkeeping is so you can manage your cash flow.", "labels": [], "entities": []}, {"text": "Cash flow, flow, means having enough money to run your business -to pay your bills.", "labels": [], "entities": [{"text": "flow", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.8962180614471436}]}], "datasetContent": [{"text": "Dataset We apply our model to the ESL podcast dataset () of 200 episodes, with an average of 17 sentences per story and 80 sentences per lecture transcript.", "labels": [], "entities": [{"text": "ESL podcast dataset", "start_pos": 34, "end_pos": 53, "type": "DATASET", "confidence": 0.8754224975903829}]}, {"text": "The gold standard alignments assign each fragment of the story to a segment of the lecture transcript.", "labels": [], "entities": []}, {"text": "We can induce segmentations at different levels of granularity on both the story and the lecture side.", "labels": [], "entities": []}, {"text": "However, given that the segmentation of the story was obtained by an automatic sentence splitter, there is no reason to attempt to reproduce this segmentation.", "labels": [], "entities": [{"text": "sentence splitter", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7751835882663727}]}, {"text": "Therefore, for quantitative evaluation purposes we follow and restrict our model to alignment structures which agree with the given segmentation of the story.", "labels": [], "entities": []}, {"text": "For all evaluations, we apply standard stemming algorithm and remove common stop words.", "labels": [], "entities": []}, {"text": "Evaluation metrics To measure the quality of segmentation of the lecture transcript, we use two standard metrics, P k () and WindowDiff (WD)), but both metrics disregard the alignment links (i.e. the topic labels).", "labels": [], "entities": []}, {"text": "Consequently, we also use the macro-averaged F 1 score on pairs of aligned span, which measures both the segmentation and alignment quality.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9462520877520243}]}, {"text": "Baseline Since there has been little previous research on this problem, we compare our results against two straightforward unsupervised baselines.", "labels": [], "entities": []}, {"text": "For the first baseline, we consider the pairwise sentence alignment (SentAlign) based on the unigram and bigram overlap.", "labels": [], "entities": []}, {"text": "The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with and then use the pairwise alignment to find their best alignment to the segments of the story.", "labels": [], "entities": []}, {"text": "Our model We evaluate our joint model of segmentation and alignment both with and without the split/merge moves.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.9809447526931763}]}, {"text": "For the model without these moves, we set the desired number of segments in the lecture to be equal to the actual number of segments in the story I.", "labels": [], "entities": []}, {"text": "In this setting, the moves can only adjust positions of the segment borders.", "labels": [], "entities": []}, {"text": "For the model with the split/merge moves, we start with the same number of segments I but it can be increased or decreased during inference.", "labels": [], "entities": []}, {"text": "For evaluation of our model, we run our inference algorithm from five random states, and take the 100,000th iteration of each chain as a sample.", "labels": [], "entities": []}, {"text": "Results are the average over these five runs.", "labels": [], "entities": []}, {"text": "Also we perform L-BFGS optimization to automatically adjust the non-informative hyperpriors after each 1,000 iterations of sampling.", "labels": [], "entities": []}, {"text": "'Uniform' denotes the minimal baseline which uniformly draws a random set of I spans for each lecture, and then aligns them to the segments of the story preserving the linear order.", "labels": [], "entities": []}, {"text": "Also, we consider two variants of the pipeline approach: segmenting the lecture on I and 2I + 1 segments, respectively.", "labels": [], "entities": [{"text": "segmenting", "start_pos": 57, "end_pos": 67, "type": "TASK", "confidence": 0.9764055013656616}]}, {"text": "Our joint model substantially outperforms the baselines.", "labels": [], "entities": []}, {"text": "The difference is statistically significant with the level p < .01 measured with the paired t-test.", "labels": [], "entities": []}, {"text": "The significant improvement over the pipeline results demonstrates benefits of joint modeling for the considered problem.", "labels": [], "entities": []}, {"text": "Moreover, additional benefits are obtained by using the DP priors and the split/merge moves (the last line in).", "labels": [], "entities": []}, {"text": "Finally, our model significantly outperforms the previously proposed supervised model: they report micro-averaged F 1 score 0.698 while our best model achieves 0.778 with the same metric.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9274152517318726}]}, {"text": "This observation confirms that lexical cohesion modeling is crucial for successful discourse analysis.", "labels": [], "entities": [{"text": "lexical cohesion modeling", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.6884939074516296}, {"text": "discourse analysis", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7513051927089691}]}], "tableCaptions": [{"text": " Table 1: Results on the ESL podcast dataset. For  all metrics, lower values are better.", "labels": [], "entities": [{"text": "ESL podcast dataset", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.8607433438301086}]}]}