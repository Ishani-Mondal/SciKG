{"title": [], "abstractContent": [{"text": "Probabilistic latent topic models have recently enjoyed much success in extracting and analyzing latent topics in text in an un-supervised way.", "labels": [], "entities": []}, {"text": "One common deficiency of existing topic models, though, is that they would notwork well for extracting cross-lingual latent topics simply because words in different languages generally do not co-occur with each other.", "labels": [], "entities": []}, {"text": "In this paper, we propose away to incorporate a bilingual dictionary into a probabilistic topic model so that we can apply topic models to extract shared latent topics in text data of different languages.", "labels": [], "entities": []}, {"text": "Specifically, we propose anew topic model called Probabilis-tic Cross-Lingual Latent Semantic Analysis (PCLSA) which extends the Proba-bilistic Latent Semantic Analysis (PLSA) model by regularizing its likelihood function with soft constraints defined based on a bilingual dictionary.", "labels": [], "entities": [{"text": "Probabilis-tic Cross-Lingual Latent Semantic Analysis (PCLSA)", "start_pos": 49, "end_pos": 110, "type": "TASK", "confidence": 0.6509260050952435}, {"text": "Proba-bilistic Latent Semantic Analysis (PLSA)", "start_pos": 129, "end_pos": 175, "type": "TASK", "confidence": 0.7195538026945931}]}, {"text": "Both qualitative and quantitative experimental results show that the PCLSA model can effectively extract cross-lingual latent topics from multilingual text data.", "labels": [], "entities": []}], "introductionContent": [{"text": "As a robust unsupervised way to perform shallow latent semantic analysis of topics in text, probabilistic topic models) have recently attracted much attention.", "labels": [], "entities": [{"text": "shallow latent semantic analysis of topics in text", "start_pos": 40, "end_pos": 90, "type": "TASK", "confidence": 0.7800277061760426}]}, {"text": "The common idea behind these models is the following.", "labels": [], "entities": []}, {"text": "A topic is represented by a multinomial word distribution so that words characterizing a topic generally have higher probabilities than other words.", "labels": [], "entities": []}, {"text": "We can then hypothesize the existence of multiple topics in text and define a generative model based on the hypothesized topics.", "labels": [], "entities": []}, {"text": "By fitting the model to text data, we can obtain an estimate of all the word distributions corresponding to the latent topics as well as the topic distributions in text.", "labels": [], "entities": []}, {"text": "Intuitively, the learned word distributions capture clusters of words that co-occur with each other probabilistically.", "labels": [], "entities": []}, {"text": "Although many topic models have been proposed and shown to be useful (see Section 2 for more detailed discussion of related work), most of them share a common deficiency: they are designed to work only for mono-lingual text data and would notwork well for extracting cross-lingual latent topics, i.e. topics shared in text data in two different natural languages.", "labels": [], "entities": []}, {"text": "The deficiency comes from the fact that all these models rely on co-occurrences of words forming a topical cluster, but words in different language generally do not co-occur with each other.", "labels": [], "entities": []}, {"text": "Thus with the existing models, we can only extract topics from text in each language, but cannot extract common topics shared in multiple languages.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel topic model, called Probabilistic Cross-Lingual Latent Semantic Analysis (PCLSA) model, which can be used to mine shared latent topics from unaligned text data in different languages.", "labels": [], "entities": [{"text": "Probabilistic Cross-Lingual Latent Semantic Analysis (PCLSA)", "start_pos": 54, "end_pos": 114, "type": "TASK", "confidence": 0.6647839397192001}]}, {"text": "PCLSA extends the Probabilistic Latent Semantic Analysis (PLSA) model by regularizing its likelihood function with soft constraints defined based on a bilingual dictionary.", "labels": [], "entities": [{"text": "Probabilistic Latent Semantic Analysis (PLSA)", "start_pos": 18, "end_pos": 63, "type": "TASK", "confidence": 0.7130977298532214}]}, {"text": "The dictionary-based constraints are key to bridge the gap of different languages and would force the captured co-occurrences of words in each language by PCLSA to be \"synchronized\" so that related words in the two languages would have similar probabilities.", "labels": [], "entities": []}, {"text": "PCLSA can be estimated efficiently using the General ExpectationMaximization (GEM) algorithm.", "labels": [], "entities": []}, {"text": "As a topic extraction algorithm, PCLSA would take a pair of unaligned document sets in different languages and a bilingual dictionary as input, and output a set of aligned word distributions in both languages that can characterize the shared topics in the two languages.", "labels": [], "entities": [{"text": "topic extraction", "start_pos": 5, "end_pos": 21, "type": "TASK", "confidence": 0.833574503660202}]}, {"text": "In addition, it also outputs a topic cov-erage distribution for each language to indicate the relative coverage of different shared topics in each language.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, no previous work has attempted to solve this topic extraction problem and generate the same output.", "labels": [], "entities": [{"text": "topic extraction", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.7270867973566055}]}, {"text": "The closest existing work to ours is the MuTo model proposed in) and the JointLDA model published recently in.", "labels": [], "entities": []}, {"text": "Both used a bilingual dictionary to bridge the language gap in a topic model.", "labels": [], "entities": []}, {"text": "However, the goals of their work are different from ours in that their models mainly focus on mining cross-lingual topics of matching word pairs and discovering the correspondence at the vocabulary level.", "labels": [], "entities": []}, {"text": "Therefore, the topics extracted using their model cannot indicate how a common topic is covered differently in the two languages, because the words in each word pair share the same probability in a common topic.", "labels": [], "entities": []}, {"text": "Our work focuses on discovering correspondence at the topic level.", "labels": [], "entities": []}, {"text": "In our model, since we only add a soft constraint on word pairs in the dictionary, their probabilities in common topics are generally different, naturally capturing which shows the different variations of a common topic in different languages.", "labels": [], "entities": []}, {"text": "We use a cross-lingual news data set and a review data set to evaluate PCLSA.", "labels": [], "entities": [{"text": "PCLSA", "start_pos": 71, "end_pos": 76, "type": "TASK", "confidence": 0.6460807919502258}]}, {"text": "We also propose a \"cross-collection\" likelihood measure to quantitatively evaluate the quality of mined topics.", "labels": [], "entities": []}, {"text": "Experimental results show that the PCLSA model can effectively extract cross-lingual latent topics from multilingual text data, and it outperforms a baseline approach using the standard PLSA on text data in each language.", "labels": [], "entities": []}], "datasetContent": [{"text": "We also quantitatively evaluate how well our PCLSA model can discover common topics among corpus in different languages.", "labels": [], "entities": []}, {"text": "We propose a \"cross-collection\" likelihood measure for this purpose.", "labels": [], "entities": []}, {"text": "The basic idea is: suppose we got k cross-lingual topics from the whole corpus, then for each topic, we split the topic into two separate set of topics, English topics and Chinese topics, using the splitting formula described before, i.e. .", "labels": [], "entities": []}, {"text": "Then, we use the word distribution of the Chinese topics (translating the words into English) to fit the English Corpus and use the word distribution of the English topics (translating the words into Chinese) to fit the Chinese Corpus.", "labels": [], "entities": [{"text": "English Corpus", "start_pos": 105, "end_pos": 119, "type": "DATASET", "confidence": 0.9601860642433167}, {"text": "Chinese Corpus", "start_pos": 220, "end_pos": 234, "type": "DATASET", "confidence": 0.9485250115394592}]}, {"text": "If the topics mined are common topics in the whole corpus, then such a \"crosscollection\" likelihood should be larger than those topics which are not commonly shared by the English and the Chinese corpus.", "labels": [], "entities": []}, {"text": "To calculate the likelihood of fitness, we use the folding-in method proposed in).", "labels": [], "entities": []}, {"text": "To translate topics from one language to another, e.g. Chinese to English, we lookup the bilingual dictionary and do word-to-word translation.", "labels": [], "entities": [{"text": "word-to-word translation", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.7558664381504059}]}, {"text": "If one Chinese word has several English translations, we simply distribute its probability mass equally to each English translation.", "labels": [], "entities": []}, {"text": "For comparison, we use the standard PLSA model as the baseline.", "labels": [], "entities": []}, {"text": "Basically, suppose PLSA mined k semantic topics in the Chinese corpus and k semantic topics in the English corpus.", "labels": [], "entities": []}, {"text": "Then, we also use the \"cross-collection\" likelihood measure to see how well those k semantic Chinese topics fit the English corpus and those k semantic English topics fit the Chinese corpus.", "labels": [], "entities": []}, {"text": "We totally collect three data sets to compare the performance.", "labels": [], "entities": []}, {"text": "For the first data set, the English corpus and Chinese corpus are comparable with each other, because they cover similar events during the same period.", "labels": [], "entities": [{"text": "English corpus and Chinese corpus", "start_pos": 28, "end_pos": 61, "type": "DATASET", "confidence": 0.7575864195823669}]}, {"text": "In the second data set, the English and Chinese corpora share some common topics during the overlap period.", "labels": [], "entities": []}, {"text": "The third data is the most tough one since the two corpora are from different periods.", "labels": [], "entities": []}, {"text": "The purpose of using these three different data sets for evaluation is to test how well PCLSA can mine common topics from either a data set where the English corpus and the Chinese corpus are comparable or a data set where the English corpus and the Chinese corpus rarely share common topics.", "labels": [], "entities": []}, {"text": "The experimental results are shown in.", "labels": [], "entities": []}, {"text": "Each row shows the \"cross-collection\" likelihood of using the \"cross-collection\" topics to fit the data set named in the first column.", "labels": [], "entities": []}, {"text": "For example, in the first row, the values are the \"cross-collection\" likelihood of using Chinese topics found by different methods from the first data set to fit English 1.", "labels": [], "entities": []}, {"text": "The last collum shows how much improvement we got from PCLSA compared with PLSA.", "labels": [], "entities": []}, {"text": "From the results, we can see that in all the data sets, our PCLSA has higher \"cross-collection\" likelihood value, which means it can find better common topics compared to the baseline method.", "labels": [], "entities": []}, {"text": "Notice that the Chinese corpora are the same in all three data sets.", "labels": [], "entities": []}, {"text": "The results show that both PCLSA and PLSA get lower \"cross-collection\" likelihood for fitting the Chinese corpora when the data set becomes \"tougher\", i.e. less topic overlapping, but the im- provement of PCLSA over PLSA does not drop much.", "labels": [], "entities": []}, {"text": "On the other hand, the improvement of PCLSA over PLSA on the three English corpora does not show any correlation with the difficulty of the data set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Synthetic Data Set from Xinhua News", "labels": [], "entities": [{"text": "Synthetic Data Set from Xinhua", "start_pos": 10, "end_pos": 40, "type": "DATASET", "confidence": 0.90890873670578}]}, {"text": " Table 4: Quantitative Evaluation of Common  Topic Finding (\"cross-collection\" log-likelihood)", "labels": [], "entities": [{"text": "Quantitative Evaluation of Common  Topic Finding", "start_pos": 10, "end_pos": 58, "type": "TASK", "confidence": 0.7358569204807281}]}]}