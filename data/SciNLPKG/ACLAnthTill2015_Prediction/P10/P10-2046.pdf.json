{"title": [{"text": "Don't 'have a clue'? Unsupervised co-learning of downward-entailing operators", "labels": [], "entities": []}], "abstractContent": [{"text": "Researchers in textual entailment have begun to consider inferences involving downward-entailing operators, an interesting and important class of lexical items that change the way inferences are made.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7127656638622284}]}, {"text": "Recent work proposed a method for learning English downward-entailing operators that requires access to a high-quality collection of negative polarity items (NPIs).", "labels": [], "entities": []}, {"text": "However, English is one of the very few languages for which such a list exists.", "labels": [], "entities": []}, {"text": "We propose the first approach that can be applied to the many languages for which there is no pre-existing high-precision database of NPIs.", "labels": [], "entities": []}, {"text": "As a case study, we apply our method to Romanian and show that our method yields good results.", "labels": [], "entities": []}, {"text": "Also, we perform a cross-linguistic analysis that suggests interesting connections to some findings in linguistic typology.", "labels": [], "entities": []}], "introductionContent": [{"text": "Cristi: \"Nicio\" ... is that adjective you've mentioned.", "labels": [], "entities": []}, {"text": "Anca: A negative pronominal adjective.", "labels": [], "entities": []}, {"text": "Cristi: You mean there are people who analyze that kind of thing?", "labels": [], "entities": []}, {"text": "Anca: The Romanian Academy.", "labels": [], "entities": [{"text": "Anca: The Romanian Academy", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.8683984398841857}]}, {"text": "Cristi: They're crazy.", "labels": [], "entities": []}, {"text": "-From the movie Police, adjective Downward-entailing operators are an interesting and varied class of lexical items that change the default way of dealing with certain types of inferences.", "labels": [], "entities": []}, {"text": "They thus play an important role in understanding natural language.", "labels": [], "entities": [{"text": "understanding natural language", "start_pos": 36, "end_pos": 66, "type": "TASK", "confidence": 0.8397687474886576}]}, {"text": "We explain what downward entailing means by first demonstrating the \"default\" behavior, which is upward entailing.", "labels": [], "entities": []}, {"text": "The word 'observed' is an example upward-entailing operator: the statement but not vice versa (we write i \u21d2 ( \u21d0) ii).", "labels": [], "entities": []}, {"text": "That is, the truth value is preserved if we replace the argument of an upward-entailing operator by a superset (a more general version); in our case, the set 'opium use' was replaced by the superset 'narcotic use'.", "labels": [], "entities": []}, {"text": "Downward-entailing (DE) (also known as downward monotonic or monotone decreasing) operators violate this default inference rule: with DE operators, reasoning instead goes from \"sets to subsets\".", "labels": [], "entities": []}, {"text": "An example is the word 'bans': 'The law bans opium use' \u21d2 (\u21d0) 'The law bans narcotic use'.", "labels": [], "entities": []}, {"text": "Although DE behavior represents an exception to the default, DE operators are as a class rather common.", "labels": [], "entities": []}, {"text": "They are also quite diverse in sense and even part of speech.", "labels": [], "entities": []}, {"text": "Some are simple negations, such as 'not', but some other English DE operators are 'without', 'reluctant to', 'to doubt', and 'to allow'.", "labels": [], "entities": []}, {"text": "1 This variety makes them hard to extract automatically.", "labels": [], "entities": []}, {"text": "Because DE operators violate the default \"sets to supersets\" inference, identifying them can potentially improve performance in many NLP tasks.", "labels": [], "entities": []}, {"text": "Perhaps the most obvious such tasks are those involving textual entailment, such as question answering, information extraction, summarization, and the evaluation of machine translation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.859295517206192}, {"text": "information extraction", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.8487235307693481}, {"text": "summarization", "start_pos": 128, "end_pos": 141, "type": "TASK", "confidence": 0.9882426857948303}, {"text": "evaluation of machine translation", "start_pos": 151, "end_pos": 184, "type": "TASK", "confidence": 0.6299543529748917}]}, {"text": "Researchers are in fact beginning to build textualentailment systems that can handle inferences involving downward-entailing operators other than simple negations, although these systems almost all rely on small handcrafted lists of DE operators.", "labels": [], "entities": []}, {"text": "Other application areas are naturallanguage generation and human-computer interaction, since downward-entailing inferences induce Some examples showing different constructions for analyzing these operators: 'The defendant does not own a blue car' \u21d2 (\u21d0) 'The defendant does not own a car'; 'They are reluctant to tango' \u21d2 (\u21d0) 'They are reluctant to dance'; 'Police doubt Smith threatened Jones' \u21d2 (\u21d0) 'Police doubt Smith threatened Jones or Brown'; 'You are allowed to use Mastercard' \u21d2 (\u21d0) 'You are allowed to use any credit card'.", "labels": [], "entities": [{"text": "naturallanguage generation", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.7351577579975128}, {"text": "Mastercard", "start_pos": 472, "end_pos": 482, "type": "DATASET", "confidence": 0.9777746200561523}]}, {"text": "The exception employs the list automatically derived by Danescu-Niculescu-Mizil, Lee, and Ducott, described later.", "labels": [], "entities": []}, {"text": "greater cognitive load than inferences in the opposite direction.", "labels": [], "entities": []}, {"text": "Most NLP systems for the applications mentioned above have only been deployed fora small subset of languages.", "labels": [], "entities": []}, {"text": "A key factor is the lack of relevant resources for other languages.", "labels": [], "entities": []}, {"text": "While one approach would be to separately develop a method to acquire such resources for each language individually, we instead aim to ameliorate the resource-scarcity problem in the case of DE operators wholesale: we propose a single unsupervised method that can extract DE operators in any language for which raw text corpora exist.", "labels": [], "entities": []}, {"text": "Overview of our work Our approach takes the English-centric work of Danescu-Niculescu-Mizil et al.", "labels": [], "entities": []}, {"text": "-DLD09 for short -as a starting point, as they present the first and, until now, only algorithm for automatically extracting DE operators from data.", "labels": [], "entities": []}, {"text": "However, our work departs significantly from DLD09 in the following key respect.", "labels": [], "entities": [{"text": "DLD09", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.9252504110336304}]}, {"text": "DLD09 critically depends on access to a highquality, carefully curated collection of negative polarity items (NPIs) -lexical items such as 'any', 'ever', or the idiom 'have a clue' that tend to occur only in negative environments (see \u00a72 for more details).", "labels": [], "entities": [{"text": "DLD09", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8881471157073975}]}, {"text": "DLD09 use NPIs as signals of the occurrence of downward-entailing operators.", "labels": [], "entities": [{"text": "DLD09", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9401670694351196}]}, {"text": "However, almost every language other than English lacks a high-quality accessible NPI list.", "labels": [], "entities": []}, {"text": "To circumvent this problem, we introduce a knowledge-lean co-learning approach.", "labels": [], "entities": []}, {"text": "Our algorithm is initialized with a very small seed set of NPIs (which we describe how to generate), and then iterates between (a) discovering a set of DE operators using a collection of pseudo-NPIs -a concept we introduce -and (b) using the newlyacquired DE operators to detect new pseudo-NPIs.", "labels": [], "entities": []}, {"text": "Why this isn't obvious Although the algorithmic idea sketched above seems quite simple, it is important to note that prior experiments in that direction have not proved fruitful.", "labels": [], "entities": []}, {"text": "Preliminary work on learning (German) NPIs using a small list of simple known DE operators did not yield strong results.", "labels": [], "entities": []}, {"text": "Hoeksema discusses why NPIs might be hard to learn from data.", "labels": [], "entities": []}, {"text": "We circumvent this problem because we are not interested in learning NPIs per se; rather, for our pur-poses, pseudo-NPIs suffice.", "labels": [], "entities": []}, {"text": "Also, our preliminary work determined that one of the most famous co-learning algorithms, hubs and authorities or HITS, is poorly suited to our problem.", "labels": [], "entities": []}, {"text": "Contributions To begin with, we apply our algorithm to produce the first large list of DE operators fora language other than English.", "labels": [], "entities": []}, {"text": "In our case study on Romanian ( \u00a74), we achieve quite high precisions at k (for example, iteration achieves a precision at 30 of 87%).", "labels": [], "entities": [{"text": "precisions", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9968696236610413}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9965671300888062}]}, {"text": "Auxiliary experiments explore the effects of using a large but noisy NPI list, should one be available for the language in question.", "labels": [], "entities": []}, {"text": "Intriguingly, we find that co-learning new pseudo-NPIs provides better results.", "labels": [], "entities": []}, {"text": "Finally ( \u00a75), we engage in some cross-linguistic analysis based on the results of applying our algorithm to English.", "labels": [], "entities": []}, {"text": "We find that there are some suggestive connections with findings in linguistic typology.", "labels": [], "entities": []}, {"text": "Appendix available A more complete account of our work and its implications can be found in aversion of this paper containing appendices, available at www.cs.cornell.edu/\u02dccristian/acl2010/.", "labels": [], "entities": [{"text": "aversion", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9602739810943604}]}], "datasetContent": [{"text": "We used Rada Mihalcea's corpus of \u22481.45 million sentences of raw Romanian newswire articles.", "labels": [], "entities": [{"text": "Rada Mihalcea's corpus of \u22481.45 million sentences of raw Romanian newswire articles", "start_pos": 8, "end_pos": 91, "type": "DATASET", "confidence": 0.7994027584791183}]}, {"text": "Note that we cannot evaluate impact on textual inference because, to our knowledge, no publicly available textual-entailment system or evaluation data for Romanian exists.", "labels": [], "entities": [{"text": "textual inference", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.6963992565870285}]}, {"text": "We therefore examine the system outputs directly to determine whether the top-ranked items are actually DE operators or not.", "labels": [], "entities": []}, {"text": "Our evaluation metric is precision at k of a given system's ranked list of candidate DE operators; it is not possible to evaluate recall since no list of Romanian DE operators exists (a problem that is precisely the motivation for this paper).", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9992569088935852}, {"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9980397820472717}]}, {"text": "To evaluate the results, two native Romanian speakers labeled the system outputs as being \"DE\", \"not DE\" or \"Hard (to decide)\".", "labels": [], "entities": [{"text": "DE", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.995571494102478}, {"text": "DE", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9886741042137146}]}, {"text": "The labeling protocol, which was somewhat complex to prevent bias, is described in the externallyavailable appendices ( \u00a77.1).", "labels": [], "entities": [{"text": "labeling", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.9532619714736938}]}, {"text": "The complete system output and annotations are publicly available at: http://www.cs.cornell.edu/\u02dccristian/acl2010/.", "labels": [], "entities": []}], "tableCaptions": []}