{"title": [{"text": "Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates", "labels": [], "entities": []}], "abstractContent": [{"text": "Despite its substantial coverage, Nom-Bank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether.", "labels": [], "entities": []}, {"text": "These arguments , which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications.", "labels": [], "entities": [{"text": "semantic processing", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8740817308425903}]}, {"text": "We present a study of implicit arguments fora select group of frequent nominal predicates.", "labels": [], "entities": []}, {"text": "We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank.", "labels": [], "entities": [{"text": "NomBank", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9166294932365417}]}, {"text": "We demonstrate the feasibility of recovering implicit arguments with a supervised classification model.", "labels": [], "entities": []}, {"text": "Our results and analyses provide a baseline for future work on this emerging task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Verbal and nominal semantic role labeling (SRL) have been studied independently of each other as well as jointly ().", "labels": [], "entities": [{"text": "nominal semantic role labeling (SRL)", "start_pos": 11, "end_pos": 47, "type": "TASK", "confidence": 0.7213610410690308}]}, {"text": "These studies have demonstrated the maturity of SRL within an evaluation setting that restricts the argument search space to the sentence containing the predicate of interest.", "labels": [], "entities": [{"text": "SRL", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9934146404266357}]}, {"text": "However, as shown by the following example from the Penn TreeBank (, this restriction excludes extra-sentential arguments: (1) [arg The two companies] [arg 1 market pulp, containerboard and white paper].", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9937381744384766}]}, {"text": "The goods could be manufactured closer to customers, saving costs.", "labels": [], "entities": []}, {"text": "The first sentence in Example 1 includes the PropBank () analysis of the verbal predicate produce, where arg 0 is the agentive producer and arg 1 is the produced entity.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.7887466549873352}]}, {"text": "The second sentence contains an instance of the nominal predicate shipping that is not associated with arguments in NomBank.", "labels": [], "entities": []}, {"text": "From the sentences in Example 1, the reader can infer that The two companies refers to the agents (arg 0 ) of the shipping predicate.", "labels": [], "entities": []}, {"text": "The reader can also infer that market pulp, containerboard and white paper refers to the shipped entities (arg 1 of shipping).", "labels": [], "entities": []}, {"text": "1 These extra-sentential arguments have not been annotated for the shipping predicate and cannot be identified by a system that restricts the argument search space to the sentence containing the predicate.", "labels": [], "entities": []}, {"text": "NomBank also ignores many within-sentence arguments.", "labels": [], "entities": []}, {"text": "This is shown in the second sentence of Example 1, where The goods can be interpreted as the arg 1 of shipping.", "labels": [], "entities": []}, {"text": "These examples demonstrate the presence of arguments that are not included in NomBank and cannot easily be identified by systems trained on the resource.", "labels": [], "entities": []}, {"text": "We refer to these arguments as implicit.", "labels": [], "entities": []}, {"text": "This paper presents our study of implicit arguments for nominal predicates.", "labels": [], "entities": []}, {"text": "We began our study by annotating implicit arguments fora select group of predicates.", "labels": [], "entities": []}, {"text": "For these predicates, we found that implicit arguments add 65% to the existing role coverage of NomBank.", "labels": [], "entities": [{"text": "NomBank", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9172511696815491}]}, {"text": "This increase has implications for tasks (e.g., question answering, information extraction, and summarization) that benefit from semantic analysis.", "labels": [], "entities": [{"text": "question answering", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8599580824375153}, {"text": "information extraction", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7885544896125793}, {"text": "summarization", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.9853011965751648}, {"text": "semantic analysis", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.8722789287567139}]}, {"text": "Using our annotations, we constructed a feature-based model for automatic implicit argument identification that unifies standard verbal and nominal SRL.", "labels": [], "entities": [{"text": "automatic implicit argument identification", "start_pos": 64, "end_pos": 106, "type": "TASK", "confidence": 0.5816359296441078}]}, {"text": "Our results indicate a 59% relative (15-point absolute) gain in F 1 over an informed baseline.", "labels": [], "entities": [{"text": "relative (15-point absolute) gain", "start_pos": 27, "end_pos": 60, "type": "METRIC", "confidence": 0.7385677744944891}, {"text": "F 1", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9763363301753998}]}, {"text": "Our analyses highlight strengths and weaknesses of the approach, providing insights for future work on this emerging task.", "labels": [], "entities": []}, {"text": "In the following section, we review related research, which is historically sparse but recently gaining traction.", "labels": [], "entities": []}, {"text": "We present our annotation effort in Section 3, and follow with our implicit argument identification model in Section 4.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.7288634181022644}]}, {"text": "In Section 5, we describe the evaluation setting and present our experimental results.", "labels": [], "entities": []}, {"text": "We analyze these results in Section 6 and conclude in Section 7.", "labels": [], "entities": []}, {"text": "made one of the earliest attempts to automatically recover extra-sentential arguments.", "labels": [], "entities": []}, {"text": "Their approach used a fine-grained domain model to assess the compatibility of candidate arguments and the slots needing to be filled.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained the feature-based logistic regression model over 816 annotated predicate instances associated with 650 implicitly filled argument positions (not all predicate instances had implicit arguments).", "labels": [], "entities": []}, {"text": "During training, a candidate three-tuple p, iarg n , c was given a positive label if the candidate implicit argument c (the primary filler) was annotated as filling the missing argument position.", "labels": [], "entities": []}, {"text": "To factor out errors from standard SRL analyses, the model used gold-standard argument labels provided by PropBank and NomBank.", "labels": [], "entities": [{"text": "SRL", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9773247241973877}, {"text": "PropBank", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.9591137170791626}, {"text": "NomBank", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.8837382793426514}]}, {"text": "As shown in (Section 3.2), implicit arguments tend to be located in close proximity to the predicate.", "labels": [], "entities": []}, {"text": "We found that using all candidate constituents c within the current and previous two sentences worked best on our development data.", "labels": [], "entities": []}, {"text": "We compared our supervised model with the simple baseline heuristic defined below: Fill iarg n for predicate instance p with the nearest constituent in the twosentence candidate window that fills arg n fora different instance of p, where all nominal predicates are normalized to their verbal forms.", "labels": [], "entities": []}, {"text": "The normalization allows an existing arg 0 for the verb invested to fill an iarg 0 for the noun investment.", "labels": [], "entities": []}, {"text": "We also evaluated an oracle model that made gold-standard predictions for candidates within the two-sentence prediction window.", "labels": [], "entities": []}, {"text": "We evaluated these models using the methodology proposed by.", "labels": [], "entities": []}, {"text": "For each missing argument position of a predicate instance, the models were required to either (1) identify a single constituent that fills the missing argument position or (2) make no prediction and leave the missing argument position unfilled.", "labels": [], "entities": []}, {"text": "We scored predictions using the Dice coefficient, which is defined as follows: P redicted is the set of tokens subsumed by the constituent predicted by the model as filling a missing argument position.", "labels": [], "entities": []}, {"text": "T rue is the set of tokens from a single annotated constituent that fills the missing argument position.", "labels": [], "entities": [{"text": "T rue", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8401892781257629}]}, {"text": "The model's prediction receives a score equal to the maximum Dice overlap across anyone of the annotated fillers.", "labels": [], "entities": [{"text": "Dice overlap", "start_pos": 61, "end_pos": 73, "type": "METRIC", "confidence": 0.9707452356815338}]}, {"text": "Precision is equal to the summed prediction scores divided by the number of argument positions filled by the model.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9940221905708313}]}, {"text": "Recall is equal to the summed prediction scores divided by the number of argument positions filled in our annotated data.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9821574687957764}, {"text": "summed prediction scores", "start_pos": 23, "end_pos": 47, "type": "METRIC", "confidence": 0.7242328524589539}]}, {"text": "Predictions not covering the head of a true filler were assigned a score of zero.: Evaluation results.", "labels": [], "entities": []}, {"text": "The second column gives the number of predicate instances evaluated.", "labels": [], "entities": []}, {"text": "The third column gives the number of ground-truth implicitly filled argument positions for the predicate instances (not all instances had implicit arguments).", "labels": [], "entities": []}, {"text": "P , R, and F 1 indicate precision, recall, and Fmeasure (\u03b2 = 1), respectively.", "labels": [], "entities": [{"text": "F 1", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9801743924617767}, {"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9967873096466064}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9995916485786438}, {"text": "Fmeasure", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9996352195739746}]}, {"text": "p-values denote the bootstrapped significance of the difference in F 1 between the baseline and discriminative models.", "labels": [], "entities": [{"text": "F 1", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9395082294940948}]}, {"text": "Oracle precision (not shown) is 100% for all predicates.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9417136311531067}]}, {"text": "Our evaluation data comprised 437 predicate instances associated with 246 implicitly filled argument positions.", "labels": [], "entities": []}, {"text": "Predicates with the highest number of implicit arguments -sale and price -showed F 1 increases of 8 points and 18.8 points, respectively.", "labels": [], "entities": [{"text": "F 1", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9949858784675598}]}, {"text": "Overall, the discriminative model increased F 1 performance 15.8 points (59.6%) over the baseline.", "labels": [], "entities": [{"text": "F 1 performance", "start_pos": 44, "end_pos": 59, "type": "METRIC", "confidence": 0.9028115471204122}]}, {"text": "We measured human performance on this task by running our undergraduate assistant's annotations against the evaluation data.", "labels": [], "entities": []}, {"text": "Our assistant achieved an overall F 1 score of 58.4% using the same candidate window as the baseline and discriminative models.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9920529325803121}]}, {"text": "The difference in F 1 between the discriminative and human results had an exact p-value of less than 0.001.", "labels": [], "entities": [{"text": "F 1", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9937945902347565}, {"text": "exact p-value", "start_pos": 74, "end_pos": 87, "type": "METRIC", "confidence": 0.9556764960289001}]}, {"text": "All significance testing was performed using a two-tailed bootstrap method similar to the one described by.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Predicates targeted for annotation. The second column gives the number of predicate instances  annotated. Pre-annotation numbers only include NomBank annotations, whereas Post-annotation num- bers include NomBank and implicit argument annotations. Role coverage indicates the percentage of  roles filled. Role average indicates how many roles, on average, are filled for an instance of a predicate's  noun form or verb form within the TreeBank. Verbal role averages were computed using PropBank.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 496, "end_pos": 504, "type": "DATASET", "confidence": 0.9629454612731934}]}, {"text": " Table 3: Evaluation results. The second column gives the number of predicate instances evaluated.  The third column gives the number of ground-truth implicitly filled argument positions for the predicate  instances (not all instances had implicit arguments). P , R, and F 1 indicate precision, recall, and F- measure (\u03b2 = 1), respectively. p-values denote the bootstrapped significance of the difference in F 1  between the baseline and discriminative models. Oracle precision (not shown) is 100% for all predicates.", "labels": [], "entities": [{"text": "precision", "start_pos": 284, "end_pos": 293, "type": "METRIC", "confidence": 0.9963219165802002}, {"text": "recall", "start_pos": 295, "end_pos": 301, "type": "METRIC", "confidence": 0.9982097148895264}, {"text": "F- measure", "start_pos": 307, "end_pos": 317, "type": "METRIC", "confidence": 0.9866398771603903}, {"text": "precision", "start_pos": 468, "end_pos": 477, "type": "METRIC", "confidence": 0.8882047533988953}]}]}