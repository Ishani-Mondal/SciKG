{"title": [{"text": "A Cognitive Cost Model of Annotations Based on Eye-Tracking Data", "labels": [], "entities": []}], "abstractContent": [{"text": "We report on an experiment to track complex decision points in linguistic meta-data annotation where the decision behavior of annotators is observed with an eye-tracking device.", "labels": [], "entities": []}, {"text": "As experimental conditions we investigate different forms of textual context and linguistic complexity classes relative to syntax and semantics.", "labels": [], "entities": []}, {"text": "Our data renders evidence that annotation performance depends on the semantic and syntactic complexity of the decision points and, more interestingly, indicates that full-scale context is mostly negligible-with the exception of semantic high-complexity cases.", "labels": [], "entities": []}, {"text": "We then induce from this observational data a cognitively grounded cost model of linguistic meta-data annotations and compare it with existing non-cognitive models.", "labels": [], "entities": []}, {"text": "Our data reveals that the cogni-tively founded model explains annotation costs (expressed in annotation time) more adequately than non-cognitive ones.", "labels": [], "entities": []}], "introductionContent": [{"text": "Today's NLP systems, in particular those relying on supervised ML approaches, are meta-data greedy.", "labels": [], "entities": []}, {"text": "Accordingly, in the past years, we have witnessed a massive quantitative growth of annotated corpora.", "labels": [], "entities": []}, {"text": "They differ in terms of the natural languages and domains being covered, the types of linguistic meta-data being solicited, and the text genres being served.", "labels": [], "entities": []}, {"text": "We have seen largescale efforts in syntactic and semantic annotations in the past related to POS tagging and parsing, on the one hand, and named entities and relations (propositions), on the other hand.", "labels": [], "entities": [{"text": "POS tagging and parsing", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.786464586853981}]}, {"text": "More recently, we are dealing with even more challenging issues such as subjective language, a large variety of co-reference and (e.g., RST-style) text structure phenomena, Since the NLP community is further extending their work into these more and more sophisticated semantic and pragmatic analytics, there seems to be no end insight for increasingly complex and diverse annotation tasks.", "labels": [], "entities": [{"text": "RST-style) text structure phenomena", "start_pos": 136, "end_pos": 171, "type": "TASK", "confidence": 0.8288793683052063}]}, {"text": "Yet, producing annotations is pretty expensive.", "labels": [], "entities": []}, {"text": "So the question comes up, how we can rationally manage these investments so that annotation campaigns are economically doable without loss in annotation quality.", "labels": [], "entities": []}, {"text": "The economics of annotations are at the core of Active Learning (AL) where those linguistic samples are focused on in the entire document collection, which are estimated as being most informative to learn an effective classification model.", "labels": [], "entities": []}, {"text": "This intentional selection bias stands in stark contrast to prevailing sampling approaches where annotation examples are randomly chosen.", "labels": [], "entities": []}, {"text": "When different approaches to AL are compared with each other, or with standard random sampling, in terms of annotation efficiency, up until now, the AL community assumed uniform annotation costs for each linguistic unit, e.g. words.", "labels": [], "entities": []}, {"text": "This claim, however, has been shown to be invalid in several studies.", "labels": [], "entities": []}, {"text": "If uniformity does not hold and, hence, the number of annotated units does not indicate the true annotation efforts required fora specific sample, empirically more adequate cost models are needed.", "labels": [], "entities": []}, {"text": "Building predictive models for annotation costs has only been addressed in few studies for now (.", "labels": [], "entities": []}, {"text": "The proposed models are based on easy-to-determine, yet not so explanatory variables (such as the number of words to be annotated), indicating that accurate models of annotation costs remain a desideratum.", "labels": [], "entities": []}, {"text": "We here, alternatively, consider different classes of syntactic and semantic complexity that might affect the cognitive load during the annotation process, with the overall goal to find additional and empirically more adequate variables for cost modeling.", "labels": [], "entities": []}, {"text": "The complexity of linguistic utterances can be judged either by structural or by behavioral criteria.", "labels": [], "entities": []}, {"text": "Structural complexity emerges, e.g., from the static topology of phrase structure trees and procedural graph traversals exploiting the topology of parse trees (see or fora survey of metrics of this type).", "labels": [], "entities": []}, {"text": "However, structural complexity criteria do not translate directly into empirically justified cost measures and thus have to betaken with care.", "labels": [], "entities": []}, {"text": "The behavioral approach accounts for this problem as it renders observational data of the annotators' eye movements.", "labels": [], "entities": []}, {"text": "The technical vehicle to gather such data are eye-trackers which have already been used in psycholinguistics).", "labels": [], "entities": []}, {"text": "Eye-trackers were able to reveal, e.g., how subjects deal with ambiguities or with sentences which require re-analysis, so-called garden path sentences.", "labels": [], "entities": []}, {"text": "The rationale behind the use of eye-tracking devices for the observation of annotation behavior is that the length of gaze durations and behavioral patterns underlying gaze movements are considered to be indicative of the hardness of the linguistic analysis and the expenditures for the search of clarifying linguistic evidence (anchor words) to resolve hard decision tasks such as phrase attachments or word sense disambiguation.", "labels": [], "entities": [{"text": "phrase attachments", "start_pos": 382, "end_pos": 400, "type": "TASK", "confidence": 0.7464758455753326}, {"text": "word sense disambiguation", "start_pos": 404, "end_pos": 429, "type": "TASK", "confidence": 0.6294926603635153}]}, {"text": "Gaze duration and search time are then taken as empirical correlates of linguistic complexity and, hence, uncover the real costs.", "labels": [], "entities": [{"text": "Gaze duration", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.8399690091609955}]}, {"text": "We therefore consider eyetracking as a promising means to get a better understanding of the nature of the linguistic annotation processes with the ultimate goal of identifying predictive factors for annotation cost models.", "labels": [], "entities": []}, {"text": "In this paper, we first describe an empirical study where we observed the annotators' reading behavior while annotating a corpus.", "labels": [], "entities": []}, {"text": "Section 2 deals with the design of the study, Section 3 discusses its results.", "labels": [], "entities": []}, {"text": "In Section 4 we then focus on the implications this study has on building cost models and compare a simple cost model mainly relying on word and character counts and additional simple descriptive characteristics with one that can be derived from experimental data as provided from eye-tracking.", "labels": [], "entities": []}, {"text": "We conclude with experiments which reveal that cognitively grounded models outperform simpler ones relative to cost prediction using annotation time as a cost measure.", "labels": [], "entities": []}, {"text": "Based on this finding, we suggest that cognitive criteria are helpful for uncovering the real costs of corpus annotation.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our study, we applied, for the first time ever to the best of our knowledge, eye-tracking to study the cognitive processes underlying the annotation of linguistic meta-data, named entities in particular.", "labels": [], "entities": []}, {"text": "In this task, a human annotator has to decide for each word whether or not it belongs to one of the entity types of interest.", "labels": [], "entities": []}, {"text": "We used the English part of the MUC7 corpus) for our study.", "labels": [], "entities": [{"text": "MUC7 corpus", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.9153618812561035}]}, {"text": "It contains New York Times articles from 1996 reporting on plane crashes.", "labels": [], "entities": []}, {"text": "These articles come already annotated with three types of named entities considered important in the newspaper domain, viz.", "labels": [], "entities": []}, {"text": "\"persons\", \"locations\", and \"organizations\".", "labels": [], "entities": []}, {"text": "Annotation of these entity types in newspaper articles is admittedly fairly easy.", "labels": [], "entities": [{"text": "Annotation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9801919460296631}]}, {"text": "We chose this rather simple setting because the participants in the experiment had no previous experience with document annotation and no serious linguistic background.", "labels": [], "entities": []}, {"text": "Moreover, the limited number of entity types reduced the amount of participants' training prior to the actual experiment, and positively affected the design and handling of the experimental apparatus (see below).", "labels": [], "entities": []}, {"text": "We triggered the annotation processes by giving our participants specific annotation examples.", "labels": [], "entities": []}, {"text": "An example consists of a text document having one single annotation phrase highlighted which then had to be semantically annotated with respect to named entity mentions.", "labels": [], "entities": []}, {"text": "The annotation task was defined such that the correct entity type had to be assigned to each word in the annotation phrase.", "labels": [], "entities": []}, {"text": "If a word belongs to none of the three entity types a fourth class called \"no entity\" had to be assigned.", "labels": [], "entities": []}, {"text": "The phrases highlighted for annotation were complex noun phrases (CNPs), each a sequence of words where a noun (or an equivalent nominal expression) constitutes the syntactic head and thus dominates dependent words such as determiners, adjectives, or other nouns or nominal expressions (including noun phrases and prepositional phrases).", "labels": [], "entities": []}, {"text": "CNPs with even more elaborate internal syntactic structures, such as coordinations, appositions, or relative clauses, were isolated from their syntactic host structure and the intervening linguistic material containing these structures was deleted to simplify overly long sentences.", "labels": [], "entities": []}, {"text": "We also discarded all CNPs that did not contain at least one entity-critical word, i.e., one which might be a named entity according to its orthographic appearance (e.g., starting with an upper-case letter).", "labels": [], "entities": []}, {"text": "It should be noted that such orthographic signals are by no means a sufficient condition for the presence of a named entity mention within a CNP.", "labels": [], "entities": []}, {"text": "The choice of CNPs as stimulus phrases is motivated by the fact that named entities are usually fully encoded by this kind of linguistic structure.", "labels": [], "entities": []}, {"text": "The chosen stimulus -an annotation example with one phrase highlighted for annotation -allows for an exact localization of the cognitive processes and annotation actions performed relative to that specific phrase.", "labels": [], "entities": []}, {"text": "The annotation examples were presented in a custom-built tool and its user interface was kept as simple as possible not to distract the eye movements of the participants.", "labels": [], "entities": []}, {"text": "It merely contained one frame showing the text of the annotation example, with the annotation phrase being highlighted.", "labels": [], "entities": []}, {"text": "A blank screen was shown after each annotation example to reset the eyes and to allow a break, if needed.", "labels": [], "entities": []}, {"text": "The time the blank screen was shown was not counted as annotation time.", "labels": [], "entities": []}, {"text": "The 80 annotation examples were presented to all participants in the same randomized order, with a balanced distribution of the complexity classes.", "labels": [], "entities": []}, {"text": "A variation of the order was hardly possible for technical and analytical reasons but is not considered critical due to extensive, pre-experimental training (see below).", "labels": [], "entities": []}, {"text": "The limitation on 80 annotation examples reduces the chances of errors due to fatigue or lack of attention that can be observed in long-lasting annotation activities.", "labels": [], "entities": []}, {"text": "Five introductory examples (not considered in the final evaluation) were given to get the subjects used to the experimental environment.", "labels": [], "entities": []}, {"text": "All annotation examples were chosen in away that they completely fitted on the screen (i.e., text length was limited) to avoid the need for scrolling (and eye distraction).", "labels": [], "entities": []}, {"text": "The position of the CNP within the respective context was randomly distributed, excluding the first and last sentence.", "labels": [], "entities": []}, {"text": "The participants used a standard keyboard to assign the entity types for each word of the annotation example.", "labels": [], "entities": []}, {"text": "All but 5 keys were removed from the keyboard to avoid extra eye movements for finger coordination (three keys for the positive entity classes, one for the negative \"no entity\" class, and one to confirm the annotation).", "labels": [], "entities": []}, {"text": "Pre-tests had shown that the participants could easily issue the annotations without looking down at the keyboard.", "labels": [], "entities": []}, {"text": "We recorded the participant's eye movements on a Tobii T60 eye-tracking device which is invisibly embedded in a 17\" TFT monitor and comparatively tolerant to head movements.", "labels": [], "entities": [{"text": "Tobii T60 eye-tracking device", "start_pos": 49, "end_pos": 78, "type": "DATASET", "confidence": 0.913535550236702}]}, {"text": "The participants were seated in a comfortable position with their head in a distance of 60-70 cm from the monitor.", "labels": [], "entities": []}, {"text": "Screen resolution was set to 1280 x 1024 px and the annotation examples were presented in the middle of the screen in a font size of 16 px and a line spacing of 5 px.", "labels": [], "entities": []}, {"text": "The presentation area had no fixed height and varied depending on the context condition and length of the newspaper article.", "labels": [], "entities": []}, {"text": "The text was always vertically centered on the screen.", "labels": [], "entities": []}, {"text": "All participants were familiarized with the annotation task and the guidelines in a preexperimental workshop where they practiced annotations on various exercise examples (about 60 minutes).", "labels": [], "entities": []}, {"text": "During the next two days, one after the other participated in the actual experiment which took between 15 and 30 minutes, including calibration of the eye-tracking device.", "labels": [], "entities": []}, {"text": "Another 20-30 minutes of training time directly preceded the experiment.", "labels": [], "entities": []}, {"text": "After the experiment, participants were interviewed and asked to fill out a questionnaire.", "labels": [], "entities": []}, {"text": "Overall, the experiment took about two hours for each participant for which they were financially compensated.", "labels": [], "entities": []}, {"text": "Participants were instructed to focus more on annotation accuracy than on annotation time as we wanted to avoid random guessing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.8186566233634949}]}, {"text": "Accordingly, as an extra incentive, we rewarded the three participants with the highest annotation accuracy with cinema vouchers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9519678950309753}]}, {"text": "None of the participants reported serious difficulties with the newspaper articles or annotation tool and all understood the annotation task very well.", "labels": [], "entities": []}, {"text": "To test how well annotation costs can be modeled by the features described above, we used the MUC7 T corpus, a re-annotation of the MUC7 corpus).", "labels": [], "entities": [{"text": "MUC7 T corpus", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.9171960552533468}, {"text": "MUC7 corpus", "start_pos": 132, "end_pos": 143, "type": "DATASET", "confidence": 0.978755384683609}]}, {"text": "MUC7 T has time tags attached to the sentences and CNPs.", "labels": [], "entities": [{"text": "MUC7 T", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9256368279457092}]}, {"text": "These time tags indicate the time it took to annotate the respective phrase for named entity mentions of the types person, location, and organization.", "labels": [], "entities": []}, {"text": "We here made use of the time tags of the 15,203 CNPs in MUC7 T . MUC7 T has been annotated by two annotators (henceforth called A and B) and so we evaluated the cost models for both annotators.", "labels": [], "entities": [{"text": "MUC7 T", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.9094889163970947}, {"text": "MUC7 T", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.8644172847270966}]}, {"text": "We learned a simple linear regression model with the annotation time as dependent variable and the features described above as independent variables.", "labels": [], "entities": []}, {"text": "The baseline model only includes the basic feature set, whereas the 'cognitive' model incorporates all features described above.", "labels": [], "entities": []}, {"text": "depicts the performance of both models induced from the data of annotator A and B.", "labels": [], "entities": []}, {"text": "The coefficient of determination (R 2 ) describes the proportion of the variance of the dependent variable that can be described by the given model.", "labels": [], "entities": [{"text": "coefficient of determination (R 2 )", "start_pos": 4, "end_pos": 39, "type": "METRIC", "confidence": 0.8690620405333382}]}, {"text": "We report adjusted R 2 to account for the different numbers of features used in both models.", "labels": [], "entities": [{"text": "R 2", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9827351570129395}]}, {"text": "For both annotators, the baseline model is significantly outperformed in terms of R 2 by our 'cognitive' model (p < 0.05).", "labels": [], "entities": [{"text": "R 2", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9554636180400848}]}, {"text": "Considering the features that were inspired from the eye-tracking study, R 2 is increased from 0.4695 to 0.6263 on the timing data of annotator A, and from 0.464 to 0.6185 on the data of annotator B.", "labels": [], "entities": [{"text": "R 2", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9889395236968994}, {"text": "timing", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9619887471199036}]}, {"text": "These numbers clearly demonstrate that annotation costs are more adequately modelled by the additional features we identified through our eye-tracking study.", "labels": [], "entities": []}, {"text": "Our 'cognitive' model now consists of 21 coefficients.", "labels": [], "entities": []}, {"text": "We tested for the significance of this model's regression terms.", "labels": [], "entities": []}, {"text": "For annotator A we found all coefficients to be significant with respect to the model (p < 0.05), for annotator Ball coefficients except one were significant.", "labels": [], "entities": []}, {"text": "shows the coefficients of annotator A's 'cognitive' model along with the standard errors and t-values.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average performance values for the 10 subjects of each experimental condition and 20 anno- tation examples of each complexity class: number of entity-critical words, mean annotation time and  standard deviations (SD), mean annotation errors, standard deviations, and error rates (number of errors  divided by number of entity-critical words).", "labels": [], "entities": [{"text": "mean annotation time and  standard deviations (SD)", "start_pos": 176, "end_pos": 226, "type": "METRIC", "confidence": 0.8742739756902059}, {"text": "error rates", "start_pos": 277, "end_pos": 288, "type": "METRIC", "confidence": 0.9406141042709351}]}, {"text": " Table 3: Average number of fixations on the anno- tation phrase and context for the document condi- tion and 20 annotation examples of each complex- ity class.", "labels": [], "entities": []}, {"text": " Table 5: Adjusted R 2 values on both models and  for annotators A and B.", "labels": [], "entities": [{"text": "Adjusted R 2", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.7456965446472168}]}, {"text": " Table 6: 'Cognitive' model of annotator A.", "labels": [], "entities": []}]}