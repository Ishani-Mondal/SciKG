{"title": [{"text": "PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9343307018280029}]}], "abstractContent": [{"text": "This paper establishes a connection between two apparently very different kinds of probabilistic models.", "labels": [], "entities": []}, {"text": "Latent Dirich-let Allocation (LDA) models are used as \"topic models\" to produce a low-dimensional representation of documents, while Probabilistic Context-Free Grammars (PCFGs) define distributions over trees.", "labels": [], "entities": []}, {"text": "The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well.", "labels": [], "entities": []}, {"text": "Adaptor Grammars (AGs) area hierarchical, non-parameteric Bayesian extension of PCFGs.", "labels": [], "entities": []}, {"text": "Exploiting the close relationship between LDA and PCFGs just described, we propose two novel probabilistic models that combine insights from LDA and AG models.", "labels": [], "entities": []}, {"text": "The first replaces the unigram component of LDA topic models with multi-word sequences or collocations generated by an AG.", "labels": [], "entities": []}, {"text": "The second extension builds on the first one to learn aspects of the internal structure of proper names.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the last few years there has been considerable interest in Bayesian inference for complex hierarchical models both in machine learning and in computational linguistics.", "labels": [], "entities": []}, {"text": "This paper establishes a theoretical connection between two very different kinds of probabilistic models: Probabilistic Context-Free Grammars (PCFGs) and a class of models known as Latent Dirichlet Allocation () models that have been used fora variety of tasks in machine learning.", "labels": [], "entities": []}, {"text": "Specifically, we show that an LDA model can be expressed as a certain kind of PCFG, so Bayesian inference for PCFGs can be used to learn LDA topic models as well.", "labels": [], "entities": []}, {"text": "The importance of this observation is primarily theoretical, as current Bayesian inference algorithms for PCFGs are less efficient than those for LDA inference.", "labels": [], "entities": []}, {"text": "However, once this link is established it suggests a variety of extensions to the LDA topic models, two of which we explore in this paper.", "labels": [], "entities": []}, {"text": "The first involves extending the LDA topic model so that it generates collocations (sequences of words) rather than individual words.", "labels": [], "entities": []}, {"text": "The second applies this idea to the problem of automatically learning internal structure of proper names (NPs), which is useful for definite NP coreference models and other applications.", "labels": [], "entities": [{"text": "learning internal structure of proper names (NPs)", "start_pos": 61, "end_pos": 110, "type": "TASK", "confidence": 0.7235960165659586}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "The next section reviews Latent Dirichlet Allocation (LDA) topic models, and the following section reviews Probabilistic Context-Free Grammars (PCFGs).", "labels": [], "entities": []}, {"text": "Section 4 shows how an LDA topic model can be expressed as a PCFG, which provides the fundamental connection between LDA and PCFGs that we exploit in the rest of the paper, and shows how it can be used to define a \"sticky topic\" version of LDA.", "labels": [], "entities": []}, {"text": "The following section reviews Adaptor Grammars (AGs), a non-parametric extension of PCFGs introduced by.", "labels": [], "entities": [{"text": "Adaptor Grammars (AGs)", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.612152898311615}]}, {"text": "Section 6 exploits the connection between LDA and PCFGs to propose an AG-based topic model that extends LDA by defining distributions over collocations rather than individual words, and section 7 applies this extension to the problem of finding the structure of proper names.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}