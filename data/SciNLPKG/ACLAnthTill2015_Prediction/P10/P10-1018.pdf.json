{"title": [{"text": "\"Was it good? It was provocative.\" Learning the meaning of scalar adjectives", "labels": [], "entities": []}], "abstractContent": [{"text": "Texts and dialogues often express information indirectly.", "labels": [], "entities": []}, {"text": "For instance, speak-ers' answers to yes/no questions do not always straightforwardly convey a 'yes' or 'no' answer.", "labels": [], "entities": []}, {"text": "The intended reply is clear in some cases (Was it good? It was great!) but uncertain in others (Was it acceptable? It was unprecedented.).", "labels": [], "entities": []}, {"text": "In this paper, we present methods for interpreting the answers to questions like these which involve scalar modifiers.", "labels": [], "entities": [{"text": "interpreting the answers to questions", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.840613579750061}]}, {"text": "We show how to ground scalar modifier meaning based on data collected from the Web.", "labels": [], "entities": [{"text": "ground scalar modifier meaning", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.6407458409667015}]}, {"text": "We learn scales between modifiers and infer the extent to which a given answer conveys 'yes' or 'no'.", "labels": [], "entities": []}, {"text": "To evaluate the methods, we collected examples of question-answer pairs involving scalar modifiers from CNN transcripts and the Dialog Act corpus and use response distributions from Mechanical Turk workers to assess the degree to which each answer conveys 'yes' or 'no'.", "labels": [], "entities": [{"text": "CNN transcripts", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.9433799386024475}, {"text": "Dialog Act corpus", "start_pos": 128, "end_pos": 145, "type": "DATASET", "confidence": 0.9035251140594482}]}, {"text": "Our experimental results closely match the Turkers' response data, demonstrating that meanings can be learned from Web data and that such meanings can drive pragmatic inference.", "labels": [], "entities": []}], "introductionContent": [{"text": "An important challenge for natural language processing is how to learn not only basic linguistic meanings but also how those meanings are systematically enriched when expressed in context.", "labels": [], "entities": []}, {"text": "For instance, answers to polar (yes/no) questions do not always explicitly contain a 'yes' or 'no', but rather give information that the hearer can use to infer such an answer in a context with some degree of certainty.", "labels": [], "entities": []}, {"text": "find that 27% of answers to polar questions do not contain a direct 'yes' or 'no' word, 44% of which they regard as failing to convey a clear 'yes' or 'no' response.", "labels": [], "entities": []}, {"text": "In some cases, interpreting the answer is straightforward (Was it bad?", "labels": [], "entities": [{"text": "interpreting the answer", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.8907302618026733}]}, {"text": "It was terrible.), but in others, what to infer from the answer is unclear (Was it good? It was provocative.).", "labels": [], "entities": []}, {"text": "It is even common for the speaker to explicitly convey his own uncertainty with such answers.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the interpretation of answers to a particular class of polar questions: ones in which the main predication involves a gradable modifier (e.g., highly unusual, not good, little) and the answer either involves another gradable modifier or a numerical expression (e.g., seven years old, twenty acres of land).", "labels": [], "entities": []}, {"text": "Interpreting such question-answer pairs requires dealing with modifier meanings, specifically, learning context-dependent scales of expressions) that determine how, and to what extent, the answer as a whole resolves the issue raised by the question.", "labels": [], "entities": []}, {"text": "We propose two methods for learning the knowledge necessary for interpreting indirect answers to questions involving gradable adjectives, depending on the type of predications in the question and the answer.", "labels": [], "entities": [{"text": "interpreting indirect answers to questions involving gradable adjectives", "start_pos": 64, "end_pos": 136, "type": "TASK", "confidence": 0.8862311094999313}]}, {"text": "The first technique deals with pairs of modifiers: we hypothesized that online, informal review corpora in which people's comments have associated ratings would provide a general-purpose database for mining scales between modifiers.", "labels": [], "entities": []}, {"text": "We thus use a large collection of online reviews to learn orderings between adjectives based on contextual entailment (good < excellent), and employ this scalar relationship to infer a yes/no answer (subject to negation and other qualifiers).", "labels": [], "entities": []}, {"text": "The second strategy targets numerical answers.", "labels": [], "entities": []}, {"text": "Since it is unclear what kind of corpora would contain the relevant information, we turn to the Web in general: we use distributional information retrieved via Web searches to assess whether the numerical measure counts as a posi-tive or negative instance of the adjective in question.", "labels": [], "entities": []}, {"text": "Both techniques exploit the same approach: using texts (the Web) to learn meanings that can drive pragmatic inference in dialogue.", "labels": [], "entities": []}, {"text": "This paper demonstrates to some extent that meaning can be grounded from text in this way.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our primary goal is to evaluate how well we can learn the relevant scalar and entailment relationships from the Web.", "labels": [], "entities": []}, {"text": "In the evaluation, we thus applied our techniques to a manually coded corpus version.", "labels": [], "entities": []}, {"text": "For the adjectival scales, we annotated each example for its main predication (modifier, or adverb-modifier bigram), including whether that predication was negated.", "labels": [], "entities": []}, {"text": "For the numerical cases, we manually constructed the initial queries: we identified the adjective and the modified entity in the question, and the unit of measure in the answer.", "labels": [], "entities": []}, {"text": "However, we believe that identifying the requisite predications and recognizing the presence of negation or embedding could be done automatically using dependency graphs.", "labels": [], "entities": []}, {"text": "Modification in answer: Precision, recall, and F1 (%) per response category.", "labels": [], "entities": [{"text": "Precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.999401330947876}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9985883831977844}, {"text": "F1", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9998182654380798}]}, {"text": "In the case of the scalar modifiers experiment, there were just two examples whose dominant response from the Turkers was 'Uncertain', so we have left that category out of the results.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.8597525358200073}]}, {"text": "In the case of the numerical experiment, there were not any 'No' answers.", "labels": [], "entities": []}, {"text": "To evaluate the techniques, we pool the Mechanical Turk 'definite yes' and 'probable yes' categories into a single category 'Yes', and we do the same for 'definite no' and 'probable no'.", "labels": [], "entities": []}, {"text": "Together with 'uncertain', this makes for threeresponse categories.", "labels": [], "entities": []}, {"text": "We count an inference as successful if it matches the dominant Turker response category.", "labels": [], "entities": []}, {"text": "To use the three-response scheme in the numerical experiment, we simply categorize the probabilities as follows: 0-0.33 = 'No', 0.33-0.66 = 'Uncertain', 0.66-1.00 = 'Yes'.", "labels": [], "entities": []}, {"text": "gives a breakdown of our system's performance on the various category subtypes.", "labels": [], "entities": []}, {"text": "The overall accuracy level is 71% (159 out of 224 inferences correct).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.999651312828064}]}, {"text": "summarizes the results per response category, for the examples in which both the question and answer contain a gradable modifier (category I), and for the numerical cases (category II).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Types of question-answer pairs, and  counts in the corpus.", "labels": [], "entities": []}, {"text": " Table 2: Mean entropy values and standard devi- ation obtained in the Mechanical Turk experiment  for each question-answer pair category.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9724699258804321}]}, {"text": " Table 3: Numbers of reviews, words and vocabulary size per rating category in the IMDB review corpus,  as well as the average number of words per review.", "labels": [], "entities": [{"text": "IMDB review corpus", "start_pos": 83, "end_pos": 101, "type": "DATASET", "confidence": 0.8861847321192423}]}, {"text": " Table 4: Summary of precision and recall (%) by  type.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9663804173469543}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9804478883743286}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.999363124370575}]}, {"text": " Table 5: Precision, recall, and F1 (%) per response  category. In the case of the scalar modifiers exper- iment, there were just two examples whose dom- inant response from the Turkers was 'Uncertain',  so we have left that category out of the results. In  the case of the numerical experiment, there were  not any 'No' answers.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9991434812545776}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9992037415504456}, {"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9997723698616028}]}, {"text": " Table 6: Precision, recall, and F1 (%) per response  category for the WordNet-based approach.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9993077516555786}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9989274144172668}, {"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9998006224632263}]}]}