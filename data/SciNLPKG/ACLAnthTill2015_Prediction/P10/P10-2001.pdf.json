{"title": [{"text": "Paraphrase Lattice for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.8181770443916321}]}], "abstractContent": [{"text": "Lattice decoding in statistical machine translation (SMT) is useful in speech translation and in the translation of Ger-man because it can handle input ambiguities such as speech recognition ambiguities and German word segmentation ambiguities.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.7762817541758219}, {"text": "speech translation", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8016807734966278}, {"text": "translation", "start_pos": 101, "end_pos": 112, "type": "TASK", "confidence": 0.9653846025466919}, {"text": "German word segmentation ambiguities", "start_pos": 207, "end_pos": 243, "type": "TASK", "confidence": 0.6593356430530548}]}, {"text": "We show that lattice decoding is also useful for handling input variations.", "labels": [], "entities": []}, {"text": "Given an input sentence, we build a lattice which represents paraphrases of the input sentence.", "labels": [], "entities": []}, {"text": "We call this a paraphrase lattice.", "labels": [], "entities": []}, {"text": "Then, we give the paraphrase lattice as an input to the lattice decoder.", "labels": [], "entities": []}, {"text": "The decoder selects the best path for decoding.", "labels": [], "entities": []}, {"text": "Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9992653727531433}, {"text": "IWSLT", "start_pos": 92, "end_pos": 97, "type": "DATASET", "confidence": 0.8746588230133057}, {"text": "Europarl datasets", "start_pos": 102, "end_pos": 119, "type": "DATASET", "confidence": 0.9758348166942596}]}], "introductionContent": [{"text": "Lattice decoding in SMT is useful in speech translation and in the translation of German (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9849083423614502}, {"text": "speech translation", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7537358105182648}, {"text": "translation of German", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.872656246026357}]}, {"text": "In speech translation, by using lattices that represent not only 1-best result but also other possibilities of speech recognition, we can take into account the ambiguities of speech recognition.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8031320571899414}, {"text": "speech recognition", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7454227209091187}, {"text": "speech recognition", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.758643239736557}]}, {"text": "Thus, the translation quality for lattice inputs is better than the quality for 1-best inputs.", "labels": [], "entities": []}, {"text": "In this paper, we show that lattice decoding is also useful for handling input variations.", "labels": [], "entities": []}, {"text": "\"Input variations\" refers to the differences of input texts with the same meaning.", "labels": [], "entities": []}, {"text": "For example, \"Is there a beauty salon?\" and \"Is there a beauty parlor?\" have the same meaning with variations in \"beauty salon\" and \"beauty parlor\".", "labels": [], "entities": []}, {"text": "Since these variations are frequently found in natural language texts, a mismatch of the expressions in source sentences and the expressions in training corpus leads to a decrease in translation quality.", "labels": [], "entities": []}, {"text": "Therefore, we propose a novel method that can handle input variations using paraphrases and lattice decoding.", "labels": [], "entities": []}, {"text": "In the proposed method, we regard a given source sentence as one of many variations.", "labels": [], "entities": []}, {"text": "Given an input sentence, we build a paraphrase lattice which represents paraphrases of the input sentence.", "labels": [], "entities": []}, {"text": "Then, we give the paraphrase lattice as an input to the Moses decoder (.", "labels": [], "entities": []}, {"text": "Moses selects the best path for decoding.", "labels": [], "entities": []}, {"text": "By using paraphrases of source sentences, we can translate expressions which are not found in a training corpus on the condition that paraphrases of them are found in the training corpus.", "labels": [], "entities": []}, {"text": "Moreover, by using lattice decoding, we can employ the source-side language model as a decoding feature.", "labels": [], "entities": []}, {"text": "Since this feature is affected by the source-side context, the decoder can choose a proper paraphrase and translate correctly.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: Related works on lattice decoding and paraphrasing are presented in Section 2.", "labels": [], "entities": []}, {"text": "The proposed method is described in Section 3.", "labels": [], "entities": []}, {"text": "Experimental results for IWSLT and Europarl dataset are presented in Section 4.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.8880788683891296}, {"text": "Europarl dataset", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.9903543889522552}]}, {"text": "Finally, the paper is concluded with a summary and a few directions for future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the proposed method, we conducted English-to-Japanese and English-toChinese translation experiments using IWSLT 2007 dataset.", "labels": [], "entities": [{"text": "English-toChinese translation", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.6143767982721329}, {"text": "IWSLT 2007 dataset", "start_pos": 127, "end_pos": 145, "type": "DATASET", "confidence": 0.9699825247128805}]}, {"text": "This dataset contains EJ and EC parallel corpus for the travel domain and consists of 40k sentences for training and about 500 sentences sets (dev1, dev2 and dev3) for development and testing.", "labels": [], "entities": []}, {"text": "We used the dev1 set for parameter tuning, the dev2 set for choosing the setting of the proposed method, which is described below, and the dev3 set for testing.", "labels": [], "entities": []}, {"text": "The English-English paraphrase list was acquired from the EC corpus for EJ translation and 53K pairs were acquired.", "labels": [], "entities": [{"text": "EC corpus", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.9643956422805786}, {"text": "EJ translation", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.7806749939918518}]}, {"text": "Similarly, 47K pairs were acquired from the EJ corpus for EC translation.", "labels": [], "entities": [{"text": "EJ corpus", "start_pos": 44, "end_pos": 53, "type": "DATASET", "confidence": 0.9082488715648651}, {"text": "EC translation", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.8944045603275299}]}], "tableCaptions": [{"text": " Table 1: Experimental results for IWSLT (%BLEU).", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.8638138175010681}, {"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9964320659637451}]}]}