{"title": [{"text": "Improved Unsupervised POS Induction through Prototype Discovery", "labels": [], "entities": [{"text": "Improved Unsupervised POS Induction", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6238220185041428}]}], "abstractContent": [{"text": "We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9635885953903198}]}, {"text": "The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories.", "labels": [], "entities": []}, {"text": "The rest of the words are subsequently mapped to these clusters.", "labels": [], "entities": []}, {"text": "We utilize morphological and distributional representations computed in a fully unsupervised manner.", "labels": [], "entities": []}, {"text": "We evaluate our algorithm on English and German, achieving the best reported results for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Part-of-speech (POS) tagging is a fundamental NLP task, used by a wide variety of applications.", "labels": [], "entities": [{"text": "Part-of-speech (POS) tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5831540346145629}]}, {"text": "However, there is no single standard POS tagging scheme, even for English.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.7281800806522369}]}, {"text": "Schemes vary significantly across corpora and even more so across languages, creating difficulties in using POS tags across domains and for multi-lingual systems (.", "labels": [], "entities": []}, {"text": "Automatic induction of POS tags from plain text can greatly alleviate this problem, as well as eliminate the efforts incurred by manual annotations.", "labels": [], "entities": []}, {"text": "It is also a problem of great theoretical interest.", "labels": [], "entities": []}, {"text": "Consequently, POS induction is a vibrant research area (see.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.9829796254634857}]}, {"text": "In this paper we present an algorithm based on the theory of prototypes, which posits that some members in cognitive categories are more central than others.", "labels": [], "entities": []}, {"text": "These practically define the category, while the membership of other elements is based on their association with the central members.", "labels": [], "entities": []}, {"text": "Our algorithm first clusters words based on a fine morphological representation.", "labels": [], "entities": []}, {"text": "It then clusters the most frequent words, defining landmark clusters which constitute the cores of the categories.", "labels": [], "entities": []}, {"text": "Finally, it maps the rest of the words to these categories.", "labels": [], "entities": []}, {"text": "The last two stages utilize a distributional representation that has been shown to be effective for unsupervised parsing.", "labels": [], "entities": []}, {"text": "We evaluated the algorithm in both English and German, using four different mapping-based and information theoretic clustering evaluation measures.", "labels": [], "entities": []}, {"text": "The results obtained are generally better than all existing POS induction algorithms.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.7718412578105927}]}, {"text": "Section 2 reviews related work.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 detail the algorithm.", "labels": [], "entities": []}, {"text": "Sections 5, 6 and 7 describe the evaluation, experimental setup and results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the clustering produced by our algorithm using an external quality measure: we take a corpus tagged by gold standard tags, tag it using the induced tags, and compare the two taggings.", "labels": [], "entities": []}, {"text": "There is no single accepted measure quantifying the similarity between two taggings.", "labels": [], "entities": []}, {"text": "In order to be as thorough as possible, we report results using four known measures, two mapping-based measures and two information theoretic ones.", "labels": [], "entities": []}, {"text": "The induced clusters have arbitrary names.", "labels": [], "entities": []}, {"text": "We define two mapping schemes between them and the gold clusters.", "labels": [], "entities": []}, {"text": "After the induced clusters are mapped, we can compute a derived accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9914229512214661}]}, {"text": "The Many-to-1 measure finds the mapping between the gold standard clusters and the induced clusters which maximizes accuracy, allowing several induced clusters to be mapped to the same gold standard cluster.", "labels": [], "entities": [{"text": "gold standard clusters", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.8736080129941305}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9994311928749084}]}, {"text": "The 1-to-1 measure finds the mapping between the induced and gold standard clusters which maximizes accuracy such that no two induced clusters are mapped to the same gold cluster.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9993532299995422}]}, {"text": "Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters.", "labels": [], "entities": []}, {"text": "As in, we use the Kuhn-Munkres algorithm to solve this problem.", "labels": [], "entities": []}, {"text": "These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa.", "labels": [], "entities": []}, {"text": "Several such measures exist; we use V ( and NVI), VI's (Meila, 2007) normalized version.", "labels": [], "entities": [{"text": "V", "start_pos": 36, "end_pos": 37, "type": "METRIC", "confidence": 0.9724045395851135}]}, {"text": "Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.7858063876628876}]}, {"text": "To address this problem we compare to three different schemes in two languages.", "labels": [], "entities": []}, {"text": "In addition, the two English schemes we compare with were designed to tag corpora contained in our training set, and have been widely and successfully used with these corpora by a large number of applications.", "labels": [], "entities": []}, {"text": "Our algorithm was run with the exact same parameters on both languages: N = 100 (high frequency threshold), n = 50 (the parameter that determines the effective number of coordinates), \u03b1 = 0.25 (cluster separation during landmark cluster generation), \u03b2 = 0.9 (cluster separation during refinement of morphological clusters).", "labels": [], "entities": []}, {"text": "The algorithm we compare within most detail is, which reports the best current results for this problem (see Section 7).", "labels": [], "entities": []}, {"text": "Since Clark's algorithm is sensitive to its initialization, we ran it a 100 times and report its average and standard deviation in each of the four measures.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 109, "end_pos": 127, "type": "METRIC", "confidence": 0.9480500817298889}]}, {"text": "In addition, we report the percentile in which our result falls with respect to these 100 runs.", "labels": [], "entities": []}, {"text": "Punctuation marks are very frequent in corpora and are easy to cluster.", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9490373730659485}]}, {"text": "As a result, including them in the evaluation greatly inflates the scores.", "labels": [], "entities": []}, {"text": "For this reason we do not assign a cluster to punctuation marks and we report results using this policy, which we recommend for future work.", "labels": [], "entities": []}, {"text": "However, to be able to directly compare with previous work, we also report results for the full POS tag set.", "labels": [], "entities": [{"text": "POS tag set", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.8498823046684265}]}, {"text": "We do so by assigning a singleton cluster to each punctuation mark (in addition to the k required clusters).", "labels": [], "entities": []}, {"text": "This simple heuristic yields very high performance on punctuation, scoring (when all other words are assumed perfect tagging) 99.6% (99.1%) 1-to-1 accuracy when evaluated against the English fine (coarse) POS tag sets, and 97.2% when evaluated against the German POS tag set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9943709969520569}, {"text": "German POS tag set", "start_pos": 256, "end_pos": 274, "type": "DATASET", "confidence": 0.9161149859428406}]}, {"text": "For English, we trained our model on the 39832 sentences which constitute sections 2-21 of the PTB-WSJ and on the 500K sentences from the NYT section of the NANC newswire corpus  Clark's standard deviation (\u03c3) and the fraction of Clark's results that scored worse than our model (%).", "labels": [], "entities": [{"text": "PTB-WSJ", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9856293797492981}, {"text": "NYT section of the NANC newswire corpus  Clark's standard deviation (\u03c3)", "start_pos": 138, "end_pos": 209, "type": "DATASET", "confidence": 0.8574322462081909}]}, {"text": "For the mapping based measures, results are accuracy percentage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9997379183769226}]}, {"text": "For V \u2208 [0, 1], higher is better.", "labels": [], "entities": []}, {"text": "For high quality output, NV I \u2208 [0, 1] as well, and lower is better.", "labels": [], "entities": [{"text": "NV I", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.8511503040790558}]}, {"text": "In each entry, the top number indicates the score when including punctuation and the bottom number the score when excluding it.", "labels": [], "entities": []}, {"text": "In English, our results are always better than Clark's.", "labels": [], "entities": []}, {"text": "In German, they are almost always better. are not punctuation.", "labels": [], "entities": []}, {"text": "The percentage of unknown words (those appearing less than five times) is 1.6%.", "labels": [], "entities": []}, {"text": "There are 45 clusters in this annotation scheme, 34 of which are not punctuation.", "labels": [], "entities": []}, {"text": "We ran each algorithm both with k=13 and k=34 (the number of desired clusters).", "labels": [], "entities": []}, {"text": "We compare the output to two annotation schemes: the fine grained PTB WSJ scheme, and the coarse grained tags defined in).", "labels": [], "entities": [{"text": "PTB WSJ scheme", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.4934244950612386}]}, {"text": "The output of the k=13 run is evaluated both against the coarse POS tag annotation (the 'Coarse k=13' scenario) and against the full PTB-WSJ annotation scheme (the 'Fine k=13' scenario).", "labels": [], "entities": [{"text": "PTB-WSJ", "start_pos": 133, "end_pos": 140, "type": "DATASET", "confidence": 0.8948402404785156}]}, {"text": "The k=34 run is evaluated against the full PTB-WSJ annotation scheme (the 'Fine k=34' scenario).", "labels": [], "entities": [{"text": "PTB-WSJ", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.8909187912940979}]}, {"text": "The POS cluster frequency distribution tends to be skewed: each of the 13 most frequent clusters in the PTB-WSJ cover more than 2.5% of the tokens (excluding punctuation) and together 86.3% of them.", "labels": [], "entities": [{"text": "PTB-WSJ", "start_pos": 104, "end_pos": 111, "type": "DATASET", "confidence": 0.8914124965667725}]}, {"text": "We therefore chose k=13, since it is both the number of coarse POS tags (excluding punctuation) as well as the number of frequent POS tags in the PTB-WSJ annotation scheme.", "labels": [], "entities": [{"text": "PTB-WSJ annotation scheme", "start_pos": 146, "end_pos": 171, "type": "DATASET", "confidence": 0.8880741397539774}]}, {"text": "We chose k=34 in order to evaluate against the full 34 tags PTB-WSJ annotation scheme (excluding punctuation) using the same number of clusters.", "labels": [], "entities": [{"text": "PTB-WSJ", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.8745097517967224}]}, {"text": "For German, we trained our model on the 20296 sentences of the NEGRA corpus and on the first 450K sentences of the DeWAC corpus (.", "labels": [], "entities": [{"text": "NEGRA corpus", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.9590033292770386}, {"text": "DeWAC corpus", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.9783235192298889}]}, {"text": "DeWAC is a corpus extracted by web crawling and is therefore out of domain.", "labels": [], "entities": [{"text": "DeWAC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8829351663589478}]}, {"text": "We report results on the NEGRA part, which includes 346320 word tokens of 49402 types.", "labels": [], "entities": [{"text": "NEGRA part", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.901332437992096}]}, {"text": "Of the tokens, 289268 (83.5%) are not punctuation.", "labels": [], "entities": [{"text": "289268", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.7713438868522644}]}, {"text": "The percentage of unknown words (those appearing less than five times) is 8.1%.", "labels": [], "entities": []}, {"text": "There are 62 clusters in this annotation scheme, 51 of which are not punctuation.", "labels": [], "entities": []}, {"text": "We ran the algorithms with k=17 and k=26.", "labels": [], "entities": []}, {"text": "k=26 was chosen since it is the number of clusters that cover each more than 0.5% of the NE-GRA tokens, and in total cover 96% of the (nonpunctuation) tokens.", "labels": [], "entities": []}, {"text": "In order to test our algorithm in another scenario, we conducted experiments with k=17 as well, which covers 89.9% of the tokens.", "labels": [], "entities": []}, {"text": "All outputs are compared against NE-GRA's gold standard scheme.", "labels": [], "entities": [{"text": "NE-GRA's gold standard", "start_pos": 33, "end_pos": 55, "type": "DATASET", "confidence": 0.862415537238121}]}, {"text": "We do not report results for k=51 (where the number of gold clusters is the same as the number of induced clusters), since our algorithm produced only 42 clusters in the landmark detection stage.", "labels": [], "entities": []}, {"text": "We could of course have modified the parameters to allow our algorithm to produce 51 clusters.", "labels": [], "entities": []}, {"text": "However, we wanted to use the exact same parameters as those used for the English experiments to minimize the issue of parameter tuning.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 119, "end_pos": 135, "type": "TASK", "confidence": 0.7557770013809204}]}, {"text": "In addition to the comparisons described above, we present results of experiments (in the 'Fine    k=13' scenario) that quantify the contribution of each component of the algorithm.", "labels": [], "entities": []}, {"text": "We ran the base distributional algorithm, a variant which uses only capitalization information (i.e., has only one nonsingleton morphological class, that of words appearing capitalized inmost of their instances) and a variant which uses no capitalization information, defining the morphological clusters according to the morphological representation alone.", "labels": [], "entities": []}, {"text": "presents results for the English and German experiments.", "labels": [], "entities": []}, {"text": "For English, our algorithm obtains better results than Clark's in all measures and scenarios.", "labels": [], "entities": []}, {"text": "It is without exception better than the average score of Clark's and inmost cases better than the maximal Clark score obtained in 100 runs.", "labels": [], "entities": [{"text": "average score", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.9644910395145416}, {"text": "Clark's", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.7375679016113281}, {"text": "maximal Clark score", "start_pos": 98, "end_pos": 117, "type": "METRIC", "confidence": 0.8139679034550985}]}, {"text": "A significant difference between our algorithm and Clark's is that the latter, like most algorithms which addressed the task, induces the clustering by maximizing a non-convex function.", "labels": [], "entities": []}, {"text": "These functions have many local maxima and the specific solution to which algorithms that maximize them converge strongly depends on their (random) initialization.", "labels": [], "entities": []}, {"text": "Therefore, their output's quality often significantly diverges from the average.", "labels": [], "entities": []}, {"text": "This issue is discussed in depth in.", "labels": [], "entities": []}, {"text": "Our algorithm is deterministic 3 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (\u00b5),", "labels": [], "entities": [{"text": "Clark's average score (\u00b5)", "start_pos": 95, "end_pos": 120, "type": "METRIC", "confidence": 0.9212041582380023}]}, {"text": " Table 2: A comparison of partial versions of the model in", "labels": [], "entities": []}, {"text": " Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-", "labels": [], "entities": []}]}