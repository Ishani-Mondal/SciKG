{"title": [{"text": "Plot Induction and Evolutionary Search for Story Generation", "labels": [], "entities": [{"text": "Plot Induction", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8190486133098602}, {"text": "Story Generation", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7798643708229065}]}], "abstractContent": [{"text": "In this paper we develop a story generator that leverages knowledge inherent in corpora without requiring extensive manual involvement.", "labels": [], "entities": []}, {"text": "A key feature in our approach is the reliance on a story planner which we acquire automatically by recording events, their participants, and their precedence relationships in a training corpus.", "labels": [], "entities": []}, {"text": "Contrary to previous work our system does not follow a generate-and-rank architecture.", "labels": [], "entities": []}, {"text": "Instead, we employ evolutionary search techniques to explore the space of possible stories which we argue are well suited to the story generation task.", "labels": [], "entities": [{"text": "story generation", "start_pos": 129, "end_pos": 145, "type": "TASK", "confidence": 0.7225970923900604}]}, {"text": "Experiments on generating simple children's stories show that our system outperforms previous data-driven approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computer story generation has met with fascination since the early days of artificial intelligence.", "labels": [], "entities": [{"text": "Computer story generation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8096929589907328}]}, {"text": "Indeed, over the years, several generators have been developed capable of creating stories that resemble human output.", "labels": [], "entities": []}, {"text": "To name only a few, TALE-SPIN generates stories through problem solving, MINSTREL relies on an episodic memory scheme, essentially a repository of previous hand-coded stories, to solve the problems in the current story, and MAKEBELIEVE () uses commonsense knowledge to generate short stories from an initial seed story (supplied by the user).", "labels": [], "entities": [{"text": "MINSTREL", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.595285952091217}]}, {"text": "A large body of more recent work views story generation as a form of agent-based planning).", "labels": [], "entities": [{"text": "story generation", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.8021231889724731}]}, {"text": "The agents act as characters with a list of goals.", "labels": [], "entities": []}, {"text": "They form plans of action and try to fulfill them.", "labels": [], "entities": []}, {"text": "Interesting stories emerge as plans interact and cause failures and possible replanning.", "labels": [], "entities": []}, {"text": "The broader appeal of computational story generation lies in its application potential.", "labels": [], "entities": [{"text": "computational story generation", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.6886284947395325}]}, {"text": "Examples include the entertainment industry and the development of tools that produce large numbers of plots automatically that might provide inspiration to professional screen writers (); rendering video games more interesting by allowing the plot to adapt dynamically to the players' actions (; and assisting teachers to create or personalize stories for their students (.", "labels": [], "entities": []}, {"text": "A major stumbling block for the widespread use of computational story generators is their reliance on expensive, manually created resources.", "labels": [], "entities": []}, {"text": "A typical story generator will make use of a knowledge base for providing detailed domain-specific information about the characters and objects involved in the story and their relations.", "labels": [], "entities": []}, {"text": "It will also have a story planner that specifies how these characters interact, what their goals are and how their actions result in different story plots.", "labels": [], "entities": []}, {"text": "Finally, a sentence planner (coupled with a surface realizer) will render an abstract story specification into natural language text.", "labels": [], "entities": []}, {"text": "Traditionally, most of this knowledge is created by hand, and the effort must be repeated for new domains, new characters and plot elements.", "labels": [], "entities": []}, {"text": "Fortunately, recent work in natural language processing has taken significant steps towards developing algorithms that learn some of this knowledge automatically from natural language corpora.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.6621365249156952}]}, {"text": "propose an unsupervised method for learning narrative schemas, chains of events whose arguments are filled with participant semantic roles defined over words.", "labels": [], "entities": []}, {"text": "An example schema is {X arrest, X charge, X raid, X seize, X confiscate, X detain, X deport}, where X stands for the argument types {police, agent, authority, government}.", "labels": [], "entities": []}, {"text": "Their approach relies on the intuition that in a coherent text events that are about the same participants are likely to be part of the same story or narrative.", "labels": [], "entities": []}, {"text": "Their model extracts narrative chains, essentially events that share argument slots and merges them into schemas.", "labels": [], "entities": []}, {"text": "The latter could be used to constructor enrich the knowledge base of a story generator.", "labels": [], "entities": []}, {"text": "In we presented a story generator that leverages knowledge inherent in corpora without requiring extensive manual involvement.", "labels": [], "entities": []}, {"text": "The generator operates over predicateargument and predicate-predicate co-occurrence tuples gathered from training data.", "labels": [], "entities": []}, {"text": "These are used to produce a large set of candidate stories which are subsequently ranked based on their interestingness and coherence.", "labels": [], "entities": []}, {"text": "The approach is unusual in that it does not involve an explicit story planning component.", "labels": [], "entities": []}, {"text": "Stories are created stochastically by selecting entities and the events they are most frequently attested with.", "labels": [], "entities": []}, {"text": "In this work we develop a story generator that is also data-driven but crucially relies on a story planner for creating meaningful stories.", "labels": [], "entities": []}, {"text": "Inspired by we acquire story plots automatically by recording events, their participants, and their precedence relationships as attested in a training corpus.", "labels": [], "entities": []}, {"text": "Entities give rise to different potential plots which in turn generate multiple stories.", "labels": [], "entities": []}, {"text": "Contrary to our previous work), we do not follow a generate-and-rank architecture.", "labels": [], "entities": []}, {"text": "Instead, we search the space of possible stories using Genetic Algorithms (GAs) which we argue are advantageous in the story generation setting, as they can search large fitness landscapes while greatly reducing the risk of getting stuck in local optima.", "labels": [], "entities": []}, {"text": "By virtue of exploring the search space more broadly, we are able to generate creative stories without an explicit interest scoring module.", "labels": [], "entities": []}, {"text": "In the remainder of this paper we give a brief overview of the system described in and discuss previous applications of GAs in natural language generation (Section 2).", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 127, "end_pos": 154, "type": "TASK", "confidence": 0.664970318476359}]}, {"text": "Next, we detail our approach, specifically how plots are created and used in conjunction with genetic search (Sections 3 and 4).", "labels": [], "entities": []}, {"text": "Finally, we present our experimental results (Sections 6 and 7) and conclude the paper with discussion of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present our experimental set-up for assessing the performance of our story generator.", "labels": [], "entities": []}, {"text": "We give details on our training corpus, system, parameters (such as the population size for the GA search), the baselines used for comparison, and explain how our system output was evaluated.", "labels": [], "entities": [{"text": "GA search", "start_pos": 96, "end_pos": 105, "type": "TASK", "confidence": 0.8563535809516907}]}, {"text": "Corpus The generator was trained on the same corpus used in, 437 stories from the Andrew Lang fairy tales collection.", "labels": [], "entities": [{"text": "Andrew Lang fairy tales collection", "start_pos": 82, "end_pos": 116, "type": "DATASET", "confidence": 0.6369605779647827}]}, {"text": "The average story length is 125.18 sentences.", "labels": [], "entities": []}, {"text": "The corpus contains 15,789 word tokens.", "labels": [], "entities": []}, {"text": "Following McIntyre and Lapata, we discarded tokens that did not appear in the Children's Printed Word Database 6 , a database of printed word frequencies as read by children aged between five and nine.", "labels": [], "entities": [{"text": "Children's Printed Word Database 6", "start_pos": 78, "end_pos": 112, "type": "DATASET", "confidence": 0.7204988400141398}]}, {"text": "From this corpus we extracted narrative schemas for 667 entities in total.", "labels": [], "entities": []}, {"text": "We disregarded any graph that contained less than 10 nodes as too small.", "labels": [], "entities": []}, {"text": "The graphs had on average 61.04 nodes, with an average clustering rate 7 of 0.027 which indicates that they are substantially connected.", "labels": [], "entities": []}, {"text": "Parameter Setting Considerable latitude is available when selecting parameters for the GA.", "labels": [], "entities": [{"text": "Parameter Setting", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8245806097984314}, {"text": "latitude", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.743590772151947}]}, {"text": "These involve the population size, crossover, and mutation rates.", "labels": [], "entities": []}, {"text": "To evaluate which setting was best, we asked two human evaluators to rate (on a 1-5 scale) stories produced with a population size ranging from 1,000 to 10,000, crossover rate of 0.1 to 0.6 and mutation rate of 0.001 to 0.1.", "labels": [], "entities": [{"text": "crossover rate", "start_pos": 161, "end_pos": 175, "type": "METRIC", "confidence": 0.9642325043678284}, {"text": "mutation rate", "start_pos": 194, "end_pos": 207, "type": "METRIC", "confidence": 0.9885081052780151}]}, {"text": "For each run of the system a limit was set to 5,000 generations.", "labels": [], "entities": []}, {"text": "The human ratings revealed that the best stories were produced fora population size of 10,000, a crossover rate of 0.1% and a mutation rate of 0.1%.", "labels": [], "entities": [{"text": "crossover rate", "start_pos": 97, "end_pos": 111, "type": "METRIC", "confidence": 0.9818483889102936}, {"text": "mutation rate", "start_pos": 126, "end_pos": 139, "type": "METRIC", "confidence": 0.987766683101654}]}, {"text": "Compared to previous work (e.g., Karamanis and Manurung 2002) our crossover rate may seem low and the mutation rate high.", "labels": [], "entities": [{"text": "crossover rate", "start_pos": 66, "end_pos": 80, "type": "METRIC", "confidence": 0.7920836806297302}, {"text": "mutation rate", "start_pos": 102, "end_pos": 115, "type": "METRIC", "confidence": 0.9676785469055176}]}, {"text": "However, it makes intuitively sense, as high crossover may lead to incoherence by disrupting canonical action sequences found in the plots.", "labels": [], "entities": []}, {"text": "On the other hand, a higher mutation will raise the likelihood of a lexical item being swapped for another and may improve overall coherence and interest.", "labels": [], "entities": []}, {"text": "The fitness function was trained on 200 documents from the fairy tales collection using Joachims's (2002) SVM light package and entity transition sequences of length 2.", "labels": [], "entities": [{"text": "Joachims's (2002) SVM light package", "start_pos": 88, "end_pos": 123, "type": "DATASET", "confidence": 0.6112654022872448}]}, {"text": "The realizer was interfaced with a trigram language model trained on the British National Corpus with the SRI toolkit.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 73, "end_pos": 96, "type": "DATASET", "confidence": 0.960362454255422}, {"text": "SRI toolkit", "start_pos": 106, "end_pos": 117, "type": "DATASET", "confidence": 0.935122013092041}]}, {"text": "Evaluation We compared the stories generated by the GA against those produced by the rank-based system described in McIntyre and Lapata (2009) and a system that creates stories from the plot graph, without any stochastic search.", "labels": [], "entities": []}, {"text": "Since plot graphs are weighted, we can simply select the graph with the highest weight.", "labels": [], "entities": []}, {"text": "After expanding all lexical variables, the chosen plot graph will give rise to different stories (e.g., castle or temple in the example above).", "labels": [], "entities": []}, {"text": "We select the story ranked highest according to our coherence function.", "labels": [], "entities": []}, {"text": "In addition, we included a baseline which randomly selects sentences from the training corpus provided they contain either of the story protagonists (i.e., entities in the input sentence).", "labels": [], "entities": []}, {"text": "Sentence length was limited to 12 words or less as this was on average the length of the sentences generated by our GA system.", "labels": [], "entities": [{"text": "Sentence length", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7178438901901245}, {"text": "GA system", "start_pos": 116, "end_pos": 125, "type": "DATASET", "confidence": 0.8315243124961853}]}, {"text": "Each system created stories for 12 input sentences, resulting in 48 (4\u00d712) stories for evaluation.", "labels": [], "entities": []}, {"text": "The sentences used commonly occurring entities in the fairy tales corpus (e.g., The child watches the bird, The emperor rules the kingdom., The wizard casts the spell.).", "labels": [], "entities": []}, {"text": "The stories were split into three sets containing four stories from each system but with only one story from each input sentence.", "labels": [], "entities": []}, {"text": "All stories had the same length, namely five sentences.", "labels": [], "entities": []}, {"text": "Human judges were presented with one of the three sets and asked to rate the stories on a scale of 1 to 5 for fluency (was the sentence grammatical?), coherence (does the story make sense overall?) and interest (how interesting is the story?).", "labels": [], "entities": [{"text": "interest", "start_pos": 202, "end_pos": 210, "type": "METRIC", "confidence": 0.980828583240509}]}, {"text": "The stories were presented in random order and participants were told that all of them were generated by a computer program.", "labels": [], "entities": []}, {"text": "They were instructed to rate more favorably interesting stories, stories that were comprehensible and overall grammatical.", "labels": [], "entities": []}, {"text": "The study was conducted over the Internet using WebExp ( and was completed by 56 volunteers, all self reported native English speakers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Human evaluation results: mean story  ratings for four story generators;  *  : p < 0.05,   *  *  : p < 0.01, significantly different from  GA-based system.", "labels": [], "entities": []}]}