{"title": [], "abstractContent": [{"text": "Many NLP tasks need accurate knowledge for semantic inference.", "labels": [], "entities": [{"text": "semantic inference", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.8643600940704346}]}, {"text": "To this end, mostly WordNet is utilized.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.9546355605125427}]}, {"text": "Yet Word-Net is limited, especially for inference between predicates.", "labels": [], "entities": [{"text": "Word-Net", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.8883430361747742}]}, {"text": "To help filling this gap, we present an algorithm that generates inference rules between predicates from FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 105, "end_pos": 113, "type": "DATASET", "confidence": 0.9184496998786926}]}, {"text": "Our experiment shows that the novel resource is effective and complements WordNet in terms of rule coverage.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9368553161621094}]}], "introductionContent": [{"text": "Many text understanding applications, such as Question Answering (QA) and Information Extraction (IE), need to infer a target textual meaning from other texts.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.7788266241550446}, {"text": "Question Answering (QA)", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.8799489974975586}, {"text": "Information Extraction (IE)", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.8626744389533997}]}, {"text": "This need was proposed as a generic semantic inference task under the Textual Entailment (TE) paradigm ).", "labels": [], "entities": [{"text": "Textual Entailment (TE) paradigm", "start_pos": 70, "end_pos": 102, "type": "TASK", "confidence": 0.7502600451310476}]}, {"text": "A fundamental component in semantic inference is the utilization of knowledge resources.", "labels": [], "entities": [{"text": "semantic inference", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8408466279506683}]}, {"text": "However, a major obstacle to improving semantic inference performance is the lack of such knowledge ().", "labels": [], "entities": []}, {"text": "We address one prominent type of inference knowledge known as entailment rules, focusing specifically on rules between predicates, such as 'cure X \u21d2 X recover'.", "labels": [], "entities": []}, {"text": "We aim at highly accurate rule acquisition, for which utilizing manually constructed sources seem appropriate.", "labels": [], "entities": [{"text": "rule acquisition", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.864807516336441}]}, {"text": "The most widely used manual resource is WordNet).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.9721983075141907}]}, {"text": "Yet it is incomplete for generating entailment rules between predicates (Section 2.1).", "labels": [], "entities": []}, {"text": "Hence, other manual resources should also be targeted.", "labels": [], "entities": []}, {"text": "In this work 1 , we explore how FrameNet () could be effectively used for generating entailment rules between predicates.", "labels": [], "entities": []}, {"text": "The detailed description of our work can be found in).", "labels": [], "entities": []}, {"text": "FrameNet is a manually constructed database based on Frame Semantics.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9306687712669373}]}, {"text": "It models the semantic argument structure of predicates in terms of prototypical situations called frames.", "labels": [], "entities": []}, {"text": "Prior work utilized FrameNet's argument mapping capabilities but took entailment relations from other resources, namely WordNet.", "labels": [], "entities": [{"text": "argument mapping", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.6179579049348831}, {"text": "WordNet", "start_pos": 120, "end_pos": 127, "type": "DATASET", "confidence": 0.9635374546051025}]}, {"text": "We propose a novel method for generating entailment rules from FrameNet by detecting the entailment relations implied in FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 121, "end_pos": 129, "type": "DATASET", "confidence": 0.9285632371902466}]}, {"text": "We utilize FrameNet's annotated sentences and relations between frames to extract both the entailment relations and their argument mappings.", "labels": [], "entities": []}, {"text": "Our analysis shows that the rules generated by our algorithm have a reasonable \"per-rule\" accuracy of about 70% 2 . We tested the generated ruleset on an entailment testbed derived from an IE benchmark and compared it both to WordNet and to state-of-the-art rule generation from FrameNet.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.8867546916007996}, {"text": "WordNet", "start_pos": 226, "end_pos": 233, "type": "DATASET", "confidence": 0.9727118611335754}]}, {"text": "Our experiment shows that our method outperforms prior work.", "labels": [], "entities": []}, {"text": "In addition, our rule-set's performance is comparable to WordNet and it is complementary to WordNet when uniting the two resources.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9694241881370544}, {"text": "WordNet", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9763022065162659}]}, {"text": "Finally, additional analysis shows that our rule-set accuracy is 90% in practical use.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9935274720191956}]}], "datasetContent": [{"text": "We would like to evaluate the overall utility of our resource for NLP applications, assessing the correctness of the actual rule applications performed in practice, as well as to compare its performance to related resources.", "labels": [], "entities": []}, {"text": "To this end, we follow the experimental setup presented in, which utilized the ACE 2005 event dataset 3 as a testbed for entailment rule-sets.", "labels": [], "entities": [{"text": "ACE 2005 event dataset 3", "start_pos": 79, "end_pos": 103, "type": "DATASET", "confidence": 0.9729825139045716}]}, {"text": "We briefly describe this setup here.", "labels": [], "entities": []}, {"text": "The task is to extract argument mentions for 26 events, such as Sue and Attack, from the ACE annotated corpus, using a given tested entailment rule-set.", "labels": [], "entities": [{"text": "Sue and Attack", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.6647943059603373}, {"text": "ACE annotated corpus", "start_pos": 89, "end_pos": 109, "type": "DATASET", "confidence": 0.9093902309735616}]}, {"text": "Each event is represented by a set of unary seed templates, one for each event argument.", "labels": [], "entities": []}, {"text": "Some seed templates for Attack are 'Attacker subj \u2190\u2212attack' and 'attack obj \u2212\u2192Target'.", "labels": [], "entities": []}, {"text": "Argument mentions are found in the ACE corpus by matching either the seed templates or templates entailing them found in the tested rule-set.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.8358132541179657}]}, {"text": "We manually added for each event its relevant WordNet synset-ids and FrameNet frame-ids, so only rules fitting the event target meaning will be extracted from the tested rule-sets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9377583861351013}, {"text": "FrameNet frame-ids", "start_pos": 69, "end_pos": 87, "type": "DATASET", "confidence": 0.7663598358631134}]}], "tableCaptions": [{"text": " Table 1: Macro average Recall (R), Precision (P)  and F1 results for the tested configurations.", "labels": [], "entities": [{"text": "Macro average Recall (R)", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.8506959279378256}, {"text": "Precision (P)", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.9728820621967316}, {"text": "F1", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9968361854553223}]}]}