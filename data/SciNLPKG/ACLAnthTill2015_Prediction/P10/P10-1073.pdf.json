{"title": [{"text": "Kernel Based Discourse Relation Recognition with Temporal Ordering Information", "labels": [], "entities": [{"text": "Kernel Based Discourse Relation Recognition", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7388015985488892}]}], "abstractContent": [{"text": "Syntactic knowledge is important for discourse relation recognition.", "labels": [], "entities": [{"text": "discourse relation recognition", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.8216700355211893}]}, {"text": "Yet only heu-ristically selected flat paths and 2-level production rules have been used to incorporate such information so far.", "labels": [], "entities": []}, {"text": "In this paper we propose using tree kernel based approach to automatically mine the syntactic information from the parse trees for discourse analysis, applying kernel function to the tree structures directly.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.718189999461174}]}, {"text": "These structural syntactic features, together with other normal flat features are incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification for both explicit and implicit relations.", "labels": [], "entities": [{"text": "discourse identification", "start_pos": 167, "end_pos": 191, "type": "TASK", "confidence": 0.742143303155899}]}, {"text": "The experiment shows tree kernel approach is able to give statistical significant improvements over flat syntactic path feature.", "labels": [], "entities": []}, {"text": "We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further incorporate information from a higher dimension space for possible better discrimination.", "labels": [], "entities": []}, {"text": "Besides, we further propose to leverage on temporal ordering information to constrain the interpretation of discourse relation, which also demonstrate statistical significant improvements for discourse relation recognition on PDTB 2.0 for both explicit and implicit as well.", "labels": [], "entities": [{"text": "discourse relation recognition", "start_pos": 192, "end_pos": 222, "type": "TASK", "confidence": 0.6594276030858358}, {"text": "PDTB 2.0", "start_pos": 226, "end_pos": 234, "type": "DATASET", "confidence": 0.8933933973312378}]}], "introductionContent": [{"text": "Discourse relations capture the internal structure and logical relationship of coherent text, including Temporal, Causal and Contrastive relations etc.", "labels": [], "entities": []}, {"text": "The ability of recognizing such relations between text units including identifying and classifying provides important information to other natural language processing systems, such as language generation, document summarization, and question answering.", "labels": [], "entities": [{"text": "language generation", "start_pos": 184, "end_pos": 203, "type": "TASK", "confidence": 0.7253241688013077}, {"text": "document summarization", "start_pos": 205, "end_pos": 227, "type": "TASK", "confidence": 0.6747052669525146}, {"text": "question answering", "start_pos": 233, "end_pos": 251, "type": "TASK", "confidence": 0.9071022570133209}]}, {"text": "For example, Causal relation can be used to answer more sophisticated, non-factoid 'Why' questions.", "labels": [], "entities": [{"text": "Why' questions", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.8606282472610474}]}, {"text": "demonstrates that modeling discourse structure requires prior linguistic analysis on syntax.", "labels": [], "entities": []}, {"text": "This shows the importance of syntactic knowledge to discourse analysis.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7563689947128296}]}, {"text": "However, most of previous work only deploys lexical and semantic features () with only two exceptions.", "labels": [], "entities": []}, {"text": "Nevertheless, only uses flat syntactic path connecting connective and arguments in the parse tree.", "labels": [], "entities": []}, {"text": "The hierarchical structured information in the trees is not well preserved in their flat syntactic path features.", "labels": [], "entities": []}, {"text": "Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for discourse analysis.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.7143917977809906}]}, {"text": "The more recent work from Lin et al.", "labels": [], "entities": []}, {"text": "(2009) uses 2-level production rules to represent parse tree information.", "labels": [], "entities": [{"text": "parse tree information", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.8848700523376465}]}, {"text": "Yet it doesn't coverall the other sub-trees structural information which can be also useful for the recognition.", "labels": [], "entities": [{"text": "recognition", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.9715274572372437}]}, {"text": "In this paper we propose using tree kernel based method to automatically mine the syntactic information from the parse trees for discourse analysis, applying kernel function to the parse tree structures directly.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.7253997772932053}]}, {"text": "These structural syntactic features, together with other flat features are then incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification.", "labels": [], "entities": [{"text": "discourse identification and classification", "start_pos": 165, "end_pos": 208, "type": "TASK", "confidence": 0.6955983340740204}]}, {"text": "The experiment shows that tree kernel is able to effectively incorporate syntactic structural information and produce statistical significant improvements over flat syntactic path feature for the recognition of both explicit and implicit relation in Penn Discourse Treebank (PDTB;).", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB", "start_pos": 250, "end_pos": 279, "type": "DATASET", "confidence": 0.9500237226486206}]}, {"text": "We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further work on a higher dimensional space for possible better discrimination.", "labels": [], "entities": []}, {"text": "Besides, inspired by the linguistic study on tense and discourse anaphor, we further propose to incorporate temporal ordering information to constrain the interpretation of discourse relation, which also demonstrates statistical significant improvements for discourse relation recognition on PDTB v2.0 for both explicit and implicit relations.", "labels": [], "entities": [{"text": "discourse relation recognition", "start_pos": 258, "end_pos": 288, "type": "TASK", "confidence": 0.6601088146368662}, {"text": "PDTB v2.0", "start_pos": 292, "end_pos": 301, "type": "DATASET", "confidence": 0.8895500302314758}]}, {"text": "The organization of the rest of the paper is as follows.", "labels": [], "entities": []}, {"text": "We briefly introduce PDTB in Section 2.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.4688431918621063}]}, {"text": "Section 3 gives the related work on tree kernel approach in NLP and its difference with production rules, and also linguistic study on tense and discourse anaphor.", "labels": [], "entities": []}, {"text": "Section 4 introduces the framework for discourse recognition, as well as the baseline feature space and the SVM classifier.", "labels": [], "entities": [{"text": "discourse recognition", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.776432603597641}]}, {"text": "We present our kernel-based method in Section 5, and the usage of temporal ordering feature in Section 6.", "labels": [], "entities": []}, {"text": "Section 7 shows the experiments and discussions.", "labels": [], "entities": []}, {"text": "We conclude our works in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we provide the results of a set of experiments focused on the task of simultaneous discourse identification and classification.", "labels": [], "entities": [{"text": "discourse identification and classification", "start_pos": 99, "end_pos": 142, "type": "TASK", "confidence": 0.6707637533545494}]}, {"text": "We experiment on PDTB v2.0 corpus.", "labels": [], "entities": [{"text": "PDTB v2.0 corpus", "start_pos": 17, "end_pos": 33, "type": "DATASET", "confidence": 0.9089845816294352}]}, {"text": "Besides four top-level discourse relations, we also consider Entity and No relations described in Section 2.", "labels": [], "entities": []}, {"text": "We directly use the golden standard parse trees in Penn TreeBank.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9932965040206909}]}, {"text": "We employ an SVM coreference resolver trained and tested on ACE 2005 with 79.5% Precision, 66.7% Recall and 72.5% F 1 to label coreference mentions of the same named entity in an article.", "labels": [], "entities": [{"text": "SVM coreference resolver", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.7088312904040018}, {"text": "ACE 2005", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9595444798469543}, {"text": "Precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9984777569770813}, {"text": "Recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9983670115470886}, {"text": "F 1", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9935329854488373}]}, {"text": "For learning, we use the binary SVMLight developed by) and Tree Kernel Toolkits developed by).", "labels": [], "entities": []}, {"text": "All classifiers are trained with default learning parameters.", "labels": [], "entities": []}, {"text": "The performance is evaluated using Accuracy which is calculated as follow: \u00ed \u00b5\u00ed\u00b0\u00b4\u00ed \u00b5\u00ed\u00b1\u0090\u00ed \u00b5\u00ed\u00b1\u0090\u00ed \u00b5\u00ed\u00b1\u00a2\u00ed \u00b5\u00ed\u00b1\u009f\u00ed \u00b5\u00ed\u00b1\u008e\u00ed \u00b5\u00ed\u00b1\u0090\u00ed \u00b5\u00ed\u00b1\u00a6 = \u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u009f\u00ed \u00b5\u00ed\u00b1\u00a2\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u0083\u00ed \u00b5\u00ed\u00b1\u009c\u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1\u00a1\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1\u00a3\u00ed \u00b5\u00ed\u00b1\u0092 + \u00ed \u00b5\u00ed\u00b1\u0087\u00ed \u00b5\u00ed\u00b1\u009f\u00ed \u00b5\u00ed\u00b1\u00a2\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u0081\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u0094\u00ed \u00b5\u00ed\u00b1\u008e\u00ed \u00b5\u00ed\u00b1\u00a1\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1\u00a3\u00ed \u00b5\u00ed\u00b1\u0092 \u00ed \u00b5\u00ed\u00b0\u00b4\u00ed \u00b5\u00ed\u00b1\u0099\u00ed \u00b5\u00ed\u00b1\u0099 Sections 2-22 are used for training and Sections 23-24 for testing.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9993659853935242}]}, {"text": "In this paper, we only consider any non-overlapping clauses/sentences pair in 3-sentence spans.", "labels": [], "entities": []}, {"text": "For training, there were 14812, 12843 and 4410 instances for Explicit, Implicit and Entity+No relations respectively; while for testing, the number was 1489, 1167 and 380.", "labels": [], "entities": []}, {"text": "lists the performance of simultaneous identification and classification on level-1 discourse senses.", "labels": [], "entities": [{"text": "simultaneous identification and classification", "start_pos": 25, "end_pos": 71, "type": "TASK", "confidence": 0.6766945719718933}]}, {"text": "In the first row, only base features described in Section 4 are used.", "labels": [], "entities": []}, {"text": "In the second row, we test's algorithm which uses heuristically defined syntactic paths and acts as a good baseline to compare with our learned-based approach using the structured information.", "labels": [], "entities": []}, {"text": "The last three rows of reports the results combining base features with three syntactic structured features (i.e. Min-Expansion, Simple-Expansion and Full-Expansion) described in Section 5.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Results of the syntactic structured ker- nels on level-1 discourse relation recognition.", "labels": [], "entities": [{"text": "discourse relation recognition", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.6216615041097006}]}, {"text": " Table 5. We observe that the  use of temporal ordering information increases  the accuracy by 3%, 3.6% and 3.2% for Explicit,  Implicit and All groups respectively. We conduct  chi square statistical significant test on All rela- tions, which shows the performance improve- ment is statistical significant (\u00ed \u00b5\u00ed\u00bc\u008c < 0.05). It indi- cates that temporal ordering information can  constrain the discourse relation types inferred  within a clause(s)/sentence(s) pair for both expli- cit and implicit relations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9996777772903442}]}, {"text": " Table 5. Results of tense and temporal order  information on level-1 discourse relations.", "labels": [], "entities": []}]}