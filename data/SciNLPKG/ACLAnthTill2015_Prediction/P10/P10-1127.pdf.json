{"title": [{"text": "Generating image descriptions using dependency relational patterns", "labels": [], "entities": [{"text": "Generating image descriptions", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.782494306564331}]}], "abstractContent": [{"text": "This paper presents a novel approach to automatic captioning of geo-tagged images by summarizing multiple web-documents that contain information related to an image's location.", "labels": [], "entities": [{"text": "automatic captioning of geo-tagged images", "start_pos": 40, "end_pos": 81, "type": "TASK", "confidence": 0.7984642148017883}]}, {"text": "The summa-rizer is biased by dependency pattern models towards sentences which contain features typically provided for different scene types such as those of churches, bridges, etc.", "labels": [], "entities": []}, {"text": "Our results show that summaries biased by dependency pattern models lead to significantly higher ROUGE scores than both n-gram language models reported in previous work and also Wikipedia base-line summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.9813025593757629}, {"text": "ROUGE scores", "start_pos": 97, "end_pos": 109, "type": "METRIC", "confidence": 0.9773211181163788}]}, {"text": "Summaries generated using dependency patterns also lead to more readable summaries than those generated without dependency patterns.", "labels": [], "entities": []}], "introductionContent": [{"text": "The number of images tagged with location information on the web is growing rapidly, facilitated by the availability of GPS (Global Position System) equipped cameras and phones, as well as by the widespread use of online social sites.", "labels": [], "entities": []}, {"text": "The majority of these images are indexed with GPS coordinates (latitude and longitude) only and/or have minimal captions.", "labels": [], "entities": []}, {"text": "This typically small amount of textual information associated with the image is of limited usefulness for image indexing, organization and search.", "labels": [], "entities": [{"text": "image indexing", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7239806056022644}]}, {"text": "Therefore methods which could automatically supplement the information available for image indexing and lead to improved image retrieval would be extremely useful.", "labels": [], "entities": [{"text": "image indexing", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.7822225987911224}, {"text": "image retrieval", "start_pos": 121, "end_pos": 136, "type": "TASK", "confidence": 0.7264590412378311}]}, {"text": "Following the general approach proposed by, in this paper we describe a method for automatic image captioning or caption enhancement starting with only a scene or subject type and a set of place names pertaining to an image -for example church, {St. Paul's,London}}.", "labels": [], "entities": [{"text": "automatic image captioning or caption enhancement", "start_pos": 83, "end_pos": 132, "type": "TASK", "confidence": 0.697916621963183}]}, {"text": "Scene type and place names can be obtained automatically given GPS coordinates and compass information using techniques such as those described in -that task is not the focus of this paper.", "labels": [], "entities": []}, {"text": "Our method applies only to images of static features of the built or natural landscape, i.e. objects with persistent geo-coordinates, such as buildings and mountains, and not to images of objects which move about in such landscapes, e.g. people, cars, clouds, etc.", "labels": [], "entities": []}, {"text": "However, our technique is suitable not only for image captioning but in any application context that requires summary descriptions of instances of object classes, where the instance is to be characterized in terms of the features typically mentioned in describing members of the class.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7288944125175476}]}, {"text": "Aker and have argued that humans appear to have a conceptual model of what is salient regarding a certain object type (e.g. church, bridge, etc.) and that this model informs their choice of what to say when describing an instance of this type.", "labels": [], "entities": []}, {"text": "They also experimented with representing such conceptual models using n-gram language models derived from corpora consisting of collections of descriptions of instances of specific object types (e.g. a corpus of descriptions of churches, a corpus of bridge descriptions, and so on) and reported results showing that incorporating such n-gram language models as a feature in a feature-based extractive summarizer improves the quality of automatically generated summaries.", "labels": [], "entities": []}, {"text": "The main weakness of n-gram language models is that they only capture very local information about short term sequences and cannot model long distance dependencies between terms.", "labels": [], "entities": []}, {"text": "For example one common and important feature of object descriptions is the simple specification of the object type, e.g. the information that the object London Bridge is abridge or that the Rhine is a river.", "labels": [], "entities": [{"text": "London Bridge is abridge", "start_pos": 153, "end_pos": 177, "type": "DATASET", "confidence": 0.9370825439691544}]}, {"text": "If this information is expressed as in the first line of, n-gram language models are likely to: Example of sentences which express the type of an object.", "labels": [], "entities": []}, {"text": "London Bridge is a bridge...", "labels": [], "entities": [{"text": "London Bridge", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9901526570320129}]}, {"text": "The Rhine (German: Rhein; Dutch: Rijn; French: Rhin; Romansh: Rain; Italian: Reno; Latin: Rhenus West Frisian Ryn) is one of the longest and most important rivers in Europe...", "labels": [], "entities": []}, {"text": "reflect it, since one would expect the tri-gram is abridge to occur with high frequency in a corpus of bridge descriptions.", "labels": [], "entities": []}, {"text": "However, if the type predication occurs with less commonly seen local context, as is the case for the object Rhine in the second row of -most important rivers -n-gram language models may well be unable to identify it.", "labels": [], "entities": []}, {"text": "Intuitively, what is important in both these cases is that there is a predication whose subject is the object instance of interest and the head of whose complement is the object type: London Bridge ... is ...", "labels": [], "entities": [{"text": "London Bridge", "start_pos": 184, "end_pos": 197, "type": "DATASET", "confidence": 0.9884546101093292}]}, {"text": "bridge and Rhine ... is ... river.", "labels": [], "entities": [{"text": "Rhine", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.9788186550140381}]}, {"text": "Sentences matching such patterns are likely to be important ones to include in a summary.", "labels": [], "entities": []}, {"text": "This intuition suggests that rather than representing object type conceptual models via corpus-derived language models as do, we do so instead using corpus-derived dependency patterns.", "labels": [], "entities": []}, {"text": "We pursue this idea in this paper, our hypothesis being that information that is important for describing objects of a given type will frequently be realized linguistically via expressions with the same dependency structure.", "labels": [], "entities": []}, {"text": "We explore this hypothesis by developing a method for deriving common dependency patterns from object type corpora (Section 2) and then incorporating these patterns into an extractive summarization system (Section 3).", "labels": [], "entities": []}, {"text": "In Section 4 we evaluate the approach both by scoring against model summaries and via a readability assessment.", "labels": [], "entities": []}, {"text": "Since our work aims to extend the work of Aker and Gaizauskas we reproduce their experiments with n-gram language models in the current setting so as to permit accurate comparison.", "labels": [], "entities": []}, {"text": "Multi-document summarizers face the problem of avoiding redundancy: often, important information which must be included in the summary is repeated several times across the document set, but must be included in the summary only once.", "labels": [], "entities": [{"text": "Multi-document summarizers", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5816970020532608}]}, {"text": "We can use the dependency pattern approach to address this problem in a novel way.", "labels": [], "entities": []}, {"text": "The common approach to avoiding redundancy is to use a text similarity measure to block the addition of a further sentence to the summary if it is too similar to one already included.", "labels": [], "entities": []}, {"text": "Instead, since specific dependency patterns express specific types of in- formation we can group the patterns into groups expressing the same type of information and then, during sentence selection, ensure that sentences matching patterns from different groups are selected in order to guarantee broad, non-redundant coverage of information relevant for inclusion in the summary.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.6844060570001602}]}, {"text": "We report work experimenting with this idea too.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our approach we used two different assessment methods: ROUGE (Lin, 2004) and manual readability.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9977920055389404}]}, {"text": "In the following we first describe the data sets used in each of these evaluations, and then we present the results of each assessment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: ROUGE scores for each single feature and Wikipedia baseline.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9916129112243652}, {"text": "Wikipedia baseline", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.9248524904251099}]}, {"text": " Table 6: ROUGE scores of feature combinations which score moderately", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9874419569969177}]}, {"text": " Table 7: Readability evaluation results: Each cell shows the percentage of summaries scoring the ranking score heading the column for each criterion in the", "labels": [], "entities": []}]}