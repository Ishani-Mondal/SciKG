{"title": [{"text": "Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules", "labels": [], "entities": [{"text": "Hierarchical Phrase-Based Translation", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.632890115181605}]}], "abstractContent": [{"text": "This paper presents a novel filtration criterion to restrict the rule extraction for the hierarchical phrase-based translation model, where a bilingual but relaxed well-formed dependency restriction is used to filter out bad rules.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7421806752681732}]}, {"text": "Furthermore, anew feature which describes the regularity that the source/target dependency edge triggers the target/source word is also proposed.", "labels": [], "entities": []}, {"text": "Experimental results show that, the new criteria weeds out about 40% rules while with translation performance improvement , and the new feature brings another improvement to the baseline system, especially on larger corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hierarchical phrase-based (HPB) model) is the state-of-the-art statistical machine translation (SMT) model.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 63, "end_pos": 100, "type": "TASK", "confidence": 0.8049493829409281}]}, {"text": "By looking for phrases that contain other phrases and replacing the subphrases with nonterminal symbols, it gets hierarchical rules.", "labels": [], "entities": []}, {"text": "Hierarchical rules are more powerful than conventional phrases since they have better generalization capability and could capture long distance reordering.", "labels": [], "entities": []}, {"text": "However, when the training corpus becomes larger, the number of rules will grow exponentially, which inevitably results in slow and memory-consuming decoding.", "labels": [], "entities": []}, {"text": "In this paper, we address the problem of reducing the hierarchical translation rule table resorting to the dependency information of bilingual languages.", "labels": [], "entities": []}, {"text": "We only keep rules that both sides are relaxed-well-formed (RWF) dependency structure (see the definition in Section 3), and discard others which do not satisfy this constraint.", "labels": [], "entities": []}, {"text": "In this way, about 40% bad rules are weeded out from the original rule table.", "labels": [], "entities": []}, {"text": "However, the performance is even better than the traditional HPB translation system.", "labels": [], "entities": [{"text": "HPB translation", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.7561998069286346}]}, {"text": "Based on the relaxed-well-formed dependency structure, we also introduce anew linguistic feature to enhance translation performance.", "labels": [], "entities": []}, {"text": "In the traditional phrase-based SMT model, there are always lexical translation probabilities based on IBM model 1 (, i.e. p(e|f ), namely, the target word e is triggered by the source word f . Intuitively, however, the generation of e is not only involved with f , sometimes may also be triggered by other context words in the source side.", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8939206600189209}]}, {"text": "Here we assume that the dependency edge (f \u2192 f \u2032 ) of word f generates target word e (we call it headword trigger in Section 4).", "labels": [], "entities": []}, {"text": "Therefore, two words in one language trigger one word in another, which provides a more sophisticated and better choice for the target word, i.e..", "labels": [], "entities": []}, {"text": "Similarly, the dependency feature works well in Chinese-to-English translation task, especially on large corpus.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.6047105491161346}]}], "datasetContent": [{"text": "In this section, we describe the experimental setting used in this work, and verify the effect of the relaxed-well-formed structure filtering and the new feature, headword trigger.", "labels": [], "entities": []}, {"text": "Experiments are carried out on the NIST 1 Chinese-English translation task with two different size of training corpora.", "labels": [], "entities": [{"text": "NIST 1 Chinese-English translation task", "start_pos": 35, "end_pos": 74, "type": "TASK", "confidence": 0.770469605922699}]}, {"text": "\u2022 FBIS: We use the FBIS corpus as the first training corpus, which contains 239K sentence pairs with 6.9M Chinese words and 8.9M English words.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.7906565368175507}]}, {"text": "\u2022 GQ: This is manually selected from the LDC 2 corpora.", "labels": [], "entities": [{"text": "GQ", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.7170971035957336}, {"text": "LDC 2 corpora", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.8847272992134094}]}, {"text": "GQ contains 1.5M sentence pairs with 41M Chinese words and 48M English words.", "labels": [], "entities": [{"text": "GQ", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.9164519309997559}]}, {"text": "In fact, FBIS is the subset of GQ.", "labels": [], "entities": [{"text": "GQ", "start_pos": 31, "end_pos": 33, "type": "DATASET", "confidence": 0.9206438064575195}]}, {"text": "For language model, we use the SRI Language Modeling Toolkit) to train a 4-gram model on the first 1/3 of the Xinhua portion of GIGAWORD corpus.", "labels": [], "entities": [{"text": "SRI Language Modeling Toolkit", "start_pos": 31, "end_pos": 60, "type": "DATASET", "confidence": 0.6887641102075577}, {"text": "Xinhua portion of GIGAWORD corpus", "start_pos": 110, "end_pos": 143, "type": "DATASET", "confidence": 0.736106538772583}]}, {"text": "And we use the NIST 2002 MT evaluation test set as our development set, and test sets as our blind test sets.", "labels": [], "entities": [{"text": "NIST 2002 MT evaluation test set", "start_pos": 15, "end_pos": 47, "type": "DATASET", "confidence": 0.928268571694692}]}, {"text": "We evaluate the translation quality using case-insensitive BLEU metric () without dropping OOV words, and the feature weights are tuned by minimum error rate training.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 59, "end_pos": 70, "type": "METRIC", "confidence": 0.9713923037052155}, {"text": "OOV", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9763045907020569}]}, {"text": "In order to get the dependency relation of the training corpus, we re-implement a beam-search style monolingual dependency parser according to).", "labels": [], "entities": []}, {"text": "Then we use the same method suggested in) to extract SCFG grammar rules within dependency constraint on both sides except that unaligned words are allowed at the edge of phrases.", "labels": [], "entities": []}, {"text": "Parameters of headword trigger are estimated as described in Section 4.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9804376363754272}]}, {"text": "As a default, the maximum initial phrase length is set to 10 and the maximum rule length of the source side is set to 5.", "labels": [], "entities": []}, {"text": "Besides, we also re-implement the decoder of Hiero (Chiang, 2007) as our baseline.", "labels": [], "entities": [{"text": "Hiero (Chiang, 2007)", "start_pos": 45, "end_pos": 65, "type": "DATASET", "confidence": 0.8930782775084177}]}, {"text": "In fact, we just exploit the dependency structure during the rule extraction phase.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.828664094209671}]}, {"text": "Therefore, we don't need to change the main decoding algorithm of the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9843912124633789}]}], "tableCaptions": [{"text": " Table 1: Rule table size with different con- straint on FBIS. Here HPB refers to the base- line hierarchal phrase-based system, RWF means  relaxed-well-formed constraint and WF represents  the well-formed structure.", "labels": [], "entities": [{"text": "RWF", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.9332699179649353}]}, {"text": " Table 2: Results of FBIS corpus. Here Tri means  the feature of head word trigger on both sides. And  we don't test the new feature on Test04 because of  the bad performance on development set. * or **  = significantly better than baseline (p < 0.05 or  0.01, respectively).", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.7295426726341248}]}, {"text": " Table 3: Results of GQ corpus. * or ** = sig- nificantly better than baseline (p < 0.05 or 0.01,  respectively).", "labels": [], "entities": [{"text": "GQ corpus", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.8730595707893372}]}]}