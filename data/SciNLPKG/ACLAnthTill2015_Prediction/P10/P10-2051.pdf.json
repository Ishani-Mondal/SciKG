{"title": [{"text": "Jointly optimizing a two-step conditional random field model for machine transliteration and its fast decoding algorithm", "labels": [], "entities": [{"text": "machine transliteration", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.7515303492546082}]}], "abstractContent": [{"text": "This paper presents a joint optimization method of a two-step conditional random field (CRF) model for machine transliter-ation and a fast decoding algorithm for the proposed method.", "labels": [], "entities": []}, {"text": "Our method lies in the category of direct orthographical mapping (DOM) between two languages without using any intermediate phonemic mapping.", "labels": [], "entities": [{"text": "direct orthographical mapping (DOM)", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.7251578370730082}]}, {"text": "In the two-step CRF model, the first CRF segments an input word into chunks and the second one converts each chunk into one unit in the target language.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method to jointly optimize the two-step CRFs and also a fast algorithm to realize it.", "labels": [], "entities": []}, {"text": "Our experiments show that the proposed method outper-forms the well-known joint source channel model (JSCM) and our proposed fast algorithm decreases the decoding time significantly.", "labels": [], "entities": []}, {"text": "Furthermore, combination of the proposed method and the JSCM gives further improvement, which outperforms state-of-the-art results in terms of top-1 accuracy .", "labels": [], "entities": [{"text": "JSCM", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.7545171976089478}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9862490296363831}]}], "introductionContent": [{"text": "There are more than 6000 languages in the world and 10 languages of them have more than 100 million native speakers.", "labels": [], "entities": []}, {"text": "With the information revolution and globalization, systems that support multiple language processing and spoken language translation become urgent demands.", "labels": [], "entities": [{"text": "spoken language translation", "start_pos": 105, "end_pos": 132, "type": "TASK", "confidence": 0.7584515810012817}]}, {"text": "The translation of named entities from alphabetic to syllabary language is usually performed through transliteration, which tries to preserve the pronunciation in the original language.", "labels": [], "entities": [{"text": "translation of named entities from alphabetic to syllabary language", "start_pos": 4, "end_pos": 71, "type": "TASK", "confidence": 0.8225211434894137}]}, {"text": "For example, in Chinese, foreign words are written with Chinese characters; in Japanese, foreign words are usually written with special char- An intuitive transliteration method) is to firstly convert a source word into phonemes, then find the corresponding phonemes in the target language, and finally convert them to the target language's written system.", "labels": [], "entities": []}, {"text": "There are two reasons why this method does notwork well: first, the named entities have diverse origins and this makes the grapheme-tophoneme conversion very difficult; second, the transliteration is usually not only determined by the pronunciation, but also affected by how they are written in the original language.", "labels": [], "entities": [{"text": "grapheme-tophoneme conversion", "start_pos": 123, "end_pos": 152, "type": "TASK", "confidence": 0.6982742249965668}]}, {"text": "Direct orthographical mapping (DOM), which performs the transliteration between two languages directly without using any intermediate phonemic mapping, is recently gaining more attention in the transliteration research community, and it is also the \"Standard Run\" of the \"NEWS 2009 Machine Transliteration Shared Task\" (.", "labels": [], "entities": [{"text": "Direct orthographical mapping (DOM)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7930193642775217}, {"text": "NEWS 2009 Machine Transliteration Shared Task\"", "start_pos": 272, "end_pos": 318, "type": "TASK", "confidence": 0.619107050555093}]}, {"text": "In this paper, we try to make our system satisfy the standard evaluation condition, which requires that the system uses the provided parallel corpus (without pronunciation) only, and cannot use any other bilingual or monolingual resources.", "labels": [], "entities": []}, {"text": "The source channel and joint source channel models (JSCMs) () have been proposed for DOM, which try to model P (T |S) and P (T, S) respectively, where T and S denote the words in the target and source languages.", "labels": [], "entities": []}, {"text": "modified the JSCM to incorporate different context information into the model for Indian languages.", "labels": [], "entities": [{"text": "JSCM", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.9201093316078186}]}, {"text": "In the \"NEWS 2009 Machine Transliteration Shared Task\", anew two-step CRF model for transliteration task has been proposed (, in which the first step is to segment a word in the source language into character chunks and the second step is to perform a context-dependent mapping from each chunk into one written unit in the target language.", "labels": [], "entities": [{"text": "NEWS 2009 Machine Transliteration Shared Task", "start_pos": 8, "end_pos": 53, "type": "TASK", "confidence": 0.7147520780563354}]}, {"text": "In this paper, we propose to jointly optimize a two-step CRF model.", "labels": [], "entities": []}, {"text": "We also propose a fast decoding algorithm to speedup the joint search.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 explains the two-step CRF method, followed by Section 3 which describes our joint optimization method and its fast decoding algorithm; Section 4 introduces a rapid implementation of a JSCM system in the weighted finite state transducer (WFST) framework; and the last section reports the experimental results and conclusions.", "labels": [], "entities": []}, {"text": "Although our method is language independent, we use an English-to-Chinese transliteration task in all the explanations and experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use several metrics from) to measure the performance of our system.", "labels": [], "entities": []}, {"text": "1. Top-1 ACC: word accuracy of the top-1 candidate 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9021901488304138}]}, {"text": "Mean F-score: fuzziness in the top-1 candidate, how close the top-1 candidate is to the reference 3.", "labels": [], "entities": [{"text": "F-score", "start_pos": 5, "end_pos": 12, "type": "METRIC", "confidence": 0.996982753276825}, {"text": "fuzziness", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9798716902732849}]}, {"text": "MRR: mean reciprocal rank, 1/MRR tells approximately the average rank of the correct result", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8280752897262573}, {"text": "mean reciprocal rank", "start_pos": 5, "end_pos": 25, "type": "METRIC", "confidence": 0.8955443302790324}, {"text": "MRR", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9903841018676758}]}], "tableCaptions": [{"text": " Table 2: Comparison of the proposed decoding  method with the previous method and the JSCM", "labels": [], "entities": [{"text": "JSCM", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.8446824550628662}]}, {"text": " Table 3: Model combination results", "labels": [], "entities": []}]}