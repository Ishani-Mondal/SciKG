{"title": [], "abstractContent": [{"text": "A central problem in historical linguistics is the identification of historically related cognate words.", "labels": [], "entities": [{"text": "historical linguistics", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.7478700280189514}, {"text": "identification of historically related cognate words", "start_pos": 51, "end_pos": 103, "type": "TASK", "confidence": 0.84400541583697}]}, {"text": "We present a generative phylogenetic model for automatically inducing cognate group structure from un-aligned word lists.", "labels": [], "entities": [{"text": "generative phylogenetic", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.9434340000152588}]}, {"text": "Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages.", "labels": [], "entities": []}, {"text": "We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes.", "labels": [], "entities": []}, {"text": "On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline approach , increasing accuracy by as much as 80%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9990710020065308}]}, {"text": "Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words.", "labels": [], "entities": []}], "introductionContent": [{"text": "A crowning achievement of historical linguistics is the comparative method, wherein linguists use word similarity to elucidate the hidden phonological and morphological processes which govern historical descent.", "labels": [], "entities": []}, {"text": "The comparative method requires reasoning about three important hidden variables: the overall phylogenetic guide tree among languages, the evolutionary parameters of the ambient changes at each branch, and the cognate group structure that specifies which words share common ancestors.", "labels": [], "entities": []}, {"text": "All three of these variables interact and inform each other, and so historical linguists often consider them jointly.", "labels": [], "entities": []}, {"text": "However, linguists are currently required to make qualitative judgments regarding the relative likelihood of certain sound changes, cognate groups, and soon.", "labels": [], "entities": []}, {"text": "Several recent statistical methods have been introduced to provide increased quantitative backing to the comparative method; others have modeled the spread of language changes and speciation (.", "labels": [], "entities": []}, {"text": "These automated methods, while providing robustness and scale in the induction of ancestral word forms and evolutionary parameters, assume that cognate groups are already known.", "labels": [], "entities": []}, {"text": "In this work, we address this limitation, presenting a model in which cognate groups can be discovered automatically.", "labels": [], "entities": []}, {"text": "Finding cognate groups is not an easy task, because underlying morphological and phonological changes can obscure relationships between words, especially for distant cognates, where simple string overlap is an inadequate measure of similarity.", "labels": [], "entities": []}, {"text": "Indeed, a standard string similarity metric like Levenshtein distance can lead to false positives.", "labels": [], "entities": []}, {"text": "Consider the often cited example of Greek /ma:ti/ and Malay /mata/, both meaning \"eye\".", "labels": [], "entities": []}, {"text": "If we were to rely on Levenshtein distance, these words would seem to be a highly attractive match as cognates: they are nearly identical, essentially differing in only a single character.", "labels": [], "entities": []}, {"text": "However, no linguist would posit that these two words are related.", "labels": [], "entities": []}, {"text": "To correctly learn that they are not related, linguists typically rely on two kinds of evidence.", "labels": [], "entities": []}, {"text": "First, because sound change is largely regular, we would need to commonly see /i/ in Greek wherever we see /a/ in Malay).", "labels": [], "entities": []}, {"text": "Second, we should look at languages closely related to Greek and Malay, to see if similar patterns hold there, too.", "labels": [], "entities": []}, {"text": "Some authors have attempted to automatically detect cognate words (, but these methods typically work on language pairs rather than on larger language families.", "labels": [], "entities": []}, {"text": "To fully automate the comparative method, it is necessary to consider multiple languages, and to do so in a model which couples cognate detection with similarity learning.", "labels": [], "entities": [{"text": "cognate detection", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.8173630833625793}]}, {"text": "In this paper, we present anew generative model for the automatic induction of cognate groups given only (1) a known family tree of languages and (2) word lists from those languages.", "labels": [], "entities": []}, {"text": "A prior on word survival generates a number of cognate groups and decides which groups are attested in each modern language.", "labels": [], "entities": [{"text": "word survival", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.724709689617157}]}, {"text": "An evolutionary model captures how each word is generated from its parent word.", "labels": [], "entities": []}, {"text": "Finally, an alignment model maps the flat word lists to cognate groups.", "labels": [], "entities": []}, {"text": "Inference requires a combination of message-passing in the evolutionary model and iterative bipartite graph matching in the alignment model.", "labels": [], "entities": []}, {"text": "In the message-passing phase, our model encodes distributions over strings as weighted finite state automata.", "labels": [], "entities": []}, {"text": "Weighted automata have been successfully applied to speech processing ( and more recently to morphology).", "labels": [], "entities": [{"text": "speech processing", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7891672253608704}]}, {"text": "Here, we present anew method for automatically compressing our message automata in away that can take into account prior information about the expected outcome of inference.", "labels": [], "entities": []}, {"text": "In this paper, we focus on a transcribed word list of 583 cognate sets from three Romance languages (Portuguese, Italian and Spanish), as well as their common ancestor Latin (.", "labels": [], "entities": []}, {"text": "We consider both the case where we know that all cognate groups have a surface form in all languages, and where we do not know that.", "labels": [], "entities": []}, {"text": "On the former, easier task we achieve identification accuracies of 90.6%.", "labels": [], "entities": [{"text": "identification", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.6026221513748169}, {"text": "accuracies", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.5395888090133667}]}, {"text": "On the latter task, we achieve F1 scores of 73.6%.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.974070280790329}]}, {"text": "Both substantially beat baseline performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this experiment, we know precisely how many cognate groups there are and that every cognate group has a word in each language.", "labels": [], "entities": []}, {"text": "While this scenario does not include all of the features of the real-world task, it represents a good test case of how well these models can perform without the non-parametric task of deciding how many clusters to use.", "labels": [], "entities": []}, {"text": "We scrambled the 583 cognate groups in the Romance dataset and ran each method to convergence.", "labels": [], "entities": [{"text": "Romance dataset", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.9102775752544403}]}, {"text": "Besides the heuristic baseline, we tried our model-based approach using Unigrams, Bigrams and Anchored Unigrams, with and without learning the parametric edit distances.", "labels": [], "entities": []}, {"text": "When we did not use learning, we set the parameters of the edit distance to (0, -3, -4) for matches, substitutions, and deletions/insertions, respectively.", "labels": [], "entities": []}, {"text": "With learning enabled, transducers were initialized with those parameters.", "labels": [], "entities": []}, {"text": "For evaluation, we report two metrics.", "labels": [], "entities": []}, {"text": "The first is pairwise accuracy for each pair of languages, averaged across pairs of words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9865877032279968}]}, {"text": "The other is accu-  racy measured in terms of the number of correctly, completely reconstructed cognate groups.", "labels": [], "entities": [{"text": "accu-  racy", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9583567579587301}]}, {"text": "shows the results under various configurations.", "labels": [], "entities": []}, {"text": "As can be seen, the kind of approximation used matters immensely.", "labels": [], "entities": []}, {"text": "In this application, positional information is important, more so than the context of the previous phoneme.", "labels": [], "entities": []}, {"text": "Both Unigrams and Bigrams significantly under-perform the baseline, while Anchored Unigrams easily outperforms it both with and without learning.", "labels": [], "entities": []}, {"text": "An initially surprising result is that learning actually harms performance under the unanchored approximations.", "labels": [], "entities": []}, {"text": "The explanation is that these topologies are not sensitive enough to context, and that the learning procedure ends up flattening the distributions.", "labels": [], "entities": []}, {"text": "In the case of unigrams -which have the least context -learning degrades performance to chance.", "labels": [], "entities": []}, {"text": "However, in the case of positional unigrams, learning reduces the error rate by more than two-thirds.", "labels": [], "entities": [{"text": "error rate", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.983823835849762}]}, {"text": "As a more realistic scenario, we consider the case where we do not know that all cognate groups have words in all languages.", "labels": [], "entities": []}, {"text": "To test our model, we ran-domly pruned 20% of the branches according the survival process of our model.", "labels": [], "entities": []}, {"text": "Because only Anchored Unigrams performed well in Experiment 1, we consider only it and the Dice's coefficient baseline.", "labels": [], "entities": []}, {"text": "The baseline needs to be augmented to support the fact that some words may not appear in all cognate groups.", "labels": [], "entities": []}, {"text": "To do this, we thresholded the bipartite matching process so that if the coefficient fell below some value, we started anew group for that word.", "labels": [], "entities": []}, {"text": "We experimented on 10 values in the range (0,1) for the baseline's threshold and report on the one (0.2) that gives the best pairwise F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.9585803151130676}]}, {"text": "Here again, we see that the positional unigrams perform much better than the baseline system.", "labels": [], "entities": []}, {"text": "The learned transducers seem to sacrifice precision for the sake of increased recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.998871386051178}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9980798959732056}]}, {"text": "This makes sense because the default edit distance parameter settings strongly favor exact matches, while the learned transducers learn more realistic substitution and deletion matrices, at the expense of making more mistakes.", "labels": [], "entities": []}, {"text": "For example, the learned transducers enable our model to correctly infer that Portuguese /d1femdu/, Spanish /defiendo/, and Italian /difEndo/ are all derived from Latin /de:fendo:/ \"defend.\"", "labels": [], "entities": []}, {"text": "Using the simple Levenshtein transducers, on the other hand, our model keeps all three separated, because the transducers cannot knowamong other things -that Portuguese /1/, Spanish /e/, and Italian /i/ are commonly substituted for one another.", "labels": [], "entities": []}, {"text": "Unfortunately, because the transducers used cannot learn contextual rules, certain transformations can be over-applied.", "labels": [], "entities": []}, {"text": "For instance, Spanish /nombRar/ \"name\" is grouped together with Portuguese /num1RaR/ \"number\" and Italian /numerare/ \"number,\" largely because the rule Portuguese /u/ \u2192 Spanish /o/ is applied outside of its normal context.", "labels": [], "entities": []}, {"text": "This sound change occurs primarily with final vowels, and does not usually occur word medially.", "labels": [], "entities": []}, {"text": "Thus, more sophisticated transducers could learn better sound laws, which could translate into improved accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9958645105361938}]}, {"text": "As a final trial, we wanted to see how each automatically found cognate group faired as compared to the \"true groups\" for actual reconstruction of proto-words.", "labels": [], "entities": []}, {"text": "for faithful reconstruction, and so we used the Ancestry Resampling system of.", "labels": [], "entities": [{"text": "Ancestry Resampling", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7957353293895721}]}, {"text": "To evaluate, we matched each Latin word with the best possible cognate group for that word.", "labels": [], "entities": []}, {"text": "The process for the matching was as follows.", "labels": [], "entities": [{"text": "matching", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.9607646465301514}]}, {"text": "If two or three of the words in an constructed cognate group agreed, we assigned the Latin word associated with the true group to it.", "labels": [], "entities": []}, {"text": "With the remainder, we executed a bipartite matching based on bigram overlap.", "labels": [], "entities": []}, {"text": "For evaluation, we examined the Levenshtein distance between the reconstructed word and the chosen Latin word.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 32, "end_pos": 52, "type": "METRIC", "confidence": 0.8838856220245361}]}, {"text": "As a kind of \"skyline,\" we compare to the edit distances reported in, which was based on complete knowledge of the cognate groups.", "labels": [], "entities": []}, {"text": "On this task, our reconstructed cognate groups had an average edit distance of 3.8 from the assigned Latin word.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.9715646803379059}]}, {"text": "This compares favorably to the edit distances reported in, who using oracle cognate assignments achieved an average Levenshtein distance of 3.0.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 116, "end_pos": 136, "type": "METRIC", "confidence": 0.7806771397590637}]}], "tableCaptions": [{"text": " Table 1: Accuracies for reconstructing cognate groups. Lev- enshtein refers to fixed parameter edit distance transducer.  Learned refers to automatically learned edit distances. Pair- wise Accuracy means averaged on each word pair; Exact  Match refers to percentage of completely and accurately re- constructed groups. For a description of the baseline, see Sec- tion 7.1.", "labels": [], "entities": [{"text": "Pair- wise Accuracy", "start_pos": 179, "end_pos": 198, "type": "METRIC", "confidence": 0.7010232210159302}]}, {"text": " Table 2: Accuracies for reconstructing incomplete groups.  Scores reported are precision, recall, and F1, averaged over  all word pairs.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9821657538414001}, {"text": "reconstructing incomplete groups", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.8601242303848267}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9997019171714783}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9988284707069397}, {"text": "F1", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9997240900993347}]}]}