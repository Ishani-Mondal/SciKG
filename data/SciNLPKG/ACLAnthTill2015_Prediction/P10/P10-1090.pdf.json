{"title": [{"text": "Convolution Kernel over Packed Parse Forest", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest.", "labels": [], "entities": []}, {"text": "As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest.", "labels": [], "entities": []}, {"text": "This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel.", "labels": [], "entities": [{"text": "parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.9658768773078918}]}, {"text": "The paper presents the formal definition of convolu-tion forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel.", "labels": [], "entities": []}, {"text": "Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.9273554980754852}, {"text": "semantic role labeling", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.6487527887026469}]}], "introductionContent": [{"text": "Parse tree and packed forest of parse trees are two widely used data structures to represent the syntactic structure information of sentences in natural language processing (NLP).", "labels": [], "entities": []}, {"text": "The structured features embedded in a parse tree have been well explored together with different machine learning algorithms and proven very useful in many NLP applications.", "labels": [], "entities": []}, {"text": "A forest compactly encodes an exponential number of parse trees.", "labels": [], "entities": []}, {"text": "In this paper, we study how to effectively explore structured features embedded in a forest using convolution kernel).", "labels": [], "entities": []}, {"text": "As we know, feature-based machine learning methods are less effective in modeling highly structured objects, such as parse tree or semantic graph in NLP.", "labels": [], "entities": []}, {"text": "This is due to the fact that it is usually very hard to represent structured objects using vectors of reasonable dimensions without losing too much information.", "labels": [], "entities": []}, {"text": "For example, it is computationally infeasible to enumerate all subtree features (using subtree a feature) fora parse tree into a linear feature vector.", "labels": [], "entities": []}, {"text": "Kernel-based machine learning method is a good way to overcome this problem.", "labels": [], "entities": [{"text": "Kernel-based machine learning", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5627267956733704}]}, {"text": "Kernel methods employ a kernel function, that must satisfy the properties of being symmetric and positive, to measure the similarity between two objects by computing implicitly the dot product of certain features of the input objects in high (or even infinite) dimensional feature spaces without enumerating all the features.", "labels": [], "entities": []}, {"text": "Many learning algorithms, such as SVM, the Perceptron learning algorithm and Voted Perceptron), can work directly with kernels by replacing the dot product with a particular kernel function.", "labels": [], "entities": []}, {"text": "This nice property of kernel methods, that implicitly calculates the dot product in a high-dimensional space over the original representations of objects, has made kernel methods an effective solution to modeling structured objects in NLP.", "labels": [], "entities": []}, {"text": "In the context of parse tree, convolution tree kernel () defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees.", "labels": [], "entities": []}, {"text": "The tree kernel has shown much success in many NLP applications like parsing), semantic role labeling, relation extraction ( ), pronoun resolution (), question classification (Zhang and and machine translation, where the tree kernel is used to compute the similarity between two NLP application instances that are usually represented by parse trees.", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9807178974151611}, {"text": "semantic role labeling", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.7313956220944723}, {"text": "relation extraction", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.8324498534202576}, {"text": "pronoun resolution", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.7526103854179382}, {"text": "question classification", "start_pos": 151, "end_pos": 174, "type": "TASK", "confidence": 0.8504217863082886}, {"text": "machine translation", "start_pos": 190, "end_pos": 209, "type": "TASK", "confidence": 0.8212030529975891}]}, {"text": "However, in those studies, the tree kernel only covers the features derived from single 1-best parse tree.", "labels": [], "entities": []}, {"text": "This may largely compromise the performance of tree kernel due to parsing errors and data sparseness.", "labels": [], "entities": [{"text": "parsing", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.9612906575202942}]}, {"text": "To address the above issues, this paper constructs a forest-based convolution kernel to mine structured features directly from packed forest.", "labels": [], "entities": []}, {"text": "A packet forest compactly encodes exponential number of n-best parse trees, and thus containing much more rich structured features than a single parse tree.", "labels": [], "entities": []}, {"text": "This advantage enables the forest kernel not only to be more robust against parsing errors, but also to be able to learn more reliable feature values and help to solve the data sparseness issue that exists in the traditional tree kernel.", "labels": [], "entities": [{"text": "parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9644012451171875}]}, {"text": "We evaluate the proposed kernel in two real NLP applications, relation extraction and semantic role labeling.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.9190947711467743}, {"text": "semantic role labeling", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.6844060818354288}]}, {"text": "Experimental results on the benchmark data show that the forest kernel significantly outperforms the tree kernel.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews the convolution tree kernel while section 3 discusses the proposed forest kernel in details.", "labels": [], "entities": []}, {"text": "Experimental results are reported in section 4.", "labels": [], "entities": []}, {"text": "Finally, we conclude the paper in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Forest kernel has abroad application potential in NLP.", "labels": [], "entities": []}, {"text": "In this section, we verify the effectiveness of the forest kernel on two NLP applications, semantic role labeling (SRL)) and relation extraction (RE)).", "labels": [], "entities": [{"text": "semantic role labeling (SRL", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.7077682375907898}, {"text": "relation extraction (RE", "start_pos": 125, "end_pos": 148, "type": "TASK", "confidence": 0.8213456571102142}]}, {"text": "In our experiments, SVM) is selected as our classifier and the one vs. others strategy is adopted to select the one with the largest margin as the final answer.", "labels": [], "entities": []}, {"text": "In our implementation, we use the binary SVMLight) and borrow the framework of the Tree Kernel Tools) to integrate our forest kernel into the SVMLight.", "labels": [], "entities": []}, {"text": "We modify Charniak parser) to output a packed forest.", "labels": [], "entities": []}, {"text": "Following previous forest-based studies), we use the marginal probabilities of hyper-edges (i.e., the Viterbi-style inside-outside probabilities and set the pruning threshold as 8) for forest pruning.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance comparison of SRL (%)", "labels": [], "entities": [{"text": "SRL", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.8192554712295532}]}, {"text": " Table 2: Performance Comparison of RE (%)  over 23 subtypes on the ACE 2004 data", "labels": [], "entities": [{"text": "RE", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9315953254699707}, {"text": "ACE 2004 data", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.9692074060440063}]}]}