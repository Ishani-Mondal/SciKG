{"title": [{"text": "Improving the Use of Pseudo-Words for Evaluating Selectional Preferences", "labels": [], "entities": [{"text": "Improving", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9745814800262451}, {"text": "Evaluating Selectional Preferences", "start_pos": 38, "end_pos": 72, "type": "TASK", "confidence": 0.7577942709128062}]}], "abstractContent": [{"text": "This paper improves the use of pseudo-words as an evaluation framework for selectional preferences.", "labels": [], "entities": []}, {"text": "While pseudo-words originally evaluated word sense disambiguation, they are now commonly used to evaluate selectional preferences.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.6398166120052338}]}, {"text": "A selectional preference model ranks a set of possible arguments fora verb by their semantic fit to the verb.", "labels": [], "entities": []}, {"text": "Pseudo-words serve as a proxy evaluation for these decisions.", "labels": [], "entities": []}, {"text": "The evaluation takes an argument of a verb like drive (e.g. car), pairs it with an alternative word (e.g. car/rock), and asks a model to identify the original.", "labels": [], "entities": []}, {"text": "This paper studies two main aspects of pseudo-word creation that affect performance results.", "labels": [], "entities": [{"text": "pseudo-word creation", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.7362167239189148}]}, {"text": "(1) Pseudo-word evaluations often evaluate only a subset of the words.", "labels": [], "entities": []}, {"text": "We show that selectional preferences should instead be evaluated on the data in its entirety.", "labels": [], "entities": []}, {"text": "(2) Different approaches to selecting partner words can produce overly optimistic evaluations.", "labels": [], "entities": []}, {"text": "We offer suggestions to address these factors and present a simple baseline that outperforms the state-of-the-art by 13% absolute on a newspaper domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "For many natural language processing (NLP) tasks, particularly those involving meaning, creating labeled test data is difficult or expensive.", "labels": [], "entities": []}, {"text": "One way to mitigate this problem is with pseudowords, a method for automatically creating test corpora without human labeling, originally proposed for word sense disambiguation (.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 151, "end_pos": 176, "type": "TASK", "confidence": 0.6565450529257456}]}, {"text": "While pseudo-words are now less often used for word sense disambigation, they area common way to evaluate selectional preferences, models that measure the strength of association between a predicate and its argument filler, e.g., that the noun lunch is a likely object of eat.", "labels": [], "entities": [{"text": "word sense disambigation", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.7375177343686422}]}, {"text": "Selectional preferences are useful for NLP tasks such as parsing and semantic role labeling (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 57, "end_pos": 64, "type": "TASK", "confidence": 0.9764493703842163}, {"text": "semantic role labeling", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.6516514321168264}]}, {"text": "Since evaluating them in isolation is difficult without labeled data, pseudoword evaluations can bean attractive evaluation framework.", "labels": [], "entities": []}, {"text": "Pseudo-word evaluations are currently used to evaluate a variety of language modeling tasks.", "labels": [], "entities": [{"text": "language modeling tasks", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.7913415431976318}]}, {"text": "However, evaluation design varies across research groups.", "labels": [], "entities": [{"text": "evaluation design", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.8924803137779236}]}, {"text": "This paper studies the evaluation itself, showing how choices can lead to overly optimistic results if the evaluation is not designed carefully.", "labels": [], "entities": []}, {"text": "We show in this paper that current methods of applying pseudo-words to selectional preferences vary greatly, and suggest improvements.", "labels": [], "entities": []}, {"text": "A pseudo-word is the concatenation of two words (e.g. house/car).", "labels": [], "entities": []}, {"text": "One word is the original in a document, and the second is the confounder.", "labels": [], "entities": []}, {"text": "Consider the following example of applying pseudo-words to the selectional restrictions of the verb focus: Original: This story focuses on the campaign.", "labels": [], "entities": []}, {"text": "Test: This story/part focuses on the campaign/meeting.", "labels": [], "entities": []}, {"text": "In the original sentence, focus has two arguments: a subject story and an object campaign.", "labels": [], "entities": []}, {"text": "In the test sentence, each argument of the verb is replaced by pseudo-words.", "labels": [], "entities": []}, {"text": "A model is evaluated by its success at determining which of the two arguments is the original word.", "labels": [], "entities": []}, {"text": "Two problems exist in the current use of pseudo-words to evaluate selectional preferences.", "labels": [], "entities": []}, {"text": "First, selectional preferences historically focus on subsets of data such as unseen words or words in certain frequency ranges.", "labels": [], "entities": []}, {"text": "While work on unseen data is important, evaluating on the entire dataset provides an accurate picture of a model's overall performance.", "labels": [], "entities": []}, {"text": "Most other NLP tasks today evaluate all test examples in a corpus.", "labels": [], "entities": []}, {"text": "We will show that seen arguments actually dominate newspaper articles, and thus propose creating test sets that include all verb-argument examples to avoid artificial evaluations.", "labels": [], "entities": []}, {"text": "Second, pseudo-word evaluations vary in how they choose confounders.", "labels": [], "entities": []}, {"text": "Previous work has attempted to maintain a similar corpus frequency to the original, but it is not clear how best to do this, nor how it affects the task's difficulty.", "labels": [], "entities": []}, {"text": "We argue in favor of using nearest-neighbor frequencies and show how using random confounders produces overly optimistic results.", "labels": [], "entities": []}, {"text": "Finally, we present a surprisingly simple baseline that outperforms the state-of-the-art and is far less memory and computationally intensive.", "labels": [], "entities": []}, {"text": "It outperforms current similarity-based approaches by over 13% when the test set includes all of the data.", "labels": [], "entities": []}, {"text": "We conclude with a suggested backoff model based on this baseline.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the New York Times (NYT) and Associated Press (APW) sections of the Gigaword Corpus, as well as the British National Corpus (BNC) for our analysis.", "labels": [], "entities": [{"text": "New York Times (NYT) and Associated Press (APW) sections of the Gigaword Corpus", "start_pos": 11, "end_pos": 90, "type": "DATASET", "confidence": 0.6987357262302848}, {"text": "British National Corpus (BNC)", "start_pos": 107, "end_pos": 136, "type": "DATASET", "confidence": 0.9686886171499888}]}, {"text": "Parsing and SRL evaluations often focus on newspaper articles and Gigaword is large enough to facilitate analysis over varying amounts of training data.", "labels": [], "entities": [{"text": "SRL evaluations", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.7482721209526062}, {"text": "Gigaword", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.6383782029151917}]}, {"text": "We parsed the data with the Stanford Parser 1 into dependency graphs.", "labels": [], "entities": [{"text": "Stanford Parser 1", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.9429110487302145}]}, {"text": "Let (v d , n) be a verb v with grammatical dependency d \u2208 {subject, object, prep} filled by noun n.", "labels": [], "entities": []}, {"text": "Pairs (v d , n) are chosen by extracting every such dependency in the graphs, setting the head predicate as v and the headword of the dependent d as n.", "labels": [], "entities": []}, {"text": "All prepositions are condensed into prep.", "labels": [], "entities": []}, {"text": "We randomly selected documents from the year 2001 in the NYT portion of the corpus as development and test sets.", "labels": [], "entities": [{"text": "NYT portion of the corpus", "start_pos": 57, "end_pos": 82, "type": "DATASET", "confidence": 0.9078743577003479}]}, {"text": "Training data for APW and NYT include all years 1994-2006 (minus NYT development and test documents).", "labels": [], "entities": [{"text": "APW", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.8738431930541992}, {"text": "NYT", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.7202747464179993}, {"text": "NYT development and test documents", "start_pos": 65, "end_pos": 99, "type": "DATASET", "confidence": 0.7099403858184814}]}, {"text": "We also identified and removed duplicate documents 2 . The BNC in its entirety is also used for training as a single data point.", "labels": [], "entities": [{"text": "BNC", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.92906653881073}]}, {"text": "We then record every seen (v d , n) pair during training that is seen two or more times and then count the number of unseen pairs in the NYT development set (1455 tests).", "labels": [], "entities": [{"text": "NYT development set", "start_pos": 137, "end_pos": 156, "type": "DATASET", "confidence": 0.8766408165295919}]}, {"text": "plots the percentage of unseen arguments against training size when trained on either NYT or APW (the APW portion is smaller in total size, and the smaller BNC is provided for comparison).", "labels": [], "entities": [{"text": "NYT", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.8545987606048584}, {"text": "APW", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.6206865906715393}, {"text": "BNC", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.9770620465278625}]}, {"text": "The first point on each line (the highest points) contains approximately the same number of words as the BNC (100 million).", "labels": [], "entities": [{"text": "BNC", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.9202960729598999}]}, {"text": "Initially, about one third of the arguments are unseen, but that percentage quickly falls close to 10% as additional training is included.", "labels": [], "entities": []}, {"text": "This suggests that an evaluation focusing only on unseen data is not representative, potentially missing up to 90% of the data.", "labels": [], "entities": []}, {"text": "1 http://nlp.stanford.edu/software/lex-parser.shtml 2 Any two documents whose first two paragraphs in the corpus files are identical.", "labels": [], "entities": []}, {"text": "Our results are thus conservative, as including all single occurrences would achieve even smaller unseen percentages.", "labels": [], "entities": []}, {"text": "The third line across the bottom of the figure is the number of unseen pairs using Google n-gram data as proxy argument counts.", "labels": [], "entities": []}, {"text": "Creating argument counts from n-gram counts is described in detail below in section 5.2.", "labels": [], "entities": []}, {"text": "We include these Web counts to illustrate how an openly available source of counts affects unseen arguments.", "labels": [], "entities": []}, {"text": "Finally, figure 2 compares which dependency types are seen the least in training.", "labels": [], "entities": []}, {"text": "Prepositions have the largest unseen percentage, but not surprisingly, also makeup less of the training examples overall.", "labels": [], "entities": []}, {"text": "In order to analyze why pairs are unseen, we analyzed the distribution of rare words across unseen and seen examples.", "labels": [], "entities": []}, {"text": "To define rare nouns, we order head words by their individual corpus frequencies.", "labels": [], "entities": []}, {"text": "A noun is rare if it occurs in the lowest 10% of the list.", "labels": [], "entities": []}, {"text": "We similarly define rare verbs over their ordered frequencies (we count verb lemmas, and do not include the syntactic relations).", "labels": [], "entities": []}, {"text": "Corpus counts covered 2 years of the AP section, and we used the development set of the NYT section to extract the seen and unseen pairs.", "labels": [], "entities": [{"text": "AP section", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.8150156140327454}, {"text": "NYT section", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.9166924953460693}]}, {"text": "shows the percentage of rare nouns and verbs that occur in unseen and seen pairs.", "labels": [], "entities": []}, {"text": "24.6% of the verbs in unseen pairs are rare, compared to only 4.5% in seen pairs.", "labels": [], "entities": []}, {"text": "The distribution of rare nouns is less contrastive: 13.3% vs 8.9%.", "labels": [], "entities": []}, {"text": "This suggests that many unseen pairs are unseen mainly because they contain low-frequency verbs, rather than because of containing low-frequency argument heads.", "labels": [], "entities": []}, {"text": "Given the large amount of seen data, we believe evaluations should include all data examples to best represent the corpus.", "labels": [], "entities": []}, {"text": "We describe our full evaluation results and include a comparison of different training sizes below.", "labels": [], "entities": []}, {"text": "Our training data is the NYT section of the Gigaword Corpus, parsed into dependency graphs.", "labels": [], "entities": [{"text": "NYT section of the Gigaword Corpus", "start_pos": 25, "end_pos": 59, "type": "DATASET", "confidence": 0.8022800286610922}]}, {"text": "We extract all (v d , n) pairs from the graph, as described in section 3.", "labels": [], "entities": []}, {"text": "We randomly chose 9 documents from the year 2001 fora development set, and 41 documents for testing.", "labels": [], "entities": []}, {"text": "The test set consisted of 6767 (v d , n) pairs.", "labels": [], "entities": []}, {"text": "All verbs and nouns are stemmed, and the development and test documents were isolated from training.", "labels": [], "entities": []}], "tableCaptions": []}