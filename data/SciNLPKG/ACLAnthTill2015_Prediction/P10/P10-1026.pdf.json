{"title": [{"text": "A Bayesian Method for Robust Estimation of Distributional Similarities", "labels": [], "entities": [{"text": "Robust Estimation of Distributional Similarities", "start_pos": 22, "end_pos": 70, "type": "TASK", "confidence": 0.8821503162384033}]}], "abstractContent": [{"text": "Existing word similarity measures are not robust to data sparseness since they rely only on the point estimation of words' context profiles obtained from a limited amount of data.", "labels": [], "entities": []}, {"text": "This paper proposes a Bayesian method for robust distributional word similarities.", "labels": [], "entities": []}, {"text": "The method uses a distribution of context profiles obtained by Bayesian estimation and takes the expectation of abase similarity measure under that distribution.", "labels": [], "entities": []}, {"text": "When the context profiles are multinomial distributions, the priors are Dirichlet, and the base measure is the Bhattacharyya coefficient, we can derive an analytical form that allows efficient calculation.", "labels": [], "entities": [{"text": "Bhattacharyya coefficient", "start_pos": 111, "end_pos": 136, "type": "METRIC", "confidence": 0.9570329785346985}]}, {"text": "For the task of word similarity estimation using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures.", "labels": [], "entities": [{"text": "word similarity estimation", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.8179592490196228}, {"text": "accuracies", "start_pos": 136, "end_pos": 146, "type": "METRIC", "confidence": 0.9691123962402344}]}], "introductionContent": [{"text": "The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the field.", "labels": [], "entities": [{"text": "The semantic similarity of words", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7503377199172974}, {"text": "computational linguistics", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.7228142768144608}]}, {"text": "Many researchers have conducted studies based on the distributional hypothesis, which states that words that occur in the same contexts tend to have similar meanings.", "labels": [], "entities": []}, {"text": "A number of semantic similarity measures have been proposed based on this hypothesis).", "labels": [], "entities": []}, {"text": "* The work was done while the author was at NICT.", "labels": [], "entities": [{"text": "NICT", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.9539902806282043}]}, {"text": "In general, most semantic similarity measures have the following form: sim(w 1 , w 2 ) = g(v(w 1 ), v(w 2 )), where v(w i ) is a vector that represents the contexts in which w i appears, which we calla context profile of w i . The function g is a function on these context profiles that is expected to produce good similarities.", "labels": [], "entities": []}, {"text": "Each dimension of the vector corresponds to a context, f k , which is typically a neighboring word or a word having dependency relations with w i in a corpus.", "labels": [], "entities": []}, {"text": "Its value, v k (w i ), is typically a co-occurrence frequency c(w i , f k ), a conditional probability p(f k |w i ), or point-wise mutual information (PMI) between w i and f k , which are all calculated from a corpus.", "labels": [], "entities": []}, {"text": "For g, various works have used the cosine, the Jaccard coefficient, or the Jensen-Shannon divergence is utilized, to name only a few measures.", "labels": [], "entities": []}, {"text": "Previous studies have focused on how to devise good contexts and a good function g for semantic similarities.", "labels": [], "entities": []}, {"text": "On the other hand, our approach in this paper is to estimate context profiles (v(w i )) robustly and thus to estimate the similarity robustly.", "labels": [], "entities": []}, {"text": "The problem here is that v(w i ) is computed from a corpus of limited size, and thus inevitably contains uncertainty and sparseness.", "labels": [], "entities": []}, {"text": "The guiding intuition behind our method is as follows.", "labels": [], "entities": []}, {"text": "All other things being equal, the similarity with a more frequent word should be larger, since it would be more reliable.", "labels": [], "entities": [{"text": "similarity", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9678817391395569}]}, {"text": "For example, if p(f k |w 1 ) and p(f k |w 2 ) for two given words w 1 and w 2 are equal, but w 1 is more frequent, we would expect that sim(w 0 , w 1 ) > sim(w 0 , w 2 ).", "labels": [], "entities": []}, {"text": "In the NLP field, data sparseness has been recognized as a serious problem and tackled in the context of language modeling and supervised machine learning.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.7220239341259003}]}, {"text": "However, to our knowledge, there has been no study that seriously dealt with data sparseness in the context of semantic similarity calculation.", "labels": [], "entities": [{"text": "semantic similarity calculation", "start_pos": 111, "end_pos": 142, "type": "TASK", "confidence": 0.7813290357589722}]}, {"text": "The data sparseness problem is usually solved by smoothing, regularization, margin maximization and soon.", "labels": [], "entities": []}, {"text": "Recently, the Bayesian approach has emerged and achieved promising results with a clearer formulation.", "labels": [], "entities": []}, {"text": "In this paper, we apply the Bayesian framework to the calculation of distributional similarity.", "labels": [], "entities": []}, {"text": "The method is straightforward: Instead of using the point estimation of v(w i ), we first estimate the distribution of the context profile, p(v(w i )), by Bayesian estimation and then take the expectation of the original similarity under this distribution as follows: The uncertainty due to data sparseness is represented by p(v(w i )), and taking the expectation enables us to take this into account.", "labels": [], "entities": []}, {"text": "The Bayesian estimation usually gives diverging distributions for infrequent observations and thus decreases the expectation value as expected.", "labels": [], "entities": []}, {"text": "The Bayesian estimation and the expectation calculation in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 59, "end_pos": 61, "type": "DATASET", "confidence": 0.9180469512939453}]}, {"text": "2 are generally difficult and usually require computationally expensive procedures.", "labels": [], "entities": []}, {"text": "Since our motivation for this research is to calculate good semantic similarities fora large set of words (e.g., one million nouns) and apply them to a wide range of NLP tasks, such costs must be minimized.", "labels": [], "entities": []}, {"text": "Our technical contribution in this paper is to show that in the case where the context profiles are multinomial distributions, the priors are Dirichlet, and the base similarity measure is the Bhattacharyya coefficient, we can derive an analytical form for Eq.", "labels": [], "entities": []}, {"text": "2, that enables efficient calculation (with some implementation tricks).", "labels": [], "entities": [{"text": "calculation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9571133852005005}]}, {"text": "In experiments, we estimate semantic similarities using a large amount of Web data in Japanese and show that the proposed measure gives better word similarities than a non-Bayesian Bhattacharyya coefficient or other well-known similarity measures such as Jensen-Shannon divergence and the cosine with PMI weights.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly introduce the Bayesian estimation and the Bhattacharyya coefficient.", "labels": [], "entities": [{"text": "estimation", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.5683844089508057}, {"text": "Bhattacharyya coefficient", "start_pos": 67, "end_pos": 92, "type": "METRIC", "confidence": 0.9624165892601013}]}, {"text": "Section 3 proposes our new Bayesian Bhattacharyya coefficient for robust similarity calculation.", "labels": [], "entities": [{"text": "Bayesian Bhattacharyya coefficient", "start_pos": 27, "end_pos": 61, "type": "METRIC", "confidence": 0.7414710323015848}, {"text": "robust similarity calculation", "start_pos": 66, "end_pos": 95, "type": "TASK", "confidence": 0.6160456041495005}]}, {"text": "Section 4 mentions some implementation issues and the solutions.", "labels": [], "entities": []}, {"text": "Then, Section 5 reports the experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our method in the calculation of similarities between nouns in Japanese.", "labels": [], "entities": [{"text": "calculation of similarities between nouns in Japanese", "start_pos": 31, "end_pos": 84, "type": "TASK", "confidence": 0.7772992338453021}]}, {"text": "Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as.", "labels": [], "entities": []}, {"text": "Given a word set, which is expected to contain similar words, we assume that a good similarity measure should output, for each word in the set, the other words in the set as similar words.", "labels": [], "entities": []}, {"text": "For given word sets, we can construct input-andanswers pairs, where the answers for each word are the other words in the set the word appears in.", "labels": [], "entities": []}, {"text": "We output a ranked list of 500 similar words for each word using a given similarity measure and checked whether they are included in the answers.", "labels": [], "entities": []}, {"text": "This setting could be seen as document retrieval, and we can use an evaluation measure such as the mean of the precision at top T (MP@T ) or the mean average precision (MAP).", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7619006633758545}, {"text": "precision at top T (MP@T )", "start_pos": 111, "end_pos": 137, "type": "METRIC", "confidence": 0.8217624492115445}, {"text": "mean average precision (MAP)", "start_pos": 145, "end_pos": 173, "type": "METRIC", "confidence": 0.9039440850416819}]}, {"text": "For each input word, P@T (precision at top T ) and AP (average precision) are defined as follows.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9526215195655823}, {"text": "AP (average precision)", "start_pos": 51, "end_pos": 73, "type": "METRIC", "confidence": 0.8247398495674133}]}, {"text": "\u03b4(w i \u2208 ans) returns 1 if the output word w i is in the answers, and 0 otherwise.", "labels": [], "entities": []}, {"text": "N is the number of outputs and R is the number of the answers.", "labels": [], "entities": []}, {"text": "MP@T and MAP are the averages of these values overall input words.", "labels": [], "entities": [{"text": "MAP", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9589877128601074}]}], "tableCaptions": [{"text": " Table 1: Performance on siblings (Set A).", "labels": [], "entities": []}, {"text": " Table 2: Performance on siblings (Set B).", "labels": [], "entities": []}, {"text": " Table 3: Performance on closed-sets (Set C).", "labels": [], "entities": []}, {"text": " Table 4: The numbers of improved, unchanged,  and degraded words in terms of MP@20 for each  evaluation set.", "labels": [], "entities": []}, {"text": " Table 5: Statistics on IDs. (A): Avg. ID of an- swers. (B): Avg. ID of system outputs. (C): Avg.  ID of correct system outputs.", "labels": [], "entities": [{"text": "Avg", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9782301187515259}, {"text": "Avg", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9510136246681213}]}]}