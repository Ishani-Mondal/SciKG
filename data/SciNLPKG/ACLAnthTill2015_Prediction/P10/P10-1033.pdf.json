{"title": [{"text": "Discriminative Pruning for Discriminative ITG Alignment", "labels": [], "entities": [{"text": "ITG Alignment", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.7929123342037201}]}], "abstractContent": [{"text": "While Inversion Transduction Grammar (ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed.", "labels": [], "entities": [{"text": "Inversion Transduction Grammar (ITG)", "start_pos": 6, "end_pos": 42, "type": "TASK", "confidence": 0.8497291704018911}, {"text": "speed", "start_pos": 141, "end_pos": 146, "type": "METRIC", "confidence": 0.9837122559547424}]}, {"text": "We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment.", "labels": [], "entities": [{"text": "Minimum Error Rate Training", "start_pos": 56, "end_pos": 83, "type": "METRIC", "confidence": 0.6735028028488159}, {"text": "ITG alignment", "start_pos": 127, "end_pos": 140, "type": "TASK", "confidence": 0.7146990597248077}]}, {"text": "Experiment results show that it is superior to all existing heuristics in ITG pruning.", "labels": [], "entities": []}, {"text": "On top of the pruning framework , we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++.", "labels": [], "entities": [{"text": "ITG alignment", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.6271831542253494}, {"text": "F-score", "start_pos": 140, "end_pos": 147, "type": "METRIC", "confidence": 0.9785067439079285}, {"text": "Bleu score", "start_pos": 152, "end_pos": 162, "type": "METRIC", "confidence": 0.9832362532615662}]}], "introductionContent": [{"text": "Inversion transduction grammar (ITG) ( is an adaptation of SCFG to bilingual parsing.", "labels": [], "entities": [{"text": "Inversion transduction grammar (ITG)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8107423881689707}, {"text": "bilingual parsing", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.6752737462520599}]}, {"text": "It does synchronous parsing of two languages with phrasal and word-level alignment as by-product.", "labels": [], "entities": []}, {"text": "For this reason ITG has gained more and more attention recently in the word alignment community (.", "labels": [], "entities": [{"text": "word alignment community", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.794542670249939}]}, {"text": "A major obstacle in ITG alignment is speed.", "labels": [], "entities": [{"text": "ITG alignment", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.9134175479412079}]}, {"text": "The original (unsupervised) ITG algorithm has complexity of O(n 6 ).", "labels": [], "entities": [{"text": "O", "start_pos": 60, "end_pos": 61, "type": "METRIC", "confidence": 0.9813801646232605}]}, {"text": "When extended to supervised/discriminative framework, ITG runs even more slowly.", "labels": [], "entities": []}, {"text": "Therefore all attempts to ITG alignment come with some pruning method.", "labels": [], "entities": [{"text": "ITG alignment", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.892268717288971}]}, {"text": "For example, do pruning based on the probabilities of links from a simpler alignment model (viz.", "labels": [], "entities": []}, {"text": "HMM); propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans.", "labels": [], "entities": [{"text": "HMM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8867151141166687}, {"text": "Tic-tac-toe pruning", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7456327378749847}]}, {"text": "As all the principles behind these techniques have certain contribution in making good pruning decision, it is tempting to incorporate all these features in ITG pruning.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel discriminative pruning framework for discriminative ITG.", "labels": [], "entities": []}, {"text": "The pruning model uses no more training data than the discriminative ITG parser itself, and it uses a log-linear model to integrate all features that help identify the correct span pair (like Model 1 probability and HMM posterior).", "labels": [], "entities": []}, {"text": "On top of the discriminative pruning method, we also propose a discriminative ITG alignment system using hierarchical phrase pairs.", "labels": [], "entities": [{"text": "ITG alignment", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.7682858109474182}]}, {"text": "In the following, some basic details on the ITG formalism and ITG parsing are first reviewed (Sections 2 and 3), followed by the definition of pruning in ITG (Section 4).", "labels": [], "entities": [{"text": "ITG parsing", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.7267930507659912}]}, {"text": "The \"Discriminative Pruning for Discriminative ITG\" model (DPDI) and our discriminative ITG (DITG) parsers will be elaborated in Sections 5 and 6 respectively.", "labels": [], "entities": []}, {"text": "The merits of DPDI and DITG are illustrated with the experiments described in Section 7.", "labels": [], "entities": [{"text": "DPDI", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.7411856055259705}, {"text": "DITG", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.6205610632896423}]}], "datasetContent": [{"text": "DPDI is evaluated against the baselines of Tictac-toe (TTT) pruning (Zhang and and Dynamic Program (DP) pruning) with respect to Chinese-to-English alignment and translation.", "labels": [], "entities": [{"text": "DPDI", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.4871291220188141}, {"text": "Tictac-toe", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.8796870112419128}]}, {"text": "Based on DPDI, HP-DITG is evaluated against the alignment systems GIZA++ and BITG.", "labels": [], "entities": [{"text": "DPDI", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.7777819633483887}, {"text": "GIZA++", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.8270859718322754}, {"text": "BITG", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9772663116455078}]}, {"text": "Four evaluation criteria are used in addition to the time spent on ITG parsing.", "labels": [], "entities": [{"text": "ITG parsing", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.7422829866409302}]}, {"text": "We will first evaluate pruning regarding the pruning decisions themselves.", "labels": [], "entities": []}, {"text": "That is, the first evaluation metric, pruning error rate (henceforth PER), measures how many correct E-spans are discarded.", "labels": [], "entities": [{"text": "pruning error rate (henceforth PER)", "start_pos": 38, "end_pos": 73, "type": "METRIC", "confidence": 0.7904646566935948}]}, {"text": "The major drawback of PER is that not all decisions in pruning would impact on alignment quality, since certain F-spans are of little use to the entire ITG parse tree.", "labels": [], "entities": [{"text": "PER", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.5732197165489197}, {"text": "ITG parse tree", "start_pos": 152, "end_pos": 166, "type": "DATASET", "confidence": 0.704669862985611}]}, {"text": "An alternative criterion is the upper bound on alignment F-score, which essentially measures how many links in annotated alignment can be kept in ITG parse.", "labels": [], "entities": [{"text": "F-score", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.639682948589325}]}, {"text": "The calculation of F-score upper bound is done in a bottom-up way like ITG parsing.", "labels": [], "entities": [{"text": "F-score upper bound", "start_pos": 19, "end_pos": 38, "type": "METRIC", "confidence": 0.9649361371994019}, {"text": "ITG parsing", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.8371977806091309}]}, {"text": "All leaf hypernodes which contain a correct link are assigned a score (known as hit) of 1.", "labels": [], "entities": []}, {"text": "The hit of a non-leaf hypernode is based on the sum of hits of its daughter hypernodes.", "labels": [], "entities": []}, {"text": "The maximal sum among all hyperedges of a hypernode is assigned to that hypernode.", "labels": [], "entities": []}, {"text": "Formally, where \u00ed \u00b5\u00ed\u00b1\u008b, \u00ed \u00b5\u00ed\u00b1\u008c, \u00ed \u00b5\u00ed\u00b1\u008d are variables for the categories in ITG grammar, and \u00ed \u00b5\u00ed\u00b1 comprises the golden links in annotated alignment.", "labels": [], "entities": [{"text": "ITG grammar", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.8137361109256744}]}, {"text": "\u00ed \u00b5\u00ed\u00b0 \u00b6 \u00ed \u00b5\u00ed\u00b1\u00a4 , \u00ed \u00b5\u00ed\u00b0 \u00b6 \u00ed \u00b5\u00ed\u00b1\u0092 , \u00ed \u00b5\u00ed\u00b0 \u00b6 \u00ed \u00b5\u00ed\u00b1\u0093 are defined in Appendix A. illustrates the calculation of the hit score for the example in Section 5.1/ 53\"\"/6\"55\"\" 5.2% 88.6% 82.4% 4 DP --11\"\"/6\"01\"\" 12.1% 86.1% 80.5%: Evaluation of DPDI against TTT (Tic-tac-toe) and DP (Dynamic Program) for HP-DITG.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.8885427117347717}, {"text": "DPDI", "start_pos": 234, "end_pos": 238, "type": "METRIC", "confidence": 0.8007490634918213}]}, {"text": "bound of precision, which should be defined as the hit score divided by the number of links produced by the system, is almost always 1.0 in practice.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9886401891708374}, {"text": "hit score divided", "start_pos": 51, "end_pos": 68, "type": "METRIC", "confidence": 0.8986392021179199}]}, {"text": "The upper bound of alignment F-score can thus be calculated as well.", "labels": [], "entities": [{"text": "alignment", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9865228533744812}, {"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.7498120069503784}]}, {"text": "Both discriminative pruning and alignment need training data and test data.", "labels": [], "entities": [{"text": "alignment", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.9596146941184998}]}, {"text": "We use the manually aligned Chinese-English dataset as used in.", "labels": [], "entities": [{"text": "Chinese-English dataset", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.7075792700052261}]}, {"text": "The 491 sentence pairs in this dataset are adapted to our own Chinese word segmentation standard.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.623053640127182}]}, {"text": "250 sentence pairs are used as training data and the other 241 are test data.", "labels": [], "entities": []}, {"text": "The corresponding numbers of F-spans in training and test data are 4590 and 3951 respectively.", "labels": [], "entities": [{"text": "F-spans", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9974168539047241}]}, {"text": "In SMT experiments, the bilingual training dataset is the NIST training set excluding the Hong Kong Law and Hong Kong Hansard, and our 5-gram language model is trained from the Xinhua section of the Gigaword corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9951765537261963}, {"text": "NIST training set", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.9642409086227417}, {"text": "Hong Kong Law and Hong Kong Hansard", "start_pos": 90, "end_pos": 125, "type": "DATASET", "confidence": 0.917791519846235}, {"text": "Gigaword corpus", "start_pos": 199, "end_pos": 214, "type": "DATASET", "confidence": 0.8931643664836884}]}, {"text": "The NIST\"03 test set is used as our development corpus and the NIST\"05 and NIST\"08 test sets are our test sets.", "labels": [], "entities": [{"text": "NIST\"03 test set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9558158397674561}, {"text": "NIST\"05", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9467374881108602}, {"text": "NIST\"08 test sets", "start_pos": 75, "end_pos": 92, "type": "DATASET", "confidence": 0.9179276108741761}]}, {"text": "The first set of experiments evaluates the performance of the three pruning methods using the small 241-sentence set.", "labels": [], "entities": []}, {"text": "Each pruning method is plugged in both W-DITG and HP-DITG.", "labels": [], "entities": []}, {"text": "IBM Model 1 and HMM alignment model are reimplemented as they are required by the three ITG pruning methods.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9099833567937216}, {"text": "HMM alignment", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.7500060200691223}]}, {"text": "The results for W-DITG are listed in.", "labels": [], "entities": [{"text": "W-DITG", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.6684518456459045}]}, {"text": "Tests 1 and 2 show that with the same beam size (i.e. number of E-spans per F-span), although DPDI spends a bit more time (due to the more complicated model), DPDI makes far less incorrect pruning decisions than the TTT.", "labels": [], "entities": [{"text": "TTT", "start_pos": 216, "end_pos": 219, "type": "DATASET", "confidence": 0.725100576877594}]}, {"text": "In terms of F-score upper bound, DPDI is 1 percent higher.", "labels": [], "entities": [{"text": "F-score", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9989150762557983}, {"text": "DPDI", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9975019097328186}]}, {"text": "DPDI achieves even larger improvement in actual F-score.", "labels": [], "entities": [{"text": "DPDI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5766359567642212}, {"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9869300127029419}]}, {"text": "To enable TTT achieving similar F-score or Fscore upper bound, the beam size has to be doubled and the time cost is more than twice the original (c.f.", "labels": [], "entities": [{"text": "TTT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9196526408195496}, {"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9975581169128418}, {"text": "Fscore upper bound", "start_pos": 43, "end_pos": 61, "type": "METRIC", "confidence": 0.976802388827006}]}, {"text": "Tests 1 and 3 in) . The DP pruning in performs much poorer than the other two pruning methods.", "labels": [], "entities": []}, {"text": "In fact, we fail to enable DP achieve the same F-score upper bound as the other two methods before DP leads to intolerable memory consumption.", "labels": [], "entities": [{"text": "F-score upper bound", "start_pos": 47, "end_pos": 66, "type": "METRIC", "confidence": 0.9720441699028015}]}, {"text": "This maybe due to the use of different HMM model implementations between our.", "labels": [], "entities": []}, {"text": "lists the results for HP-DITG.", "labels": [], "entities": [{"text": "HP-DITG", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.9325516819953918}]}, {"text": "Roughly the same observation as in W-DITG can be made.", "labels": [], "entities": []}, {"text": "In addition to the superiority of DPDI, it can also be noted that HP-DITG achieves much higher Fscore and F-score upper bound.", "labels": [], "entities": [{"text": "DPDI", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.8717801570892334}, {"text": "Fscore", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9995211362838745}, {"text": "F-score", "start_pos": 106, "end_pos": 113, "type": "METRIC", "confidence": 0.9974402189254761}]}, {"text": "This shows that hierarchical phrase is a powerful tool in rectifying the 1-to-1 constraint in ITG.", "labels": [], "entities": []}, {"text": "Note also that while TTT in Test 3 gets roughly the same F-score upper bound as DPDI in Test 1, the corresponding F-score is slightly worse.", "labels": [], "entities": [{"text": "TTT", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.5014036297798157}, {"text": "F-score upper bound", "start_pos": 57, "end_pos": 76, "type": "METRIC", "confidence": 0.9781390428543091}, {"text": "DPDI", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.4848781228065491}, {"text": "F-score", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.9983306527137756}]}, {"text": "A possible explanation is that better pruning not only speeds up the parsing/alignment process but also guides the search process to focus on the most promising region of the search space.", "labels": [], "entities": [{"text": "parsing/alignment", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.866158922513326}]}, {"text": "lists the word alignment time cost and SMT performance of different pruning methods.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7208545655012131}, {"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9665910601615906}]}, {"text": "HP-DITG using DPDI achieves the best Bleu score with acceptable time cost.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9477548599243164}]}, {"text": "compares HP-DITG to HMM, GIZA++) and BITG.", "labels": [], "entities": [{"text": "BITG", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9948070645332336}]}, {"text": "It shows that HP-DITG (with DPDI) is better than the three baselines both in alignment F-score and Bleu score.", "labels": [], "entities": [{"text": "DPDI", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.8942248821258545}, {"text": "alignment", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9160398840904236}, {"text": "F-score", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.5223394632339478}, {"text": "Bleu score", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9597051739692688}]}, {"text": "Note that the Bleu score differences between HP-DITG and the three baselines are statistically significant).", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9750064015388489}, {"text": "HP-DITG", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9406653642654419}]}, {"text": "An explanation of the better performance by HP-DITG is the better phrase pair extraction due to DPDI.", "labels": [], "entities": [{"text": "phrase pair extraction", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.7361337145169576}]}, {"text": "On the one hand, a good phrase pair often fails to be extracted due to a link inconsistent with the pair.", "labels": [], "entities": []}, {"text": "On the other hand, ITG pruning can be considered as phrase pair selection, and good ITG pruning like DPDI guides the subsequent ITG alignment process so that less links inconsistent to good phrase pairs are produced.", "labels": [], "entities": [{"text": "phrase pair selection", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.7453935543696085}, {"text": "ITG alignment", "start_pos": 128, "end_pos": 141, "type": "TASK", "confidence": 0.6585764288902283}]}, {"text": "This also explains (in) why DPDI with beam size 10 leads to higher Bleu than TTT with beam size 20, even though both pruning methods lead to roughly the same alignment F-score.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9992431402206421}, {"text": "alignment F-score", "start_pos": 158, "end_pos": 175, "type": "METRIC", "confidence": 0.8085139691829681}]}], "tableCaptions": [{"text": " Table 1: Evaluation of DPDI against TTT (Tic-tac-toe) and DP (Dynamic Program) for W-DITG", "labels": [], "entities": [{"text": "DPDI", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.8594472408294678}, {"text": "TTT", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.836900532245636}, {"text": "W-DITG", "start_pos": 84, "end_pos": 90, "type": "TASK", "confidence": 0.46965292096138}]}, {"text": " Table 2: Evaluation of DPDI against TTT (Tic-tac-toe) and DP (Dynamic Program) for HP-DITG.", "labels": [], "entities": [{"text": "DPDI", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.749846875667572}, {"text": "TTT", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.8084304332733154}]}, {"text": " Table 3: Evaluation of DPDI against TTT and  DP for HP-DITG", "labels": [], "entities": []}, {"text": " Table 4: Evaluation of DPDI against HMM, Gi- za++ and BITG", "labels": [], "entities": [{"text": "BITG", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9900471568107605}]}]}