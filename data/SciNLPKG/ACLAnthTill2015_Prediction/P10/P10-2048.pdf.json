{"title": [{"text": "Cross Lingual Adaptation: An Experiment on Sentiment Classifications", "labels": [], "entities": [{"text": "Cross Lingual Adaptation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7485195199648539}, {"text": "Sentiment Classifications", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.941074013710022}]}], "abstractContent": [{"text": "In this paper, we study the problem of using an annotated corpus in English for the same natural language processing task in another language.", "labels": [], "entities": []}, {"text": "While various machine translation systems are available, automated translation is still far from perfect.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7348729074001312}, {"text": "automated translation", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.6928047239780426}]}, {"text": "To minimize the noise introduced by translations, we propose to use only key 'reliable\" parts from the translations and apply structural correspondence learning (SCL) to find a low dimensional representation shared by the two languages.", "labels": [], "entities": []}, {"text": "We perform experiments on an English-Chinese sentiment classification task and compare our results with a previous co-training approach.", "labels": [], "entities": [{"text": "English-Chinese sentiment classification task", "start_pos": 29, "end_pos": 74, "type": "TASK", "confidence": 0.7230508327484131}]}, {"text": "To alleviate the problem of data sparseness, we create extra pseudo-examples for SCL by making queries to a search engine.", "labels": [], "entities": [{"text": "SCL", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9498416185379028}]}, {"text": "Experiments on real-world on-line review data demonstrate the two techniques can effectively improve the performance compared to previous work.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we are interested in the problem of transferring knowledge gained from data gathered in one language to another language.", "labels": [], "entities": [{"text": "transferring knowledge gained from data gathered in one language", "start_pos": 50, "end_pos": 114, "type": "TASK", "confidence": 0.8302261630694071}]}, {"text": "A simple and straightforward solution for this problem might be to use automatic machine translations.", "labels": [], "entities": []}, {"text": "However, while machine translation has been the subject of a great deal of development in recent years, many of the recent gains in performance manifest as syntactically as opposed to semantically correct sentences.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7622320652008057}]}, {"text": "For example, \"PIANYI\" is a word mainly used in positive comments in Chinese but its translation from the online Google translator is always \"cheap\", a word typically used in a negative context in English.", "labels": [], "entities": []}, {"text": "To reduce this kind of error introduced by the translator, Wan in) applied a co-training scheme.", "labels": [], "entities": []}, {"text": "In this setting classifiers are trained in both languages and the two classifiers teach each other for the unlabeled examples.", "labels": [], "entities": []}, {"text": "The co-training approach manages to boost the performance as it allows the text similarity in the target language to compete with the \"fake\" similarity from the translated texts.", "labels": [], "entities": []}, {"text": "However, the translated texts are still used as training data and thus can potentially mislead the classifier.", "labels": [], "entities": []}, {"text": "As we are not really interested in predicting something on the language created by the translator, but rather on the real one, it maybe better to further diminish the role of the translated texts in the learning process.", "labels": [], "entities": []}, {"text": "Motivated by this observation, we suggest hereto view this problem as a special case of domain adaptation, in the source domain, we mainly observe English features, while in the other domain mostly features from Chinese.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7267723828554153}]}, {"text": "The problem we address is how to associate the features under a unified setting.", "labels": [], "entities": []}, {"text": "There has been a lot of work in domain adaption for NLP and one suitable choice for our problem is the approach based on structural correspondence learning (SCL) as in () and).", "labels": [], "entities": []}, {"text": "The key idea of SCL is to identify a low-dimensional representations that capture correspondence between features from both domains (x sand x tin our case) by modeling their correlations with some special pivot features.", "labels": [], "entities": [{"text": "SCL", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9715985655784607}]}, {"text": "The SCL approach is a good fit for our problem as it performs knowledge transfer through identifying important features.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7695191502571106}]}, {"text": "In the cross-lingual setting, we can restrict the translated texts by using them only through the pivot features.", "labels": [], "entities": []}, {"text": "We believe this form is more robust to errors in the language produced by the translator.", "labels": [], "entities": []}, {"text": "Adapting language resources and knowledge to anew language was first studied for general text categorization and information retrieval as in, where the authors translate a keyword lexicon to perform cross-lingual text categorization.", "labels": [], "entities": [{"text": "Adapting language", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9091213047504425}, {"text": "general text categorization", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.644977480173111}, {"text": "information retrieval", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.7613521218299866}, {"text": "cross-lingual text categorization", "start_pos": 199, "end_pos": 232, "type": "TASK", "confidence": 0.6126824220021566}]}, {"text": "In (, different shortcomings of lexicon-based translation scheme was discussed for the more semantic-oriented task subjective analysis, instead the authors proposed to use a parallel-corpus, apply the classifier in the source language and use the corresponding sentences in the target language to train anew classifier.", "labels": [], "entities": []}, {"text": "With the rapid development of automatic machine translations, translating the whole corpus becomes a plausible option.", "labels": [], "entities": [{"text": "automatic machine translations", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.7161572972933451}, {"text": "translating", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.9697843194007874}]}, {"text": "One can either choose to translate a corpus in the target language and apply the classifier in the source language to obtain labeled data, or directly translated the existing data set to the new language.", "labels": [], "entities": []}, {"text": "Various experiments of the first strategy are performed in () for the subjective analysis task and an average 65 F1 score was reported.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9860424697399139}]}, {"text": "In, the authors propose to combine both strategies with ensemble learning and train a bi-lingual classifier.", "labels": [], "entities": []}, {"text": "In this paper, we are also interested in exploring whether a search engine can be used to improve the performance of NLP systems through reducing the effect of data sparseness.", "labels": [], "entities": []}, {"text": "As the SCL algorithm we use here is based on co-occurrence statistics, we adopt a simple approach of creating pseudo-examples from the query counts returned by Google.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results on the Positive Reviews", "labels": [], "entities": []}, {"text": " Table 3: Results on the Negative Reviews", "labels": [], "entities": []}, {"text": " Table 4: Overall Accuracy of Different Methods", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9777894020080566}]}]}