{"title": [{"text": "A Statistical Model for Lost Language Decipherment", "labels": [], "entities": [{"text": "Lost Language Decipherment", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.6302188336849213}]}], "abstractContent": [{"text": "In this paper we propose a method for the automatic decipherment of lost languages.", "labels": [], "entities": [{"text": "automatic decipherment of lost languages", "start_pos": 42, "end_pos": 82, "type": "TASK", "confidence": 0.7320373892784119}]}, {"text": "Given a non-parallel corpus in a known related language, our model produces both alphabetic mappings and translations of words into their corresponding cognates.", "labels": [], "entities": []}, {"text": "We employ a non-parametric Bayesian framework to simultaneously capture both low-level character mappings and high-level morphemic correspondences.", "labels": [], "entities": []}, {"text": "This formulation enables us to encode some of the linguistic intuitions that have guided human decipherers.", "labels": [], "entities": []}, {"text": "When applied to the ancient Semitic language Ugaritic, the model correctly maps 29 of 30 letters to their Hebrew counterparts, and deduces the correct Hebrew cognate for 60% of the Ugaritic words which have cognates in Hebrew.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dozens of lost languages have been deciphered by humans in the last two centuries.", "labels": [], "entities": []}, {"text": "In each case, the decipherment has been considered a major intellectual breakthrough, often the culmination of decades of scholarly efforts.", "labels": [], "entities": []}, {"text": "Computers have played no role in the decipherment any of these languages.", "labels": [], "entities": []}, {"text": "In fact, skeptics argue that computers do not possess the \"logic and intuition\" required to unravel the mysteries of ancient scripts.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that at least some of this logic and intuition can be successfully modeled, allowing computational tools to be used in the decipherment process.", "labels": [], "entities": []}, {"text": "Our definition of the computational decipherment task closely follows the setup typically faced by human decipherers).", "labels": [], "entities": []}, {"text": "Our input consists of texts in a lost language and a corpus of non-parallel data in a known related language.", "labels": [], "entities": []}, {"text": "The decipherment itself involves two related subtasks: (i) finding the mapping between alphabets of the known and lost languages, and (ii) translating words in the lost language into corresponding cognates of the known language.", "labels": [], "entities": []}, {"text": "While there is no single formula that human decipherers have employed, manual efforts have focused on several guiding principles.", "labels": [], "entities": []}, {"text": "A common starting point is to compare letter and word frequencies between the lost and known languages.", "labels": [], "entities": []}, {"text": "In the presence of cognates the correct mapping between the languages will reveal similarities in frequency, both at the character and lexical level.", "labels": [], "entities": []}, {"text": "In addition, morphological analysis plays a crucial role here, as highly frequent morpheme correspondences can be particularly revealing.", "labels": [], "entities": []}, {"text": "In fact, these three strands of analysis (character frequency, morphology, and lexical frequency) are intertwined throughout the human decipherment process.", "labels": [], "entities": []}, {"text": "Partial knowledge of each drives discovery in the others.", "labels": [], "entities": []}, {"text": "We capture these intuitions in a generative Bayesian model.", "labels": [], "entities": []}, {"text": "This model assumes that each word in the lost language is composed of morphemes which were generated with latent counterparts in the known language.", "labels": [], "entities": []}, {"text": "We model bilingual morpheme pairs as arising through a series of Dirichlet processes.", "labels": [], "entities": []}, {"text": "This allows us to assign probabilities based both on character-level correspondences (using a character-edit base distribution) as well as higher-level morpheme correspondences.", "labels": [], "entities": []}, {"text": "In addition, our model carries out an implicit morphological analysis of the lost language, utilizing the known morphological structure of the related language.", "labels": [], "entities": []}, {"text": "This model structure allows us to capture the interplay between the character-and morpheme-level correspondences that humans have used in the manual decipherment process.", "labels": [], "entities": []}, {"text": "In addition, we introduce a novel technique for imposing structural sparsity constraints on character-level mappings.", "labels": [], "entities": []}, {"text": "We assume that an accurate alphabetic mapping between related languages will be sparse in the following way: each letter will map to a very limited subset of letters in the other language.", "labels": [], "entities": []}, {"text": "We capture this intuition by adapting the so-called \"spike and slab\" prior to the Dirichlet-multinomial setting.", "labels": [], "entities": []}, {"text": "For each pair of characters in the two languages, we posit an indicator variable which controls the prior likelihood of character substitutions.", "labels": [], "entities": []}, {"text": "We define a joint prior over these indicator variables which encourages sparse settings.", "labels": [], "entities": []}, {"text": "We applied our model to a corpus of Ugaritic, an ancient Semitic language discovered in 1928.", "labels": [], "entities": []}, {"text": "Ugaritic was manually deciphered in 1932, using knowledge of Hebrew, a related language.", "labels": [], "entities": []}, {"text": "We compare our method against the only existing decipherment baseline, an HMM-based character substitution cipher).", "labels": [], "entities": []}, {"text": "The baseline correctly maps the majority of letters -22 out of 30 -to their correct Hebrew counterparts, but only correctly translates 29% of all cognates.", "labels": [], "entities": []}, {"text": "In comparison, our method yields correct mappings for 29 of 30 letters, and correctly translates 60.4% of all cognates.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on four separate decipherment tasks: (i) Learning alphabetic mappings, (ii) translating cognates, (iii) identifying cognates, and (iv) morphological segmentation.", "labels": [], "entities": [{"text": "translating cognates", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.906933456659317}, {"text": "morphological segmentation", "start_pos": 157, "end_pos": 183, "type": "TASK", "confidence": 0.709576204419136}]}, {"text": "As a baseline for the first three of these tasks (learning alphabetic mappings and translating and identifying cognates), we adapt the HMM-based method of for learning letter substitution ciphers.", "labels": [], "entities": [{"text": "translating and identifying cognates", "start_pos": 83, "end_pos": 119, "type": "TASK", "confidence": 0.8076744973659515}, {"text": "learning letter substitution ciphers", "start_pos": 159, "end_pos": 195, "type": "TASK", "confidence": 0.7115820348262787}]}, {"text": "In its original setting, this model was used to map written texts to spoken language, under the assumption that each character was emitted from a hidden phonemic state.", "labels": [], "entities": []}, {"text": "In our adaptation, we assume instead that each Ugaritic character was generated by a hidden Hebrew letter.", "labels": [], "entities": []}, {"text": "Hebrew character trigram transition probabilities are estimated using the Hebrew Bible, and Hebrew to Ugaritic character emission probabilities are learned using EM.", "labels": [], "entities": []}, {"text": "Finally, the highest prob-ability sequence of latent Hebrew letters is predicted for each Ugaritic word-form, using Viterbi decoding.", "labels": [], "entities": []}, {"text": "Alphabetic Mapping The first essential step towards successful decipherment is recovering the mapping between the symbols of the lost language and the alphabet of a known language.", "labels": [], "entities": [{"text": "Alphabetic Mapping", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8416866958141327}]}, {"text": "As a gold standard for this comparison, we use the wellestablished relationship between the Ugaritic and Hebrew alphabets.", "labels": [], "entities": []}, {"text": "This mapping is not one-to-one but is generally quite sparse.", "labels": [], "entities": []}, {"text": "Of the 30 Ugaritic symbols, 28 map predominantly to a single Hebrew letter, and the remaining two map to two different letters.", "labels": [], "entities": []}, {"text": "As the Hebrew alphabet contains only 22 letters, six map to two distinct Ugaritic letters and two map to three distinct Ugaritic letters.", "labels": [], "entities": []}, {"text": "We recover our model's predicted alphabetic mappings by simply examining the sampled values of the binary indicator variables \u03bb u,h for each Ugaritic-Hebrew letter pair (u, h).", "labels": [], "entities": []}, {"text": "Due to our structural sparsity prior P ( \u20d7 \u03bb), the predicted mappings are sparse: each Ugaritic letter maps to only a single Hebrew letter, and most Hebrew letters map to only a single Ugaritic letter.", "labels": [], "entities": []}, {"text": "To recover alphabetic mappings from the HMM substitution cipher baseline, we predict the Hebrew letter h which maximizes the model's probability P (h|u), for each Ugaritic letter u.", "labels": [], "entities": [{"text": "HMM substitution cipher baseline", "start_pos": 40, "end_pos": 72, "type": "DATASET", "confidence": 0.5820809081196785}]}, {"text": "To evaluate these mappings, we simply count the number of Ugaritic letters that are correctly mapped to one of their Hebrew reflexes.", "labels": [], "entities": []}, {"text": "By this measure, the baseline recovers correct mappings for 22 out of 30 Ugaritic characters (73.3%).", "labels": [], "entities": [{"text": "correct", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9716189503669739}]}, {"text": "Our model recovers correct mappings for all but one (very low frequency) Ugaritic characters, yielding 96.67% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9987320303916931}]}, {"text": "Cognate Decipherment We compare the decipherment accuracy for Ugaritic words that have corresponding Hebrew cognates.", "labels": [], "entities": []}, {"text": "We evaluate our model's predictions on each distinct Ugaritic word-form at both the type and token level.", "labels": [], "entities": []}, {"text": "As shows, our method correctly translates over 60% of all distinct Ugaritic word-forms with Hebrew cognates and over 71% of the individual morphemes that compose them, outperforming the baseline by significant margins.", "labels": [], "entities": []}, {"text": "Accuracy improves when the frequency of the wordforms is taken into account (token-level evaluation), indicating that the model is able to decipher frequent words more accurately than infre- quent words.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9927403330802917}]}, {"text": "We also measure the average Levenshtein distance between predicted and actual cognate word-forms.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 28, "end_pos": 48, "type": "METRIC", "confidence": 0.788787305355072}]}, {"text": "On average, our model's predictions lie 0.52 edit operations from the true cognate, whereas the baseline's predictions average a distance of 1.26 edit operations.", "labels": [], "entities": []}, {"text": "Finally, we evaluated the performance of our model when the structural sparsity constraints are not used.", "labels": [], "entities": []}, {"text": "As shows, performance degrades significantly in the absence of these priors, indicating the importance of modeling the sparsity of character mappings.", "labels": [], "entities": []}, {"text": "Cognate identification We evaluate our model's ability to identify cognates using the sampled indicator variables c i . As before, we compare our performance against the HMM substitution cipher baseline.", "labels": [], "entities": [{"text": "Cognate identification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7969320714473724}]}, {"text": "To produce baseline cognate identification predictions, we calculate the probability of each latent Hebrew letter sequence predicted by the HMM, and compare it to a uniform character-level Ugaritic language model (as done by our model, to avoid automatically assigning higher cognate probability to shorter Ugaritic words).", "labels": [], "entities": []}, {"text": "For both our model and the baseline, we can vary the threshold for cognate identification by raising or lowering the cognate prior P (c i ).", "labels": [], "entities": [{"text": "cognate identification", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.7176247537136078}]}, {"text": "As the prior is set higher, we detect more true cognates, but the false positive rate increases as well.", "labels": [], "entities": [{"text": "false positive rate", "start_pos": 66, "end_pos": 85, "type": "METRIC", "confidence": 0.7883005738258362}]}, {"text": "shows the ROC curve obtained by varying this prior both for our model and the baseline.", "labels": [], "entities": [{"text": "ROC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9872937798500061}]}, {"text": "At all operating points, our model outperforms the baseline, and both models always predict better than chance.", "labels": [], "entities": []}, {"text": "In practice for our model, we use a high cognate prior, thus only ruling out precision recall f-measure Morfessor 88.87% 67.48% 76.71% Our Model 86.62% 90.53% 88.53%: Morphological segmentation accuracy fora standard unsupervised baseline and our model.", "labels": [], "entities": [{"text": "precision recall f-measure Morfessor 88.87", "start_pos": 77, "end_pos": 119, "type": "METRIC", "confidence": 0.8258803725242615}, {"text": "Morphological segmentation", "start_pos": 167, "end_pos": 193, "type": "TASK", "confidence": 0.7563146948814392}, {"text": "accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.9112828373908997}]}, {"text": "those Ugaritic word-forms which are very unlikely to have Hebrew cognates.", "labels": [], "entities": []}, {"text": "Morphological segmentation Finally, we evaluate the accuracy of our model's morphological segmentation for Ugaritic words.", "labels": [], "entities": [{"text": "Morphological segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8424450755119324}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9994611144065857}]}, {"text": "As a baseline for this comparison, we use Morfessor Categories-MAP (.", "labels": [], "entities": []}, {"text": "As shows, our model provides a significant boost in performance, especially for recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9900015592575073}]}, {"text": "This result is consistent with previous work showing that morphological annotations can be projected to new languages lacking annotation (, but generalizes those results to the case where parallel data is unavailable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of cognate translations, mea- sured with respect to complete word-forms and  morphemes, for the HMM-based substitution ci- pher baseline, our complete model, and our model  without the structural sparsity priors. Note that the  baseline does not provide per-morpheme results,  as it does not predict morpheme boundaries.", "labels": [], "entities": [{"text": "mea- sured", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.8499336640040079}]}]}