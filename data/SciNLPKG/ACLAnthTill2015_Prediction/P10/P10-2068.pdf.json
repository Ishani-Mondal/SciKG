{"title": [{"text": "An Active Learning Approach to Finding Related Terms", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel system that helps non-experts find sets of similar words.", "labels": [], "entities": []}, {"text": "The user begins by specifying one or more seed words.", "labels": [], "entities": []}, {"text": "The system then iteratively suggests a series of candidate words, which the user can either accept or reject.", "labels": [], "entities": []}, {"text": "Current techniques for this task typically boot-strap a classifier based on a fixed seed set.", "labels": [], "entities": []}, {"text": "In contrast, our system involves the user throughout the labeling process, using active learning to intelligently explore the space of similar words.", "labels": [], "entities": [{"text": "labeling process", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.8868573009967804}]}, {"text": "In particular, our system can take advantage of negative examples provided by the user.", "labels": [], "entities": []}, {"text": "Our system combines multiple pre-existing sources of similarity data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (\"synonyms of crash,\" \"types of car,\" etc.).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9290679097175598}]}, {"text": "We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Set expansion is a well-studied NLP problem where a machine-learning algorithm is given a fixed set of seed words and asked to find additional members of the implied set.", "labels": [], "entities": [{"text": "Set expansion", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7795875668525696}]}, {"text": "For example, given the seed set {\"elephant,\" \"horse,\" \"bat\"}, the algorithm is expected to return other mammals.", "labels": [], "entities": []}, {"text": "Past work, e.g., generally focuses on semi-automatic acquisition of the remaining members of the set by mining large amounts of unlabeled data.", "labels": [], "entities": []}, {"text": "State-of-the-art set expansion systems work well for well-defined sets of nouns, e.g. \"US Presidents,\" particularly when given a large seed set.", "labels": [], "entities": []}, {"text": "Set expansions is more difficult with fewer seed words and for other kinds of sets.", "labels": [], "entities": [{"text": "Set expansions", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7543766498565674}]}, {"text": "The seed words may have multiple senses and the user may have in mind a variety of attributes that the answer must match.", "labels": [], "entities": []}, {"text": "For example, suppose the seed word is \"jaguar\".", "labels": [], "entities": []}, {"text": "First, there is sense ambiguity; we could be referring to either a \"large cat\" or a \"car.\"", "labels": [], "entities": []}, {"text": "Beyond this, we might have in mind various more (or less) specific groups: \"Mexican animals,\" \"predators,\" \"luxury cars,\" \"British cars,\" etc.", "labels": [], "entities": []}, {"text": "We propose a system which addresses several shortcomings of many set expansion systems.", "labels": [], "entities": []}, {"text": "First, these systems can be difficult to use.", "labels": [], "entities": []}, {"text": "As explored by , non-expert users produce seed sets that lead to poor quality expansions, fora variety of reasons including ambiguity and lack of coverage.", "labels": [], "entities": []}, {"text": "Even for expert users, constructing seed sets can be a laborious and timeconsuming process.", "labels": [], "entities": []}, {"text": "Second, most set expansion systems do not use negative examples, which can be very useful for weeding out other bad answers.", "labels": [], "entities": []}, {"text": "Third, many set expansion systems concentrate on noun classes such as \"US Presidents\" and are not effective or do not apply to other kinds of sets.", "labels": [], "entities": [{"text": "set expansion", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.7586083710193634}, {"text": "US Presidents\"", "start_pos": 71, "end_pos": 85, "type": "DATASET", "confidence": 0.8907760779062907}]}, {"text": "Our system works as follows.", "labels": [], "entities": []}, {"text": "The user initially thinks of at least one seed word belonging to the desired set.", "labels": [], "entities": []}, {"text": "One at a time, the system presents candidate words to the user and asks whether the candidate fits the concept.", "labels": [], "entities": []}, {"text": "The user's answer is fed back into the system, which takes into account this new information and presents anew candidate to the user.", "labels": [], "entities": []}, {"text": "This continues until the user is satisfied with the compiled list of \"Yes\" answers.", "labels": [], "entities": []}, {"text": "Our system uses both positive and negative examples to guide the search, allowing it to recover from initially poor seed words.", "labels": [], "entities": []}, {"text": "By using multiple sources of similarity data, our system captures a variety of kinds of similarity.", "labels": [], "entities": []}, {"text": "Our system replaces the potentially difficult problem of thinking of many seed words with the easier task of answering yes/no questions.", "labels": [], "entities": [{"text": "answering yes/no questions", "start_pos": 109, "end_pos": 135, "type": "TASK", "confidence": 0.7710322380065918}]}, {"text": "The downside is a possibly increased amount of user interaction (although standard set expansion requires a non-trivial amount of user interaction to build the seed set).", "labels": [], "entities": []}, {"text": "There are many practical uses for such a system.", "labels": [], "entities": []}, {"text": "Building a better, more comprehensive thesaurus/gazetteer is one obvious application.", "labels": [], "entities": []}, {"text": "Another application is in high-precision query expansion, where a human manually builds a list of ex-pansion terms.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.6759042590856552}]}, {"text": "Suppose we are looking for pages discussing \"public safety.\"", "labels": [], "entities": []}, {"text": "Then synonyms (or near-synonyms) of \"safety\" would be useful (e.g. \"security\") but also non-synonyms such as \"precautions\" or \"prevention\" are also likely to return good results.", "labels": [], "entities": []}, {"text": "In this case, the concept we are interested in is \"Words which imply that safety is being discussed.\"", "labels": [], "entities": []}, {"text": "Another interesting direction not pursued in this paper is using our system as part of a more-traditional set expansion system to build seed sets more quickly.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given a seed set sand a complete target set G, it is easy to evaluate our system; we say \"Yes\" to anything in G, \"No\" to everything else, and see how many of the candidate words are in G.", "labels": [], "entities": []}, {"text": "However, building a complete gold-standard G is in practice prohibitively difficult; instead, we are only capable of saying whether or not a word belongs to G when presented with that word.", "labels": [], "entities": []}, {"text": "To evaluate a particular active learning algorithm, we can just run the algorithm manually, and see how many candidate words we say \"Yes\" to (note that this will not give us an accurate estimate of the recall of our algorithm).", "labels": [], "entities": [{"text": "recall", "start_pos": 202, "end_pos": 208, "type": "METRIC", "confidence": 0.9979288578033447}]}, {"text": "Evaluating several different algorithms for the same sand G is more difficult.", "labels": [], "entities": []}, {"text": "We could run each algorithm separately, but there are several problems with this approach.", "labels": [], "entities": []}, {"text": "First, we might unconsciously (or consciously) bias the results in favor of our preferred algorithms.", "labels": [], "entities": []}, {"text": "Second, it would be fairly difficult to be consistent across multiple runs.", "labels": [], "entities": []}, {"text": "Third, it would be inefficient, since we would label the same words multiple times for different algorithms.", "labels": [], "entities": []}, {"text": "We solved this problem by building a labeling system which runs all algorithms that we wish to test in parallel.", "labels": [], "entities": []}, {"text": "At each step, we pick a random algorithm and either present its current candidate to the user or, if that candidate has already been labeled, we supply that algorithm with the given answer.", "labels": [], "entities": []}, {"text": "We do NOT ever give an algorithm a labeled training example unless it actually asks for it -this guarantees that the combined system is equivalent to running each algorithm separately.", "labels": [], "entities": []}, {"text": "This procedure has the property that the user cannot tell which algorithms presented which words.", "labels": [], "entities": []}, {"text": "To evaluate the relative contribution of active learning, we consider aversion of our system where active learning is disabled.", "labels": [], "entities": []}, {"text": "Instead of retraining the system every iteration, we train it once on the seed set sand keep the weight vector \u03b8 fixed from iteration to iteration.", "labels": [], "entities": []}, {"text": "We evaluated our algorithms along three axes.", "labels": [], "entities": []}, {"text": "First, the method for choosing \u03b8: Untrained and Logistic (U and L).", "labels": [], "entities": []}, {"text": "Second, the data sources used: each source separately (M for Moby, W for WordNet, D for distributional similarity), and all three in combination (MWD).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9444534182548523}]}, {"text": "Third, whether active learning is used (+/-).", "labels": [], "entities": []}, {"text": "Thus, logistic regression using Moby and no active learning is L(M,-).", "labels": [], "entities": []}, {"text": "For logistic regression, we set the regularization penalty \u03c3 2 to 1, based on qualitative analysis during development (before seeing the test data).", "labels": [], "entities": []}, {"text": "We also compared the performance of our algorithms to the popular online thesaurus http://thesaurus.com.", "labels": [], "entities": []}, {"text": "The entries in this thesaurus are similar to Moby, except that each word may have multiple sense-disambiguated entries.", "labels": [], "entities": []}, {"text": "For each seed word w, we downloaded the page for wand extracted a set of synonyms entries for that word.", "labels": [], "entities": []}, {"text": "To compare fairly with our algorithms, we propose a word-by-word method for exploring the thesaurus, intended to model a user scanning the thesaurus.", "labels": [], "entities": []}, {"text": "This method checks the first 3 words from each entry; if none of these are labeled \"Yes,\" it moves onto the next entry.", "labels": [], "entities": []}, {"text": "We omit details for lack of space.", "labels": [], "entities": []}, {"text": "We designed a test set containing different types of similarity.", "labels": [], "entities": []}, {"text": "shows each category, with examples of specific similarity queries.", "labels": [], "entities": []}, {"text": "For each type, we tested on five different queries.", "labels": [], "entities": []}, {"text": "For each query, the first author built the seed set by writing down the first three words that came to mind.", "labels": [], "entities": []}, {"text": "For most queries this was easy.", "labels": [], "entities": []}, {"text": "However, for the similarity type Hard Synonyms, coming up with more than one seed word was considerably more difficult.", "labels": [], "entities": []}, {"text": "To build seed sets for these queries, we ran our evaluation system using a single seed word and took the first two positive candidates; this ensured that we were not biasing our seed set in favor of a particular algorithm or data set.", "labels": [], "entities": []}, {"text": "For each query, we ran our evaluation system until each algorithm had suggested 25 candidate words, fora total of 625 labeled words per algorithm.", "labels": [], "entities": []}, {"text": "We measured performance using mean average precision (MAP), which corresponds to area under the precision-recall curve.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 30, "end_pos": 58, "type": "METRIC", "confidence": 0.9410427610079447}, {"text": "precision-recall", "start_pos": 96, "end_pos": 112, "type": "METRIC", "confidence": 0.9855983853340149}]}, {"text": "It gives an overall assessment across different stopping points.", "labels": [], "entities": []}, {"text": "shows results for an informative subset of the tested algorithms.", "labels": [], "entities": []}, {"text": "There are many conclusions we can draw.", "labels": [], "entities": []}, {"text": "Thesaurus.Com performs poorly overall; our best system, L(MWD,+), outscores it by 164%.", "labels": [], "entities": [{"text": "Thesaurus.Com", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9245771169662476}]}, {"text": "The next group of al-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Categories and examples", "labels": [], "entities": []}, {"text": " Table 4: Results by category", "labels": [], "entities": []}]}