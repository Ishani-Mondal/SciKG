{"title": [], "abstractContent": [{"text": "Many researchers are trying to use information extraction (IE) to create large-scale knowledge bases from natural language text on the Web.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.8627292394638062}]}, {"text": "However, the primary approach (su-pervised learning of relation-specific extrac-tors) requires manually-labeled training data for each relation and doesn't scale to the thousands of relations encoded in Web text.", "labels": [], "entities": []}, {"text": "This paper presents LUCHS, a self-supervised, relation-specific IE system which learns 5025 relations-more than an order of magnitude greater than any previous approach-with an average F1 score of 61%.", "labels": [], "entities": [{"text": "IE", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.8179101943969727}, {"text": "F1 score", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9883780181407928}]}, {"text": "Crucial to LUCHS's performance is an automated system for dynamic lexicon learning, which allows it to learn accurately from heuristically-generated training data, which is often noisy and sparse.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information extraction (IE), the process of generating relational data from natural-language text, has gained popularity for its potential applications in Web search, question answering and other tasks.", "labels": [], "entities": [{"text": "Information extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.870228749513626}, {"text": "question answering", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.9087464809417725}]}, {"text": "Two main approaches have been attempted: \u2022 Supervised learning of relation-specific extractors (e.g.,), and \u2022 \"Open\" IE -self-supervised learning of unlexicalized, relation-independent extractors (e.g., Textrunner ().", "labels": [], "entities": []}, {"text": "Unfortunately, both methods have problems.", "labels": [], "entities": []}, {"text": "Supervised approaches require manually-labeled training data for each relation and hence can't scale to handle the thousands of relations encoded in Web text.", "labels": [], "entities": []}, {"text": "Open extraction is more scalable, but has lower precision and recall.", "labels": [], "entities": [{"text": "Open extraction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7844682931900024}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.999422550201416}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9988937973976135}]}, {"text": "Furthermore, open extraction doesn't canonicalize relations, so any application using the output must deal with homonymy and synonymy.", "labels": [], "entities": [{"text": "open extraction", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.702193409204483}]}, {"text": "A third approach, sometimes refered to as weak supervision, is to heuristically match values from a database to text, thus generating a set of training data for self-supervised learning of relationspecific extractors).", "labels": [], "entities": []}, {"text": "With the Kylin system () applied this idea to Wikipedia by matching values of an article's infobox 1 attributes to corresponding sentences in the article, and suggested that their approach could extract thousands of relations ( . Unfortunately, however, they never tested the idea on more than a dozen relations.", "labels": [], "entities": []}, {"text": "Indeed, no one has demonstrated a practical way to extract more than about one hundred relations.", "labels": [], "entities": []}, {"text": "We note that Wikipedia's infobox 'ontology' is a particularly interesting target for extraction.", "labels": [], "entities": []}, {"text": "As a by-product of thousands of contributors, it is broad in coverage and growing quickly.", "labels": [], "entities": [{"text": "coverage", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9761815071105957}]}, {"text": "Unfortunately, the schemata are surprisingly noisy and most are sparsely populated; challenging conditions for extraction.", "labels": [], "entities": []}, {"text": "This paper presents LUCHS, an autonomous, self-supervised system, which learns 5025 relational extractors -an order of magnitude greater than any previous effort.", "labels": [], "entities": []}, {"text": "Like Kylin, LUCHS creates training data by matching Wikipedia attribute values with corresponding sentences, but by itself, this method was insufficient for accurate extraction of most relations.", "labels": [], "entities": []}, {"text": "Thus, LUCHS introduces anew technique, dynamic lexicon features, which dramatically improves performance when learning from sparse data and that way enables scalability.", "labels": [], "entities": []}, {"text": "In order to handle sparsity in its heuristically-generated training data, LUCHS generates custom lexicon features when learning each relational extractor.", "labels": [], "entities": []}], "datasetContent": [{"text": "We start by evaluating end-to-end performance of LUCHS when applied to Wikipedia text, then analyze the characteristics of its components.", "labels": [], "entities": []}, {"text": "Our experiments use the 10/2008 English Wikipedia dump.", "labels": [], "entities": [{"text": "10/2008 English Wikipedia dump", "start_pos": 24, "end_pos": 54, "type": "DATASET", "confidence": 0.8197035193443298}]}], "tableCaptions": [{"text": " Table 1: Impact of Lexicon and Gaussian features.  Cross-Training (CT) is essential to improve per- formance.", "labels": [], "entities": []}, {"text": " Table 2: Lexicon and Gaussian features greatly ex- pand F1 score (F1-LUCHS) over the baseline (F1- B), in particular for attributes with few training ex- amples. Gains are mainly due to increased recall.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9470613300800323}, {"text": "F1-LUCHS)", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9505853950977325}, {"text": "F1- B)", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9669071733951569}, {"text": "Gains", "start_pos": 163, "end_pos": 168, "type": "METRIC", "confidence": 0.9933736324310303}, {"text": "recall", "start_pos": 197, "end_pos": 203, "type": "METRIC", "confidence": 0.999242901802063}]}]}