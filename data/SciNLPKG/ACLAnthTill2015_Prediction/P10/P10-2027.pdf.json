{"title": [{"text": "Fixed Length Word Suffix for Factored Statistical Machine Translation", "labels": [], "entities": [{"text": "Fixed Length Word Suffix", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5278077721595764}, {"text": "Statistical Machine Translation", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.6143890122572581}]}], "abstractContent": [{"text": "Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors.", "labels": [], "entities": [{"text": "Factored Statistical Machine Translation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6009217947721481}, {"text": "Phrase Based SMT", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.698110818862915}]}, {"text": "Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output.", "labels": [], "entities": [{"text": "Part of Speech tags", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.5770106986165047}]}, {"text": "However, high quality part of speech taggers are not available in open domain for many languages.", "labels": [], "entities": [{"text": "speech taggers", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.739136129617691}]}, {"text": "In this paper we used fixed length word suffix as anew factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.870442807674408}, {"text": "WMT Spanish to English", "start_pos": 206, "end_pos": 228, "type": "TASK", "confidence": 0.5765635520219803}]}], "introductionContent": [{"text": "Statistical Machine Translation(SMT) is currently the state of the art solution to the machine translation.", "labels": [], "entities": [{"text": "Statistical Machine Translation(SMT)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8394868671894073}, {"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7634726762771606}]}, {"text": "Phrase based SMT is also among the top performing approaches available as of today.", "labels": [], "entities": [{"text": "Phrase based SMT", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8501113057136536}]}, {"text": "This approach is a purely lexical approach, using surface forms of the words in the parallel corpus to generate the translations and estimate probabilities.", "labels": [], "entities": []}, {"text": "It is possible to incorporate syntactical information into this framework through different ways.", "labels": [], "entities": []}, {"text": "Source side syntax based re-ordering as preprocessing step, dependency based reordering models, cohesive decoding features are among many available successful attempts for the integration of syntax into the translation model.", "labels": [], "entities": []}, {"text": "Factored translation modeling is another way to achieve this goal.", "labels": [], "entities": [{"text": "Factored translation modeling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.73559041817983}]}, {"text": "These models allow each word to be represented as a vector of factors rather than a single surface form.", "labels": [], "entities": []}, {"text": "Factors can represent richer expression power on each word.", "labels": [], "entities": []}, {"text": "Any factors such as word stems, gender, part of speech, tense, etc.", "labels": [], "entities": [{"text": "word stems", "start_pos": 20, "end_pos": 30, "type": "TASK", "confidence": 0.6922700703144073}]}, {"text": "can be easily used in this framework.", "labels": [], "entities": []}, {"text": "Previous work in factored translation modeling have reported consistent improvements from Part of Speech(POS) tags, morphology, gender, and case factors . In another work, have achieved improvement using Combinational Categorial Grammar (CCG) super-tag factors.", "labels": [], "entities": [{"text": "factored translation modeling", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.7366991937160492}]}, {"text": "Creating the factors is done as a preprocessing step, and so far, most of the experiments have assumed existence of external tools for the creation of these factors (i. e. Part of speech taggers, CCG parsers, etc.).", "labels": [], "entities": []}, {"text": "Unfortunately high quality language processing tools, especially for the open domain, are not available for most languages.", "labels": [], "entities": []}, {"text": "While linguistically identifiable representations (i.e. POS tags, CCG supertags, etc) have been very frequently used as factors in many applications including MT, simpler representations have also been effective in achieving the same result in other application areas., were able to use fixed length suffixes as features for training a POS tagging.", "labels": [], "entities": [{"text": "MT", "start_pos": 159, "end_pos": 161, "type": "TASK", "confidence": 0.9358987212181091}, {"text": "POS tagging", "start_pos": 336, "end_pos": 347, "type": "TASK", "confidence": 0.7304577827453613}]}, {"text": "In another work showed that reversing middle chunks of the words while keeping the first and last part intact, does not decrease listeners' recognition ability.", "labels": [], "entities": []}, {"text": "This result is very relevant to Machine Translation, suggesting that inaccurate context which is usually modeled with n-gram language models, can still be as effective as accurate surface forms.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.9016251564025879}]}, {"text": "Another research (Rawlinson 1997) confirms this finding; this time in textual domain, observing that randomization of letters in the middle of words has little or no effect on the ability of skilled readers to understand the text.", "labels": [], "entities": []}, {"text": "These results suggest that the inexpensive representational factors which do not need unavailable tools might also be worth investigating.", "labels": [], "entities": []}, {"text": "These results encouraged us to introduce language independent simple factors for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.8078666031360626}]}, {"text": "In this paper, following the work of Grzymala-Busse et. al. we used fixed length suf-fix as word factor, to lower the perplexity of the language model, and have the factors roughly function as part of speech tags, thus increasing the grammaticality of the translation results.", "labels": [], "entities": []}, {"text": "We were able to obtain consistent, significant improvements over our baseline in 3 different experiments, large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system.", "labels": [], "entities": [{"text": "NIST", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.8670865297317505}, {"text": "WMT Spanish to English", "start_pos": 150, "end_pos": 172, "type": "TASK", "confidence": 0.5427746847271919}]}, {"text": "The rest of this paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly reviews the Factored Translation Models.", "labels": [], "entities": []}, {"text": "In section 3 we will introduce our model, and section 4 will contain the experiments and the analysis of the results, and finally, we will conclude this paper in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used Moses implementation of the factored model for training the feature weights, and SRI toolkit for building n-gram language models.", "labels": [], "entities": [{"text": "SRI", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.6970228552818298}]}, {"text": "The baseline for all systems included the moses system with lexicalized re-ordering, SRI 5-gram language models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU score, English to Iraqi Transtac sys- tem, comparing Factored and Baseline systems.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9711478352546692}, {"text": "Iraqi Transtac sys- tem", "start_pos": 33, "end_pos": 56, "type": "DATASET", "confidence": 0.7997864246368408}]}, {"text": " Table 2: BLEU score, Spanish to English WMT sys- tem, comparing Factored and Baseline systems.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9760844707489014}]}, {"text": " Table 3: BLEU score, Arabic to English NIST 2009  system, comparing Factored and Baseline systems.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9734569787979126}, {"text": "NIST 2009  system", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.9137528936068217}]}]}