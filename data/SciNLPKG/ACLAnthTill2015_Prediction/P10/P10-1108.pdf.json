{"title": [{"text": "Efficient Inference Through Cascades of Weighted Tree Transducers", "labels": [], "entities": []}], "abstractContent": [{"text": "Weighted tree transducers have been proposed as useful formal models for representing syntactic natural language processing applications, but there has been little description of inference algorithms for these automata beyond formal foundations.", "labels": [], "entities": []}, {"text": "We give a detailed description of algorithms for application of cascades of weighted tree transducers to weighted tree acceptors, connecting formal theory with actual practice.", "labels": [], "entities": []}, {"text": "Additionally, we present novel on-the-fly variants of these algorithms , and compare their performance on a syntax machine translation cascade based on (Yamada and Knight, 2001).", "labels": [], "entities": [{"text": "syntax machine translation cascade", "start_pos": 108, "end_pos": 142, "type": "TASK", "confidence": 0.7262092530727386}]}, {"text": "1 Motivation Weighted finite-state transducers have found recent favor as models of natural language (Mohri, 1997).", "labels": [], "entities": []}, {"text": "In order to make actual use of systems built with these formalisms we must first calculate the set of possible weighted outputs allowed by the transducer given some input, which we call forward application, or the set of possible weighted inputs given some output, which we call backward application.", "labels": [], "entities": []}, {"text": "After application we can do some inference on this result, such as determining its k highest weighted elements.", "labels": [], "entities": []}, {"text": "We may also want to divide up our problems into manageable chunks, each represented by a transducer.", "labels": [], "entities": []}, {"text": "As noted by Woods (1980), it is easier for designers to write several small transducers where each performs a simple transformation, rather than painstakingly construct a single complicated device.", "labels": [], "entities": []}, {"text": "We would like to know, then, the result of transformation of input or output by a cascade of transducers, one operating after the other.", "labels": [], "entities": []}, {"text": "As we will see, there are various strategies for approaching this problem.", "labels": [], "entities": []}, {"text": "We will consider offline composition, bucket brigade application , and on-the-fly application.", "labels": [], "entities": [{"text": "bucket brigade application", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.872815728187561}]}, {"text": "Application of cascades of weighted string transducers (WSTs) has been well-studied (Mohri, 1997).", "labels": [], "entities": [{"text": "weighted string transducers (WSTs)", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.7108955383300781}]}, {"text": "Less well-studied but of more recent interest is application of cascades of weighted tree transducers (WTTs).", "labels": [], "entities": [{"text": "weighted tree transducers (WTTs)", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.682248498002688}]}, {"text": "We tackle application of WTT cascades in this work, presenting: \u2022 explicit algorithms for application of WTT cascades \u2022 novel algorithms for on-the-fly application of WTT cascades, and \u2022 experiments comparing the performance of these algorithms.", "labels": [], "entities": [{"text": "WTT cascades", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.7814131081104279}]}, {"text": "2 Strategies for the string case Before we discuss application of WTTs, it is helpful to recall the solution to this problem in the WST domain.", "labels": [], "entities": [{"text": "WTTs", "start_pos": 66, "end_pos": 70, "type": "TASK", "confidence": 0.6108264327049255}]}, {"text": "We recall previous formal presentations of WSTs (Mohri, 1997) and note informally that they maybe represented as directed graphs with designated start and end states and edges labeled with input symbols, output symbols, and weights.", "labels": [], "entities": [{"text": "WSTs", "start_pos": 43, "end_pos": 47, "type": "TASK", "confidence": 0.9289057850837708}]}, {"text": "1 Fortunately, the solution for WSTs is practically trivial-we achieve application through a series of embedding, composition, and projection operations.", "labels": [], "entities": [{"text": "WSTs", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.9697478413581848}]}, {"text": "Embedding is simply the act of representing a string or regular string language as an identity WST.", "labels": [], "entities": []}, {"text": "Composition of WSTs, that is, generating a single WST that captures the transformations of two input WSTs used in sequence, is not at all trivial, but has been well covered in, e.g., (Mohri, 2009), where directly implementable algorithms can be found.", "labels": [], "entities": []}, {"text": "Finally, projection is another trivial operation-the domain or range language can be obtained from a WST by ignoring the output or input symbols, respectively, on its arcs, and summing weights on otherwise identical arcs.", "labels": [], "entities": []}, {"text": "By embedding an input, composing the result with the given WST, and projecting the result, forward application is accomplished.", "labels": [], "entities": []}, {"text": "2 We are then left with a weighted string acceptor (WSA), essentially a weighted, labeled graph, which can be traversed 1 We assume throughout this paper that weights are in R+ \u222a {+\u221e}, that the weight of a path is calculated as the product of the weights of its edges, and that the weight of a (not necessarily finite) set T of paths is calculated as the sum of the weights of the paths of T.", "labels": [], "entities": []}, {"text": "2 For backward applications, the roles of input and output are simply exchanged.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The main purpose of this paper has been to present novel algorithms for performing application.", "labels": [], "entities": []}, {"text": "However, it is important to demonstrate these algorithms on real data.", "labels": [], "entities": []}, {"text": "We thus demonstrate bucket-brigade and on-the-fly backward application on atypical NLP task cast as a cascade of wLNT.", "labels": [], "entities": []}, {"text": "We adapt the Japanese-to-English transla-: Transducer types and available methods of forward and backward application of a cascade.", "labels": [], "entities": []}, {"text": "oc = offline composition, bb = bucket brigade, otf = on the fly.", "labels": [], "entities": []}, {"text": "tion model of by transforming it from an English-tree-to-Japanese-string model to an English-tree-to-Japanese-tree model.", "labels": [], "entities": []}, {"text": "The Japanese trees are unlabeled, meaning they have syntactic structure but all nodes are labeled \"X\".", "labels": [], "entities": []}, {"text": "We then cast this modified model as a cascade of LNT tree transducers.", "labels": [], "entities": []}, {"text": "Space does not permit a detailed description, but some example rules are in.", "labels": [], "entities": []}, {"text": "The rotation transducer R, a sample of which is in, has 6,453 rules, the insertion transducer I,, has 8,122 rules, and the translation transducer, T ,, has 37,311 rules.", "labels": [], "entities": []}, {"text": "We add an English syntax language model L to the cascade of transducers just described to better simulate an actual machine translation decoding task.", "labels": [], "entities": [{"text": "machine translation decoding task", "start_pos": 116, "end_pos": 149, "type": "TASK", "confidence": 0.773491770029068}]}, {"text": "The language model is cast as an identity WTT and thus fits naturally into the experimental framework.", "labels": [], "entities": []}, {"text": "In our experiments we try several different language models to demonstrate varying performance of the application algorithms.", "labels": [], "entities": []}, {"text": "The most realistic language model is a PCFG.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.9197147488594055}]}, {"text": "Each rule captures the probability of a particular sequence of child labels given a parent label.", "labels": [], "entities": []}, {"text": "This model has 7,765 rules.", "labels": [], "entities": []}, {"text": "To demonstrate more extreme cases of the usefulness of the on-the-fly approach, we build a language model that recognizes exactly the 2,087 trees in the training corpus, each with equal weight.", "labels": [], "entities": []}, {"text": "Finally, to be ultraspecific, we include a form of the \"specific\" language model just described, but only allow the English counterpart of the particular Japanese sentence being decoded in the language.", "labels": [], "entities": []}, {"text": "The goal in our experiments is to apply a singletree t backward through the cascade L\u2022R\u2022I \u2022T \u2022t and find the 1-best path in the application WRTG.", "labels": [], "entities": [{"text": "WRTG", "start_pos": 140, "end_pos": 144, "type": "DATASET", "confidence": 0.8844354748725891}]}, {"text": "We evaluate the speed of each approach: bucket brigade and on-the-fly.", "labels": [], "entities": []}, {"text": "The algorithm we use to obtain the 1-best path is a modification of the kbest algorithm of.", "labels": [], "entities": []}, {"text": "Our algorithm finds the 1-best path in a WRTG and admits an on-the-fly approach.", "labels": [], "entities": [{"text": "WRTG", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.8629884123802185}]}, {"text": "The results of the experiments are shown in: Timing results to obtain 1-best from application through a weighted tree transducer cascade, using on-the-fly vs. bucket brigade backward application techniques.", "labels": [], "entities": []}, {"text": "pcfg = model recognizes any tree licensed by a pcfg built from observed data, exact = model recognizes each of 2,000+ trees with equal weight, 1-sent = model recognizes exactly one tree.", "labels": [], "entities": []}, {"text": "experiment that uses an English PCFG language model.", "labels": [], "entities": []}, {"text": "The results for the other two language models demonstrate more keenly the potential advantage that an on-the-fly approach provides-the simultaneous incorporation of information from all models allows application to be done more effectively than if each information source is considered in sequence.", "labels": [], "entities": []}, {"text": "In the \"exact\" case, where a very large language model that simply recognizes each of the 2,087 trees in the training corpus is used, the final application is so large that it overwhelms the resources of a 4gb MacBook Pro, while the on-the-fly approach does not suffer from this problem.", "labels": [], "entities": []}, {"text": "The \"1-sent\" case is presented to demonstrate the ripple effect caused by using on-the fly.", "labels": [], "entities": []}, {"text": "In the other two cases, a very large language model generally overwhelms the timing statistics, regardless of the method being used.", "labels": [], "entities": []}, {"text": "But a language model that represents exactly one sentence is very small, and thus the effects of simultaneous inference are readily apparent-the time to retrieve the 1-best sentence is reduced by two orders of magnitude in this experiment.", "labels": [], "entities": []}], "tableCaptions": []}