{"title": [{"text": "Learning Lexicalized Reordering Models from Reordering Graphs", "labels": [], "entities": []}], "abstractContent": [{"text": "Lexicalized reordering models play a crucial role in phrase-based translation systems.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.7678349018096924}]}, {"text": "They are usually learned from the word-aligned bilingual corpus by examining the reordering relations of adjacent phrases.", "labels": [], "entities": []}, {"text": "Instead of just checking whether there is one phrase adjacent to a given phrase, we argue that it is important to take the number of adjacent phrases into account for better estimations of reordering models.", "labels": [], "entities": []}, {"text": "We propose to use a structure named reordering graph, which represents all phrase segmentations of a sentence pair, to learn lex-icalized reordering models efficiently.", "labels": [], "entities": []}, {"text": "Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method.", "labels": [], "entities": [{"text": "NIST Chinese-English test sets", "start_pos": 28, "end_pos": 58, "type": "DATASET", "confidence": 0.9609367996454239}]}], "introductionContent": [{"text": "Phrase-based translation systems () prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations.", "labels": [], "entities": [{"text": "Phrase-based translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6927579343318939}, {"text": "translation", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.9646083116531372}, {"text": "machine translation evaluations", "start_pos": 125, "end_pos": 156, "type": "TASK", "confidence": 0.8207993507385254}]}, {"text": "While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases.", "labels": [], "entities": [{"text": "memorizing local translation", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.7256924510002136}]}, {"text": "As a result, it is important to develop effective reordering models to capture such non-local reordering.", "labels": [], "entities": []}, {"text": "The early phrase-based paradigm () applies a simple distance-based distortion penalty to model the phrase movements.", "labels": [], "entities": []}, {"text": "More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering;; Koehn et al.,.", "labels": [], "entities": []}, {"text": "These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D).", "labels": [], "entities": []}, {"text": "Take the bilingual phrase bp in(a) for example.", "labels": [], "entities": []}, {"text": "The wordbased reordering model () analyzes the word alignments at positions (s \u2212 1, u \u2212 1) and (s \u2212 1, v + 1).", "labels": [], "entities": []}, {"text": "The orientation of bp is set to D because the position (s \u2212 1, v + 1) contains no word alignment.", "labels": [], "entities": []}, {"text": "The phrase-based reordering model) determines the presence of the adjacent bilingual phrase located in position (s \u2212 1, v + 1) and then treats the orientation of bp as S.", "labels": [], "entities": []}, {"text": "Given no constraint on maximum phrase length, the hierarchical phrase reordering model () also analyzes the adjacent bilingual phrases for bp and identifies its orientation as S.", "labels": [], "entities": []}, {"text": "However, given a bilingual phrase, the abovementioned models just consider the presence of an adjacent bilingual phrase rather than the number of adjacent bilingual phrases.", "labels": [], "entities": []}, {"text": "See the examples inure 1 for illustration.", "labels": [], "entities": []}, {"text": "In(a), bp is in a swap order with only one bilingual phrase.", "labels": [], "entities": []}, {"text": "In(b), bp swaps with three bilingual phrases.", "labels": [], "entities": []}, {"text": "Lexicalized reordering models do not distinguish different numbers of adjacent phrase pairs, and just give bp the same count in the swap orientation.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel method to better estimate the reordering probabilities with the consideration of varying numbers of adjacent bilingual phrases.", "labels": [], "entities": []}, {"text": "Our method uses reordering graphs to represent all phrase segmentations of parallel sentence pairs, and then gets the fractional counts of bilingual phrases for orientations from reordering graphs in an inside-outside fashion.", "labels": [], "entities": []}, {"text": "Experimental results indicate that our method achieves significant improvements over the traditional lexicalized reordering model (.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: in Section 2, we first give a brief introduction to the traditional lexicalized reordering model.", "labels": [], "entities": []}, {"text": "Then we introduce our method to estimate the reordering probabilities from reordering graphs.", "labels": [], "entities": []}, {"text": "The experimental results are reported in Section 3.", "labels": [], "entities": []}, {"text": "Finally, we end with a conclusion and future work in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two different sizes of training corpora are used in our experiments: one is a small-scale corpus that comes from FBIS corpus consisting of 239K bilingual sentence pairs, the other is a large-scale corpus that includes 1.55M bilingual sentence pairs from LDC.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 113, "end_pos": 124, "type": "DATASET", "confidence": 0.878844141960144}]}, {"text": "The 2002 NIST MT evaluation test data is used as the development set and the NIST MT test data are the test sets.", "labels": [], "entities": [{"text": "NIST MT evaluation test data", "start_pos": 9, "end_pos": 37, "type": "DATASET", "confidence": 0.8951164126396179}, {"text": "NIST MT test data", "start_pos": 77, "end_pos": 94, "type": "DATASET", "confidence": 0.9200681000947952}]}, {"text": "We choose the MOSES 1 ( as the experimental decoder.", "labels": [], "entities": [{"text": "MOSES 1", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.7798246145248413}]}, {"text": "GIZA++ and the heuristics \"grow-diag-final-and\" are used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7.", "labels": [], "entities": []}, {"text": "We use SRILM Toolkits) to train a 4-gram language model on the Xinhua portion of Gigaword corpus.", "labels": [], "entities": [{"text": "SRILM Toolkits", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.7730588018894196}, {"text": "Xinhua portion of Gigaword corpus", "start_pos": 63, "end_pos": 96, "type": "DATASET", "confidence": 0.6571524024009705}]}, {"text": "In exception to the reordering probabilities, we use the same features in the comparative experiments.", "labels": [], "entities": []}, {"text": "During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training to tune various feature weights.", "labels": [], "entities": [{"text": "stack", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9656868577003479}]}, {"text": "The translation quality is evaluated by case-insensitive BLEU-4 metric ().", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9517716765403748}, {"text": "BLEU-4", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9856300950050354}]}, {"text": "Finally, we conduct paired bootstrap sampling to test the significance in BLEU scores differences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.992739737033844}]}, {"text": "shows the results of experiments with the large training corpus.", "labels": [], "entities": []}, {"text": "In the experiments of the msd-fe model, in exception to the MT-05 test set, our method is superior to the baseline method.", "labels": [], "entities": [{"text": "MT-05 test set", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.9024228851000468}]}, {"text": "The BLEU scores by our method are 32.44, 33.24 and 31.64, which obtain 0.86, 0.85 and 0.15 gains on three test set, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9993535876274109}]}, {"text": "For the msdbidirectional-fe model, the BLEU scores produced by our approach are 33.29, 34.49 and 32.79 on the three test sets, with 0.86, 1.42 and 1.1 points higher than the baseline method, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9993544220924377}]}], "tableCaptions": [{"text": " Table 1: The \u03b1 and \u03b2 values of the bilingual phrases  shown in", "labels": [], "entities": []}, {"text": " Table 2: Experimental results with the small-scale cor- pus. m-f: msd-fe reordering model. m-b-f: msd- bidirectional-fe reordering model. RG: probabilities esti- mation based on Reordering Graph. * or **: significantly  better than baseline (p < 0 .05 or p < 0 .01 ).", "labels": [], "entities": [{"text": "RG", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.8679641485214233}]}, {"text": " Table 3: Experimental results with the large-scale cor- pus.", "labels": [], "entities": []}]}