{"title": [{"text": "Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages", "labels": [], "entities": [{"text": "Improving Word Alignment", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.8269656697909037}, {"text": "Machine Translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7117328941822052}]}], "abstractContent": [{"text": "We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7747751176357269}, {"text": "translation", "start_pos": 79, "end_pos": 90, "type": "TASK", "confidence": 0.9690057039260864}]}, {"text": "Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuris-tics.", "labels": [], "entities": []}, {"text": "We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words.", "labels": [], "entities": [{"text": "English-to-Pashto translation task", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.7255898316701254}]}, {"text": "The combined alignment outper-forms the baseline alignment, with significantly higher F-scores and better translation performance.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9977356195449829}]}], "introductionContent": [{"text": "Word alignment usually serves as the starting point and foundation fora statistical machine translation (SMT) system.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7269411236047745}, {"text": "statistical machine translation (SMT)", "start_pos": 72, "end_pos": 109, "type": "TASK", "confidence": 0.7726818273464838}]}, {"text": "It has received a significant amount of research over the years, notably in (.", "labels": [], "entities": []}, {"text": "They all focused on the improvement of word alignment models.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.8115765452384949}]}, {"text": "In this work, we leverage existing aligners and generate multiple sets of word alignments based on complementary information, then combine them to get the final alignment for phrase training.", "labels": [], "entities": [{"text": "phrase training", "start_pos": 175, "end_pos": 190, "type": "TASK", "confidence": 0.8771611750125885}]}, {"text": "The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example.", "labels": [], "entities": []}, {"text": "This makes the approach especially appealing for SMT on low-resource languages.", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9962462782859802}]}, {"text": "Most of the research on alignment combination in the past has focused on how to combine the alignments from two different directions, sourceto-target and target-to-source.", "labels": [], "entities": [{"text": "alignment combination", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.9605195224285126}]}, {"text": "Usually people start from the intersection of two sets of alignments, and gradually add links in the union based on certain heuristics, as in (, to achieve a better balance compared to using either intersection (high precision) or union (high recall).", "labels": [], "entities": [{"text": "precision", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9083945751190186}, {"text": "recall", "start_pos": 243, "end_pos": 249, "type": "METRIC", "confidence": 0.9887928366661072}]}, {"text": "In () a maximum entropy approach was proposed to combine multiple alignments based on a set of linguistic and alignment features.", "labels": [], "entities": []}, {"text": "A different approach was presented in, which again concentrated on the combination of two sets of alignments, but with a different criterion.", "labels": [], "entities": []}, {"text": "It tries to maximize the number of phrases that can be extracted in the combined alignments.", "labels": [], "entities": []}, {"text": "A greedy search method was utilized and it achieved higher translation performance than the baseline.", "labels": [], "entities": []}, {"text": "More recently, an alignment selection approach was proposed in, which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold.", "labels": [], "entities": [{"text": "alignment selection", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.9221749007701874}]}, {"text": "The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model).", "labels": [], "entities": []}, {"text": "In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.", "labels": [], "entities": []}, {"text": "There is no need fora pre-determined threshold as used in.", "labels": [], "entities": []}, {"text": "Also, we utilize various knowledge sources to enrich the alignments instead of using different aligners.", "labels": [], "entities": []}, {"text": "Our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low-resource languages.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We present three different sets of alignments in Section 2 for an English-to-Pashto MT task.", "labels": [], "entities": [{"text": "MT task", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.8540914952754974}]}, {"text": "In Section 3, we propose the alignment combination algorithm.", "labels": [], "entities": [{"text": "alignment combination", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.9699878692626953}]}, {"text": "The experimental results are reported in Section 4.", "labels": [], "entities": []}, {"text": "We conclude the paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Alignment precision, recall and F-score  (B: baseline; V: VP-based reordering; S: stem- ming; P: partial word; X: VP-reordered partial  word).", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.7863397002220154}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.7374536395072937}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9994416832923889}, {"text": "F-score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9979821443557739}]}, {"text": " Table 2: Improvement in BLEU scores (B: base- line; V: VP-based reordering; S: stemming; P: par- tial word; X: VP-reordered partial word).", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.9693048596382141}]}]}