{"title": [{"text": "Dynamic Programming for Linear-Time Incremental Parsing", "labels": [], "entities": [{"text": "Linear-Time Incremental Parsing", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.6245424548784891}]}], "abstractContent": [{"text": "Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming.", "labels": [], "entities": []}, {"text": "We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging \"equivalent\" stacks based on feature values.", "labels": [], "entities": []}, {"text": "Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss inaccuracy.", "labels": [], "entities": []}, {"text": "Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster.", "labels": [], "entities": []}], "introductionContent": [{"text": "In terms of search strategy, most parsing algorithms in current use for data-driven parsing can be divided into two broad categories: dynamic programming which includes the dominant CKY algorithm, and greedy search which includes most incremental parsing methods such as shift-reduce.", "labels": [], "entities": []}, {"text": "Both have pros and cons: the former performs an exact search (in cubic time) over an exponentially large space, while the latter is much faster (in linear-time) and is psycholinguistically motivated, but its greedy nature may suffer from severe search errors, as it only explores a tiny fraction of the whole space even with abeam.", "labels": [], "entities": []}, {"text": "Can we combine the advantages of both approaches, that is, construct an incremental parser 1 is a notable exception: the MST algorithm is exact search but not dynamic programming. that runs in (almost) linear-time, yet searches over a huge space with dynamic programming?", "labels": [], "entities": []}, {"text": "Theoretically, the answer is negative, as shows that context-free parsing can be used to compute matrix multiplication, where sub-cubic algorithms are largely impractical.", "labels": [], "entities": [{"text": "matrix multiplication", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.7605048716068268}]}, {"text": "We instead propose a dynamic programming alogorithm for shift-reduce parsing which runs in polynomial time in theory, but linear-time (with beam search) in practice.", "labels": [], "entities": []}, {"text": "The key idea is to merge equivalent stacks according to feature functions, inspired by Earley parsing and generalized LR parsing.", "labels": [], "entities": []}, {"text": "However, our formalism is more flexible and our algorithm more practical.", "labels": [], "entities": []}, {"text": "Specifically, we make the following contributions: \u2022 theoretically, we show that fora large class of modern shift-reduce parsers, dynamic programming is in fact possible and runs in polynomial time as long as the feature functions are bounded and monotonic (which almost always holds in practice); \u2022 practically, dynamic programming is up to five times faster (with the same accuracy) as conventional beam-search on top of a stateof-the-art shift-reduce dependency parser; \u2022 as a by-product, dynamic programming can output a forest encoding exponentially many trees, out of which we can draw better and longer k-best lists than beam search can; \u2022 finally, better and faster search also leads to better and faster learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 375, "end_pos": 383, "type": "METRIC", "confidence": 0.9973899722099304}]}, {"text": "Our final parser achieves the best (unlabeled) accuracies that we are aware of in both English and Chinese among dependency parsers trained on the Penn Treebanks.", "labels": [], "entities": [{"text": "Penn Treebanks", "start_pos": 147, "end_pos": 161, "type": "DATASET", "confidence": 0.9952559769153595}]}, {"text": "Being linear-time, it is also much faster than most other parsers, even with a pure Python implementation.", "labels": [], "entities": []}, {"text": "where \u2113 is the step, c is the cost, and the shift cost \u03be and reduce costs \u03bb and \u03c1 are: Figure 1: Deductive system of vanilla shift-reduce.", "labels": [], "entities": []}, {"text": "For convenience of presentation and experimentation, we will focus on shift-reduce parsing for dependency structures in the remainder of this paper, though our formalism and algorithm can also be applied to phrase-structure parsing.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 207, "end_pos": 231, "type": "TASK", "confidence": 0.7869322299957275}]}], "datasetContent": [{"text": "We first reimplemented the reference shift-reduce parser of in Python (henceforth \"non-DP\"), and then extended it to do dynamic programing (henceforth \"DP\").", "labels": [], "entities": []}, {"text": "We evaluate their performances on the standard Penn Treebank (PTB) English dependency parsing task 7 using the standard split: secs 02-21 for training, 22 for development, and 23 for testing.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) English dependency parsing task 7", "start_pos": 47, "end_pos": 100, "type": "DATASET", "confidence": 0.8728396177291871}]}, {"text": "Both DP and non-DP parsers use the same feature templates in.", "labels": [], "entities": []}, {"text": "4.1-4.2, we use a baseline model trained with non-DP for both DP and non-DP, so that we can do a side-by-side comparison of search quality; in Sec.", "labels": [], "entities": []}, {"text": "4.3 we will retrain the model with DP and compare it against training with non-DP.", "labels": [], "entities": [{"text": "DP", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.8898348808288574}]}], "tableCaptions": [{"text": " Table 2: Perceptron iterations with DP (left) and  non-DP (right). Early updates happen much more  often with DP due to equivalent state merging,  which leads to faster training (time in minutes).", "labels": [], "entities": []}, {"text": " Table 3: Final test results on English (PTB). Our  parser (in pure Python) has the highest accuracy  among dependency parsers trained on the Tree- bank, and is also much faster than major parsers.   \u2020 converted from constituency trees. C=C/C++,  Py=Python, Ja=Java. Time is in seconds per sen- tence. Search spaces:  \u2021 linear; others exponential.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9985857009887695}, {"text": "Tree- bank", "start_pos": 142, "end_pos": 152, "type": "DATASET", "confidence": 0.8314948081970215}]}, {"text": " Table 4: Final test results on Chinese (CTB5).   \u2020 The transition parser in Zhang and Clark (2008).", "labels": [], "entities": [{"text": "Chinese (CTB5)", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.7647614926099777}]}]}