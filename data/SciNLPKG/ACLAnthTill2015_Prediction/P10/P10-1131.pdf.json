{"title": [], "abstractContent": [{"text": "We present an approach to multilingual grammar induction that exploits a phylogeny-structured model of parameter drift.", "labels": [], "entities": [{"text": "multilingual grammar induction", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.7659264802932739}]}, {"text": "Our method does not require any translated texts or token-level alignments.", "labels": [], "entities": []}, {"text": "Instead, the phylogenetic prior couples languages at a parameter level.", "labels": [], "entities": []}, {"text": "Joint induction in the multilingual model substantially outperforms independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages.", "labels": [], "entities": []}, {"text": "Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%.", "labels": [], "entities": [{"text": "error reductions", "start_pos": 56, "end_pos": 72, "type": "METRIC", "confidence": 0.9515180289745331}]}], "introductionContent": [{"text": "Learning multiple languages together should be easier than learning them separately.", "labels": [], "entities": []}, {"text": "For example, in the domain of syntactic parsing, a range of recent work has exploited the mutual constraint between two languages' parses of the same bitext).", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7857843041419983}]}, {"text": "Moreover, in the context of unsupervised part-of-speech induction in the context of phonology) show that extending beyond two languages can provide increasing benefit.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.7211141288280487}]}, {"text": "However, multitexts are only available for limited languages and domains.", "labels": [], "entities": []}, {"text": "In this work, we consider unsupervised grammar induction without bitexts or multitexts.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.691634938120842}]}, {"text": "Without translation examples, multilingual constraints cannot be exploited at the sentence token level.", "labels": [], "entities": []}, {"text": "Rather, we capture multilingual constraints at a parameter level, using a phylogeny-structured prior to tie together the various individual languages' learning problems.", "labels": [], "entities": []}, {"text": "Our joint, hierarchical prior couples model parameters for different languages in away that respects knowledge about how the languages evolved.", "labels": [], "entities": []}, {"text": "Aspects of this work are closely related to and.", "labels": [], "entities": []}, {"text": "present a model for jointly learning English and Chinese dependency grammars without bitexts.", "labels": [], "entities": []}, {"text": "In their work, structurally constrained covariance in a logistic normal prior is used to couple parameters between the two languages.", "labels": [], "entities": []}, {"text": "Our work, though also different in technical approach, differs most centrally in the extension to multiple languages and the use of a phylogeny.", "labels": [], "entities": []}, {"text": "considers an entirely different problem, phonological reconstruction, but shares with this work both the use of a phylogenetic structure as well as the use of log-linear parameterization of local model components.", "labels": [], "entities": [{"text": "phonological reconstruction", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.7178956270217896}]}, {"text": "Our work differs from theirs primarily in the task (syntax vs. phonology) and the variables governed by the phylogeny: in our model it is the grammar parameters that drift (in the prior) rather than individual word forms (in the likelihood model).", "labels": [], "entities": []}, {"text": "Specifically, we consider dependency induction in the DMV model of.", "labels": [], "entities": [{"text": "dependency induction", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.8046320378780365}]}, {"text": "Our data is a collection of standard dependency data sets in eight languages: English, Dutch, Danish, Swedish, Spanish, Portuguese, Slovene, and Chinese.", "labels": [], "entities": []}, {"text": "Our focus is not the DMV model itself, which is well-studied, but rather the prior which couples the various languages' parameters.", "labels": [], "entities": []}, {"text": "While some choices of prior structure can greatly complicate inference, we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods.", "labels": [], "entities": []}, {"text": "In our experiments, joint multilingual learning substantially outperforms independent monolingual learning.", "labels": [], "entities": []}, {"text": "Using a limited phylogeny that only couples languages within linguistic families reduces error by 5.6% over the monolingual baseline.", "labels": [], "entities": [{"text": "error", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9984037280082703}]}, {"text": "Using a flat, global phylogeny gives a greater reduction, almost 10%.", "labels": [], "entities": []}, {"text": "Finally, a more articulated phylogeny that captures both inter-and intrafamily effects gives an even larger average relative error reduction of 21.1%.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 116, "end_pos": 140, "type": "METRIC", "confidence": 0.8659627636273702}]}], "datasetContent": [{"text": "For each setting, we evaluated the directed dependency accuracy of the minimum Bayes risk (MBR) dependency parses produced by our models under maximum (posterior) likelihood parameter estimates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.8923624157905579}, {"text": "minimum Bayes risk (MBR) dependency parses", "start_pos": 71, "end_pos": 113, "type": "METRIC", "confidence": 0.7419804595410824}]}, {"text": "We computed accuracies separately for each language in each condition.", "labels": [], "entities": []}, {"text": "In addition, for multilingual models, we computed the relative error reduction over the strong monolingual baseline, macro-averaged over languages.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 54, "end_pos": 78, "type": "METRIC", "confidence": 0.6936439474423727}]}], "tableCaptions": [{"text": " Table 2: Directed dependency accuracy of monolingual and multilingual models, and relative error reduction over the monolin- gual baseline with SHARED features macro-averaged over languages. Multilingual models outperformed monolingual models  in general, with larger gains from increasing numbers of languages. Additionally, more nuanced phylogenetic structures out- performed cruder ones.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9708563089370728}]}]}