{"title": [{"text": "Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization", "labels": [], "entities": [{"text": "Initialization", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.5518910884857178}]}], "abstractContent": [{"text": "We consider the search fora maximum likelihood assignment of hidden derivations and grammar weights fora proba-bilistic context-free grammar, the problem approximately solved by \"Viterbi training .\" We show that solving and even approximating Viterbi training for PCFGs is NP-hard.", "labels": [], "entities": []}, {"text": "We motivate the use of uniform-at-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters , providing an approximate bound on the log-likelihood.", "labels": [], "entities": [{"text": "Viterbi EM", "start_pos": 60, "end_pos": 70, "type": "DATASET", "confidence": 0.882901519536972}]}], "introductionContent": [{"text": "Probabilistic context-free grammars are an essential ingredient in many natural language processing models, inter alia).", "labels": [], "entities": []}, {"text": "Various algorithms for training such models have been proposed, including unsupervised methods.", "labels": [], "entities": []}, {"text": "Many of these are based on the expectationmaximization (EM) algorithm.", "labels": [], "entities": []}, {"text": "There are alternatives to EM, and one such alternative is Viterbi EM, also called \"hard\" EM or \"sparse\" EM ().", "labels": [], "entities": []}, {"text": "Instead of using the parameters (which are maintained in the algorithm's current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters.", "labels": [], "entities": []}, {"text": "Viterbi EM and variants have been used in various settings in natural language processing.", "labels": [], "entities": [{"text": "Viterbi EM", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9338089227676392}]}, {"text": "Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal \"Viterbi training.\"", "labels": [], "entities": []}, {"text": "In this paper, we explore Viterbi training for probabilistic context-free grammars.", "labels": [], "entities": []}, {"text": "We first show that under the assumption that P = NP, solving and even approximating the Viterbi training problem is hard.", "labels": [], "entities": []}, {"text": "This result holds even for hidden Markov models.", "labels": [], "entities": []}, {"text": "We extend the main hardness result to the EM algorithm (giving an alternative proof to this known result), as well as the problem of conditional Viterbi training.", "labels": [], "entities": []}, {"text": "We then describe a \"competitiveness\" result for uniform initialization of Viterbi EM: we show that initialization of the trees in an E-step which uses uniform distributions over the trees is optimal with respect to a certain approximate bound.", "labels": [], "entities": [{"text": "Viterbi EM", "start_pos": 74, "end_pos": 84, "type": "DATASET", "confidence": 0.9327147006988525}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "\u00a72 gives background on PCFGs and introduces some notation.", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.6989780068397522}]}, {"text": "\u00a73 explains Viterbi training, the declarative form of Viterbi EM.", "labels": [], "entities": []}, {"text": "\u00a74 describes a hardness result for Viterbi training.", "labels": [], "entities": []}, {"text": "\u00a75 extends this result to a hardness result of approximation and \u00a76 further extends these results for other cases.", "labels": [], "entities": []}, {"text": "\u00a77 describes the advantages in using uniform-at-random initialization for Viterbi training.", "labels": [], "entities": []}, {"text": "We relate these results to work on the k-means problem in \u00a78.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}