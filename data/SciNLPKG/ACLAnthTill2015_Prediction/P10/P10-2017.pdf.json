{"title": [{"text": "Exemplar-Based Models for Word Meaning In Context", "labels": [], "entities": [{"text": "Word Meaning In Context", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7842744961380959}]}], "abstractContent": [{"text": "This paper describes ongoing work on dis-tributional models for word meaning in context.", "labels": [], "entities": []}, {"text": "We abandon the usual one-vector-per-word paradigm in favor of an exemplar model that activates only relevant occurrences.", "labels": [], "entities": []}, {"text": "On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional models area popular framework for representing word meaning.", "labels": [], "entities": []}, {"text": "They describe a lemma through a high-dimensional vector that records co-occurrence with context features over a large corpus.", "labels": [], "entities": []}, {"text": "Distributional models have been used in many NLP analysis tasks (, as well as for cognitive modeling).", "labels": [], "entities": [{"text": "NLP analysis tasks", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.9321909546852112}]}, {"text": "Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9937724471092224}]}, {"text": "Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems.", "labels": [], "entities": []}, {"text": "Also, they can be used directly for testing paraphrase applicability (), a task that has recently become prominent in the context of textual entailment (.", "labels": [], "entities": []}, {"text": "However, polysemy is a fundamental problem for distributional models.", "labels": [], "entities": []}, {"text": "Typically, distributional models compute a single \"type\" vector fora target word, which contains cooccurrence counts for all the occurrences of the target in a large corpus.", "labels": [], "entities": []}, {"text": "If the target is polysemous, this vector mixes contextual features for all the senses of the target.", "labels": [], "entities": []}, {"text": "For example, among the top 20 features for coach, we get match and team (for the \"trainer\" sense) as well as driver and car (for the \"bus\" sense).", "labels": [], "entities": []}, {"text": "This problem has typically been approached by modifying the type vector fora target to better match a given context.", "labels": [], "entities": []}, {"text": "In the terms of research on human concept representation, which often employs feature vector representations, the use of type vectors can be understood as a prototype-based approach, which uses a single vector per category.", "labels": [], "entities": [{"text": "human concept representation", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.6754464010397593}]}, {"text": "From this angle, computing prototypes throws away much interesting distributional information.", "labels": [], "entities": []}, {"text": "A rival class of models is that of exemplar models, which memorize each seen instance of a category and perform categorization by comparing anew stimulus to each remembered exemplar vector.", "labels": [], "entities": []}, {"text": "We can address the polysemy issue through an exemplar model by simply removing all exemplars that are \"not relevant\" for the present context, or conversely activating only the relevant ones.", "labels": [], "entities": []}, {"text": "For the coach example, in the context of a text about motorways, presumably an instance like \"The coach drove a steady 45 mph\" would be activated, while \"The team lost all games since the new coach arrived\" would not.", "labels": [], "entities": []}, {"text": "In this paper, we present an exemplar-based distributional model for modeling word meaning in context, applying the model to the task of deciding paraphrase applicability.", "labels": [], "entities": []}, {"text": "With a very simple vector representation and just using activation, we outperform the state-of-the-art prototype models.", "labels": [], "entities": []}, {"text": "We perform an in-depth error analysis to identify stable parameters for this class of models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on predicting paraphrases from the Lexical Substitution (LexSub) dataset.", "labels": [], "entities": [{"text": "Lexical Substitution (LexSub) dataset", "start_pos": 57, "end_pos": 94, "type": "DATASET", "confidence": 0.5811476608117422}]}, {"text": "This dataset consists of 2000 instances of 200 target words in sentential contexts, with paraphrases for each target word instance generated by up to 6 participants.", "labels": [], "entities": []}, {"text": "Paraphrases are ranked by the number of annotators that chose them (cf..", "labels": [], "entities": []}, {"text": "Following, we take the list of paraphrase candidates fora target as given (computed by pooling all paraphrases that LexSub annotators proposed for the target) and use the models to rank them for any given sentence context.", "labels": [], "entities": []}, {"text": "As exemplars, we create bag-of-words cooccurrence vectors from the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9491046071052551}]}, {"text": "These vectors represent instances of a target word by the other words in the same sentence, lemmatized and POStagged, minus stop words.", "labels": [], "entities": []}, {"text": "E.g., if the lemma gnurge occurs twice in the BNC, once in the sentence \"The dog will gnurge the other dog\", and once in \"The old windows gnurged\", the exemplar set for gnurge contains the vectors [dog-n: 2, othera:1] and [old-a: 1, window-n: 1].", "labels": [], "entities": [{"text": "BNC", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.9533032178878784}]}, {"text": "For exemplar similarity, we use the standard Cosine similarity, and for the similarity of two exemplar sets, the Cosine of their centroids.: Activation of T or P individually on the full LexSub dataset (GAP evaluation) sion (GAP), which interpolates the precision values of top-n prediction lists for increasing n.", "labels": [], "entities": [{"text": "LexSub dataset (GAP evaluation) sion (GAP)", "start_pos": 187, "end_pos": 229, "type": "DATASET", "confidence": 0.9050837755203247}, {"text": "precision", "start_pos": 254, "end_pos": 263, "type": "METRIC", "confidence": 0.9775660634040833}]}, {"text": "Let G = q 1 , . .", "labels": [], "entities": []}, {"text": ", q m be the list of gold paraphrases with gold weights y 1 , . .", "labels": [], "entities": []}, {"text": ", y m . Let P = p 1 , . .", "labels": [], "entities": []}, {"text": ", p n be the list of model predictions as ranked by the model, and let x 1 , . .", "labels": [], "entities": []}, {"text": ", x n be the gold weights associated with them (assume  where G \u2286 P . Let I(x i ) = 1 if pi \u2208 G, and zero otherwise.", "labels": [], "entities": []}, {"text": "We write xi = 1 ii k=1 x k for the average gold weight of the first i model predictions, and analogously y i . Then Since the model may rank multiple paraphrases the same, we average over 10 random permutations of equally ranked paraphrases.", "labels": [], "entities": []}, {"text": "We report mean GAP overall items in the dataset.", "labels": [], "entities": [{"text": "GAP", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9041940569877625}]}, {"text": "We first computed two models that activate either the paraphrase or the target, but not both.", "labels": [], "entities": []}, {"text": "Model 1, actT, activates only the target, using the complete P as paraphrase, and ranking paraphrases by sim(P, act(T, s)).", "labels": [], "entities": []}, {"text": "Model 2, actP, activates only the paraphrase, using s as the target word, ranking by sim(act(P, s), s).", "labels": [], "entities": []}, {"text": "The results for these models are shown in Table 2, with both kNN and percentage activation: kNN activation with a parameter of 10 means that the 10 closest neighbors were activated, while percentage with a parameter of 10 means that the closest 10% of the exemplars were used.", "labels": [], "entities": []}, {"text": "Note first that we computed a random baseline (last row) with a GAP of 28.5.", "labels": [], "entities": [{"text": "GAP", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9942241311073303}]}, {"text": "The second-to-last row (\"no activation\") shows two more informed baselines.", "labels": [], "entities": []}, {"text": "The actT \"no act\" result (34.6) corresponds to a prototype-based model that ranks paraphrase candidates by the distance between their type vectors and the target's type vector.", "labels": [], "entities": []}, {"text": "Virtually all exemplar models outperform this prototype model.", "labels": [], "entities": []}, {"text": "Note also that both actT and actP show the best results for small values of the activation parameter.", "labels": [], "entities": []}, {"text": "This indicates paraphrases can be judged on the basis of a rather small number of exemplars.", "labels": [], "entities": []}, {"text": "Nevertheless, actT and actP differ with regard to the details of their optimal activation.", "labels": [], "entities": []}, {"text": "For actT, a small absolute number of activated exemplars works best , while actP yields the best results fora small percentage of paraphrase exemplars.", "labels": [], "entities": []}, {"text": "This can be explained by the different functions played by actT and actP (cf. Section 3): Activation of the paraphrase must allow a guess about whether there is reasonable interpretation of P in the context s.", "labels": [], "entities": []}, {"text": "This appears to require a reasonably-sized sample from P . In contrast, target activation merely has to counteract the sparsity of s, and activation of too many exemplars from T leads to oversmoothing.", "labels": [], "entities": []}, {"text": "We obtained significances by computing 95% and 99% confidence intervals with bootstrap resampling.", "labels": [], "entities": [{"text": "99% confidence intervals", "start_pos": 47, "end_pos": 71, "type": "METRIC", "confidence": 0.7351336181163788}]}, {"text": "As a rule of thumb, we find that 0.4% difference in GAP corresponds to a significant difference at the 95% level, and 0.7% difference in GAP to significance at the 99% level.", "labels": [], "entities": [{"text": "GAP", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.8716725707054138}]}, {"text": "The four activation methods (i.e., columns in are significantly different from each other, with the exception of the pair actT/kNN and actP/kNN (n.s.), so that we get the following order: where > means \"significantly outperforms\".", "labels": [], "entities": []}, {"text": "In particular, the best method (actT/kNN) outperforms all other methods at p<0.01.", "labels": [], "entities": []}, {"text": "Here, the best parameter setting (10% activation) is also significantly better than the next-one one (20% activation).", "labels": [], "entities": []}, {"text": "With the exception of actT/perc, all activation methods significantly outperform the best baseline (actP, no activation).", "labels": [], "entities": []}, {"text": "Based on these observations, we computed a third model, actTP, that activates both T (by kNN) and P (by percentage), ranking paraphrases by sim(act(P, s), act(T, s)).", "labels": [], "entities": []}, {"text": "We find the overall best model at a similar location in parameter space as for actT and actP (cf.  we fix the actP activation level, we find comparatively large performance differences between the T activation settings k=5 and k=10 (highly significant for 10% actP, and significant for 20% and 30% actP).", "labels": [], "entities": []}, {"text": "On the other hand, when we fix the actT activation level, changes in actP activation generally have an insignificant impact.", "labels": [], "entities": []}, {"text": "Somewhat disappointingly, we are notable to surpass the best result for actP alone.", "labels": [], "entities": [{"text": "actP", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.6699053049087524}]}, {"text": "This indicates that -at least in the current vector space -the sparsity of sis less of a problem than the \"dilution\" of s that we face when we representing the target word by exemplars of T close to s.", "labels": [], "entities": []}, {"text": "Note, however, that the numerically worse performance of the best actTP model is still not significantly different from the best actP model.", "labels": [], "entities": []}, {"text": "Influence of POS and frequency.", "labels": [], "entities": []}, {"text": "An analysis of the results by target part-of-speech showed that the globally optimal parameters also yield the best results for individual POS, even though there are substantial differences among POS.", "labels": [], "entities": []}, {"text": "For actT, the best results emerge for all POS with kNN activation with k between 10 and 30.", "labels": [], "entities": []}, {"text": "For k=20, we obtain a GAP of 35.3 (verbs), 38.2 (nouns), and 35.1 (adjectives).", "labels": [], "entities": [{"text": "GAP", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9977661371231079}]}, {"text": "For actP, the best parameter for all POS was activation of 10%, with GAPs of 36.9 (verbs), 41.4 (nouns), and 37.5 (adjectives).", "labels": [], "entities": [{"text": "GAPs", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9811158180236816}]}, {"text": "Interestingly, the results for actTP (verbs: 38.4, nouns: 40.6, adjectives: 36.9) are better than actP for verbs, but worse for nouns and adjectives, which indicates that the sparsity problem might be more prominent than for the other POS.", "labels": [], "entities": []}, {"text": "In all three models, we found a clear effect of target and paraphrase frequency, with deteriorating performance for the highest-frequency targets as well as for the lemmas with the highest average paraphrase frequency.", "labels": [], "entities": []}, {"text": "Many of the other models are syntax-based and are therefore only applicable to a subset of the LexSub data.", "labels": [], "entities": [{"text": "LexSub data", "start_pos": 95, "end_pos": 106, "type": "DATASET", "confidence": 0.9874581694602966}]}, {"text": "We have re-evaluated our exemplar models on the subsets we used in compare these models against our best previous exemplar models and show that our models outperform these models across the board.", "labels": [], "entities": []}, {"text": "3 Due to the small sizes of these datasets, statistical significance is more difficult to attain.", "labels": [], "entities": [{"text": "significance", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.5718599557876587}]}, {"text": "On EP09, the differences among our models are not significant, but the difference between them and the original EP09 model is.", "labels": [], "entities": [{"text": "EP09", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.9726616144180298}]}, {"text": "4 On EP08, all differences are significant except for actP vs. actTP.", "labels": [], "entities": [{"text": "EP08", "start_pos": 5, "end_pos": 9, "type": "DATASET", "confidence": 0.954839825630188}]}, {"text": "We note that both the EP08 and the EP09 datasets appear to be simpler to model than the complete Lexical Substitution dataset, at least by our exemplar-based models.", "labels": [], "entities": [{"text": "EP08", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.9403607249259949}, {"text": "EP09 datasets", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.950005054473877}, {"text": "Lexical Substitution dataset", "start_pos": 97, "end_pos": 125, "type": "DATASET", "confidence": 0.6022806862990061}]}, {"text": "This underscores an old insight: namely, that direct syntactic neighbors, such as arguments and modifiers, provide strong clues as to word sense.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Activation of T or P individually on the  full LexSub dataset (GAP evaluation)", "labels": [], "entities": [{"text": "LexSub dataset", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.9903793632984161}]}, {"text": " Table 3: Joint activation of P and T on the full  LexSub dataset (GAP evaluation)", "labels": [], "entities": [{"text": "LexSub dataset", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.9894428551197052}]}, {"text": " Table 4: Comparison to other models on two sub- sets of LexSub (GAP evaluation)", "labels": [], "entities": [{"text": "LexSub", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.981920063495636}]}]}