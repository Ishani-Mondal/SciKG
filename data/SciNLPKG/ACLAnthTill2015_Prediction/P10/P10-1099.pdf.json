{"title": [{"text": "Open-Domain Semantic Role Labeling by Modeling Word Spans", "labels": [], "entities": [{"text": "Open-Domain Semantic Role Labeling", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.555226169526577}]}], "abstractContent": [{"text": "Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data.", "labels": [], "entities": []}, {"text": "Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text.", "labels": [], "entities": [{"text": "Semantic role labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.756374180316925}]}, {"text": "We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system.", "labels": [], "entities": [{"text": "open-domain semantic role labeling", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.6222938820719719}]}, {"text": "We leverage recently-developed techniques for learning representations of text using latent-variable language models , and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 210, "end_pos": 232, "type": "TASK", "confidence": 0.673232634862264}]}, {"text": "In experiments , our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.", "labels": [], "entities": [{"text": "error", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9956395626068115}]}], "introductionContent": [{"text": "In recent semantic role labeling (SRL) competitions such as the shared tasks of, supervised SRL systems have been trained on newswire text, and then tested on both an in-domain test set (Wall Street Journal text) and an out-of-domain test set (fiction).", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8090875148773193}, {"text": "SRL", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9496724605560303}, {"text": "Wall Street Journal text", "start_pos": 187, "end_pos": 211, "type": "DATASET", "confidence": 0.9585996717214584}]}, {"text": "All systems tested on these datasets to date have exhibited a significant drop-off in performance on the out-of-domain tests, often performing 15% worse or more on the fiction test sets.", "labels": [], "entities": [{"text": "fiction test sets", "start_pos": 168, "end_pos": 185, "type": "DATASET", "confidence": 0.7734751005967458}]}, {"text": "Yet the baseline from CoNLL 2005 suggests that the fiction texts are actually easier than the newswire texts.", "labels": [], "entities": [{"text": "CoNLL 2005", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.9542551636695862}]}, {"text": "Such observations expose a weakness of current supervised natural language processing (NLP) technology for SRL: systems learn to identify semantic roles for the subset of language contained in the training data, but are not yet good at generalizing to language that has not been seen before.", "labels": [], "entities": [{"text": "supervised natural language processing (NLP)", "start_pos": 47, "end_pos": 91, "type": "TASK", "confidence": 0.7520647134099688}, {"text": "SRL", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9940599799156189}]}, {"text": "We aim to build an open-domain supervised SRL system; that is, one whose performance on out-of-domain tests approaches the same level of performance as that of state-of-the-art systems on in-domain tests.", "labels": [], "entities": [{"text": "SRL", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.8971808552742004}]}, {"text": "Importantly, an open-domain system must not use any new labeled data beyond what is included in the original training text when running on anew domain.", "labels": [], "entities": []}, {"text": "This allows the system to be ported to any new domain without any manual effort.", "labels": [], "entities": []}, {"text": "In particular, it ought to apply to arbitrary Web documents, which are drawn from a huge variety of domains.", "labels": [], "entities": []}, {"text": "Recent theoretical and empirical evidence suggests that the fault for poor performance on out-ofdomain tests lies with the representations, or sets of features, traditionally used in supervised NLP.", "labels": [], "entities": []}, {"text": "Building on recent efforts in domain adaptation, we develop unsupervised techniques for learning new representations of text.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7238966822624207}]}, {"text": "Using latent-variable language models, we learn representations of texts that provide novel kinds of features to our supervised learning algorithms.", "labels": [], "entities": []}, {"text": "Similar representations have proven useful in domain-adaptation for part-of-speech tagging and phrase chunking.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7142939865589142}, {"text": "phrase chunking", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.7900707721710205}]}, {"text": "We demonstrate how to learn representations that are effective for SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9935306906700134}]}, {"text": "Experiments on out-of-domain test sets show that our learned representations can dramatically improve out-of-domain performance, and narrow the gap between in-domain and out-of-domain performance by half.", "labels": [], "entities": []}, {"text": "The next section provides background information on learning representations for NLP tasks using latent-variable language models.", "labels": [], "entities": []}, {"text": "Section 3 presents our experimental setup for testing opendomain SRL.", "labels": [], "entities": [{"text": "opendomain SRL", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.4975081831216812}]}, {"text": "Sections 4, 5, 6 describe our SRL system: first, how we identify predicates in opendomain text, then how our baseline technique identifies and classifies arguments, and finally how we learn representations for improving argument identification and classification on out-of-domain text.", "labels": [], "entities": [{"text": "SRL", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.975727915763855}, {"text": "argument identification and classification", "start_pos": 220, "end_pos": 262, "type": "TASK", "confidence": 0.7402119189500809}]}, {"text": "Section 7 presents previous work, and Section 8 concludes and outlines directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test our open-domain semantic role labeling system using data from the CoNLL 2005 shared task ().", "labels": [], "entities": [{"text": "open-domain semantic role labeling", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.5723708048462868}, {"text": "CoNLL 2005 shared task", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.9292776882648468}]}, {"text": "We use the standard training set, consisting of sections 02-21 of the Wall Street Journal (WSJ) portion of the Penn Treebank, labeled with PropBank () annotations for predicates and arguments.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) portion of the Penn Treebank", "start_pos": 70, "end_pos": 124, "type": "DATASET", "confidence": 0.9507004510272633}]}, {"text": "We perform our tests on the Brown corpus () test data from CoNLL 2005, consisting of 3 sections (ck01-ck03) of propbanked Brown corpus data.", "labels": [], "entities": [{"text": "Brown corpus () test data from CoNLL 2005", "start_pos": 28, "end_pos": 69, "type": "DATASET", "confidence": 0.9400254860520363}, {"text": "Brown corpus data", "start_pos": 122, "end_pos": 139, "type": "DATASET", "confidence": 0.8444064259529114}]}, {"text": "This test set consists of 426 sentences containing 7,159 tokens, 804 propositions, and 2,177 arguments.", "labels": [], "entities": []}, {"text": "While the training data contains newswire text, the test sentences are drawn from the domain of \"general fiction,\" and contain an entirely different style (or styles) of English.", "labels": [], "entities": []}, {"text": "The data also includes a second test set of in-domain text (section 23 of the Treebank), which we refer to as the WSJ test set and use as a reference point.", "labels": [], "entities": [{"text": "WSJ test set", "start_pos": 114, "end_pos": 126, "type": "DATASET", "confidence": 0.9613468050956726}]}, {"text": "Every sentence in the dataset is automatically annotated with a number of NLP pipeline systems, including part-of-speech (POS) tags, phrase chunk labels, namedentity tags, and full parse information by multiple parsers.", "labels": [], "entities": []}, {"text": "These pipeline systems are important for generating features for SRL, and one key reason for the poor performance of SRL systems on the Brown corpus is that the pipeline systems themselves perform worse.", "labels": [], "entities": [{"text": "SRL", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9838306903839111}, {"text": "Brown corpus", "start_pos": 136, "end_pos": 148, "type": "DATASET", "confidence": 0.9596081078052521}]}, {"text": "The Charniak parser, for instance, drops from an F1 of 88.25 on the WSJ test to a F1 of 80.84 on the Brown corpus.", "labels": [], "entities": [{"text": "F1", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9983165264129639}, {"text": "WSJ test", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.9430627822875977}, {"text": "F1", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9928373694419861}, {"text": "Brown corpus", "start_pos": 101, "end_pos": 113, "type": "DATASET", "confidence": 0.9816924333572388}]}, {"text": "For the chunker and POS tagger, the drop-offs are less severe: 94.89 to currently have the bestperforming SRL system on the Brown corpus test set with an F1 score of 68.81 (80.8 for the WSJ test).", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 20, "end_pos": 30, "type": "TASK", "confidence": 0.7045544683933258}, {"text": "Brown corpus test set", "start_pos": 124, "end_pos": 145, "type": "DATASET", "confidence": 0.9659608155488968}, {"text": "F1 score", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9874252676963806}, {"text": "WSJ test", "start_pos": 186, "end_pos": 194, "type": "DATASET", "confidence": 0.9083738327026367}]}, {"text": "They use a discriminative reranking approach to jointly predict the best set of argument boundaries and the best set of argument labels fora predicate.", "labels": [], "entities": []}, {"text": "Like the best systems from the CoNLL 2005 shared task (), they also use features from multiple parses to remain robust in the face of parser error.", "labels": [], "entities": [{"text": "CoNLL 2005 shared task", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.9096251875162125}]}, {"text": "Owing to the established difficulty of the Brown test set and the different domains of the Brown test and WSJ training data, this dataset makes for an excellent testbed for open-domain semantic role labeling.", "labels": [], "entities": [{"text": "Brown test set", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.8242262601852417}, {"text": "WSJ training data", "start_pos": 106, "end_pos": 123, "type": "DATASET", "confidence": 0.8711151480674744}, {"text": "open-domain semantic role labeling", "start_pos": 173, "end_pos": 207, "type": "TASK", "confidence": 0.6248688399791718}]}], "tableCaptions": [{"text": " Table 1: Using HMM features in predicate iden- tification reduces error in out-of-domain tests by  34.3% overall, and by 27.1% for OOV predicates.  \"Freq\" refers to frequency in the training data.  There were 831 predicates in total; 51 never ap- peared in training and 98 appeared at most twice.", "labels": [], "entities": [{"text": "error", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9915002584457397}, {"text": "Freq", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9874956011772156}]}, {"text": " Table 2: Na\u00a8\u0131veNa\u00a8\u0131ve path features improve our base- line, but not enough to match the state-of-the-art.  Toutanova et al. do not report (NR) separate val- ues for precision and recall on this dataset. Dif- ferences in both precision and recall between the  baseline and the other systems are statistically sig- nificant at p < 0.01 using the two-tailed Fisher's  exact test.", "labels": [], "entities": [{"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9986661672592163}, {"text": "recall", "start_pos": 180, "end_pos": 186, "type": "METRIC", "confidence": 0.9934684038162231}, {"text": "Dif- ferences", "start_pos": 204, "end_pos": 217, "type": "METRIC", "confidence": 0.8854295214017233}, {"text": "precision", "start_pos": 226, "end_pos": 235, "type": "METRIC", "confidence": 0.9979928731918335}, {"text": "recall", "start_pos": 240, "end_pos": 246, "type": "METRIC", "confidence": 0.9827165603637695}]}, {"text": " Table 3: Span-HMM features significantly im- prove over state-of-the-art results in out-of- domain SRL. Differences in both precision and re- call between the baseline and the Span-HMM sys- tems are statistically significant at p < 0.01 using  the two-tailed Fisher's exact test.", "labels": [], "entities": [{"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9992421865463257}, {"text": "re- call", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9518200357755026}]}, {"text": " Table 3. All three  versions of the Span-HMM outperform Toutanova  et al.'s system on the Brown corpus, with the  Multi-Span-HMM gaining 5 points in F1. The  Multi-Span-HMM model improves over the Base- line+HMM+Paths model by 7 points in precision,  and 5.3 points in recall. Among the Span-HMM  models, the use of more states in the Span-HMM- BaseByLength model evidently outweighed the  cost of splitting the model into separate versions  for different length spans. Using multiple in- dependent copies of the Span-HMMs provides a  small (0.7) gain in precision and recall. Dif- ferences among the different Span-HMM models", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9348848462104797}, {"text": "F1", "start_pos": 150, "end_pos": 152, "type": "METRIC", "confidence": 0.9879488945007324}, {"text": "precision", "start_pos": 240, "end_pos": 249, "type": "METRIC", "confidence": 0.9991087317466736}, {"text": "recall", "start_pos": 270, "end_pos": 276, "type": "METRIC", "confidence": 0.9986670017242432}, {"text": "precision", "start_pos": 556, "end_pos": 565, "type": "METRIC", "confidence": 0.9991206526756287}, {"text": "recall", "start_pos": 570, "end_pos": 576, "type": "METRIC", "confidence": 0.997393012046814}, {"text": "Dif- ferences", "start_pos": 578, "end_pos": 591, "type": "METRIC", "confidence": 0.9481595158576965}]}, {"text": " Table 4: Multi-Span-HMM has a much smaller  drop-off in F1 than comparable systems on out- of-domain test data vs in-domain test data.", "labels": [], "entities": [{"text": "F1", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.9993600249290466}]}, {"text": " Table 5: SRL results (F1) on the Brown test corpus broken down by role type. BL is the Base- line+HMM+Paths model, MSH is the Multi-Span-HMM model. Column 8 to 16 are all adjuncts (AM-).  We omit roles with ten or fewer examples.", "labels": [], "entities": [{"text": "SRL", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.817634105682373}, {"text": "F1", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9891456365585327}, {"text": "Brown test corpus", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.9125809073448181}, {"text": "BL", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9918867945671082}]}, {"text": " Table 6: Baseline (BL) and Multi-Span-HMM  (MSH) performance on argument identification  (Id.F1) and argument classification.", "labels": [], "entities": [{"text": "Baseline (BL)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.7432466596364975}, {"text": "argument identification", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.7363231778144836}, {"text": "argument classification", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.7605535089969635}]}]}