{"title": [{"text": "Blocked Inference in Bayesian Tree Substitution Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "Learning a tree substitution grammar is very challenging due to derivational ambiguity.", "labels": [], "entities": []}, {"text": "Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al., 2009), biasing towards small grammars composed of small generalis-able productions.", "labels": [], "entities": []}, {"text": "In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sam-pler in place of the previous method's local Gibbs sampler.", "labels": [], "entities": []}, {"text": "The blocked sam-pler makes considerably larger moves than the local sampler and consequently converges in less time.", "labels": [], "entities": []}, {"text": "A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar.", "labels": [], "entities": []}, {"text": "This enables efficient blocked inference for training and also improves the parsing algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9726124405860901}]}, {"text": "Both algorithms are shown to improve parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9818940758705139}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9359510540962219}]}], "introductionContent": [{"text": "Tree Substitution Grammar (TSG) is a compelling grammar formalism which allows nonterminal rewrites in the form of trees, thereby enabling the modelling of complex linguistic phenomena such as argument frames, lexical agreement and idiomatic phrases.", "labels": [], "entities": [{"text": "Tree Substitution Grammar (TSG)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8204671790202459}]}, {"text": "A fundamental problem with TSGs is that they are difficult to estimate, even in the supervised scenario where treebanked data is available.", "labels": [], "entities": [{"text": "TSGs", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.9103966951370239}]}, {"text": "This is because treebanks are typically not annotated with their TSG derivations (how to decompose a tree into elementary tree fragments); instead the derivation needs to be inferred.", "labels": [], "entities": []}, {"text": "In recent work we proposed a TSG model which infers an optimal decomposition under a nonparametric Bayesian prior ( . This used a Gibbs sampler for training, which repeatedly samples for every node in every training tree a binary value indicating whether the node is or is not a substitution point in the tree's derivation.", "labels": [], "entities": []}, {"text": "Aggregated over the whole corpus, these values and the underlying trees specify the weighted grammar.", "labels": [], "entities": []}, {"text": "Local Gibbs samplers, although conceptually simple, suffer from slow convergence (a.k.a. poor mixing).", "labels": [], "entities": []}, {"text": "The sampler can get easily stuck because many locally improbable decisions are required to escape from a locally optimal solution.", "labels": [], "entities": []}, {"text": "This problem manifests itself both locally to a sentence and globally over the training sample.", "labels": [], "entities": []}, {"text": "The net result is a sampler that is non-convergent, overly dependent on its initialisation and cannot be said to be sampling from the posterior.", "labels": [], "entities": []}, {"text": "In this paper we present a blocked MetropolisHasting sampler for learning a TSG, similar to.", "labels": [], "entities": []}, {"text": "The sampler jointly updates all the substitution variables in a tree, making much larger moves than the local single-variable sampler.", "labels": [], "entities": []}, {"text": "A critical issue when developing a Metroplis-Hastings sampler is choosing a suitable proposal distribution, which must have the same support as the true distribution.", "labels": [], "entities": [{"text": "Metroplis-Hastings sampler", "start_pos": 35, "end_pos": 61, "type": "DATASET", "confidence": 0.9533220529556274}]}, {"text": "For our model the natural proposal distribution is a MAP point estimate, however this cannot be represented directly as it is infinitely large.", "labels": [], "entities": [{"text": "MAP point estimate", "start_pos": 53, "end_pos": 71, "type": "METRIC", "confidence": 0.7471525271733602}]}, {"text": "To solve this problem we develop a grammar transformation which can succinctly represent an infinite TSG in an equivalent finite Context Free Grammar (CFG).", "labels": [], "entities": []}, {"text": "The transformed grammar can be used as a proposal distribution, from which samples can be drawn in polynomial time.", "labels": [], "entities": []}, {"text": "Empirically, the blocked sampler converges in fewer iterations and in less time than the local Gibbs sampler.", "labels": [], "entities": []}, {"text": "In addition, we also show how the transformed grammar can be used for parsing, which yields theoretical and empirical improvements over our previous method which truncated the grammar.", "labels": [], "entities": [{"text": "parsing", "start_pos": 70, "end_pos": 77, "type": "TASK", "confidence": 0.9764324426651001}]}], "datasetContent": [{"text": "We tested our model on the Penn treebank using the same data setup as . Specifically, we used only section 2 for training and section 22 (devel) for reporting results.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.9947687685489655}]}, {"text": "Our models were all sampled for 5k iterations with hyperparameter inference for \u03b1 c and s c \u2200 c \u2208 N , but in contrast to our previous approach we did not use annealing which we did not find to help generalisation accuracy.", "labels": [], "entities": [{"text": "generalisation", "start_pos": 198, "end_pos": 212, "type": "TASK", "confidence": 0.9567158222198486}, {"text": "accuracy", "start_pos": 213, "end_pos": 221, "type": "METRIC", "confidence": 0.9417270421981812}]}, {"text": "The MH acceptance rates were in excess of 99% across both training and parsing.", "labels": [], "entities": [{"text": "MH", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8396058678627014}, {"text": "acceptance", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.882732629776001}, {"text": "parsing", "start_pos": 71, "end_pos": 78, "type": "TASK", "confidence": 0.952400267124176}]}, {"text": "All results are averages over three runs.", "labels": [], "entities": []}, {"text": "For training the blocked MH sampler exhibits faster convergence than the local Gibbs sampler, as shown in.", "labels": [], "entities": [{"text": "convergence", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.9699367880821228}]}, {"text": "Irrespective of the initialisation the blocked sampler finds higher likelihood states in many fewer iterations (the same trend continues until iteration 5k).", "labels": [], "entities": []}, {"text": "To be fair, the blocked sampler is slower per iteration (roughly 50% worse) due to the higher overheads of the grammar transform and performing dynamic programming (despite nominal optimisation).", "labels": [], "entities": []}, {"text": "Even after accounting for the time differ-.", "labels": [], "entities": []}, {"text": "The blocked sampler results in better generalisation F1 scores than the local Gibbs sampler, irrespective of the initialisation condition or parsing method used.", "labels": [], "entities": [{"text": "F1", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.8901598453521729}]}, {"text": "The use of the grammar transform in parsing also yields better scores irrespective of the underlying model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9723371267318726}]}, {"text": "Together these results strongly advocate the use of the grammar transform for inference in infinite TSGs.", "labels": [], "entities": []}, {"text": "We also trained the model on the standard Penn treebank training set (sections 2-21).", "labels": [], "entities": [{"text": "Penn treebank training set", "start_pos": 42, "end_pos": 68, "type": "DATASET", "confidence": 0.9901805967092514}]}, {"text": "We initialised the model with the final sample from a run on the small training set, and used the blocked sampler for 6500 iterations.", "labels": [], "entities": []}, {"text": "Averaged over three runs, the test F1 (section 23) was 85.3 an improveiteration than the local sampler.", "labels": [], "entities": [{"text": "F1", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9507501125335693}]}, {"text": "Our baseline 'Local maximal init' slightly exceeds previously reported score of 76.89% ( ).", "labels": [], "entities": [{"text": "Local maximal init'", "start_pos": 14, "end_pos": 33, "type": "METRIC", "confidence": 0.8663121610879898}]}, {"text": "ment over our earlier 84.0 (  although still well below state-of-the-art parsers.", "labels": [], "entities": []}, {"text": "We conjecture that the performance gap is due to the model using an overly simplistic treatment of unknown words, and also a further mixing problems with the sampler.", "labels": [], "entities": []}, {"text": "For the full data set the counts are much larger in magnitude which leads to stronger modes.", "labels": [], "entities": []}, {"text": "The sampler has difficulty escaping such modes and therefore is slower to mix.", "labels": [], "entities": []}, {"text": "One way to solve the mixing problem is for the sampler to make more global moves, e.g., with table label resampling or split-merge).", "labels": [], "entities": [{"text": "table label resampling", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.5821753740310669}]}, {"text": "Another way is to use a variational approximation instead of MCMC sampling.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Development F1 scores using the truncated pars- ing algorithm and the novel grammar transform algorithm for  four different training configurations.", "labels": [], "entities": [{"text": "F1", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.978500247001648}, {"text": "truncated pars- ing algorithm", "start_pos": 42, "end_pos": 71, "type": "METRIC", "confidence": 0.726737779378891}]}]}