{"title": [], "abstractContent": [{"text": "The pipeline of most Phrase-Based Statistical Machine Translation (PB-SMT) systems starts from automatically word aligned parallel corpus.", "labels": [], "entities": [{"text": "Phrase-Based Statistical Machine Translation (PB-SMT)", "start_pos": 21, "end_pos": 74, "type": "TASK", "confidence": 0.7114721238613129}]}, {"text": "But word appears to be too fine-grained in some cases such as non-compositional phrasal equivalences, where no clear word alignments exist.", "labels": [], "entities": []}, {"text": "Using words as inputs to PB-SMT pipeline has inborn deficiency.", "labels": [], "entities": []}, {"text": "This paper proposes pseudo-word as anew start point for PB-SMT pipeline.", "labels": [], "entities": [{"text": "PB-SMT pipeline", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.5913784205913544}]}, {"text": "Pseudo-word is a kind of basic multi-word expression that characterizes minimal sequence of consecutive words in sense of translation.", "labels": [], "entities": []}, {"text": "By casting pseudo-word searching problem into a parsing framework, we search for pseudo-words in a monolingual way and a bilingual synchronous way.", "labels": [], "entities": [{"text": "pseudo-word searching problem", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.7852289279301962}]}, {"text": "Experiments show that pseudo-word significantly outperforms word for PB-SMT model in both travel translation domain and news translation domain.", "labels": [], "entities": [{"text": "travel translation domain", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.7736005385716757}, {"text": "news translation domain", "start_pos": 120, "end_pos": 143, "type": "TASK", "confidence": 0.7622876862684885}]}], "introductionContent": [{"text": "The pipeline of most Phrase-Based Statistical Machine Translation (PB-SMT) systems starts from automatically word aligned parallel corpus generated from word-based models, proceeds with step of induction of phrase table ( or synchronous grammar and with model weights tuning step.", "labels": [], "entities": [{"text": "Phrase-Based Statistical Machine Translation (PB-SMT)", "start_pos": 21, "end_pos": 74, "type": "TASK", "confidence": 0.7108541003295353}]}, {"text": "Words are taken as inputs to PB-SMT at the very beginning of the pipeline.", "labels": [], "entities": []}, {"text": "But there is a deficiency in such manner that word is too finegrained in some cases such as non-compositional phrasal equivalences, where clear word alignments do not exist.", "labels": [], "entities": []}, {"text": "For example in Chinese-toEnglish translation, \" \u60f3 \" and \"would like to\" constitute a 1-to-n phrasal equivalence, \"\u591a\u5c11 \u94b1\" and \"how much is it\" constitute a m-to-n phrasal equivalence.", "labels": [], "entities": []}, {"text": "No clear word alignments are therein such phrasal equivalences.", "labels": [], "entities": []}, {"text": "Moreover, should basic translational unit be word or coarsegrained multi-word is an open problem for optimizing SMT models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9821826815605164}]}, {"text": "Some researchers have explored coarsegrained translational unit for machine translation.", "labels": [], "entities": [{"text": "coarsegrained translational", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.7296181619167328}, {"text": "machine translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.797269880771637}]}, {"text": "attempted to directly learn phrasal alignments instead of word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.6864388287067413}]}, {"text": "But computational complexity is prohibitively high for the exponentially large number of decompositions of a sentence pair into phrase pairs.", "labels": [], "entities": []}, {"text": "Cherry and and  used synchronous ITG () and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem.", "labels": [], "entities": []}, {"text": "induced phrasal synchronous grammar, which aimed at finding hierarchical phrasal equivalences.", "labels": [], "entities": []}, {"text": "Another direction of questioning word as basic translational unit is to directly question word segmentation on languages where word boundaries are not orthographically marked.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7842244803905487}]}, {"text": "In Chineseto-English translation task where Chinese word boundaries are not marked, used word aligner to build a Chinese dictionary to resegment Chinese sentence.", "labels": [], "entities": []}, {"text": "used a Bayesian semi-supervised method that combines Chinese word segmentation model and Chinese-to-English translation model to derive a Chinese segmentation suitable for machine translation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.5870274106661478}, {"text": "machine translation", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.7345054149627686}]}, {"text": "There are also researches focusing on the impact of various segmentation tools on machine translation ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.8186284303665161}]}, {"text": "Since there are many 1-to-n phrasal equivalences in Chinese-to-English translation (, only focusing on Chinese word as basic translational unit is not adequate to model 1-to-n translations.", "labels": [], "entities": []}, {"text": "tackle this problem by using word aligner to bootstrap bilingual segmentation suitable for machine translation.", "labels": [], "entities": [{"text": "bootstrap bilingual segmentation", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.6141660908857981}, {"text": "machine translation", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.754144698381424}]}, {"text": "detect bilingual multi-word ex-pressions by monotonically segmenting a given Spanish-English sentence pair into bilingual units, where word aligner is also used.", "labels": [], "entities": []}, {"text": "IBM model 3, 4, 5) and Deng and are another kind of related works that allow 1-to-n alignments, but they rarely questioned if such alignments exist in word units level, that is, they rarely questioned word as basic translational unit.", "labels": [], "entities": []}, {"text": "Moreover, m-ton alignments were not modeled.", "labels": [], "entities": []}, {"text": "This paper focuses on determining the basic translational units on both language sides without using word aligner before feeding them into PB-SMT pipeline.", "labels": [], "entities": []}, {"text": "We call such basic translational unit as pseudo-word to differentiate with word.", "labels": [], "entities": []}, {"text": "Pseudo-word is a kind of multi-word expression (includes both unary word and multi-word).", "labels": [], "entities": []}, {"text": "Pseudo-word searching problem is the same to decomposition of a given sentence into pseudowords.", "labels": [], "entities": []}, {"text": "We assume that such decomposition is in the Gibbs distribution.", "labels": [], "entities": []}, {"text": "We use a measurement, which characterizes pseudo-word as minimal sequence of consecutive words in sense of translation, as potential function in Gibbs distribution.", "labels": [], "entities": []}, {"text": "Note that the number of decomposition of one sentence into pseudo-words grows exponentially with sentence length.", "labels": [], "entities": []}, {"text": "By fitting decomposition problem into parsing framework, we can find optimal pseudo-word sequence in polynomial time.", "labels": [], "entities": []}, {"text": "Then we feed pseudo-words into PB-SMT pipeline, and find that pseudo-words as basic translational units improve translation performance over words as basic translational units.", "labels": [], "entities": []}, {"text": "Further experiments of removing the power of higher order language model and longer max phrase length, which are inherent in pseudowords, show that pseudo-words still improve translational performance significantly over unary words.", "labels": [], "entities": []}, {"text": "This paper is structured as follows: In section 2, we define the task of searching for pseudowords and its solution.", "labels": [], "entities": []}, {"text": "We present experimental results and analyses of using pseudo-words in PB-SMT model in section 3.", "labels": [], "entities": []}, {"text": "The conclusion is presented at section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, pseudo-words are fed into PB-SMT pipeline.", "labels": [], "entities": []}, {"text": "The pipeline uses GIZA++ model 4 ( for pseudo-word alignment, uses Moses ( as phrase-based decoder, uses the SRI Language Modeling Toolkit to train language model with modified Kneser-Ney smoothing (.", "labels": [], "entities": [{"text": "pseudo-word alignment", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7553656697273254}, {"text": "SRI Language Modeling Toolkit", "start_pos": 109, "end_pos": 138, "type": "DATASET", "confidence": 0.7910011410713196}]}, {"text": "Note that MERT is still on original words of target language.", "labels": [], "entities": [{"text": "MERT", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9374539852142334}]}, {"text": "In our experiments, pseudo-word length is limited to no more than six unary words on both sides of the language pair.", "labels": [], "entities": []}, {"text": "We conduct experiments on Chinese-toEnglish machine translation.", "labels": [], "entities": [{"text": "Chinese-toEnglish machine translation", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.5356553594271342}]}, {"text": "Two data sets are adopted, one is small corpus of IWSLT-2008 BTEC task of spoken language translation in travel domain.", "labels": [], "entities": [{"text": "IWSLT-2008 BTEC task", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.8590735594431559}, {"text": "spoken language translation", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.5837173163890839}]}, {"text": "Statistics of corpora, \"Ch\" denotes Chinese, \"En\" denotes English, \"Sent.\" row is the number of sentence pairs, \"word\" row is the number of words, \"ASL\" denotes average sentence length.", "labels": [], "entities": [{"text": "ASL", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.9315910935401917}]}, {"text": "For small corpus, we use CSTAR03 as development set, use IWSLT08 official test set for test.", "labels": [], "entities": [{"text": "CSTAR03", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.9258794188499451}, {"text": "IWSLT08 official test set", "start_pos": 57, "end_pos": 82, "type": "DATASET", "confidence": 0.9246945679187775}]}, {"text": "A 5-gram language model is trained on English side of parallel corpus.", "labels": [], "entities": []}, {"text": "For large corpus, we use NIST02 as development set, use NIST03 as test set.", "labels": [], "entities": [{"text": "NIST02", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.941980242729187}, {"text": "NIST03", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.9599806070327759}]}, {"text": "Xinhua portion of the English Gigaword3 corpus is used together with English side of large corpus to train a 4-gram language model.", "labels": [], "entities": [{"text": "English Gigaword3 corpus", "start_pos": 22, "end_pos": 46, "type": "DATASET", "confidence": 0.6426767309506735}]}, {"text": "Experimental results are evaluated by caseinsensitive BLEU-4 ().", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.990288496017456}]}, {"text": "Closest reference sentence length is used for brevity penalty.", "labels": [], "entities": []}, {"text": "Additionally, NIST score) and METEOR () are also used to check the consistency of experimental results.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.6199534833431244}, {"text": "METEOR", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9966713786125183}, {"text": "consistency", "start_pos": 67, "end_pos": 78, "type": "METRIC", "confidence": 0.984689474105835}]}, {"text": "Statistical significance in BLEU score differences was tested by paired bootstrap re-sampling).", "labels": [], "entities": [{"text": "BLEU score differences", "start_pos": 28, "end_pos": 50, "type": "METRIC", "confidence": 0.966862142086029}]}], "tableCaptions": [{"text": " Table 1. Statistics of corpora, \"Ch\" denotes Chinese,  \"En\" denotes English, \"Sent.\" row is the number of  sentence pairs, \"word\" row is the number of words,  \"ASL\" denotes average sentence length.", "labels": [], "entities": [{"text": "ASL", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.9298245906829834}]}, {"text": " Table 2. Baseline performances on test sets of small  corpus and large corpus.", "labels": [], "entities": []}, {"text": " Table 3. Performance of using pseudo-words on small data.", "labels": [], "entities": []}, {"text": " Table  4 presents its performances on small corpus.", "labels": [], "entities": []}, {"text": " Table 5. Performance of using pseudo-words on large  corpus.", "labels": [], "entities": []}, {"text": " Table 6. Performance of pseudo-word unpacking on  large corpus.", "labels": [], "entities": []}]}