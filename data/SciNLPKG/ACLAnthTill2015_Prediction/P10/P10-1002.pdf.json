{"title": [{"text": "Dependency Parsing and Projection Based on Word-Pair Classification", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6759164035320282}]}], "abstractContent": [{"text": "In this paper we describe an intuitionistic method for dependency parsing, where a classifier is used to determine whether a pair of words forms a dependency edge.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.8568359911441803}]}, {"text": "And we also propose an effective strategy for dependency projection, where the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree.", "labels": [], "entities": [{"text": "dependency projection", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.9046337306499481}]}, {"text": "Experiments show that, the classifier trained on the projected classification instances significantly out-performs previous projected dependency parsers.", "labels": [], "entities": []}, {"text": "More importantly, when this clas-sifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised dependency parsing achieves the stateof-the-art in recent years ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8113262951374054}]}, {"text": "Since it is costly and difficult to build humanannotated treebanks, a lot of works have also been devoted to the utilization of unannotated text.", "labels": [], "entities": []}, {"text": "For example, the unsupervised dependency parsing () which is totally based on unannotated data, and the semisupervised dependency parsing (  which is based on both annotated and unannotated data.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.6624984443187714}, {"text": "semisupervised dependency parsing", "start_pos": 104, "end_pos": 137, "type": "TASK", "confidence": 0.6656458377838135}]}, {"text": "Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus ().", "labels": [], "entities": []}, {"text": "For dependency projection, the relationship between words in the parsed sentences can be simply projected across the word alignment to words in the unparsed sentences, according to the DCA assumption ().", "labels": [], "entities": [{"text": "dependency projection", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.86268749833107}]}, {"text": "Such a projection procedure suffers much from the word alignment errors and syntactic isomerism between languages, which usually lead to relationship projection conflict and incomplete projected dependency structures.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.6815658807754517}]}, {"text": "To tackle this problem, use some filtering rules to reduce noise, and some hand-designed rules to handle language heterogeneity.", "labels": [], "entities": []}, {"text": "perform dependency projection and annotation adaptation with quasi-synchronous grammar features.", "labels": [], "entities": [{"text": "dependency projection", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.8417134284973145}]}, {"text": "resort to a dynamic programming procedure to search fora completed projected tree.", "labels": [], "entities": []}, {"text": "However, these strategies are all confined to the same category that dependency projection must produce completed projected trees.", "labels": [], "entities": [{"text": "dependency projection", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.8250955045223236}]}, {"text": "Because of the free translation, the syntactic isomerism between languages and word alignment errors, it would be strained to completely project the dependency structure from one language to another.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.674772247672081}]}, {"text": "We propose an effective method for dependency projection, which does not have to produce complete projected trees.", "labels": [], "entities": [{"text": "dependency projection", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.9135030210018158}]}, {"text": "Given a wordaligned bilingual corpus with source language sentences parsed, the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language.", "labels": [], "entities": []}, {"text": "A dependency relationship is a boolean value that represents whether this word pair forms a dependency edge.", "labels": [], "entities": []}, {"text": "Thus a set of classification instances are obtained.", "labels": [], "entities": []}, {"text": "Meanwhile, we propose an intuitionistic model for dependency parsing, which uses a classifier to determine whether a pair of words form a dependency edge.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8564693927764893}]}, {"text": "The classifier can then be trained on the projected classification instance set, so as to build a projected dependency parser without the need of complete projected trees.", "labels": [], "entities": []}, {"text": "Experimental results show that, the classifier trained on the projected classification instances significantly outperforms the projected dependency parsers in previous works.", "labels": [], "entities": []}, {"text": "The classifier trained on the Chinese projected classification instances achieves a precision of 58.59% on the CTB standard test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9991424083709717}, {"text": "CTB standard test set", "start_pos": 111, "end_pos": 132, "type": "DATASET", "confidence": 0.9783457964658737}]}, {"text": "More importantly, when this classifier is integrated into a 2nd-ordered maximum spanning tree (MST) dependency parser) in a weighted average manner, significant improvement is obtained over the MST baselines.", "labels": [], "entities": []}, {"text": "For the 2nd-order MST parser trained on Penn Chinese Treebank (CTB) 5.0, the classifier give an precision increment of 0.5 points.", "labels": [], "entities": [{"text": "MST parser", "start_pos": 18, "end_pos": 28, "type": "TASK", "confidence": 0.8708859384059906}, {"text": "Penn Chinese Treebank (CTB) 5.0", "start_pos": 40, "end_pos": 71, "type": "DATASET", "confidence": 0.9803859676633563}, {"text": "precision increment", "start_pos": 96, "end_pos": 115, "type": "METRIC", "confidence": 0.9795496761798859}]}, {"text": "Especially for the parser trained on the smaller CTB 1.0, more than 1 points precision increment is obtained.", "labels": [], "entities": [{"text": "CTB 1.0", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.8727062344551086}, {"text": "precision increment", "start_pos": 77, "end_pos": 96, "type": "METRIC", "confidence": 0.9402714967727661}]}, {"text": "In the rest of this paper, we first describe the word-pair classification model for dependency parsing (section 2) and the generation method of projected classification instances (section 3).", "labels": [], "entities": [{"text": "word-pair classification", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.7268014550209045}, {"text": "dependency parsing", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.8311145901679993}]}, {"text": "Then we describe an application of the projected parser: boosting a state-of-the-art 2nd-ordered MST parser (section 4).", "labels": [], "entities": [{"text": "MST parser", "start_pos": 97, "end_pos": 107, "type": "TASK", "confidence": 0.787560224533081}]}, {"text": "After the comparisons with previous works on dependency parsing and projection, we finally five the experimental results.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8970405459403992}]}], "datasetContent": [{"text": "In this section, we first validate the word-pair classification model by experimenting on humanannotated treebanks.", "labels": [], "entities": [{"text": "word-pair classification", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.77883580327034}]}, {"text": "Then we investigate the effectiveness of the dependency projection by evaluating the projected classifiers trained on the projected classification instances.", "labels": [], "entities": [{"text": "dependency projection", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.8415611386299133}]}, {"text": "Finally, we report the performance of the integrated dependency parser which integrates the projected classifier and the 2nd-ordered MST dependency parser.", "labels": [], "entities": [{"text": "MST dependency parser", "start_pos": 133, "end_pos": 154, "type": "TASK", "confidence": 0.6665952205657959}]}, {"text": "We evaluate the parsing accuracy by the precision of lexical heads, which is the percentage of the words that have found their correct parents.", "labels": [], "entities": [{"text": "parsing", "start_pos": 16, "end_pos": 23, "type": "TASK", "confidence": 0.9480143189430237}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9503483772277832}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9991759657859802}]}], "tableCaptions": [{"text": " Table 2: The corpus partition for WSJ and CTB  5.0.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.8876373767852783}, {"text": "CTB  5.0", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.8059861361980438}]}, {"text": " Table 3: Performance of the word-pair classifica- tion model on WSJ and CTB 5.0, compared with  the current state-of-the-art models.", "labels": [], "entities": [{"text": "WSJ and CTB 5.0", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.8017749190330505}]}, {"text": " Table 4: The performance of the projected classi- fier on the test sets of CTB 2.0 and CTB 5.0, com- pared with the performance of previous works on  the corresponding test sets.", "labels": [], "entities": [{"text": "CTB 2.0", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.9412662386894226}, {"text": "CTB 5.0", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.8505388796329498}]}, {"text": " Table 5: Performance improvement brought by  the projected classifier to the baseline 2nd-ordered  MST parsers trained on CTB 1.0 and CTB 5.0, re- spectively.", "labels": [], "entities": [{"text": "CTB 1.0", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.9348093271255493}, {"text": "CTB 5.0", "start_pos": 135, "end_pos": 142, "type": "DATASET", "confidence": 0.860002338886261}]}]}