{"title": [{"text": "\"Ask not what Textual Entailment can do for You...\"", "labels": [], "entities": []}], "abstractContent": [{"text": "We challenge the NLP community to participate in a large-scale, distributed effort to design and build resources for developing and evaluating solutions to new and existing NLP tasks in the context of Recognizing Textual Entailment.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment", "start_pos": 201, "end_pos": 231, "type": "TASK", "confidence": 0.824308435122172}]}, {"text": "We argue that the single global label with which RTE examples are annotated is insufficient to effectively evaluate RTE system performance ; to promote research on smaller, related NLP tasks, we believe more detailed annotation and evaluation are needed, and that this effort will benefit not just RTE researchers, but the NLP community as a whole.", "labels": [], "entities": [{"text": "RTE system", "start_pos": 116, "end_pos": 126, "type": "TASK", "confidence": 0.884272426366806}]}, {"text": "We use insights from successful RTE systems to propose a model for identifying and annotating textual inference phenomena in textual entailment examples , and we present the results of a pilot annotation study that show this model is feasible and the results immediately useful .", "labels": [], "entities": []}], "introductionContent": [{"text": "Much of the work in the field of Natural Language Processing is founded on an assumption of semantic compositionality: that there are identifiable, separable components of an unspecified inference process that will develop as research in NLP progresses.", "labels": [], "entities": []}, {"text": "Tasks such as Named Entity and coreference resolution, syntactic and shallow semantic parsing, and information and relation extraction have been identified as worthwhile tasks and pursued by numerous researchers.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.956367164850235}, {"text": "syntactic and shallow semantic parsing", "start_pos": 55, "end_pos": 93, "type": "TASK", "confidence": 0.6421134531497955}, {"text": "information and relation extraction", "start_pos": 99, "end_pos": 134, "type": "TASK", "confidence": 0.6001565754413605}]}, {"text": "While many have (nearly) immediate application to real world tasks like search, many are also motivated by their potential contribution to more ambitious Natural Language tasks.", "labels": [], "entities": []}, {"text": "It is clear that the components/tasks identified so far do not suffice in themselves to solve tasks requiring more complex reasoning and synthesis of information; many other tasks must be solved to achieve human-like performance on tasks such as Question Answering.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 246, "end_pos": 264, "type": "TASK", "confidence": 0.7983509302139282}]}, {"text": "But there is no clear process for identifying potential tasks (other than consensus by a sufficient number of researchers), nor for quantifying their potential contribution to existing NLP tasks, let alone to Natural Language Understanding.", "labels": [], "entities": [{"text": "Natural Language Understanding", "start_pos": 209, "end_pos": 239, "type": "TASK", "confidence": 0.6461086372534434}]}, {"text": "Recent \"grand challenges\" such as Learning by Reading, Learning To Read, and Machine Reading are prompting more careful thought about the way these tasks relate, and what tasks must be solved in order to understand text sufficiently well to reliably reason with it.", "labels": [], "entities": [{"text": "Machine Reading", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.8209720849990845}]}, {"text": "This is an appropriate time to consider a systematic process for identifying semantic analysis tasks relevant to natural language understanding, and for assessing their potential impact on NLU system performance.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 113, "end_pos": 143, "type": "TASK", "confidence": 0.6104813814163208}]}, {"text": "Research on Recognizing Textual Entailment (RTE), largely motivated by a \"grand challenge\" now in its sixth year, has already begun to address some of the problems identified above.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 12, "end_pos": 48, "type": "TASK", "confidence": 0.7766783535480499}]}, {"text": "Techniques developed for RTE have now been successfully applied in the domains of Question Answering () and Machine Translation (,).", "labels": [], "entities": [{"text": "RTE", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9272316098213196}, {"text": "Question Answering", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.8112753331661224}, {"text": "Machine Translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.8411488831043243}]}, {"text": "The RTE challenge examples are drawn from multiple domains, providing a relatively task-neutral setting in which to evaluate contributions of different component solutions, and RTE researchers have already made incremental progress by identifying sub-problems of entailment, and developing ad-hoc solutions for them.", "labels": [], "entities": []}, {"text": "In this paper we challenge the NLP community to contribute to a joint, long-term effort to identify, formalize, and solve textual inference problems motivated by the Recognizing Textual Entailment setting, in the following ways: (a) Making the Recognizing Textual Entailment setting a central component of evaluation for relevant NLP tasks such as NER, Coreference, parsing, data acquisition and application, and others.", "labels": [], "entities": [{"text": "NER", "start_pos": 348, "end_pos": 351, "type": "TASK", "confidence": 0.9373157024383545}, {"text": "parsing", "start_pos": 366, "end_pos": 373, "type": "TASK", "confidence": 0.8006957173347473}, {"text": "data acquisition and application", "start_pos": 375, "end_pos": 407, "type": "TASK", "confidence": 0.8133533596992493}]}, {"text": "While many \"component\" tasks are considered (almost) solved in terms of expected improvements in performance on task-specific corpora, it is not clear that this translates to strong performance in the RTE domain, due either to problems arising from unrelated, unsolved entailment phenomena that co-occur in the same examples, or to domain change effects.", "labels": [], "entities": []}, {"text": "The RTE task offers an application-driven setting for evaluating abroad range of NLP solutions, and will reinforce good practices by NLP researchers.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.6776744723320007}]}, {"text": "The RTE task has been designed specifically to exercise textual inference capabilities, in a format that would make RTE systems potentially useful components in other \"deep\" NLP tasks such as Question Answering and Machine Translation.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.8643677830696106}, {"text": "Question Answering", "start_pos": 192, "end_pos": 210, "type": "TASK", "confidence": 0.850987434387207}, {"text": "Machine Translation", "start_pos": 215, "end_pos": 234, "type": "TASK", "confidence": 0.8046882450580597}]}, {"text": "1 (b) Identifying relevant linguistic phenomena, interactions between phenomena, and their likely impact on RTE/textual inference.", "labels": [], "entities": [{"text": "Identifying relevant linguistic phenomena", "start_pos": 6, "end_pos": 47, "type": "TASK", "confidence": 0.8770194798707962}, {"text": "RTE/textual inference", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.9016434848308563}]}, {"text": "Determining the correct label fora single textual entailment example requires human analysts to make many smaller, localized decisions which may depend on each other.", "labels": [], "entities": []}, {"text": "A broad, carefully conducted effort to identify and annotate such local phenomena in RTE corpora would allow their distributions in RTE examples to be quantified, and allow evaluation of NLP solutions in the context of RTE.", "labels": [], "entities": []}, {"text": "It would also allow assessment of the potential impact of a solution to a specific sub-problem on the RTE task, and of interactions between phenomena.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 102, "end_pos": 110, "type": "TASK", "confidence": 0.9256254434585571}]}, {"text": "Such phenomena will almost certainly correspond to elements of linguistic theory; but this approach brings a data-driven approach to focus attention on those phenomena that are well-represented in the RTE corpora, and which can be identified with sufficiently close agreement.", "labels": [], "entities": []}, {"text": "(c) Developing resources and approaches that allow more detailed assessment of RTE systems.", "labels": [], "entities": [{"text": "RTE", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.8813911080360413}]}, {"text": "At present, it is hard to know what specific capabilities different RTE systems have, and hence, which aspects of successful systems are worth emulating or reusing.", "labels": [], "entities": []}, {"text": "An evaluation framework that could offer insights into the kinds of sub-problems a given system can reliably solve would make it easier to identify significant advances, and thereby promote more rapid advances through reuse of successful solutions and focus on unresolved problems.", "labels": [], "entities": []}, {"text": "In this paper we demonstrate that Textual Entailment systems are already \"interesting\", in that they have made significant progress beyond a \"smart\" lexical baseline that is surprisingly hard to beat (section 2).", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8367513120174408}]}, {"text": "We argue that Textual Entailment, as an application that clearly requires sophisticated textual inference to perform well, requires the solution of a range of sub-problems, some familiar and some not yet known.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8351059854030609}]}, {"text": "We therefore propose RTE as a promising and worthwhile task for large-scale community involvement, as it motivates the study of many other NLP problems in the context of general textual inference.", "labels": [], "entities": [{"text": "RTE", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9755919575691223}]}, {"text": "We outline the limitations of the present model of evaluation of RTE performance, and identify kinds of evaluation that would promote understanding of the way individual components can impact Textual Entailment system performance, and allow better objective evaluation of RTE system behavior without imposing additional burdens on RTE participants.", "labels": [], "entities": [{"text": "RTE performance", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.8554792702198029}]}, {"text": "We use this to motivate a large-scale annotation effort to provide data with the mark-up sufficient to support these goals.", "labels": [], "entities": []}, {"text": "To stimulate discussion of suitable annotation and evaluation models, we propose a candidate model, and provide results from a pilot annotation effort (section 3).", "labels": [], "entities": []}, {"text": "This pilot study establishes the feasibility of an inference-motivated annotation effort, and its results offer a quantitative insight into the difficulty of the TE task, and the distribution of a number of entailment-relevant linguistic phenomena over a representative sample from the NIST TAC RTE 5 challenge corpus.", "labels": [], "entities": [{"text": "TE task", "start_pos": 162, "end_pos": 169, "type": "TASK", "confidence": 0.9309235215187073}, {"text": "NIST TAC RTE 5 challenge corpus", "start_pos": 286, "end_pos": 317, "type": "DATASET", "confidence": 0.9350625574588776}]}, {"text": "We argue that such an evaluation and annotation effort can identify relevant subproblems whose solution will benefit not only Textual Entailment but a range of other long-standing NLP tasks, and can stimulate development of new ones.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7021000683307648}]}, {"text": "We also show how this data can be used to investigate the behavior of some of the highest-scoring RTE systems from the most recent challenge (section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "An ablation study that formed part of the official RTE 5 evaluation attempted to evaluate the contribution of publicly available knowledge resources such as WordNet, VerbOcean (), and DIRT () used by many of the systems.", "labels": [], "entities": [{"text": "RTE 5 evaluation", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.8433056076367696}, {"text": "WordNet", "start_pos": 157, "end_pos": 164, "type": "DATASET", "confidence": 0.9732632040977478}]}, {"text": "The observed contribution was inmost cases limited or non-existent.", "labels": [], "entities": []}, {"text": "It is premature, however, to conclude that these resources have little potential impact on RTE system performance: most RTE researchers agree that the real contribution of individual resources is difficult to assess.", "labels": [], "entities": [{"text": "RTE", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9658913612365723}]}, {"text": "As the example in illustrates, most RTE examples require a number of phenomena to be correctly resolved in order to reliably determine the correct label (the Interaction problem); a perfect coreference resolver might as a result yield little improvement on the standard RTE evaluation, even though coreference resolution is clearly required by human readers in a significant percentage of RTE examples.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 190, "end_pos": 210, "type": "TASK", "confidence": 0.8284026980400085}, {"text": "coreference resolution", "start_pos": 298, "end_pos": 320, "type": "TASK", "confidence": 0.8153745532035828}]}, {"text": "Various efforts have been made by individual research teams to address specific capabilities that are intuitively required for good RTE performance, such as (de, and the formal treatment of entailment phenomena in () depends on and formalizes a divide-and-conquer approach to entailment resolution.", "labels": [], "entities": [{"text": "RTE", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.9715006351470947}, {"text": "entailment resolution", "start_pos": 276, "end_pos": 297, "type": "TASK", "confidence": 0.8611354827880859}]}, {"text": "But the phenomena-specific capabilities described in these approaches are far from complete, and many are not yet invented.", "labels": [], "entities": []}, {"text": "To devote real effort to identify and develop such capabilities, researchers must be confident that the resources (and the will!) exist to create and evaluate their solutions, and that the resource can be shown to be relevant to a sufficiently large subset of the NLP community.", "labels": [], "entities": []}, {"text": "While there is widespread belief that there are many relevant entailment phenomena, though each individually maybe relevant to relatively few RTE examples (the Sparseness problem), we know of no systematic analysis to determine what those phenomena are, and how sparsely represented they are in existing RTE data.", "labels": [], "entities": []}, {"text": "If it were even known what phenomena were relevant to specific entailment examples, it might be possible to more accurately distinguish system capabilities, and promote adoption of successful solutions to sub-problems.", "labels": [], "entities": []}, {"text": "An annotation-side solution also maintains the desirable agnosticism of the RTE problem formulation, by not imposing the requirement on system developers of generating an explanation for each answer.", "labels": [], "entities": [{"text": "RTE problem formulation", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.8129353324572245}]}, {"text": "Of course, if examples were also annotated with explanations in a consistent format, this could form the basis of anew evaluation of the kind essayed in the pilot study in ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top performing systems in the RTE 5 2- way task.", "labels": [], "entities": [{"text": "RTE 5 2- way task", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.5244759370883306}]}, {"text": " Table 2: In each cell, top row shows observed  agreement and bottom row shows the number of  correct (positive, negative) examples on which the  pair of systems agree.", "labels": [], "entities": []}, {"text": " Table 3: Occurrence statistics for domains in the  annotated data.", "labels": [], "entities": [{"text": "Occurrence", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9484076499938965}]}, {"text": " Table 4: Occurrence statistics for hypothesis struc- ture features.", "labels": [], "entities": []}, {"text": " Table 5: Occurrence statistics for entailment phe- nomena and knowledge resources", "labels": [], "entities": []}, {"text": " Table 6: Occurrences of negative-only phenomena", "labels": [], "entities": []}, {"text": " Table 7: Accuracy in predicting the label based on the phenomena and top-5 system labels.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9929747581481934}]}]}