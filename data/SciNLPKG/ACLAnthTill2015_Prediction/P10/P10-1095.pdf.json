{"title": [{"text": "A new Approach to Improving Multilingual Summarization using a Genetic Algorithm", "labels": [], "entities": [{"text": "Approach", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9114383459091187}, {"text": "Improving Multilingual Summarization", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.8814425468444824}]}], "abstractContent": [{"text": "Automated summarization methods can be defined as \"language-independent,\" if they are not based on any language-specific knowledge.", "labels": [], "entities": []}, {"text": "Such methods can be used for multilingual summarization defined by Mani (2001) as \"processing several languages, with summary in the same language as input.\"", "labels": [], "entities": [{"text": "multilingual summarization", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.5084656625986099}]}, {"text": "In this paper , we introduce MUSE, a language-independent approach for extractive sum-marization based on the linear optimization of several sentence ranking measures using a genetic algorithm.", "labels": [], "entities": [{"text": "MUSE", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.7186828851699829}]}, {"text": "We tested our methodology on two languages-English and Hebrew-and evaluated its performance with ROUGE-1 Recall vs. state-of-the-art extractive summarization approaches.", "labels": [], "entities": [{"text": "ROUGE-1 Recall", "start_pos": 97, "end_pos": 111, "type": "METRIC", "confidence": 0.8208889365196228}, {"text": "extractive summarization", "start_pos": 133, "end_pos": 157, "type": "TASK", "confidence": 0.6633268594741821}]}, {"text": "Our results show that MUSE performs better than the best known multilingual approach (TextRank 1) in both languages.", "labels": [], "entities": [{"text": "MUSE", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.7824863195419312}]}, {"text": "Moreover, our experimental results on a bilingual (English and Hebrew) document collection suggest that MUSE does not need to be retrained on each language and the same model can be used across at least two different languages.", "labels": [], "entities": [{"text": "MUSE", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.6235940456390381}]}], "introductionContent": [{"text": "Document summaries should use a minimum number of words to express a document's main ideas.", "labels": [], "entities": [{"text": "Document summaries", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8168318867683411}]}, {"text": "As such, high quality summaries can significantly reduce the information overload many professionals in a variety of fields must contend We evaluated several summarizers-SUMMA, MEAD, Microsoft Word Autosummarize and TextRank-on the DUC 2002 corpus.", "labels": [], "entities": [{"text": "TextRank-on the DUC 2002 corpus", "start_pos": 216, "end_pos": 247, "type": "DATASET", "confidence": 0.8350183248519898}]}, {"text": "Our results show that TextRank performed best.", "labels": [], "entities": [{"text": "TextRank", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.8670436143875122}]}, {"text": "In addition, TextRank can be considered languageindependent as long as it does not perform any morphological analysis. with on a daily basis (, assist in the automated classification and filtering of documents, and increase search engines precision.", "labels": [], "entities": [{"text": "automated classification and filtering of documents", "start_pos": 158, "end_pos": 209, "type": "TASK", "confidence": 0.7608796954154968}, {"text": "precision", "start_pos": 239, "end_pos": 248, "type": "METRIC", "confidence": 0.9962072372436523}]}, {"text": "Automated summarization methods can use different levels of linguistic analysis: morphological, syntactic, semantic and discourse/pragmatic.", "labels": [], "entities": []}, {"text": "Although the summary quality is expected to improve when a summarization technique includes language specific knowledge, the inclusion of that knowledge impedes the use of the summarizer on multiple languages.", "labels": [], "entities": [{"text": "summarization", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.9822289943695068}]}, {"text": "Only systems that perform equally well on different languages without language-specific knowledge (including linguistic analysis) can be considered language-independent summarizers.", "labels": [], "entities": []}, {"text": "The publication of information on the Internet in an ever-increasing variety of languages 2 dictates the importance of developing multilingual summarization approaches.", "labels": [], "entities": []}, {"text": "There is a particular need for language-independent statistical techniques that can be readily applied to text in any language without depending on language-specific linguistic tools.", "labels": [], "entities": []}, {"text": "In the absence of such techniques, the only alternative to language-independent summarization would be the labor-intensive translation of the entire document into a common language.", "labels": [], "entities": [{"text": "summarization", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.7546513676643372}]}, {"text": "Here we introduce MUSE (MUltilingual Sentence Extractor), anew approach to multilingual single-document extractive summarization where summarization is considered as an optimization or a search problem.", "labels": [], "entities": [{"text": "MUSE", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.6551167964935303}, {"text": "MUltilingual Sentence Extractor)", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.7065184786915779}, {"text": "multilingual single-document extractive summarization", "start_pos": 75, "end_pos": 128, "type": "TASK", "confidence": 0.6121462136507034}, {"text": "summarization", "start_pos": 135, "end_pos": 148, "type": "TASK", "confidence": 0.958423912525177}]}, {"text": "We use a Genetic Algorithm (GA) to find an optimal weighted linear combination of 31 statistical sentence scoring methods that are all language-independent and are based on either a vector or a graph representation of a document, where both representations are based on a word segmentation.", "labels": [], "entities": [{"text": "statistical sentence scoring", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.6637537876764933}]}, {"text": "We have evaluated our approach on two monolingual corpora of English and Hebrew documents and, additionally, on one bilingual corpora comprising English and Hebrew documents.", "labels": [], "entities": []}, {"text": "Our evaluation experiments sought to -Compare the GA-based approach for singledocument extractive summarization (MUSE) to the best known sentence scoring methods.", "labels": [], "entities": [{"text": "singledocument extractive summarization (MUSE)", "start_pos": 72, "end_pos": 118, "type": "TASK", "confidence": 0.78650863468647}]}, {"text": "-Determine whether the same weighting model is applicable across two different languages.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section describes the related work in statistical extractive summarization.", "labels": [], "entities": [{"text": "statistical extractive summarization", "start_pos": 47, "end_pos": 83, "type": "TASK", "confidence": 0.859354555606842}]}, {"text": "Section 3 introduces MUSE, the GA-based approach to multilingual single-document extractive summarization.", "labels": [], "entities": [{"text": "MUSE", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.7436162233352661}, {"text": "multilingual single-document extractive summarization", "start_pos": 52, "end_pos": 105, "type": "TASK", "confidence": 0.6497064530849457}]}, {"text": "Section 4 presents our experimental results on monolingual and bilingual corpora.", "labels": [], "entities": []}, {"text": "Our conclusions and suggestions for future work comprise the final section.", "labels": [], "entities": []}], "datasetContent": [{"text": "The English text material we used in our experiments comprised the corpus of summarized documents available to the single document summarization task at the Document Understanding Conference, 2002.", "labels": [], "entities": [{"text": "single document summarization task at the Document Understanding Conference", "start_pos": 115, "end_pos": 190, "type": "TASK", "confidence": 0.6451921727922227}]}, {"text": "This benchmark dataset contains 533 news articles, each accompanied by two to three human-generated abstracts of approximately 100 words each.", "labels": [], "entities": []}, {"text": "For the Hebrew language, however, to the best of our knowledge, no summarization benchmarks exist.", "labels": [], "entities": []}, {"text": "To generate a corpus of summarized Hebrew texts, therefore, we setup an experiment where human assessors were given 50 news articles of 250 to 830 words each from the Website of the Haaretz newspaper.", "labels": [], "entities": []}, {"text": "8 All assessors were provided with the Tool Assisting Human Assessors (TAHA) software tool 9 that enables sentences to be easily selected and stored for later inclusion in the document extract.", "labels": [], "entities": [{"text": "Tool Assisting Human Assessors (TAHA)", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.7502922458308083}]}, {"text": "In total, 70 undergraduate students from the Department of Information Systems Engineering, Ben Gurion University of the Negev participated in the experiment.", "labels": [], "entities": [{"text": "Information Systems Engineering", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.6744262476762136}]}, {"text": "Each student participant was randomly assigned ten different documents and instructed to (1) spend at least five minutes on each document, (2) ignore dialogs and quotations, (3) read the whole document before beginning sentence extraction, (4) ignore redundant, repetitive, and overly detailed information, and (5) remain within the minimal and maximal summary length constraints (95 and 100 words, respectively).", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 219, "end_pos": 238, "type": "TASK", "confidence": 0.7147909551858902}]}, {"text": "Summaries were assessed for quality by comparing each student's summary to those of all the other students using the ROUGE evalua- Although the same set of splitting rules maybe used for many different languages, separate splitters were used for English and Hebrew because the MEAD splitter tool is restricted to European languages.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 117, "end_pos": 122, "type": "METRIC", "confidence": 0.9208446145057678}, {"text": "MEAD splitter", "start_pos": 277, "end_pos": 290, "type": "TASK", "confidence": 0.680415004491806}]}, {"text": "8 http://www.haaretz.co.il 9 TAHA can be provided upon request tion toolkit adapted to Hebrew 10 and the ROUGE-1 metric (.", "labels": [], "entities": [{"text": "TAHA", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9927377104759216}, {"text": "ROUGE-1", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9184523224830627}]}, {"text": "We filtered all the summaries produced by assessors that received average ROUGE score below 0.5, i. e. agreed with the rest of assessors in less than 50% of cases.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.9852536618709564}]}, {"text": "Finally, our corpus of summarized Hebrew texts was compiled from the summaries of about 60% of the most consistent assessors, with an average of seven extracts per single document . The ROUGE scores of the selected assessors are distributed between 50 and 57 percents.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 186, "end_pos": 191, "type": "METRIC", "confidence": 0.9947035908699036}]}, {"text": "The third, bilingual, experimental corpus was assembled from documents in both languages.", "labels": [], "entities": []}, {"text": "We evaluated English and Hebrew summaries using ROUGE-1, 2, 3, 4, L, SU and W metrics, described in.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9537885189056396}]}, {"text": "In agreement with Lin's (2004) conclusion, our results for the different metrics were not statistically distinguishable.", "labels": [], "entities": []}, {"text": "However, ROUGE-1 showed the largest variation across the methods.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.8760532140731812}]}, {"text": "In the following comparisons, all results are presented in terms of the ROUGE-1 Recall metric.", "labels": [], "entities": [{"text": "ROUGE-1 Recall metric", "start_pos": 72, "end_pos": 93, "type": "METRIC", "confidence": 0.8434093395868937}]}, {"text": "We estimated the ROUGE metric using 10-fold cross validation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.9810805916786194}]}, {"text": "The results of training and testing comprise the average ROUGE values obtained for English, Hebrew, and bilingual corpora.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9938960671424866}]}, {"text": "Since we experimented with a different number of English and Hebrew documents (533 and 50, respectively), we have created 10 balanced bilingual corpora, each with the same number of English and Hebrew documents, by combining approximately 50 randomly selected English documents with all 50 Hebrew documents.", "labels": [], "entities": []}, {"text": "Each corpus was then subjected to 10-fold cross validation, and the average results for training and testing were calculated.", "labels": [], "entities": []}, {"text": "We compared our approach (1) with a multilingual version of TextRank (denoted by ML TR) () as the best known multilingual summarizer, (2) with Microsoft Word's Autosummarize function 12 (denoted by MS SUM) as a widely used commercial summa- The regular expressions specifying \"word\" were adapted to Hebrew alphabet.", "labels": [], "entities": []}, {"text": "The same toolkit was used for summaries evaluation on rizer, and (3) with the best single scoring method in each corpus.", "labels": [], "entities": [{"text": "summaries evaluation", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.9304606020450592}]}, {"text": "As a baseline, we compiled summaries created from the initial sentences (denoted by POS F).", "labels": [], "entities": [{"text": "POS F", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.9106975793838501}]}, {"text": "shows the comparative results (ROUGE mean values) for English, Hebrew, and bilingual corpora, with the best summarizers on top.", "labels": [], "entities": [{"text": "ROUGE mean values", "start_pos": 31, "end_pos": 48, "type": "METRIC", "confidence": 0.9678192933400472}]}, {"text": "Pairwise comparisons between summarizers indicated that all methods (except POS F and ML TR in the English and bilingual corpora and D COV J and POS F in the Hebrew corpus) were significantly different at the 95% confidence level.", "labels": [], "entities": []}, {"text": "MUSE performed significantly better than TextRank in all three corpora and better than the best single methods COV DEG in English and D COV J in Hebrew corpora respectively.", "labels": [], "entities": [{"text": "COV DEG", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.8442188203334808}]}, {"text": "Two sets of features-the full set of 31 sentence scoring metrics and the 10 best bilingual metrics determined in our previous work 13 using a clustering analysis of the methods results on both corpora-were tested on the bilingual corpus.", "labels": [], "entities": []}, {"text": "The experimental results show that the optimized combination of the 10 best metrics is not significantly distinguishable from the best single metric in the multilingual corpus -COV DEG.", "labels": [], "entities": [{"text": "COV DEG", "start_pos": 177, "end_pos": 184, "type": "DATASET", "confidence": 0.8852652013301849}]}, {"text": "The difference between the combination of all 31 metrics and COV DEG is significant only with a onetailed p-value of 0.0798 (considered not very significant).", "labels": [], "entities": [{"text": "COV DEG", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.4636436253786087}]}, {"text": "Both combinations significantly outperformed all the other summarizers that were compared.", "labels": [], "entities": []}, {"text": "contains the results of MUSEtrained weights for all 31 metrics.", "labels": [], "entities": [{"text": "MUSEtrained", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.6777482032775879}]}, {"text": "Our experiments showed that the removal of highly-correlated metrics (the metric with the lower ROUGE value out of each pair of highlycorrelated metrics) from the linear combination slightly improved summarization quality, but the improvement was not statistically significant.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9949674010276794}, {"text": "summarization", "start_pos": 200, "end_pos": 213, "type": "TASK", "confidence": 0.9825546145439148}]}, {"text": "Discarding bottom ranked features (up to 50%), also, did not affect the results significantly.", "labels": [], "entities": []}, {"text": "shows the best vectors generated from training MUSE on all the documents in the English, Hebrew, and multilingual (one of 10 balanced) corpora and their ROUGE training scores and number of GA iterations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 153, "end_pos": 158, "type": "METRIC", "confidence": 0.9698372483253479}]}, {"text": "While the optimal values of the weights are expected to be nonnegative, among the actual results are some negative values.", "labels": [], "entities": []}, {"text": "Although there is no simple explanation for this outcome, it maybe related to a well-known phenomenon from Numerical Analysis called over-relaxation (Friedman submitted to publication.", "labels": [], "entities": [{"text": "Numerical Analysis", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.730634868144989}]}, {"text": "For example, Laplace equation \u03c6 xx + \u03c6 yy = 0 is iteratively solved over a grid of points as follows: At each grid point let \u03c6 (n) , \u03c6 (n) denote then th iteration as calculated from the differential equation and its modified final value, respectively.", "labels": [], "entities": []}, {"text": "The final value is chosen as \u03c9\u03c6 (n) + (1 \u2212 \u03c9)\u03c6 (n\u22121) . While the sum of the two weights is obviously 1, the optimal value of \u03c9, which minimizes the number of iterations needed for convergence, usually satisfies 1 < \u03c9 < 2 (i.e., the second weight 1 \u2212 \u03c9 is negative) and approaches 2 the finer the grid gets.", "labels": [], "entities": []}, {"text": "Though somewhat unexpected, this surprising result can be rigorously proved.", "labels": [], "entities": []}, {"text": "Assuming efficient implementation, most metrics have a linear computational complexity relative to the total number of words in a document -O(n).", "labels": [], "entities": []}, {"text": "As a result, MUSE total computation time, given a trained model, is also linear (at factor of the number of metrics in a combination).", "labels": [], "entities": [{"text": "MUSE total computation time", "start_pos": 13, "end_pos": 40, "type": "METRIC", "confidence": 0.7305921763181686}]}, {"text": "The training time is proportional to the number of GA iterations multiplied by the number of individuals in a population times the fitness evaluation (ROUGE) time.", "labels": [], "entities": [{"text": "fitness evaluation (ROUGE) time", "start_pos": 131, "end_pos": 162, "type": "METRIC", "confidence": 0.8589231073856354}]}, {"text": "On average, in our experiments the GA performed 5 \u2212 6 iterations-selection and reproduction-before reaching convergence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of 10-fold cross validation  ENG  HEB MULT  Train 0.4483 0.5993 0.5205  Test  0.4461 0.5936 0.5027", "labels": [], "entities": [{"text": "ENG  HEB MULT  Train 0.4483 0.5993 0.5205  Test  0.4461 0.5936 0.5027", "start_pos": 47, "end_pos": 116, "type": "METRIC", "confidence": 0.6850084906274622}]}, {"text": " Table 4: Summarization performance. Mean  ROUGE-1", "labels": [], "entities": [{"text": "Mean  ROUGE-1", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.8257821798324585}]}, {"text": " Table 5: Induced weights for the best linear com- bination of scoring metrics", "labels": [], "entities": []}]}