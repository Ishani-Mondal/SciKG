{"title": [], "abstractContent": [{"text": "Despite the existence of several noun phrase coref-erence resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different corefer-ence resolution systems.", "labels": [], "entities": [{"text": "coref-erence resolution", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.7276125252246857}]}, {"text": "This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.8729906976222992}]}, {"text": "Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios.", "labels": [], "entities": []}, {"text": "With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution , we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.9594404697418213}, {"text": "coreference resolution", "start_pos": 200, "end_pos": 222, "type": "TASK", "confidence": 0.7347917556762695}]}, {"text": "Reconcile is designed to facilitate the rapid creation of corefer-ence resolution systems, easy implementation of new feature sets and approaches to coreference resolution , and empirical evaluation of coreference re-solvers across a variety of benchmark data sets and standard scoring metrics.", "labels": [], "entities": [{"text": "corefer-ence resolution", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.8089945912361145}, {"text": "coreference resolution", "start_pos": 149, "end_pos": 171, "type": "TASK", "confidence": 0.8778981864452362}]}, {"text": "We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-of-the-art systems on six benchmark data sets.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.8988155126571655}]}], "introductionContent": [{"text": "Noun phrase coreference resolution (or simply coreference resolution) is the problem of identifying all noun phrases (NPs) that refer to the same entity in a text.", "labels": [], "entities": [{"text": "Noun phrase coreference resolution (or simply coreference resolution)", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.7040311157703399}, {"text": "identifying all noun phrases (NPs) that refer to the same entity in a text", "start_pos": 88, "end_pos": 162, "type": "TASK", "confidence": 0.5874677486717701}]}, {"text": "The problem of coreference resolution is fundamental in the field of natural language processing (NLP) because of its usefulness for other NLP tasks, as well as the theoretical interest in understanding the computational mechanisms involved in government, binding and linguistic reference.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.975143551826477}, {"text": "natural language processing (NLP)", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.8181064029534658}, {"text": "government, binding and linguistic reference", "start_pos": 244, "end_pos": 288, "type": "TASK", "confidence": 0.5887029568354288}]}, {"text": "Several formal evaluations have been conducted for the coreference resolution task (e.g.,, ACE NIST (2004)), and the data sets created for these evaluations have become standard benchmarks in the field (e.g., MUC and ACE data sets).", "labels": [], "entities": [{"text": "coreference resolution task", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.9509749809900919}, {"text": "ACE NIST (2004))", "start_pos": 91, "end_pos": 107, "type": "DATASET", "confidence": 0.9314907550811767}, {"text": "MUC", "start_pos": 209, "end_pos": 212, "type": "DATASET", "confidence": 0.72735595703125}, {"text": "ACE data sets", "start_pos": 217, "end_pos": 230, "type": "DATASET", "confidence": 0.9152655204137167}]}, {"text": "However, it is still frustratingly difficult to compare results across different coreference resolution systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.9070577323436737}]}, {"text": "Reported coreference resolution scores vary wildly across data sets, evaluation metrics, and system configurations.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.8883357048034668}]}, {"text": "We believe that one root cause of these disparities is the high cost of implementing an end-toend coreference resolution system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.7787314057350159}]}, {"text": "Coreference resolution is a complex problem, and successful systems must tackle a variety of non-trivial subproblems that are central to the coreference taske.g., mention/markable detection, anaphor identification -and that require substantial implementation efforts.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9465426504611969}, {"text": "mention/markable detection", "start_pos": 163, "end_pos": 189, "type": "TASK", "confidence": 0.5780719220638275}, {"text": "anaphor identification", "start_pos": 191, "end_pos": 213, "type": "TASK", "confidence": 0.6888090223073959}]}, {"text": "As a result, many researchers exploit gold-standard annotations, when available, as a substitute for component technologies to solve these subproblems.", "labels": [], "entities": []}, {"text": "For example, many published research results use gold standard annotations to identify NPs (substituting for mention/markable detection), to distinguish anaphoric NPs from nonanaphoric NPs (substituting for anaphoricity determination), to identify named entities (substituting for named entity recognition), and to identify the semantic types of NPs (substituting for semantic class identification).", "labels": [], "entities": []}, {"text": "Unfortunately, the use of gold standard annotations for key/critical component technologies leads to an unrealistic evaluation setting, and makes it impossible to directly compare results against coreference resolvers that solve all of these subproblems from scratch.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 196, "end_pos": 217, "type": "TASK", "confidence": 0.888655811548233}]}, {"text": "Comparison of coreference resolvers is further hindered by the use of several competing (and non-trivial) evaluation measures, and data sets that have substantially different task definitions and annotation formats.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.9350375831127167}]}, {"text": "Additionally, coreference resolution is a pervasive problem in NLP and many NLP applications could benefit from an effective coreference resolver that can be easily configured and customized.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.9600887298583984}, {"text": "coreference resolver", "start_pos": 125, "end_pos": 145, "type": "TASK", "confidence": 0.810029923915863}]}, {"text": "To address these issues, we have created a platform for coreference resolution, called Reconcile, that can serve as a software infrastructure to support the creation of, experimentation with, and evaluation of coreference resolvers.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.9487393498420715}, {"text": "coreference resolvers", "start_pos": 210, "end_pos": 231, "type": "TASK", "confidence": 0.8048737943172455}]}, {"text": "Reconcile was designed with the following seven desiderata in mind: \u2022 implement the basic underlying software ar-chitecture of contemporary state-of-the-art learning-based coreference resolution systems; \u2022 support experimentation on most of the standard coreference resolution data sets; \u2022 implement most popular coreference resolution scoring metrics; \u2022 exhibit state-of-the-art coreference resolution performance (i.e., it can be configured to create a resolver that achieves performance close to the best reported results); \u2022 can be easily extended with new methods and features; \u2022 is relatively fast and easy to configure and run; \u2022 has a set of pre-built resolvers that can be used as black-box coreference resolution systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 172, "end_pos": 194, "type": "TASK", "confidence": 0.8121390044689178}, {"text": "coreference resolution scoring", "start_pos": 313, "end_pos": 343, "type": "TASK", "confidence": 0.8021209438641866}, {"text": "coreference resolution", "start_pos": 700, "end_pos": 722, "type": "TASK", "confidence": 0.7174261212348938}]}, {"text": "While several other coreference resolution systems are publicly available (e.g.,, and), none meets all seven of these desiderata (see Related Work).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.9481551051139832}]}, {"text": "Reconcile is a modular software platform that abstracts the basic architecture of most contemporary supervised learningbased coreference resolution systems (e.g.,,,) and achieves performance comparable to the state-of-the-art on several benchmark data sets.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 125, "end_pos": 147, "type": "TASK", "confidence": 0.7112277150154114}]}, {"text": "Additionally, Reconcile can be easily reconfigured to use different algorithms, features, preprocessing elements, evaluation settings and metrics.", "labels": [], "entities": []}, {"text": "In the rest of this paper, we review related work (Section 2), describe Reconcile's organization and components (Section 3) and show experimental results for Reconcile on six data sets and two evaluation metrics (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "The first two rows of show the performance of Reconcile 2010 . For all data sets, B 3 scores are higher than MUC scores.", "labels": [], "entities": [{"text": "Reconcile 2010", "start_pos": 46, "end_pos": 60, "type": "DATASET", "confidence": 0.8476379215717316}, {"text": "B 3 scores", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9813096125920614}, {"text": "MUC", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.6650715470314026}]}, {"text": "The MUC score is highest for the MUC6 data set, while B 3 scores are higher for the ACE data sets as compared to the MUC data sets.", "labels": [], "entities": [{"text": "MUC score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.6715627908706665}, {"text": "MUC6 data set", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9739818175633749}, {"text": "B 3 scores", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9248324632644653}, {"text": "ACE data sets", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.9830660025278727}, {"text": "MUC data sets", "start_pos": 117, "end_pos": 130, "type": "DATASET", "confidence": 0.9580479264259338}]}, {"text": "Due to the difficulties outlined in Section 1, results for Reconcile presented here are directly comparable only to a limited number of scores reported in the literature.", "labels": [], "entities": [{"text": "Reconcile", "start_pos": 59, "end_pos": 68, "type": "TASK", "confidence": 0.8958279490470886}]}, {"text": "The bottom three rows of list these comparable scores, which show that Reconcile 2010 exhibits state-ofthe-art performance for supervised learning-based coreference resolvers.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 153, "end_pos": 174, "type": "TASK", "confidence": 0.7346033751964569}]}, {"text": "A more detailed study of Reconcile-based coreference resolution systems in different evaluation scenarios can be found in.", "labels": [], "entities": [{"text": "Reconcile-based coreference resolution", "start_pos": 25, "end_pos": 63, "type": "TASK", "confidence": 0.7583548823992411}]}], "tableCaptions": [{"text": " Table 3: Scores for Reconcile on six data sets and scores for comparable coreference systems.", "labels": [], "entities": [{"text": "Reconcile", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.9481369853019714}]}]}