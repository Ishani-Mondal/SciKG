{"title": [{"text": "Towards robust multi-tool tagging. An OWL/DL-based approach", "labels": [], "entities": [{"text": "multi-tool tagging", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.6595483869314194}]}], "abstractContent": [{"text": "This paper describes a series of experiments to test the hypothesis that the parallel application of multiple NLP tools and the integration of their results improves the correctness and robustness of the resulting analysis.", "labels": [], "entities": []}, {"text": "It is shown how annotations created by seven NLP tools are mapped onto tool-independent descriptions that are defined with reference to an ontology of linguistic annotations, and how a majority vote and ontological consistency constraints can be used to integrate multiple alternative analyses of the same token in a consistent way.", "labels": [], "entities": []}, {"text": "For morphosyntactic (parts of speech) and morphological annotations of three Ger-man corpora, the resulting merged sets of ontological descriptions are evaluated in comparison to (ontological representation of) existing reference annotations.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Six experiments were conducted with the goal to evaluate the prediction of word classes and morphological features on parts of three corpora of German newspaper articles: NEGRA (Skut et al., 1998), TIGER (), and the Potsdam Commentary Corpus.", "labels": [], "entities": [{"text": "NEGRA (Skut et al., 1998)", "start_pos": 171, "end_pos": 196, "type": "DATASET", "confidence": 0.7775850743055344}, {"text": "TIGER", "start_pos": 198, "end_pos": 203, "type": "METRIC", "confidence": 0.7209972739219666}, {"text": "Potsdam Commentary Corpus", "start_pos": 216, "end_pos": 241, "type": "DATASET", "confidence": 0.9177001317342123}]}, {"text": "From every corpus 10,000 tokens were considered for the analysis.", "labels": [], "entities": []}, {"text": "TIGER and NEGRA are well-known resources that also influenced the design of several of the tools considered.", "labels": [], "entities": [{"text": "TIGER", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6209489107131958}, {"text": "NEGRA", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.6821020245552063}]}, {"text": "For this reason, the PCC was consulted, a small collection of newspaper commentaries, 30,000 tokens in total, annotated with TIGER-style parts of speech and syntax (by members of the TIGER project).", "labels": [], "entities": [{"text": "PCC", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.6218928098678589}]}, {"text": "None of the tools considered here were trained on this data, so that it provides independent test data.", "labels": [], "entities": []}, {"text": "The ontological descriptions were evaluated for recall: 7 In, T is a text (a list of tokens) with T = (t 1 , ..., tn ), D predicted (t) are descriptions retrieved from the NLP analyses of the token t, and D target (t) is the set of descriptions that correspond to the original annotation oft in the corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9646899104118347}]}, {"text": "Precision and accuracy may not be appropriate measurements in this case: Annotation schemes differ in their expressiveness, so that a description predicted by an NLP tool but not found in the reference annotation may nevertheless be correct.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9924681782722473}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9988943934440613}]}, {"text": "The RFTagger, for example, assigns demonstrative pronouns the feature '3rd person', that is not found in TIGER/NEGRA-style annotation because of its redundancy.", "labels": [], "entities": [{"text": "RFTagger", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9794946908950806}]}, {"text": "shows that the recall of rdf:type descriptions (for word classes) increases continuously with the number of NLP tools applied.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9989979863166809}]}, {"text": "The combination of all seven tools actually shows a better recall than the best-performing single NLP tool.", "labels": [], "entities": [{"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.999355137348175}]}, {"text": "(The NEGRA corpus is an apparent exception only; the exceptionally high recall of the Stanford Tagger reflects the fact that it was trained on NEGRA.)", "labels": [], "entities": [{"text": "NEGRA corpus", "start_pos": 5, "end_pos": 17, "type": "DATASET", "confidence": 0.749970018863678}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9991623163223267}, {"text": "Stanford Tagger", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.8901233375072479}]}], "tableCaptions": [{"text": " Table 1: Confidence scores for diese in ex. (1)", "labels": [], "entities": [{"text": "Confidence scores", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9585134387016296}]}, {"text": " Table 2: Recall for rdf:type descriptions for word classes", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8243401050567627}]}]}