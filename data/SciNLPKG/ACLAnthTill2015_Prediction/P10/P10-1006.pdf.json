{"title": [{"text": "Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation", "labels": [], "entities": [{"text": "Structural Semantic Relatedness", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8392809430758158}, {"text": "Named Entity Disambiguation", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.709732860326767}]}], "abstractContent": [{"text": "Name ambiguity problem has raised urgent demands for efficient, high-quality named entity disambiguation methods.", "labels": [], "entities": []}, {"text": "In recent years, the increasing availability of large-scale, rich semantic knowledge sources (such as Wikipe-dia and WordNet) creates new opportunities to enhance the named entity disambiguation by developing algorithms which can exploit these knowledge sources at best.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.8825258612632751}, {"text": "named entity disambiguation", "start_pos": 167, "end_pos": 194, "type": "TASK", "confidence": 0.7216972907384237}]}, {"text": "The problem is that these knowledge sources are heterogeneous and most of the semantic knowledge within them is embedded in complex structures, such as graphs and networks.", "labels": [], "entities": []}, {"text": "This paper proposes a knowledge-based method, called Structural Semantic Relatedness (SSR), which can enhance the named entity disambiguation by capturing and leveraging the structural semantic knowledge in multiple knowledge sources.", "labels": [], "entities": [{"text": "Structural Semantic Relatedness (SSR)", "start_pos": 53, "end_pos": 90, "type": "TASK", "confidence": 0.7273504038651785}]}, {"text": "Empirical results show that, in comparison with the classical BOW based methods and social network based methods, our method can significantly improve the disambiguation performance by respectively 8.7% and 14.7%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Name ambiguity problem is common on the Web.", "labels": [], "entities": [{"text": "Name ambiguity", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8680669665336609}]}, {"text": "For example, the name \"Michael Jordan\" represents more than ten persons in the Google search results.", "labels": [], "entities": []}, {"text": "Some of them are shown below:", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess the performance of our method and compare it with traditional methods, we conduct a series of experiments.", "labels": [], "entities": []}, {"text": "In the experiments, we evaluate the proposed SSR method on the task of personal name disambiguation, which is the most common type of named entity disambiguation.", "labels": [], "entities": [{"text": "SSR", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.8641237020492554}, {"text": "personal name disambiguation", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.6140007177988688}]}, {"text": "In the following, we first explain the general experimental settings in Section 4.1, 4.2 and 4.3; then evaluate and discuss the performance of our method in Section 4.4.", "labels": [], "entities": []}, {"text": "We adopted the measures used in WePS1 to evaluate the performance of name disambiguation.", "labels": [], "entities": [{"text": "WePS1", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8718566298484802}, {"text": "name disambiguation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.8000687062740326}]}, {"text": "These measures are: Purity (Pur): measures the homogeneity of name observations in the same cluster; Inverse purity (Inv_Pur): measures the completeness of a cluster; F-Measure (F): the harmonic mean of purity and inverse purity.", "labels": [], "entities": [{"text": "Purity (Pur)", "start_pos": 20, "end_pos": 32, "type": "METRIC", "confidence": 0.8611751943826675}, {"text": "Inverse purity (Inv_Pur)", "start_pos": 101, "end_pos": 125, "type": "METRIC", "confidence": 0.9214298895427159}, {"text": "F-Measure (F)", "start_pos": 167, "end_pos": 180, "type": "METRIC", "confidence": 0.953549712896347}, {"text": "inverse purity", "start_pos": 214, "end_pos": 228, "type": "METRIC", "confidence": 0.8916747570037842}]}, {"text": "The detailed definitions of these measures can be found in.", "labels": [], "entities": []}, {"text": "We use Fmeasure as the primary measure just liking WePS1 and WePS2.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9910655617713928}, {"text": "WePS1", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.827383816242218}, {"text": "WePS2", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.9177746176719666}]}, {"text": "We compared our method with four baselines: (1) BOW: The first one is the traditional Bag of Words model (BOW) based methods: hierarchical agglomerative clustering (HAC) over term vector similarity, where the features including single words and NEs, and all the features are weighted using TFIDF.", "labels": [], "entities": [{"text": "BOW", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9915111660957336}]}, {"text": "This baseline is also the state-of-art method in WePS1 and WePS2.", "labels": [], "entities": [{"text": "WePS1", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9148537516593933}, {"text": "WePS2", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.953787624835968}]}, {"text": "(2) SocialNetwork: The second one is the social network based methods, which is the same as the method described in: HAC over the similarity obtained through random walkover the social network built from the web pages of the top N search results.", "labels": [], "entities": [{"text": "HAC", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.8562067747116089}]}, {"text": "(3)SSRNoKnowledge: The third one is used as a baseline for evaluating the efficiency of semantic knowledge: HAC over the similarity computed on semantic-graph with no knowledge integrated, i.e., the similarity is computed as: The fourth one is used as a baseline for evaluating the efficiency of the semantic knowledge embedded in complex structures: HAC over the similarity computed by only integrating the explicit semantic relations, i.e., the similarity is computed as:", "labels": [], "entities": [{"text": "SSRNoKnowledge", "start_pos": 3, "end_pos": 17, "type": "DATASET", "confidence": 0.7576200366020203}, {"text": "HAC", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.989446759223938}, {"text": "HAC", "start_pos": 351, "end_pos": 354, "type": "METRIC", "confidence": 0.8445121645927429}]}], "tableCaptions": [{"text": " Table 2. The lexical relatedness table of four selected  WordNet concepts", "labels": [], "entities": [{"text": "WordNet", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9412550330162048}]}, {"text": " Table 4. The structural semantic relatedness of the  semantic-graph shown in", "labels": [], "entities": []}, {"text": " Table 5. Performance results of baselines and SSR  methods", "labels": [], "entities": [{"text": "SSR", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9813414812088013}]}]}