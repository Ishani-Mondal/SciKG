{"title": [{"text": "A Joint Rule Selection Model for Hierarchical Phrase-based Translation *", "labels": [], "entities": [{"text": "Hierarchical Phrase-based Translation", "start_pos": 33, "end_pos": 70, "type": "TASK", "confidence": 0.7814581195513407}]}], "abstractContent": [{"text": "In hierarchical phrase-based SMT systems , statistical models are integrated to guide the hierarchical rule selection for better translation performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8108183145523071}]}, {"text": "Previous work mainly focused on the selection of either the source side of a hierarchical rule or the target side of a hierarchical rule rather than considering both of them simultaneously.", "labels": [], "entities": []}, {"text": "This paper presents a joint model to predict the selection of hierarchical rules.", "labels": [], "entities": []}, {"text": "The proposed model is estimated based on four sub-models where the rich context knowledge from both source and target sides is leveraged.", "labels": [], "entities": []}, {"text": "Our method can be easily incorporated into the practical SMT systems with the log-linear model framework.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9950529932975769}]}, {"text": "The experimental results show that our method can yield significant improvements in performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hierarchical phrase-based model has strong expression capabilities of translation knowledge.", "labels": [], "entities": []}, {"text": "It cannot only maintain the strength of phrase translation in traditional phrase-based models (), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models; ;.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7506607174873352}, {"text": "syntactic based statistical machine translation (SMT)", "start_pos": 172, "end_pos": 225, "type": "TASK", "confidence": 0.7880255356431007}]}, {"text": "In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8032987117767334}, {"text": "rule matching", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.7452302873134613}]}, {"text": "SMT decoders are forced to face the challenge of * This work was finished while the first author visited Microsoft Research Asia as an intern.", "labels": [], "entities": [{"text": "SMT decoders", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.874057948589325}, {"text": "Microsoft Research Asia", "start_pos": 105, "end_pos": 128, "type": "DATASET", "confidence": 0.8620994091033936}]}, {"text": "proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule.", "labels": [], "entities": [{"text": "hypothesis generation", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7787643074989319}]}, {"text": "Improper rule selections may result in poor translations.", "labels": [], "entities": []}, {"text": "There is some related work about the hierarchical rule selection.", "labels": [], "entities": [{"text": "hierarchical rule selection", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6013796230157217}]}, {"text": "In the original work, the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.8206593990325928}]}, {"text": "Extending this work, ( ) integrate rich context information of non-terminals to predict the target-side rule selection.", "labels": [], "entities": []}, {"text": "Different from the above work where the probability distribution of source-side rule selection is uniform, ( proposes to select sourceside rules based on the captured function words which often play an important role in word reordering.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 220, "end_pos": 235, "type": "TASK", "confidence": 0.7013741135597229}]}, {"text": "There is also some work considering to involve more rich contexts to guide the source-side rule selection.) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules.", "labels": [], "entities": []}, {"text": "All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together.", "labels": [], "entities": []}, {"text": "The separation of these two tasks, however, weakens the high interrelation between them.", "labels": [], "entities": []}, {"text": "In this paper, we propose to integrate both source-side and target-side rule selection in a unified model.", "labels": [], "entities": []}, {"text": "The intuition is that the joint selection of source-side and target-side rules is more reliable as it conducts the search in a larger space than the single selection task does.", "labels": [], "entities": []}, {"text": "It is expected that these two kinds of selection can help and affect each other, which may potentially lead to better hierarchical rule selections with a relative global optimum instead of a local optimum that might be reached in the pre-vious methods.", "labels": [], "entities": []}, {"text": "Our proposed joint probability model is factored into four sub-models that can be further classified into source-side and targetside rule selection models or context-based and context-free selection models.", "labels": [], "entities": []}, {"text": "The context-based models explore rich context features from both source and target sides, including function words, part-of-speech (POS) tags, syntactic structure information and soon.", "labels": [], "entities": []}, {"text": "Our model can be easily incorporated as an independent feature into the practical hierarchical phrase-based systems with the log-linear model framework.", "labels": [], "entities": []}, {"text": "The experimental results indicate our method can improve the system performance significantly.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implement a hierarchical phrase-based system similar to the Hiero () and evaluate our method on the Chinese-to-English translation task.", "labels": [], "entities": [{"text": "Chinese-to-English translation task", "start_pos": 103, "end_pos": 138, "type": "TASK", "confidence": 0.7661106288433075}]}, {"text": "Our bilingual training data comes from FBIS corpus, which consists of around 160K sentence pairs where the source data is parsed by the Berkeley parser.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.8997917771339417}]}, {"text": "The ME training toolkit, developed by, is used to train our CBSM and CBTM.", "labels": [], "entities": [{"text": "ME training toolkit", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.6364131967226664}, {"text": "CBTM", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.8219521045684814}]}, {"text": "The training size of constructed positive instances for both CBSM and CBTM is 4.68M, while the training size of constructed negative instances is 3.74M and 3.03M respectively.", "labels": [], "entities": [{"text": "CBTM", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.7537357211112976}]}, {"text": "Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling).", "labels": [], "entities": [{"text": "BLEU score differences", "start_pos": 28, "end_pos": 50, "type": "METRIC", "confidence": 0.9599437316258749}]}], "tableCaptions": [{"text": " Table 1: Comparison results, our method is signif- icantly better than the baseline, as well as the other  two approaches (p < 0.01)", "labels": [], "entities": []}, {"text": " Table 2: Sub-model effect upon the performance,  *: significantly better than baseline (p < 0.01)", "labels": [], "entities": []}]}