{"title": [{"text": "Phrase-based Statistical Language Generation using Graphical Models and Active Learning", "labels": [], "entities": [{"text": "Phrase-based Statistical Language Generation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8236871510744095}]}], "abstractContent": [{"text": "Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process.", "labels": [], "entities": [{"text": "trainable language generation", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.7364015976587931}]}, {"text": "Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains.", "labels": [], "entities": []}, {"text": "This paper presents BAGEL, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.8291754722595215}]}, {"text": "A human evaluation shows that BAGEL can generate natural and informative utterances from unseen inputs in the information presentation domain.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 30, "end_pos": 35, "type": "TASK", "confidence": 0.5967768430709839}]}, {"text": "Additionally, generation performance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.", "labels": [], "entities": []}], "introductionContent": [{"text": "The field of natural language generation (NLG) is one of the last areas of computational linguistics to embrace statistical methods.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.8085092206796011}]}, {"text": "Over the past decade, statistical NLG has followed two lines of research.", "labels": [], "entities": [{"text": "statistical NLG", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.6551739573478699}]}, {"text": "The first one, pioneered by, introduces statistics in the generation process by training a model which reranks candidate outputs of a handcrafted generator.", "labels": [], "entities": []}, {"text": "While their HALOGEN system uses an n-gram language model trained on news articles, other systems have used hierarchical syntactic models, models trained on user ratings of * This research was partly funded by the UK EPSRC under grant agreement EP/F013930/1 and funded by the EU FP7 Programme under grant agreement 216594 (CLASSiC project: www.classic-project.org).", "labels": [], "entities": [{"text": "UK EPSRC under grant agreement EP/F013930/1", "start_pos": 213, "end_pos": 256, "type": "DATASET", "confidence": 0.861540961265564}]}, {"text": "utterance quality (), or alignment models trained on speaker-specific corpora).", "labels": [], "entities": []}, {"text": "A second line of research has focused on introducing statistics at the generation decision level, by training models that find the set of generation parameters maximising an objective function, e.g. producing a target linguistic style, generating the most likely context-free derivations given a corpus, or maximising the expected reward using reinforcement learning (.", "labels": [], "entities": []}, {"text": "While such methods do not suffer from the computational cost of an overgeneration phase, they still require a handcrafted generator to define the generation decision space within which statistics can be used to find an optimal solution.", "labels": [], "entities": []}, {"text": "This paper presents BAGEL (Bayesian networks for generation using active learning), an NLG system that can be fully trained from aligned data.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9764556884765625}]}, {"text": "While the main requirement of the generator is to produce natural utterances within a dialogue system domain, a second objective is to minimise the overall development effort.", "labels": [], "entities": []}, {"text": "In this regard, a major advantage of data-driven methods is the shift of the effort from model design and implementation to data annotation.", "labels": [], "entities": []}, {"text": "In the case of NLG systems, learning to produce paraphrases can be facilitated by collecting data from a large sample of annotators.", "labels": [], "entities": []}, {"text": "Our meaning representation should therefore (a) be intuitive enough to be understood by untrained annotators, and (b) provide useful generalisation properties for generating unseen inputs.", "labels": [], "entities": []}, {"text": "Section 2 describes BAGEL's meaning representation, which satisfies both requirements.", "labels": [], "entities": [{"text": "BAGEL's meaning representation", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.5211541056632996}]}, {"text": "Section 3 then details how our meaning representation is mapped to a phrase sequence, using a dynamic Bayesian network with backoff smoothing.", "labels": [], "entities": []}, {"text": "Within a given domain, the same semantic concept can occur in different utterances.", "labels": [], "entities": []}, {"text": "Section 4 details how BAGEL exploits this redundancy to improve generation performance on sparse datasets, by guiding the data collection process using certainty-based active learning (.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 22, "end_pos": 27, "type": "TASK", "confidence": 0.49437108635902405}]}, {"text": "We train BAGEL in the information presentation domain, from a corpus of utterances produced by 42 untrained annotators (see Section 5.1).", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 9, "end_pos": 14, "type": "TASK", "confidence": 0.487833708524704}, {"text": "information presentation domain", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.8143464922904968}]}, {"text": "An automated evaluation metric is used to compare preliminary model and training configurations in Section 5.2, while Section 5.3 shows that the resulting system produces natural and informative utterances, according to 18 human judges.", "labels": [], "entities": []}, {"text": "Finally, our human evaluation shows that training using active learning significantly improves generation performance on sparse datasets, yielding results close to the human gold standard using a fraction of the data.", "labels": [], "entities": []}], "datasetContent": [{"text": "BAGEL's factored language models are trained using the SRILM toolkit, and decoding is performed using GMTK's junction tree inference algorithm ().", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.881447970867157}, {"text": "SRILM toolkit", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.8919654786586761}, {"text": "GMTK", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.9513993859291077}]}, {"text": "Since each active learning iteration requires generating all training utterances in our domain, they are generated using a larger clique pruning threshold than the test utterances used for evaluation.", "labels": [], "entities": []}, {"text": "We first evaluate BAGEL using the BLEU automated metric (), which measures the word n-gram overlap between the generated utterances and the 2 reference paraphrases over a test corpus (with n up to 4).", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.8136560320854187}, {"text": "BLEU automated metric", "start_pos": 34, "end_pos": 55, "type": "METRIC", "confidence": 0.9463448723157247}]}, {"text": "While BLEU suffers from known issues such as a bias towards statistical NLG systems, it provides useful information when comparing similar systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9932076334953308}]}, {"text": "We evaluate BAGEL for different training set sizes, model dependencies, and active learning parameters.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.6450554132461548}]}, {"text": "Our results are averaged over a 10-fold cross-validation over distinct dialogue acts, i.e. dialogue acts used for testing are not seen at training time, and all systems are tested on the same folds.", "labels": [], "entities": []}, {"text": "The training and test sets respectively contain an average of 181 and 21 distinct dialogue acts, and each dialogue act is associated with two paraphrases, resulting in 362 training utterances.", "labels": [], "entities": []}, {"text": "The normalisation process took around 4 person-hour for 404 utterances.", "labels": [], "entities": []}, {"text": "We do not evaluate performance on dialogue acts used for training, as the training examples can trivially be used as generation templates.", "labels": [], "entities": []}, {"text": "Results: shows that adding a dependency on the future semantic stack improves performances for all training set sizes, despite the added model complexity.", "labels": [], "entities": []}, {"text": "Backing off to partial stacks also improves performance, but only for sparse training sets.", "labels": [], "entities": []}, {"text": "compares the full model trained using random sampling in with the same model trained using certainty-based active learning, for different values of k.", "labels": [], "entities": []}, {"text": "As our dataset only contains two paraphrases per dialogue act, the same dialogue act can only be queried twice during the active learning procedure.", "labels": [], "entities": []}, {"text": "A consequence is that the training set used for active learning converges towards the randomly sampled set as its size increases.", "labels": [], "entities": []}, {"text": "Results show that increasing the training set one utterance at a time using active learning (k = 1) significantly outperforms random sampling when using 40, 80, and 100 utterances (p < .05, two-tailed).", "labels": [], "entities": []}, {"text": "Increasing the number of utterances to be queried at each iteration to k = 10 results in a smaller performance increase.", "labels": [], "entities": []}, {"text": "A possi-  ble explanation is that the model is likely to assign low probabilities to similar inputs, thus any value above k = 1 might result in redundant queries within an iteration.", "labels": [], "entities": []}, {"text": "As the length of the semantic stack sequence is not known before decoding, the active learning selection criterion presented in (9) is biased towards longer utterances, which tend to have a lower probability.", "labels": [], "entities": []}, {"text": "However, shows that normalising the log probability by the number of semantic stacks does not improve overall learning performance.", "labels": [], "entities": []}, {"text": "Although a possible explanation is that longer inputs tend to contain more information to learn from, shows that a baseline selecting the largest remaining semantic input at each iteration performs worse than the active learning scheme for training sets above 20 utterances.", "labels": [], "entities": []}, {"text": "The full log probability selection criterion defined in is therefore used throughout the rest of the paper (with k = 1).", "labels": [], "entities": []}, {"text": "While automated metrics provide useful information for comparing different systems, human feedback is needed to assess (a) the quality of BAGEL's outputs, and (b) whether training models using active learning has a significant impact on user perceptions.", "labels": [], "entities": []}, {"text": "We evaluate BAGEL through a largescale subjective rating experiment using Amazon's Mechanical Turk service.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 12, "end_pos": 17, "type": "TASK", "confidence": 0.5188875794410706}, {"text": "Amazon's Mechanical Turk service", "start_pos": 74, "end_pos": 106, "type": "DATASET", "confidence": 0.84455406665802}]}, {"text": "For each dialogue act in our domain, participants are presented with a 'gold standard' human utterance from our dataset, which they must compare with utterances generated by models trained with and without active learning on a set of 20, 40, 100, and 362 utterances (full training set), as well as with the second human utterance in our dataset.", "labels": [], "entities": []}, {"text": "The judges are then asked to evaluate the informativeness and naturalness of each of the 8 utterances on a 5 point likert-scale.", "labels": [], "entities": []}, {"text": "Naturalness is defined as whether the utterance could have been produced by a human, and informativeness is defined as whether it contains all the information in the gold standard utterance.", "labels": [], "entities": []}, {"text": "Each utterance is taken from the test folds of the cross-validation experiment presented in Section 5.2, i.e. the models are trained on up to 90% of the data and the training set does not contain the dialogue act being tested.", "labels": [], "entities": [{"text": "Section 5.2", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.8816979229450226}]}, {"text": "Results: compare the naturalness and informativeness scores of each system averaged overall 202 dialogue acts.", "labels": [], "entities": []}, {"text": "A paired t-test shows that models trained on 40 utterances or less produce utterances that are rated significantly lower than human utterances for both naturalness and informativeness (p < .05, two-tailed).", "labels": [], "entities": []}, {"text": "However, models trained on 100 utterances or more do not perform significantly worse than human utterances for both dimensions, with a mean difference below .10 over 202 comparisons.", "labels": [], "entities": []}, {"text": "Given the large sample size, this result suggests that BAGEL can successfully learn our domain using a fraction of our initial dataset.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.6004813313484192}]}, {"text": "As far as the learning method is concerned, a paired t-test shows that models trained on 20 and 40 utterances using active learning significantly outperform models trained using random sampling, for both dimensions (p < .05).", "labels": [], "entities": []}, {"text": "The largest increase is observed using 20 utterances, i.e. the naturalness increases by .49 and the informativeness by .37.", "labels": [], "entities": []}, {"text": "When training on 100 utterances, the effect of active learning becomes insignificant.", "labels": [], "entities": []}, {"text": "In- terestingly, while models trained on 100 utterances outperform models trained on 40 utterances using random sampling (p < .05), they do not significantly outperform models trained on 40 utterances using active learning (p = .15 for naturalness and p = .41 for informativeness).", "labels": [], "entities": []}, {"text": "These results suggest that certainty-based active learning is beneficial for training a generator from a limited amount of data given the domain size.", "labels": [], "entities": []}, {"text": "Looking back at the results presented in Section 5.2, we find that the BLEU score correlates with a Pearson correlation coefficient of .42 with the mean naturalness score and .35 with the mean informativeness score, overall folds of all systems tested (n = 70, p < .01).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9736903011798859}, {"text": "Pearson correlation coefficient", "start_pos": 100, "end_pos": 131, "type": "METRIC", "confidence": 0.9703596830368042}]}, {"text": "This is lower than previous correlations reported by in the shipping forecast domain with nonexpert judges (r = .80), possibly because our domain is larger and more open to subjectivity.", "labels": [], "entities": [{"text": "shipping forecast domain", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.5266072650750478}]}], "tableCaptions": []}