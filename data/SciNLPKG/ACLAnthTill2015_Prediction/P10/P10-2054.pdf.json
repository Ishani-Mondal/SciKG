{"title": [{"text": "An Entity-Level Approach to Information Extraction", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7077846527099609}]}], "abstractContent": [{"text": "We present a generative model of template-filling in which coreference resolution and role assignment are jointly determined.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.9489339888095856}, {"text": "role assignment", "start_pos": 86, "end_pos": 101, "type": "TASK", "confidence": 0.7982643246650696}]}, {"text": "Underlying template roles first generate abstract entities, which in turn generate concrete textual mentions.", "labels": [], "entities": []}, {"text": "On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%.", "labels": [], "entities": [{"text": "error", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.9964864253997803}]}], "introductionContent": [{"text": "Template-filling information extraction (IE) systems must merge information across multiple sentences to identify all role fillers of interest.", "labels": [], "entities": [{"text": "Template-filling information extraction (IE)", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8354763487974802}]}, {"text": "For instance, in the MUC4 terrorism event extraction task, the entity filling the individual perpetrator role often occurs multiple times, variously as proper, nominal, or pronominal mentions.", "labels": [], "entities": [{"text": "MUC4 terrorism event extraction task", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.8749038934707641}]}, {"text": "However, most template-filling systems assign roles to individual textual mentions using only local context as evidence, leaving aggregation for post-processing.", "labels": [], "entities": []}, {"text": "While prior work has acknowledged that coreference resolution and discourse analysis are integral to accurate role identification, to our knowledge no model has been proposed which jointly models these phenomena.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.9755055606365204}, {"text": "discourse analysis", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7282460182905197}, {"text": "role identification", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7688526511192322}]}, {"text": "In this work, we describe an entity-centered approach to template-filling IE problems.", "labels": [], "entities": [{"text": "IE problems", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.8408732116222382}]}, {"text": "Our model jointly merges surface mentions into underlying entities (coreference resolution) and assigns roles to those discovered entities.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.9548168182373047}]}, {"text": "In the generative process proposed here, document entities are generated for each template role, along with a set of non-template entities.", "labels": [], "entities": []}, {"text": "These entities then generate mentions in a process sensitive to both lexical and structural properties of the mention.", "labels": [], "entities": []}, {"text": "Our model outperforms a discriminative mention-level baseline.", "labels": [], "entities": []}, {"text": "Moreover, since our model is generative, it: Example of the corporate acquisitions role-filling task.", "labels": [], "entities": []}, {"text": "In (a), an example template specifying the entities playing each domain role.", "labels": [], "entities": []}, {"text": "In (b), an example document with coreferent mentions sharing the same role label.", "labels": [], "entities": []}, {"text": "Note that pronoun mentions provide direct clues to entity roles.", "labels": [], "entities": []}, {"text": "can naturally incorporate unannotated data, which further increases accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9974129796028137}]}], "datasetContent": [{"text": "We present results on the corporate acquisitions task, which consists of 600 annotated documents split into a 300/300 train/test split.", "labels": [], "entities": [{"text": "corporate acquisitions task", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.7531352241834005}]}, {"text": "We use 50 training documents as a development set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on corporate acquisition tasks with given  role mention boundaries. We report mention role accuracy  and entity role accuracy (correctly labeling all entity men- tions).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.7253326773643494}, {"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.6648430228233337}]}, {"text": " Table 2: Results on corporate acquisitions data where men- tion boundaries are not provided. Systems must determine  which mentions are template role-fillers as well as label them.  ROLE ID only evaluates the binary decision of whether a  mention is a template role-filler or not. OVERALL includes  correctly labeling mentions. Our BEST system, see Sec- tion 5, adds extra unannotated data to our JOINT+PRO sys- tem.", "labels": [], "entities": [{"text": "ROLE ID", "start_pos": 183, "end_pos": 190, "type": "METRIC", "confidence": 0.9475950300693512}, {"text": "OVERALL", "start_pos": 282, "end_pos": 289, "type": "METRIC", "confidence": 0.8540043234825134}, {"text": "BEST", "start_pos": 333, "end_pos": 337, "type": "METRIC", "confidence": 0.9435099363327026}]}]}