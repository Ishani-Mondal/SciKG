{"title": [{"text": "Modeling Semantic Relevance for Question-Answer Pairs in Web Social Communities", "labels": [], "entities": [{"text": "Modeling Semantic Relevance", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8296542763710022}]}], "abstractContent": [{"text": "Quantifying the semantic relevance between questions and their candidate answers is essential to answer detection in social media corpora.", "labels": [], "entities": [{"text": "answer detection", "start_pos": 97, "end_pos": 113, "type": "TASK", "confidence": 0.9220170378684998}]}, {"text": "In this paper, a deep belief network is proposed to model the semantic relevance for question-answer pairs.", "labels": [], "entities": []}, {"text": "Observing the textual similarity between the community-driven question-answering (cQA) dataset and the forum dataset, we present a novel learning strategy to promote the performance of our method on the social community datasets without hand-annotating work.", "labels": [], "entities": []}, {"text": "The experimental results show that our method outperforms the traditional approaches on both the cQA and the forum corpora.", "labels": [], "entities": [{"text": "cQA", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.9638248085975647}]}], "introductionContent": [{"text": "In natural language processing (NLP) and information retrieval (IR) fields, question answering (QA) problem has attracted much attention over the past few years.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.828255045413971}, {"text": "question answering (QA) problem", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.8976970911026001}]}, {"text": "Nevertheless, most of the QA researches mainly focus on locating the exact answer to a given factoid question in the related documents.", "labels": [], "entities": []}, {"text": "The most well known international evaluation on the factoid QA task is the Text REtrieval Conference (TREC) 1 , and the annotated questions and answers released by TREC have become important resources for the researchers.", "labels": [], "entities": [{"text": "factoid QA task", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.7435356577237447}, {"text": "Text REtrieval Conference (TREC)", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.6307157228390375}]}, {"text": "However, when facing a non-factoid question such as why, how, or what about, however, almost no automatic QA systems work very well.", "labels": [], "entities": [{"text": "QA", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.9384557008743286}]}, {"text": "The user-generated question-answer pairs are definitely of great importance to solve the nonfactoid questions.", "labels": [], "entities": []}, {"text": "Obviously, these natural QA pairs are usually created during people's communication via Internet social media, among which we are interested in the community-driven http://trec.nist.gov question-answering (cQA) sites and online forums.", "labels": [], "entities": []}, {"text": "The cQA sites (or systems) provide platforms where users can either ask questions or deliver answers, and best answers are selected manually (e.g., Baidu Zhidao 2 and Yahoo! Answers 3 ).", "labels": [], "entities": []}, {"text": "Comparing with cQA sites, online forums have more virtual society characteristics, where people hold discussions in certain domains, such as techniques, travel, sports, etc.", "labels": [], "entities": []}, {"text": "Online forums contain a huge number of QA pairs, and much noise information is involved.", "labels": [], "entities": []}, {"text": "To make use of the QA pairs in cQA sites and online forums, one has to face the challenging problem of distinguishing the questions and their answers from the noise.", "labels": [], "entities": []}, {"text": "According to our investigation, the data in the community based sites, especially for the forums, have two obvious characteristics: (a) a post usually includes a very short content, and when a person is initializing or replying a post, an informal tone tends to be used; (b) most of the posts are useless, which makes the community become a noisy environment for question-answer detection.", "labels": [], "entities": [{"text": "question-answer detection", "start_pos": 363, "end_pos": 388, "type": "TASK", "confidence": 0.8345935046672821}]}, {"text": "In this paper, a novel approach for modeling the semantic relevance for QA pairs in the social media sites is proposed.", "labels": [], "entities": []}, {"text": "We concentrate on the following two problems: 1.", "labels": [], "entities": []}, {"text": "How to model the semantic relationship between two short texts using simple textual features?", "labels": [], "entities": []}, {"text": "As mentioned above, the user generated questions and their answers via social media are always short texts.", "labels": [], "entities": []}, {"text": "The limitation of length leads to the sparsity of the word features.", "labels": [], "entities": []}, {"text": "In addition, the word frequency is usually either 0 or 1, that is, the frequency offers little information except the occurrence of a word.", "labels": [], "entities": [{"text": "word frequency", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.7752225697040558}]}, {"text": "Because of this situation, the traditional relevance computing methods based on word co-occurrence, such as Cosine similarity and KL-divergence, are not effective for question-answer semantic modeling.", "labels": [], "entities": [{"text": "Cosine similarity", "start_pos": 108, "end_pos": 125, "type": "METRIC", "confidence": 0.8502876162528992}, {"text": "question-answer semantic modeling", "start_pos": 167, "end_pos": 200, "type": "TASK", "confidence": 0.8813279668490092}]}, {"text": "Most researchers try to introduce structural features or users' behavior to improve the models performance, by contrast, the effect of textual features is not obvious.", "labels": [], "entities": []}, {"text": "2. How to train a model so that it has good performance on both cQA and forum datasets?", "labels": [], "entities": [{"text": "cQA and forum datasets", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.7317694574594498}]}, {"text": "So far, people have been doing QA researches on the cQA and the forum datasets separately (, and no one has noticed the relationship between the two kinds of data.", "labels": [], "entities": [{"text": "cQA and the forum datasets", "start_pos": 52, "end_pos": 78, "type": "DATASET", "confidence": 0.723080313205719}]}, {"text": "Since both the cQA systems and the online forums are open platforms for people to communicate, the QA pairs in the cQA systems have similarity with those in the forums.", "labels": [], "entities": []}, {"text": "In this case, it is highly valuable and desirable to propose a training strategy to improve the model's performance on both of the two kinds of datasets.", "labels": [], "entities": []}, {"text": "In addition, it is possible to avoid the expensive and arduous hand-annotating work by introducing the method.", "labels": [], "entities": []}, {"text": "To solve the first problem, we present a deep belief network (DBN) to model the semantic relevance between questions and their answers.", "labels": [], "entities": []}, {"text": "The network establishes the semantic relationship for QA pairs by minimizing the answer-to-question reconstructing error.", "labels": [], "entities": []}, {"text": "Using only word features, our model outperforms the traditional methods on question-answer relevance calculating.", "labels": [], "entities": [{"text": "question-answer relevance calculating", "start_pos": 75, "end_pos": 112, "type": "TASK", "confidence": 0.8225651383399963}]}, {"text": "For the second problem, we make our model to learn the semantic knowledge from the solved question threads in the cQA system.", "labels": [], "entities": []}, {"text": "Instead of mining the structure based features from cQA pages and forum threads individually, we consider the textual similarity between the two kinds of data.", "labels": [], "entities": []}, {"text": "The semantic information learned from cQA corpus is helpful to detect answers in forums, which makes our model show good performance on social media corpora.", "labels": [], "entities": [{"text": "cQA corpus", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.8828593194484711}]}, {"text": "Thanks to the labels for the best answers existing in the threads, no manual work is needed in our strategy.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 surveys the related work.", "labels": [], "entities": []}, {"text": "Section 3 introduces the deep belief network for answer detection.", "labels": [], "entities": [{"text": "answer detection", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.9724131524562836}]}, {"text": "In Section 4, the homogenous data based learning strategy is described.", "labels": [], "entities": []}, {"text": "Experimental result is given in Section 5.", "labels": [], "entities": []}, {"text": "Finally, conclusions and future directions are drawn in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our question-answer semantic relevance computing method, we compare our approach with the popular methods on the answer detecting task.", "labels": [], "entities": [{"text": "question-answer semantic relevance computing", "start_pos": 16, "end_pos": 60, "type": "TASK", "confidence": 0.8045926243066788}, {"text": "answer detecting task", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.8034252027670542}]}, {"text": "Architecture of the Network: To build the deep belief network, we use a 1500-1500-1000-600 architecture, which means the three layers of the network have individually 1,500\u00d71,500, 1,500\u00d71,000 and 1,000\u00d7600 units.", "labels": [], "entities": []}, {"text": "Using the network, a 1,500-dimensional binary vector is finally mapped to a 600-dimensional real-value vector.", "labels": [], "entities": []}, {"text": "During the pretraining stage, the bottom layer is greedily pretrained for 200 passes through the entire training set, and each of the rest two layers is greedily pretrained for 50 passes.", "labels": [], "entities": []}, {"text": "For fine-tuning we apply the method of conjugate gradients , with three line searches performed in each pass.", "labels": [], "entities": []}, {"text": "This algorithm is performed for 50 passes to fine-tune the network.", "labels": [], "entities": []}, {"text": "Dataset: we have crawled 20,000 pages of \"solved question\" from the computer and network category of Baidu Zhidao as the cQA corpus.", "labels": [], "entities": [{"text": "cQA corpus", "start_pos": 121, "end_pos": 131, "type": "DATASET", "confidence": 0.9756257236003876}]}, {"text": "Correspondingly we obtain 90,000 threads from ComputerFansClub, which is an online forum on computer knowledge.", "labels": [], "entities": []}, {"text": "We take the forum threads as our forum corpus.", "labels": [], "entities": []}, {"text": "From the cQA corpus, we extract 12,600 human generated QA pairs as the training set without any manual work to label the best answers.", "labels": [], "entities": [{"text": "cQA corpus", "start_pos": 9, "end_pos": 19, "type": "DATASET", "confidence": 0.9759004712104797}]}, {"text": "We get the contents from another 2,000 cQA pages to form a testing set, each content of which includes one question and 4.5 candidate answers on average, with one best answer among them.", "labels": [], "entities": []}, {"text": "To get another testing dataset, we randomly select 2,000 threads from the forum corpus.", "labels": [], "entities": [{"text": "forum corpus", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.7372893393039703}]}, {"text": "For this training set, human work are necessary to label the best answers in the posts of the threads.", "labels": [], "entities": []}, {"text": "There are 7 posts included in each thread on average, among which one question and at least one answer exist.", "labels": [], "entities": []}, {"text": "Baseline: To show the performance of our method, three main popular relevance computing methods for ranking candidate answers are considered as our baselines.", "labels": [], "entities": []}, {"text": "We will briefly introduce them: Cosine Similarity.", "labels": [], "entities": [{"text": "Cosine Similarity", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.615705668926239}]}, {"text": "Given a question q and its candidate answer a, their cosine similarity can be computed as follows: where w q k and w a k stand for the weight of the kth word in the question and the answer respectively.", "labels": [], "entities": []}, {"text": "The weights can beget by computing the product of term frequency (tf ) and inverse document frequency (idf ) HowNet based Similarity.", "labels": [], "entities": [{"text": "term frequency (tf )", "start_pos": 50, "end_pos": 70, "type": "METRIC", "confidence": 0.8668344616889954}, {"text": "HowNet", "start_pos": 109, "end_pos": 115, "type": "DATASET", "confidence": 0.8680238723754883}]}, {"text": "HowNet 6 is an electronic world knowledge system, which serves as a powerful tool for meaning computation inhuman language technology.", "labels": [], "entities": [{"text": "HowNet 6", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.948243647813797}, {"text": "meaning computation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.9226211607456207}]}, {"text": "Normally the similarity between two passages can be calculated by two steps: (1) matching the most semantic-similar words in each passages greedily using the API's provided by HowNet; (2) computing the weighted average similarities of the word pairs.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 176, "end_pos": 182, "type": "DATASET", "confidence": 0.9694934487342834}]}, {"text": "This strategy is taken as a baseline method for computing the relevance between questions and answers.", "labels": [], "entities": []}, {"text": "Given a question q and its candidate answer a, we can construct unigram language model M q and unigram language model M a . Then we compute KLdivergence between M q and M a as below:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on Forum Dataset", "labels": [], "entities": [{"text": "Forum Dataset", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.9620038270950317}]}, {"text": " Table 2: Results on cQA Dataset", "labels": [], "entities": [{"text": "cQA Dataset", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.9861696362495422}]}]}