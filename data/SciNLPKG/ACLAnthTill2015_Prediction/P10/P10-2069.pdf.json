{"title": [{"text": "Learning Better Data Representation using Inference-Driven Metric Learning", "labels": [], "entities": [{"text": "Learning Better Data Representation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6164783909916878}]}], "abstractContent": [{"text": "We initiate a study comparing effectiveness of the transformed spaces learned by recently proposed supervised, and semi-supervised metric learning algorithms to those generated by previously proposed unsupervised dimensionality reduction methods (e.g., PCA).", "labels": [], "entities": []}, {"text": "Through a variety of experiments on different real-world datasets, we find IDML-IT, a semi-supervised metric learning algorithm to be the most effective.", "labels": [], "entities": []}], "introductionContent": [{"text": "Because of the high-dimensional nature of NLP datasets, estimating a large number of parameters (a parameter for each dimension), often from a limited amount of labeled data, is a challenging task for statistical learners.", "labels": [], "entities": []}, {"text": "Faced with this challenge, various unsupervised dimensionality reduction methods have been developed over the years, e.g., Principal Components Analysis (PCA).", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.781832367181778}, {"text": "Principal Components Analysis (PCA)", "start_pos": 123, "end_pos": 158, "type": "TASK", "confidence": 0.7660779456297556}]}, {"text": "Recently, several supervised metric learning algorithms have been proposed.", "labels": [], "entities": []}, {"text": "IDML-IT () is another such method which exploits labeled as well as unlabeled data during metric learning.", "labels": [], "entities": [{"text": "IDML-IT", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8306659460067749}]}, {"text": "These methods learn a Mahalanobis distance metric to compute distance between a pair of data instances, which can also be interpreted as learning a transformation of the input data, as we shall see in Section 2.1.", "labels": [], "entities": []}, {"text": "In this paper, we make the following contributions: Even though different supervised and semisupervised metric learning algorithms have recently been proposed, effectiveness of the transformed spaces learned by them in NLP datasets has not been studied before.", "labels": [], "entities": []}, {"text": "In this paper, we address that gap: we compare effectiveness of classifiers trained on the transformed spaces learned by metric learning methods to those generated by previously proposed unsupervised dimensionality reduction methods.", "labels": [], "entities": []}, {"text": "We find IDML-IT, a semi-supervised metric learning algorithm to be the most effective.", "labels": [], "entities": []}], "datasetContent": [{"text": "Dimension Description of the datasets used during experiments in Section 3 are presented in.", "labels": [], "entities": []}, {"text": "The first four datasets -Electronics, Books, Kitchen, and DVDs -are from the sentiment domain and previously used in (.", "labels": [], "entities": [{"text": "sentiment domain", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.9079810082912445}]}, {"text": "WebKB is a text classification dataset derived from.", "labels": [], "entities": [{"text": "WebKB", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9634317755699158}, {"text": "text classification", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.6999973952770233}]}, {"text": "For details regarding features and data pre-processing, we refer the reader to the origin of these datasets cited above.", "labels": [], "entities": []}, {"text": "One extra preprocessing that we did was that we only considered features which occurred more 20 times in the entire dataset to make the problem more computationally tractable and also since the infrequently occurring features usually contribute noise.", "labels": [], "entities": []}, {"text": "We use classification error (lower is better) as the evaluation metric.", "labels": [], "entities": [{"text": "classification error", "start_pos": 7, "end_pos": 27, "type": "METRIC", "confidence": 0.7471073865890503}]}, {"text": "We experiment with the following ways of estimating transformation matrix P : Original 2 : We set P = I, where I is the d \u00d7 d identity matrix.", "labels": [], "entities": []}, {"text": "Hence, the data is not transformed in this case.", "labels": [], "entities": []}, {"text": "Original RP PCA ITML IDML-IT \u00b5 \u00b1 \u03c3 \u00b5 \u00b1 \u03c3 \u00b5 \u00b1 \u03c3 \u00b5 \u00b1 \u03c3 \u00b5 \u00b1 \u03c3 Electronics 47.9 \u00b1 1.1 49.0 \u00b1 1.2 43.2 \u00b1 0.9 34.9 \u00b1 0.5 34.0\u00b10.5 Books 50.0 \u00b1 1.0 49.4 \u00b1 1.0 47.9 \u00b1 0.7 42.1 \u00b1 0.7 40.6\u00b10.7 Kitchen 49.8 \u00b1 1.1 49.6 \u00b1 0.9 48.6 \u00b1 0.8 31.1 \u00b1 0.5 30.0\u00b10.5 DVDs 50.1 \u00b1 0.5 49.9 \u00b1 0.7 49.4 \u00b1 0.6 42.1 \u00b1 0.4 41.2\u00b10.5 WebKB 33.1 \u00b1 0.4 33.1 \u00b1 0.3 33.1 \u00b1 0.3 30.0 \u00b1 0.4 28.7\u00b10.5: Comparison of transductive % classification errors (lower is better) over graphs constructed using different methods (see Section 3.3), with n l = 50 and nu = 1450.", "labels": [], "entities": []}, {"text": "All results are averaged over ten trials.", "labels": [], "entities": []}, {"text": "All hyperparameters are tuned on a separate random split.: Comparison of transductive % classification errors (lower is better) over graphs constructed using different methods (see Section 3.3), with n l = 100 and nu = 1400.", "labels": [], "entities": []}, {"text": "All results are averaged over ten trials.", "labels": [], "entities": []}, {"text": "All hyperparameters are tuned on a separate random split.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Comparison of transductive % classification errors (lower is better) over graphs constructed  using different methods (see Section 3.3), with n l = 50 and n u = 1450. All results are averaged over  ten trials. All hyperparameters are tuned on a separate random split.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of transductive % classification errors (lower is better) over graphs constructed  using different methods (see Section 3.3), with n l = 100 and n u = 1400. All results are averaged over  ten trials. All hyperparameters are tuned on a separate random split.", "labels": [], "entities": []}]}