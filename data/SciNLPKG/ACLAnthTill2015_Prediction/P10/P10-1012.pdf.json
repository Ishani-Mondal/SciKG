{"title": [{"text": "Automatic Evaluation Method for Machine Translation using Noun-Phrase Chunking", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.793781578540802}]}], "abstractContent": [{"text": "As described in this paper, we propose anew automatic evaluation method for machine translation using noun-phrase chunking.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7709071040153503}, {"text": "noun-phrase chunking", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.7422589361667633}]}, {"text": "Our method correctly determines the matching words between two sentences using corresponding noun phrases.", "labels": [], "entities": []}, {"text": "Moreover, our method determines the similarity between two sentences in terms of the noun-phrase order of appearance.", "labels": [], "entities": []}, {"text": "Evaluation experiments were conducted to calculate the correlation among human judgments, along with the scores produced using automatic evaluation methods for MT outputs obtained from the 12 machine translation systems in NTCIR-7.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 160, "end_pos": 170, "type": "TASK", "confidence": 0.9059378504753113}, {"text": "NTCIR-7", "start_pos": 223, "end_pos": 230, "type": "DATASET", "confidence": 0.9578831195831299}]}, {"text": "Experimental results show that our method obtained the highest correlations among the methods in both sentence-level adequacy and fluency.", "labels": [], "entities": []}], "introductionContent": [{"text": "High-quality automatic evaluation has become increasingly important as various machine translation systems have developed.", "labels": [], "entities": [{"text": "automatic evaluation", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.5420242547988892}, {"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7600761950016022}]}, {"text": "The scores of some automatic evaluation methods can obtain high correlation with human judgment in document-level automatic evaluation.", "labels": [], "entities": [{"text": "document-level automatic evaluation", "start_pos": 99, "end_pos": 134, "type": "TASK", "confidence": 0.5878910919030508}]}, {"text": "However, sentence-level automatic evaluation is insufficient.", "labels": [], "entities": []}, {"text": "A great gap exists between language processing of automatic evaluation and the processing by humans.", "labels": [], "entities": []}, {"text": "Therefore, in recent years, various automatic evaluation methods particularly addressing sentence-level automatic evaluations have been proposed.", "labels": [], "entities": []}, {"text": "Methods based on word strings (e.g., BLEU(), NIST), METEOR(), ROUGE-L(), and IMPACT) calculate matching scores using only common words between MT outputs and references from bilingual humans.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9976053237915039}, {"text": "METEOR", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.966509997844696}, {"text": "ROUGE-L", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9839566349983215}, {"text": "IMPACT", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.82185298204422}, {"text": "MT outputs", "start_pos": 143, "end_pos": 153, "type": "TASK", "confidence": 0.8799924552440643}]}, {"text": "However, these methods cannot determine the correct word correspondences sufficiently because they fail to focus solely on phrase correspondences.", "labels": [], "entities": []}, {"text": "Moreover, various methods using syntactic analytical tools are proposed to address the sentence structure.", "labels": [], "entities": []}, {"text": "Nevertheless, those methods depend strongly on the quality of the syntactic analytical tools.", "labels": [], "entities": []}, {"text": "As described herein, for use with MT systems, we propose anew automatic evaluation method using noun-phrase chunking to obtain higher sentence-level correlations.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.989084780216217}, {"text": "noun-phrase chunking", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.7111228406429291}]}, {"text": "Using noun phrases produced by chunking, our method yields the correct word correspondences and determines the similarity between two sentences in terms of the noun phrase order of appearance.", "labels": [], "entities": []}, {"text": "Evaluation experiments using MT outputs obtained by 12 machine translation systems in NTCIR-7() demonstrate that the scores obtained using our system yield the highest correlation with the human judgments among the automatic evaluation methods in both sentence-level adequacy and fluency.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9601494669914246}, {"text": "NTCIR-7", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9497095942497253}]}, {"text": "Moreover, the differences between correlation coefficients obtained using our method and other methods are statistically significant at the 5% or lower significance level for adequacy.", "labels": [], "entities": []}, {"text": "Results confirmed that our method using noun-phrase chunking is effective for automatic evaluation for machine translation.", "labels": [], "entities": [{"text": "noun-phrase chunking", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7691874206066132}, {"text": "machine translation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7618081867694855}]}], "datasetContent": [{"text": "The system based on our method has four processes.", "labels": [], "entities": []}, {"text": "First, the system determines the corre-spondences of noun phrases between MT outputs and references using chunking.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 74, "end_pos": 84, "type": "TASK", "confidence": 0.8779206573963165}]}, {"text": "Secondly, the system calculates word-level scores based on the correct matched words using the determined correspondences of noun phrases.", "labels": [], "entities": []}, {"text": "Next, the system calculates phrase-level scores based on the noun-phrase order of appearance.", "labels": [], "entities": []}, {"text": "The system calculates the final scores combining word-level scores and phrase-level scores.", "labels": [], "entities": []}, {"text": "We calculated the correlation between the scores obtained using our method and scores produced by human judgment.", "labels": [], "entities": []}, {"text": "The system based on our method obtained the evaluation scores for 1,200 English output sentences related to the patent sentences.", "labels": [], "entities": []}, {"text": "These English output sentences are sentences that 12 machine translation systems in NTCIR-7 translated from 100 Japanese sentences.", "labels": [], "entities": [{"text": "NTCIR-7", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.856985330581665}]}, {"text": "Moreover, the number of references to each English sentence in 100 English sentences is four.", "labels": [], "entities": []}, {"text": "These references were obtained from four bilingual humans.", "labels": [], "entities": []}, {"text": "presents types of the 12 machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7128197997808456}]}, {"text": "Moreover, three human judges evaluated 1,200 English output sentences from the perspective of adequacy and fluency on a scale of 1-5.", "labels": [], "entities": []}, {"text": "We used the median value in the evaluation results of three human judges as the final scores of 1-5.", "labels": [], "entities": []}, {"text": "We calculated Pearson's correlation efficient and Spearman's rank correlation efficient between the scores obtained using our method and the scores by human judgments in terms of sentence-level adequacy and fluency.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 14, "end_pos": 35, "type": "METRIC", "confidence": 0.8468696475028992}]}, {"text": "Additionally, we calculated the correlations between the scores using seven other methods and the scores by human judgments to compare our method with other automatic evaluation methods.", "labels": [], "entities": []}, {"text": "The other seven methods were IMPACT, ROUGE-L, BLEU 1 , NIST, NMG-WN, METEOR 2 , and WER(.", "labels": [], "entities": [{"text": "IMPACT", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.834077775478363}, {"text": "ROUGE-L", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9902646541595459}, {"text": "BLEU 1", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.983110249042511}, {"text": "METEOR 2", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9708903133869171}, {"text": "WER", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.991413950920105}]}, {"text": "Using our method, 0.1 was used as the value of the parameter \u03b1 in Eqs.", "labels": [], "entities": []}, {"text": "(3)-(10) and 1.1 was used as the value of the parameter \u03b2 in Eqs.", "labels": [], "entities": [{"text": "Eqs", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.8818344473838806}]}, {"text": "(1)-(10).", "labels": [], "entities": []}, {"text": "Moreover, 0.3 was used as the value of the parameter \u03b4 in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 58, "end_pos": 60, "type": "DATASET", "confidence": 0.9316058158874512}]}, {"text": "Moreover, we obtained the noun phrases using a shallow parser as the chunking tool.", "labels": [], "entities": []}, {"text": "We revised some erroneous results that were obtained using the chunking tool.", "labels": [], "entities": []}, {"text": "As described in this paper, we performed comparison experiments using our method and seven other methods., bold typeface signifies the maximum correlation coefficients among eight automatic evaluation methods.", "labels": [], "entities": []}, {"text": "Underlining in our method signifies that the differences between correlation coefficients obtained using our method and IMPACT are statistically significant at the 5% significance level.", "labels": [], "entities": [{"text": "IMPACT", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.5467612147331238}]}, {"text": "Moreover, \"Avg.\" signifies the average of the correlation coefficients obtained by 12 machine translation systems in respective automatic evaluation methods, and \"All\" are the correlation coefficients using the scores of 1,200 output sentences obtained using the 12 machine translation systems.", "labels": [], "entities": [{"text": "Avg.", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9982864260673523}]}], "tableCaptions": [{"text": " Table 2: Pearson's correlation coefficient for sentence-level adequacy.  No. 1  No. 2  No. 3  No. 4  No. 5  No. 6  No. 7  Our method  0.7862 0.4989 0.5970 0.5713 0.6581 0.6779 0.7682  IMPACT  0.7639  0.4487  0.5980  0.5371  0.6371  0.6255  0.7249  ROUGE-L  0.7597  0.4264 0.6111 0.5229  0.6183  0.5927  0.7079  BLEU  0.6473  0.2463  0.4230  0.4336  0.3727  0.4124  0.5340  NIST  0.5135  0.2756  0.4142  0.3086  0.2553  0.2300  0.3628  NMG-WN  0.7010  0.3432  0.6067  0.4719  0.5441  0.5885  0.5906  METEOR  0.4509  0.0892  0.3907  0.2781  0.3120  0.2744  0.3937  WER  0.7464  0.4114  0.5519  0.5185  0.5461  0.5970  0.6902  Our method II  0.7870  0.5066  0.5967  0.5191  0.6529  0.6635  0.7698", "labels": [], "entities": [{"text": "BLEU", "start_pos": 312, "end_pos": 316, "type": "METRIC", "confidence": 0.9917259216308594}, {"text": "NIST", "start_pos": 374, "end_pos": 378, "type": "DATASET", "confidence": 0.9204784035682678}]}, {"text": " Table 3: Pearson's correlation coefficient for sentence-level fluency.  No. 1  No. 2  No. 3  No. 4  No. 5  No. 6  No. 7  Our method  0.5853 0.3782 0.5689  0.4673  0.5739 0.5344 0.7193  IMPACT  0.5581  0.3407  0.5821  0.4586 0.5768 0.4852  0.6896  ROUGE-L  0.5551  0.3056 0.5925 0.4391  0.5666  0.4475  0.6756  BLEU  0.4793  0.0963  0.4488  0.3033  0.4690  0.3602  0.5272  NIST  0.4139  0.0257  0.4987  0.1682  0.3923  0.2236  0.3749  NMG-WN  0.5782  0.3090  0.5434 0.4680 0.5070  0.5234  0.5363  METEOR  0.4050  0.1405  0.4420  0.1825  0.4259  0.2336  0.4873  WER  0.5143  0.3031  0.5220  0.4262  0.4936  0.4405  0.6351  Our method II  0.5831  0.3689  0.5753  0.3991  0.5610  0.5445  0.7186", "labels": [], "entities": [{"text": "BLEU", "start_pos": 311, "end_pos": 315, "type": "METRIC", "confidence": 0.993449866771698}, {"text": "NIST", "start_pos": 373, "end_pos": 377, "type": "DATASET", "confidence": 0.9116991758346558}, {"text": "WER", "start_pos": 561, "end_pos": 564, "type": "METRIC", "confidence": 0.87044757604599}]}, {"text": " Table 4: Spearman's rank correlation coefficient for sentence-level adequacy.  No. 1  No. 2  No. 3  No. 4  No. 5  No. 6  No. 7  Our method  0.7456 0.5049 0.5837 0.5146 0.6514 0.6557 0.6746  IMPACT  0.7336  0.4881  0.5992  0.4741  0.6382  0.5841  0.6409  ROUGE-L  0.7304  0.4822 0.6092 0.4572  0.6135  0.5365  0.6368  BLEU  0.5525  0.2206  0.4327  0.3449  0.3230  0.2805  0.4375  NIST  0.5032  0.2438  0.4218  0.2489  0.2342  0.1534  0.3529  NMG-WN  0.7541 0.3829  0.5579  0.4472  0.5560  0.5828  0.6263  METEOR  0.4409  0.1509  0.4018  0.2580  0.3085  0.1991  0.4115  WER  0.6566  0.4147  0.5478  0.4272  0.5524  0.4884  0.5539  Our method II  0.7478  0.4972  0.5817  0.4892  0.6437  0.6428  0.6707", "labels": [], "entities": [{"text": "BLEU", "start_pos": 318, "end_pos": 322, "type": "METRIC", "confidence": 0.9943780303001404}, {"text": "NIST", "start_pos": 380, "end_pos": 384, "type": "DATASET", "confidence": 0.9248250722885132}]}, {"text": " Table 5: Spearman's rank correlation coefficient for sentence-level fluency.  No. 1  No. 2  No. 3  No. 4  No. 5  No. 6  No. 7  Our method  0.5697 0.3299  0.5446  0.4199  0.5733  0.5060 0.6459  IMPACT  0.5481  0.3285  0.5572  0.3976 0.5960 0.4317  0.6334  ROUGE-L  0.5470  0.3041 0.5646 0.3661  0.5638  0.3879  0.6255  BLEU  0.4157  0.0559  0.4286  0.2018  0.4475  0.2569  0.4909  NIST  0.4209  0.0185  0.4559  0.1093  0.3186  0.1898  0.3634  NMG-WN  0.5569 0.3461 0.5381 0.4300 0.5052 0.5264 0.5328  METEOR  0.4608  0.1429  0.4438  0.1783  0.4073  0.1596  0.4821  WER  0.4469  0.2395  0.5087  0.3292  0.4995  0.3482  0.5637  Our method II  0.5659  0.3216  0.5484  0.3773  0.5638  0.5211  0.6343", "labels": [], "entities": [{"text": "BLEU", "start_pos": 319, "end_pos": 323, "type": "METRIC", "confidence": 0.9880956411361694}]}]}