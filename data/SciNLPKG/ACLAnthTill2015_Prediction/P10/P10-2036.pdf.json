{"title": [], "abstractContent": [{"text": "A strong inductive bias is essential in un-supervised grammar induction.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7283843457698822}]}, {"text": "We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types.", "labels": [], "entities": []}, {"text": "Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Gra\u00e7a et al.", "labels": [], "entities": []}, {"text": "In experiments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%.", "labels": [], "entities": [{"text": "standard expectation maximization (EM) baseline", "start_pos": 72, "end_pos": 119, "type": "METRIC", "confidence": 0.6523894497326442}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.6049966812133789}]}, {"text": "Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%.", "labels": [], "entities": []}, {"text": "On English in particular, we show that our approach improves on several other state-of-the-art techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "We investigate an unsupervised learning method for dependency parsing models that imposes sparsity biases on the dependency types.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8371247053146362}]}, {"text": "We assume a corpus annotated with POS tags, where the task is to induce a dependency model from the tags for corpus sentences.", "labels": [], "entities": []}, {"text": "In this setting, the type of a dependency is defined as a pair: tag of the dependent (also known as the child), and tag of the head (also known as the parent).", "labels": [], "entities": []}, {"text": "Given that POS tags are designed to convey information about grammatical relations, it is reasonable to assume that only some of the possible dependency types will be realized fora given language.", "labels": [], "entities": []}, {"text": "For instance, in English it is ungrammatical for nouns to dominate verbs, adjectives to dominate adverbs, and determiners to dominate almost any part of speech.", "labels": [], "entities": []}, {"text": "Thus, the realized dependency types should be a sparse subset of all possible types.", "labels": [], "entities": []}, {"text": "Previous work in unsupervised grammar induction has tried to achieve sparsity through priors., and proposed hierarchical Dirichlet process priors.", "labels": [], "entities": [{"text": "unsupervised grammar induction", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.6854096253712972}]}, {"text": "experimented with a discounting Dirichlet prior, which encourages a standard dependency parsing model (see Section 2) to limit the number of dependent types for each head type.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.6874250769615173}]}, {"text": "Our experiments show a more effective sparsity pattern is one that limits the total number of unique head-dependent tag pairs.", "labels": [], "entities": []}, {"text": "This kind of sparsity bias avoids inducing competition between dependent types for each head type.", "labels": [], "entities": []}, {"text": "We can achieve the desired bias with a constraint on model posteriors during learning, using the posterior regularization (PR) framework (.", "labels": [], "entities": []}, {"text": "Specifically, to implement PR we augment the maximum marginal likelihood objective of the dependency model with a term that penalizes head-dependent tag distributions that are too permissive.", "labels": [], "entities": [{"text": "PR", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9787138104438782}]}, {"text": "Although not focused on sparsity, several other studies use soft parameter sharing to couple different types of dependencies.", "labels": [], "entities": []}, {"text": "To this end, and investigated logistic normal priors, and used a backoff scheme.", "labels": [], "entities": []}, {"text": "We compare to their results in Section 5.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.8035228550434113}]}, {"text": "The remainder of this paper is organized as fol-lows.", "labels": [], "entities": []}, {"text": "Section 2 and 3 review the models and several previous approaches for learning them.", "labels": [], "entities": []}, {"text": "Section 4 describes learning with PR.", "labels": [], "entities": []}, {"text": "Section 5 describes experiments across 12 languages and Section 6 analyzes the results.", "labels": [], "entities": []}, {"text": "For additional details on this work see .", "labels": [], "entities": []}], "datasetContent": [{"text": "We: Attachment accuracy results.", "labels": [], "entities": [{"text": "Attachment", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.8830790519714355}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9286364316940308}]}, {"text": "Column 1: VcVs used for the E-DMV models.", "labels": [], "entities": []}, {"text": "Column 3: Best PR result for each model, which is chosen by applying each of the two types of constraints (PR-S and PR-AS) and trying \u03c3 \u2208 {80, 100, 120, 140, 160, 180}.", "labels": [], "entities": []}, {"text": "Columns 4 & 5: Constraint type and \u03c3 that produced the values in column 3. and Manning, which we refer to as K&M.", "labels": [], "entities": [{"text": "Manning", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9921522736549377}]}, {"text": "We always train for 100 iterations and evaluate on the test set using Viterbi parses.", "labels": [], "entities": []}, {"text": "Before evaluating, we smooth the resulting models by adding e \u221210 to each learned parameter, merely to remove the chance of zero probabilities for unseen events.", "labels": [], "entities": []}, {"text": "(We did not tune this as it should make very little difference for final parses.)", "labels": [], "entities": []}, {"text": "We score models by their attachment accuracy -the fraction of words assigned the correct parent.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.5090760588645935}]}], "tableCaptions": [{"text": " Table 1: Attachment accuracy results. Column 1: Vc-", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9639250636100769}]}, {"text": " Table 2: Comparison with previous published results. Rows", "labels": [], "entities": [{"text": "Rows", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.954345703125}]}, {"text": " Table 3: Attachment accuracy results. The parameters used are the best settings found for English. Values for hyperparameters", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9557166695594788}]}]}