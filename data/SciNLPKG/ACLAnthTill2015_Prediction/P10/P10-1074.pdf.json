{"title": [{"text": "Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data", "labels": [], "entities": [{"text": "Improving Joint Parsing and Named Entity Recognition", "start_pos": 29, "end_pos": 81, "type": "TASK", "confidence": 0.703397159065519}]}], "abstractContent": [{"text": "One of the main obstacles to producing high quality joint models is the lack of jointly annotated data.", "labels": [], "entities": []}, {"text": "Joint model-ing of multiple natural language processing tasks outperforms single-task models learned from the same data, but still under-performs compared to single-task models learned on the more abundant quantities of available single-task annotated data.", "labels": [], "entities": []}, {"text": "In this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model.", "labels": [], "entities": []}, {"text": "Our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model.", "labels": [], "entities": []}, {"text": "Experiments on joint parsing and named entity recognition , using the OntoNotes corpus, show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data.", "labels": [], "entities": [{"text": "joint parsing", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.6761347949504852}, {"text": "named entity recognition", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.6254873871803284}, {"text": "OntoNotes corpus", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.9427512288093567}]}], "introductionContent": [{"text": "Joint learning of multiple types of linguistic structure results in models which produce more consistent outputs, and for which performance improves across all aspects of the joint structure.", "labels": [], "entities": []}, {"text": "Joint models can be particularly useful for producing analyses of sentences which are used as input for higher-level, more semantically-oriented systems, such as question answering and machine translation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.8759572505950928}, {"text": "machine translation", "start_pos": 185, "end_pos": 204, "type": "TASK", "confidence": 0.7886891961097717}]}, {"text": "These high-level systems typically combine the outputs from many low-level systems, such as parsing, named entity recognition (NER) and coreference resolution.", "labels": [], "entities": [{"text": "parsing", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.9689491987228394}, {"text": "named entity recognition (NER)", "start_pos": 101, "end_pos": 131, "type": "TASK", "confidence": 0.7690929770469666}, {"text": "coreference resolution", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.9515362977981567}]}, {"text": "When trained separately, these single-task models can produce outputs which are inconsistent with one another, such as named entities which do not correspond to any nodes in the parse tree (see for an example).", "labels": [], "entities": []}, {"text": "Moreover, one expects that the different types of annotations should provide useful information to one another, and that modeling them jointly should improve performance.", "labels": [], "entities": []}, {"text": "Because a named entity should correspond to anode in the parse tree, strong evidence about either aspect of the model should positively impact the other aspect.", "labels": [], "entities": []}, {"text": "However, designing joint models which actually improve performance has proven challenging.", "labels": [], "entities": []}, {"text": "The CoNLL 2008 shared task () was on joint parsing and semantic role labeling, but the best systems were the ones which completely decoupled the tasks.", "labels": [], "entities": [{"text": "CoNLL 2008 shared task", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8316400349140167}, {"text": "semantic role labeling", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.5869629184405009}]}, {"text": "While negative results are rarely published, this was not the first failed attempt at joint parsing and semantic role labeling).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.5987651944160461}]}, {"text": "There have been some recent successes with joint modeling.", "labels": [], "entities": []}, {"text": "built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and learned a joint model of lemmatization and POS tagging which outperformed a pipelined model.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 133, "end_pos": 144, "type": "TASK", "confidence": 0.7432309985160828}]}, {"text": "presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing.", "labels": [], "entities": [{"text": "joint morphological segmentation and tagging", "start_pos": 48, "end_pos": 92, "type": "TASK", "confidence": 0.641577959060669}, {"text": "parsing of Hebrew", "start_pos": 149, "end_pos": 166, "type": "TASK", "confidence": 0.7951730489730835}]}, {"text": "No discussion of joint modeling would be complete without mention of (), who trained a Collinsstyle generative parser) over a syntactic structure augmented with the template entity and template relations annotations for the MUC-7 shared task.", "labels": [], "entities": [{"text": "Collinsstyle generative parser", "start_pos": 87, "end_pos": 117, "type": "TASK", "confidence": 0.5988162457942963}, {"text": "MUC-7 shared task", "start_pos": 224, "end_pos": 241, "type": "TASK", "confidence": 0.597374161084493}]}, {"text": "One significant limitation for many joint models is the lack of jointly annotated data.", "labels": [], "entities": []}, {"text": "We built a joint model of parsing and named entity recognition (, which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9848243594169617}, {"text": "named entity recognition", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6108546853065491}]}, {"text": "However, the performance of our model, trained using the OntoNotes corpus (), fell short of separate parsing and named entity models trained on larger corpora, annotated with only one type of information.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.900743305683136}]}, {"text": "This paper addresses the problem of how to learn high-quality joint models with smaller quantities of jointly-annotated data that has been augmented with larger amounts of single-task annotated data.", "labels": [], "entities": []}, {"text": "To our knowledge this work is the first attempt at such a task.", "labels": [], "entities": []}, {"text": "We use a hierarchical prior to link a joint model trained on jointly-annotated data with other single-task models trained on single-task annotated data.", "labels": [], "entities": []}, {"text": "The key to making this work is for the joint model to share some features with each of the single-task models.", "labels": [], "entities": []}, {"text": "Then, the singly-annotated data can be used to influence the feature weights for the shared features in the joint model.", "labels": [], "entities": []}, {"text": "This is an important contribution, because it provides all the benefits of joint modeling, but without the high cost of jointly annotating large corpora.", "labels": [], "entities": [{"text": "joint modeling", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.7663683891296387}]}, {"text": "We applied our hierarchical joint model to parsing and named entity recognition, and it reduced errors by over 20% on both tasks when compared to a joint model trained on only the jointly annotated data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.9793841242790222}, {"text": "named entity recognition", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.5969167153040568}, {"text": "errors", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.994289755821228}]}], "datasetContent": [{"text": "We compared our hierarchical joint model to a regular (non-hierarchical) joint model, and to parseonly and NER-only models.", "labels": [], "entities": []}, {"text": "Our baseline experiments were modeled after those in (, and while our results were not identical (we updated to a newer release of the data), we had similar results and found the same general trends with respect to how the joint model improved on the single models.", "labels": [], "entities": []}, {"text": "We used OntoNotes 3.0 (), and made the same data modifications as) to ensure consistency between the parsing and named entity annotations.", "labels": [], "entities": [{"text": "consistency", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.961566686630249}]}, {"text": "gives the number of training and test sentences.", "labels": [], "entities": []}, {"text": "For each section of the data (ABC, MNB, NBC, PRI, VOA) we ran experiments training a linear-chain CRF on only the named entity information, a CRF-CFG parser on only the parse information, a joint parser and named entity recognizer, and our hierarchical model.", "labels": [], "entities": []}, {"text": "For the hierarchical model, we used the CNN portion of the data (5093 sentences) for the extra named entity data (and ignored the parse trees) and the remaining portions combined for the extra parse data (and ignored the named entity annotations).", "labels": [], "entities": []}, {"text": "We used \u03c3 * = 1.0 and \u03c3 m = 0.1, which were chosen based on early experiments on development data.", "labels": [], "entities": []}, {"text": "Small changes to \u03c3 m do not appear to have much influence, but larger changes do.", "labels": [], "entities": []}, {"text": "We similarly decided how many iterations to run stochastic gradient descent for (20) based on early development data experiments.", "labels": [], "entities": []}, {"text": "We did not run this experiment on the CNN portion of the data, because the CNN data was already being used as the extra NER data.", "labels": [], "entities": [{"text": "CNN portion of the data", "start_pos": 38, "end_pos": 61, "type": "DATASET", "confidence": 0.8039808511734009}, {"text": "CNN data", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9028313755989075}, {"text": "NER data", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.732061356306076}]}, {"text": "As shows, the hierarchical model did substantially better than the joint model overall, which is not surprising given the extra data to which it had access.", "labels": [], "entities": []}, {"text": "Looking at the smaller corpora (NBC and MNB) we seethe largest gains, with both parse and NER performance improving by about 8% F1.", "labels": [], "entities": [{"text": "MNB", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.824550211429596}, {"text": "F1", "start_pos": 128, "end_pos": 130, "type": "METRIC", "confidence": 0.9991621971130371}]}, {"text": "ABC saw about a 6% gain on both tasks, and VOA saw a 1% gain on both.", "labels": [], "entities": [{"text": "VOA", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.922410249710083}]}, {"text": "Our one negative result is in the PRI portion: parsing improves slightly, but NER performance decreases by almost 2%.", "labels": [], "entities": [{"text": "PRI", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.844398558139801}, {"text": "parsing", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.9806936383247375}, {"text": "NER", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9105275869369507}]}, {"text": "The same experiment on development data resulted in a performance increase, so we are not sure why we saw a decrease here.", "labels": [], "entities": []}, {"text": "One general trend, which is not surprising, is that the hierarchical model helps the smaller datasets more than the large ones.", "labels": [], "entities": []}, {"text": "The source of this is twofold: lower baselines are generally easier to improve upon, and the larger corpora had less singlyannotated data to provide improvements, because it was composed of the remaining, smaller, sections of OntoNotes.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 226, "end_pos": 235, "type": "DATASET", "confidence": 0.9504879117012024}]}, {"text": "We found it interesting that the gains tended to be similar on both tasks for all datasets, and believe this fact is due to our use of roughly the same amount of singly-annotated data for both parsing and NER.", "labels": [], "entities": [{"text": "parsing", "start_pos": 193, "end_pos": 200, "type": "TASK", "confidence": 0.9698099493980408}, {"text": "NER", "start_pos": 205, "end_pos": 208, "type": "TASK", "confidence": 0.7764143943786621}]}, {"text": "One possible conflating factor in these experiments is that of domain drift.", "labels": [], "entities": []}, {"text": "While we tried to: Full parse and NER results for the six datasets.", "labels": [], "entities": []}, {"text": "Parse trees were evaluated using evalB, and named entities were scored using micro-averaged F-measure (conlleval).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9068015217781067}]}, {"text": "get the most similar annotated data available -data which was annotated by the same annotators, and all of which is broadcast news -these are still different domains.", "labels": [], "entities": []}, {"text": "While this is likely to have a negative effect on results, we also believe this scenario to be a more realistic than if it were to also be data drawn from the exact same distribution.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training and test set sizes for the five  datasets in sentences. The file ranges refer to  the numbers within the names of the original  OntoNotes files.", "labels": [], "entities": [{"text": "OntoNotes files", "start_pos": 147, "end_pos": 162, "type": "DATASET", "confidence": 0.8914242386817932}]}, {"text": " Table 2: Full parse and NER results for the six datasets. Parse trees were evaluated using evalB, and  named entities were scored using micro-averaged F-measure (conlleval).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.8719927668571472}]}]}