{"title": [{"text": "Automatic Evaluation of Linguistic Quality in Multi-Document Summarization", "labels": [], "entities": [{"text": "Summarization", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.7768698334693909}]}], "abstractContent": [{"text": "To date, few attempts have been made to develop and validate methods for automatic evaluation of linguistic quality in text summarization.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.6527966409921646}]}, {"text": "We present the first systematic assessment of several diverse classes of metrics designed to capture various aspects of well-written text.", "labels": [], "entities": []}, {"text": "We train and test linguistic quality models on consecutive years of NIST evaluation data in order to show the generality of results.", "labels": [], "entities": [{"text": "NIST evaluation data", "start_pos": 68, "end_pos": 88, "type": "DATASET", "confidence": 0.8915398716926575}]}, {"text": "For grammaticality, the best results come from a set of syntactic features.", "labels": [], "entities": []}, {"text": "Focus, coherence and referential clarity are best evaluated by a class of features measuring local coherence on the basis of cosine similarity between sentences, coreference information , and summarization specific features.", "labels": [], "entities": [{"text": "referential clarity", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.5731576979160309}]}, {"text": "Our best results are 90% accuracy for pair-wise comparisons of competing systems over a test set of several inputs and 70% for ranking summaries of a specific input.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9996639490127563}]}], "introductionContent": [{"text": "Efforts for the development of automatic text summarizers have focused almost exclusively on improving content selection capabilities of systems, ignoring the linguistic quality of the system output.", "labels": [], "entities": [{"text": "text summarizers", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.7163473963737488}]}, {"text": "Part of the reason for this imbalance is the existence of ROUGE (), the system for automatic evaluation of content selection, which allows for frequent evaluation during system development and for reporting results of experiments performed outside of the annual NIST-led evaluations, the Document Understanding Conference (DUC) and the Text Analysis Conference (TAC) . Few metrics, however, have been proposed for evaluating linguistic quality and none have been validated on data from NIST evaluations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9914729595184326}, {"text": "Text Analysis Conference (TAC)", "start_pos": 336, "end_pos": 366, "type": "TASK", "confidence": 0.810143361488978}]}, {"text": "In their pioneering work on automatic evaluation of summary coherence, provide a correlation analysis between human coherence assessments and (1) semantic relatedness between adjacent sentences and (2) measures that characterize how mentions of the same entity in different syntactic positions are spread across adjacent sentences.", "labels": [], "entities": []}, {"text": "Several of their models exhibit a statistically significant agreement with human ratings and complement each other, yielding an even higher correlation when combined.", "labels": [], "entities": []}, {"text": "Lapata and and both show the effectiveness of entity-based coherence in evaluating summaries.", "labels": [], "entities": []}, {"text": "However, fewer than five automatic summarizers were used in these studies.", "labels": [], "entities": [{"text": "summarizers", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9099963307380676}]}, {"text": "Further, both sets of experiments perform evaluations of mixed sets of human-produced and machine-produced summaries, so the results maybe influenced by the ease of discriminating between a human and machine written summary.", "labels": [], "entities": []}, {"text": "Therefore, we believe it is an open question how well these features predict the quality of automatically generated summaries.", "labels": [], "entities": []}, {"text": "In this work, we focus on linguistic quality evaluation for automatic systems only.", "labels": [], "entities": [{"text": "linguistic quality evaluation", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.6137989858786265}]}, {"text": "We analyze how well different types of features can rank good and poor machine-produced summaries.", "labels": [], "entities": []}, {"text": "Good performance on this task is the most desired property of evaluation metrics during system development.", "labels": [], "entities": []}, {"text": "We begin in Section 2 by reviewing the various aspects of linguistic quality that are relevant for machine-produced summaries and currently used in manual evaluations.", "labels": [], "entities": []}, {"text": "In Section 3, we introduce and motivate diverse classes of features to capture vocabulary, sentence fluency, and local coherence properties of summaries.", "labels": [], "entities": []}, {"text": "We evaluate the predictive power of these linguistic quality metrics by training and testing models on consecutive years of NIST evaluations (data described in Section 4).", "labels": [], "entities": [{"text": "NIST", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.8096708655357361}]}, {"text": "We test the performance of different sets of features separately and in combination with each other (Section 5).", "labels": [], "entities": []}, {"text": "Results are presented in Section 6, showing the robustness of each class and their abilities to reproduce human rankings of systems and summaries with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9948104619979858}]}], "datasetContent": [{"text": "We use the summaries from DUC 2006 for training and feature development and DUC 2007 served as the test set.", "labels": [], "entities": [{"text": "DUC 2006", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.9514803886413574}, {"text": "DUC 2007", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9579023122787476}]}, {"text": "Validating the results on consecutive years of evaluation is important, as results that hold for the data in one year might not carryover to the next, as happened for example in's work.", "labels": [], "entities": []}, {"text": "Following, we report summary ranking accuracy as the fraction of correct pairwise rankings in the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.7355612516403198}]}, {"text": "We use a Ranking SVM (SV M light) to score summaries using our features.", "labels": [], "entities": []}, {"text": "The Ranking SVM seeks to minimize the number of discordant pairs (pairs in which the gold standard has x 1 ranked strictly higher than x 2 , but the learner ranks x 2 strictly higher than x 1 ).", "labels": [], "entities": []}, {"text": "The output of the ranker is always areal valued score, so a global rank order is always obtained.", "labels": [], "entities": []}, {"text": "The default regularization parameter was used.", "labels": [], "entities": []}, {"text": "We examine the predictive power of our features for each of the five linguistic quality questions in two settings.", "labels": [], "entities": []}, {"text": "In system-level evaluation, we would like to rank all participating systems according to their performance on the entire test set.", "labels": [], "entities": []}, {"text": "In inputlevel evaluation, we would like to rank all summaries produced fora single given input.", "labels": [], "entities": []}, {"text": "For input-level evaluation, the pairs are formed from summaries of the same input.", "labels": [], "entities": []}, {"text": "Pairs in which the gold standard ratings are tied are not included.", "labels": [], "entities": [{"text": "gold standard ratings", "start_pos": 19, "end_pos": 40, "type": "METRIC", "confidence": 0.8203485409418741}]}, {"text": "After removing the ties, the test set consists of 13K to 16K pairs for each linguistic quality question.", "labels": [], "entities": []}, {"text": "Note that there were 45 inputs and 32 automatic systems in DUC 2007.", "labels": [], "entities": [{"text": "DUC 2007", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.9706633687019348}]}, {"text": "So, there area total of 45\u00b7 32 2 = 22, 320 possible summary pairs.", "labels": [], "entities": []}, {"text": "For system-level evaluation, we treat the realvalued output of the SVM ranker for each summary as the linguistic quality score.", "labels": [], "entities": []}, {"text": "The 45 individual scores for summaries produced by a given system are averaged to obtain an overall score for the system.", "labels": [], "entities": []}, {"text": "The gold-standard system-level quality rating is equal to the average human ratings for the system's summaries over the 45 inputs.", "labels": [], "entities": []}, {"text": "At the system level, there are about 500 non-tied pairs in the test set for each question.", "labels": [], "entities": []}, {"text": "For both evaluation settings, a random baseline which ranked the summaries in a random order would have an expected pairwise accuracy of 50%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9597617983818054}]}, {"text": "System-level accuracies for each class of features are shown in.", "labels": [], "entities": []}, {"text": "All classes of features perform well, with at least a 20% absolute increase inaccuracy over the random baseline (50% accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9982641339302063}]}, {"text": "For each of the linguistic quality questions, the corresponding best class of features gives prediction accuracies around 90%.", "labels": [], "entities": [{"text": "prediction accuracies", "start_pos": 93, "end_pos": 114, "type": "METRIC", "confidence": 0.7994681894779205}]}, {"text": "In other words, if these features were used to fully automatically compare systems that participated in the 2007 DUC evaluation, only one out often comparisons would have been incorrect.", "labels": [], "entities": [{"text": "DUC evaluation", "start_pos": 113, "end_pos": 127, "type": "DATASET", "confidence": 0.7686952650547028}]}, {"text": "These results set a high standard for future work on automatic system-level evaluation of linguistic quality.", "labels": [], "entities": []}, {"text": "The state-of-the-art entity coherence features perform well but are not the best for any of the five aspects of linguistic quality.", "labels": [], "entities": []}, {"text": "As expected, sentence fluency is the best feature class for grammaticality.", "labels": [], "entities": [{"text": "sentence fluency", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.6943826526403427}]}, {"text": "For all four other questions, the best feature set is Continuity, which is a combination of summarization specific features, coreference features and cosine similarity of adjacent sentences.", "labels": [], "entities": []}, {"text": "Continuity features outperform entity coherence by 3 to 4% absolute difference on referential quality, focus, and coherence.", "labels": [], "entities": []}, {"text": "Accuracies from the language: System-level prediction accuracies (%) model features are within 1% of entity coherence for these three aspects of summary quality.", "labels": [], "entities": []}, {"text": "Coh-Metrix, which has been proposed as a comprehensive characterization of text, does not perform as well as the language model and the entity coherence classes, which contain considerably fewer features related to only one aspect of text.", "labels": [], "entities": []}, {"text": "The classes of features specific to named entities and noun phrase syntax are the weakest predictors.", "labels": [], "entities": []}, {"text": "It is apparent from the results that continuity, entity coherence, sentence fluency and language models are the most powerful classes of features that should be used in automation of evaluation and against which novel predictors of text quality should be compared.", "labels": [], "entities": [{"text": "continuity", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9779148101806641}]}, {"text": "Combining all feature classes with the meta ranker only yields higher results for grammaticality.", "labels": [], "entities": []}, {"text": "For the other aspects of linguistic quality, it is better to use Continuity by itself to rank systems.", "labels": [], "entities": []}, {"text": "One certainly unexpected result is that features designed to capture one aspect of well-written text turnout to perform well for other questions as well.", "labels": [], "entities": []}, {"text": "For instance, entity coherence and continuity features predict grammaticality with very high accuracy of around 90%, and are surpassed only by the sentence fluency features.", "labels": [], "entities": [{"text": "continuity", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9680876135826111}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9987288117408752}]}, {"text": "These findings warrant further investigation because we would not expect characteristics of local transitions indicative of text structure to have anything to do with sentence grammaticality or fluency.", "labels": [], "entities": []}, {"text": "The results are probably due to the significant correlation between structure and grammaticality.", "labels": [], "entities": []}, {"text": "The results of the input-level ranking experiments are shown in.", "labels": [], "entities": []}, {"text": "Understandably, inputlevel prediction is more difficult and the results are lower compared to the system-level predictions: even with wrong predictions for some of the summaries by two systems, the overall judgment that one system is better than the other over the entire test set can still be accurate.", "labels": [], "entities": [{"text": "inputlevel prediction", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.7747303247451782}]}, {"text": "While for system-level predictions the meta ranker was only useful for grammaticality, at the input level it outperforms every individual feature class for each of the five questions, obtaining accuracies around 70%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 194, "end_pos": 204, "type": "METRIC", "confidence": 0.9937447905540466}]}, {"text": "These input-level accuracies compare favorably with automatic evaluation metrics for other natural language processing tasks.", "labels": [], "entities": []}, {"text": "For example, at the 2008 ACL Workshop on Statistical Machine Translation, all fifteen automatic evaluation metrics, including variants of BLEU scores, achieved between 42% and 56% pairwise accuracy with human judgments at the sentence level).", "labels": [], "entities": [{"text": "ACL Workshop on Statistical Machine Translation", "start_pos": 25, "end_pos": 72, "type": "TASK", "confidence": 0.5744908501704534}, {"text": "BLEU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9976275563240051}, {"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.8771228194236755}]}, {"text": "As in system-level prediction, for referential clarity, focus, and structure, the best feature class is Continuity.", "labels": [], "entities": [{"text": "system-level prediction", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.7657163739204407}, {"text": "referential clarity", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.6489544212818146}]}, {"text": "Sentence fluency again is the best class for identifying grammaticality.", "labels": [], "entities": [{"text": "Sentence fluency", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8835461437702179}, {"text": "identifying grammaticality", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.8747783899307251}]}, {"text": "Coh-Metrix features are now best for determining redundancy.", "labels": [], "entities": []}, {"text": "Both Coh-Metrix and Continuity (the top two features for redundancy) include overlap measures between adjacent sentences, which serve as a good proxy for redundancy.", "labels": [], "entities": []}, {"text": "Surprisingly, the relative performance of the feature classes at input level is not the same as for system-level prediction.", "labels": [], "entities": [{"text": "system-level prediction", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.684677243232727}]}, {"text": "For example, the language model features, which are the second best class for the system-level, do not fare as well at the input-level.", "labels": [], "entities": []}, {"text": "Word co-occurrence which obtained good accuracies at the system level is the least useful class at the input level with accuracies just above chance in all cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman correlations between the man- ual ratings for systems averaged over the 50 inputs  in 2006; * p < .05", "labels": [], "entities": []}, {"text": " Table 2: System-level prediction accuracies (%)", "labels": [], "entities": [{"text": "System-level prediction", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7155380845069885}, {"text": "accuracies", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.5858371257781982}]}, {"text": " Table 3: Input-level prediction accuracies (%)", "labels": [], "entities": [{"text": "Input-level prediction accuracies", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.8114002744356791}]}, {"text": " Table 4: Ablation within the Continuity class;  pairwise accuracy for input-level predictions (%)", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9971988201141357}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9678993821144104}]}, {"text": " Table 5: Input-level prediction accuracies for  human-written summaries (%)", "labels": [], "entities": [{"text": "Input-level", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9395672082901001}]}]}