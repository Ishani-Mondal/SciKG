{"title": [{"text": "Hard Constraints for Grammatical Function Labelling", "labels": [], "entities": [{"text": "Grammatical Function Labelling", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.8035005927085876}]}], "abstractContent": [{"text": "For languages with (semi-) free word order (such as German), labelling grammatical functions on top of phrase-structural constituent analyses is crucial for making them interpretable.", "labels": [], "entities": []}, {"text": "Unfortunately, most statistical classifiers consider only local information for function labelling and fail to capture important restrictions on the distribution of core argument functions such as subject, object etc., namely that there is at most one subject (etc.) per clause.", "labels": [], "entities": []}, {"text": "We augment a statistical classifier with an integer linear program imposing hard linguistic constraints on the solution space output by the classifier, capturing global distributional restrictions.", "labels": [], "entities": []}, {"text": "We show that this improves labelling quality, in particular for argument grammatical functions , in an intrinsic evaluation, and, importantly , grammar coverage for treebank-based (Lexical-Functional) grammar acquisition and parsing, in an extrinsic evaluation .", "labels": [], "entities": [{"text": "Lexical-Functional) grammar acquisition and parsing", "start_pos": 181, "end_pos": 232, "type": "TASK", "confidence": 0.5924830089012781}]}], "introductionContent": [{"text": "Phrase or constituent structure is often regarded as an analysis step guiding semantic interpretation, while grammatical functions (i. e. subject, object, modifier etc.) provide important information relevant to determining predicate-argument structure.", "labels": [], "entities": [{"text": "Phrase or constituent structure", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.817451000213623}, {"text": "semantic interpretation", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.7269857674837112}]}, {"text": "In languages with restricted word order (e. g. English), core grammatical functions can often be recovered from configurational information in constituent structure analyses.", "labels": [], "entities": []}, {"text": "By contrast, simple constituent structures are not sufficient for less configurational languages, which tend to encode grammatical functions by morphological means).", "labels": [], "entities": []}, {"text": "Case features, for instance, can be important indicators of grammatical functions.", "labels": [], "entities": []}, {"text": "Unfortunately, many of these languages (including German) exhibit strong syncretism where morphological cues can be highly ambiguous with respect to functional information.", "labels": [], "entities": []}, {"text": "Statistical classifiers have been successfully used to label constituent structure parser output with grammatical function information).", "labels": [], "entities": [{"text": "label constituent structure parser output", "start_pos": 55, "end_pos": 96, "type": "TASK", "confidence": 0.6778049170970917}]}, {"text": "However, as these approaches tend to use only limited and local context information for learning and prediction, they often fail to enforce simple yet important global linguistic constraints that exist for most languages, e. g. that there will beat most one subject (object) per sentence/clause.", "labels": [], "entities": []}, {"text": "\"Hard\" linguistic constraints, such as these, tend to affect mostly the \"core grammatical functions\", i. e. the argument functions (rather thane. g. adjuncts) of a particular predicate.", "labels": [], "entities": []}, {"text": "As these functions constitute the core meaning of a sentence (as in: who did what to whom), it is important to get them right.", "labels": [], "entities": []}, {"text": "We present a system that adds grammatical function labels to constituent parser output for German in a postprocessing step.", "labels": [], "entities": []}, {"text": "We combine a statistical classifier with an integer linear program (ILP) to model non-violable global linguistic constraints, restricting the solution space of the classifier to those labellings that comply with our set of global constraints.", "labels": [], "entities": []}, {"text": "There are, of course, many other ways of including functional information into the output of a syntactic parser.", "labels": [], "entities": []}, {"text": "show that merging some linguistically motivated function labels with specific syntactic categories can improve the performance of a PCFG model on Penn-II En-glish data.", "labels": [], "entities": [{"text": "Penn-II En-glish data", "start_pos": 146, "end_pos": 167, "type": "DATASET", "confidence": 0.9286878705024719}]}, {"text": "present a statistical model that alternates between functional and configurational information for constituency tree parsing and Hebrew data.", "labels": [], "entities": [{"text": "constituency tree parsing", "start_pos": 99, "end_pos": 124, "type": "TASK", "confidence": 0.6609620054562887}]}, {"text": "Dependency parsers like the MST parser) and Malt parser () use function labels as core part of their underlying formalism.", "labels": [], "entities": [{"text": "Dependency parsers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.753484308719635}]}, {"text": "In this paper, we focus on phrase structure parsing with function labelling as a post-processing step.", "labels": [], "entities": [{"text": "phrase structure parsing", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.8742804924647013}]}, {"text": "Integer linear programs have already been successfully used in related fields including semantic role labelling (), relation and entity classification ( ), sentence compression and dependency parsing.", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.6735764940579733}, {"text": "relation and entity classification", "start_pos": 116, "end_pos": 150, "type": "TASK", "confidence": 0.6699158400297165}, {"text": "sentence compression", "start_pos": 156, "end_pos": 176, "type": "TASK", "confidence": 0.7995781302452087}, {"text": "dependency parsing", "start_pos": 181, "end_pos": 199, "type": "TASK", "confidence": 0.8177061378955841}]}, {"text": "Early work on function labelling for German () reports 94.2% accuracy on gold data (a very early version of the TiGer Treebank () using Markov models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9994577765464783}, {"text": "TiGer Treebank", "start_pos": 112, "end_pos": 126, "type": "DATASET", "confidence": 0.9187572002410889}]}, {"text": "uses a system similar to -but more restricted than -ours to label syntactic chunks derived from the TiGer Treebank.", "labels": [], "entities": [{"text": "TiGer Treebank", "start_pos": 100, "end_pos": 114, "type": "DATASET", "confidence": 0.9514391720294952}]}, {"text": "His research focusses on the correct selection of predefined subcategorisation frames fora verb (see also).", "labels": [], "entities": []}, {"text": "By contrast, our research does not involve subcategorisation frames as an external resource, instead opting fora less knowledge-intensive approach.", "labels": [], "entities": []}, {"text": "Klenner's system was evaluated on gold treebank data and used a small set of 7 dependency labels.", "labels": [], "entities": [{"text": "gold treebank data", "start_pos": 34, "end_pos": 52, "type": "DATASET", "confidence": 0.7808247009913126}]}, {"text": "We show that an ILP-based approach can be scaled to a large and comprehensive set of 42 labels, achieving 97.99% label accuracy on gold standard trees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9613949060440063}]}, {"text": "Furthermore, we apply the system to automatically parsed data using a state-ofthe-art statistical phrase-structure parser with a label accuracy of 94.10%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.8195653557777405}]}, {"text": "In both cases, the ILPbased approach improves the quality of argument function labelling when compared with a non-ILPapproach.", "labels": [], "entities": []}, {"text": "Finally, we show that the approach substantially improves the quality and coverage (from 93.6% to 98.4%) of treebank-based LexicalFunctional Grammars for German over previous work in.", "labels": [], "entities": [{"text": "coverage", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.987231969833374}]}, {"text": "The paper is structured as follows: Section 2 presents basic data demonstrating the challenges presented by German word order and case syncretism for the function labeller.", "labels": [], "entities": []}, {"text": "Section 3 de-2 shows that for our data a model with merged category and function labels (but without hard constraints!) performs slightly worse than the ILP approach developed in this paper.", "labels": [], "entities": []}, {"text": "scribes the labeller including the feature model of the classifier and the integer linear program used to pick the correct labelling.", "labels": [], "entities": []}, {"text": "The evaluation part (Section 4) is split into an intrinsic evaluation measuring the quality of the labelling directly using the German TiGer Treebank (, and an extrinsic evaluation where we test the impact of the constraint-based labelling on treebankbased automatic LFG grammar acquisition.", "labels": [], "entities": [{"text": "German TiGer Treebank", "start_pos": 128, "end_pos": 149, "type": "DATASET", "confidence": 0.8439500530560812}, {"text": "LFG grammar acquisition", "start_pos": 267, "end_pos": 290, "type": "TASK", "confidence": 0.6483538349469503}]}], "datasetContent": [{"text": "We conducted a number of experiments using 1,866 sentences of the TiGer Dependency Bank () as our test set.", "labels": [], "entities": [{"text": "TiGer Dependency Bank", "start_pos": 66, "end_pos": 87, "type": "DATASET", "confidence": 0.9519685109456381}]}, {"text": "The TiGerDB is apart of the TiGer Treebank semi-automatically converted into a dependency representation.", "labels": [], "entities": [{"text": "TiGer Treebank", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.8469258248806}]}, {"text": "We use the manually labelled TiGer trees corresponding to the sentences in the TiGerDB for assessing the labelling quality in the intrinsic evaluation, and the dependencies from TiGerDB for assessing the quality and coverage of the automatically acquired LFG resources in the extrinsic evaluation.", "labels": [], "entities": []}, {"text": "In order to test on real parser output, the test set was parsed with the Berkeley Parser () trained on 48k sentences of the TiGer corpus, excluding the test set.", "labels": [], "entities": [{"text": "TiGer corpus", "start_pos": 124, "end_pos": 136, "type": "DATASET", "confidence": 0.9178158938884735}]}, {"text": "Since the Berkeley Parser assumes projective structures, the training data and test data were made projective by raising non-projective nodes in the tree The maximum entropy classifier of the function labeller was trained on 46,473 sentences of the TiGer Treebank (excluding the test set) which yields about 1.2 million nodes as training samples.", "labels": [], "entities": [{"text": "TiGer Treebank", "start_pos": 249, "end_pos": 263, "type": "DATASET", "confidence": 0.9088494777679443}]}, {"text": "For training the Maximum Entropy Model, we used the BLMVM algorithm) with a width factor of 1.0 ( implemented in an open-source C++ library from Tsujii Laboratory.", "labels": [], "entities": [{"text": "BLMVM", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.963439404964447}]}, {"text": "The integer linear program was solved with the simplex algorithm in combination with a branch-and-bound method using the freely available GLPK.", "labels": [], "entities": [{"text": "GLPK", "start_pos": 138, "end_pos": 142, "type": "DATASET", "confidence": 0.9505196809768677}]}, {"text": "In the intrinsic evaluation, we measured the quality of the labelling itself.", "labels": [], "entities": []}, {"text": "We used the node span evaluation method of) which takes only those nodes into account which have been recognised correctly by the parser, i.e. if there are two nodes in the parse and the reference treebank tree which cover the same word span.", "labels": [], "entities": []}, {"text": "Unlike Blaheta and Charniak (2000) however, we do not require the two nodes to carry the same syntactic category label.", "labels": [], "entities": []}, {"text": "12 shows the results of the node span evaluation.", "labels": [], "entities": []}, {"text": "The labeller achieves close to 98% label accuracy on gold treebank trees which shows that the feature model captures the differences between the individual labels well.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.8926743268966675}]}, {"text": "Results on parser output are about 4 percentage points (absolute) lower as parsing errors can distort local context features for the classifier even if the node itself has been parsed correctly.", "labels": [], "entities": []}, {"text": "The addition of the ILP constraints improves results only slightly since the constraints affect only (a small number of) argument labels while the evaluation considers all 40 labels occurring in the test set.", "labels": [], "entities": []}, {"text": "Since the constraints restrict the selection of certain labels, a less probable label has to be picked by the labeller if the most probable is not available.", "labels": [], "entities": []}, {"text": "If the classifier is ranking labels sensibly, the correct label should emerge.", "labels": [], "entities": []}, {"text": "However, with an incorrect ranking, the ILP constraints might also introduce new errors.: label accuracy and error reduction (all labels) for node span evaluation, * statistically significant, sign test, \u03b1 = 0.01 ( As the main target of the constraint set are argument functions, we also tested the quality of argument labels.: node span results for the test set, argument functions only (SB, EP, PD, OA, OA2, DA, OG, OP, OC), * statistically significant, sign test, \u03b1 = 0.01 ( For comparison and to establish a highly competitive baseline, we use the best-scoring system in (), trained and tested on exactly the same data sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.8432517647743225}]}, {"text": "This purely statistical labeller achieves accuracy of 96.44% (gold) and 92.81% (parser) for all labels, and fscores of 89.88% (gold) and 84.98% (parser) for argument labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9995891451835632}, {"text": "fscores", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9984084963798523}]}, {"text": "show that our system (with and even without ILP constraints) comprehensively outperforms all corresponding baseline scores.", "labels": [], "entities": []}, {"text": "The node span evaluation defines a correct labelling by taking only those nodes (in parser output) into account that have a corresponding node in the reference tree.", "labels": [], "entities": []}, {"text": "However, as this restricts attention to correctly parsed nodes, the results are somewhat over-optimistic.", "labels": [], "entities": []}, {"text": "provides the results obtained from an evalb evaluation of the same data sets.", "labels": [], "entities": []}, {"text": "The gold standard scores are high confirming our previous findings about the performance of the function labeller.", "labels": [], "entities": []}, {"text": "However, the results on parser output are much worse.", "labels": [], "entities": []}, {"text": "The evaluation scores are now taking the parsing quality into account.", "labels": [], "entities": [{"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9678179025650024}]}, {"text": "The considerable drop in quality between gold trees and parser output clearly shows that a good parse tree is an important prerequisite for reasonable function labelling.", "labels": [], "entities": []}, {"text": "This is in accordance with previous findings by who emphasise the importance of syntactic parsing for the closely related task of semantic role labelling.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7347239851951599}, {"text": "semantic role labelling", "start_pos": 130, "end_pos": 153, "type": "TASK", "confidence": 0.6357838213443756}]}, {"text": "Over the last number of years, treebank-based deep grammar acquisition has emerged as an attractive alternative to hand-crafting resources within the HPSG, CCG and LFG paradigms ().", "labels": [], "entities": [{"text": "treebank-based deep grammar acquisition", "start_pos": 31, "end_pos": 70, "type": "TASK", "confidence": 0.6085415929555893}, {"text": "HPSG", "start_pos": 150, "end_pos": 154, "type": "DATASET", "confidence": 0.9364572763442993}]}, {"text": "While most of the initial development work focussed on English, more recently efforts have branched to other languages.", "labels": [], "entities": []}, {"text": "Below we concentrate on LFG.) is a constraint-based theory of grammar with minimally two levels of representation: c(onstituent)-structure and f(unctional)-structure.", "labels": [], "entities": []}, {"text": "C-structure (CFG trees) captures language specific surface configurations such as word order and the hierarchical grouping of words into phrases, while f-structure represents more abstract (and somewhat more language independent) grammatical relations (essentially bilexical labelled dependencies with some morphological and semantic information, approximating to basic predicate-argument structures) in the form of attribute-value structures.", "labels": [], "entities": []}, {"text": "F-structures are defined in terms of equations annotated to nodes in c-structure trees (grammar rules).", "labels": [], "entities": []}, {"text": "Treebank-based LFG acquisition was originally developed for English and is based on an f-structure annotation algorithm that annotates c-structure trees (from a treebank or parser output) with f-structure equations, which are read off of the tree and passed onto a constraint solver producing an f-structure for the given sentence.", "labels": [], "entities": [{"text": "Treebank-based LFG acquisition", "start_pos": 0, "end_pos": 30, "type": "DATASET", "confidence": 0.8264164328575134}]}, {"text": "The English annotation algorithm (for Penn-II treebank-style trees) relies heavily on configurational and categorial information, translating this into grammatical functional information (subject, object etc.) represented at f-structure.", "labels": [], "entities": [{"text": "Penn-II treebank-style trees", "start_pos": 38, "end_pos": 66, "type": "DATASET", "confidence": 0.9591412544250488}]}, {"text": "LFG is \"functional\" in the mathematical sense, in that argument grammatical functions have to be single valued (there cannot be two or more subjects etc. in the same clause).", "labels": [], "entities": []}, {"text": "In fact, if two or more values are assigned to a single argument grammatical function in a local tree, the LFG constraint solver will produce a clash (i. e. it will fail to produce an f-structure) and the sentence will be considered ungrammatical (in other words, the corresponding c-structure tree will be uninterpretable).", "labels": [], "entities": []}, {"text": "Rehbein and develop an f-structure annotation algorithm for German based on the TiGer treebank resource.", "labels": [], "entities": [{"text": "TiGer treebank resource", "start_pos": 80, "end_pos": 103, "type": "DATASET", "confidence": 0.9485579133033752}]}, {"text": "Unlike the English annotation algorithm and because of the language-particular properties of German (see Section 2), the German annotation algorithm cannot rely on c-structure configurational information, but instead heavily uses TiGer function labels in the treebank.", "labels": [], "entities": []}, {"text": "Learning function labels is therefore crucial to the German LFG annotation algorithm, in particular when parsing raw text.", "labels": [], "entities": [{"text": "parsing raw text", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.8935503959655762}]}, {"text": "Because of the strong case syncretism in German, traditional classification models using local information only run the risk of predicting multiple occurences of the same function (subject, object etc.) at the same level, causing feature clashes in the constraint solver with no f-structure being produced. and identify this as a major problem resulting in a considerable loss in coverage of the German annotation algorithm compared to English, in particular for parsing raw text, where TiGer function labels have to be supplied by a machine-learning-based method and where the coverage of the LFG annotation algorithm drops to 93.62% with corresponding drops in recall and f-scores for the f-structure evaluations.", "labels": [], "entities": [{"text": "parsing raw text", "start_pos": 463, "end_pos": 479, "type": "TASK", "confidence": 0.9044456283251444}, {"text": "recall", "start_pos": 663, "end_pos": 669, "type": "METRIC", "confidence": 0.9987394213676453}]}, {"text": "Below we test whether the coverage problems caused by incorrect multiple assignments of grammatical functions can be addressed using the combination of classifier with ILP constraints developed in this paper.", "labels": [], "entities": []}, {"text": "We report experiments where automatically parsed and labelled data are handed over to an LFG f-structure computation algorithm.", "labels": [], "entities": []}, {"text": "The f-structures produced are converted into a dependency triple representation () and evaluated against TiGerDB.: f-structure evaluation results for the test set against TigerDB shows the results of the f-structure evaluation against TiGerDB, with 84.07% f-score upper-bound results for the f-structure annotation algorithm on the original TiGer treebank trees with hand-annotated function labels.", "labels": [], "entities": [{"text": "TigerDB", "start_pos": 171, "end_pos": 178, "type": "DATASET", "confidence": 0.97214275598526}, {"text": "TiGer treebank trees", "start_pos": 341, "end_pos": 361, "type": "DATASET", "confidence": 0.907804528872172}]}, {"text": "Using the function labeller without ILP constraints results in drastic drops in coverage (between 4.5% and 6.5% points absolute) and hence recall (6% and 12%) and f-score (3.5% and 9.5%) for both gold trees and parser output (compared to upper bounds).", "labels": [], "entities": [{"text": "coverage", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9841492772102356}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9994729161262512}, {"text": "f-score", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.9856815338134766}]}, {"text": "By contrast, with ILP constraints, the loss in coverage observed above almost completely disappears and recall and f-scores improve by between 4.4% and 5.5% (recall) and 3% (f-score) absolute (over without ILP constraints).", "labels": [], "entities": [{"text": "coverage", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.983428418636322}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9994363188743591}, {"text": "f-scores", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.796826958656311}, {"text": "recall", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.9951966404914856}]}, {"text": "For comparison, we repeated the experiment using the bestscoring method of.", "labels": [], "entities": []}, {"text": "Rehbein trains the Berkeley Parser to learn an extended category set, merging TiGer function labels with syntactic categories, where the parser outputs fully-labelled trees.", "labels": [], "entities": []}, {"text": "The results show that this approach suffers from the same drop in coverage as the classifier without ILP constraints, with recall about 7% and f-score about 4% (absolute) lower than for the classifier with ILP constraints.", "labels": [], "entities": [{"text": "coverage", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9980169534683228}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9993138313293457}, {"text": "f-score", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.962399959564209}]}, {"text": "shows the dramatic effect of the ILP constraints on the number of sentences in the test set that have multiple argument functions of the same type within the same clause.", "labels": [], "entities": []}, {"text": "With ILP constraints, the problem disappears and therefore, less feature-clashes occur during f-structure computation.", "labels": [], "entities": []}, {"text": "no constraints constraints gold 185 0 parser 212 0 In order to assess whether ILP constraints help with coverage only or whether they affect the quality of the f-structures as well, we repeat the experiment in, however this time evaluating only on those sentences that receive an f-structure, ignoring the rest.", "labels": [], "entities": []}, {"text": "shows that the impact of ILP constraints on quality is much less dramatic than on coverage, with only very small variations in precison, recall and f-scores across the board, and small increases over  Early work on automatic LFG acquisition and parsing for German is presented in, adapting the English Annotation Algorithm to an earlier and smaller version of the TiGer treebank (without morphological information) and training a parser to learn merged Tiger function-category labels, and reporting 95.75% coverage and an f-score of 74.56% f-structure quality against 2,000 gold treebank trees automatically converted into f-structures.", "labels": [], "entities": [{"text": "coverage", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9693235158920288}, {"text": "precison", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9900526404380798}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9866166114807129}, {"text": "LFG acquisition and parsing", "start_pos": 225, "end_pos": 252, "type": "TASK", "confidence": 0.609798900783062}, {"text": "TiGer treebank", "start_pos": 364, "end_pos": 378, "type": "DATASET", "confidence": 0.8767161965370178}]}, {"text": "uses the larger Release 2 of the treebank (with morphological information) reporting 77.79% f-score and coverage of 93.62% (Ta-ble 8) against the dependencies in the TiGerDB test set.", "labels": [], "entities": [{"text": "f-score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9929932951927185}, {"text": "coverage", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9975501894950867}, {"text": "Ta-ble 8)", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.8643472194671631}, {"text": "TiGerDB test set", "start_pos": 166, "end_pos": 182, "type": "DATASET", "confidence": 0.9398542046546936}]}, {"text": "The only rule-based approach to German LFG-parsing we are aware of is the hand-crafted German grammar in the ParGram Project).", "labels": [], "entities": [{"text": "ParGram Project", "start_pos": 109, "end_pos": 124, "type": "DATASET", "confidence": 0.9189853370189667}]}, {"text": "Forst (2007) reports 83.01% dependency f-score evaluated against a set of 1,497 sentences of the TiGerDB.", "labels": [], "entities": [{"text": "83.01% dependency f-score", "start_pos": 21, "end_pos": 46, "type": "METRIC", "confidence": 0.6203419715166092}, {"text": "TiGerDB", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9185604453086853}]}, {"text": "It is very difficult to compare results across the board, as individual papers use (i) different versions of the treebank, (ii) different (sections of) gold-standards to evaluate against (gold TiGer trees in TigerDB, the dependency representations provided by TigerDB, automatically generated gold-standards etc.) and (iii) different label/grammatical function sets.", "labels": [], "entities": [{"text": "TigerDB", "start_pos": 208, "end_pos": 215, "type": "DATASET", "confidence": 0.8608927726745605}]}, {"text": "Furthermore, (iv) coverage differs drastically (with the hand-crafted LFG resources achieving about 80% full f-structures) and finally, (v) some of the grammars evaluated having been used in the generation of the gold standards, possibly introducing a bias towards these resources: the German hand-crafted LFG was used to produce).", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9941231608390808}]}, {"text": "In order to put the results into some perspective, shows an evaluation of our resources against a set of automatically generated gold standard f-structures produced by using the f-structure annotation algorithm on the original hand-labelled TiGer gold trees in the section corresponding to TiGerDB: without ILP constraints we achieve a dependency f-score of", "labels": [], "entities": [{"text": "TiGer gold trees", "start_pos": 241, "end_pos": 257, "type": "DATASET", "confidence": 0.8620293935139974}, {"text": "TiGerDB", "start_pos": 290, "end_pos": 297, "type": "DATASET", "confidence": 0.9047601819038391}]}], "tableCaptions": [{"text": " Table 2: label accuracy and error reduction (all labels) for  node span evaluation, * statistically significant, sign test, \u03b1 =  0.01 (", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9124174118041992}]}, {"text": " Table 3: node span results for the test set, argument functions  only (SB, EP, PD, OA, OA2, DA, OG, OP, OC), * statistically  significant, sign test, \u03b1 = 0.01 (", "labels": [], "entities": []}, {"text": " Table 4: evalb results for the test set", "labels": [], "entities": []}, {"text": " Table 5: node span results for the test set using constraints  with automatically extracted subcat frames", "labels": [], "entities": []}, {"text": " Table 6: f-structure evaluation results for the test set against  TigerDB", "labels": [], "entities": [{"text": "TigerDB", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9462750554084778}]}, {"text": " Table 8: f-structure evaluation results for parser output ex- cluding sentences without f-structures", "labels": [], "entities": []}, {"text": " Table 9: f-structure evaluation results for the test set against  automatically generated goldstandard (1,850 sentences)", "labels": [], "entities": []}]}