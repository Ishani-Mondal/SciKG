{"title": [{"text": "Metadata-Aware Measures for Answer Summarization in Community Question Answering", "labels": [], "entities": [{"text": "Answer Summarization in Community Question Answering", "start_pos": 28, "end_pos": 80, "type": "TASK", "confidence": 0.6885065585374832}]}], "abstractContent": [{"text": "This paper presents a framework for automatically processing information coming from community Question Answering (cQA) portals with the purpose of generating a trustful, complete, relevant and succinct summary in response to a question.", "labels": [], "entities": [{"text": "automatically processing information coming from community Question Answering (cQA) portals", "start_pos": 36, "end_pos": 127, "type": "TASK", "confidence": 0.6953408643603325}]}, {"text": "We exploit the metadata intrinsically present in User Generated Content (UGC) to bias automatic multi-document summa-rization techniques toward high quality information.", "labels": [], "entities": []}, {"text": "We adopt a representation of concepts alternative to n-grams and propose two concept-scoring functions based on semantic overlap.", "labels": [], "entities": []}, {"text": "Experimental results on data drawn from Yahoo!", "labels": [], "entities": []}, {"text": "Answers demonstrate the effectiveness of our method in terms of ROUGE scores.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9939724802970886}]}, {"text": "We show that the information contained in the best answers voted by users of cQA portals can be successfully complemented by our method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Community Question Answering (cQA) portals are an example of Social Media where the information need of a user is expressed in the form of a question for which a best answer is picked among the ones generated by other users.", "labels": [], "entities": [{"text": "Community Question Answering (cQA) portals", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.77167118872915}]}, {"text": "cQA websites are becoming an increasingly popular complement to search engines: overnight, a user can expect a human-crafted, natural language answer tailored to her specific needs.", "labels": [], "entities": []}, {"text": "We have to be aware, though, that User Generated Content (UGC) is often redundant, noisy and untrustworthy (Jeon et al., The research was conducted while the first author was visiting Tsinghua University.;).", "labels": [], "entities": [{"text": "User Generated Content (UGC)", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.447757030526797}]}, {"text": "Interestingly, a great amount of information is embedded in the metadata generated as a byproduct of users' action and interaction on Social Media.", "labels": [], "entities": []}, {"text": "Much valuable information is contained in answers other than the chosen best one (.", "labels": [], "entities": []}, {"text": "Our work aims to show that such information can be successfully extracted and made available by exploiting metadata to distill cQA content.", "labels": [], "entities": []}, {"text": "To this end, we casted the problem to an instance of the query-biased multi-document summarization task, where the question was seen as a query and the available answers as documents to be summarized.", "labels": [], "entities": [{"text": "multi-document summarization task", "start_pos": 70, "end_pos": 103, "type": "TASK", "confidence": 0.6389090319474539}]}, {"text": "We mapped each characteristic that an ideal answer should present to a measurable property that we wished the final summary could exhibit: \u2022 Quality to assess trustfulness in the source, \u2022 Coverage to ensure completeness of the information presented, \u2022 Relevance to keep focused on the user's information need and \u2022 Novelty to avoid redundancy.", "labels": [], "entities": []}, {"text": "Quality of the information was assessed via Machine Learning (ML) techniques under best answer supervision in a vector space consisting of linguistic and statistical features about the answers and their authors.", "labels": [], "entities": []}, {"text": "Coverage was estimated by semantic comparison with the knowledge space of a corpus of answers to similar questions which had been retrieved through the Yahoo!", "labels": [], "entities": []}, {"text": "Answers API . Relevance was computed as information overlap between an answer and its question, while Novelty was calculated as inverse overlap with all other answers to the same question.", "labels": [], "entities": [{"text": "Relevance", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9847869277000427}]}, {"text": "A score was assigned to each concept in an answer according to the above properties.", "labels": [], "entities": []}, {"text": "A score-maximizing summary under a maximum coverage model was then computed by solving an associated Integer Linear Programming problem.", "labels": [], "entities": []}, {"text": "We chose to express concepts in the form of Basic Elements (BE), a semantic unit developed at ISI 2 and modeled semantic overlap as intersection in the equivalence classes of two concepts (formal definitions will be given in section 2.3).", "labels": [], "entities": []}, {"text": "The objective of our work was to present what we believe is a valuable conceptual framework; more advance machine learning and summarization techniques would most likely improve the performances.", "labels": [], "entities": [{"text": "summarization", "start_pos": 127, "end_pos": 140, "type": "TASK", "confidence": 0.9785811305046082}]}, {"text": "The remaining of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section Quality, Coverage, Relevance and Novelty measures are presented; we explain how they were calculated and combined to generate a final summary of all answers to a question.", "labels": [], "entities": [{"text": "Quality", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9664735794067383}, {"text": "Relevance", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9807850122451782}]}, {"text": "Experiments are illustrated in Section 3, where we give evidence of the effectiveness of our method.", "labels": [], "entities": []}, {"text": "We list related work in Section 5, discuss possible alternative approaches in Section 4 and provide our conclusions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The initial dataset was composed of 216,563 questions and 1,982,006 answers written by 171,676 user in 100 categories from the Yahoo!", "labels": [], "entities": []}, {"text": "Answers portal . We will refer to this dataset as the \"unfiltered version\".", "labels": [], "entities": [{"text": "Answers portal", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8608632385730743}]}, {"text": "The metadata described in section 2.1 was extracted and normalized; quality experiments (Section 3.2) were then conducted.", "labels": [], "entities": []}, {"text": "The unfiltered version was later reduced to 89,814 question-answer pairs that showed statistical and linguistic properties which made them particularly adequate for our purpose.", "labels": [], "entities": []}, {"text": "In particular, trivial, factoid and encyclopedia-answerable questions were removed by applying a series of patterns for the identification of complex questions.", "labels": [], "entities": []}, {"text": "The work by indicates some categories of questions that are particularly suitable for summarization, but due to the lack of high-performing question classifiers we resorted to human-crafted question patterns.", "labels": [], "entities": [{"text": "summarization", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.9928892850875854}]}, {"text": "Some pattern examples are the following: \u2022 {Why,What is the reason} We also removed questions that showed statistical values outside of convenient ranges: the number of answers, length of the longest answer and length of the sum of all answers (both absolute and normalized) were taken in consideration.", "labels": [], "entities": []}, {"text": "In particular we discarded questions with the following characteristics: \u2022 there were less than three answers \u2022 the longest answer was over 400 words (likely a copy-and-paste) \u2022 the sum of the length of all answers outside of the (100, 1000) words interval \u2022 the average length of answers was outside of the (50, 300) words interval At this point a second version of the dataset was created to evaluate the summarization performance under scoring function (6) and; it was generated by manually selecting questions that arouse subjective, human interest from the previous 89,814 question-answer pairs.", "labels": [], "entities": [{"text": "summarization", "start_pos": 407, "end_pos": 420, "type": "TASK", "confidence": 0.9705975651741028}]}, {"text": "The dataset size was thus reduced to 358 answers to 100 questions that were manually summarized (refer to Section 3.3).", "labels": [], "entities": []}, {"text": "From now on we will refer to this second version of the dataset as the \"filtered version\".", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summarization Evaluation on filtered dataset (re-", "labels": [], "entities": [{"text": "Summarization", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9518489837646484}]}]}