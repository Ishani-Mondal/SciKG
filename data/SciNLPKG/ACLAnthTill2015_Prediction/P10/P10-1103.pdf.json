{"title": [{"text": "Optimising Information Presentation for Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Optimising Information Presentation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7726947069168091}]}], "abstractContent": [{"text": "We present a novel approach to Information Presentation (IP) in Spoken Dialogue Systems (SDS) using a data-driven statistical optimisation framework for content planning and attribute selection.", "labels": [], "entities": [{"text": "Information Presentation (IP)", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.7279820799827575}, {"text": "attribute selection", "start_pos": 174, "end_pos": 193, "type": "TASK", "confidence": 0.6899917423725128}]}, {"text": "First we collect data in a Wizard-of-Oz (WoZ) experiment and use it to build a supervised model of human behaviour.", "labels": [], "entities": []}, {"text": "This forms a baseline for measuring the performance of optimised policies, developed from this data using Reinforcement Learning (RL) methods.", "labels": [], "entities": []}, {"text": "We show that the optimised policies significantly outperform the baselines in a variety of generation scenarios: while the supervised model is able to attain up to 87.6% of the possible reward on this task, the RL policies are significantly better in 5 out of 6 scenarios, gaining up to 91.5% of the total possible reward.", "labels": [], "entities": []}, {"text": "The RL policies perform especially well in more complex scenarios.", "labels": [], "entities": [{"text": "RL", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8668681979179382}]}, {"text": "We are also the first to show that adding predictive \"lower level\" features (e.g. from the NLG realiser) is important for optimising IP strategies according to user preferences.", "labels": [], "entities": [{"text": "NLG realiser", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9163312017917633}]}, {"text": "This provides new insights into the nature of the IP problem for SDS.", "labels": [], "entities": [{"text": "SDS", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9740663766860962}]}], "introductionContent": [{"text": "Work on evaluating SDS suggests that the Information Presentation (IP) phase is the primary contributor to dialogue duration (), and as such, is a central aspect of SDS design.", "labels": [], "entities": [{"text": "SDS", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9698875546455383}, {"text": "SDS", "start_pos": 165, "end_pos": 168, "type": "TASK", "confidence": 0.9713530540466309}]}, {"text": "During this phase the system returns a set of items (\"hits\") from a database, which match the user's current search constraints.", "labels": [], "entities": []}, {"text": "An inherent problem in this task is the trade-off between presenting \"enough\" information to the user (for example helping them to feel confident that they have a good overview of the search results) versus keeping the utterances short and understandable.", "labels": [], "entities": []}, {"text": "In the following we show that IP for SDS can be treated as a data-driven joint optimisation problem, and that this outperforms a supervised model of human 'wizard' behaviour on a particular IP task (presenting sets of search results to a user).", "labels": [], "entities": []}, {"text": "A similar approach has been applied to the problem of Referring Expression Generation in dialogue).", "labels": [], "entities": [{"text": "Referring Expression Generation in dialogue", "start_pos": 54, "end_pos": 97, "type": "TASK", "confidence": 0.9182505369186401}]}], "datasetContent": [{"text": "We collected 213 dialogues with 18 subjects and 2 wizards ( . Each user performed a total of 12 tasks, where no task set was seen twice by anyone wizard.", "labels": [], "entities": []}, {"text": "The majority of users were from a range of backgrounds in a higher education institute, in the age range 20-30, native speakers of English, and none had prior experience of.", "labels": [], "entities": []}, {"text": "[C:] The wizard then chooses which strategy and which attributes to generate next, by clicking radio buttons.", "labels": [], "entities": []}, {"text": "The attribute/s specified in the last user query are pre-selected by default.", "labels": [], "entities": []}, {"text": "The strategies can only be combined in the orders as specified in [D:] An utterance is automatically generated by the NLG realiser every time the wizard selects a strategy, and is displayed in an intermediate text panel.", "labels": [], "entities": []}, {"text": "[E:] The wizard can decide to add the generated utterance to the final output panel or to start over again.", "labels": [], "entities": []}, {"text": "The text in the final panel is sent to the user via TTS, once the wizard decides to stop generating.", "labels": [], "entities": [{"text": "TTS", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.6552492380142212}]}, {"text": "We now formulate the problem as a Markov Decision Process (MDP), where states are NLG dialogue contexts and actions are NLG decisions.", "labels": [], "entities": []}, {"text": "Each state-action pair is associated with a transition probability, which is the probability of moving from state sat time t to state sat time t + 1 after having performed action a when instate s.", "labels": [], "entities": []}, {"text": "This transition probability is computed by the environment model (i.e. the user simulation and realiser), and explicitly captures the uncertainty in the generation environment.", "labels": [], "entities": []}, {"text": "This is a major difference to other non-statistical planning approaches.", "labels": [], "entities": []}, {"text": "Each transition is also associated with a reinforcement signal (or \"reward\") r t+1 describing how good the result of action a was when performed instate s.", "labels": [], "entities": []}, {"text": "The aim of the MDP is to maximise long-term expected reward of its decisions, resulting in a policy which maps each possible state to an appropriate action in that state.", "labels": [], "entities": []}, {"text": "We treat IP as a hierarchical joint optimisation problem, where first one of the IP structures (1-3) is chosen and then the number of attributes is decided, as shown in.", "labels": [], "entities": []}, {"text": "At each generation step, the MDP can choose 1-5 attributes (e.g. cuisine, price range, location, food quality, and/or service quality).", "labels": [], "entities": []}, {"text": "Generation stops as soon as the user is predicted to select an item, i.e. the IP task is successful.", "labels": [], "entities": []}, {"text": "(Note that the same constraint is operational for the WoZ baseline.)).", "labels": [], "entities": [{"text": "WoZ baseline.", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.950348824262619}]}, {"text": "We trained the policy using the SHARSHA algorithm) with linear function approximation, and the simulation environment described in Section 4.", "labels": [], "entities": [{"text": "SHARSHA", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.4555276334285736}]}, {"text": "The policy was trained for 60,000 iterations.", "labels": [], "entities": []}, {"text": "We compare the learned strategies against the WoZ baseline as described in Section 3.3.", "labels": [], "entities": [{"text": "WoZ baseline", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.8760706186294556}]}, {"text": "For attribute selection we choose a majority baseline (randomly choosing between 3 or 4 attributes) since the attribute selection models learned by Supervised Learning on the WoZ data didn't show significant improvements.", "labels": [], "entities": [{"text": "attribute selection", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8019398748874664}, {"text": "WoZ data", "start_pos": 175, "end_pos": 183, "type": "DATASET", "confidence": 0.9646463692188263}]}, {"text": "For training, we used the user simulation model most similar to the data, see Section 4.1.", "labels": [], "entities": []}, {"text": "For testing, we test with the different user simulation model (the one which is most dissimilar to the data).", "labels": [], "entities": []}, {"text": "We first investigate how well IP structure (without attribute choice) can be learned in increasingly complex generation scenarios.", "labels": [], "entities": []}, {"text": "A generation scenario is a combination of a particular kind of NLG realiser (template vs. stochastic) along with different levels of variation introduced by certain features of the dialogue context.", "labels": [], "entities": []}, {"text": "In general, the stochastic realiser introduces more variation in lower level features than the template-based realiser.", "labels": [], "entities": []}, {"text": "The Focus model introduces more variation with respect to #DBhits and #attributes as described in Section 4.2.", "labels": [], "entities": []}, {"text": "We therefore investigate the following cases: 1.1.", "labels": [], "entities": []}, {"text": "IP structure choice, Template realiser: Predicted next user action varies according to the bi-gram model (P (a u,t |IP s,t )); Number of sentences and attributes per IP strategy is set by defaults, reflecting a template-based realiser.", "labels": [], "entities": []}, {"text": "IP structure choice, Stochastic realiser: IP structure where number of attributes per NLG turn is given at the beginning of each episode (e.g. set by the DM); Sentence generation according to the SPaRKy stochastic realiser model as described in Section 3.2.", "labels": [], "entities": [{"text": "Sentence generation", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.9273311793804169}]}, {"text": "We then investigate different scenarios for jointly optimising IP structure (IPS) and attribute selection (Attr) decisions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Kullback-Leibler divergence for the dif- ferent User Simulations (US)", "labels": [], "entities": [{"text": "User Simulations", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.6909461915493011}]}, {"text": " Table 3: Test results for 1000 dialogues, where *** denotes that the RL policy is significantly (p < .001)  better than the Baseline policy.", "labels": [], "entities": [{"text": "RL", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.6336320042610168}]}]}