{"title": [], "abstractContent": [{"text": "We investigate coreference relationships between NPs with the same head noun.", "labels": [], "entities": []}, {"text": "It is relatively common in unsupervised work to assume that such pairs are coreferent-but this is not always true, especially if realistic mention detection is used.", "labels": [], "entities": []}, {"text": "We describe the distribution of non-coreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some same-head NPs using syntactic features, improving precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 202, "end_pos": 211, "type": "METRIC", "confidence": 0.9978645443916321}]}], "introductionContent": [{"text": "Full NP coreference, the task of discovering which non-pronominal NPs in a discourse refer to the same entity, is widely known to be challenging.", "labels": [], "entities": [{"text": "NP coreference", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.7134353369474411}]}, {"text": "In practice, however, most work focuses on the subtask of linking NPs with different head words.", "labels": [], "entities": []}, {"text": "Decisions involving NPs with the same headword have not attracted nearly as much attention, and many systems, especially unsupervised ones, operate under the assumption that all same-head pairs corefer.", "labels": [], "entities": []}, {"text": "This is by no means always the casethere are several systematic exceptions to the rule.", "labels": [], "entities": []}, {"text": "In this paper, we show that these exceptions are fairly common, and describe an unsupervised system which learns to distinguish them from coreferent same-head pairs.", "labels": [], "entities": []}, {"text": "There are several reasons why relatively little attention has been paid to same-head pairs.", "labels": [], "entities": []}, {"text": "Primarily, this is because they area comparatively easy subtask in a notoriously difficult area; shows that, among NPs headed by common nouns, those which have an exact match earlier in the document are the easiest to resolve (variant MUC score .82 on MUC-6) and while those with partial matches are quite a bit harder (.53), by far the worst performance is on those without any match at all.", "labels": [], "entities": [{"text": "variant MUC score .82", "start_pos": 227, "end_pos": 248, "type": "METRIC", "confidence": 0.6500620424747467}, {"text": "MUC-6", "start_pos": 252, "end_pos": 257, "type": "DATASET", "confidence": 0.8663884997367859}]}, {"text": "This effect is magnified by most popular metrics for coreference, which reward finding links within large clusters more than they punish proposing spurious links, making it hard to improve performance by linking conservatively.", "labels": [], "entities": [{"text": "coreference", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.9496212005615234}]}, {"text": "Systems that use gold mention boundaries (the locations of NPs marked by annotators) have even less need to worry about same-head relationships, since most NPs which disobey the conventional assumption are not marked as mentions.", "labels": [], "entities": []}, {"text": "In this paper, we count how often same-head pairs fail to corefer in the MUC-6 corpus, showing that gold mention detection hides most such pairs, but more realistic detection finds large numbers.", "labels": [], "entities": [{"text": "MUC-6 corpus", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.9510347247123718}, {"text": "gold mention detection", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.6944831808408102}]}, {"text": "We also present an unsupervised generative model which learns to make certain samehead pairs non-coreferent.", "labels": [], "entities": []}, {"text": "The model is based on the idea that pronoun referents are likely to be salient noun phrases in the discourse, so we can learn about NP antecedents using pronominal antecedents as a starting point.", "labels": [], "entities": []}, {"text": "Pronoun anaphora, in turn, is learnable from raw data).", "labels": [], "entities": []}, {"text": "Since our model links fewer NPs than the baseline, it improves precision but decreases recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9994939565658569}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9988166093826294}]}, {"text": "This tradeoff is favorable for CEAF, but not for b 3 .", "labels": [], "entities": [{"text": "CEAF", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.7347667217254639}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Oracle, system and baseline scores on MUC-6 test data. Gold mentions leave little room  for improvement between baseline and oracle; detecting more mentions widens the gap between  them. With realistic mention detection, precision and CEAF scores improve over baselines, while recall  and f-scores drop.", "labels": [], "entities": [{"text": "MUC-6 test data", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.888072669506073}, {"text": "precision", "start_pos": 231, "end_pos": 240, "type": "METRIC", "confidence": 0.999703586101532}, {"text": "CEAF", "start_pos": 245, "end_pos": 249, "type": "METRIC", "confidence": 0.996727466583252}, {"text": "recall", "start_pos": 287, "end_pos": 293, "type": "METRIC", "confidence": 0.9994372725486755}, {"text": "f-scores", "start_pos": 299, "end_pos": 307, "type": "METRIC", "confidence": 0.9167619347572327}]}]}