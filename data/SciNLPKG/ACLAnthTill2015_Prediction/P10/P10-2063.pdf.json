{"title": [{"text": "Simultaneous Tokenization and Part-of-Speech Tagging for Arabic without a Morphological Analyzer", "labels": [], "entities": [{"text": "Part-of-Speech Tagging", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.6582057029008865}]}], "abstractContent": [{"text": "We describe an approach to simultaneous tokenization and part-of-speech tagging that is based on separating the closed and open-class items, and focusing on the likelihood of the possible stems of the open-class words.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.6698334366083145}]}, {"text": "By encoding some basic linguistic information, the machine learning task is simplified, while achieving state-of-the-art tokenization results and competitive POS results, although with a reduced tag set and some evaluation difficulties.", "labels": [], "entities": [{"text": "POS", "start_pos": 158, "end_pos": 161, "type": "METRIC", "confidence": 0.8775211572647095}]}], "introductionContent": [{"text": "Research on the problem of morphological disambiguation of Arabic has noted that techniques developed for lexical disambiguation in English do not easily transfer over, since the affixation present in Arabic creates a very different tag set than for English, in terms of the number and complexity of tags.", "labels": [], "entities": [{"text": "morphological disambiguation of Arabic", "start_pos": 27, "end_pos": 65, "type": "TASK", "confidence": 0.8234514147043228}]}, {"text": "In additional to inflectional morphology, the POS tags encode more complex tokenization sequences, such as preposition + noun or noun + possessive pronoun.", "labels": [], "entities": []}, {"text": "One approach taken to this problem is to use a morphological analyzer such as BAMA-v2.0) or SAMA-v3.1 ( , which generates a list of all possible morphological analyses fora given token.", "labels": [], "entities": [{"text": "BAMA-v2.0", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.750019907951355}]}, {"text": "Machine learning approaches can model separate aspects of a solution (e.g., \"has a pronominal clitic\") and then combine them to select the most appropriate solution from among this list.", "labels": [], "entities": []}, {"text": "A benefit of this approach is that by picking a single solution from the morphological analyzer, the part-ofspeech and tokenization comes as a unit).", "labels": [], "entities": []}, {"text": "In contrast, other approaches have used a pipelined approach, with separate models to first do tokenization and then part-of-speech tagging (.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 95, "end_pos": 107, "type": "TASK", "confidence": 0.9622561931610107}, {"text": "part-of-speech tagging", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.6735703647136688}]}, {"text": "While these approaches have somewhat lower performance than the joint approach, they have the advantage that they do not rely on the presence of a full-blown morphological analyzer, which may not always be available or appropriate as the data shifts to different genres or Arabic dialects.", "labels": [], "entities": []}, {"text": "In this work we present a novel approach to this problem that allows us to do simultaneous tokenization and core part-of-speech tagging with a simple classifier, without using a full-blown morphological analyzer.", "labels": [], "entities": [{"text": "core part-of-speech tagging", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.596067210038503}]}, {"text": "We distinguish between closed-class and open-class categories of words, and encode regular expressions that express the morphological patterns for the former, and simple regular expressions for the latter that provide only the generic templates for affixation.", "labels": [], "entities": []}, {"text": "We find that a simple baseline for the closed-class words already works very well, and for the open-class words we classify only the possible stems for all such expressions.", "labels": [], "entities": []}, {"text": "This is however sufficient for tokenization and core POS tagging, since the stem identifies the appropriate regular expression, which then in turn makes explicit, simultaneously, the tokenization and part-of-speech information.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.9772365093231201}, {"text": "POS tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.8035714328289032}]}], "datasetContent": [{"text": "We worked with ATB3-v3.2, following the training/devtest split in () on a previous release of the same data.", "labels": [], "entities": [{"text": "ATB3-v3.2", "start_pos": 15, "end_pos": 24, "type": "DATASET", "confidence": 0.970879316329956}]}, {"text": "We keep a listing (List #1) of all (source token TEXT, solution) pairs seen during training.", "labels": [], "entities": [{"text": "TEXT", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9458715915679932}]}, {"text": "For an open-class solution, \"solution\" is the gold label as described in Section 3.", "labels": [], "entities": []}, {"text": "For a closed-class solution, \"solution\" is the name of the single pos-matching regex.", "labels": [], "entities": []}, {"text": "In addition, for every regex seen during training that pos-matches some source token TEXT, we keep a listing (List #2) of all ((regex-group-name, text), POS-tag) tuples.", "labels": [], "entities": []}, {"text": "We use the information in List #1 to choose a solution for all words seen in training in the Baseline and Run 2 below, and in Run 3, for words text-matching a closed-class expression.", "labels": [], "entities": []}, {"text": "We use List #2 to disambiguate all remaining cases of POS ambiguity, wherever a solution comes from.", "labels": [], "entities": []}, {"text": "For example, if wlm is seen during testing, List #1 will be consulted to find the most common solution (REGEX #1 or #2), and in either case, List #2 will be consulted to determine the most frequent tag for was a prefix.", "labels": [], "entities": [{"text": "REGEX #1", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9180839856465658}]}, {"text": "While there is certainly room for improvement here, this works quite well since the tags for the affixes do not vary much.", "labels": [], "entities": []}, {"text": "We score the solution fora source token instance as correct for tokenization if it exactly matches the TEXT split for the tree tokens derived from that source token instance in the ATB.", "labels": [], "entities": [{"text": "TEXT split", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.9732868075370789}, {"text": "ATB", "start_pos": 181, "end_pos": 184, "type": "DATASET", "confidence": 0.9311563968658447}]}, {"text": "It is correct for POS if correct for tokenization and if each tree token has the same POS tag as the reduced core tag for that tree token in the ATB.", "labels": [], "entities": []}, {"text": "For a simple baseline, if a source token TEXT is in List #1 then we simply use the most frequent stored solution.", "labels": [], "entities": [{"text": "TEXT", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9609066247940063}]}, {"text": "Otherwise we run the TEXT through all the regexes.", "labels": [], "entities": [{"text": "TEXT", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9011365175247192}]}, {"text": "If it text-matches any closed-class expression, we pick a random choice from among those regexes and otherwise from the open-class regexes that it text-matches.", "labels": [], "entities": []}, {"text": "Any POS ambiguities fora regex group are disambiguated: Results for Baseline and two runs.", "labels": [], "entities": []}, {"text": "Origin \"stored\" means that the appropriate regex came from the list stored during training.", "labels": [], "entities": []}, {"text": "Origins \"open\" and \"closed\" are random choices from the open or closed regexes for the source token.", "labels": [], "entities": []}, {"text": "\"Mallet\" means that it comes from the label output by the CRF classifier.", "labels": [], "entities": []}, {"text": "using List #2, as discussed above.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The score is very high for the words seen during training, but much lower for open-class words that were not.", "labels": [], "entities": []}, {"text": "As expected, almost all (except 27) instances of closed-class words were seen during training.", "labels": [], "entities": []}, {"text": "For run 2, we continue to use the stored solution if the token was seen in training.", "labels": [], "entities": []}, {"text": "If not, then if the TEXT matches one or more closed-class regexes, we randomly choose one.", "labels": [], "entities": [{"text": "TEXT", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9841050505638123}]}, {"text": "Otherwise, if the CRF classifier has produced an open-class match for that token, we use that (and otherwise, in only 10 cases, use a random open-class match).", "labels": [], "entities": []}, {"text": "There is a significant improvement in the score for the openclass items, and therefore in the overall results.", "labels": [], "entities": []}, {"text": "For run 3, we put more of a burden on the classifier.", "labels": [], "entities": []}, {"text": "If a word matches any closed-class expression, we either use the most frequent occurence during training (if it was seen), or use a random maching closed-class expression (if not).", "labels": [], "entities": []}, {"text": "If the word doesn't match a closed-class expression, we use the mallet result.", "labels": [], "entities": []}, {"text": "The mallet score goes up, almost certainly because the score is now including results on words that were seen during training.", "labels": [], "entities": []}, {"text": "The overall POS result for run 3 is slightly less than run 2.", "labels": [], "entities": [{"text": "POS", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9986433386802673}]}, {"text": "(95.099% compared to 95.147%).", "labels": [], "entities": []}, {"text": "It is not a simple matter to compare results with previous work, due to differing evaluation techniques, data sets, and POS tag sets.", "labels": [], "entities": []}, {"text": "With different data sets and training sizes, Habash and Rambow (2005) report 99.3% word accuracy on tokenization, and reports a score of 99.1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9516644477844238}, {"text": "tokenization", "start_pos": 100, "end_pos": 112, "type": "TASK", "confidence": 0.9697837233543396}]}, {"text": "reported 97.6% on the LDC-supplied reduced tag set, and reported 96.6%.", "labels": [], "entities": [{"text": "LDC-supplied reduced tag set", "start_pos": 22, "end_pos": 50, "type": "DATASET", "confidence": 0.8125908523797989}]}, {"text": "The LDCsupplied tag set used is smaller than the one in this paper, but does distinguish between NOUN and ADJ.", "labels": [], "entities": [{"text": "NOUN", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.7128033638000488}]}, {"text": "However, both) assume gold tokenization for evaluation of POS results, which we do not.", "labels": [], "entities": []}, {"text": "The \"MorphPOS\" task in (, 96.4%, is somewhat similar to ours in that it scores on a \"core tag\", but unlike for us there is only one such tag fora source token (easier) but it distinguishes between NOUN and ADJ (harder).", "labels": [], "entities": [{"text": "NOUN", "start_pos": 197, "end_pos": 201, "type": "DATASET", "confidence": 0.5926303863525391}]}, {"text": "We would like to do a direct comparison by simply runing the above systems on the exact same data and evaluating them the same way.", "labels": [], "entities": []}, {"text": "However, this unfortunately has to wait until new versions are released that work with the current version of the SAMA morphological analyzer and ATB.", "labels": [], "entities": [{"text": "SAMA morphological analyzer", "start_pos": 114, "end_pos": 141, "type": "TASK", "confidence": 0.7091912229855856}, {"text": "ATB", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.8491554260253906}]}], "tableCaptions": [{"text": " Table 5: Results for Baseline and two runs. Origin \"stored\" means that the appropriate regex came from  the list stored during training. Origins \"open\" and \"closed\" are random choices from the open or closed  regexes for the source token. \"Mallet\" means that it comes from the label output by the CRF classifier.", "labels": [], "entities": []}]}