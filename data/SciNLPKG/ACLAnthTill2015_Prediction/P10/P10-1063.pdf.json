{"title": [{"text": "TrustRank: Inducing Trust in Automatic Translations via Ranking", "labels": [], "entities": [{"text": "Inducing Trust in Automatic Translations", "start_pos": 11, "end_pos": 51, "type": "TASK", "confidence": 0.5643616914749146}]}], "abstractContent": [{"text": "The adoption of Machine Translation technology for commercial applications is hampered by the lack of trust associated with machine-translated output.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.8747855722904205}]}, {"text": "In this paper , we describe TrustRank, an MT system enhanced with a capability to rank the quality of translation outputs from good to bad.", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9853532910346985}]}, {"text": "This enables the user to set a quality threshold, granting the user control over the quality of the translations.", "labels": [], "entities": []}, {"text": "We quantify the gains we obtain in translation quality, and show that our solution works on a wide variety of domains and language pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "The accuracy of machine translation (MT) software has steadily increased over the last 20 years to achieve levels at which large-scale commercial applications of the technology have become feasible.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9992160797119141}, {"text": "machine translation (MT) software", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.8482071310281754}]}, {"text": "However, widespread adoption of MT technology remains hampered by the lack of trust associated with machine-translated output.", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9937627911567688}]}, {"text": "This lack of trust is a normal reaction to the erratic translation quality delivered by current state-of-theart MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.9665849208831787}]}, {"text": "Unfortunately, the lack of predictable quality discourages the adoption of largescale automatic translation solutions.", "labels": [], "entities": [{"text": "largescale automatic translation", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.6003454625606537}]}, {"text": "Consider the case of a commercial enterprise that hosts reviews written by travellers on its website.", "labels": [], "entities": []}, {"text": "These reviews contain useful information about hotels, restaurants, attractions, etc.", "labels": [], "entities": []}, {"text": "There is a large and continuous stream of reviews posted on this site, and the large majority is written in English.", "labels": [], "entities": []}, {"text": "In addition, there is a large set of potential customers who would prefer to have these reviews available in their (non-English) native languages.", "labels": [], "entities": []}, {"text": "As such, this enterprise presents the perfect opportunity for the deployment of a large-volume MT solution.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.984126091003418}]}, {"text": "However, travel reviews present specific challenges: the reviews tend to have poor spelling, loose grammar, and broad topics of discussion.", "labels": [], "entities": []}, {"text": "The result is unpredictable levels of MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9860198497772217}]}, {"text": "This is undesirable for the commercial enterprise, who is not content to simply reach abroad audience, but also wants to deliver a high-quality product to that audience.", "labels": [], "entities": []}, {"text": "We propose the following solution.", "labels": [], "entities": []}, {"text": "We develop TrustRank, an MT system enhanced with a capability to rank the quality of translation outputs from good to bad.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9849364161491394}]}, {"text": "This enables the user to set a quality threshold, granting the user control over the quality of the translations that it employs in its product.", "labels": [], "entities": []}, {"text": "With this enhancement, MT adoption stops being a binary should-we-or-shouldn'twe question.", "labels": [], "entities": [{"text": "MT adoption", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.9824961423873901}]}, {"text": "Rather, each user can make a personal trade-off between the scope and the quality of their product.", "labels": [], "entities": []}], "datasetContent": [{"text": "The MT system used by TrustRank (TrustRank-MT) is a statistical phrase-based MT system similar to ().", "labels": [], "entities": []}, {"text": "As a reference point regarding the performance of this system, we use the official WMT09 parallel data, monolingual data, and development tuning set (news-dev2009a) to train baseline TrustRank-MT systems for each of the ten WMT09 language pairs.", "labels": [], "entities": [{"text": "WMT09 parallel data", "start_pos": 83, "end_pos": 102, "type": "DATASET", "confidence": 0.8271743059158325}, {"text": "WMT09 language pairs", "start_pos": 224, "end_pos": 244, "type": "DATASET", "confidence": 0.8401087323824564}]}, {"text": "Our system produces translations that are competitive with state-of-the-art systems.", "labels": [], "entities": [{"text": "translations", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.96145099401474}]}, {"text": "We show our baselinesystem BLEU scores on the official development test set (news-dev2009b) for the WMT09 task in, along with the BLEU scores reported for the baseline Moses system (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9585759043693542}, {"text": "WMT09 task", "start_pos": 100, "end_pos": 110, "type": "DATASET", "confidence": 0.7568349540233612}, {"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9988430738449097}]}, {"text": "For each of the domains we consider, we partition the data sets as follows.", "labels": [], "entities": []}, {"text": "We first set aside 3000 documents, which we call the Regression set  set, on which the MT system is trained.", "labels": [], "entities": [{"text": "Regression set  set", "start_pos": 53, "end_pos": 72, "type": "DATASET", "confidence": 0.7753457824389139}, {"text": "MT", "start_pos": 87, "end_pos": 89, "type": "TASK", "confidence": 0.9592128396034241}]}, {"text": "From the Regression set, we set aside 1000 parallel documents to be used as a blind test set (called Regression Test) for our experiments.", "labels": [], "entities": []}, {"text": "An additional set of 1000 parallel documents is used as a development set, and the rest of 1000 parallel documents is used as the regression-model training set.", "labels": [], "entities": []}, {"text": "We have also performed learning-curve experiments using between 100 and 2000 documents for regression-model training.", "labels": [], "entities": []}, {"text": "We do not go into the details of these experiments here for lack of space.", "labels": [], "entities": []}, {"text": "The conclusion derived from these experiments is that 1000 documents is the point where the learning-curves level off.", "labels": [], "entities": []}, {"text": "In, we provide a few data points with respect to the data size of these sets (tokenized word-count on the source side).", "labels": [], "entities": []}, {"text": "We also report the BLEU performance of the TrustRank-MT system on the Regression Test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9995182752609253}, {"text": "Regression Test set", "start_pos": 70, "end_pos": 89, "type": "DATASET", "confidence": 0.7947626908620199}]}, {"text": "Note that the differences between the BLEU scores reported in and the BLEU scores under the WMT09 label in reflect differences in the genres of these sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9841163754463196}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9879297018051147}, {"text": "WMT09 label", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.9056259393692017}]}, {"text": "The official development test set (news-dev2009b) for the WMT09 task is news only.", "labels": [], "entities": [{"text": "WMT09 task", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.6204212307929993}]}, {"text": "The regression Test sets have the same distribution between Europarl data and news as the corresponding training data set for each language pair.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.9778023064136505}]}, {"text": "In this section, we present the performance of TrustRank on a variety of language pairs.", "labels": [], "entities": []}, {"text": "We report the BLEU score obtained on our 1000-document regression Test, as well as ranking and regression performance using the rAcc, vBLEU\u2206, MAE, and TE metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9995193481445312}, {"text": "vBLEU\u2206", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.8265189826488495}, {"text": "MAE", "start_pos": 142, "end_pos": 145, "type": "METRIC", "confidence": 0.9537836313247681}, {"text": "TE", "start_pos": 151, "end_pos": 153, "type": "METRIC", "confidence": 0.9924693703651428}]}, {"text": "As the numbers for the ranking and regression metrics show, the same trends we observed for English-Spanish hold for many other language pairs as well.", "labels": [], "entities": []}, {"text": "Some domains, such as HiTech, are easier to rank regardless of the language pair, and the quality gains are consistently high (+9.9 average vBLEU\u2206 for the 5 language pairs considered).", "labels": [], "entities": [{"text": "HiTech", "start_pos": 22, "end_pos": 28, "type": "DATASET", "confidence": 0.9548693299293518}, {"text": "vBLEU", "start_pos": 140, "end_pos": 145, "type": "METRIC", "confidence": 0.967585027217865}]}, {"text": "Other domains, such as WMT09 and Travel, are more difficult to rank.", "labels": [], "entities": [{"text": "WMT09", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.858217716217041}, {"text": "Travel", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.8395119309425354}]}, {"text": "However, the WMT09 English-Hungarian data set appears to be better suited for ranking, as the vBLEU\u2206 numbers are higher compared to the rest of the language pairs from this domain (+4.3 vBLEU\u2206 for Eng-Hun, +7.1 vBLEU\u2206 for Hun-Eng).", "labels": [], "entities": [{"text": "WMT09 English-Hungarian data set", "start_pos": 13, "end_pos": 45, "type": "DATASET", "confidence": 0.9488506019115448}, {"text": "vBLEU", "start_pos": 94, "end_pos": 99, "type": "METRIC", "confidence": 0.9215194582939148}]}, {"text": "For Travel, EnglishDutch is also an outlier in terms of quality gains (+12.9 vBLEU\u2206).", "labels": [], "entities": [{"text": "EnglishDutch", "start_pos": 12, "end_pos": 24, "type": "DATASET", "confidence": 0.8924649953842163}, {"text": "vBLEU", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9679797291755676}]}, {"text": "Overall, the results indicate that TrustRank obtains consistent performance across a large variety of language pairs.", "labels": [], "entities": []}, {"text": "Similar with the conclusion for English-Spanish, the regression performance is currently too poor to allow us to confidently use the absolute document-level predicted BLEU numbers as indicators of translation accuracy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9860964417457581}, {"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.7729892134666443}]}], "tableCaptions": [{"text": " Table 1: BLEU scores (uncased) for the  TrustRank-MT system compared to Moses  (WMT09 data).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992362260818481}, {"text": "WMT09 data", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.9245218932628632}]}, {"text": " Table 4: Lower-and upper-bounds for ranking and  regression accuracy (English-Spanish).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9678876996040344}]}, {"text": " Table 3: Detailed performance using all features (English-Spanish).", "labels": [], "entities": []}, {"text": " Table 5:  \"Time-constrained\" performance  (English-Spanish).", "labels": [], "entities": []}, {"text": " Table 6:  Travel Eng-Fra, Travel Eng-Dut, and HiTech Eng- Rus. These plots illustrate the tendency of the pre- dicted BLEU values to correlate with the actual  BLEU scores. The amount of correlation visible in  these plots matches the performance numbers pro- vided in", "labels": [], "entities": [{"text": "HiTech Eng- Rus", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.7281934469938278}, {"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9528555870056152}, {"text": "BLEU", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.9478439688682556}]}, {"text": " Table 6: Performance of TrustRank on a variety of  domains and language pairs.", "labels": [], "entities": []}]}