{"title": [{"text": "Employing Personal/Impersonal Views in Supervised and Semi-supervised Sentiment Classification", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.8027198612689972}]}], "abstractContent": [{"text": "In this paper, we adopt two views, personal and impersonal views, and systematically employ them in both supervised and semi-supervised sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 136, "end_pos": 160, "type": "TASK", "confidence": 0.8601091802120209}]}, {"text": "Here, personal views consist of those sentences which directly express speaker's feeling and preference towards a target object while impersonal views focus on statements towards a target object for evaluation.", "labels": [], "entities": []}, {"text": "To obtain them, an unsupervised mining approach is proposed.", "labels": [], "entities": []}, {"text": "On this basis, an ensemble method and a co-training algorithm are explored to employ the two views in supervised and semi-supervised sentiment classification respectively.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 133, "end_pos": 157, "type": "TASK", "confidence": 0.8041179478168488}]}, {"text": "Experimental results across eight domains demonstrate the effectiveness of our proposed approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "As a special task of text classification, sentiment classification aims to classify a text according to the expressed sentimental polarities of opinions such as 'thumb up' or 'thumb down' on the movies ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7576338052749634}, {"text": "sentiment classification", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.9442770779132843}]}, {"text": "This task has recently received considerable interests in the Natural Language Processing (NLP) community due to its wide applications.", "labels": [], "entities": []}, {"text": "In general, the objective of sentiment classification can be represented as a kind of binary relation R, defined as an ordered triple, where X is an object set including different kinds of people (e.g. writers, reviewers, or users), Y is another object set including the target objects (e.g. products, events, or even some people), and G is a subset of the Cartesian product X Y \u00d7 . The concerned relation in sentiment classification is X 's evaluation on Y, such as 'thumb up', 'thumb down', 'favorable', and 'unfavorable'.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.8996078372001648}, {"text": "sentiment classification", "start_pos": 409, "end_pos": 433, "type": "TASK", "confidence": 0.8936504125595093}]}, {"text": "Such relation is usually expressed in text by stating the information involving either a person (one element in X ) or a target object itself (one element in Y ).", "labels": [], "entities": []}, {"text": "The first type of statement called personal view, e.g. 'I am so happy with this book', contains X 's \"subjective\" feeling and preference towards a target object, which directly expresses sentimental evaluation.", "labels": [], "entities": []}, {"text": "This kind of information is normally domain-independent and serves as highly relevant clues to sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.952004998922348}]}, {"text": "The latter type of statement called impersonal view, e.g. 'it is too small', contains Y 's \"objective\" (i.e. or at least criteria-based) evaluation of the target object.", "labels": [], "entities": []}, {"text": "This kind of information tends to contain much domain-specific classification knowledge.", "labels": [], "entities": [{"text": "domain-specific classification", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.5618570744991302}]}, {"text": "Although such information is sometimes not as explicit as personal views in classifying the sentiment of a text, speaker's sentiment is usually implied by the evaluation result.", "labels": [], "entities": []}, {"text": "It is well-known that sentiment classification is very domain-specific (, so it is critical to eliminate its dependence on a large-scale labeled data for its wide applications.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.9674068987369537}]}, {"text": "Since the unlabeled data is ample and easy to collect, a successful semi-supervised sentiment classification system would significantly minimize the involvement of labor and time.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.8919210135936737}]}, {"text": "Therefore, given the two different views mentioned above, one promising application is to adopt them in co-training algorithms, which has been proven to bean effective semi-supervised learning strategy of incorporating unlabeled data to further improve the classification performance ().", "labels": [], "entities": []}, {"text": "In addition, we would show that personal/impersonal views are linguistically marked and mining them in text can be easily performed without special annotation.", "labels": [], "entities": []}, {"text": "In this paper, we systematically employ personal/impersonal views in supervised and semi-supervised sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 100, "end_pos": 124, "type": "TASK", "confidence": 0.8649370968341827}]}, {"text": "First, an unsupervised bootstrapping method is adopted to automatically separate one document into personal and impersonal views.", "labels": [], "entities": []}, {"text": "Then, both views are employed in supervised sentiment classification via an ensemble of individual classifiers generated by each view.", "labels": [], "entities": [{"text": "supervised sentiment classification", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.6691278020540873}]}, {"text": "Finally, a co-training algorithm is proposed to incorporate unlabeled data for semi-supervised sentiment classification.", "labels": [], "entities": [{"text": "semi-supervised sentiment classification", "start_pos": 79, "end_pos": 119, "type": "TASK", "confidence": 0.7119871775309244}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the related work of sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.9339503645896912}]}, {"text": "Section 3 presents our unsupervised approach for mining personal and impersonal views.", "labels": [], "entities": []}, {"text": "Section 4 and Section 5 propose our supervised and semi-supervised methods on sentiment classification respectively.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.9495585858821869}]}, {"text": "Experimental results are presented and analyzed in Section 6.", "labels": [], "entities": []}, {"text": "Section 7 discusses on the differences between personal/impersonal and subjective/objective.", "labels": [], "entities": []}, {"text": "Finally, Section 8 draws our conclusions and outlines the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have systematically explored our method on product reviews from eight domains: book, DVD, electronic appliances, kitchen appliances, health, network, pet and software.", "labels": [], "entities": []}, {"text": "The product reviews on the first four domains (book, DVD, electronic, and kitchen appliances) come from the multi-domain sentiment classification corpus, collected from http://www.amazon.com/ by 2 . Besides, we also collect the product views from http://www.amazon.com/ on other four domains (health, network, pet and software) . Each of the eight domains contains 1000 positive and 1000 negative reviews.", "labels": [], "entities": []}, {"text": "gives the distribution of personal and impersonal sentences in the training data (75% labeled data of all data).", "labels": [], "entities": []}, {"text": "It shows that there are more impersonal sentences than personal ones in each domain, in particular in the DVD domain, where the number of impersonal sentences is at least twice as many as that of personal sentences.", "labels": [], "entities": [{"text": "DVD domain", "start_pos": 106, "end_pos": 116, "type": "DATASET", "confidence": 0.9306897222995758}]}, {"text": "This unusual phenomenon is mainly attributed to the fact that many objective descriptions, e.g. the movie plot introductions, are expressed in the DVD domain which makes the extracted personal and impersonal sentences rather unbalanced.", "labels": [], "entities": []}, {"text": "We apply both support vector machine (SVM) and Maximum Entropy (ME) algorithms with the help of the SVM-light  4-fold cross validation is performed for supervised sentiment classification.", "labels": [], "entities": [{"text": "supervised sentiment classification", "start_pos": 152, "end_pos": 187, "type": "TASK", "confidence": 0.6853830814361572}]}, {"text": "For comparison, we generate two random views by randomly splitting the whole feature space into two parts.", "labels": [], "entities": []}, {"text": "Each part is seen as a view and used to train a classifier.", "labels": [], "entities": []}, {"text": "The combination (two random view classifiers along with the single-view classifier f ) results are shown in the last column of.", "labels": [], "entities": []}, {"text": "The comparison between random two views and our proposed two views will clarify whether the performance gain comes truly from our proposed two-view mining, or simply from using the classifier combination strategy.", "labels": [], "entities": []}, {"text": "shows the performances of different classifiers, where the single-view classifier f which uses all sentences for training and testing, is considered as our baseline.", "labels": [], "entities": []}, {"text": "Note that the baseline performances of the first four domains are worse than the ones reported in.", "labels": [], "entities": []}, {"text": "But their experiment is performed with only one split on the data with 80% as the training data and 20% as the testing data, which means the size of their training data is larger than ours.", "labels": [], "entities": []}, {"text": "Also, we find that our performances are similar to the ones (described as fully supervised results) reported in where the same data in the four domains are used and 10-fold cross validation is performed.", "labels": [], "entities": []}, {"text": "f . Similar to the sentence distributions, the difference in the classification performances between these two views in the DVD domain is the largest (0.6931 vs. 0.7663).", "labels": [], "entities": []}, {"text": "Both the combination methods (stacking and product rule) significantly outperform the baseline in each domain (p-value<0.01) with a decent average performance improvement of 2.61%.", "labels": [], "entities": []}, {"text": "Although the performance difference between the product rule and stacking is not significant, the product rule is obviously a better choice as it involves much easier implementation.", "labels": [], "entities": [{"text": "stacking", "start_pos": 65, "end_pos": 73, "type": "TASK", "confidence": 0.9748710989952087}]}, {"text": "Therefore, in the semi-supervised learning process, we only use the product rule to combine the individual classifiers.", "labels": [], "entities": []}, {"text": "Finally, it shows that random generation of two views with the combination method of the product rule only slightly outperforms the baseline on the average (0.7858 vs. 0.7823) but performs much worse than our unsupervised mining of personal and impersonal views.", "labels": [], "entities": []}, {"text": "We systematically evaluate and compare our two-view learning method with various semi-supervised ones as follows: Self-training, which uses the unlabeled data in a bootstrapping way like co-training yet limits the number of classifiers and the number of views to one.", "labels": [], "entities": []}, {"text": "Only the baseline classifier f is used to select most confident unlabeled samples in each iteration.", "labels": [], "entities": []}, {"text": "Transductive SVM, which seeks the largest separation between labeled and unlabeled data through regularization).", "labels": [], "entities": []}, {"text": "We implement it with the help of the SVM-light tool.", "labels": [], "entities": []}, {"text": "Co-training with random two-view generation (briefly called co-training with random views), where two views are generated by randomly splitting the whole feature space into two parts.", "labels": [], "entities": []}, {"text": "In semi-supervised sentiment classification, the data are randomly partitioned into labeled training data, unlabeled data, and testing data with the proportion of 10%, 70% and 20% respectively.", "labels": [], "entities": [{"text": "semi-supervised sentiment classification", "start_pos": 3, "end_pos": 43, "type": "TASK", "confidence": 0.6722595393657684}]}, {"text": "reports the classification accuracies in all iterations, where baseline indicates the supervised classifier f trained on the 10% data; both co-training and single classifier and co-training and combined classifier refer to co-training using our proposed personal and impersonal views.", "labels": [], "entities": []}, {"text": "But the former merely applies the baseline classifier f trained the new labeled data to test on the testing data while the latter applies the combined classifier  2 n n n = = = . For clarity, results of other methods (e.g. self-training, transductive SVM) are not shown in but will be reported in later.", "labels": [], "entities": [{"text": "SVM", "start_pos": 251, "end_pos": 254, "type": "TASK", "confidence": 0.7224472165107727}]}, {"text": "shows that co-training and combined classifier always outperforms co-training and single classifier.", "labels": [], "entities": []}, {"text": "This again justifies the effectiveness of our two-view learning on supervised sentiment classification.", "labels": [], "entities": [{"text": "supervised sentiment classification", "start_pos": 67, "end_pos": 102, "type": "TASK", "confidence": 0.664247045914332}]}, {"text": "One open question is whether the unlabeled data improve the performance.", "labels": [], "entities": []}, {"text": "Let us set aside the influence of the combination strategy and focus on the effectiveness of semi-supervised learning by comparing the baseline and co-training and single classifier.", "labels": [], "entities": []}, {"text": "shows different results on different domains.", "labels": [], "entities": []}, {"text": "Semi-supervised learning fails on the DVD domain while on the three domains of book, electronic, and software, semi-supervised learning benefits slightly (p-value>0.05).", "labels": [], "entities": []}, {"text": "In contrast, semi-supervised learning benefits much on the other four domains (health, kitchen, network, and pet) from using unlabeled data and the performance improvements are statistically significant (p-value<0.01).", "labels": [], "entities": []}, {"text": "Overall speaking, we think that the unlabeled data are very helpful as they lead to about 4% accuracy improvement on the average except for the DVD domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9994458556175232}, {"text": "DVD domain", "start_pos": 144, "end_pos": 154, "type": "DATASET", "confidence": 0.9589389860630035}]}, {"text": "Along with the supervised combination strategy, our approach can significantly improve the performance more than 7% on the average compared to the baseline.", "labels": [], "entities": []}, {"text": "shows the classification results of different methods with different sizes of the labeled data: 5%, 10%, and 15% of all data, where the testing data are kept the same (20% of all data).", "labels": [], "entities": []}, {"text": "Specifically, the results of other methods including self-training, transductive SVM, and random views are presented when 10% labeled data are used in training.", "labels": [], "entities": [{"text": "SVM", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.7749868631362915}]}, {"text": "It shows that self-training performs much worse than our approach and fails to improve the performance of five of the eight domains.", "labels": [], "entities": []}, {"text": "Transductive SVM performs even worse and can only improve the performance of the \"software\" domain.", "labels": [], "entities": []}, {"text": "Although co-training with random views outperforms the baseline on four of the eight domains, it performs worse than co-training and single classifier.", "labels": [], "entities": []}, {"text": "This suggests that the impressive improvements are mainly due to our unsupervised two-view mining rather than the combination strategy.", "labels": [], "entities": []}, {"text": "Figure 5 also shows that our approach is rather robust and achieves excellent performances in different training data sizes, although our approach fails on two domains, i.e. book and DVD, when only 5% of the labeled data are used.", "labels": [], "entities": []}, {"text": "This failure maybe due to that some of the samples in these two domains are too ambiguous and hard to classify.", "labels": [], "entities": []}, {"text": "Manual checking shows that quite a lot of samples on these two domains are even too difficult for professionals to give a high-confident label.", "labels": [], "entities": []}, {"text": "Another possible reason is that there exist too many objective descriptions in these two domains, thus introducing too much noisy information for semi-supervised learning.", "labels": [], "entities": []}, {"text": "The effectiveness of different sizes of chosen samples in each iteration is also evaluated like 1 2 3 6 n n n = = = and 1 2 3 3, 6 n n n = = = (This assignment is considered because the personal view classifier performs worse than the other two classifiers).", "labels": [], "entities": []}, {"text": "Our experimental results are still unsuccessful in the DVD domain and do not show much difference on other domains.", "labels": [], "entities": [{"text": "DVD domain", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.9531013071537018}]}, {"text": "We also test the co-training approach without the single-view classifier f . Experimental results show that the inclusion of the single-view classifier 3 f slightly helps the co-training approach.", "labels": [], "entities": []}, {"text": "The detailed discussion of the results is omitted due to space limit.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of supervised sentiment classification", "labels": [], "entities": [{"text": "supervised sentiment classification", "start_pos": 25, "end_pos": 60, "type": "TASK", "confidence": 0.7625483671824137}]}]}