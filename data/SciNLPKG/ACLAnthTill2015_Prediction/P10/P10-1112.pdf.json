{"title": [{"text": "Simple, Accurate Parsing with an All-Fragments Grammar", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a simple but accurate parser which exploits both large tree fragments and symbol refinement.", "labels": [], "entities": []}, {"text": "We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and tree-substitution grammar learning.", "labels": [], "entities": []}, {"text": "We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement.", "labels": [], "entities": []}, {"text": "Moreover , our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams.", "labels": [], "entities": []}, {"text": "Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-the-art lexicalized and latent-variable parsers.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.995315670967102}, {"text": "F1", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9997262358665466}]}, {"text": "Additional specific contributions center on making implicit all-fragments parsing efficient , including a coarse-to-fine inference scheme and anew graph encoding.", "labels": [], "entities": [{"text": "implicit all-fragments parsing", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.5350922147432963}]}], "introductionContent": [{"text": "Modern NLP systems have increasingly used dataintensive models that capture many or even all substructures from the training data.", "labels": [], "entities": []}, {"text": "In the domain of syntactic parsing, the idea that all training fragments 1 might be relevant to parsing has along history, including tree-substitution grammar (data-oriented parsing) approaches and tree kernel approaches ().", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7208936214447021}]}, {"text": "For machine translation, the key modern advancement has been the ability to represent and memorize large training substructures, be it in contiguous phrases ( or syntactic trees (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8110145628452301}]}, {"text": "In all such systems, a central challenge is efficiency: there are generally a combinatorial number of substructures in the training data, and it is impractical to explicitly extract them all.", "labels": [], "entities": []}, {"text": "On both efficiency and statistical grounds, much recent TSG work has focused on fragment selection.", "labels": [], "entities": [{"text": "TSG", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9531872272491455}, {"text": "fragment selection", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.9104636609554291}]}, {"text": "At the same time, many high-performance parsers have focused on symbol refinement approaches, wherein PCFG independence assumptions are weakened not by increasing rule sizes but by subdividing coarse treebank symbols into many subcategories either using structural annotation or lexicalization).", "labels": [], "entities": [{"text": "symbol refinement", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7284615635871887}]}, {"text": "Indeed, a recent trend has shown high accuracies from models which are dedicated to inducing such subcategories).", "labels": [], "entities": []}, {"text": "In this paper, we present a simplified parser which combines the two basic ideas, using both large fragments and symbol refinement, to provide non-local and local context respectively.", "labels": [], "entities": []}, {"text": "The two approaches turnout to be highly complementary; even the simplest (deterministic) symbol refinement and a basic use of an all-fragments grammar combine to give accuracies substantially above recent work on treesubstitution grammar based parsers and approaching top refinement-based parsers.", "labels": [], "entities": []}, {"text": "For example, our best result on the English WSJ task is an F1 of over 88%, where recent TSG parsers 2 achieve 82-84% and top refinement-based parsers 3 achieve 88-90% (e.g.,).", "labels": [], "entities": [{"text": "WSJ task", "start_pos": 44, "end_pos": 52, "type": "TASK", "confidence": 0.5149834603071213}, {"text": "F1", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9988215565681458}]}, {"text": "Rather than select fragments, we use a simplification of the PCFG-reduction of DOP to work with all fragments.", "labels": [], "entities": []}, {"text": "This reduction is a flexible, implicit representation of the fragments that, rather than extracting an intractably large grammar over fragment types, indexes all nodes in the training treebank and uses a compact grammar over indexed node tokens.", "labels": [], "entities": []}, {"text": "This indexed grammar, when appropriately marginalized, is equivalent to one in which all fragments are explicitly extracted.", "labels": [], "entities": []}, {"text": "Our work is the first to apply this reduction to full-scale parsing.", "labels": [], "entities": [{"text": "full-scale parsing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.5213630497455597}]}, {"text": "In this direction, we present a coarse-to-fine inference scheme and a compact graph encoding of the training set, which, together, make parsing manageable.", "labels": [], "entities": []}, {"text": "This tractability allows us to avoid selection of fragments, and work with all fragments.", "labels": [], "entities": []}, {"text": "Of course, having a grammar that includes all training substructures is only desirable to the extent that those structures can be appropriately weighted.", "labels": [], "entities": []}, {"text": "Implicit representations like those used here do not allow arbitrary weightings of fragments.", "labels": [], "entities": []}, {"text": "However, we use a simple weighting scheme which does decompose appropriately over the implicit encoding, and which is flexible enough to allow weights to depend not only on frequency but also on fragment size, node patterns, and certain lexical properties.", "labels": [], "entities": []}, {"text": "Similar ideas have been explored in,.", "labels": [], "entities": []}, {"text": "Our model empirically affirms the effectiveness of such a flexible weighting scheme in full-scale experiments.", "labels": [], "entities": []}, {"text": "We also investigate parsing without an explicit lexicon.", "labels": [], "entities": []}, {"text": "The all-fragments approach has the advantage that parsing down to the character level requires no special treatment; we show that an explicit lexicon is not needed when sentences are considered as strings of characters rather than words.", "labels": [], "entities": []}, {"text": "This avoids the need for complex unknown word models and other specialized lexical resources.", "labels": [], "entities": []}, {"text": "The main contribution of this work is to show practical, tractable methods for working with an all-fragments model, without an explicit lexicon.", "labels": [], "entities": []}, {"text": "In the parsing case, the central result is that accuracies in the range of state-of-the-art parsers (i.e., over 88% F1 on English WSJ) can be obtained with no sampling, no latent-variable modeling, no smoothing, and even no explicit lexicon (hence negligible training overall).", "labels": [], "entities": [{"text": "parsing", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.9636704921722412}, {"text": "F1", "start_pos": 116, "end_pos": 118, "type": "METRIC", "confidence": 0.9947827458381653}, {"text": "English WSJ", "start_pos": 122, "end_pos": 133, "type": "DATASET", "confidence": 0.8184772431850433}]}, {"text": "These techniques, however, are not limited to the case of monolingual parsing, offering extensions to models of machine translation, semantic interpretation, and other areas in which a similar tension exists between the desire to extract many large structures and the computational cost of doing so.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.729987770318985}, {"text": "semantic interpretation", "start_pos": 133, "end_pos": 156, "type": "TASK", "confidence": 0.7481832504272461}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: All-fragments WSJ results (accuracy F1 and exact  match EX) for the constituent, rule-sum and variational ob- jectives, using parent annotation and one level of markoviza- tion.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.3841657042503357}, {"text": "accuracy F1", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.8803224563598633}, {"text": "exact  match EX", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.9729998509089152}]}, {"text": " Table 3: All-fragments WSJ results for the character-level  parsing model, using parent annotation and one level of  markovization.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.44887474179267883}, {"text": "character-level  parsing", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.6537041813135147}]}, {"text": " Table 4: F1 for a basic PCFG, and incorporation of basic  refinement, all-fragments and both, for WSJ dev-set (\u2264 40  words). P = 1 means parent annotation of all non-terminals,  including the preterminal tags. H = 1 means one level of  markovization. Results from", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9988042116165161}]}, {"text": " Table 5: Our WSJ test set parsing accuracies, compared  to recent fragment-based parsers and top refinement-based  parsers. Basic Refinement is our all-fragments grammar with  parent annotation. Additional Refinement adds determinis- tic refinement of Klein and Manning (2003) (Section 5.3).", "labels": [], "entities": [{"text": "WSJ test", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.928080290555954}]}]}