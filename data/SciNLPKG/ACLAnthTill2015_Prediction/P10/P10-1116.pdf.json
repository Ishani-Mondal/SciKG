{"title": [{"text": "Topic Models for Word Sense Disambiguation and Token-based Idiom Detection", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.6094087064266205}, {"text": "Token-based Idiom Detection", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.583139439423879}]}], "abstractContent": [{"text": "This paper presents a probabilistic model for sense disambiguation which chooses the best sense based on the conditional probability of sense paraphrases given a context.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.7502191364765167}]}, {"text": "We use a topic model to decompose this conditional probability into two conditional probabilities with latent variables.", "labels": [], "entities": []}, {"text": "We propose three different instanti-ations of the model for solving sense dis-ambiguation problems with different degrees of resource availability.", "labels": [], "entities": []}, {"text": "The proposed models are tested on three different tasks: coarse-grained word sense disam-biguation, fine-grained word sense disam-biguation, and detection of literal vs. non-literal usages of potentially idiomatic expressions.", "labels": [], "entities": [{"text": "detection of literal vs. non-literal usages of potentially idiomatic expressions", "start_pos": 145, "end_pos": 225, "type": "TASK", "confidence": 0.6646021485328675}]}, {"text": "In all three cases, we outper-form state-of-the-art systems either quantitatively or statistically significantly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense disambiguation (WSD) is the task of automatically determining the correct sense fora target word given the context in which it occurs.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8346689790487289}]}, {"text": "WSD is an important problem in NLP and an essential preprocessing step for many applications, including machine translation, question answering and information extraction.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.64610755443573}, {"text": "machine translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.8287037014961243}, {"text": "question answering", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.9155515432357788}, {"text": "information extraction", "start_pos": 148, "end_pos": 170, "type": "TASK", "confidence": 0.8338581919670105}]}, {"text": "However, WSD is a difficult task, and despite the fact that it has been the focus of much research over the years, stateof-the-art systems are still often not good enough for real-world applications.", "labels": [], "entities": [{"text": "WSD", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9673415422439575}]}, {"text": "One major factor that makes WSD difficult is a relative lack of manually annotated corpora, which hampers the performance of supervised systems.", "labels": [], "entities": [{"text": "WSD", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9925333261489868}]}, {"text": "To address this problem, there has been a significant amount of work on unsupervised WSD that does not require manually sensedisambiguated training data (see for an overview).", "labels": [], "entities": [{"text": "WSD", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9325610995292664}]}, {"text": "Recently, several researchers have experimented with topic models) for sense disambiguation and induction.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.8240790367126465}]}, {"text": "Topic models are generative probabilistic models of text corpora in which each document is modelled as a mixture over (latent) topics, which are in turn represented by a distribution over words.", "labels": [], "entities": []}, {"text": "Previous approaches using topic models for sense disambiguation either embed topic features in a supervised model or rely heavily on the structure of hierarchical lexicons such as WordNet ( . In this paper, we propose a novel framework which is fairly resource-poor in that it requires only 1) a large unlabelled corpus from which to estimate the topics distributions, and 2) paraphrases for the possible target senses.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7099855542182922}, {"text": "WordNet", "start_pos": 180, "end_pos": 187, "type": "DATASET", "confidence": 0.9348856210708618}]}, {"text": "The paraphrases can be user-supplied or can betaken from existing resources.", "labels": [], "entities": []}, {"text": "We approach the sense disambiguation task by choosing the best sense based on the conditional probability of sense paraphrases given a context.", "labels": [], "entities": [{"text": "sense disambiguation task", "start_pos": 16, "end_pos": 41, "type": "TASK", "confidence": 0.7934132317701975}]}, {"text": "We propose three models which are suitable for different situations: Model I requires knowledge of the prior distribution over senses and directly maximizes the conditional probability of a sense given the context; Model II maximizes this conditional probability by maximizing the cosine value of two topic-document vectors (one for the sense and one for the context).", "labels": [], "entities": []}, {"text": "We apply these models to coarse-and fine-grained WSD and find that they outperform comparable systems for both tasks.", "labels": [], "entities": []}, {"text": "We also test our framework on the related task of idiom detection, which involves distinguishing literal and nonliteral usages of potentially ambiguous expressions such as rock the boat.", "labels": [], "entities": [{"text": "idiom detection", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7409631162881851}]}, {"text": "For this task, we propose a third model.", "labels": [], "entities": []}, {"text": "Model III calculates the probability of a sense given a context according to the component words of the sense paraphrase.", "labels": [], "entities": []}, {"text": "Specifically, it chooses the sense type which maximizes the probability (given the context) of the paraphrase component word with the highest likelihood of occurring in that context.", "labels": [], "entities": []}, {"text": "This model also outperforms state-of-the-art systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our models on three different tasks: coarse-grained WSD, fine-grained WSD and literal vs. nonliteral sense detection.", "labels": [], "entities": [{"text": "literal vs. nonliteral sense detection", "start_pos": 90, "end_pos": 128, "type": "TASK", "confidence": 0.6037810921669007}]}, {"text": "In this section we discuss our experimental set-up.", "labels": [], "entities": []}, {"text": "We start by describing the three datasets for evaluation and another dataset for probability estimation.", "labels": [], "entities": [{"text": "probability estimation", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.6255839616060257}]}, {"text": "We also discuss how we choose sense paraphrases and instance contexts.", "labels": [], "entities": []}, {"text": "Data We use three datasets for evaluation.", "labels": [], "entities": []}, {"text": "The coarse-grained task is evaluated on the Semeval-2007 Task-07 benchmark dataset released by.", "labels": [], "entities": [{"text": "Semeval-2007 Task-07 benchmark dataset", "start_pos": 44, "end_pos": 82, "type": "DATASET", "confidence": 0.800987109541893}]}, {"text": "The dataset consists of 5377 words of running text from five different articles: the first three were obtained from the WSJ corpus, the fourth was the Wikipedia entry for computer programming, and the fifth was an excerpt of Amy Steedman's Knights of the Art, biographies of Italian painters.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 120, "end_pos": 130, "type": "DATASET", "confidence": 0.984451562166214}]}, {"text": "The proportion of the non news text, the last two articles, constitutes 51.87% of the whole testing set.", "labels": [], "entities": []}, {"text": "It consists of 1108 nouns, 591 verbs, 362 adjectives, and 208 adverbs.", "labels": [], "entities": []}, {"text": "The data were annotated with coarse-grained senses which were obtained by clustering senses from the WordNet 2.1 sense inventory based on the procedure proposed by.", "labels": [], "entities": [{"text": "WordNet 2.1 sense inventory", "start_pos": 101, "end_pos": 128, "type": "DATASET", "confidence": 0.9152432233095169}]}, {"text": "To determine whether our model is also suitable for fine-grained WSD, we test on the data provided by for the Semeval-2007 Task-17 (English fine-grained all-words task).", "labels": [], "entities": [{"text": "WSD", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9235163927078247}]}, {"text": "This dataset is a subset of the set from Task-07.", "labels": [], "entities": []}, {"text": "It comprises the three WSJ articles from.", "labels": [], "entities": [{"text": "WSJ articles", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.8959039747714996}]}, {"text": "A total of 465 lemmas were selected as instances from about 3500 words of text.", "labels": [], "entities": []}, {"text": "There are 10 instances marked as 'U' (undecided sense tag).", "labels": [], "entities": [{"text": "U", "start_pos": 34, "end_pos": 35, "type": "METRIC", "confidence": 0.942677915096283}]}, {"text": "Of the remaining 455 instances, 159 are nouns and 296 are verbs.", "labels": [], "entities": []}, {"text": "The sense inventory is from WordNet 2.1.", "labels": [], "entities": [{"text": "WordNet 2.1", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.9199278354644775}]}, {"text": "Finally, we test our model on the related sense disambiguation task of distinguishing literal and nonliteral usages of potentially ambiguous expressions such as break the ice.", "labels": [], "entities": [{"text": "distinguishing literal and nonliteral usages of potentially ambiguous expressions such as break the ice", "start_pos": 71, "end_pos": 174, "type": "TASK", "confidence": 0.5451157093048096}]}, {"text": "For this, we use the dataset from Sporleder and Li (2009) as a test set.", "labels": [], "entities": []}, {"text": "This dataset consists of 3964 instances of 17 potential English idioms which were manually annotated as literal or nonliteral.", "labels": [], "entities": []}, {"text": "A Wikipedia dump 2 is used to estimate the multinomial word-topic distribution.", "labels": [], "entities": []}, {"text": "This dataset, which consists of 320,000 articles, 3 is significantly larger than SemCor, which is the dataset used by Boyd- . All markup from the Wikipedia dump was stripped off using the same filter as the ESA implementation, and stopwords were filtered out using the Snowball (Porter, October 2001) stopword list.", "labels": [], "entities": [{"text": "Snowball (Porter, October 2001) stopword list", "start_pos": 269, "end_pos": 314, "type": "DATASET", "confidence": 0.943219867017534}]}, {"text": "In addition, words with a Wikipedia document frequency of 1 were filtered out.", "labels": [], "entities": []}, {"text": "The lemmatized version of the corpus consists of 299,825 lexical units.", "labels": [], "entities": []}, {"text": "The test sets were POS-tagged and lemmatized using RASP ().", "labels": [], "entities": [{"text": "RASP", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.4779618978500366}]}, {"text": "The inference processes are run on the lemmatized version of the corpus.", "labels": [], "entities": []}, {"text": "For the Semeval-2007 Task 17 English all-words, the organizers do not supply the part-of-speech and lemma information of the target instances.", "labels": [], "entities": [{"text": "Semeval-2007 Task 17 English all-words", "start_pos": 8, "end_pos": 46, "type": "TASK", "confidence": 0.7965104937553406}]}, {"text": "In order to avoid the wrong predic- We use the English snapshot of 2009-07-13 All articles of fewer than 100 words were discarded.", "labels": [], "entities": [{"text": "English snapshot of 2009-07-13", "start_pos": 47, "end_pos": 77, "type": "DATASET", "confidence": 0.9428873062133789}]}, {"text": "tions caused by tagging or lemmatization errors, we manually corrected any bad tags and lemmas for the target instances.", "labels": [], "entities": []}, {"text": "Sense Paraphrases For word sense disambiguation tasks, the paraphrases of the sense keys are represented by information from WordNet 2.1..", "labels": [], "entities": [{"text": "word sense disambiguation tasks", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.7730135917663574}]}, {"text": "To obtain the paraphrases, we use the word forms, glosses and example sentences of the synset itself and a set of selected reference synsets (i.e., synsets linked to the target synset by specific semantic relations, see).", "labels": [], "entities": []}, {"text": "We excluded the 'hypernym reference synsets', since information common to all of the child synsets may confuse the disambiguation process.", "labels": [], "entities": []}, {"text": "For the literal vs. nonliteral sense detection task, we selected the paraphrases of the nonliteral meaning from several online idiom dictionaries.", "labels": [], "entities": [{"text": "literal vs. nonliteral sense detection", "start_pos": 8, "end_pos": 46, "type": "TASK", "confidence": 0.6398178577423096}]}, {"text": "For the literal senses, we used 2-3 manually selected words with which we tried to capture (aspects of) the literal meaning of the expression.", "labels": [], "entities": []}, {"text": "For instance, the literal 'paraphrases' that we chose for 'break the ice' were ice, water and snow.", "labels": [], "entities": []}, {"text": "The paraphrases are shorter for the idiom task than for the WSD task, because the meaning descriptions from the idiom dictionaries are shorter than what we get from WordNet.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 60, "end_pos": 68, "type": "TASK", "confidence": 0.7274014055728912}, {"text": "WordNet", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.9452834725379944}]}, {"text": "In the latter case, each sense can be represented by its synset as well as its reference synsets.", "labels": [], "entities": []}, {"text": "Instance Context We experimented with different context sizes for the disambiguation task.", "labels": [], "entities": [{"text": "disambiguation task", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.9062182307243347}]}, {"text": "The five different context settings that we used for the WSD tasks are: collocations (1w), \u00b15-word window (5w), \u00b110-word window (10w), current sentence, and whole text.", "labels": [], "entities": [{"text": "WSD tasks", "start_pos": 57, "end_pos": 66, "type": "TASK", "confidence": 0.9144831001758575}]}, {"text": "Because the idiom corpus also includes explicitly marked paragraph boundaries, we included 'paragraph' as a sixth type of context size for the idiom sense detection task.", "labels": [], "entities": [{"text": "idiom sense detection task", "start_pos": 143, "end_pos": 169, "type": "TASK", "confidence": 0.7073086947202682}]}, {"text": "As mentioned above, we test our proposed sense disambiguation framework on three tasks.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7173431813716888}]}, {"text": "We start by describing the sampling experiments for: Selected reference synsets from WordNet that were used for different parts-of-speech to obtain word sense paraphrase.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.9736084938049316}, {"text": "word sense paraphrase", "start_pos": 148, "end_pos": 169, "type": "TASK", "confidence": 0.6851780315240225}]}, {"text": "N(noun), V(verb), A(adj), R(adv).", "labels": [], "entities": []}, {"text": "estimating the word-topic distribution from the Wikipedia dump.", "labels": [], "entities": [{"text": "Wikipedia dump", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.949577271938324}]}, {"text": "We used the package provided by with the suggested Dirichlet hyper-parameters . In order to avoid statistical instability, the final result is averaged over the last 50 iterations.", "labels": [], "entities": []}, {"text": "We did four rounds of sampling with 1000, 500, 250, and 125 topics respectively.", "labels": [], "entities": []}, {"text": "The final word-topic distribution is a normalized concatenate of the four distributions estimated in each round.", "labels": [], "entities": []}, {"text": "In average, the sampling program run on the Wikipedia dump consumed 20G memory, and each round took about one week on a single AMD Dual-Core 1000MHZ processor.", "labels": [], "entities": [{"text": "Wikipedia dump", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.929179459810257}]}], "tableCaptions": [{"text": " Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).", "labels": [], "entities": [{"text": "F-score)", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9760898053646088}]}, {"text": " Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).", "labels": [], "entities": [{"text": "attempted rate (Ate.)", "start_pos": 60, "end_pos": 81, "type": "METRIC", "confidence": 0.9346330881118774}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9990999698638916}, {"text": "Pre.)", "start_pos": 94, "end_pos": 99, "type": "METRIC", "confidence": 0.8760406374931335}, {"text": "recall (Rec.)", "start_pos": 102, "end_pos": 115, "type": "METRIC", "confidence": 0.9372704029083252}, {"text": "F-score (F1)", "start_pos": 117, "end_pos": 129, "type": "METRIC", "confidence": 0.834849625825882}]}, {"text": " Table 5: Performance on the literal or nonliteral  sense disambiguation task on idioms. literal pre- cision (Prec l ), literal recall (Rec l ), literal F-score  (F l ), accuracy(Acc.).", "labels": [], "entities": [{"text": "literal or nonliteral  sense disambiguation task", "start_pos": 29, "end_pos": 77, "type": "TASK", "confidence": 0.6994589517513911}, {"text": "recall (Rec l )", "start_pos": 128, "end_pos": 143, "type": "METRIC", "confidence": 0.9021363019943237}, {"text": "F-score  (F l )", "start_pos": 153, "end_pos": 168, "type": "METRIC", "confidence": 0.9051968574523925}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9994317889213562}, {"text": "Acc.", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.9851515889167786}]}]}