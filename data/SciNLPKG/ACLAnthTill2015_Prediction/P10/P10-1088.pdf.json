{"title": [{"text": "Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation", "labels": [], "entities": [{"text": "Bucking the Trend", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8783849676450094}, {"text": "Statistical Machine Translation", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.8259562055269877}]}], "abstractContent": [{"text": "We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8126048445701599}]}, {"text": "The main challenge is how to buck the trend of diminishing returns that is commonly encountered.", "labels": [], "entities": []}, {"text": "We present an active learning-style data solic-itation algorithm to meet this challenge.", "labels": [], "entities": []}, {"text": "We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 38, "end_pos": 60, "type": "DATASET", "confidence": 0.9409539500872294}]}], "introductionContent": [{"text": "shows the learning curves for two state of the art statistical machine translation (SMT) systems for Urdu-English translation.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 51, "end_pos": 88, "type": "TASK", "confidence": 0.8263961871465048}, {"text": "Urdu-English translation", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.7030570805072784}]}, {"text": "Observe how the learning curves rise rapidly at first but then a trend of diminishing returns occurs: put simply, the curves flatten.", "labels": [], "entities": []}, {"text": "This paper investigates whether we can buck the trend of diminishing returns, and if so, how we can do it effectively.", "labels": [], "entities": []}, {"text": "Active learning (AL) has been applied to SMT recently ( ) but they were interested in starting with a tiny seed set of data, and they stopped their investigations after only adding a relatively tiny amount of data as depicted in.", "labels": [], "entities": [{"text": "Active learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7417276799678802}, {"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9949362277984619}]}, {"text": "In contrast, we are interested in applying AL when a large amount of data already exists as is the case for many important lanuage pairs.", "labels": [], "entities": []}, {"text": "We develop an AL algorithm that focuses on keeping annotation costs (measured by time in seconds) low.", "labels": [], "entities": []}, {"text": "It succeeds in doing this by only soliciting translations for parts of sentences.", "labels": [], "entities": []}, {"text": "We show that this gets a savings inhuman annotation time above and beyond what the reduction in # words annotated would have indicated by a factor of about three and speculate as to why.", "labels": [], "entities": []}, {"text": "The y-axis measures BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9673983752727509}]}, {"text": "Note the diminishing returns as more data is added.", "labels": [], "entities": []}, {"text": "Also note how relatively early on in the process previous studies were terminated.", "labels": [], "entities": []}, {"text": "In contrast, the focus of our main experiments doesn't even begin until much higher performance has already been achieved with a period of diminishing returns firmly established.", "labels": [], "entities": []}, {"text": "We conduct experiments for Urdu-English translation, gathering annotations via Amazon Mechanical Turk (MTurk) and show that we can indeed buck the trend of diminishing returns, achieving an order of magnitude increase in the rate of improvement in performance.", "labels": [], "entities": [{"text": "Urdu-English translation", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.604193702340126}, {"text": "Amazon Mechanical Turk (MTurk)", "start_pos": 79, "end_pos": 109, "type": "DATASET", "confidence": 0.92315145333608}]}, {"text": "Section 2 discusses related work; Section 3 discusses preliminary experiments that show the guiding principles behind the algorithm we use; Section 4 explains our method for soliciting new translation data; Section 5 presents our main results; and Section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we report on results of simulation experiments that help to illustrate and motivate the design decisions of the algorithm we present in Section 4.", "labels": [], "entities": []}, {"text": "We use the Urdu-English language pack 1 from the Linguistic Data Consortium (LDC), which contains \u2248 88000 Urdu-English sentence translation pairs, amounting to \u2248 1.7 million Urdu words translated into English.", "labels": [], "entities": [{"text": "Linguistic Data Consortium (LDC)", "start_pos": 49, "end_pos": 81, "type": "DATASET", "confidence": 0.856487919886907}]}, {"text": "All experiments in this paper evaluate on a genre-balanced split of the NIST2008 Urdu-English test set.", "labels": [], "entities": [{"text": "NIST2008 Urdu-English test set", "start_pos": 72, "end_pos": 102, "type": "DATASET", "confidence": 0.955165445804596}]}, {"text": "In addition, the language pack contains an Urdu-English dictionary consisting of \u2248 114000 entries.", "labels": [], "entities": []}, {"text": "In all the experiments, we use the dictionary at every iteration of training.", "labels": [], "entities": []}, {"text": "This will make it harder for us to show our methods providing substantial gains since the dictionary will provide a higher base performance to begin with.", "labels": [], "entities": []}, {"text": "However, it would be artificial to ignore dictionary resources when they exist.", "labels": [], "entities": []}, {"text": "We experiment with two translation models: hierarchical phrase-based translation and syntax augmented translation (), both of which are implemented in the Joshua decoder (.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.6900158226490021}, {"text": "syntax augmented translation", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.6676601270834605}]}, {"text": "We hereafter refer to these systems as jHier and jSyntax, respectively.", "labels": [], "entities": []}, {"text": "We will now present results of experiments with different methods for growing MT training data.", "labels": [], "entities": [{"text": "MT training", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.9268813729286194}]}, {"text": "The results are organized into three areas of investigations: 1.", "labels": [], "entities": []}, {"text": "managing uncertainty; and 3.", "labels": [], "entities": []}, {"text": "how to automatically detect when to stop soliciting annotations from a pool of data.", "labels": [], "entities": []}], "tableCaptions": []}