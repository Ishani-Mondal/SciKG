{"title": [{"text": "Word representations: A simple and general method for semi-supervised learning", "labels": [], "entities": [{"text": "Word representations", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.673684686422348}]}], "abstractContent": [{"text": "If we take an existing supervised NLP system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9985957741737366}]}, {"text": "We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking.", "labels": [], "entities": []}, {"text": "We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9988767504692078}]}, {"text": "We find further improvements by combining different word representations.", "labels": [], "entities": []}, {"text": "You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.", "labels": [], "entities": []}, {"text": "com/projects/wordreprs/", "labels": [], "entities": []}], "introductionContent": [{"text": "By using unlabelled data to reduce data sparsity in the labeled training data, semi-supervised approaches improve generalization accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.8963251113891602}]}, {"text": "Semi-supervised models such as,, and achieve state-of-the-art accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9990500807762146}]}, {"text": "However, these approaches dictate a particular choice of model and training regime.", "labels": [], "entities": []}, {"text": "It can be tricky and time-consuming to adapt an existing supervised NLP system to use these semi-supervised techniques.", "labels": [], "entities": []}, {"text": "It is preferable to use a simple and general method to adapt existing supervised NLP systems to be semi-supervised.", "labels": [], "entities": []}, {"text": "One approach that is becoming popular is to use unsupervised methods to induce word features-or to download word features that have already been induced-plug these word features into an existing system, and observe a significant increase inaccuracy.", "labels": [], "entities": []}, {"text": "But which word features are good for what tasks?", "labels": [], "entities": []}, {"text": "Should we prefer certain word features?", "labels": [], "entities": []}, {"text": "A word representation is a mathematical object associated with each word, often a vector.", "labels": [], "entities": []}, {"text": "Each dimension's value corresponds to a feature and might even have a semantic or grammatical interpretation, so we call it a word feature.", "labels": [], "entities": []}, {"text": "Conventionally, supervised lexicalized NLP approaches take a word and convert it to a symbolic ID, which is then transformed into a feature vector using a one-hot representation: The feature vector has the same length as the size of the vocabulary, and only one dimension is on.", "labels": [], "entities": []}, {"text": "However, the one-hot representation of a word suffers from data sparsity: Namely, for words that are rare in the labeled training data, their corresponding model parameters will be poorly estimated.", "labels": [], "entities": []}, {"text": "Moreover, attest time, the model cannot handle words that do not appear in the labeled training data.", "labels": [], "entities": []}, {"text": "These limitations of one-hot word representations have prompted researchers to investigate unsupervised methods for inducing word representations overlarge unlabeled corpora.", "labels": [], "entities": []}, {"text": "Word features can be hand-designed, but our goal is to learn them.", "labels": [], "entities": []}, {"text": "One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical.", "labels": [], "entities": []}, {"text": "This technique was used by a variety of researchers (.", "labels": [], "entities": []}, {"text": "This leads to a one-hot representation over a smaller vocabulary size.", "labels": [], "entities": []}, {"text": "Neural language models (, on the other hand, induce dense real-valued low-dimensional word embeddings using unsupervised approaches.", "labels": [], "entities": []}, {"text": "fora more complete list of references on neural language models.)", "labels": [], "entities": []}, {"text": "Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9618127942085266}]}, {"text": "But different word representations have never been systematically compared in a controlled way.", "labels": [], "entities": []}, {"text": "In this work, we compare different techniques for inducing word representations, evaluating them on the tasks of named entity recognition (NER) and chunking.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 113, "end_pos": 143, "type": "TASK", "confidence": 0.8091378311316172}]}, {"text": "We retract former negative results published in about embeddings, given training improvements that we describe in Section 7.1.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the hypothesis that one can take an existing, near state-of-the-art, supervised NLP system, and improve its accuracy by including word representations as word features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9977912902832031}]}, {"text": "This technique for turning a supervised approach into a semi-supervised one is general and task-agnostic.", "labels": [], "entities": []}, {"text": "However, we wish to find out if certain word representations are preferable for certain tasks.", "labels": [], "entities": []}, {"text": "finds that the representations that are good for NER are poor for search query classification, and vice-versa.", "labels": [], "entities": [{"text": "NER", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9411025643348694}, {"text": "search query classification", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.6484890182813009}]}, {"text": "We apply clustering and distributed representations to NER and chunking, which allows us to compare our semi-supervised models to those of Ando and and.", "labels": [], "entities": []}, {"text": "You can visit http://metaoptimize.com/ projects/wordreprs/ to find: The word representations we induced, which you can download and use in your experiments; The code for inducing the word representations, which you can use to induce word representations on your own data; The NER and chunking system, with code for replicating our experiments..", "labels": [], "entities": []}, {"text": "A robust risk minimization based named entity recognition system.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.6316179037094116}]}, {"text": "Zhao, H., Chen, W.,.", "labels": [], "entities": []}, {"text": "Multilingual dependency learning: a huge feature engineering method to semantic dependency parsing.", "labels": [], "entities": [{"text": "Multilingual dependency learning", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7151947220166525}, {"text": "semantic dependency parsing", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.6918857097625732}]}, {"text": "CoNLL (pp. 55-60).", "labels": [], "entities": [{"text": "CoNLL (pp. 55-60)", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9401995062828064}]}], "tableCaptions": [{"text": " Table 2: Final chunking F1 results. In the last section, we", "labels": [], "entities": [{"text": "Final chunking", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.5287982076406479}, {"text": "F1", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.8948220014572144}]}, {"text": " Table 3: Final NER F1 results, showing the cumulative", "labels": [], "entities": [{"text": "NER", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.4642166793346405}, {"text": "F1", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.31452351808547974}]}]}