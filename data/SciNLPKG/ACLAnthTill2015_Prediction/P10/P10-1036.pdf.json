{"title": [], "abstractContent": [{"text": "We propose a novel self-training method fora parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9963443875312805}]}, {"text": "The idea is to train the su-pertagger on large amounts of parser output , so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highest-scoring derivation.", "labels": [], "entities": []}, {"text": "Since the supertag-ger supplies fewer supertags overall, the parsing speed is increased.", "labels": [], "entities": [{"text": "parsing", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.9692448973655701}]}, {"text": "We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtaining significant speed increases on newspaper text with no loss inaccuracy.", "labels": [], "entities": [{"text": "speed", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9863951206207275}]}, {"text": "We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9990956783294678}]}], "introductionContent": [{"text": "In many NLP tasks and applications, e.g. distributional similarity and question answering (), large volumes of text and detailed syntactic information are both critical for high performance.", "labels": [], "entities": [{"text": "question answering", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8824292719364166}]}, {"text": "To avoid a tradeoff between these two, we need to increase parsing speed, but without losing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 59, "end_pos": 66, "type": "TASK", "confidence": 0.9638009071350098}, {"text": "speed", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.6035075187683105}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9982261061668396}]}, {"text": "Parsing with lexicalised grammar formalisms, such as Lexicalised Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG;), can be made more efficient using a supertagger.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9713057279586792}]}, {"text": "call supertagging almost parsing because of the significant reduction in ambiguity which occurs once the supertags have been assigned.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the CCG parser and supertagger described in.", "labels": [], "entities": []}, {"text": "Since the CCG lexical category set used by the supertagger is much larger than the Penn Treebank POS tag set, the accuracy of supertagging is much lower than POS tagging; hence the CCG supertagger assigns multiple supertags 1 to a word, when the local context does not provide enough information to decide on the correct supertag.", "labels": [], "entities": [{"text": "Penn Treebank POS tag set", "start_pos": 83, "end_pos": 108, "type": "DATASET", "confidence": 0.9768347382545471}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9994677901268005}]}, {"text": "The supertagger feeds lexical categories to the parser, and the two interact, sometimes using multiple passes over a sentence.", "labels": [], "entities": []}, {"text": "If a spanning analysis cannot be found by the parser, the number of lexical categories supplied by the supertagger is increased.", "labels": [], "entities": []}, {"text": "The supertagger-parser interaction influences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence.", "labels": [], "entities": []}, {"text": "Our goal is to increase parsing speed without loss of accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9819722771644592}, {"text": "speed", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.7995001077651978}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9980977177619934}]}, {"text": "The technique we use is a form of self-training, in which the output of the parser is used to train the supertagger component.", "labels": [], "entities": []}, {"text": "The existing literature on self-training reports mixed results.  were unable to improve the accuracy of POS tagging using self-training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.999390721321106}, {"text": "POS tagging", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.9446334838867188}]}, {"text": "In contrast, report improved accuracy through self-training fora twostage parser and re-ranker.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9993788003921509}]}, {"text": "Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adaptive supertagger.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9993041753768921}]}, {"text": "The adaptive supertagger produces lexical categories that the parser would have used in the final derivation when using the baseline model.", "labels": [], "entities": []}, {"text": "However, it does so with much lower ambiguity levels, and potentially during an earlier pass, which means sentences are parsed faster.", "labels": [], "entities": []}, {"text": "By increasing the ambiguity level of the adaptive models to match the baseline system, we can also slightly increase supertagging accuracy, which can lead to higher parsing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9799480438232422}, {"text": "parsing", "start_pos": 165, "end_pos": 172, "type": "TASK", "confidence": 0.9632858037948608}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.8852366805076599}]}, {"text": "Using the parser to generate training data also has the advantage that it is not a domain specific process.", "labels": [], "entities": []}, {"text": "Previous work has shown that parsers typically perform poorly outside of their training domain.", "labels": [], "entities": []}, {"text": "Using a newspapertrained parser, we constructed new training sets for Wikipedia and biomedical text.", "labels": [], "entities": []}, {"text": "These were used to create new supertagging models adapted to the different domains.", "labels": [], "entities": []}, {"text": "The self-training method of adapting the supertagger to suit the parser increased parsing speed by more than 50% across all three domains, without loss of accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 82, "end_pos": 89, "type": "TASK", "confidence": 0.9628332853317261}, {"text": "speed", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.5687916874885559}, {"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9954320192337036}]}, {"text": "Using an adapted supertagger with ambiguity levels tuned to match the baseline system, we were also able to increase F-score on labelled grammatical relations by 0.75%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.9967581629753113}]}], "datasetContent": [{"text": "We have used Sections 02-21 of CCGbank were parsed to produce the training data for adaptation.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.7996935844421387}, {"text": "adaptation", "start_pos": 84, "end_pos": 94, "type": "TASK", "confidence": 0.9723951816558838}]}, {"text": "This text was tokenised using the C&C tools tokeniser and parsed using our baseline models.", "labels": [], "entities": [{"text": "C&C tools tokeniser", "start_pos": 34, "end_pos": 53, "type": "DATASET", "confidence": 0.7398884952068329}]}, {"text": "For the smaller training sets, sentences from 1988 were used as they would be most similar in style to the evaluation corpus.", "labels": [], "entities": []}, {"text": "In all experiments the sentences from 1989 were excluded to ensure no overlap occurred with CCGbank.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9831094145774841}]}, {"text": "As Wikipedia text we have used 794,024,397 tokens (51,673,069 sentences) from Wikipedia articles.", "labels": [], "entities": []}, {"text": "This text was processed in the same way as the NANC data to produce parser-annotated training data.", "labels": [], "entities": [{"text": "NANC data", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.9618898332118988}]}, {"text": "For supertagger evaluation, one thousand sentences were manually annotated with CCG lexical categories and POS tags.", "labels": [], "entities": []}, {"text": "For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations () in the style of.", "labels": [], "entities": []}, {"text": "Both sets of annotations were produced by manually correcting the output of the baseline system.", "labels": [], "entities": []}, {"text": "The annotation was performed by Stephen Clark and Laura Rimell.", "labels": [], "entities": []}, {"text": "For the biomedical domain we have used several different resources.", "labels": [], "entities": []}, {"text": "As gold standard data for supertagger evaluation we have used supertagged GENIA data (, annotated by.", "labels": [], "entities": [{"text": "GENIA data", "start_pos": 74, "end_pos": 84, "type": "DATASET", "confidence": 0.9204857051372528}]}, {"text": "For parsing evaluation, grammatical relations from the BioInfer corpus were used ( same post-processing process as to convert the C&C parser output to Stanford format grammatical relations).", "labels": [], "entities": [{"text": "parsing evaluation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9523958265781403}, {"text": "BioInfer corpus", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.9520138502120972}]}, {"text": "For adaptive training we have used 1,900,618,859 tokens (76,739,723 sentences) from the MEDLINE abstracts tokenised by.", "labels": [], "entities": []}, {"text": "These sentences were POS-tagged and parsed twice, once as for the newswire and Wikipedia data, and then again, using the bio-specific models developed by.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.9038225412368774}]}, {"text": "Statistics for the sentences in the training sets are given in.", "labels": [], "entities": []}, {"text": "For speed evaluation we held out three sets of sentences from each domain-specific corpus.", "labels": [], "entities": []}, {"text": "Specifically, we used 30,000, 4,000 and 2,000 unique sentences of length 5-20, 21-40 and 41-250 tokens respectively.", "labels": [], "entities": []}, {"text": "Speeds on these length controlled sets were combined to calculate an overall parsing speed for the text in each domain.", "labels": [], "entities": []}, {"text": "Note that more than 20% of the Wikipedia sentences were less than five words in length and the overall distribution is skewed towards shorter sentences compared to the other corpora.", "labels": [], "entities": []}, {"text": "We used the hybrid parsing model described in, and the Viterbi decoder to find the highest-scoring derivation.", "labels": [], "entities": [{"text": "Viterbi decoder", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.8763097822666168}]}, {"text": "The multipass supertagger-parser interaction was also used.", "labels": [], "entities": []}, {"text": "The test data was excluded from training data for the supertagger for all of the newswire and Wikipedia models.", "labels": [], "entities": []}, {"text": "For the biomedical models ten-fold cross validation was used.", "labels": [], "entities": []}, {"text": "The accuracy of supertagging is measured by multi-tagging at the first \u03b2 level and considering a word correct if the correct tag is amongst any of the assigned tags.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994090795516968}]}, {"text": "For the biomedical parser evaluation we have used the parsing model and grammatical relation conversion script from.", "labels": [], "entities": [{"text": "biomedical parser evaluation", "start_pos": 8, "end_pos": 36, "type": "TASK", "confidence": 0.8350130319595337}]}, {"text": "Our timing measurements are calculated in two ways.", "labels": [], "entities": [{"text": "timing", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9876392483711243}]}, {"text": "Overall times were measured using the C&C parser's timers.", "labels": [], "entities": [{"text": "C&C parser", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.9285819381475449}]}, {"text": "Individual sentence measurements were made using the Intel timing registers, since standard methods are not accurate enough for the short time it takes to parse a single sentence.", "labels": [], "entities": []}, {"text": "To check whether changes were statistically significant we applied the test described by.", "labels": [], "entities": []}, {"text": "This measures the probability that two sets of responses are drawn from the same distribution, where a score below 0.05 is considered significant.", "labels": [], "entities": []}, {"text": "Models were trained on an Intel Core2Duo 3GHz with 4GB of RAM.", "labels": [], "entities": []}, {"text": "The evaluation was performed on a dual quad-core Intel Xeon 2.27GHz with 16GB of RAM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for sentences in the supertagger  training data. Sentences containing more than 250  tokens were not included in our data sets.", "labels": [], "entities": []}, {"text": " Table 2: Speed improvements on newswire, using various amounts of parser-annotated NANC data.", "labels": [], "entities": [{"text": "Speed", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9712607264518738}, {"text": "NANC data", "start_pos": 84, "end_pos": 93, "type": "DATASET", "confidence": 0.8961462676525116}]}, {"text": " Table 3: Breakdown of the source of changes in speed. The test sentences are divided into nine sets  based on the change in parsing behaviour between the baseline model and a model trained using MIRA,  Sections 02-21 of CCGbank and 4,000,000 NANC sentences.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 196, "end_pos": 200, "type": "DATASET", "confidence": 0.5241701602935791}, {"text": "CCGbank", "start_pos": 221, "end_pos": 228, "type": "DATASET", "confidence": 0.9374427199363708}]}, {"text": " Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9396343231201172}, {"text": "NANC data", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.9078740477561951}]}, {"text": " Table 5: Cross-corpus speed improvement, models trained with MIRA and 4,000,000 sentences. The  highlighted values are the top speed for each evaluation set and results that are statistically indistinguish- able from it.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.4997042417526245}]}, {"text": " Table 6: Evaluation of top models on Section 23 of  CCGbank. All changes in F-score are statistically  significant.", "labels": [], "entities": [{"text": "Section 23 of  CCGbank", "start_pos": 38, "end_pos": 60, "type": "DATASET", "confidence": 0.8716669827699661}, {"text": "F-score", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.997636079788208}]}, {"text": " Table 7: Cross-corpus accuracy optimisation, models trained with GIS and 400,000 sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.958552360534668}, {"text": "GIS", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.696607768535614}]}, {"text": " Table 8: Comparison of annotation methods for  extra data. The multi-taggers used \u03b2 values 0.075  and 0.001, and dictionary cutoffs 20 and 150, for  taggers a and b respectively.", "labels": [], "entities": []}, {"text": " Table 9: Cross-corpus speed for the baseline  model on data sets balanced on sentence length.", "labels": [], "entities": []}, {"text": " Table 10: Performance comparison for models us- ing extra gold standard biomedical data. Models  were trained with GIS and 4,000,000 extra sen- tences, and are tested using a POS-tagger trained  on biomedical data.", "labels": [], "entities": [{"text": "GIS", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9661173820495605}]}]}