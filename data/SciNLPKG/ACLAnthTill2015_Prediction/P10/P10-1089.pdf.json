{"title": [{"text": "Creating Robust Supervised Classifiers via Web-Scale N-gram Data", "labels": [], "entities": [{"text": "Web-Scale N-gram Data", "start_pos": 43, "end_pos": 64, "type": "DATASET", "confidence": 0.7225245038668314}]}], "abstractContent": [{"text": "In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classifiers.", "labels": [], "entities": []}, {"text": "We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus.", "labels": [], "entities": []}, {"text": "We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering , spelling correction, noun compound bracketing, and verb part-of-speech dis-ambiguation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9994471669197083}, {"text": "adjective ordering", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7349693775177002}, {"text": "spelling correction", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.8418129980564117}, {"text": "noun compound bracketing", "start_pos": 155, "end_pos": 179, "type": "TASK", "confidence": 0.7076897025108337}, {"text": "verb part-of-speech dis-ambiguation", "start_pos": 185, "end_pos": 220, "type": "TASK", "confidence": 0.6022246678670248}]}, {"text": "More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many NLP systems use web-scale N-gram counts.", "labels": [], "entities": []}, {"text": "demonstrate good performance on eight tasks using unsupervised web-based models.", "labels": [], "entities": []}, {"text": "They show web counts are superior to counts from a large corpus.", "labels": [], "entities": []}, {"text": "propose unsupervised and supervised systems that use counts from Google's N-gram corpus).", "labels": [], "entities": []}, {"text": "Web-based models perform particularly well on generation tasks, where systems choose between competing sequences of output text (such as different spellings), as opposed to analysis tasks, where systems choose between abstract labels (such as part-of-speech tags or parse trees).", "labels": [], "entities": []}, {"text": "In this work, we address two natural and related questions which these previous studies leave open: 1.", "labels": [], "entities": []}, {"text": "Is there a benefit in combining web-scale counts with the features used in state-of-theart supervised approaches?", "labels": [], "entities": []}, {"text": "2. How well do web-based models perform on new domains or when labeled data is scarce?", "labels": [], "entities": []}, {"text": "We address these questions on two generation and two analysis tasks, using both existing N-gram data and a novel web-scale N-gram corpus that includes part-of-speech information (Section 2).", "labels": [], "entities": []}, {"text": "While previous work has combined web-scale features with other features in specific classification problems (), we provide a multi-task, multi-domain comparison.", "labels": [], "entities": []}, {"text": "Some may question why supervised approaches are needed at all for generation problems.", "labels": [], "entities": []}, {"text": "Why not solely rely on direct evidence from a giant corpus?", "labels": [], "entities": []}, {"text": "For example, for the task of prenominal adjective ordering (Section 3), a system that needs to describe a ball that is both big and red can simply check that big red is more common on the web than red big, and order the adjectives accordingly.", "labels": [], "entities": [{"text": "prenominal adjective ordering", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.605498323837916}]}, {"text": "It is, however, suboptimal to only use N-gram data.", "labels": [], "entities": []}, {"text": "For example, ordering adjectives by direct web evidence performs 7% worse than our best supervised system (Section 3.2).", "labels": [], "entities": [{"text": "ordering adjectives by direct web evidence", "start_pos": 13, "end_pos": 55, "type": "TASK", "confidence": 0.8214563926060995}]}, {"text": "No matter how large the web becomes, there will always be plausible constructions that never occur.", "labels": [], "entities": []}, {"text": "For example, there are currently no pages indexed by Google with the preferred adjective ordering for bedraggled 56-year-old.", "labels": [], "entities": []}, {"text": "Also, in a particular domain, words may have a non-standard usage.", "labels": [], "entities": []}, {"text": "Systems trained on labeled data can learn the domain usage and leverage other regularities, such as suffixes and transitivity for adjective ordering.", "labels": [], "entities": []}, {"text": "With these benefits, systems trained on labeled data have become the dominant technology in academic NLP.", "labels": [], "entities": []}, {"text": "There is a growing recognition, however, that these systems are highly domain dependent.", "labels": [], "entities": []}, {"text": "For example, parsers trained on annotated newspaper text perform poorly on other genres.", "labels": [], "entities": []}, {"text": "While many approaches have adapted NLP systems to specific domains (, these techniques assume the system knows on which domain it is being used, and that it has access to representative data in that domain.", "labels": [], "entities": []}, {"text": "These assumptions are unrealistic in many real-world situations; for example, when automatically processing a heterogeneous collection of web pages.", "labels": [], "entities": []}, {"text": "How well do supervised and unsupervised NLP systems perform when used uncustomized, out-of-the-box on new domains, and how can we best design our systems for robust open-domain performance?", "labels": [], "entities": []}, {"text": "Our results show that using web-scale N-gram data in supervised systems advances the state-ofthe-art performance on standard analysis and generation tasks.", "labels": [], "entities": []}, {"text": "More importantly, when operating out-of-domain, or when labeled data is not plentiful, using web-scale N-gram data not only helps achieve good performance -it is essential.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the benefit of N-gram data on multiclass classification problems.", "labels": [], "entities": [{"text": "multiclass classification", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.7575600743293762}]}, {"text": "For each task, we have some labeled data indicating the correct output for each example.", "labels": [], "entities": []}, {"text": "We evaluate with accuracy: the percentage of examples correctly classified in test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9996817111968994}]}, {"text": "We use one in-domain and two out-ofdomain test sets for each task.", "labels": [], "entities": []}, {"text": "Statistical significance is assessed with McNemar's test, p<0.01.", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.7511217594146729}, {"text": "McNemar's test", "start_pos": 42, "end_pos": 56, "type": "METRIC", "confidence": 0.7233493328094482}]}, {"text": "We provide results for unsupervised approaches and the majority-class baseline for each task.", "labels": [], "entities": []}, {"text": "For our supervised approaches, we represent the examples as feature vectors, and learn a classifier on the training vectors.", "labels": [], "entities": []}, {"text": "There are two feature classes: features that use N-grams (N-GM) and those that do not (LEX).", "labels": [], "entities": [{"text": "LEX", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9738444089889526}]}, {"text": "N-GM features are real-valued features giving the log-count of a particular N-gram in the auxiliary web corpus.", "labels": [], "entities": []}, {"text": "LEX features are binary features that indicate the presence or absence of a particular string at a given position in the input.", "labels": [], "entities": []}, {"text": "The name LEX emphasizes that they identify specific lexical items.", "labels": [], "entities": []}, {"text": "The instantiations of both types of features depend on the task and are described in the corresponding sections.", "labels": [], "entities": []}, {"text": "Each classifier is a linear Support Vector Machine (SVM), trained using LIBLINEAR) on the standard domain.", "labels": [], "entities": []}, {"text": "We use the one-vsall strategy when there are more than two classes (in Section 4).", "labels": [], "entities": []}, {"text": "We plot learning curves to measure the accuracy of the classifier when the number of labeled training examples varies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9991570711135864}]}, {"text": "The size of the N-gram data and its counts remain constant.", "labels": [], "entities": []}, {"text": "We always optimize the SVM's (L2) regularization parameter on the in-domain development set.", "labels": [], "entities": []}, {"text": "We present results with L2-SVM, but achieve similar results with L1-SVM and logistic regression.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Adjective ordering accuracy (%). SVM  and Malouf (2000) trained on BNC, tested on  BNC (IN), Gutenberg (O1), and Medline (O2).", "labels": [], "entities": [{"text": "Adjective ordering", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7727514207363129}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9589160084724426}]}, {"text": " Table 2: Spelling correction accuracy (%). SVM  trained on NYT, tested on NYT (IN) and out-of- domain Gutenberg (O1) and Medline (O2).", "labels": [], "entities": [{"text": "Spelling correction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8677628934383392}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9730385541915894}, {"text": "NYT", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.9356766939163208}, {"text": "NYT", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.9229104518890381}]}, {"text": " Table 3: NC-bracketing accuracy (%). SVM  trained on WSJ, tested on WSJ (IN) and out-of- domain Grolier (O1) and Medline (O2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9313219785690308}, {"text": "WSJ", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.9396860003471375}, {"text": "WSJ", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.8680280447006226}, {"text": "Medline", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.9423871040344238}]}, {"text": " Table 4: Verb-POS-disambiguation accuracy (%)  trained on WSJ, tested on WSJ (IN) and out-of- domain Brown (O1) and Medline (O2).", "labels": [], "entities": [{"text": "Verb-POS-disambiguation", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8968474864959717}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.8530713319778442}, {"text": "WSJ", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.8858266472816467}, {"text": "WSJ", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.8990680575370789}, {"text": "Medline", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9487805366516113}]}]}