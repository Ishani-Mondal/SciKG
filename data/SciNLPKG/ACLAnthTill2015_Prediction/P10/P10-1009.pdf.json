{"title": [{"text": "A Risk Minimization Framework for Extractive Speech Summarization", "labels": [], "entities": [{"text": "Risk Minimization", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.6922760009765625}, {"text": "Extractive Speech Summarization", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.6693795919418335}]}], "abstractContent": [{"text": "In this paper, we formulate extractive summarization as a risk minimization problem and propose a unified probabilis-tic framework that naturally combines supervised and unsupervised summarization models to inherit their individual merits as well as to overcome their inherent limitations.", "labels": [], "entities": [{"text": "summarization", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.6392867565155029}]}, {"text": "In addition, the introduction of various loss functions also provides the sum-marization framework with a flexible but systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively.", "labels": [], "entities": []}, {"text": "Experiments on speech summarization show that the methods deduced from our framework are very competitive with existing summa-rization approaches.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.6822235882282257}]}], "introductionContent": [{"text": "Automated summarization systems which enable user to quickly digest the important information conveyed by either a single or a cluster of documents are indispensible for managing the rapidly growing amount of textual information and multimedia content).", "labels": [], "entities": []}, {"text": "On the other hand, due to the maturity of text summarization, the research paradigm has been extended to speech summarization over the years ().", "labels": [], "entities": [{"text": "text summarization", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.71582892537117}, {"text": "speech summarization", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.7208406329154968}]}, {"text": "Speech summarization is expected to distill important information and remove redundant and incorrect information caused by recognition errors from spoken documents, enabling user to efficiently review spoken documents and understand the associated topics quickly.", "labels": [], "entities": [{"text": "Speech summarization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6903942227363586}]}, {"text": "It would also be useful for improving the efficiency of a number of potential applications like retrieval and mining of large volumes of spoken documents.", "labels": [], "entities": [{"text": "mining of large volumes of spoken documents", "start_pos": 110, "end_pos": 153, "type": "TASK", "confidence": 0.7765832373074123}]}, {"text": "A summary can be either abstractive or extractive.", "labels": [], "entities": []}, {"text": "In abstractive summarization, a fluent and concise abstract that reflects the key concepts of a document is generated, whereas in extractive summarization, the summary is usually formed by selecting salient sentences from the original document).", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.5392878949642181}]}, {"text": "The former requires highly sophisticated natural language processing techniques, including semantic representation and inference, as well as natural language generation, while this would make abstractive approaches difficult to replicate or extend from constrained domains to more general domains.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 141, "end_pos": 168, "type": "TASK", "confidence": 0.7083369294802347}]}, {"text": "In addition to being extractive or abstractive, a summary may also be generated by considering several other aspects like being generic or query-oriented summarization, singledocument or multi-document summarization, and so forth.", "labels": [], "entities": []}, {"text": "The readers may refer to) fora comprehensive overview of automatic text summarization.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.5801781912644705}]}, {"text": "In this paper, we focus exclusively on generic, singledocument extractive summarization which forms the building block for many other summarization tasks.", "labels": [], "entities": [{"text": "singledocument extractive summarization", "start_pos": 48, "end_pos": 87, "type": "TASK", "confidence": 0.4968371589978536}, {"text": "summarization", "start_pos": 134, "end_pos": 147, "type": "TASK", "confidence": 0.9780300855636597}]}, {"text": "Aside from traditional ad-hoc extractive summarization methods), machine-learning approaches with either supervised or unsupervised learning strategies have gained much attention and been applied with empirical success to many summarization tasks ().", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 227, "end_pos": 246, "type": "TASK", "confidence": 0.9222924411296844}]}, {"text": "For supervised learning strategies, the summarization task is usually cast as a two-class (summary and nonsummary) sentence-classification problem: A sentence with a set of indicative features is input to the classifier (or summarizer) and a decision is then returned from it on the basis of these features.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.9163758158683777}]}, {"text": "In general, they usually require a training set, comprised of several documents and their corresponding handcrafted summaries (or labeled data), to train the classifiers.", "labels": [], "entities": []}, {"text": "However, manual labeling is expensive in terms of time and personnel.", "labels": [], "entities": [{"text": "manual labeling", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.5545421838760376}]}, {"text": "The other potential problem is the socalled \"bag-of-sentences\" assumption implicitly made by most of these summarizers.", "labels": [], "entities": []}, {"text": "That is, sentences are classified independently of each other, without leveraging the dependence relationships among the sentences or the global structure of the document.", "labels": [], "entities": []}, {"text": "Another line of thought attempts to conduct document summarization using unsupervised machine-learning approaches, getting around the need for manually labeled training data.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.5223327875137329}]}, {"text": "Most previous studies conducted along this line have their roots in the concept of sentence centrality).", "labels": [], "entities": [{"text": "sentence centrality", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7140976041555405}]}, {"text": "Put simply, sentences more similar to others are deemed more salient to the main theme of the document; such sentences thus will be selected as part of the summary.", "labels": [], "entities": []}, {"text": "Even though the performance of unsupervised summarizers is usually worse than that of supervised summarizers, their domain-independent and easy-to-implement properties still make them attractive.", "labels": [], "entities": []}, {"text": "Building on these observations, we expect that researches conducted along the above-mentioned two directions could complement each other, and it might be possible to inherit their individual merits to overcome their inherent limitations.", "labels": [], "entities": []}, {"text": "In this paper, we present a probabilistic summarization framework stemming from Bayes decision theory for speech summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9719662070274353}, {"text": "speech summarization", "start_pos": 106, "end_pos": 126, "type": "TASK", "confidence": 0.6577100604772568}]}, {"text": "This framework cannot only naturally integrate the above-mentioned two modeling paradigms but also provide a flexible yet systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively.", "labels": [], "entities": []}, {"text": "Moreover, we also illustrate how the proposed framework can unify several existing summarization models.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "We start by reviewing related work on extractive summarization.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.810094952583313}]}, {"text": "In Section 3 we formulate the extractive summarization task as a risk minimization problem, followed by a detailed elucidation of the proposed methods in Section 4.", "labels": [], "entities": [{"text": "extractive summarization task", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.8724489013353983}, {"text": "risk minimization", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.777855783700943}]}, {"text": "Then, the experimental setup and a series of experiments and associated discussions are presented in Sections 5 and 6, respectively.", "labels": [], "entities": []}, {"text": "Finally, Section 7 concludes our presentation and discusses avenues for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the assessment of summarization performance, we adopted the widely used ROUGE measure) because of its higher correlation with human judgments.", "labels": [], "entities": [{"text": "summarization", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9682465195655823}, {"text": "ROUGE measure", "start_pos": 76, "end_pos": 89, "type": "METRIC", "confidence": 0.9870083630084991}]}, {"text": "It evaluates the quality of the summarization by counting the number of overlapping units, such as N-grams, longest common subsequences or skip-bigram, between the automatic summary and a set of reference summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9801682233810425}]}, {"text": "Three variants of the ROGUE measure were used to quantify the utility of the proposed method.", "labels": [], "entities": [{"text": "ROGUE measure", "start_pos": 22, "end_pos": 35, "type": "METRIC", "confidence": 0.9667698740959167}]}, {"text": "They are, respectively, the ROUGE-1 (unigram) measure, the ROUGE-2 (bigram) measure and the ROUGE-L (longest common subsequence) measure).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.8430169820785522}, {"text": "ROUGE-2 (bigram) measure", "start_pos": 59, "end_pos": 83, "type": "METRIC", "confidence": 0.7731645226478576}, {"text": "ROUGE-L", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.8275150060653687}]}, {"text": "The summarization ratio, defined as the ratio of the number of words in the automatic (or manual) summary to that in the reference transcript of a spoken document, was set to 10% in this research.", "labels": [], "entities": [{"text": "summarization ratio", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.8634423017501831}]}, {"text": "Since increasing the summary length tends to increase the chance of getting higher scores in the recall rate of the various ROUGE measures and might not always select the right number of informative words in the automatic summary as compared to the reference summary, all the experimental results reported hereafter are obtained by calculating the F-scores of these ROUGE measures, respectively).", "labels": [], "entities": [{"text": "recall rate", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9877130389213562}, {"text": "F-scores", "start_pos": 348, "end_pos": 356, "type": "METRIC", "confidence": 0.9942095279693604}]}, {"text": "Table 1 shows the levels of agreement (the Kappa statistic and ROUGE measures) between the three subjects for important sentence ranking.", "labels": [], "entities": [{"text": "agreement", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.955405592918396}, {"text": "ROUGE", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9948467016220093}]}, {"text": "They seem to reflect the fact that people may not always agree with each other in selecting the important sentences for representing a given document.", "labels": [], "entities": []}, {"text": "In the first set of experiments, we evaluate the baseline performance of the LM and BC summarizers (cf. Sections 4.1 and 4.2), respectively.", "labels": [], "entities": [{"text": "BC", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.8538921475410461}]}, {"text": "The corresponding results are detailed in,   where the values in the parentheses are the associated 95% confidence intervals.", "labels": [], "entities": []}, {"text": "It is also worth mentioning that TD denotes the summarization results obtained based on manual transcripts of the spoken documents while SD denotes the results using the speech recognition transcripts which may contain speech recognition errors and sentence boundary detection errors.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 249, "end_pos": 276, "type": "TASK", "confidence": 0.5959028700987498}]}, {"text": "In this research, sentence boundaries were determined by speech pauses.", "labels": [], "entities": []}, {"text": "For the TD case, the acoustic features were obtained by aligning the manual transcripts to their spoken documents counterpart by performing word-level forced alignment.", "labels": [], "entities": [{"text": "word-level forced alignment", "start_pos": 140, "end_pos": 167, "type": "TASK", "confidence": 0.6489319900671641}]}, {"text": "Furthermore, the ROGUE measures, in essence, are evaluated by counting the number of overlapping units between the automatic summary and the reference summary; the corresponding evaluation results, therefore, would be severely affected by speech recognition errors when applying the various ROUGE measures to quantify the performance of speech summarization.", "labels": [], "entities": [{"text": "ROGUE", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.9118668437004089}, {"text": "speech summarization", "start_pos": 337, "end_pos": 357, "type": "TASK", "confidence": 0.6193510144948959}]}, {"text": "In order to get rid of the cofounding effect of this factor, it is assumed that the selected summary sentences can also be presented in speech form (besides text form) such that users can directly listen to the audio segments of the summary sentences to bypass the problem caused by speech recognition errors.", "labels": [], "entities": []}, {"text": "Consequently, we can align the ASR transcripts of the summary sentences to their respective audio segments to obtain the correct (manual) transcripts for the summarization performance evaluation (i.e., for the SD case).", "labels": [], "entities": [{"text": "summarization", "start_pos": 158, "end_pos": 171, "type": "TASK", "confidence": 0.9712271094322205}]}, {"text": "Observing we notice two particularities.", "labels": [], "entities": []}, {"text": "First, there are significant performance gaps between summarization using the manual transcripts and the erroneous speech recognition transcripts.", "labels": [], "entities": [{"text": "summarization", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.9831221699714661}]}, {"text": "The relative performance degradations are about 15%, 34% and 23%, respectively, for ROUGE-1, ROUGE2 and ROUGE-L measures.", "labels": [], "entities": []}, {"text": "One possible explanation is that the erroneous speech recognition transcripts of spoken sentences would probably carry wrong information and thus deviate somewhat from representing the true theme of the spoken document.", "labels": [], "entities": [{"text": "speech recognition transcripts of spoken sentences", "start_pos": 47, "end_pos": 97, "type": "TASK", "confidence": 0.812027762333552}]}, {"text": "Second, the supervised summarizer (i.e., BC) outperforms the unsupervised summarizer (i.e., LM).", "labels": [], "entities": [{"text": "BC", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9969350099563599}]}, {"text": "The better performance of BC can be further explained by two reasons.", "labels": [], "entities": [{"text": "BC", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.5418917536735535}]}, {"text": "One is that BC is trained with the handcrafted document-summary sentence labels in the development set while LM is instead conducted in a purely unsupervised manner.", "labels": [], "entities": [{"text": "BC", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.5207546353340149}]}, {"text": "Another is that BC utilizes a rich set of features to characterize a given spoken sentence while LM is constructed solely on the basis of the lexical (unigram) information.", "labels": [], "entities": []}, {"text": "We then turn our attention to investigate the utility of several methods deduced from our proposed summarization framework.", "labels": [], "entities": [{"text": "summarization", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.9815054535865784}]}, {"text": "We first consider the case when a 0-1 loss function is used), which just show a simple combination of BC and LM.", "labels": [], "entities": [{"text": "BC", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9364420175552368}]}, {"text": "As can be seen from the first row of, such a combination can give about 4% to 5% absolute improvements as compared to the results of BC illustrated in.", "labels": [], "entities": [{"text": "absolute", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9762195944786072}, {"text": "BC", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.9817055463790894}]}, {"text": "It in some sense confirms the feasibility of combining the supervised and unsupervised summarizers.", "labels": [], "entities": []}, {"text": "Moreover, we consider the use of the loss functions defined in (11) (denoted by SIM) and (12) (denoted by MMR), and the corresponding results are shown in the second and the third rows of: The results achieved by several methods derived from the proposed summarization framework.", "labels": [], "entities": [{"text": "summarization", "start_pos": 255, "end_pos": 268, "type": "TASK", "confidence": 0.9624107480049133}]}, {"text": "MMR delivers higher summarization performance than SIM (especially for the SD case), which in turn verifies the merit of incorporating the MMR concept into the proposed framework for extractive summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.9807621836662292}, {"text": "extractive summarization", "start_pos": 183, "end_pos": 207, "type": "TASK", "confidence": 0.6454826295375824}]}, {"text": "If we further compare the results achieved by MMR with those of BC and LM as shown in, we can find significant improvements both for the TD and SD cases.", "labels": [], "entities": [{"text": "MMR", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.949324905872345}]}, {"text": "By and large, for the TD case, the proposed summarization method offers relative performance improvements of about 19%, 23% and 19%, respectively, in the ROUGE-1, ROUGE-2 and ROUGE-L measures as compared to the BC baseline; while the relative improvements are 29%, 46% and 31%, respectively, in the same measurements for the SD case.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.7782570123672485}, {"text": "BC baseline", "start_pos": 211, "end_pos": 222, "type": "DATASET", "confidence": 0.7476509213447571}]}, {"text": "On the other hand, the performance gap between the TD and SD cases are reduced to a good extent by using the proposed summarization framework.", "labels": [], "entities": []}, {"text": "In the next set of experiments, we simply assume the sentence prior probability \uf028 \uf029 j S P defined in (8) is uniformly distributed, namely, we do not use any supervised information cue but use the lexical information only.", "labels": [], "entities": []}, {"text": "The importance of a given sentence is thus considered from two angles: 1) the relationship between a sentence and the whole document, and 2) the relationship between the sentence and the other individual sentences.", "labels": [], "entities": []}, {"text": "The corresponding results are illustrated in the lower part of (denoted by Uniform).", "labels": [], "entities": []}, {"text": "We can see that the additional consideration of the sentence-sentence relationship appears to be beneficial as compared to that only considering the document-sentence relevance information (cf. the second row of).", "labels": [], "entities": []}, {"text": "It also gives competitive results as compared to the performance of BC (cf. the first row of for the SD case.", "labels": [], "entities": [{"text": "BC", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9642977118492126}]}], "tableCaptions": [{"text": " Table 1: The agreement among the subjects for impor- tant sentence ranking for the evaluation set.", "labels": [], "entities": []}, {"text": " Table 3: The results achieved by the BC and LM summarizers, respectively.", "labels": [], "entities": [{"text": "BC", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9755168557167053}]}, {"text": " Table 4: The results achieved by several methods derived from the proposed summarization framework.", "labels": [], "entities": [{"text": "summarization", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.9739407300949097}]}, {"text": " Table 5. It  should be noted that the LEAD-based method  simply extracts the first few sentences in a doc- ument as the summary. To our surprise, CRF  does not provide superior results as compared to  the other summarization methods. One possible  explanation is that the structural evidence of the  spoken documents in the test set is not strong  enough for CRF to show its advantage of model- ing the local structural information among sen- tences. On the other hand, LexRank gives a very", "labels": [], "entities": [{"text": "LexRank", "start_pos": 471, "end_pos": 478, "type": "DATASET", "confidence": 0.9759292602539062}]}]}