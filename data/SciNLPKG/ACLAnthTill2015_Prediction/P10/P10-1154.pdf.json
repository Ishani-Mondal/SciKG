{"title": [{"text": "Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems", "labels": [], "entities": [{"text": "Word Sense Disambiguation Rivaling Supervised", "start_pos": 15, "end_pos": 60, "type": "TASK", "confidence": 0.7171664834022522}]}], "abstractContent": [{"text": "One of the main obstacles to high-performance Word Sense Disambigua-tion (WSD) is the knowledge acquisition bottleneck.", "labels": [], "entities": [{"text": "Word Sense Disambigua-tion (WSD)", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.7417445282141367}, {"text": "knowledge acquisition", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.777306467294693}]}, {"text": "In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource , namely Wikipedia.", "labels": [], "entities": []}, {"text": "We show that, when provided with avast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge lies at the core of Word Sense Disambiguation (WSD), the task of computationally identifying the meanings of words in context).", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.7836902042229971}]}, {"text": "In the recent years, two main approaches have been studied that rely on a fixed sense inventory, i.e., supervised and knowledgebased methods.", "labels": [], "entities": []}, {"text": "In order to achieve high performance, supervised approaches require large training sets where instances (target words in context) are hand-annotated with the most appropriate word senses.", "labels": [], "entities": []}, {"text": "Producing this kind of knowledge is extremely costly: at a throughput of one sense annotation per minute and tagging one thousand examples per word, dozens of person-years would be required for enabling a supervised classifier to disambiguate all the words in the English lexicon with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 290, "end_pos": 298, "type": "METRIC", "confidence": 0.9782400727272034}]}, {"text": "In contrast, knowledge-based approaches exploit the information contained in wide-coverage lexical resources, such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 118, "end_pos": 125, "type": "DATASET", "confidence": 0.9681805968284607}]}, {"text": "However, it has been demonstrated that the amount of lexical and semantic information contained in such resources is typically insufficient for high-performance WSD (Cuadros and).", "labels": [], "entities": []}, {"text": "Several methods have been proposed to automatically extend existing resources (cf. Section 2) and it has been shown that highlyinterconnected semantic networks have a great impact on WSD (.", "labels": [], "entities": [{"text": "WSD", "start_pos": 183, "end_pos": 186, "type": "TASK", "confidence": 0.7567332983016968}]}, {"text": "However, to date, the real potential of knowledge-rich WSD systems has been shown only in the presence of either a large manually-developed extension of WordNet () or sophisticated WSD algorithms ( . The contributions of this paper are two-fold.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.963215172290802}]}, {"text": "First, we relieve the knowledge acquisition bottleneck by developing a methodology to extend WordNet with millions of semantic relations.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.8035763800144196}]}, {"text": "The relations are harvested from an encyclopedic resource, namely Wikipedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.9553034901618958}]}, {"text": "Wikipedia pages are automatically associated with WordNet senses, and topical, semantic associative relations from Wikipedia are transferred to WordNet, thus producing a much richer lexical resource.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 144, "end_pos": 151, "type": "DATASET", "confidence": 0.9367457628250122}]}, {"text": "Second, two simple knowledge-based algorithms that exploit our extended WordNet are applied to standard WSD datasets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9570477604866028}, {"text": "WSD datasets", "start_pos": 104, "end_pos": 116, "type": "DATASET", "confidence": 0.7655995190143585}]}, {"text": "The results show that the integration of vast amounts of semantic relations in knowledge-based systems yields performance competitive with state-of-the-art supervised approaches on open-text WSD.", "labels": [], "entities": []}, {"text": "In addition, we support previous findings from  that in a domain-specific WSD scenario knowledge-based systems perform better than supervised ones, and we show that, given enough knowledge, simple algorithms perform better than more sophisticated ones.", "labels": [], "entities": [{"text": "WSD", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9205593466758728}]}], "datasetContent": [{"text": "We perform two sets of experiments: we first evaluate the intrinsic quality of our mapping (Section 4.1) and then quantify the impact of WordNet++ for coarse-grained (Section 4.2) and domainspecific WSD (Section 4.3).", "labels": [], "entities": []}, {"text": "We first conducted an evaluation of the mapping quality.", "labels": [], "entities": []}, {"text": "To create a gold standard for evaluation, we started from the set of all lemmas contained both in WordNet and Wikipedia: the intersection between the two resources includes 80,295 lemmas which correspond to 105,797 WordNet senses and 199,735 Wikipedia pages.", "labels": [], "entities": []}, {"text": "The average polysemy is 1.3 and 2.5 for WordNet senses and Wikipages, respectively (2.8 and 4.7 when excluding monosemous words).", "labels": [], "entities": [{"text": "polysemy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9805691242218018}]}, {"text": "We selected a random sample of 1,000 Wikipages and asked an annotator with previous experience in lexicographic annotation to provide the correct WordNet sense for each page title (an empty sense label was given if no correct mapping was possible).", "labels": [], "entities": []}, {"text": "505 non-empty mappings were found, i.e. Wikipedia pages with a corresponding WordNet sense.", "labels": [], "entities": [{"text": "WordNet sense", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.9208901226520538}]}, {"text": "In order to quantify the quality of the annotations and the difficulty of the task, a second annotator sense tagged a subset of 200 pages from the original sample.", "labels": [], "entities": []}, {"text": "We computed the inter-annotator agreement using the kappa coefficient and found out that our annotators achieved an agreement coefficient \u03ba of 0.9, indicating almost perfect agreement.", "labels": [], "entities": [{"text": "agreement coefficient \u03ba", "start_pos": 116, "end_pos": 139, "type": "METRIC", "confidence": 0.9002195795377096}]}, {"text": "summarizes the performance of our disambiguation algorithm against the manually annotated dataset.", "labels": [], "entities": []}, {"text": "Evaluation is performed in terms of standard measures of precision (the ratio of correct sense labels to the non-empty labels output by the mapping algorithm), recall (the ratio of correct sense labels to the total of non-empty labels in the gold standard) and F 1 -measure ( 2P RP +R ).", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9989610910415649}, {"text": "recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9996273517608643}, {"text": "F 1 -measure ( 2P RP +R", "start_pos": 261, "end_pos": 284, "type": "METRIC", "confidence": 0.7878669069872962}]}, {"text": "We also calculate accuracy, which accounts for Results and discussion.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9995550513267517}]}, {"text": "The results show that our method improves on the baseline by a large margin and that higher performance can be achieved by using more disambiguation information.", "labels": [], "entities": []}, {"text": "That is, using a richer disambiguation context helps to better choose the most appropriate WordNet sense fora Wikipedia page.", "labels": [], "entities": []}, {"text": "The combination of structural and gloss information attains a slight variation in terms of precision (\u22120.3% and +0.8% compared to Structure and Gloss respectively), but a significantly high increase in recall (+9.4% and +13.3%).", "labels": [], "entities": [{"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9994885921478271}, {"text": "recall", "start_pos": 202, "end_pos": 208, "type": "METRIC", "confidence": 0.9997991919517517}]}, {"text": "This implies that the different disambiguation contexts only partially overlap and, when used separately, each produces different mappings with a similar level of precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9964917302131653}]}, {"text": "In the joint approach, the harmonic mean of precision and recall, i.e. F 1 , is in fact 5 and 8 points higher than when separately using structural and gloss information, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9962502121925354}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9985448122024536}, {"text": "F 1", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9943612515926361}]}, {"text": "As for the baselines, the most frequent sense is just 0.6% and 0.4% above the random baseline in terms of F 1 and accuracy, respectively.", "labels": [], "entities": [{"text": "F 1", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9956746399402618}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9980231523513794}]}, {"text": "A \u03c7 2 test reveals in fact no statistically significant difference at p < 0.05.", "labels": [], "entities": []}, {"text": "This is related to the random distribution of senses in our dataset and the Wikipedia unbiased coverage of WordNet senses.", "labels": [], "entities": []}, {"text": "So select- We leave out the evaluation of different contexts fora Wikipage for the sake of brevity.", "labels": [], "entities": []}, {"text": "During prototyping we found that the best results were given by using the largest context available, as reported in ing the most frequent sense rather than any other sense for each target page represents a choice as arbitrary as picking a sense at random.", "labels": [], "entities": []}, {"text": "The final mapping contains 81,533 pairs of Wikipages and word senses they map to, covering 55.7% of the noun senses in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9616522789001465}]}, {"text": "Using our best performing mapping we are able to extend WordNet with 1,902,859 semantic edges: of these, 97.93% are deemed novel, i.e. no direct edge could previously be found between the synsets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.9397027492523193}]}, {"text": "In addition, we performed a stricter evaluation of the novelty of our relations by checking whether these can still be found indirectly by searching fora connecting path between the two synsets of interest.", "labels": [], "entities": []}, {"text": "Here we found that 91.3%, 87.2% and 78.9% of the relations are novel to WordNet when performing a graph search of maximum depth of 2, 3 and 4, respectively.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9524255394935608}]}], "tableCaptions": [{"text": " Table 1: Performance of the mapping algorithm.", "labels": [], "entities": []}, {"text": " Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).", "labels": [], "entities": [{"text": "Semeval-2007 coarse- grained all-words WSD", "start_pos": 25, "end_pos": 67, "type": "TASK", "confidence": 0.4385913784305255}]}, {"text": " Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.", "labels": [], "entities": []}, {"text": " Table 4: Performance on the Sports and Finance  sections of the dataset from Koeling et al. (2005):   \u2020 indicates results from Agirre et al. (2009).", "labels": [], "entities": []}]}