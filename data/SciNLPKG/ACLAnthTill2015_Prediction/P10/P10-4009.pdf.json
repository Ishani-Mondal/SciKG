{"title": [{"text": "Personalising speech-to-speech translation in the EMIME project", "labels": [], "entities": [{"text": "Personalising speech-to-speech translation", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.9140607515970866}]}], "abstractContent": [{"text": "In the EMIME project we have studied un-supervised cross-lingual speaker adaptation.", "labels": [], "entities": [{"text": "cross-lingual speaker adaptation", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.6272546052932739}]}, {"text": "We have employed an HMM statistical framework for both speech recognition and synthesis which provides transformation mechanisms to adapt the synthesized voice in TTS (text-to-speech) using the recognized voice in ASR (automatic speech recognition).", "labels": [], "entities": [{"text": "speech recognition and synthesis", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.7244279384613037}, {"text": "ASR (automatic speech recognition", "start_pos": 214, "end_pos": 247, "type": "TASK", "confidence": 0.5721854448318482}]}, {"text": "An important application for this research is personalised speech-to-speech translation that will use the voice of the speaker in the input language to utter the translated sentences in the output language.", "labels": [], "entities": [{"text": "personalised speech-to-speech translation", "start_pos": 46, "end_pos": 87, "type": "TASK", "confidence": 0.6561017135779063}]}, {"text": "In mobile environments this enhances the users' interaction across language barriers by making the output speech sound more like the original speaker's way of speaking, even if she or he could not speak the output language.", "labels": [], "entities": []}], "introductionContent": [{"text": "A mobile real-time speech-to-speech translation (S2ST) device is one of the grand challenges in natural language processing (NLP).", "labels": [], "entities": [{"text": "speech-to-speech translation (S2ST)", "start_pos": 19, "end_pos": 54, "type": "TASK", "confidence": 0.800058126449585}, {"text": "natural language processing (NLP)", "start_pos": 96, "end_pos": 129, "type": "TASK", "confidence": 0.7790964245796204}]}, {"text": "It involves several important NLP research areas: automatic speech recognition (ASR), statistical machine translation (SMT) and speech synthesis, also known as text-to-speech (TTS).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.8052977025508881}, {"text": "statistical machine translation (SMT)", "start_pos": 86, "end_pos": 123, "type": "TASK", "confidence": 0.8214179476102194}, {"text": "speech synthesis", "start_pos": 128, "end_pos": 144, "type": "TASK", "confidence": 0.7542803585529327}]}, {"text": "In recent years significant advance have also been made in relevant technological devices: the size of powerful computers has decreased to fit in a mobile phone and fast WiFi and 3G networks have spread widely to connect them to even more powerful computation servers.", "labels": [], "entities": []}, {"text": "Several hand-held S2ST applications and devices have already become available, for example by IBM, Google or Jibbigo 1 , but there are still serious limitations in vocabulary and language selection and performance.", "labels": [], "entities": []}, {"text": "When an S2ST device is used in practical human interaction across a language barrier, one feature that is often missed is the personalization of the output voice.", "labels": [], "entities": []}, {"text": "Whoever speaks to the device in whatever manner, the output voice always sounds the same.", "labels": [], "entities": []}, {"text": "Producing high-quality synthesis voices is expensive and even if the system had many output voices, it is hard to select one that would sound like the input voice.", "labels": [], "entities": []}, {"text": "There are many features in the output voice that could raise the interaction experience to a much more natural level, for example, emotions, speaking rate, loudness and the speaker identity.", "labels": [], "entities": []}, {"text": "After the recent development in hidden Markov model (HMM) based TTS, it has become possible to adapt the output voice using model transformations that can be estimated from a small number of speech samples.", "labels": [], "entities": []}, {"text": "These techniques, for instance the maximum likelihood linear regression (MLLR), are adopted from HMM-based ASR where they are very powerful in fast adaptation of speaker and recording environment characteristics.", "labels": [], "entities": [{"text": "HMM-based ASR", "start_pos": 97, "end_pos": 110, "type": "TASK", "confidence": 0.45108485221862793}]}, {"text": "Using hierarchical regression trees, the TTS and ASR models can further be coupled in away that enables unsupervised TTS adaptation ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.6167361736297607}, {"text": "TTS adaptation", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.9069680273532867}]}, {"text": "In unsupervised adaptation samples are annotated by applying ASR.", "labels": [], "entities": []}, {"text": "By eliminating the need for human intervention it becomes possible to perform voice adaptation for TTS in almost real-time.", "labels": [], "entities": [{"text": "voice adaptation", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.7632418870925903}, {"text": "TTS", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.7634756565093994}]}, {"text": "The target in the EMIME project 2 is to study unsupervised cross-lingual speaker adaptation for S2ST systems.", "labels": [], "entities": [{"text": "cross-lingual speaker adaptation", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.6044234732786814}]}, {"text": "The first results of the project have been, for example, to bridge the gap between the ASR and TTS (, to improve the baseline ASR () and SMT (de) systems for morphologically rich languages, and to develop robust TTS ().", "labels": [], "entities": []}, {"text": "The next step has been preliminary experiments in intra-lingual and cross-lingual speaker adaptation ().", "labels": [], "entities": [{"text": "cross-lingual speaker adaptation", "start_pos": 68, "end_pos": 100, "type": "TASK", "confidence": 0.603649745384852}]}, {"text": "For cross-lingual adaptation several new methods have been proposed for mapping the HMM states, adaptation data and model transformations (.", "labels": [], "entities": [{"text": "cross-lingual adaptation", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.7944252789020538}]}, {"text": "In this presentation we can demonstrate the various new results in ASR, SMT and TTS.", "labels": [], "entities": [{"text": "ASR", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9243413209915161}, {"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9842990636825562}, {"text": "TTS", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.6408065557479858}]}, {"text": "Even though the project is still ongoing, we have an initial version of mobile S2ST system and crosslingual speaker adaptation to show.", "labels": [], "entities": [{"text": "crosslingual speaker adaptation", "start_pos": 95, "end_pos": 126, "type": "TASK", "confidence": 0.6068727572758993}]}], "datasetContent": [], "tableCaptions": []}