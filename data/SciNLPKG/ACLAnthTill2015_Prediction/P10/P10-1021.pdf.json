{"title": [{"text": "Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure", "labels": [], "entities": []}], "abstractContent": [{"text": "The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load.", "labels": [], "entities": []}, {"text": "There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated.", "labels": [], "entities": []}, {"text": "Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors.", "labels": [], "entities": []}, {"text": "In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Psycholinguists have long realized that language comprehension is highly incremental, with readers and listeners continuously extracting the meaning of utterances on a word-by-word basis.", "labels": [], "entities": []}, {"text": "As soon as they encounter a word in a sentence, they integrate it as fully as possible into a representation of the sentence thus far.", "labels": [], "entities": []}, {"text": "Recent research suggests that language comprehension can also be highly predictive, i.e., comprehenders are able to anticipate upcoming linguistic material.", "labels": [], "entities": []}, {"text": "This is beneficial as it gives them more time to keep up with the input, and predictions can be used to compensate for problems with noise or ambiguity.", "labels": [], "entities": []}, {"text": "Two types of prediction have been observed in the literature.", "labels": [], "entities": []}, {"text": "The first type is semantic prediction, as evidenced in semantic priming: a word that is preceded by a semantically related prime or a semantically congruous sentence fragment is processed faster).", "labels": [], "entities": [{"text": "semantic prediction", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7962465882301331}]}, {"text": "Another example is argument prediction: listeners are able to launch eye-movements to the predicted argument of a verb before having encountered it, e.g., they will fixate an edible object as soon as they hear the word eat (.", "labels": [], "entities": [{"text": "argument prediction", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.8613835573196411}]}, {"text": "The second type of prediction is syntactic prediction.", "labels": [], "entities": [{"text": "syntactic prediction", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.7538699209690094}]}, {"text": "Comprehenders are faster at naming words that are syntactically compatible with prior context, even when they bear no semantic relationship to the context (.", "labels": [], "entities": []}, {"text": "Another instance of syntactic prediction has been reported by: following the word either, readers predictor and the complement that follows it, and process it faster compared to a control condition without either.", "labels": [], "entities": [{"text": "syntactic prediction", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.8441480696201324}]}, {"text": "Thus, human language processing takes advantage of the constraints imposed by the preceding semantic and syntactic context to derive expectations about the upcoming input.", "labels": [], "entities": []}, {"text": "Much recent work has focused on developing computational measures of these constraints and expectations.", "labels": [], "entities": []}, {"text": "Again, the literature is split into syntactic and semantic models.", "labels": [], "entities": []}, {"text": "Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar.", "labels": [], "entities": [{"text": "surprisal", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9886482357978821}]}, {"text": "Modeling work on semantic constraint focuses on the degree to which a word is related to its preceding context.", "labels": [], "entities": []}, {"text": "use Latent Semantic Analysis (LSA, to assess the degree of contextual constraint exerted on a word by its context.", "labels": [], "entities": []}, {"text": "In this framework, word meanings are represented as vectors in a high dimensional space and distance in this space is interpreted as an index of processing difficulty.", "labels": [], "entities": []}, {"text": "Other work) models contextual constraint in information theoretic terms.", "labels": [], "entities": []}, {"text": "The assumption is that words carry prior semantic expectations which are updated upon seeing the next word.", "labels": [], "entities": []}, {"text": "Expectations are represented by a vector of probabilities which reflects the likely location in semantic space of the upcoming word.", "labels": [], "entities": []}, {"text": "The measures discussed above are typically computed automatically on real-language corpora using data-driven methods and their predictions are verified through analysis of eye-movements that people make while reading.", "labels": [], "entities": []}, {"text": "Ample evidence) demonstrates that eye-movements are related to the moment-to-moment cognitive activities of readers.", "labels": [], "entities": []}, {"text": "They also provide an accurate temporal record of the on-line processing of natural language, and through the analysis of eyemovement measurements (e.g., the amount of time spent looking at a word) can give insight into the processing difficulty involved in reading.", "labels": [], "entities": []}, {"text": "In this paper, we investigate a model of prediction that is incremental and takes into account syntactic as well as semantic constraint.", "labels": [], "entities": []}, {"text": "The model essentially integrates the predictions of an incremental parser together with those of a semantic space model.", "labels": [], "entities": []}, {"text": "The latter creates meaning representations compositionally, and therefore builds semantic expectations for word sequences (e.g., phrases, sentences, even documents) rather than isolated words.", "labels": [], "entities": []}, {"text": "Some existing models of sentence processing integrate semantic information into a probabilistic parser); however, the semantic component of these models is limited to semantic role information, rather than attempting to build a full semantic representation fora sentence.", "labels": [], "entities": []}, {"text": "Furthermore, the models of and do not explicitly model prediction, but rather focus on accounting for garden path effects.", "labels": [], "entities": []}, {"text": "The proposed model simultaneously captures semantic and syntactic effects in a single measure which we empirically show is predictive of processing difficulty as manifested in eyemovements.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Coefficients of the baseline LME model  for total reading time", "labels": [], "entities": []}, {"text": " Table 5: Intercorrelations between model factors", "labels": [], "entities": []}]}