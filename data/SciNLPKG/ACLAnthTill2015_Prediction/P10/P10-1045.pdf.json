{"title": [{"text": "Latent variable models of selectional preference", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the application of so-called topic models to selectional preference induction.", "labels": [], "entities": [{"text": "selectional preference induction", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.8249828219413757}]}, {"text": "Three models related to Latent Dirichlet Allocation, a proven method for modelling document-word co-occurrences, are presented and evaluated on datasets of human plausibility judgements.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.5903285642464956}]}, {"text": "Compared to previously proposed techniques, these models perform very competitively, especially for infrequent predicate-argument combinations where they exceed the quality of Web-scale predictions while using relatively little data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language researchers have long been aware that many words place semantic restrictions on the words with which they can co-occur in a syntactic relationship.", "labels": [], "entities": []}, {"text": "Violations of these restrictions make the sense of a sentence odd or implausible: (1) Colourless green ideas sleep furiously.", "labels": [], "entities": []}, {"text": "(2) The deer shot the hunter.", "labels": [], "entities": []}, {"text": "Recognising whether or not a selectional restriction is satisfied can bean important trigger for metaphorical interpretations and also plays a role in the time course of human sentence processing (.", "labels": [], "entities": [{"text": "metaphorical interpretations", "start_pos": 97, "end_pos": 125, "type": "TASK", "confidence": 0.9190041720867157}]}, {"text": "A more relaxed notion of selectional preference captures the idea that certain classes of entities are more likely than others to fill a given argument slot of a predicate.", "labels": [], "entities": []}, {"text": "In Natural Language Processing, knowledge about probable, less probable and wholly infelicitous predicateargument pairs is of value for numerous applications, for example semantic role labelling ().", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 171, "end_pos": 194, "type": "TASK", "confidence": 0.6638000905513763}]}, {"text": "The notion of selectional preference is not restricted to surface-level predicates such as verbs and modifiers, but also extends to semantic frames and inference rules (.", "labels": [], "entities": []}, {"text": "The fundamental problem that selectional preference models must address is data sparsity: in many cases insufficient corpus data is available to reliably measure the plausibility of a predicate-argument pair by counting its observed frequency.", "labels": [], "entities": []}, {"text": "A rarely seen pair maybe fundamentally implausible (a carrot laughed) or plausible but rarely expressed (a manservant laughed).", "labels": [], "entities": []}, {"text": "In general, it is beneficial to smooth plausibility estimates by integrating knowledge about the frequency of other, similar predicate-argument pairs.", "labels": [], "entities": []}, {"text": "The task thus share some of the nature of language modelling; however, it is a task less amenable to approaches that require very large training corpora and one where the semantic quality of a model is of greater importance.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7152263224124908}]}, {"text": "This paper takes up tools (\"topic models\") that have been proven successful in modelling document-word co-occurrences and adapts them to the task of selectional preference learning.", "labels": [], "entities": [{"text": "selectional preference learning", "start_pos": 149, "end_pos": 180, "type": "TASK", "confidence": 0.7178618113199869}]}, {"text": "Advantages of these models include a well-defined generative model that handles sparse data well, the ability to jointly induce semantic classes and predicate-specific distributions over those classes, and the enhanced statistical strength achieved by sharing knowledge across predicates.", "labels": [], "entities": []}, {"text": "Section 2 surveys prior work on selectional preference modelling and on semantic applications of topic models.", "labels": [], "entities": [{"text": "selectional preference modelling", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.7931824723879496}]}, {"text": "Section 3 describes the models used in our experiments.", "labels": [], "entities": []}, {"text": "Section 4 provides details of the experimental design.", "labels": [], "entities": []}, {"text": "Section 5 presents results for our models on the task of predicting human plausibility judgements for predicate-argument combinations; we show that performance is generally competi-tive with or superior to a number of other models, including models using Web-scale resources, especially for low-frequency examples.", "labels": [], "entities": [{"text": "predicting human plausibility judgements for predicate-argument combinations", "start_pos": 57, "end_pos": 133, "type": "TASK", "confidence": 0.794516955103193}]}, {"text": "In Section 6 we wrap up by summarising the paper's conclusions and sketching directions for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the document modelling literature, probabilistic topic models are often evaluated on the likelihood they assign to unseen documents; however, it has been shown that higher log likelihood scores do not necessarily correlate with more semantically coherent induced topics ().", "labels": [], "entities": [{"text": "document modelling", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7097927778959274}]}, {"text": "One popular method for evaluating selectional preference models is by testing the correlation between their predictions and human judgements of plausibility on a dataset of predicate-argument pairs.", "labels": [], "entities": []}, {"text": "This can be viewed as a more semantically relevant measurement of model quality than likelihood-based methods, and also permits comparison with nonprobabilistic models.", "labels": [], "entities": []}, {"text": "In Section 5, we use two plausibility datasets to evaluate our models and compare to other previously published results.", "labels": [], "entities": []}, {"text": "We trained our models on the 90-million word written component of the British National Corpus, parsed with the RASP toolkit ().", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 70, "end_pos": 93, "type": "DATASET", "confidence": 0.930574893951416}, {"text": "RASP toolkit", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.9133365154266357}]}, {"text": "Predicates occurring with just one argument type were removed, as were all tokens containing non-alphabetic characters; no other filtering was done.", "labels": [], "entities": []}, {"text": "The resulting datasets consisted of 3,587,172 verb-object observations with 7,954 predicate types and 80,107 argument types, 3,732,470 noun-noun observations with 68,303 predicate types and 105,425 argument types, and 3,843,346 adjective-noun observations with 29,975 predicate types and 62,595 argument types.", "labels": [], "entities": []}, {"text": "During development we used the verb-noun plausibility dataset from to direct the design of the system.", "labels": [], "entities": []}, {"text": "Unless stated otherwise, all results are based on runs of 1,000 iterations with 100 classes, with a 200-iteration burnin period after which hyperparameters were reestimated every 50 iterations.", "labels": [], "entities": []}, {"text": "The probabilities estimated by the models (P (n|v, r) for LDA and P (n, v|r) for ROOTH-and DUAL-LDA) were sampled every 50 iterations post-burnin and averaged over three runs to smooth out variance.", "labels": [], "entities": [{"text": "ROOTH-and DUAL-LDA", "start_pos": 81, "end_pos": 99, "type": "METRIC", "confidence": 0.683794379234314}]}, {"text": "To compare plausibility scores for different predicates, we require the joint probability P (n, v|r); as LDA does not provide this, we approximate P LDA (n, v|r) = P BN C (v|r)P LDA (n|v, r), where P BN C (v|r) is proportional to the frequency with which predicate v is observed as an instance of relation r in the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 315, "end_pos": 318, "type": "DATASET", "confidence": 0.9424902200698853}]}, {"text": "For comparison, we reimplemented the methods of and.", "labels": [], "entities": []}, {"text": "As mentioned above, Rooth et al. use a latent-variable model similar to (4) but without priors, trained via EM.", "labels": [], "entities": []}, {"text": "Our implementation (henceforth ROOTH-EM) chooses the number of classes from the range (20, 25, . .", "labels": [], "entities": [{"text": "ROOTH-EM", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.7939245700836182}]}, {"text": ", 50) through 5-fold cross-validation on a held-out log-likelihood measure.", "labels": [], "entities": []}, {"text": "Settings outside this range did not give good results.", "labels": [], "entities": []}, {"text": "Again, we run for 1,000 iterations and average predictions over LDA 0 Nouns: agreement, contract, permission, treaty, deal, . .", "labels": [], "entities": []}, {"text": "1 Nouns information, datum, detail, evidence, material, . .", "labels": [], "entities": [{"text": "detail", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9894853234291077}]}, {"text": "., is a non-probabilistic method that smooths predicate-argument counts with counts for other observed arguments of the same predicate, weighted by the similarity between arguments.", "labels": [], "entities": []}, {"text": "Following their description, we use a 2,000-dimensional space of syntactic co-occurrence features appropriate to the relation being predicted, weight features with the G 2 transformation and compute similarity with the cosine measure.", "labels": [], "entities": []}, {"text": "shows sample semantic classes induced by models trained on the corpus of BNC verb-object co-occurrences.", "labels": [], "entities": []}, {"text": "LDA clusters nouns only, while ROOTH-LDA and ROOTH-EM learn classes that generate both nouns and verbs and DUAL-LDA clusters nouns and verbs separately.", "labels": [], "entities": [{"text": "ROOTH-LDA", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9186760187149048}]}, {"text": "The LDA clusters are generally sensible: class 0 is exemplified by agreement and contract and class 1 by information and datum.", "labels": [], "entities": []}, {"text": "There are some unintuitive blips, for example country appears between knowledge and understanding in class 2.", "labels": [], "entities": []}, {"text": "The ROOTH-LDA classes also feel right: class 0 deals with nouns such as force, team and army which one might join, armor lead and class 1 corresponds to \"things that can be opened or closed\" such as a door, an eye or a mouth (though the model also makes the questionable prediction that all these items can plausibly be locked or slammed).", "labels": [], "entities": [{"text": "ROOTH-LDA", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9472285509109497}]}, {"text": "The DUAL-LDA classes are notably less coherent, especially when it comes to clustering verbs: DUAL-LDA's class 0V, like ROOTH-LDA's class 0, has verbs that take groups as objects but its class 1V mixes sensible conflations (turn, round) with very common verbs such as see and have and the unrelated break.", "labels": [], "entities": []}, {"text": "The general impression given by inspection of the DUAL-LDA model is that it has problems with mixing and does not manage to learn a good model; we have tried a number of solutions (e.g., blocked sampling of argument and predicate classes), without overcoming this brittleness.", "labels": [], "entities": []}, {"text": "Unsurprisingly, ROOTH-EM's classes have a similar feel to ROOTH-LDA; our general impression is that some of ROOTH-EM's classes look even more coherent than the LDAbased models, presumably because it does not use priors to smooth its per-class distributions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Most probable words for sample semantic classes induced from verb-object observations", "labels": [], "entities": []}, {"text": " Table 2: Results (Pearson r and Spearman \u03c1 correlations) on Keller and Lapata's (2003) plausibility data", "labels": [], "entities": [{"text": "Pearson r and Spearman \u03c1 correlations", "start_pos": 19, "end_pos": 56, "type": "METRIC", "confidence": 0.7974559168020884}]}]}