{"title": [], "abstractContent": [{"text": "The core-adjunct argument distinction is a basic one in the theory of argument structure.", "labels": [], "entities": []}, {"text": "The task of distinguishing between the two has strong relations to various basic NLP tasks such as syntactic parsing, semantic role labeling and subcategoriza-tion acquisition.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.736924558877945}, {"text": "semantic role labeling", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.6724201242129008}, {"text": "subcategoriza-tion acquisition", "start_pos": 145, "end_pos": 175, "type": "TASK", "confidence": 0.7479453980922699}]}, {"text": "This paper presents a novel unsupervised algorithm for the task that uses no supervised models, utilizing instead state-of-the-art syntactic induction algorithms.", "labels": [], "entities": []}, {"text": "This is the first work to tackle this task in a fully unsupervised scenario.", "labels": [], "entities": []}], "introductionContent": [{"text": "The distinction between core arguments (henceforth, cores) and adjuncts is included inmost theories on argument structure).", "labels": [], "entities": []}, {"text": "The distinction can be viewed syntactically, as one between obligatory and optional arguments, or semantically, as one between arguments whose meanings are predicate dependent and independent.", "labels": [], "entities": []}, {"text": "The latter (cores) are those whose function in the described event is to a large extent determined by the predicate, and are obligatory.", "labels": [], "entities": []}, {"text": "Adjuncts are optional arguments which, like adverbs, modify the meaning of the described event in a predictable or predicate-independent manner.", "labels": [], "entities": []}, {"text": "Consider the following examples: The marked argument is a core in 1 and an adjunct in 2 and 3.", "labels": [], "entities": []}, {"text": "Adjuncts form an independent semantic unit and their semantic role can often be inferred independently of the predicate (e.g., is usually a temporal modifier).", "labels": [], "entities": []}, {"text": "Core roles are more predicate-specific, e.g., [on his colleague] has a different meaning with the verbs 'operate' and 'count'.", "labels": [], "entities": [{"text": "count", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.9623880386352539}]}, {"text": "Sometimes the same argument plays a different role in different sentences.", "labels": [], "entities": []}, {"text": "In (3), [in the park] places a well-defined situation (Yuri playing football) in a certain location.", "labels": [], "entities": []}, {"text": "However, in \"The troops are based [in the park]\", the same argument is obligatory, since being based requires a place to be based in.", "labels": [], "entities": []}, {"text": "Distinguishing between the two argument types has been discussed extensively in various formulations in the NLP literature, notably in PP attachment, semantic role labeling (SRL) and subcategorization acquisition.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 135, "end_pos": 148, "type": "TASK", "confidence": 0.9338301420211792}, {"text": "semantic role labeling (SRL)", "start_pos": 150, "end_pos": 178, "type": "TASK", "confidence": 0.777312825123469}, {"text": "subcategorization acquisition", "start_pos": 183, "end_pos": 212, "type": "TASK", "confidence": 0.8468277454376221}]}, {"text": "However, no work has tackled it yet in a fully unsupervised scenario.", "labels": [], "entities": []}, {"text": "Unsupervised models reduce reliance on the costly and error prone manual multi-layer annotation (POS tagging, parsing, core-adjunct tagging) commonly used for this task.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.6350816786289215}]}, {"text": "They also allow to examine the nature of the distinction and to what extent it is accounted for in real data in a theory-independent manner.", "labels": [], "entities": []}, {"text": "In this paper we present a fully unsupervised algorithm for core-adjunct classification.", "labels": [], "entities": [{"text": "core-adjunct classification", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.7404899895191193}]}, {"text": "We utilize leading fully unsupervised grammar induction and POS induction algorithms.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.6477545946836472}]}, {"text": "We focus on prepositional arguments, since non-prepositional ones are generally cores.", "labels": [], "entities": []}, {"text": "The algorithm uses three measures based on different characterizations of the core-adjunct distinction, and combines them using an ensemble method followed by self-training.", "labels": [], "entities": []}, {"text": "The measures used are based on selectional preference, predicate-slot collocation and argument-slot collocation.", "labels": [], "entities": []}, {"text": "We evaluate against PropBank (), obtaining roughly 70% accuracy when evaluated on the prepositional arguments and more than 80% for the entire argument set.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.8299255967140198}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9994016885757446}]}, {"text": "These results are substantially better than those obtained by a non-trivial baseline.", "labels": [], "entities": []}, {"text": "Section 2 discusses the core-adjunct distinction.", "labels": [], "entities": []}, {"text": "Section 3 describes the algorithm.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 present our experimental setup and results.", "labels": [], "entities": []}, {"text": "FrameNet (FN) () is a large-scale lexicon based on frame semantics.", "labels": [], "entities": [{"text": "FrameNet (FN)", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.6472801268100739}]}, {"text": "It takes a different approach from PB to semantic roles.", "labels": [], "entities": []}, {"text": "Like PB, it distinguishes between core and non-core arguments, but it does so for each and every frame separately.", "labels": [], "entities": []}, {"text": "It does not commit that a semantic role is consistently tagged as a core or a non-core across frames.", "labels": [], "entities": []}, {"text": "For example, the semantic role 'path' is considered core in the 'Self Motion' frame, but as non-core in the 'Placing' frame.", "labels": [], "entities": []}, {"text": "Another difference is that FN does not allow any type of non-core argument to attach to a given frame.", "labels": [], "entities": [{"text": "FN", "start_pos": 27, "end_pos": 29, "type": "DATASET", "confidence": 0.6772059202194214}]}, {"text": "For instance, while the 'Getting' frame allows a 'Duration' non-core argument, the 'Active Perception' frame does not.", "labels": [], "entities": []}, {"text": "PB and FN tend to agree in clear (prototypical) cases, but to differ in others.", "labels": [], "entities": [{"text": "PB", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8477634191513062}, {"text": "FN", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.8489634394645691}]}, {"text": "For instance, both schemes would tag \"Yuri played football [in the park]\" as an adjunct and \"The commander placed a guard [in the park]\" as a core.", "labels": [], "entities": []}, {"text": "However, in \"He walked [into his office]\", the marked argument is tagged as a directional adjunct in PB but as a 'Direction' core in FN.", "labels": [], "entities": [{"text": "FN", "start_pos": 133, "end_pos": 135, "type": "DATASET", "confidence": 0.8739118576049805}]}, {"text": "Under both schemes, non-cores are usually confined to a few specific semantic domains, notably time, place and manner, in contrast to cores that are not restricted in their scope of applicability.", "labels": [], "entities": []}, {"text": "This approach is quite common, e.g., the COBUILD English grammar) categorizes adjuncts to be of manner, aspect, opinion, place, time, frequency, duration, degree, extent, emphasis, focus and probability.", "labels": [], "entities": [{"text": "COBUILD English grammar", "start_pos": 41, "end_pos": 64, "type": "DATASET", "confidence": 0.9377925594647726}, {"text": "probability", "start_pos": 191, "end_pos": 202, "type": "METRIC", "confidence": 0.9754855632781982}]}, {"text": "Work in SRL does not tackle the core-adjunct task separately but as part of general argument classification.", "labels": [], "entities": [{"text": "SRL", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9321941137313843}, {"text": "general argument classification", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.6223224997520447}]}, {"text": "Supervised approaches obtain an almost perfect score in distinguishing between the two in an in-domain scenario.", "labels": [], "entities": []}, {"text": "For instance, the confusion matrix in () indicates that their model scores 99.5% accuracy on this task.", "labels": [], "entities": [{"text": "confusion matrix", "start_pos": 18, "end_pos": 34, "type": "METRIC", "confidence": 0.9517130553722382}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9990512728691101}]}, {"text": "However, adaptation results are lower, with the best two models in the) achieving 95.3%) and 95.6% () accuracy in an adaptation between the relatively similar corpora WSJ and Brown.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.999471127986908}, {"text": "WSJ", "start_pos": 167, "end_pos": 170, "type": "DATASET", "confidence": 0.8283999562263489}]}, {"text": "Despite the high performance in supervised scenarios, tackling the task in an unsupervised manner is not easy.", "labels": [], "entities": []}, {"text": "The success of supervised methods stems from the fact that the predicate-slot combination (slot is represented in this paper by its preposition) strongly determines whether a given argument is an adjunct or a core (see Section 3.4).", "labels": [], "entities": []}, {"text": "Supervised models are provided with an annotated corpus from which they can easily learn the mapping between predicate-slot pairs and their core/adjunct label.", "labels": [], "entities": []}, {"text": "However, induction of the mapping in an unsupervised manner must be based on inherent core-adjunct properties.", "labels": [], "entities": []}, {"text": "In addition, supervised models utilize supervised parsers and POS taggers, while the current state-of-the-art in unsupervised parsing and POS tagging is considerably worse than their supervised counterparts.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.6944719403982162}, {"text": "POS tagging", "start_pos": 138, "end_pos": 149, "type": "TASK", "confidence": 0.7789221107959747}]}, {"text": "This challenge has some resemblance to un-supervised detection of multiword expressions (MWEs).", "labels": [], "entities": [{"text": "un-supervised detection of multiword expressions (MWEs)", "start_pos": 39, "end_pos": 94, "type": "TASK", "confidence": 0.7142195887863636}]}, {"text": "An important MWE sub-class is that of phrasal verbs, which are also characterized by verb-preposition pairs () (see also).", "labels": [], "entities": []}, {"text": "Both tasks aim to determine semantic compositionality, which is a highly challenging task.", "labels": [], "entities": [{"text": "determine semantic compositionality", "start_pos": 18, "end_pos": 53, "type": "TASK", "confidence": 0.6159380078315735}]}, {"text": "Few works addressed unsupervised SRL-related tasks.", "labels": [], "entities": [{"text": "SRL-related tasks", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9203097522258759}]}, {"text": "The setup of), who presented a Bayesian Network model for argument classification, is perhaps closest to ours.", "labels": [], "entities": [{"text": "argument classification", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.8014312386512756}]}, {"text": "Their work relied on a supervised parser and a rule-based argument identification (both during training and testing)., while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet () verb lexicon, in addition to supervised parses.", "labels": [], "entities": [{"text": "rule-based argument identification", "start_pos": 47, "end_pos": 81, "type": "TASK", "confidence": 0.7011492053667704}]}, {"text": "Finally, tackled the argument identification task alone and did not perform argument classification of any sort.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.8863157331943512}, {"text": "argument classification", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.743121474981308}]}, {"text": "PP attachment is the task of determining whether a prepositional phrase which immediately follows a noun phrase attaches to the latter or to the preceding verb.", "labels": [], "entities": [{"text": "PP attachment is the task of determining whether a prepositional phrase which immediately follows a noun phrase attaches to the latter or to the preceding verb", "start_pos": 0, "end_pos": 159, "type": "Description", "confidence": 0.7862695994285437}]}, {"text": "This task's relation to the core-adjunct distinction was addressed in several works.", "labels": [], "entities": []}, {"text": "For instance, the results of indicate that their PP attachment system works better for cores than for adjuncts.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.8022577464580536}]}, {"text": "suggest a system that jointly tackles the PP attachment and the core-adjunct distinction tasks.", "labels": [], "entities": []}, {"text": "Unlike in this work, their classifier requires extensive supervision including WordNet, language-specific features and a supervised parser.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.9385741949081421}]}, {"text": "Their features are generally motivated by common linguistic considerations.", "labels": [], "entities": []}, {"text": "Features found adaptable to a completely unsupervised scenario are used in this work as well.", "labels": [], "entities": []}, {"text": "The core-adjunct distinction is included in many syntactic annotation schemes.", "labels": [], "entities": []}, {"text": "Although the Penn Treebank does not explicitly annotate adjuncts and cores, a few works suggested mapping its annotation (including function tags) to core-adjunct labels.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.9945906400680542}]}, {"text": "Such a mapping was presented in).", "labels": [], "entities": []}, {"text": "In his Model 2, Collins modifies his parser to provide a coreadjunct prediction, thereby improving its performance.", "labels": [], "entities": []}, {"text": "The Combinatory Categorial Grammar (CCG) formulation models the core-adjunct distinction explicitly.", "labels": [], "entities": []}, {"text": "Therefore, any CCG parser can be used as a core-adjunct classifier.", "labels": [], "entities": []}, {"text": "This task specifies for each predicate the number, type and order of obligatory arguments.", "labels": [], "entities": []}, {"text": "Determining the allowable subcategorization frames fora given predicate necessarily involves separating its cores from its allowable adjuncts (which are not framed).", "labels": [], "entities": []}, {"text": "Notable works in the field include).", "labels": [], "entities": []}, {"text": "All these works used a parsed corpus in order to collect, for each predicate, a set of hypothesized subcategorization frames, to be filtered by hypothesis testing methods.", "labels": [], "entities": []}, {"text": "This line of work differs from ours in a few aspects.", "labels": [], "entities": []}, {"text": "First, all works use manual or supervised syntactic annotations, usually including a POS tagger.", "labels": [], "entities": []}, {"text": "Second, the common approach to the task focuses on syntax and tries to identify the entire frame, rather than to tag each argument separately.", "labels": [], "entities": []}, {"text": "Finally, most works address the task at the verb type level, trying to detect the allowable frames for each type.", "labels": [], "entities": []}, {"text": "Consequently, the common evaluation focuses on the quality of the allowable frames acquired for each verb type, and not on the classification of specific arguments in a given corpus.", "labels": [], "entities": []}, {"text": "Such a token level evaluation was conducted in a few works), but often with a small number of verbs or a small number of frames.", "labels": [], "entities": []}, {"text": "A discussion of the differences between type and token level evaluation can be found in ( ).", "labels": [], "entities": []}, {"text": "The core-adjunct distinction task was tackled in the context of child language acquisition.", "labels": [], "entities": [{"text": "core-adjunct distinction", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.7009139209985733}, {"text": "child language acquisition", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.5977742870648702}]}, {"text": "Villavicencio (2002) developed a classifier based on preposition selection and frequency information for modeling the distinction for locative prepositional phrases.", "labels": [], "entities": []}, {"text": "Her approach is not entirely corpus based, as it assumes the input sentences are given in a basic logical form.", "labels": [], "entities": []}, {"text": "The study of prepositions is a vibrant research area in NLP.", "labels": [], "entities": []}, {"text": "A special issue of Computational Linguistics, which includes an extensive survey of related work, was recently devoted to the field (Baldwin et al., 2009).", "labels": [], "entities": [{"text": "Computational Linguistics", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.814658135175705}]}], "datasetContent": [{"text": "Experiments were conducted in two scenarios.", "labels": [], "entities": []}, {"text": "In the 'SID' (supervised identification of prepositions and verbs) scenario, a gold standard list of prepositions was provided.", "labels": [], "entities": [{"text": "SID' (supervised identification of prepositions and verbs)", "start_pos": 8, "end_pos": 66, "type": "TASK", "confidence": 0.855763179063797}]}, {"text": "The list was generated by taking every word tagged by the preposition tag ('IN') in at least one of its instances under the gold standard annotation of the WSJ sections 2-21.", "labels": [], "entities": [{"text": "IN", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.8560723662376404}, {"text": "WSJ sections 2-21", "start_pos": 156, "end_pos": 173, "type": "DATASET", "confidence": 0.8963509599367777}]}, {"text": "Verbs were identified using MXPOST.", "labels": [], "entities": [{"text": "MXPOST", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.8137025237083435}]}, {"text": "Words tagged with any of the verb tags, except of the auxiliary verbs ('have', 'be' and 'do') were considered predicates.", "labels": [], "entities": []}, {"text": "This scenario decouples the accuracy of the algorithm from the quality of the unsupervised POS tagging.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9995784163475037}, {"text": "POS tagging", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.5864705294370651}]}, {"text": "In the 'Fully Unsupervised' scenario, prepositions and verbs were identified using Clark's tagger.", "labels": [], "entities": []}, {"text": "It was asked to produce a tagging into 34 classes.", "labels": [], "entities": []}, {"text": "The classes corresponding to prepositions and to verbs were manually identified.", "labels": [], "entities": []}, {"text": "Prepositions in the test set were detected with 84.2% precision and 91.6% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9989858269691467}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9966624975204468}]}, {"text": "The prediction of whether a word belongs to an open class or a closed was based on the output of the Prototype tagger ( ).", "labels": [], "entities": []}, {"text": "The Prototype tagger provided significantly more ac-curate predictions in this context than Clark's.", "labels": [], "entities": [{"text": "Prototype tagger", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.6755759119987488}]}, {"text": "The 39832 sentences of PropBank's sections 2-21 were used as a test set without bounding their lengths . Cores were defined to be any argument bearing the labels 'A0' -'A5', 'C-A0' -'C-A5' or 'R-A0' -'R-A5'.", "labels": [], "entities": [{"text": "PropBank's sections 2-21", "start_pos": 23, "end_pos": 47, "type": "DATASET", "confidence": 0.9290346652269363}]}, {"text": "Adjuncts were defined to be arguments bearing the labels 'AM', 'C-AM' or 'R-AM'.", "labels": [], "entities": []}, {"text": "Modals ('AM-MOD') and negation modifiers ('AM-NEG') were omitted since they do not represent adjuncts.", "labels": [], "entities": []}, {"text": "The test set includes 213473 arguments, 45939 (21.5%) are prepositional.", "labels": [], "entities": []}, {"text": "Of the latter, 22442 (48.9%) are cores and 23497 (51.1%) are adjuncts.", "labels": [], "entities": []}, {"text": "The non-prepositional arguments include 145767 (87%) cores and 21767 (13%) adjuncts.", "labels": [], "entities": []}, {"text": "The average number of words per argument is 5.1.", "labels": [], "entities": []}, {"text": "The NANC corpus was used as a training set.", "labels": [], "entities": [{"text": "NANC corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9726494252681732}]}, {"text": "Only sentences of length not greater than 10 excluding punctuation were used (see Section 3.2), totaling 4955181 sentences.", "labels": [], "entities": []}, {"text": "7673878 (5635810) arguments were identified in the 'SID' ('Fully Unsupervised') scenario.", "labels": [], "entities": []}, {"text": "The average number of words per argument is 1.6 (1.7).", "labels": [], "entities": []}, {"text": "Since this is the first work to tackle this task using neither manual nor supervised syntactic annotation, there is no previous work to compare to.", "labels": [], "entities": []}, {"text": "However, we do compare against a non-trivial baseline, which closely follows the rationale of cores as obligatory arguments.", "labels": [], "entities": []}, {"text": "Our Window Baseline tags a corpus using MX-POST and computes, for each predicate and preposition, the ratio between the number of times that the preposition appeared in a window of W words after the verb and the total number of times that the verb appeared.", "labels": [], "entities": []}, {"text": "If this number exceeds a certain threshold \u03b2, all arguments having that predicate and preposition are tagged as cores.", "labels": [], "entities": []}, {"text": "Otherwise, they are tagged as adjuncts.", "labels": [], "entities": []}, {"text": "We used 18.7M sentences from NANC of unbounded length for this baseline.", "labels": [], "entities": [{"text": "NANC", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.9252098798751831}]}, {"text": "W and \u03b2 were fine-tuned against the test set 9 . We also report results for partial versions of the algorithm, starting with the three measures used (selectional preference, predicate-slot collocation and argument-slot collocation).", "labels": [], "entities": []}, {"text": "Results for the ensemble classifier (prior to the bootstrapping stage) are presented in two variants: one in which the ensemble is used to tag arguments for which all three measures give a prediction (the 'Ensemble(Intersection)' classifier) and one in which the ensemble tags all arguments for which at least one classifier gives a prediction (the 'Ensemble(Union)' classifier).", "labels": [], "entities": []}, {"text": "For the latter, a tie is broken in favor of the core label.", "labels": [], "entities": []}, {"text": "The 'Ensemble(Union)' classifier is not apart of our model and is evaluated only as a reference.", "labels": [], "entities": []}, {"text": "In order to provide a broader perspective on the task, we compare the measures in the basis of our algorithm to simplified or alternative measures.", "labels": [], "entities": []}, {"text": "We experiment with the following measures: 1.", "labels": [], "entities": []}, {"text": "Simple SP -a selectional preference measure defined to be P r(head|slot, predicate).", "labels": [], "entities": []}, {"text": "2. Vast Corpus SP -similar to 'Simple SP' but with a much larger corpus.", "labels": [], "entities": [{"text": "Vast Corpus SP", "start_pos": 3, "end_pos": 17, "type": "TASK", "confidence": 0.589195321003596}]}, {"text": "It uses roughly 100M arguments which were extracted from the web-crawling based corpus of () and the British National Corpus).", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 101, "end_pos": 124, "type": "DATASET", "confidence": 0.9116417368253072}]}, {"text": "3. Thesaurus SP -a selectional preference measure which follows the paradigm of (Erk, 2007) (Section 3.3) and defines the similarity between two heads to be the Jaccard affinity between their two entries in Lin's automatically compiled thesaurus . 4. Pr(slot|predicate) -an alternative to the used predicate-slot collocation measure.", "labels": [], "entities": [{"text": "Pr", "start_pos": 251, "end_pos": 253, "type": "METRIC", "confidence": 0.9543135762214661}]}, {"text": "5. PMI(slot, head) -an alternative to the used argument-slot collocation measure.", "labels": [], "entities": [{"text": "PMI", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.9535073637962341}]}, {"text": "6. Head Dependence -the entropy of the predicate distribution given the slot and the head (following)): HD(s, h) = \u2212\u03a3 p P r(p|s, h) \u00b7 log(P r(p|s, h)) Low entropy implies a core.", "labels": [], "entities": []}, {"text": "For each of the scenarios and the algorithms, we report accuracy, coverage and effective accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9996800422668457}, {"text": "coverage", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9970393180847168}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9600514769554138}]}, {"text": "Effective accuracy is defined to be the accuracy obtained when all out of coverage arguments are tagged as adjuncts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.670329749584198}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.998906135559082}]}, {"text": "This procedure always yields a classifier with 100% coverage and therefore provides an even ground for comparing the algorithms' performance.", "labels": [], "entities": []}, {"text": "We see accuracy as important on its own right since increasing coverage is often straightforward given easily obtainable larger training corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9988422989845276}, {"text": "coverage", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9812684655189514}]}, {"text": "accuracy is defined to be the accuracy resulting from labeling each out of coverage argument with an adjunct label.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9968051910400391}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9990276098251343}]}, {"text": "The rows represent the following models (left to right): selectional preference, predicate-slot collocation, argument-slot collocation, 'Ensemble(Intersection)', 'Ensemble(Union)' and the 'Ensemble(Intersection)' followed by self-training (see Section 3.4  Another reason is that a high accuracy classifier may provide training data to be used by subsequent supervised algorithms.", "labels": [], "entities": []}, {"text": "For completeness, we also provide results for the entire set of arguments.", "labels": [], "entities": []}, {"text": "The great majority of non-prepositional arguments are cores (87% in the test set).", "labels": [], "entities": []}, {"text": "We therefore tag all non-prepositional as cores and tag prepositional arguments using our model.", "labels": [], "entities": []}, {"text": "In order to minimize supervision, we distinguish between the prepositional and the nonprepositional arguments using Clark's tagger.", "labels": [], "entities": []}, {"text": "Finally, we experiment on a scenario where even argument identification on the test set is not provided, but performed by the algorithm of (, which uses neither syntactic nor SRL annotation but does utilize a supervised POS tagger.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.7594634592533112}]}, {"text": "We therefore run it in the 'SID' scenario.", "labels": [], "entities": [{"text": "SID", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.8848886489868164}]}, {"text": "We apply it to the sentences of length at most 10 contained in sections 2-21 of PB (11586 arguments in 6007 sentences).", "labels": [], "entities": [{"text": "PB", "start_pos": 80, "end_pos": 82, "type": "DATASET", "confidence": 0.9641425609588623}]}, {"text": "Non-prepositional arguments are invariably tagged as cores and out of coverage prepositional arguments as adjuncts.", "labels": [], "entities": []}, {"text": "We report labeled and unlabeled recall, precision and F-scores for this experiment.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9961698651313782}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9995668530464172}, {"text": "F-scores", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.998591959476471}]}, {"text": "An unlabeled match is defined to bean argument that agrees in its boundaries with a gold standard argument and a labeled match requires in addition that the arguments agree in their core/adjunct label.", "labels": [], "entities": []}, {"text": "We also report labeling accuracy which is the ratio between the number of labeled matches and the number of unlabeled matches 11 . presents the results of our main experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9682936668395996}]}, {"text": "In both scenarios, the most accurate of the three basic classifiers was the argument-slot collocation classifier.", "labels": [], "entities": []}, {"text": "This is an indication that the collocation between the argument and the preposition is more indicative of the core/adjunct label than the obligatoriness of the slot (as expressed by the predicate-slot collocation).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the various models. Accuracy, coverage and effective accuracy are presented in percents. Effective", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9992917776107788}, {"text": "coverage", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9982835054397583}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9759508371353149}]}, {"text": " Table 2: Comparison of the measures used by our model to alternative measures in the 'SID' scenario. Results are in percents.", "labels": [], "entities": [{"text": "SID'", "start_pos": 87, "end_pos": 91, "type": "TASK", "confidence": 0.8453193306922913}]}, {"text": " Table 3: Unlabeled and labeled scores for the experi-", "labels": [], "entities": []}]}