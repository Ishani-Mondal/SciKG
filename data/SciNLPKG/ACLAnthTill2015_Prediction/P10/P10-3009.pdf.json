{"title": [{"text": "Growing Related Words from Seed via User Behaviors: A Re-ranking Based Approach", "labels": [], "entities": []}], "abstractContent": [{"text": "Motivated by Google Sets, we study the problem of growing related words from a single seed word by leveraging user behaviors hiding in user records of Chinese input method.", "labels": [], "entities": []}, {"text": "Our proposed method is motivated by the observation that the more frequently two words co-occur in user records, the more related they are.", "labels": [], "entities": []}, {"text": "First, we utilize user behaviors to generate candidate words.", "labels": [], "entities": []}, {"text": "Then, we utilize search engine to enrich candidate words with adequate semantic features.", "labels": [], "entities": []}, {"text": "Finally, we reorder candidate words according to their semantic rela-tedness to the seed word.", "labels": [], "entities": []}, {"text": "Experimental results on a Chinese input method dataset show that our method gains better performance.", "labels": [], "entities": [{"text": "Chinese input method dataset", "start_pos": 26, "end_pos": 54, "type": "DATASET", "confidence": 0.5967824310064316}]}], "introductionContent": [{"text": "What is the relationship between \"\u81ea\u7136\u8bed\u8a00\u5904 \u7406\" (Natural Language Processing) and \"\u4eba\u5de5\u667a \u80fd \" (Artificial Intelligence)?", "labels": [], "entities": []}, {"text": "We may regard NLP as a research branch of AI.", "labels": [], "entities": []}, {"text": "Problems arise when we want to find more words related to the input query/seed word.", "labels": [], "entities": []}, {"text": "For example, if seed word \" \u81ea \u7136 \u8bed \u8a00 \u5904 \u7406 \" (Natural Language Processing) is entered into Google Sets, Google Sets returns an ordered list of related words such as \"\u4eba\u5de5\u667a\u80fd\" (Artificial Intelligence) and \"\u8ba1\u7b97\u673a\" (Computer).", "labels": [], "entities": []}, {"text": "Generally speaking, it performs a large-scale clustering algorithm that can gather related words.", "labels": [], "entities": []}, {"text": "In this paper, we want to investigate the advantage of user behaviors and re-ranking framework in related words retrieval task using Chinese input method user records.", "labels": [], "entities": [{"text": "words retrieval task", "start_pos": 106, "end_pos": 126, "type": "TASK", "confidence": 0.7856810887654623}]}, {"text": "We construct a User-Word bipartite graph to represent the information hiding in user records.", "labels": [], "entities": []}, {"text": "The bipartite graph keeps users on one side and words on the other side.", "labels": [], "entities": []}, {"text": "The underlying idea is that the more frequently two words co-occur in user records, the more related they are.", "labels": [], "entities": []}, {"text": "For example, \"\u673a\u5668\u7ffb \u8bd1\" (Machine Translation) is quite related to \"\u4e2d \u6587\u5206\u8bcd\" (Chinese Word Segmentation) because the two words are usually used together by researchers in natural language processing community.", "labels": [], "entities": [{"text": "Machine Translation)", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.8141822119553884}, {"text": "Chinese Word Segmentation)", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.6459724232554436}]}, {"text": "As a result, user behaviors offer anew perspective for measuring relatedness between words.", "labels": [], "entities": []}, {"text": "On the other hand, we can also recommend related words to users in order to enhance user experiences.", "labels": [], "entities": []}, {"text": "Researchers are always willing to accept related terminologies in their research fields.", "labels": [], "entities": []}, {"text": "However, the method is purely statistics based if we only consider co-occurrence aspect.", "labels": [], "entities": []}, {"text": "We want to add semantic features.", "labels": [], "entities": []}, {"text": "utilize search engine to supply web queries with more semantic context and gains better results for query suggestion task.", "labels": [], "entities": []}, {"text": "We borrow their idea in this paper.", "labels": [], "entities": []}, {"text": "User behaviors provide statistic information to generate candidate words.", "labels": [], "entities": []}, {"text": "Then, we can enrich candidate words with additional semantic features using search engine to retrieve more relevant candidates earlier.", "labels": [], "entities": []}, {"text": "Statistical and semantic features can complement each other.", "labels": [], "entities": []}, {"text": "Therefore, we can gain better performance if we consider them together.", "labels": [], "entities": []}, {"text": "The contributions of this paper are threefold.", "labels": [], "entities": []}, {"text": "First, we introduce user behaviors in related word retrieval task and construct a User-Word bipartite graph from user behaviors.", "labels": [], "entities": [{"text": "word retrieval task", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7909965217113495}]}, {"text": "Words are used by users, and it is reasonable to measure relatedness between words by analyzing user behaviors.", "labels": [], "entities": []}, {"text": "Second, we take the advantage of semantic features using search engine to reorder candidate words.", "labels": [], "entities": []}, {"text": "We aim to return more relevant candidates earlier.", "labels": [], "entities": []}, {"text": "Finally, our method is unsupervised and language independent, which means that we do not require any training set or manual labeling efforts.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Some related works are discussed in Section 2.", "labels": [], "entities": []}, {"text": "Then we introduce our method for related words retrieval in Section 3.", "labels": [], "entities": [{"text": "related words retrieval", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.6577000220616659}]}, {"text": "Experiment results and discussions are showed in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes the whole paper and gives some future works.", "labels": [], "entities": []}, {"text": "For related words retrieval task, Google Sets (Google, 2010) provides a remarkably interesting tool for finding words related to an input word.", "labels": [], "entities": [{"text": "related words retrieval task", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.7211699858307838}, {"text": "Google Sets (Google, 2010)", "start_pos": 34, "end_pos": 60, "type": "DATASET", "confidence": 0.842626554625375}]}, {"text": "As stated in (, Google Sets performs poor results for input words in Chinese language.", "labels": [], "entities": []}, {"text": "Bayesian Sets () offers an alternative method for related words retrieval under the framework of Bayesian inference.", "labels": [], "entities": [{"text": "related words retrieval", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.6164504190286001}]}, {"text": "It computes a score for each candidate word by comparing the posterior probability of that word given the input, to the prior probability of that candidate word.", "labels": [], "entities": []}, {"text": "Then, it returns a ranked list of candidate words according to their computed scores.", "labels": [], "entities": []}, {"text": "Recently, introduce user behaviors in new word detection task via a collaborative filtering manner.", "labels": [], "entities": [{"text": "word detection task", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8374569217363993}]}, {"text": "They extend their method to related word retrieval task.", "labels": [], "entities": [{"text": "word retrieval task", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8138010303179423}]}, {"text": "Moreover, they prove that user behaviors provide anew point for new word detection and related word retrieval tasks.", "labels": [], "entities": [{"text": "word detection", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.7534285485744476}, {"text": "word retrieval", "start_pos": 95, "end_pos": 109, "type": "TASK", "confidence": 0.6974808275699615}]}, {"text": "However, their method is purely statistical method without considering semantic features.", "labels": [], "entities": []}, {"text": "We can regard related word retrieval task as problem of measuring the semantic relatedness between pairs of very short texts.", "labels": [], "entities": [{"text": "related word retrieval", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.609650323788325}]}, {"text": "introduce a web kernel function for measuring semantic similarities using snippets of search results.", "labels": [], "entities": []}, {"text": "This work is followed by,.", "labels": [], "entities": []}, {"text": "They combine the web kernel with other metrics of similarity between word vectors, such as Jaccard Coefficient and KL Divergence to enhance the result.", "labels": [], "entities": []}, {"text": "In this paper, we follow the similar idea of using search engine to enrich semantic features of a query word.", "labels": [], "entities": []}, {"text": "We regard the returned snippets as the context of a query word.", "labels": [], "entities": []}, {"text": "And then we reorder candidate words and expect more relevant candidate words can be retrieved earlier.", "labels": [], "entities": []}, {"text": "More details are given in Section 3.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we demonstrate our experiment results.", "labels": [], "entities": []}, {"text": "First, we introduce the dataset used in this paper and some statistics of the dataset.", "labels": [], "entities": []}, {"text": "Then, we build our ground truth for related word retrieval task using Baidu encyclopedia.", "labels": [], "entities": [{"text": "word retrieval", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.7092955410480499}]}, {"text": "Third, we give some example of related word retrieval task.", "labels": [], "entities": [{"text": "word retrieval task", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7865630090236664}]}, {"text": "We show that more related words can be returned earlier if we consider semantic features.", "labels": [], "entities": []}, {"text": "Finally, we make further analysis of the parameter tuning mentioned before.", "labels": [], "entities": []}, {"text": "We carryout our experiment on Sogou Chinese input method dataset.", "labels": [], "entities": [{"text": "Sogou Chinese input method dataset", "start_pos": 30, "end_pos": 64, "type": "DATASET", "confidence": 0.9415241479873657}]}, {"text": "The dataset contains 10,000 users and 183,870 words, and the number of edges in the constructed bipartite graph is 42,250,718.", "labels": [], "entities": []}, {"text": "As we can see, the dataset is quite sparse, because most of the users tend to use only a small number of words.", "labels": [], "entities": []}, {"text": "For related word retrieval task, we need to judge whether a candidate word is related to the input seed word.", "labels": [], "entities": [{"text": "related word retrieval task", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.7874637693166733}]}, {"text": "We can ask domain experts to answer this question.", "labels": [], "entities": []}, {"text": "However, it needs a lot of manual efforts.", "labels": [], "entities": []}, {"text": "To alleviate this problem, we adopt Baidu encyclopedia () as our ground truth.", "labels": [], "entities": []}, {"text": "In Baidu encyclopedia, volunteers give a set of words that are related to the particular seed word.", "labels": [], "entities": []}, {"text": "As related words are provided by human, we are confident enough to use them as our ground truth.", "labels": [], "entities": []}, {"text": "We randomly select 2,000 seed words as our validation set.", "labels": [], "entities": []}, {"text": "However, whether two words are related is quite subjective.", "labels": [], "entities": []}, {"text": "In this paper, Baidu encyclopedia is only used as a relatively accurate standard for evaluation.", "labels": [], "entities": [{"text": "Baidu encyclopedia", "start_pos": 15, "end_pos": 33, "type": "DATASET", "confidence": 0.8101280927658081}]}, {"text": "We just want to investigate whether user behaviors and re-ranking framework is helpful in the related word retrieval task under various evaluation metrics.", "labels": [], "entities": [{"text": "word retrieval task", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7845745285352071}]}, {"text": "We give a simple example of our method in.", "labels": [], "entities": []}, {"text": "The input seed word is \"\u673a\u5668\u5b66\u4e60\" (Machine Learning).", "labels": [], "entities": []}, {"text": "Generally speaking, all these returned candidate words are relevant to the seed word to certain degree, which indicates the effectiveness of our method.", "labels": [], "entities": []}, {"text": "Words Related to \"Machine Learning\"  In this paper, we use three evaluation metrics to validate the performance of our method: 1.", "labels": [], "entities": []}, {"text": "Precision@N (P@N).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9827856421470642}]}, {"text": "P@N measures how much percent of the topmost results returned are correct.", "labels": [], "entities": [{"text": "P", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9392059445381165}]}, {"text": "We consider P@5 and P@10. 2. Binary preference measure (Bpref)).", "labels": [], "entities": [{"text": "Binary preference measure (Bpref))", "start_pos": 29, "end_pos": 63, "type": "METRIC", "confidence": 0.8088045219580332}]}, {"text": "As we cannot list all the related words of an input seed word, we use Bpref to evaluate our method.", "labels": [], "entities": [{"text": "Bpref", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.876262366771698}]}, {"text": "For an input seed word with R judged candidate words where r is a related word and n is a nonrelated word.", "labels": [], "entities": []}, {"text": "Bpref is defined as follow: Mean reciprocal rank of the first retrieved result (MRR).", "labels": [], "entities": [{"text": "Bpref", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9815564155578613}, {"text": "Mean reciprocal rank of the first retrieved result (MRR)", "start_pos": 28, "end_pos": 84, "type": "METRIC", "confidence": 0.7571729665452783}]}, {"text": "For a sample of input seed words W, rank i is the rank of the first related candidate word for the input seed word w i , MRR is the average of the reciprocal ranks of results, which is defined as follow:", "labels": [], "entities": [{"text": "MRR", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9974787831306458}]}], "tableCaptions": [{"text": " Table 3. Comparisons with Bayesian Sets", "labels": [], "entities": []}, {"text": " Table 4. We can see that N =  20 gives relatively best results, which indicates  that we should select Top 20 candidate words for  re-ranking.", "labels": [], "entities": []}, {"text": " Table 4. Comparisons with Re-ranking Method", "labels": [], "entities": []}]}