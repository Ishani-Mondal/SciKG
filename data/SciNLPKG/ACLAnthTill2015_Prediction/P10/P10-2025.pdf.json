{"title": [], "abstractContent": [{"text": "We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual proba-bilistic model.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7830905914306641}]}, {"text": "Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.7899652719497681}]}, {"text": "We design a generative model for word alignment that uses synonym information as a regularization term.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.7723789811134338}]}, {"text": "The experimental results show that our proposed method significantly improves word alignment quality.", "labels": [], "entities": [{"text": "word alignment quality", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.8246631026268005}]}], "introductionContent": [{"text": "Word alignment is an essential step inmost phrase and syntax based statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7183158844709396}, {"text": "statistical machine translation (SMT)", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.8132714728514353}]}, {"text": "It is an inference problem of word correspondences between different languages given parallel sentence pairs.", "labels": [], "entities": []}, {"text": "Accurate word alignment can induce high quality phrase detection and translation probability, which leads to a significant improvement in SMT performance.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7188805937767029}, {"text": "phrase detection", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.719694048166275}, {"text": "SMT", "start_pos": 138, "end_pos": 141, "type": "TASK", "confidence": 0.9956562519073486}]}, {"text": "Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.7827467322349548}]}, {"text": "One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus.", "labels": [], "entities": [{"text": "word alignment quality", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7824562788009644}]}, {"text": "This monolingual knowledge makes it easier to determine corresponding words correctly.", "labels": [], "entities": []}, {"text": "For instance, functional words in one language tend to correspond to functional words in another language, and the syntactic dependency of words in each language can help the alignment process (.", "labels": [], "entities": []}, {"text": "It has been shown that such grammatical information works as a constraint in word alignment models and improves word alignment quality.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.7846634685993195}, {"text": "word alignment", "start_pos": 112, "end_pos": 126, "type": "TASK", "confidence": 0.7748335003852844}]}, {"text": "A large number of monolingual lexical semantic resources such as WordNet have been constructed in more than fifty languages).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9596655964851379}]}, {"text": "They include wordlevel relations such as synonyms, hypernyms and hyponyms.", "labels": [], "entities": []}, {"text": "Synonym information is particularly helpful for word alignment because we can expect a synonym to correspond to the same word in a different language.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.7907118499279022}]}, {"text": "In this paper, we explore a method for using synonym information effectively to improve word alignment quality.", "labels": [], "entities": [{"text": "word alignment quality", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.8084855278333029}]}, {"text": "In general, synonym relations are defined in terms of word sense, not in terms of word form.", "labels": [], "entities": []}, {"text": "In other words, synonym relations are usually context or domain dependent.", "labels": [], "entities": []}, {"text": "For instance, 'head' and 'chief' are synonyms in contexts referring to working environment, while 'head' and 'forefront' are synonyms in contexts referring to physical positions.", "labels": [], "entities": []}, {"text": "It is difficult, however, to imagine a context where 'chief' and 'forefront' are synonyms.", "labels": [], "entities": []}, {"text": "Therefore, it is easy to imagine that simply replacing all occurrences of 'chief' and 'forefront' with 'head' do sometimes harm with word alignment accuracy, and we have to model either the context or senses of words.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 133, "end_pos": 147, "type": "TASK", "confidence": 0.75757896900177}, {"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9024083018302917}]}, {"text": "We propose a novel method that incorporates synonyms from monolingual resources in a bilingual word alignment model.", "labels": [], "entities": []}, {"text": "We formulate a synonym pair generative model with a topic variable and use this model as a regularization term with a bilingual word alignment model.", "labels": [], "entities": []}, {"text": "The topic variable in our synonym model is helpful for disambiguating the meanings of synonyms.", "labels": [], "entities": []}, {"text": "We extend HM-BiTAM, which is a HMM-based word alignment model with a latent topic, with a novel synonym pair generative model.", "labels": [], "entities": [{"text": "HMM-based word alignment", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.6046434839566549}]}, {"text": "We applied the proposed method to an English-French word alignment task and successfully improved the word", "labels": [], "entities": [{"text": "English-French word alignment task", "start_pos": 37, "end_pos": 71, "type": "TASK", "confidence": 0.6988670527935028}]}], "datasetContent": [{"text": "For an empirical evaluation of the proposed method, we used a bilingual parallel corpus of English-French Hansards (.", "labels": [], "entities": []}, {"text": "The corpus consists of over 1 million sentence pairs, which include 447 manually wordaligned sentences.", "labels": [], "entities": []}, {"text": "We selected 100 sentence pairs randomly from the manually word-aligned sentences as development data for tuning the regularization weight \u03b6, and used the 347 remaining sentence pairs as evaluation data.", "labels": [], "entities": []}, {"text": "We also randomly selected 10k, 50k, and 100k sized sentence pairs from the corpus as additional training data.", "labels": [], "entities": []}, {"text": "We ran the unsupervised training of our proposed word alignment model on the additional training data and the 347 sentence pairs of the evaluation data.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.7915185391902924}]}, {"text": "Note that manual word alignment of the 347 sentence pairs was not used for the unsupervised training.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7005365490913391}]}, {"text": "After the unsupervised training, we evaluated the word alignment performance of our proposed method by comparing the manual word alignment of the 347 sentence pairs with the prediction provided by the trained model.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.7221532911062241}]}, {"text": "We collected English and French synonym pairs from WordNet 2.1 and WOLF 0.1.4, respectively.", "labels": [], "entities": [{"text": "WordNet 2.1", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.9171682894229889}, {"text": "WOLF 0.1.4", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.8792842328548431}]}, {"text": "WOLF is a semantic resource constructed from the Princeton WordNet and various multilingual resources.", "labels": [], "entities": [{"text": "WOLF", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.898301899433136}, {"text": "Princeton WordNet", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.90094855427742}]}, {"text": "We selected synonym pairs where both words were included in the bilingual training set.", "labels": [], "entities": []}, {"text": "We compared the word alignment performance of our model with that of GIZA++ 1.03 1, and HMBiTAM () implemented by us.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7889799475669861}]}, {"text": "GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to \u03b6 = 0 in eq.", "labels": [], "entities": []}, {"text": "7. We adopted K = 3 topics, following the setting in ().", "labels": [], "entities": []}, {"text": "We trained the word alignment in two directions: English to French, and French to English.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.7326618432998657}]}, {"text": "The alignment results for both directions were refined with 'GROW' heuristics to yield high precision and high recall in accordance with previous work).", "labels": [], "entities": [{"text": "GROW", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9921504259109497}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9977768063545227}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9989632368087769}]}, {"text": "We evaluated these results for precision, recall, Fmeasure and alignment error rate (AER), which are standard metrics for word alignment accuracy).", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9996044039726257}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9994378685951233}, {"text": "Fmeasure", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9979487061500549}, {"text": "alignment error rate (AER)", "start_pos": 63, "end_pos": 89, "type": "METRIC", "confidence": 0.9594860772291819}, {"text": "word alignment", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.6942802518606186}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.43963131308555603}]}, {"text": "The best results are indicated in bold type.", "labels": [], "entities": []}, {"text": "The additional data set sizes are (a) 10k, (b) 50k, (c) 100k.", "labels": [], "entities": []}, {"text": "shows the word alignment accuracy of the three methods trained with 10k, 50k, and 100k additional sentence pairs.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7482170164585114}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9823371171951294}]}, {"text": "For all settings, our proposed method outperformed other conventional methods.", "labels": [], "entities": []}, {"text": "This result shows that synonym information is effective for improving word alignment quality as we expected.", "labels": [], "entities": [{"text": "word alignment quality", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.802493671576182}]}, {"text": "As mentioned in Sections 1 and 3.1, the main idea of our proposed method is to introduce latent topics for modeling synonym pairs, and then to utilize the synonym pair model for the regularization of word alignment models.", "labels": [], "entities": [{"text": "regularization of word alignment", "start_pos": 182, "end_pos": 214, "type": "TASK", "confidence": 0.6687160506844521}]}, {"text": "We expect the latent topics to be useful for modeling polysemous words included in synonym pairs and to enable us to incorporate synonym information effectively into word alignment models.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 166, "end_pos": 180, "type": "TASK", "confidence": 0.7235552817583084}]}, {"text": "To confirm the effect of the synonym pair model with latent topics, we also tested GIZA++ and HMBiTAM with what we call Synonym Replacement Heuristics (SRH), where all of the synonym pairs in the bilingual training sentences were simply replaced with a representative word.", "labels": [], "entities": [{"text": "Synonym Replacement Heuristics (SRH)", "start_pos": 120, "end_pos": 156, "type": "TASK", "confidence": 0.8009458233912786}]}, {"text": "For instance, the words 'sick' and 'ill' in the bilingual sentences: The number of vocabularies in the 10k, 50k and 100k data sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of word alignment accuracy.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7166601419448853}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9187089204788208}]}, {"text": " Table 2: The number of vocabularies in the 10k,  50k and 100k data sets.", "labels": [], "entities": []}]}