{"title": [{"text": "Boosting-based System Combination for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7673113346099854}]}], "abstractContent": [{"text": "In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 142, "end_pos": 179, "type": "TASK", "confidence": 0.7798146357138952}]}, {"text": "Our method is based on the framework of boosting.", "labels": [], "entities": []}, {"text": "First, a sequence of weak translation systems is generated from a baseline system in an iterative manner.", "labels": [], "entities": []}, {"text": "Then, a strong translation system is built from the ensemble of these weak translation systems.", "labels": [], "entities": []}, {"text": "To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work.", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9853190183639526}]}, {"text": "We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrase-based system and a syntax-based system.", "labels": [], "entities": [{"text": "Chinese-to-English Machine Translation (MT) tasks", "start_pos": 26, "end_pos": 75, "type": "TASK", "confidence": 0.8086752934115273}]}, {"text": "The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems.", "labels": [], "entities": [{"text": "NIST evaluation test sets", "start_pos": 34, "end_pos": 59, "type": "DATASET", "confidence": 0.8864539414644241}, {"text": "translation", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.94287109375}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.8568490743637085}]}], "introductionContent": [{"text": "Recent research on Statistical Machine Translation (SMT) has achieved substantial progress.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 19, "end_pos": 56, "type": "TASK", "confidence": 0.902730276187261}]}, {"text": "Many SMT frameworks have been developed, including phrase-based SMT (, hierarchical phrase-based SMT, syntax-based SMT; ;), etc.", "labels": [], "entities": [{"text": "SMT", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.9786306023597717}, {"text": "phrase-based SMT", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.5443584620952606}]}, {"text": "With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9857736229896545}, {"text": "SMT", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.9603412747383118}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.7535063624382019}]}, {"text": "The basic idea of system combination is to extractor generate a translation by voting from an ensemble of translation outputs.", "labels": [], "entities": []}, {"text": "Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination) simply selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination, can generate new translations differing from any of the original translations.", "labels": [], "entities": []}, {"text": "One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs.", "labels": [], "entities": [{"text": "SMT system combination", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.9186979333559672}]}, {"text": "To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models.", "labels": [], "entities": []}, {"text": "However, this requirement cannot be met in many cases, since we do not always have the access to multiple SMT engines due to the high cost of developing and tuning SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9799243211746216}, {"text": "SMT", "start_pos": 164, "end_pos": 167, "type": "TASK", "confidence": 0.9385798573493958}]}, {"text": "To reduce the burden of system development, it might be a nice way to combine a set of translation systems built from a single translation engine.", "labels": [], "entities": []}, {"text": "A key issue here is how to generate an ensemble of diversified translation systems from a single translation engine in a principled way.", "labels": [], "entities": []}, {"text": "Addressing this issue, we propose a boostingbased system combination method to learn a combined translation system from a single SMT engine.", "labels": [], "entities": [{"text": "SMT", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.9703988432884216}]}, {"text": "In this method, a sequence of weak translation systems is generated from a baseline system in an iterative manner.", "labels": [], "entities": []}, {"text": "In each iteration, anew weak translation system is learned, focusing more on the sentences that are relatively poorly translated by the previous weak translation system.", "labels": [], "entities": []}, {"text": "Finally, a strong translation system is built from the ensemble of the weak translation systems.", "labels": [], "entities": []}, {"text": "Our experiments are conducted on Chinese-toEnglish translation in three state-of-the-art SMT systems, including a phrase-based system, a hierarchical phrase-based system and a syntax-based Input: a model u, a sequence of (training) samples {(f 1 , r 1 ), ..., (f m , rm )} where f i is the i-th source sentence, and r i is the set of reference translations for f i . Output: anew translation system Initialize: D 1 (i) = 1 / m for all i = 1, ..., m For t = 1, ..., T 1.", "labels": [], "entities": [{"text": "Chinese-toEnglish translation", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.5960792005062103}, {"text": "SMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.988381564617157}]}, {"text": "Train a translation system u(\u03bb * t ) on {(f i , r i )} using distribution D t 2.", "labels": [], "entities": []}, {"text": "Calculate the error rate t where l i is the loss on the i-th training sample, and Z t is the normalization factor.", "labels": [], "entities": [{"text": "error rate", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9811562299728394}]}, {"text": "Output the final system:: Boosting-based System Combination system.", "labels": [], "entities": []}, {"text": "All the systems are evaluated on three NIST MT evaluation test sets.", "labels": [], "entities": [{"text": "NIST MT evaluation test sets", "start_pos": 39, "end_pos": 67, "type": "DATASET", "confidence": 0.8187986731529235}]}, {"text": "Experimental results show that our method leads to significant improvements in translation accuracy over the baseline systems.", "labels": [], "entities": [{"text": "translation", "start_pos": 79, "end_pos": 90, "type": "TASK", "confidence": 0.9373294115066528}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.8839596509933472}]}], "datasetContent": [{"text": "Our experiments are conducted on Chinese-toEnglish translation in three SMT systems.", "labels": [], "entities": [{"text": "Chinese-toEnglish translation", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.5614984482526779}, {"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9742530584335327}]}, {"text": "Our bilingual data consists of 140K sentence pairs in the FBIS data set . GIZA++ is employed to perform the bi-directional word alignment between the source and target sentences, and the final word alignment is generated using the intersect-diag-grow method.", "labels": [], "entities": [{"text": "FBIS data set", "start_pos": 58, "end_pos": 71, "type": "DATASET", "confidence": 0.9637115796407064}]}, {"text": "All the word-aligned bilingual sentence pairs are used to extract phrases and rules for the baseline systems.", "labels": [], "entities": []}, {"text": "A 5-gram language model is trained on the target-side of the bilingual data and the Xinhua portion of English Gigaword corpus.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 102, "end_pos": 125, "type": "DATASET", "confidence": 0.8365071018536886}]}, {"text": "Berkeley Parser is used to generate the English parse trees for the rule extraction of the syntax-based system.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.7882819473743439}]}, {"text": "The data set used for weight training in boostingbased system combination comes from NIST MT03 evaluation set.", "labels": [], "entities": [{"text": "NIST MT03 evaluation set", "start_pos": 85, "end_pos": 109, "type": "DATASET", "confidence": 0.926167219877243}]}, {"text": "To speedup MERT, all the sentences with more than 20 Chinese words are removed.", "labels": [], "entities": [{"text": "MERT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.526496946811676}]}, {"text": "The test sets are the NIST evaluation sets of MT04, MT05 and MT06.", "labels": [], "entities": [{"text": "NIST evaluation sets", "start_pos": 22, "end_pos": 42, "type": "DATASET", "confidence": 0.886331836382548}, {"text": "MT04", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9088399410247803}, {"text": "MT05", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.9073925018310547}, {"text": "MT06", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.9501513838768005}]}, {"text": "The translation quality is evaluated in terms of case-insensitive NIST version BLEU metric.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9527069926261902}, {"text": "NIST version", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.7600277066230774}, {"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.8790961503982544}]}, {"text": "Statistical significant testis conducted using the bootstrap resampling method proposed by.", "labels": [], "entities": [{"text": "bootstrap resampling", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7593950629234314}]}, {"text": "Beam search and cube pruning are used to prune the search space in all the three baseline systems.", "labels": [], "entities": [{"text": "Beam search", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6941007524728775}]}, {"text": "By default, both of the beam size and the size of n-best list are set to 20.", "labels": [], "entities": []}, {"text": "In the settings of boosting-based system combination, the maximum number of iterations is set to 30, and k (in Equation 7) is set to 5.", "labels": [], "entities": []}, {"text": "The ngram consensuses-based features (in Equation 9) used in system combination ranges from unigram to 4-gram.", "labels": [], "entities": []}, {"text": "First we investigate the effectiveness of the boosting-based system combination on the three systems.", "labels": [], "entities": []}, {"text": "Figures 2-5 show the BLEU curves on the development and test sets, where the X-axis is the iteration number, and the Y-axis is the BLEU score of the system generated by the boostingbased system combination.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9984500408172607}, {"text": "BLEU score", "start_pos": 131, "end_pos": 141, "type": "METRIC", "confidence": 0.9833884537220001}]}, {"text": "The points at iteration 1 stand for the performance of the baseline systems.", "labels": [], "entities": []}, {"text": "We see, first of all, that all the three systems are improved during iterations on the development set.", "labels": [], "entities": []}, {"text": "This trend also holds on the test sets.", "labels": [], "entities": []}, {"text": "After 5, 7 and 8 iterations, relatively stable improvements are achieved by the phrase-based system, the Hiero system and the syntax-based system, respectively.", "labels": [], "entities": [{"text": "Hiero system", "start_pos": 105, "end_pos": 117, "type": "DATASET", "confidence": 0.8973795473575592}]}, {"text": "The BLEU scores tend to converge to the stable values after 20 iterations for all the systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9991368651390076}]}, {"text": "also show that the boosting-based system combination seems to be more helpful to the phrase-based system than to the Hiero system and the syntax-based system.", "labels": [], "entities": [{"text": "Hiero system", "start_pos": 117, "end_pos": 129, "type": "DATASET", "confidence": 0.8997425436973572}]}, {"text": "For the phrase-based system, it yields over 0.6 BLEU point gains just after the 3rd iteration on all the data sets.) on the development and test sets.", "labels": [], "entities": [{"text": "BLEU point gains", "start_pos": 48, "end_pos": 64, "type": "METRIC", "confidence": 0.9696477055549622}]}, {"text": "* = significantly better than baseline (p < 0.05).", "labels": [], "entities": []}, {"text": "hieves significant BLEU improvements after 15 iterations, and the highest BLEU scores are generally yielded after 20 iterations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9989521503448486}, {"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9987866282463074}]}, {"text": "Also as shown in, over 0.7 BLEU point gains are obtained on the phrase-based system after 10 iterations.", "labels": [], "entities": [{"text": "BLEU point gains", "start_pos": 27, "end_pos": 43, "type": "METRIC", "confidence": 0.945135772228241}]}, {"text": "The largest BLEU improvement on the phrase-based system is over 1 BLEU point inmost cases.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9989078044891357}, {"text": "BLEU point", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9735625982284546}]}, {"text": "These results reflect that our method is relatively more effective for the phrase-based system than for the other two systems, and thus confirms the fact we observed in Figures 2-5.", "labels": [], "entities": []}, {"text": "We also investigate the impact of n-best list size on the performance of baseline systems.", "labels": [], "entities": []}, {"text": "For the comparison, we show the performance of the baseline systems with the n-best list size of 600 (Baseline+600best in) which equals to the maximum number of translation candidates accessed in the final combination system (combine 30 member systems, i.e. Boosing-30Iterations)., Baseline+600best obtains stable improvements over Baseline.", "labels": [], "entities": []}, {"text": "It indicates that the access to larger n-best lists is helpful to improve the performance of baseline systems.", "labels": [], "entities": []}, {"text": "However, the improvements achieved by Baseline+600best are modest compared to the improvements achieved by Boosting-30Iterations.", "labels": [], "entities": [{"text": "Boosting-30Iterations", "start_pos": 107, "end_pos": 128, "type": "DATASET", "confidence": 0.9223909378051758}]}, {"text": "These results indicate that the SMT systems can benefit more from the diversified outputs of member systems rather than from larger n-best lists produced by a single system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9935621023178101}]}, {"text": "In this set of experiments, we evaluate the oracle performance on the n-best lists of the baseline systems and the combined systems generated by boosting-based system combination.", "labels": [], "entities": []}, {"text": "Our primary goal here is to study the impact of our method on the upper-bound performance.", "labels": [], "entities": []}, {"text": "shows the results, where Baseline+600best stands for the top-600 translation candidates generated by the baseline systems, and Boosting-30iterations stands for the ensemble of 30 member systems' top-20 translation candidates.", "labels": [], "entities": [{"text": "Boosting-30iterations", "start_pos": 127, "end_pos": 148, "type": "METRIC", "confidence": 0.757505476474762}]}, {"text": "As expected, the oracle performance of Boosting-30Iterations is significantly higher than that of Baseline+600best.", "labels": [], "entities": []}, {"text": "This result indicates that our method can provide much \"better\" translation candidates for system combination than enlarging the size of n-best list naively.", "labels": [], "entities": []}, {"text": "It also gives us a rational explanation for the significant improvements achieved by our method as shown in Section 5.3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of the results", "labels": [], "entities": []}, {"text": " Table 2: Oracle performance of various systems.  * = significantly better than baseline (p < 0.05).", "labels": [], "entities": []}]}