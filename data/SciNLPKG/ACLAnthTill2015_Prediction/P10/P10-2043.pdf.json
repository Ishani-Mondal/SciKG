{"title": [{"text": "Online Generation of Locality Sensitive Hash Signatures", "labels": [], "entities": []}], "abstractContent": [{"text": "Motivated by the recent interest in streaming algorithms for processing large text collections, we revisit the work of Ravichandran et al.", "labels": [], "entities": []}, {"text": "(2005) on using the Locality Sensitive Hash (LSH) method of Charikar (2002) to enable fast, approximate comparisons of vector cosine similarity.", "labels": [], "entities": []}, {"text": "For the common case of feature updates being additive over a data stream, we show that LSH signatures can be maintained online, without additional approximation error, and with lower memory requirements than when using the standard offline technique.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has been a surge of interest in adapting results from the streaming algorithms community to problems in processing large text collections.", "labels": [], "entities": []}, {"text": "The term streaming refers to a model where data is made available sequentially, and it is assumed that resource limitations preclude storing the entirety of the data for offline (batch) processing.", "labels": [], "entities": []}, {"text": "Statistics of interest are approximated via online, randomized algorithms.", "labels": [], "entities": []}, {"text": "Examples of text applications include: collecting approximate counts; Van Durme and Lall, 2009a), finding top-n elements (), estimating term co-occurrence (, adaptive language modeling (, and building top-k ranklists based on pointwise mutual information).", "labels": [], "entities": [{"text": "adaptive language modeling", "start_pos": 158, "end_pos": 184, "type": "TASK", "confidence": 0.6230574945608774}]}, {"text": "Here we revisit the work of on building word similarity measures from large text collections by using the Locality Sensitive Hash (LSH) method of.", "labels": [], "entities": []}, {"text": "For the common case of feature updates being additive over a data stream (such as when tracking lexical co-occurrence), we show that LSH signatures can be maintained online, without additional approximation error, and with lower memory requirements than when using the standard offline technique.", "labels": [], "entities": []}, {"text": "We envision this method being used in conjunction with dynamic clustering algorithms, fora variety of applications.", "labels": [], "entities": []}, {"text": "For example, made use of LSH signatures generated over individual tweets, for the purpose of first story detection.", "labels": [], "entities": [{"text": "first story detection", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.7885100642840067}]}, {"text": "Streaming LSH should allow for the clustering of Twitter authors, based on the tweets they generate, with signatures continually updated over the Twitter stream.", "labels": [], "entities": []}], "datasetContent": [{"text": "Similar to the experiments of, we evaluated the fidelity of signature generation in the context of calculating distributional similarity between words across a large text collection: in our case, articles taken from the NYTimes portion of the Gigaword corpus.", "labels": [], "entities": [{"text": "signature generation", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.7445375919342041}, {"text": "NYTimes portion of the Gigaword corpus", "start_pos": 220, "end_pos": 258, "type": "DATASET", "confidence": 0.8639858464399973}]}, {"text": "The collection was processed as a stream, sentence by sentence, using bigram fea- tures.", "labels": [], "entities": []}, {"text": "This gave a stream of 773,185,086 tokens, with 1,138,467 unique types.", "labels": [], "entities": []}, {"text": "Given the number of types, this led to a (sparse) feature space with dimension on the order of 2.5 million.", "labels": [], "entities": []}, {"text": "After compiling signatures, fifty-thousand x, y pairs of types were randomly sampled by selecting x and y each independently, with replacement, from those types with at least 10 tokens in the stream (where 310,327 types satisfied this constraint).", "labels": [], "entities": []}, {"text": "The true cosine values between each such x and y was computed based on offline calculation, and compared to the cosine similarity predicted by the Hamming distance between the signatures for x and y.", "labels": [], "entities": []}, {"text": "Unless otherwise specified, the random pool size was fixed at m = 10, 000.", "labels": [], "entities": []}, {"text": "visually reaffirms the trade-off in LSH between the number of bits and the accuracy of cosine prediction across the range of cosine values.", "labels": [], "entities": [{"text": "LSH", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.8604015707969666}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.99933260679245}]}, {"text": "As the underlying vectors are strictly positive, the true cosine is restricted to. shows the absolute error between truth and prediction fora similar sample, measured using signatures of a variety of bit lengths.", "labels": [], "entities": [{"text": "absolute error", "start_pos": 93, "end_pos": 107, "type": "METRIC", "confidence": 0.9176706671714783}]}, {"text": "Here we see horizontal bands arising from truly orthogonal vectors leading to step-wise absolute error values tracked to Hamming distance.", "labels": [], "entities": []}, {"text": "compares the online and batch LSH algorithms, giving the mean absolute error between predicted and actual cosine values, computed for the fifty-thousand element sample, using signatures of various lengths.", "labels": [], "entities": [{"text": "mean absolute error", "start_pos": 57, "end_pos": 76, "type": "METRIC", "confidence": 0.6786497533321381}]}, {"text": "These results confirm that we achieve the same level of accuracy with online updates as compared to the standard method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9990696310997009}]}, {"text": "shows how a pool size as low as m = 100 gives reasonable variation in random values, and that m = 10, 000 is sufficient.", "labels": [], "entities": []}, {"text": "When using a standard 32 bit floating point representation, this is just 40 KBytes of memory, as compared to, e.g., the 2.5 GBytes required to store 256 random vectors each containing 2.5 million elements. is based on taking an example for each of three part-of-speech categories, and reporting the resultant top-5 words as according to approximated cosine similarity.", "labels": [], "entities": []}, {"text": "Depending on the intended application, these results indicate a range of potentially sufficient signature lengths.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Mean absolute error when using signatures gener-", "labels": [], "entities": [{"text": "Mean absolute error", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8907757997512817}]}]}