{"title": [{"text": "Extraction and Approximation of Numerical Attributes from the Web", "labels": [], "entities": [{"text": "Extraction and Approximation of Numerical Attributes", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.6468018169204394}]}], "abstractContent": [{"text": "We present a novel framework for automated extraction and approximation of numerical object attributes such as height and weight from the Web.", "labels": [], "entities": [{"text": "automated extraction and approximation of numerical object attributes", "start_pos": 33, "end_pos": 102, "type": "TASK", "confidence": 0.78166364133358}]}, {"text": "Given an object-attribute pair, we discover and analyze attribute information fora set of comparable objects in order to infer the desired value.", "labels": [], "entities": []}, {"text": "This allows us to approximate the desired numerical values even when no exact values can be found in the text.", "labels": [], "entities": []}, {"text": "Our framework makes use of relation defining patterns and WordNet similarity information.", "labels": [], "entities": []}, {"text": "First, we obtain from the Web and WordNet a list of terms similar to the given object.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9631242752075195}]}, {"text": "Then we retrieve attribute values for each term in this list, and information that allows us to compare different objects in the list and to infer the attribute value range.", "labels": [], "entities": []}, {"text": "Finally, we combine the retrieved data for all terms from the list to selector approximate the requested value.", "labels": [], "entities": []}, {"text": "We evaluate our method using automated question answering, WordNet enrichment, and comparison with answers given in Wikipedia and by leading search engines.", "labels": [], "entities": [{"text": "question answering", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7505755722522736}]}, {"text": "In all of these, our framework provides a significant improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information on various numerical properties of physical objects, such as length, width and weight is fundamental in question answering frameworks and for answering search engine queries.", "labels": [], "entities": [{"text": "question answering", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.8427181243896484}, {"text": "answering search engine queries", "start_pos": 154, "end_pos": 185, "type": "TASK", "confidence": 0.8794265389442444}]}, {"text": "While in some cases manual annotation of objects with numerical properties is possible, it is a hard and labor intensive task, and is impractical for dealing with the vast amount of objects of interest.", "labels": [], "entities": []}, {"text": "Hence, there is a need for automated semantic acquisition algorithms targeting such properties.", "labels": [], "entities": [{"text": "semantic acquisition", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.7863421440124512}]}, {"text": "In addition to answering direct questions, the ability to make a crude comparison or estimation of object attributes is important as well.", "labels": [], "entities": []}, {"text": "For example, it allows to disambiguate relationships between objects such as X part-of Y or X inside Y.", "labels": [], "entities": []}, {"text": "Thus, a coarse approximation of the height of a house and a window is sufficient to decide that in the 'house window' nominal compound, 'window' is very likely to be apart of house and not vice versa.", "labels": [], "entities": []}, {"text": "Such relationship information can, in turn, help summarization, machine translation or textual entailment tasks.", "labels": [], "entities": [{"text": "summarization", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.992165207862854}, {"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7720504403114319}, {"text": "textual entailment", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7224622368812561}]}, {"text": "Due to the importance of relationship and attribute acquisition in NLP, numerous methods were proposed for extraction of various lexical relationships and attributes from text.", "labels": [], "entities": [{"text": "relationship and attribute acquisition", "start_pos": 25, "end_pos": 63, "type": "TASK", "confidence": 0.6712888479232788}]}, {"text": "Some of these methods can be successfully used for extracting numerical attributes.", "labels": [], "entities": [{"text": "extracting numerical attributes", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.8401350975036621}]}, {"text": "However, numerical attribute extraction is substantially different in two aspects, verification and approximation.", "labels": [], "entities": [{"text": "numerical attribute extraction", "start_pos": 9, "end_pos": 39, "type": "TASK", "confidence": 0.7058395147323608}]}, {"text": "First, unlike most general lexical attributes, numerical attribute values are comparable.", "labels": [], "entities": []}, {"text": "It usually makes no sense to compare the names of two actors, but it is meaningful to compare their ages.", "labels": [], "entities": []}, {"text": "The ability to compare values of different objects allows to improve attribute extraction precision by verifying consistency with attributes of other similar objects.", "labels": [], "entities": [{"text": "attribute extraction", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.7017611861228943}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.8711025714874268}]}, {"text": "For example, suppose that for Toyota Corolla width we found two different values, 1.695m and 27cm.", "labels": [], "entities": []}, {"text": "The second value can be either an extraction error or a length of a toy car.", "labels": [], "entities": []}, {"text": "Extracting and looking at width values for different car brands and for 'cars' in general we find: \u2022 Boundaries: Maximal car width is 2.195m, minimal is 88cm.", "labels": [], "entities": [{"text": "width", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9565013647079468}, {"text": "Maximal car width", "start_pos": 113, "end_pos": 130, "type": "METRIC", "confidence": 0.6578655441602071}]}, {"text": "\u2022 Average: Estimated avg.", "labels": [], "entities": [{"text": "Average", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.9932356476783752}, {"text": "Estimated avg", "start_pos": 11, "end_pos": 24, "type": "METRIC", "confidence": 0.8719555139541626}]}, {"text": "\u2022 Direct/indirect comparisons: Toyota Corolla is wider than Toyota Corona.", "labels": [], "entities": []}, {"text": "\u2022 Distribution: Car width is distributed normally around the average.", "labels": [], "entities": [{"text": "Distribution", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.8928585648536682}, {"text": "width", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.6268826127052307}]}, {"text": "Usage of all this knowledge allows us to select the correct value of 1.695m and reject other values.", "labels": [], "entities": []}, {"text": "Thus we can increase the precision of value extraction by finding and analyzing an entire group of comparable objects.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9992150068283081}, {"text": "value extraction", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7755166590213776}]}, {"text": "Second, while it is usually meaningless and impossible to approximate general lexical attribute values like an actor's name, numerical attributes can be estimated even if they are not explicitly mentioned in the text.", "labels": [], "entities": []}, {"text": "In general, attribute extraction frameworks usually attempt to discover a single correct value (e.g., capital city of a country) or a set of distinct correct values (e.g., actors of a movie).", "labels": [], "entities": [{"text": "attribute extraction", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.7358071357011795}]}, {"text": "So there is essentially nothing to do when there is no explicit information present in the text fora given object and an attribute.", "labels": [], "entities": []}, {"text": "In contrast, in numerical attribute extraction it is possible to provide an approximation even when no explicit information is present in the text, by using values of comparable objects for which information is provided.", "labels": [], "entities": [{"text": "numerical attribute extraction", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.648894210656484}]}, {"text": "In this paper we present a pattern-based framework that takes advantage of the properties of similar objects to improve extraction precision and allow approximation of requested numerical object properties.", "labels": [], "entities": [{"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.991466760635376}]}, {"text": "Our framework comprises three main stages.", "labels": [], "entities": []}, {"text": "First, given an object name we utilize WordNet and pattern-based extraction to find a list of similar objects and their category labels.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9260445833206177}]}, {"text": "Second, we utilize a predefined set of lexical patterns in order to extract attribute values of these objects and available comparison/boundary information.", "labels": [], "entities": []}, {"text": "Finally, we analyze the obtained information and selector approximate the attribute value for the given (object, attribute) pair.", "labels": [], "entities": []}, {"text": "We performed a thorough evaluation using three different applications: Question Answering (QA), WordNet (WN) enrichment, and comparison with Wikipedia and answers provided by leading search engines.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.8587109208106994}, {"text": "WordNet (WN) enrichment", "start_pos": 96, "end_pos": 119, "type": "TASK", "confidence": 0.7803955554962159}]}, {"text": "QA evaluation was based on a designed dataset of 1250 questions on size, height, width, weight, and depth, for which we created a gold standard and compared against it automatically 1 . For WN enrichment evaluation, our framework discovered size and weight values for 300 WN physical objects, and the quality of results was evaluated by human judges.", "labels": [], "entities": [{"text": "QA", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7716108560562134}, {"text": "WN enrichment evaluation", "start_pos": 190, "end_pos": 214, "type": "TASK", "confidence": 0.9136799573898315}]}, {"text": "For interactive search, we compared our results to information obtained through Wikipedia, Google and Wolfram Alpha.", "labels": [], "entities": [{"text": "Wolfram Alpha", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.927344799041748}]}, {"text": "Utilization of information about comparable objects provided a significant boost to numerical attribute extraction quality, and allowed a meaningful approximation of missing attribute values.", "labels": [], "entities": [{"text": "numerical attribute extraction", "start_pos": 84, "end_pos": 114, "type": "TASK", "confidence": 0.6289136509100596}]}, {"text": "Section 2 discusses related work, Section 3 details the algorithmic framework, Section 4 describes the experimental setup, and Section 5 presents our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed automated question answering (QA) evaluation, human-based WN enrichment evaluation, and human-based comparison of our results to data available through Wikipedia and to the top results of leading search engines.", "labels": [], "entities": [{"text": "question answering (QA) evaluation", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.8594590723514557}, {"text": "WN enrichment evaluation", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.9636437694231669}]}, {"text": "In order to test the main system components, we ran our framework under five different conditions: \u2022 FULL: All system components were used.", "labels": [], "entities": [{"text": "FULL", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9994082450866699}]}, {"text": "\u2022 DIRECT: Only direct pattern-based acquisition of attribute values (Section 3.2, value extraction) for the given object was used, as done inmost general-purpose attribute acquisition systems.", "labels": [], "entities": [{"text": "DIRECT", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9572361707687378}, {"text": "value extraction", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.749538779258728}, {"text": "general-purpose attribute acquisition", "start_pos": 146, "end_pos": 183, "type": "TASK", "confidence": 0.7103005250295004}]}, {"text": "If several values were extracted, the most common value was used as an answer.", "labels": [], "entities": []}, {"text": "\u2022 NOCB: No boundary and no comparison data were collected and processed (P compare and P bounds were empty).", "labels": [], "entities": [{"text": "NOCB", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9922974705696106}]}, {"text": "We only collected and processed a set of values for the similar objects.", "labels": [], "entities": []}, {"text": "\u2022 NOB: As in FULL but no boundary data was collected and processed (P bounds was empty).", "labels": [], "entities": [{"text": "NOB", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9981826543807983}, {"text": "FULL", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.49756696820259094}]}, {"text": "\u2022 NOC: As in FULL but no comparison data was collected and processed (P compare was empty).", "labels": [], "entities": [{"text": "NOC", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9981423616409302}, {"text": "FULL", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9047819375991821}, {"text": "P compare", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.8975262641906738}]}, {"text": "We created two QA datasets, Web and TREC based.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.7307705879211426}]}, {"text": "We created QA datasets for size, height, width, weight, and depth attributes.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.7311491072177887}]}, {"text": "For each attribute we extracted from the Web 250 questions in the following way.", "labels": [], "entities": []}, {"text": "First, we collected several thousand questions, querying for the following patterns: \"How long/tall/wide/heavy/deep/high is\",\"What is the size/width/height/depth/weight of\".", "labels": [], "entities": []}, {"text": "Then we manually filtered out non-questions and heavily context-specific questions, e.g., \"what is the width of the triangle\".", "labels": [], "entities": []}, {"text": "Next, we retained only a single question for each entity by removing duplicates.", "labels": [], "entities": []}, {"text": "For each of the extracted questions we manually assigned a gold standard answer using trusted resources including books and reliable Web data.", "labels": [], "entities": []}, {"text": "For some questions, the exact answer is the only possible one (e.g., the height of a person), while for others it is only the center of a distribution (e.g., the weight of a coffee cup).", "labels": [], "entities": [{"text": "exact answer", "start_pos": 24, "end_pos": 36, "type": "METRIC", "confidence": 0.9393840432167053}]}, {"text": "Questions with no trusted and exact answers were eliminated.", "labels": [], "entities": []}, {"text": "From the remaining questions we randomly selected 250 questions for each attribute.", "labels": [], "entities": []}, {"text": "As a small complementary dataset we used relevant questions from the TREC Question Answering.", "labels": [], "entities": [{"text": "TREC Question Answering", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.8466609716415405}]}, {"text": "From 4355 questions found in this set we collected 55 (17 size, 2 weight, 3 width, 3 depth and 30 height) questions.", "labels": [], "entities": []}, {"text": "Some example questions from our datasets are (correct answers are in parentheses): How tall is Michelle Obama?", "labels": [], "entities": []}, {"text": "(180cm); How tall is the tallest penguin?", "labels": [], "entities": []}, {"text": "(122cm); What is the height of a tennis net?", "labels": [], "entities": []}, {"text": "(92cm); What is the depth of the Nile river?", "labels": [], "entities": []}, {"text": "(1000cm = 10 meters); How heavy is a cup of coffee?", "labels": [], "entities": []}, {"text": "(360gr); How heavy is a giraffe?; What is the width of a DNA molecule?", "labels": [], "entities": []}, {"text": "(2e-7cm); What is the width of a cow?", "labels": [], "entities": [{"text": "width", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9668102264404297}]}, {"text": "Evaluation against the datasets was done automatically.", "labels": [], "entities": []}, {"text": "For each question and each condition our framework returned a numerical value marked as either an exact answer or as an approximation.", "labels": [], "entities": []}, {"text": "In cases where no data was found for an approximation (no similar objects with values were found), our framework returned no answer.", "labels": [], "entities": []}, {"text": "We computed precision 4 , comparing results to the gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.996343195438385}]}, {"text": "Approximate answers are considered to be correct if the approximation is within 10% of the gold standard value.", "labels": [], "entities": []}, {"text": "While a choice of 10% maybe too strict for some applications and too generous for others, it still allows to estimate the quality of our framework.", "labels": [], "entities": []}, {"text": "We manually selected 300 WN entities from about 1000 randomly selected objects below the object tree in WN, by filtering out entities that clearly do not possess any of the addressed numerical attributes.", "labels": [], "entities": []}, {"text": "Evaluation was done using human subjects.", "labels": [], "entities": []}, {"text": "It is difficult to do an automated evaluation, since the nature of the data is different from that of the QA dataset.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 106, "end_pos": 116, "type": "DATASET", "confidence": 0.9032596945762634}]}, {"text": "Most of the questions asked over the Web target named entities like specific car brands, places and actors.", "labels": [], "entities": []}, {"text": "There is usually little or no variability in attribute values of such objects, and the major source of extraction errors is name ambiguity of the requested objects.", "labels": [], "entities": []}, {"text": "WordNet physical objects, in contrast, are much less specific and their attributes such assize and weight rarely have a single correct value, but usually possess an acceptable numerical range.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.937127411365509}]}, {"text": "For example, the majority of the selected objects like 'apple' are too general to assign an exact size.", "labels": [], "entities": []}, {"text": "Also, it is unclear how to define acceptable values and an approximation range.", "labels": [], "entities": []}, {"text": "Crudeness of desired approximation depends both on potential applications and on object type.", "labels": [], "entities": []}, {"text": "Some objects show much greater variability in size (and hence a greater range of acceptable approximations) than others.", "labels": [], "entities": []}, {"text": "This property of the dataset makes it difficult to provide a meaningful gold standard for the evaluation.", "labels": [], "entities": []}, {"text": "Hence in order to estimate the quality of our results we turn to an evaluation based on human judges.", "labels": [], "entities": []}, {"text": "In this evaluation we use only approximate retrieved values, keeping out the small amount of returned exact values 5 . We have mixed (Object, Attribute name, Attribute value) triplets obtained through each of the conditions, and asked human subjects to assign these to one of the following categories: \u2022 The attribute value is reasonable for the given object.", "labels": [], "entities": []}, {"text": "\u2022 The value is a very crude approximation of the given object attribute.", "labels": [], "entities": []}, {"text": "\u2022 The value is incorrect or clearly misleading.", "labels": [], "entities": []}, {"text": "\u2022 The object is not familiar enough tome so I cannot answer the question.", "labels": [], "entities": []}, {"text": "Each evaluator was provided with a random sample of 40 triplets.", "labels": [], "entities": []}, {"text": "In addition we mixed in 5 manually created clearly correct triplets and 5 clearly incorrect ones.", "labels": [], "entities": []}, {"text": "We used five subjects, and the agreement (inter-annotator Kappa) on shared evaluated triplets was 0.72.", "labels": [], "entities": [{"text": "inter-annotator Kappa)", "start_pos": 42, "end_pos": 64, "type": "METRIC", "confidence": 0.7627869645754496}]}], "tableCaptions": [{"text": " Table 2: Percentage of exact and approximate values for the", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9752933979034424}, {"text": "exact", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9753442406654358}]}, {"text": " Table 3: Human evaluation of approximations for the WN", "labels": [], "entities": [{"text": "WN", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.5534837245941162}]}, {"text": " Table 4: Comparison of our attribute extraction framework", "labels": [], "entities": [{"text": "attribute extraction", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7700133323669434}]}]}