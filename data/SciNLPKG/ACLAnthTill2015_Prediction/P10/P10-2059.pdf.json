{"title": [{"text": "Classification of Feedback Expressions in Multimodal Data", "labels": [], "entities": [{"text": "Classification of Feedback Expressions", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7941590249538422}]}], "abstractContent": [{"text": "This paper addresses the issue of how linguistic feedback expressions, prosody and head gestures, i.e. head movements and face expressions, relate to one another in a collection of eight video-recorded Dan-ish map-task dialogues.", "labels": [], "entities": []}, {"text": "The study shows that in these data, prosodic features and head gestures significantly improve automatic classification of dialogue act labels for linguistic expressions of feedback.", "labels": [], "entities": [{"text": "automatic classification of dialogue act labels", "start_pos": 94, "end_pos": 141, "type": "TASK", "confidence": 0.6818955689668655}]}], "introductionContent": [{"text": "Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see for an overview).", "labels": [], "entities": []}, {"text": "Others have looked at the application of machine learning algorithms to annotated multimodal corpora.", "labels": [], "entities": []}, {"text": "For example, and find that machine learning algorithms can be trained to recognise some of the functions of head movements, while show that there is a dependence between focus of attention and assignment of dialogue act labels.", "labels": [], "entities": []}, {"text": "Related are also the studies by Rieks op den Akker and and: both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.", "labels": [], "entities": [{"text": "automatic segmentation of dialogue acts", "start_pos": 98, "end_pos": 137, "type": "TASK", "confidence": 0.744742625951767}]}, {"text": "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.", "labels": [], "entities": []}, {"text": "obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. and study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues and find correlations between the various modalities both within and across speakers.", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.6339520514011383}]}, {"text": "Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (.", "labels": [], "entities": []}, {"text": "Our work is inline with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.", "labels": [], "entities": []}, {"text": "In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.", "labels": [], "entities": []}, {"text": "Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.", "labels": [], "entities": []}, {"text": "We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.", "labels": [], "entities": [{"text": "classification of dialogue acts", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.8535777181386948}, {"text": "semantic interpretation of feedback expressions", "start_pos": 135, "end_pos": 182, "type": "TASK", "confidence": 0.8022032499313354}]}, {"text": "The results, which partly confirm those obtained on a smaller dataset in, must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.", "labels": [], "entities": []}, {"text": "The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.", "labels": [], "entities": [{"text": "head nods and jerks", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7656855955719948}]}, {"text": "In Section 2 we describe the multimodal Danish corpus.", "labels": [], "entities": [{"text": "Danish corpus", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.8607441186904907}]}, {"text": "In Section 3, we describe how the prosody of feedback expressions is annotated, how their content is coded in terms of dialogue act, turn and agreement labels, and we provide inter-coder agreement measures.", "labels": [], "entities": []}, {"text": "In Section 4 we account for the annotation of head gestures, including inter-coder agreements results.", "labels": [], "entities": []}, {"text": "Section 5 contains a description of the resulting datasets and a discussion of the results obtained in the classification experiments.", "labels": [], "entities": []}, {"text": "Section 6 is the conclusion.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1 shows how the  various types are distributed across the 466 feed- back expressions in our data.", "labels": [], "entities": []}, {"text": " Table 2: Inter-coder agreement on feedback ex- pression annotation", "labels": [], "entities": []}, {"text": " Table 3: Inter-coder agreement on head gesture  annotation", "labels": [], "entities": [{"text": "head gesture  annotation", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.7545674244562784}]}, {"text": " Table 4: Yes and No datasets", "labels": [], "entities": []}, {"text": " Table 5: Classification results with prosodic fea- tures", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9499651193618774}]}, {"text": " Table 6: Classification results with head gesture  features", "labels": [], "entities": []}, {"text": " Table 7: Classification results with fewer head  movements", "labels": [], "entities": []}]}