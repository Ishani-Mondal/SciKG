{"title": [{"text": "A Hybrid Hierarchical Model for Multi-Document Summarization", "labels": [], "entities": [{"text": "Multi-Document Summarization", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.6562885493040085}]}], "abstractContent": [{"text": "Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document sum-marization.", "labels": [], "entities": [{"text": "Scoring sentences in documents given abstract summaries created by humans", "start_pos": 0, "end_pos": 73, "type": "TASK", "confidence": 0.8360660314559937}]}, {"text": "In this paper, we formulate ex-tractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference.", "labels": [], "entities": [{"text": "pattern discovery", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.76917564868927}]}, {"text": "We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model.", "labels": [], "entities": []}, {"text": "Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary.", "labels": [], "entities": []}, {"text": "Our system advances current state-of-the-art improving ROUGE scores by \u223c7%.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 55, "end_pos": 67, "type": "METRIC", "confidence": 0.9401915967464447}]}, {"text": "Generated summaries are less redundant and more coherent based upon manual quality evaluations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Extractive approach to multi-document summarization (MDS) produces a summary by selecting sentences from original documents.", "labels": [], "entities": [{"text": "multi-document summarization (MDS)", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.8365684628486634}]}, {"text": "Document Understanding Conferences (DUC), now TAC, fosters the effort on building MDS systems, which take document clusters (documents on a same topic) and description of the desired summary focus as input and output a word length limited summary.", "labels": [], "entities": [{"text": "Document Understanding Conferences (DUC)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6754839917023977}]}, {"text": "Human summaries are provided for training summarization models and measuring the performance of machine generated summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.8855431079864502}]}, {"text": "Extractive summarization methods can be classified into two groups: supervised methods that rely on provided document-summary pairs, and unsupervised methods based upon properties derived from document clusters.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8791162669658661}]}, {"text": "Supervised methods treat the summarization task as a classification/regression problem, e.g.,).", "labels": [], "entities": [{"text": "summarization task", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.919201523065567}]}, {"text": "Each candidate sentence is classified as summary or non-summary based on the features that they pose and those with highest scores are selected.", "labels": [], "entities": []}, {"text": "Unsupervised methods aim to score sentences based on semantic groupings extracted from documents, e.g.,, etc.", "labels": [], "entities": []}, {"text": "Such models can yield comparable or better performance on DUC and other evaluations, since representing documents as topic distributions rather than bags of words diminishes the effect of lexical variability.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there is no previous research which utilizes the best features of both approaches for MDS as presented in this paper.", "labels": [], "entities": [{"text": "MDS", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9613425731658936}]}, {"text": "In this paper, we present a novel approach that formulates MDS as a prediction problem based on a two-step hybrid model: a generative model for hierarchical topic discovery and a regression model for inference.", "labels": [], "entities": [{"text": "formulates MDS", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.7800832092761993}, {"text": "hierarchical topic discovery", "start_pos": 144, "end_pos": 172, "type": "TASK", "confidence": 0.675233781337738}]}, {"text": "We investigate if a hierarchical model can be adopted to discover salient characteristics of sentences organized into hierarchies utilizing human generated summary text.", "labels": [], "entities": []}, {"text": "We present a probabilistic topic model on sentence level building on hierarchical Latent Dirichlet Allocation (hLDA) (), which is a generalization of LDA ().", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (hLDA)", "start_pos": 82, "end_pos": 116, "type": "METRIC", "confidence": 0.8231648802757263}]}, {"text": "We construct a hybrid learning algorithm by extracting salient features to characterize summary sentences, and implement a regression model for inference.", "labels": [], "entities": []}, {"text": "Contributions of this work are: \u2212 construction of hierarchical probabilistic model designed to discover the topic structures of all sentences.", "labels": [], "entities": []}, {"text": "Our focus is on identifying similarities of candidate sentences to summary sentences using a novel tree based sentence scoring algorithm, concerning topic distributions at different levels of the discovered hierarchy as described in \u00a7 3 and \u00a7 4, \u2212 representation of sentences by meta-features to characterize their candidacy for inclusion in summary text.", "labels": [], "entities": []}, {"text": "Our aim is to find features that can best represent summary sentences as described in \u00a7 5, \u2212 implementation of a feasible inference method based on a regression model to enable scoring of sentences in test document clusters without retraining, (which has not been investigated in generative summarization models) described in \u00a7 5.2.", "labels": [], "entities": [{"text": "generative summarization", "start_pos": 280, "end_pos": 304, "type": "TASK", "confidence": 0.9115982055664062}]}, {"text": "We show in \u00a7 6 that our hybrid summarizer achieves comparable (if not better) ROUGE score on the challenging task of extracting the summaries of multiple newswire documents.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 78, "end_pos": 89, "type": "METRIC", "confidence": 0.9780134856700897}, {"text": "extracting the summaries of multiple newswire documents", "start_pos": 117, "end_pos": 172, "type": "TASK", "confidence": 0.8241676603044782}]}, {"text": "The human evaluations confirm that our hybrid model can produce coherent and non-redundant summaries.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe a number of experiments using our hybrid model on 100 document clusters each containing 25 news articles from DUC2005-2006 tasks.", "labels": [], "entities": [{"text": "DUC2005-2006", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.8830835223197937}]}, {"text": "We evaluate the performance of HybHSum using 45 document clusters each containing 25 news articles from DUC2007 task.", "labels": [], "entities": [{"text": "HybHSum", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.8885127305984497}, {"text": "DUC2007 task", "start_pos": 104, "end_pos": 116, "type": "DATASET", "confidence": 0.9043761193752289}]}, {"text": "From these sets, we collected 80K and 25K sentences to compile training and testing data respectively.", "labels": [], "entities": []}, {"text": "The task is to create max.", "labels": [], "entities": []}, {"text": "250 word long summary for each document cluster.", "labels": [], "entities": []}, {"text": "We use Gibbs sampling for inference in hLDA and sumHLDA.", "labels": [], "entities": [{"text": "sumHLDA", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.7503054738044739}]}, {"text": "The hLDA is used to capture abstraction and specificity of words in documents ().", "labels": [], "entities": []}, {"text": "Contrary to typical hLDA models, to efficiently represent sentences in summarization task, we set ascending values for Dirichlet hyper-parameter \u03b7 as the level increases, encouraging mid to low level distributions to generate as many words as in higher levels, e.g., fora tree of depth=3, \u03b7 = {0.125, 0.5, 1}.", "labels": [], "entities": [{"text": "summarization", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.9688948392868042}]}, {"text": "This causes sentences share paths only when they include similar concepts, starting higher level topics of the tree.", "labels": [], "entities": []}, {"text": "For SVR, we set = 0.1 using the default choice, which is the inverse of the average of \u03c6(f) T \u03c6(f), dot product of kernelized input vectors.", "labels": [], "entities": []}, {"text": "We use greedy optimization during training based on ROUGE scores to find best regularizer C = 10 \u22121 ..10 2 using the Gaussian kernel.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9695791602134705}]}, {"text": "We applied feature extraction of \u00a7 5.1 to compile the training and testing datasets.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.6912487894296646}]}, {"text": "ROUGE is used for performance measure (), which evaluates summaries based on the maxium number of overlapping units between generated summary text and a set of human summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8943964838981628}]}, {"text": "We use R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams).", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9850956797599792}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9557841420173645}]}, {"text": "Experiment 1: sumHLDA Parameter Analysis: In sumHLDA we introduce a prior different than the standard nested CRP (nCRP).", "labels": [], "entities": [{"text": "sumHLDA Parameter Analysis", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.5733994245529175}]}, {"text": "Here, we illustrate that this prior is practical in learning hierarchical topics for summarization task.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.9202496409416199}]}, {"text": "We use sentences from the human generated summaries during the discovery of hierarchical topics of sentences in document clusters.", "labels": [], "entities": []}, {"text": "Since summary sentences generally contain abstract words, they are indicative of sentences in documents and should produce minimal amount of new topics (if not none).", "labels": [], "entities": []}, {"text": "To implement this, in nCRP prior of sumHLDA, we use dual hyper-parameters and choose a very small value for summary sentences, \u03b3 s = 10e \u22124 \u03b3 o . We compare the results to hLDA) with nCRP prior which uses only one free parameter, \u03b3.", "labels": [], "entities": [{"text": "sumHLDA", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.8038852214813232}]}, {"text": "To analyze this prior, we generate a corpus of 1300 sentences of a document cluster in DUC2005.", "labels": [], "entities": [{"text": "DUC2005", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9654909372329712}]}, {"text": "We repeated the experiment for 9 other clusters of similar size and averaged the total number of generated topics.", "labels": [], "entities": []}, {"text": "We show results for different values of \u03b3 and \u03b3 o hyper-parameters and tree depths.", "labels": [], "entities": []}, {"text": "sumHLDA and hLDA for different \u03b3 and \u03b3o and tree depths.", "labels": [], "entities": [{"text": "sumHLDA", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5568011999130249}]}, {"text": "\u03b3s = 10e \u22124 is used for sumHLDA for each depth.", "labels": [], "entities": [{"text": "sumHLDA", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9802979826927185}]}, {"text": "We use the following multi-document summarization models along with the Baseline presented in Experiment 2 to evaluate HybSumm.", "labels": [], "entities": [{"text": "HybSumm", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.7103180289268494}]}, {"text": "PYTHY : (Toutanova et al., 2007) A stateof-the-art supervised summarization system that ranked first in overall ROUGE evaluations in DUC2007.", "labels": [], "entities": [{"text": "PYTHY", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.526358962059021}, {"text": "summarization", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.7884379029273987}, {"text": "DUC2007", "start_pos": 133, "end_pos": 140, "type": "DATASET", "confidence": 0.8562464714050293}]}, {"text": "Similar to HybHSum, human generated summaries are used to train a sentence ranking system using a classifier model.", "labels": [], "entities": []}, {"text": "HIERSUM : (Haghighi and Vanderwende, 2009) A generative summarization method based on topic models, which uses sentences as an additional level.", "labels": [], "entities": [{"text": "HIERSUM", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7373564839363098}, {"text": "generative summarization", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.9510107636451721}]}, {"text": "Using an approximation for inference, sentences are greedily added to a summary so long as they decrease KL-divergence.", "labels": [], "entities": []}, {"text": "HybFSum (Hybrid Flat Summarizer): To investigate the performance of hierarchical topic model, we build another hybrid model using flat LDA ().", "labels": [], "entities": []}, {"text": "In LDA each sentence is a superposition of all K topics with sentence specific weights, there is no hierarchical relation between topics.", "labels": [], "entities": []}, {"text": "We keep the parameters and the features of the regression model of hierarchical HybHSum intact for consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 99, "end_pos": 110, "type": "METRIC", "confidence": 0.970491349697113}]}, {"text": "We only change the sentence scoring method.", "labels": [], "entities": [{"text": "sentence scoring", "start_pos": 19, "end_pos": 35, "type": "TASK", "confidence": 0.7114192098379135}]}, {"text": "Instead of the new tree-based sentence scoring ( \u00a7 4), we present a similar method using topics from LDA on sentence level.", "labels": [], "entities": []}, {"text": "Note that in LDA the topic-word distributions \u03c6 are over entire vocabulary, and topic mixing proportions for sentences \u03b8 are overall the topics discovered from sentences in a document cluster.", "labels": [], "entities": []}, {"text": "Hence, we define sim 1 and sim 2 measures for LDA using topic-word proportions \u03c6 (in place of discrete topic-word distributions from each level in Eq.2) and topic mixing weights \u03b8 in sentences (in place of topic proportions in Eq.3) respectively.", "labels": [], "entities": [{"text": "Eq.3", "start_pos": 227, "end_pos": 231, "type": "DATASET", "confidence": 0.9618724584579468}]}, {"text": "Maximum matching score is calculated as same as in HybHSum.", "labels": [], "entities": [{"text": "matching score", "start_pos": 8, "end_pos": 22, "type": "METRIC", "confidence": 0.963849812746048}, {"text": "HybHSum", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9205625653266907}]}, {"text": "The HybHSum 2 achieves the best performance on R-1 and R-4 and comparable on R-2.", "labels": [], "entities": []}, {"text": "When stop words are used the HybHSum 2 outperforms stateof-the-art by 2.5-7% except R-2 (with statistical significance).", "labels": [], "entities": []}, {"text": "Note that R-2 is a measure of bigram recall and sumHLDA of HybHSum 2 is built on unigrams rather than bigrams.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9742670655250549}, {"text": "sumHLDA", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9888749122619629}]}, {"text": "Compared to the HybFSum built on LDA, both HybHSum 1&2 yield better performance indicating the effectiveness of using hierarchical topic model in summarization task.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.9073337316513062}]}, {"text": "HybHSum 2 appear to be less redundant than HybFSum capturing not only common terms but also specific words in, due to the new hierarchical tree-based sentence scoring which characterizes sentences on deeper level.", "labels": [], "entities": []}, {"text": "Similarly, HybHSum 1&2 far exceeds baseline built on simple classifier.", "labels": [], "entities": []}, {"text": "The results justify the performance gain by using our novel tree-based scoring method.", "labels": [], "entities": []}, {"text": "Although the ROUGE scores for HybHSum 1 and HybHSum 2 are not significantly different, the sumHLDA is more suitable for summarization tasks than hLDA.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9971810579299927}, {"text": "summarization tasks", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.9257120788097382}]}, {"text": "HybHSum 2 is comparable to (if not better than) fully generative HIERSUM.", "labels": [], "entities": [{"text": "generative HIERSUM", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7725244760513306}]}, {"text": "This indicates that with our regression model built on training data, summaries can be efficiently generated for test documents (suitable for online systems).", "labels": [], "entities": []}, {"text": "Experiment 4: Manual Evaluations Here, we manually evaluate quality of summaries, a common DUC task.", "labels": [], "entities": []}, {"text": "Human annotators are given two sets of summary text for each document set, generated from two approaches: best hierarchical hybrid HybHSum 2 and flat hybrid HybFSum models, and are asked to mark the better summary New federal rules for organic food will assure consumers that the products are grown and processed to the same standards nationwide.", "labels": [], "entities": []}, {"text": "But as sales grew more than 20 percent a year through the 1990s, organic food came to account for $1 of every $100 spent on food, and in 1997 t he a g en c y to o kn o ti c e , proposing national organic standards for all food.", "labels": [], "entities": []}, {"text": "The Agriculture Department began to propose standards for all organic foods in the late 1990's because their sale had grown more than 20 percent a year in that decade.", "labels": [], "entities": []}, {"text": "In January 1999 the USDA approved a \"certified organic\" label for meats and poultry that were raised without growth hormones, pesticide-treated feed, and antibiotics.", "labels": [], "entities": []}, {"text": "Output   Results are statistically significant based on t-test.", "labels": [], "entities": [{"text": "Output", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9870216250419617}]}, {"text": "T ie indicates evaluations where two summaries are rated equal.", "labels": [], "entities": []}, {"text": "according to five criteria: non-redundancy (which summary is less redundant), coherence (which summary is more coherent), focus and readability (content and not include unnecessary details), responsiveness and overall performance.", "labels": [], "entities": []}, {"text": "We asked 4 annotators to rate DUC2007 predicted summaries (45 summary pairs per annotator).", "labels": [], "entities": [{"text": "DUC2007", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.8479335308074951}]}, {"text": "A total of 92 pairs are judged and evaluation results in frequencies are shown in.", "labels": [], "entities": []}, {"text": "The participants rated HybHSum 2 generated summaries more coherent and focused compared to HybFSum.", "labels": [], "entities": [{"text": "HybFSum", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.9272972345352173}]}, {"text": "All results in are statistically significant (based on t-test on 95% confidence level.) indicating that HybHSum 2 summaries are rated significantly better.", "labels": [], "entities": [{"text": "HybHSum 2 summaries", "start_pos": 104, "end_pos": 123, "type": "DATASET", "confidence": 0.7204472025235494}]}], "tableCaptions": [{"text": " Table 1: Average # of topics per document cluster from", "labels": [], "entities": []}, {"text": " Table 2: ROUGE results (with stop-words) on DUC2006", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9893321990966797}, {"text": "DUC2006", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9061600565910339}]}, {"text": " Table 3: ROUGE results of the best systems on  DUC2007 dataset (best results are bolded.)", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9841646552085876}, {"text": "DUC2007 dataset", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9856739640235901}]}, {"text": " Table 4: Frequency results of manual quality evaluations.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9444787502288818}]}]}