{"title": [{"text": "Minimized models and grammar-informed initialization for supertagging with highly ambiguous lexicons", "labels": [], "entities": []}], "abstractContent": [{"text": "We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming.", "labels": [], "entities": []}, {"text": "Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.9538368582725525}, {"text": "CCG-TUT corpora", "start_pos": 166, "end_pos": 181, "type": "DATASET", "confidence": 0.8262714743614197}]}, {"text": "The strategies provide further error reductions when combined.", "labels": [], "entities": []}, {"text": "We describe anew two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.", "labels": [], "entities": []}], "introductionContent": [{"text": "Creating accurate part-of-speech (POS) taggers using a tag dictionary and unlabeled data is an interesting task with practical applications.", "labels": [], "entities": [{"text": "part-of-speech (POS) taggers", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.6522995471954346}]}, {"text": "It has been explored at length in the literature since, though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora.", "labels": [], "entities": []}, {"text": "Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage.", "labels": [], "entities": []}, {"text": "Most work has focused on POS-tagging for English using the Penn Treebank (, such as (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.996414452791214}]}, {"text": "This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.9928971529006958}]}, {"text": "The most ambiguous word has 7 different POS tags associated with it.", "labels": [], "entities": []}, {"text": "Most methods have employed some variant of Expectation Maximization (EM) to learn parameters fora bigram or trigram Hidden Markov Model (HMM).", "labels": [], "entities": []}, {"text": "achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.8842107057571411}]}, {"text": "A more challenging task is learning supertaggers for lexicalized grammar formalisms such as Combinatory Categorial Grammar (CCG)).", "labels": [], "entities": []}, {"text": "For example, CCGbank) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags.", "labels": [], "entities": [{"text": "CCGbank)", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.9350121915340424}]}, {"text": "This provides a much more challenging starting point for the semi-supervised methods typically applied to the task.", "labels": [], "entities": []}, {"text": "Yet, this is an important task since creating grammars and resources for CCG parsers for new domains and languages is highly labor-and knowledge-intensive.", "labels": [], "entities": []}, {"text": "uses grammar-informed initialization for HMM tag transitions based on the universal combinatory rules of the CCG formalism to obtain 56.1% accuracy on ambiguous word tokens, a large improvement over the 33.0% accuracy obtained with uniform initialization for tag transitions.", "labels": [], "entities": [{"text": "HMM tag transitions", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8168119390805563}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9977860450744629}, {"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.9850913882255554}]}, {"text": "The strategies employed in and are complementary.", "labels": [], "entities": []}, {"text": "The former reduces the model size globally given a data set, while the latter biases bitag transitions toward those which are more likely based on a universal grammar without reference to any data.", "labels": [], "entities": []}, {"text": "In this paper, we show how these strategies maybe combined straightforwardly to produce improvements on the task of learning supertaggers from lexicons that have not been filtered in anyway.", "labels": [], "entities": []}, {"text": "We demonstrate their cross-lingual effectiveness on CCGbank (English) and the Italian CCG-TUT corpus ().", "labels": [], "entities": [{"text": "Italian CCG-TUT corpus", "start_pos": 78, "end_pos": 100, "type": "DATASET", "confidence": 0.7943230072657267}]}, {"text": "We find a consistent improved performance by using each of the methods compared to basic EM, and further improvements by using them in combination.", "labels": [], "entities": []}, {"text": "Applying the approach of naively to CCG supertagging is intractable due to the high level of ambiguity.", "labels": [], "entities": []}, {"text": "We deal with this by defining anew two-stage integer programming formulation that identifies minimal grammars efficiently and effectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the four strategies described in Sections 3 and 4, summarized below: EM HMM uniformly initialized, EM training.", "labels": [], "entities": []}, {"text": "EM+IP IP minimization using initial grammar provided by EM.", "labels": [], "entities": [{"text": "EM+IP IP minimization", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8196171283721924}, {"text": "EM", "start_pos": 56, "end_pos": 58, "type": "DATASET", "confidence": 0.9504204988479614}]}, {"text": "EM GI HMM with grammar-informed initialization, EM training.", "labels": [], "entities": [{"text": "EM GI HMM", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7955301403999329}]}, {"text": "EM GI +IP GI IP minimization using initial grammar/lexicon provided by EM GI and additional grammar-informed IP objective.", "labels": [], "entities": [{"text": "EM", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.9429108500480652}, {"text": "IP minimization", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.6826227009296417}, {"text": "EM GI", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.9050793349742889}]}, {"text": "For EM+IP and EM GI +IP GI , the minimization and EM training processes are iterated until the resulting grammar and lexicon remain unchanged.", "labels": [], "entities": []}, {"text": "Forty EM iterations are used for all cases.", "labels": [], "entities": []}, {"text": "We also include a baseline which randomly chooses a tag from those associated with each word in the lexicon, averaged over three runs.", "labels": [], "entities": []}, {"text": "Accuracy on ambiguous word tokens.", "labels": [], "entities": []}, {"text": "We evaluate the performance in terms of tagging accuracy with respect to gold tags for ambiguous words in held-out test sets for English and Italian.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9813639521598816}]}, {"text": "We consider results with and without punctuation.", "labels": [], "entities": []}, {"text": "Recall that unlike much previous work, we do not collect the lexicon (tag dictionary) from the test set: this means the model must handle unknown words and the possibility of having missing lexical entries for covering the test set.", "labels": [], "entities": []}, {"text": "Precision and recall of grammar and lexicon.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8805133700370789}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9782700538635254}]}, {"text": "In addition to accuracy, we measure precision and able/combinable bigrams.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9994389414787292}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9995406866073608}]}, {"text": "The reason for this is that the \"categories\" for punctuation in CCGbank are for the most part not actual categories; for example, the period \".\" has the categories \".\" and \"S\".", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9433227777481079}]}, {"text": "As such, these supertags are outside of the categorial system: their use in derivations requires phrase structure rules that are not derivable from the CCG combinatory rules.", "labels": [], "entities": []}, {"text": "recall for each model on the observed bitag grammar and observed lexicon on the test set.", "labels": [], "entities": []}, {"text": "We calculate them as follows, for an observed grammar or lexicon X: This provides a measure of model performance on bitag types for the grammar and lexical entry types for the lexicon, rather than tokens.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for the training data used to ex- tract lexicons for CCGbank and CCG-TUT. Dis- tinct: # of distinct lexical categories; Max: # of  categories for the most ambiguous word; Type  ambig: per word type category ambiguity; Tok  ambig: per word token category ambiguity.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9208539128303528}]}, {"text": " Table 2: Supertagging accuracy for CCGbank sec- tions 22-24. Accuracies are reported for four  settings-(1) ambiguous word tokens in the test  corpus, (2) ambiguous word tokens, ignoring  punctuation, (3) all word tokens, and (4) all word  tokens except punctuation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9593501687049866}, {"text": "Accuracies", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9983593821525574}]}, {"text": " Table 3: Comparison of grammar/lexicon ob- served in the model tagging vs. gold tagging  in terms of precision and recall measures for su- pertagging on CCGbank data.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9991826415061951}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9987542629241943}, {"text": "CCGbank data", "start_pos": 154, "end_pos": 166, "type": "DATASET", "confidence": 0.9822192490100861}]}, {"text": " Table 3. The basic EM  model has very low precision for the grammar, in- dicating it proposes many unnecessary bitags; it achieves better recall because of the sheer num- ber of bitags it proposes", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9987792372703552}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9985861778259277}]}, {"text": " Table 4: Comparison of tag assignments from the gold tags versus model tags obtained on the test set.  The table shows tag assignments (and their counts for each method) for the and in in the CCGbank test  sections. The number of distinct tags assigned by each method is given in parentheses. L train is the  lexicon obtained from sections 0-18 of CCGbank that is used as the basis for EM training.", "labels": [], "entities": [{"text": "CCGbank test  sections", "start_pos": 193, "end_pos": 215, "type": "DATASET", "confidence": 0.9379610220591227}, {"text": "CCGbank", "start_pos": 349, "end_pos": 356, "type": "DATASET", "confidence": 0.9706956148147583}, {"text": "EM training", "start_pos": 387, "end_pos": 398, "type": "TASK", "confidence": 0.9139431416988373}]}, {"text": " Table 5: Comparison of supertagging results for  CCG-TUT. Accuracies are for ambiguous word  tokens in the test corpus, ignoring punctuation.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9933531284332275}]}, {"text": " Table 6: Comparison of grammar/lexicon ob- served in the model tagging vs. gold tagging  in terms of precision and recall measures for su- pertagging on CCG-TUT.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9991806149482727}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9985587000846863}, {"text": "CCG-TUT", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.9616735577583313}]}]}