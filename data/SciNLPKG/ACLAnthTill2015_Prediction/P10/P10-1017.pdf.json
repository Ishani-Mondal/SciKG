{"title": [], "abstractContent": [{"text": "We present a simple yet powerful hierarchical search algorithm for automatic word alignment.", "labels": [], "entities": [{"text": "automatic word alignment", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6066596706708273}]}, {"text": "Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list.", "labels": [], "entities": []}, {"text": "We score a given alignment within the forest with a flexible, linear discrimina-tive model incorporating hundreds of features , and trained on a relatively small amount of annotated data.", "labels": [], "entities": []}, {"text": "We report results on Arabic-English word alignment and translation tasks.", "labels": [], "entities": [{"text": "Arabic-English word alignment", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.6410957276821136}, {"text": "translation tasks", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.8542999029159546}]}, {"text": "Our model out-performs a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9814432263374329}, {"text": "BLEU score", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9791302382946014}, {"text": "machine translation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.721893236041069}]}], "introductionContent": [{"text": "Automatic word alignment is generally accepted as a first step in training any statistical machine translation system.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6856150180101395}, {"text": "statistical machine translation", "start_pos": 79, "end_pos": 110, "type": "TASK", "confidence": 0.6334024667739868}]}, {"text": "It is a vital prerequisite for generating translation tables, phrase tables, or syntactic transformation rules.", "labels": [], "entities": []}, {"text": "Generative alignment models like IBM have been in wide use for over 15 years, and while not perfect (see), they are completely unsupervised, requiring no annotated training data to learn alignments that have powered many current state-of-the-art translation system.", "labels": [], "entities": [{"text": "IBM", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.9030748605728149}]}, {"text": "Today, there exist human-annotated alignments and an abundance of other information for many language pairs potentially useful for inducing accurate alignments.", "labels": [], "entities": []}, {"text": "How can we take advantage of all of this data at our fingertips?", "labels": [], "entities": []}, {"text": "Using feature functions that encode extra information is one good way.", "labels": [], "entities": []}, {"text": "Unfortunately, as points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story.", "labels": [], "entities": []}, {"text": "This difficulty has motivated much recent work in discriminative modeling for word alignment).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.7710613906383514}]}, {"text": "We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure.", "labels": [], "entities": []}, {"text": "We borrow ideas from both k-best parsing ( and forest-based, and hierarchical phrase-based translation, and apply them to word alignment.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.6943605542182922}, {"text": "word alignment", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.7979347705841064}]}, {"text": "Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments.", "labels": [], "entities": []}, {"text": "Our algorithm yields a forest of: Example of approximate search through a hypergraph with beam size = 5.", "labels": [], "entities": []}, {"text": "Each black square implies a partial alignment.", "labels": [], "entities": []}, {"text": "Each partial alignment at each node is ranked according to its model score.", "labels": [], "entities": []}, {"text": "In this figure, we see that the partial alignment implied by the 1-best hypothesis at the leftmost NP node is constructed by composing the best hypothesis at the terminal node labeled \"the\" and the 2nd-best hypothesis at the terminal node labeled \"man\".", "labels": [], "entities": []}, {"text": "(We ignore terminal nodes in this toy example.)", "labels": [], "entities": []}, {"text": "Hypotheses at the root node imply full alignment structures.", "labels": [], "entities": []}, {"text": "word alignments, from which we can efficiently extract the k-best.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7023911625146866}]}, {"text": "We handle an arbitrary number of features, compute them efficiently, and score alignments using a linear model.", "labels": [], "entities": []}, {"text": "We train the parameters of the model using averaged perceptron) modified for structured outputs, but can easily fit into a max-margin or related framework.", "labels": [], "entities": []}, {"text": "Finally, we use relatively little training data to achieve accurate word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.7077704221010208}]}, {"text": "Our model can generate arbitrary alignments and learn from arbitrary gold alignments.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model and and resulting alignments on Arabic-English data against those induced by IBM Model-4 using GIZA++) with both the union and grow-diagfinal heuristics.", "labels": [], "entities": []}, {"text": "We use 1,000 sentence pairs and gold alignments from LDC2006E86 to train model parameters: 800 sentences for training, 100 for testing, and 100 as a second held-out development set to decide when to stop perceptron training.", "labels": [], "entities": [{"text": "LDC2006E86", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.9385541081428528}]}, {"text": "We also align the test data using GIZA++ along with 50 million words of English.", "labels": [], "entities": []}, {"text": "We use a standard training procedure: 5 iterations of Model-1, 5 iterations of HMM, 3 iterations of Model-3, and 3 iterations of Model-4.", "labels": [], "entities": []}, {"text": "We align a corpus of 50 million words with GIZA++ Model-4, and extract translation rules from a 5.4 million word core subset.", "labels": [], "entities": [{"text": "GIZA++ Model-4", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.8723676999409994}]}, {"text": "We align the same core subset with our trained hypergraph alignment model, and extract a second set of translation rules.", "labels": [], "entities": [{"text": "hypergraph alignment", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.6509349048137665}]}, {"text": "For each set of translation rules, we train a machine translation system and decode a held-out test corpus for which we report results below.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.699860081076622}]}, {"text": "We use a syntax-based translation system for these experiments.", "labels": [], "entities": []}, {"text": "This system transforms Arabic strings into target English syntax trees Translation rules are extracted from (e-tree, f -string, alignment) triples as in ().", "labels": [], "entities": []}, {"text": "We use a randomized language model (similar to that of) of 472 million English words.", "labels": [], "entities": []}, {"text": "We tune the the parameters of the MT system on a held-out development corpus of 1,172 parallel sentences, and test on a heldout parallel corpus of 746 parallel sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9513558745384216}]}, {"text": "Both corpora are drawn from the NIST 2004 and 2006 evaluation data, with no overlap at the document or segment level with our training data.", "labels": [], "entities": [{"text": "NIST 2004 and 2006 evaluation data", "start_pos": 32, "end_pos": 66, "type": "DATASET", "confidence": 0.9477403263250986}]}, {"text": "show the results of our MT experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9775225520133972}]}, {"text": "Our hypergraph alignment algorithm allows us a 1.1 BLEU increase over the best baseline system, Model-4 grow-diag-final.", "labels": [], "entities": [{"text": "hypergraph alignment", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.6152182817459106}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.997123658657074}]}, {"text": "This is statistically significant at the p < 0.01 level.", "labels": [], "entities": []}, {"text": "We also report a 2.4 BLEU increase over a system trained with alignments from Model-4 union.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9992384910583496}, {"text": "Model-4 union", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.899216502904892}]}], "tableCaptions": [{"text": " Table 2: F-measure, Precision, Recall, the resulting BLEU score, and number of unknown words on a  held-out test corpus for three types of alignments. BLEU scores are case-insensitive IBM BLEU. We  show a 1.1 BLEU increase over the strongest baseline, Model-4 grow-diag-final. This is statistically  significant at the p < 0.01 level.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.991678774356842}, {"text": "Precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9976207613945007}, {"text": "Recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9980762004852295}, {"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9979318380355835}, {"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9940844178199768}, {"text": "BLEU", "start_pos": 189, "end_pos": 193, "type": "METRIC", "confidence": 0.7884473204612732}, {"text": "BLEU", "start_pos": 210, "end_pos": 214, "type": "METRIC", "confidence": 0.9908662438392639}]}]}