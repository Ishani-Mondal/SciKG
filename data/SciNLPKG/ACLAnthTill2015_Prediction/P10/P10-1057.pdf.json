{"title": [{"text": "Identifying Non-explicit Citing Sentences for Citation-based Summarization", "labels": [], "entities": [{"text": "Identifying Non-explicit Citing Sentences", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8915406465530396}, {"text": "Summarization", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.8199650049209595}]}], "abstractContent": [{"text": "Identifying background (context) information in scientific articles can help scholars understand major contributions in their research area more easily.", "labels": [], "entities": []}, {"text": "In this paper, we propose a general framework based on probabilistic inference to extract such context information from scientific papers.", "labels": [], "entities": []}, {"text": "We model the sentences in an article and their lexical similarities as a Markov Random Field tuned to detect the patterns that context data create, and employ a Belief Propagation mechanism to detect likely context sentences.", "labels": [], "entities": []}, {"text": "We also address the problem of generating surveys of scientific papers.", "labels": [], "entities": [{"text": "generating surveys of scientific papers", "start_pos": 31, "end_pos": 70, "type": "TASK", "confidence": 0.7631855964660644}]}, {"text": "Our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone.", "labels": [], "entities": [{"text": "pyramid", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9961898326873779}]}], "introductionContent": [{"text": "In scientific literature, scholars use citations to refer to external sources.", "labels": [], "entities": []}, {"text": "These secondary sources are essential in comprehending the new research.", "labels": [], "entities": []}, {"text": "Previous work has shown the importance of citations in scientific domains and indicated that citations include survey-worthy information.", "labels": [], "entities": []}, {"text": "A citation to a paper in a scientific article may contain explicit information about the cited research.", "labels": [], "entities": []}, {"text": "The following example is an excerpt from a CoNLL paper 1 that contains information about Eisner's work on bottom-up parsers and the notion of span in parsing: \"Another use of bottom-up is due to, who introduced the notion of a span.\"", "labels": [], "entities": [{"text": "CoNLL paper 1", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.9337474306424459}]}, {"text": "However, the citation to a paper may not always include explicit information about the cited paper: \"This approach is one of those described in Eisner (1996)\" Although this sentence alone does not provide any information about the cited paper, it suggests that its surrounding sentences describe the proposed approach in Eisner's paper: \"...", "labels": [], "entities": []}, {"text": "In an all pairs approach, every possible pair of two tokens in a sentence is considered and some score is assigned to the possibility of this pair having a (directed) dependency relation.", "labels": [], "entities": []}, {"text": "Using that information as building blocks, the parser then searches for the best parse for the sentence.", "labels": [], "entities": []}, {"text": "This approach is one of those described in Eisner (1996).\"", "labels": [], "entities": []}, {"text": "We refer to such implicit citations that contain information about a specific secondary source but do not explicitly cite it, as sentences with context information or context sentences for short.", "labels": [], "entities": []}, {"text": "We look at the patterns that such sentences create and observe that context sentences occur withing a small neighborhood of explicit citations.", "labels": [], "entities": []}, {"text": "We also discuss the problem of extracting context sentences fora source-reference article pair.", "labels": [], "entities": []}, {"text": "We propose a general framework that looks at each sentence as a random variable whose value determines its state about the target paper.", "labels": [], "entities": []}, {"text": "In summary, our proposed model is based on the probabilistic inference of these random variables using graphical models.", "labels": [], "entities": []}, {"text": "Finally we give evidence on how such sentences can help us produce better surveys of research areas.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Preceded by a review of prior work in Section 2, we explain the data collection and our annotation process in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 explains our methodology and is followed by experimental setup in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The intrinsic evaluation of our methodology means to directly compare the output of our method with the gold standards obtained from the annotated data.", "labels": [], "entities": []}, {"text": "Our methodology finds the sentences that cite a reference implicitly.", "labels": [], "entities": []}, {"text": "Therefore the output of the inference method is a vector, \u03c5, of 1's and 0's, whereby a 1 at element i means that sentence i in the source document is a context sentence about the reference while a 0 means an explicit citation or neither.", "labels": [], "entities": []}, {"text": "The gold standard for each paper-reference pair, \u03c9 (obtained from the annotated vectors in Section 3.1 by changing all Cs to 0s), is also a vector of the same format and dimensionality.", "labels": [], "entities": []}, {"text": "Precision, recall, and F \u03b2 for this task can be defined as where 1 is a vector of 1's with the same dimensionality and \u03b2 is a non-negative real number.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.985079824924469}, {"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9991409778594971}, {"text": "F \u03b2", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9860917925834656}]}], "tableCaptions": [{"text": " Table 1: Papers chosen from AAN as source papers for the evaluation corpus, together with their publi- cation year, number of references (in AAN) and number of sentences. Papers marked with  *  are used to  calculate inter-judge agreement.", "labels": [], "entities": [{"text": "AAN", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.8534408807754517}, {"text": "AAN", "start_pos": 142, "end_pos": 145, "type": "DATASET", "confidence": 0.6431326270103455}]}, {"text": " Table 3: Average \u03ba coefficient as inter-judge  agreement for annotations of two sets", "labels": [], "entities": [{"text": "Average \u03ba coefficient", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.7554508646329244}]}, {"text": " Table 4: The distribution of gaps in the annotated  data", "labels": [], "entities": []}, {"text": " Table 7: Average F \u03b2=3 for similarity based baseline (B 1 ), discourse-based baseline (B 2 ), a supervised  method (SVM) and three MRF-based methods.", "labels": [], "entities": [{"text": "Average F \u03b2", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8721813559532166}]}, {"text": " Table 8: A portion of the QA survey generated by LexRank using the context information.", "labels": [], "entities": [{"text": "QA survey generated", "start_pos": 27, "end_pos": 46, "type": "DATASET", "confidence": 0.8406162063280741}, {"text": "LexRank", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.5793779492378235}]}]}