{"title": [{"text": "Domain Adaptation of Maximum Entropy Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate a recently proposed Bayesian adaptation method for building style-adapted maximum entropy language models for speech recognition, given a large corpus of written language data and a small corpus of speech transcripts.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.7687128782272339}]}, {"text": "Experiments show that the method consistently outperforms linear interpolation which is typically used in such cases.", "labels": [], "entities": []}], "introductionContent": [{"text": "In large vocabulary speech recognition, a language model (LM) is typically estimated from large amounts of written text data.", "labels": [], "entities": [{"text": "large vocabulary speech recognition", "start_pos": 3, "end_pos": 38, "type": "TASK", "confidence": 0.6578874960541725}]}, {"text": "However, recognition is typically applied to speech that is stylistically different from written language.", "labels": [], "entities": []}, {"text": "For example, in an often-tried setting, speech recognition is applied to broadcast news, that includes introductory segments, conversations and spontaneous interviews.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7582502067089081}]}, {"text": "To decrease the mismatch between training and test data, often a small amount of speech data is human-transcribed.", "labels": [], "entities": []}, {"text": "A LM is then built by interpolating the models estimated from large corpus of written language and the small corpus of transcribed data.", "labels": [], "entities": []}, {"text": "However, in practice, different models might be of different importance depending on the word context.", "labels": [], "entities": []}, {"text": "Global interpolation doesn't take such variability into account and all predictions are weighted across models identically, regardless of the context.", "labels": [], "entities": []}, {"text": "In this paper we investigate a recently proposed Bayesian adaptation approach) for adapting a conditional maximum entropy (ME) LM to anew domain, given a large corpus of out-of-domain training data and a small corpus of in-domain data.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is that we show how the suggested hierarchical adaptation can be used with suitable priors and combined with the class-based speedup technique) to adapt ME LMs in large-vocabulary speech recognition when the amount of target data is small.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 216, "end_pos": 234, "type": "TASK", "confidence": 0.7380914688110352}]}, {"text": "The results outperform the conventional linear interpolation of background and target models in both N -grams and ME models.", "labels": [], "entities": []}, {"text": "It seems that with the adapted ME models, the same recognition accuracy for the target evaluation data can be obtained with 50% less adaptation data than in interpolated ME models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.7494431734085083}]}], "datasetContent": [{"text": "In this section, we look at experimental results over two speech recognition tasks.", "labels": [], "entities": [{"text": "speech recognition tasks", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.8044983943303426}]}], "tableCaptions": [{"text": " Table 2: Perplexity, WER and LER results comparing pooled and interpolated N -gram models and  interpolated and adapted ME models, with changing amount of available in-domain data.", "labels": [], "entities": [{"text": "WER", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9807842969894409}, {"text": "LER", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9942004680633545}]}]}