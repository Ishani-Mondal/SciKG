{"title": [{"text": "Global Learning of Focused Entailment Graphs", "labels": [], "entities": [{"text": "Global Learning of Focused Entailment Graphs", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7097151180108389}]}], "abstractContent": [{"text": "We propose a global algorithm for learning entailment relations between predicates.", "labels": [], "entities": [{"text": "learning entailment relations between predicates", "start_pos": 34, "end_pos": 82, "type": "TASK", "confidence": 0.7891989588737488}]}, {"text": "We define a graph structure over predicates that represents entailment relations as directed edges, and use a global transitivity constraint on the graph to learn the optimal set of edges, by formulating the optimization problem as an Integer Linear Program.", "labels": [], "entities": []}, {"text": "We motivate this graph with an application that provides a hierarchical summary fora set of propositions that focus on a target concept, and show that our global algorithm improves performance by more than 10% over baseline algorithms .", "labels": [], "entities": []}], "introductionContent": [{"text": "The Textual Entailment (TE) paradigm ( ) is a generic framework for applied semantic inference.", "labels": [], "entities": [{"text": "Textual Entailment (TE) paradigm", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.7935906151930491}]}, {"text": "The objective of TE is to recognize whether a target meaning can be inferred from a given text.", "labels": [], "entities": [{"text": "TE", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9707002639770508}]}, {"text": "For example, a Question Answering system has to recognize that 'alcohol affects blood pressure' is inferred from 'alcohol reduces blood pressure' to answer the question 'What affects blood pressure?'", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8400137722492218}]}, {"text": "TE systems require extensive knowledge of entailment patterns, often captured as entailment rules: rules that specify a directional inference relation between two text fragments (when the rule is bidirectional this is known as paraphrasing).", "labels": [], "entities": []}, {"text": "An important type of entailment rule refers to propositional templates, i.e., propositions comprising a predicate and arguments, possibly replaced by variables.", "labels": [], "entities": []}, {"text": "The rule required for the previous example would be 'X reduce Y \u2192 X affect Y'.", "labels": [], "entities": []}, {"text": "Because facts and knowledge are mostly expressed by propositions, such entailment rules are central to the TE task.", "labels": [], "entities": [{"text": "TE task", "start_pos": 107, "end_pos": 114, "type": "TASK", "confidence": 0.9213940501213074}]}, {"text": "This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g. (. Previous work has focused on learning each entailment rule in isolation.", "labels": [], "entities": []}, {"text": "However, it is clear that there are interactions between rules.", "labels": [], "entities": []}, {"text": "A prominent example is that entailment is a transitive relation, and thus the rules 'X \u2192 Y ' and 'Y \u2192 Z' imply the rule 'X \u2192 Z'.", "labels": [], "entities": []}, {"text": "In this paper we take advantage of these global interactions to improve entailment rule learning.", "labels": [], "entities": [{"text": "entailment rule learning", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.7302664915720621}]}, {"text": "First, we describe a structure termed an entailment graph that models entailment relations between propositional templates (Section 3).", "labels": [], "entities": []}, {"text": "Next, we show that we can present propositions according to an entailment hierarchy derived from the graph, and suggest a novel hierarchical presentation scheme for corpus propositions referring to a target concept.", "labels": [], "entities": []}, {"text": "As in this application each graph focuses on a single concept, we term those focused entailment graphs (Section 4).", "labels": [], "entities": []}, {"text": "In the core section of the paper, we present an algorithm that uses a global approach to learn the entailment relations of focused entailment graphs (Section 5).", "labels": [], "entities": []}, {"text": "We define a global function and look for the graph that maximizes that function under a transitivity constraint.", "labels": [], "entities": []}, {"text": "The optimization problem is formulated as an Integer Linear Program (ILP) and solved with an ILP solver.", "labels": [], "entities": []}, {"text": "We show that this leads to an optimal solution with respect to the global function, and demonstrate that the algorithm outperforms methods that utilize only local information by more than 10%, as well as methods that employ a greedy optimization algorithm rather than an ILP solver (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents our evaluation, which is geared for the application proposed in Section 4.", "labels": [], "entities": []}, {"text": "A health-care corpus of 632MB was harvested from the web and parsed with the Minipar parser).", "labels": [], "entities": [{"text": "Minipar parser", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.9300072193145752}]}, {"text": "The corpus contains 2,307,585 sentences and almost 50 million word tokens.", "labels": [], "entities": []}, {"text": "We used the Unified Medical Language System (UMLS) to annotate medical concepts in the corpus.", "labels": [], "entities": []}, {"text": "The UMLS is a database that maps natural language phrases to over one million concept identifiers in the health-care domain (termed CUIs).", "labels": [], "entities": []}, {"text": "We annotated all nouns and noun phrases that are in the UMLS with their possibly multiple CUIs.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9341171979904175}]}, {"text": "We extracted all propositional templates from the corpus, where both argument instantiations are medical concepts, i.e., annotated with a CUI (\u223c50,000 templates).", "labels": [], "entities": []}, {"text": "When computing distributional similarity scores, a template is represented as a feature vector of the CUIs that instantiate its arguments.", "labels": [], "entities": []}, {"text": "To evaluate the performance of our algorithm, we constructed 23 gold standard entailment graphs.", "labels": [], "entities": []}, {"text": "First, 23 medical concepts, representing typical topics of interest in the medical domain, were manually selected from a list of the most frequent concepts in the corpus.", "labels": [], "entities": []}, {"text": "For each concept, nodes were defined by extracting all propositional templates for which the target concept instantiated an argument at least K(= 3) times (average number of graph nodes=22.04, std=3.66, max=26, min=13).", "labels": [], "entities": []}, {"text": "Ten medical students constructed the gold standard of graph edges.", "labels": [], "entities": []}, {"text": "Each concept graph was annotated by two students.", "labels": [], "entities": []}, {"text": "Following RTE-5 practice (, after initial annotation the two students met fora reconciliation phase.", "labels": [], "entities": [{"text": "RTE-5", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.6620274782180786}]}, {"text": "They worked to reach an agreement on differences and corrected their graphs.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement was calculated using the Kappa statistic both before (\u03ba = 0.59) and after (\u03ba = 0.9) reconciliation.", "labels": [], "entities": []}, {"text": "882 edges were included in the 23 graphs out of a possible 10,364, providing a sufficiently large data set.", "labels": [], "entities": []}, {"text": "The graphs were randomly split into a development set (11 graphs) and a test set (12 graphs) . The entailment graph fragment in is from the gold standard.", "labels": [], "entities": []}, {"text": "The graphs learned by our algorithm were evaluated by two measures, one evaluating the graph directly, and the other motivated by our application: (1) F 1 of the learned edges compared to the gold standard edges (2) Our application provides a summary of propositions extracted from the corpus.", "labels": [], "entities": [{"text": "F 1", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.989771157503128}]}, {"text": "Note that we infer new propositions by propagating inference transitively through the graph.", "labels": [], "entities": []}, {"text": "Thus, we compute F 1 for the set of propositions inferred from the learned graph, compared to the set inferred based on the gold standard graph.", "labels": [], "entities": [{"text": "F 1", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9883889853954315}]}, {"text": "For example, given the proposition from the corpus 'relaxation reduces nausea' and the edge 'X reduce nausea \u2192 X help with nausea', we evaluate the set {'relaxation reduces nausea', 'relaxation helps with nausea'}.", "labels": [], "entities": []}, {"text": "The final score for an algorithm is a macro-average over the 12 graphs of the Test set concepts were: asthma, chemotherapy, diarrhea, FDA, headache, HPV, lungs, mouth, salmonella, seizure, smoking and X-ray.", "labels": [], "entities": [{"text": "FDA", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.9030176997184753}]}], "tableCaptions": [{"text": " Table 1: Results for all experiments", "labels": [], "entities": []}]}