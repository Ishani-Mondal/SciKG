{"title": [{"text": "Starting From Scratch in Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.6400279104709625}]}], "abstractContent": [{"text": "A fundamental step in sentence comprehension involves assigning semantic roles to sentence constituents.", "labels": [], "entities": [{"text": "sentence comprehension", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7296356558799744}]}, {"text": "To accomplish this, the listener must parse the sentence, find constituents that are candidate arguments , and assign semantic roles to those constituents.", "labels": [], "entities": []}, {"text": "Each step depends on prior lexical and syntactic knowledge.", "labels": [], "entities": []}, {"text": "Where do children learning their first languages begin in solving this problem?", "labels": [], "entities": []}, {"text": "In this paper we focus on the parsing and argument-identification steps that precede Semantic Role Labeling (SRL) training.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9628046751022339}, {"text": "Semantic Role Labeling (SRL) training", "start_pos": 85, "end_pos": 122, "type": "TASK", "confidence": 0.8141332353864398}]}, {"text": "We combine a simplified SRL with an un-supervised HMM part of speech tagger, and experiment with psycholinguistically-motivated ways to label clusters resulting from the HMM so that they can be used to parse input for the SRL system.", "labels": [], "entities": [{"text": "SRL", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.919114351272583}, {"text": "SRL", "start_pos": 222, "end_pos": 225, "type": "TASK", "confidence": 0.9513819813728333}]}, {"text": "The results show that proposed shallow representations of sentence structure are robust to reductions in parsing accuracy, and that the contribution of alternative representations of sentence structure to successful semantic role labeling varies with the integrity of the parsing and argument-identification stages.", "labels": [], "entities": [{"text": "parsing", "start_pos": 105, "end_pos": 112, "type": "TASK", "confidence": 0.9656423926353455}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.8976719379425049}, {"text": "semantic role labeling", "start_pos": 216, "end_pos": 238, "type": "TASK", "confidence": 0.6668941974639893}]}], "introductionContent": [{"text": "In this paper we present experiments with an automatic system for semantic role labeling that is designed to model aspects of human language acquisition.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.6673324505488077}, {"text": "human language acquisition", "start_pos": 126, "end_pos": 152, "type": "TASK", "confidence": 0.6274174551169077}]}, {"text": "This simplified SRL system is inspired by the syntactic bootstrapping theory, and by an account of syntactic bootstrapping known as 'structure-mapping'.", "labels": [], "entities": [{"text": "SRL", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9666284918785095}]}, {"text": "Syntactic bootstrapping theory proposes that young children use their very partial knowledge of syntax to guide sentence comprehension.", "labels": [], "entities": [{"text": "Syntactic bootstrapping theory", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8897826671600342}]}, {"text": "The structure-mapping account makes three key assumptions: First, sentence comprehension is grounded by the acquisition of an initial set of concrete nouns.", "labels": [], "entities": []}, {"text": "Nouns are arguably less dependent on prior linguistic knowledge for their acquisition than are verbs; thus children are assumed to be able to identify the referents of some nouns via cross-situational observation ().", "labels": [], "entities": []}, {"text": "Second, these nouns, once identified, yield a skeletal sentence structure.", "labels": [], "entities": []}, {"text": "Children treat each noun as a candidate argument, and thus interpret the number of nouns in the sentence as a cue to its semantic predicate-argument structure.", "labels": [], "entities": []}, {"text": "Third, children represent sentences in an abstract format that permits generalization to new verbs ).", "labels": [], "entities": []}, {"text": "The structure-mapping account of early syntactic bootstrapping makes strong predictions, including predictions of tell-tale errors.", "labels": [], "entities": []}, {"text": "In the sentence \"Ellen and John laughed\", an intransitive verb appears with two nouns.", "labels": [], "entities": []}, {"text": "If young children rely on representations of sentences as simple as an ordered set of nouns, then they should have trouble distinguishing such sentences from transitive sentences.", "labels": [], "entities": []}, {"text": "Experimental evidence suggests that they do: 21-month-olds mistakenly interpreted word order in sentences such as \"The girl and the boy kradded\" as conveying agent-patient roles ).", "labels": [], "entities": []}, {"text": "Previous computational experiments with a system for automatic semantic role labeling (BabySRL:) showed that it is possible to learn to assign basic semantic roles based on the shallow sentence representations proposed by the structure-mapping view.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7003211975097656}, {"text": "BabySRL", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.6007510423660278}]}, {"text": "Furthermore, these simple structural features were robust to drastic reductions in the integrity of the semantic-role feedback.", "labels": [], "entities": []}, {"text": "These experiments showed that representations of sentence structure as simple as 'first of two nouns' are useful, but the experiments relied on perfect knowledge of arguments and predicates as a start to classification.", "labels": [], "entities": []}, {"text": "Perfect built-in parsing finesses two problems facing the human learner.", "labels": [], "entities": []}, {"text": "The first problem involves classifying words by part-of-speech.", "labels": [], "entities": []}, {"text": "Proposed solutions to this problem in the NLP and human language acquisition literatures focus on distributional learning as a key data source (e.g.,).", "labels": [], "entities": []}, {"text": "Importantly, infants are good at learning distributional patterns (.", "labels": [], "entities": []}, {"text": "Here we use a fairly standard Hidden Markov Model (HMM) to generate clusters of words that occur in similar distributional contexts in a corpus of input sentences.", "labels": [], "entities": []}, {"text": "The second problem facing the learner is more contentious: Having identified clusters of distributionally-similar words, how do children figure out what role these clusters of words should play in a sentence interpretation system?", "labels": [], "entities": [{"text": "sentence interpretation", "start_pos": 199, "end_pos": 222, "type": "TASK", "confidence": 0.7310073226690292}]}, {"text": "Some clusters contain nouns, which are candidate arguments; others contain verbs, which take arguments.", "labels": [], "entities": []}, {"text": "How is the child to know which are which?", "labels": [], "entities": []}, {"text": "In order to use the output of the HMM tagger to process sentences for input to an SRL model, we must find away to automatically label the clusters.", "labels": [], "entities": []}, {"text": "Our strategies for automatic argument and predicate identification, spelled out below, reflect core claims of the structure-mapping theory: (1) The meanings of some concrete nouns can be learned without prior linguistic knowledge; these concrete nouns are assumed based on their meanings to be possible arguments; (2) verbs are identified, not primarily by learning their meanings via observation, but rather by learning about their syntactic argument-taking behavior in sentences.", "labels": [], "entities": [{"text": "automatic argument and predicate identification", "start_pos": 19, "end_pos": 66, "type": "TASK", "confidence": 0.6392220020294189}]}, {"text": "By using the HMM part-of-speech tagger in this way, we can ask how the simple structural features that we propose children start withstand up to reductions in parsing accuracy.", "labels": [], "entities": [{"text": "HMM part-of-speech tagger", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.5972590347131094}, {"text": "parsing", "start_pos": 159, "end_pos": 166, "type": "TASK", "confidence": 0.9571583867073059}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.8467682003974915}]}, {"text": "In doing so, we move to a parser derived from a particular theoretical account of how the human learner might classify words, and link them into a system for sentence comprehension.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first evaluate these parsers (the first stage of our SRL system) on unsupervised POS tagging.", "labels": [], "entities": [{"text": "SRL", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.8818222284317017}, {"text": "POS tagging", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.6597323268651962}]}, {"text": "shows the performance of the four systems using Variation of Information to measure match between gold states and unsupervised parsers as we vary the amount of text they receive.", "labels": [], "entities": []}, {"text": "Each point on the graph represents the average result over 10 runs of the HMM with different samples of the unlabeled CDS.", "labels": [], "entities": []}, {"text": "Another common measure for unsupervised POS (when there are more states than tags) is a many to one greedy mapping of states to tags.", "labels": [], "entities": []}, {"text": "It is known that EM gives a better many to one score than VB trained HMM, and likewise we see that here: with all data EM gives 0.75 matching, VB gives 0.74, while both EM+Funct and VB+Funct reach 0.80.", "labels": [], "entities": [{"text": "matching", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9603626132011414}]}, {"text": "Adding the function/content word split to the HMM structure improves both EM and VB estimation in terms of both tag matching accuracy and information.", "labels": [], "entities": [{"text": "EM", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.7167629599571228}, {"text": "VB estimation", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.5626496076583862}, {"text": "tag matching", "start_pos": 112, "end_pos": 124, "type": "TASK", "confidence": 0.6943256258964539}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9114483594894409}]}, {"text": "However, these measures look at the parser only in isolation.", "labels": [], "entities": []}, {"text": "What is more important to us is how useful the provided word clusters are for future semantic processing.", "labels": [], "entities": [{"text": "semantic processing", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.8427088856697083}]}, {"text": "In the next sections we use the outputs of our four parsers to identify arguments and predicates.", "labels": [], "entities": []}, {"text": "Two groups of curves appear in: the upper group shows the primary argument identification accuracy and the bottom group shows the predicate identification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9347849488258362}, {"text": "predicate identification", "start_pos": 130, "end_pos": 154, "type": "TASK", "confidence": 0.6237294375896454}, {"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.612495481967926}]}, {"text": "We evaluate compared to gold tagged data with true argument and predicate boundaries.", "labels": [], "entities": []}, {"text": "The primary argument (A0-4) identification accuracy is the F1 value, with precision calculated as the proportion of identified arguments that appear as part of a true argument, and recall as the proportion of true arguments that have some state identified as an argument.", "labels": [], "entities": [{"text": "A0-4) identification accuracy", "start_pos": 22, "end_pos": 51, "type": "METRIC", "confidence": 0.703233040869236}, {"text": "F1", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9993230104446411}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9994811415672302}, {"text": "recall", "start_pos": 181, "end_pos": 187, "type": "METRIC", "confidence": 0.999420166015625}]}, {"text": "F1 is calculated similarly for predicate identification, as one state per sentence is identified as the predicate.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9928457140922546}, {"text": "predicate identification", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.9374916255474091}]}, {"text": "As shown in, argument identification F1 is higher than predicate identification (which is to be expected, given that predicate identification depends on accurate arguments), and as we add more seed nouns the argument identification improves.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.7859942018985748}, {"text": "F1", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.577549397945404}, {"text": "predicate identification", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.7132429331541061}, {"text": "predicate identification", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.7371391355991364}, {"text": "argument identification", "start_pos": 208, "end_pos": 231, "type": "TASK", "confidence": 0.7110585868358612}]}, {"text": "Surprisingly, despite the clear differences in unsupervised POS performance seen in, the different parsers do not yield very different argument and predicate identification.", "labels": [], "entities": []}, {"text": "As we will see in the next section, however, when the arguments identified in this step are used to train SRL classifier, distinctions between parsers reappear, suggesting that argument identification F1 masks systematic patterns in the errors.", "labels": [], "entities": [{"text": "SRL classifier", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7675991952419281}]}], "tableCaptions": [{"text": " Table 1: SRL result comparison when trained with best unsupervised argument identifier versus trained with gold arguments.", "labels": [], "entities": [{"text": "SRL", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6874051094055176}]}]}