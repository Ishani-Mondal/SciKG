{"title": [{"text": "Distributional Similarity vs. PU Learning for Entity Set Expansion", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributional similarity is a classic technique for entity set expansion, where the system is given a set of seed entities of a particular class, and is asked to expand the set using a corpus to obtain more entities of the same class as represented by the seeds.", "labels": [], "entities": [{"text": "Distributional similarity", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7890579104423523}, {"text": "entity set expansion", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.6597483058770498}]}, {"text": "This paper shows that a machine learning model called positive and unla-beled learning (PU learning) can model the set expansion problem better.", "labels": [], "entities": []}, {"text": "Based on the test results of 10 corpora, we show that a PU learning technique outperformed distributional similarity significantly.", "labels": [], "entities": []}], "introductionContent": [{"text": "The entity set expansion problem is defined as follows: Given a set S of seed entities of a particular class, and a set D of candidate entities (e.g., extracted from a text corpus), we wish to determine which of the entities in D belong to S.", "labels": [], "entities": [{"text": "entity set expansion", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7125489314397176}]}, {"text": "In other words, we \"expand\" the set S based on the given seeds.", "labels": [], "entities": []}, {"text": "This is clearly a classification problem which requires arriving at a binary decision for each entity in D (belonging to S or not).", "labels": [], "entities": []}, {"text": "However, in practice, the problem is often solved as a ranking problem, i.e., ranking the entities in D based on their likelihoods of belonging to S.", "labels": [], "entities": []}, {"text": "The classic method for solving this problem is based on distributional similarity).", "labels": [], "entities": []}, {"text": "The approach works by comparing the similarity of the surrounding word distributions of each candidate entity with the seed entities, and then ranking the candidate entities using their similarity scores.", "labels": [], "entities": []}, {"text": "In machine learning, there is a class of semisupervised learning algorithms that learns from positive and unlabeled examples (PU learning for short).", "labels": [], "entities": []}, {"text": "The key characteristic of PU learning is that there is no negative training example available for learning.", "labels": [], "entities": [{"text": "PU learning", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.8953004479408264}]}, {"text": "This class of algorithms is less known to the natural language processing (NLP) community compared to some other semisupervised learning models and algorithms.", "labels": [], "entities": []}, {"text": "PU learning is a two-class classification model.", "labels": [], "entities": []}, {"text": "It is stated as follows (: Given a set P of positive examples of a particular class and a set U of unlabeled examples (containing hidden positive and negative cases), a classifier is built using P and U for classifying the data in U or future test cases.", "labels": [], "entities": []}, {"text": "The results can be either binary decisions (whether each test case belongs to the positive class or not), or a ranking based on how likely each test case belongs to the positive class represented by P. Clearly, the set expansion problem can be mapped into PU learning exactly, with Sand D as P and U respectively.", "labels": [], "entities": []}, {"text": "This paper shows that a PU learning method called S-EM ( outperforms distributional similarity considerably based on the results from 10 corpora.", "labels": [], "entities": []}, {"text": "The experiments involved extracting named entities (e.g., product and organization names) of the same type or class as the given seeds.", "labels": [], "entities": []}, {"text": "Additionally, we also compared S-EM with a recent method, called Bayesian Sets (), which was designed specifically for set expansion.", "labels": [], "entities": [{"text": "set expansion", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.7919784188270569}]}, {"text": "It also does not perform as well as PU learning.", "labels": [], "entities": []}, {"text": "We will explain why PU learning performs better than both methods in Section 5.", "labels": [], "entities": [{"text": "PU learning", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.8005157709121704}]}, {"text": "We believe that this finding is of interest to the NLP community.", "labels": [], "entities": []}, {"text": "There is another approach used in the Web environment for entity set expansion.", "labels": [], "entities": [{"text": "entity set expansion", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.7036135395367941}]}, {"text": "It exploits Web page structures to identify lists of items using wrapper induction or other techniques.", "labels": [], "entities": []}, {"text": "The idea is that items in the same list are often of the same type.", "labels": [], "entities": []}, {"text": "This approach is used by Google Sets and Boo!Wa!", "labels": [], "entities": [{"text": "Boo!Wa!", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.7735335528850555}]}, {"text": "(. However, as it relies on Web page structures, it is not applicable to general free texts.", "labels": [], "entities": []}], "datasetContent": [{"text": "We empirically evaluate the three techniques in this section.", "labels": [], "entities": []}, {"text": "We implemented distribution similarity and Bayesian Sets.", "labels": [], "entities": []}, {"text": "S-EM was downloaded from http://www.cs.uic.edu/~liub/S-EM/S-EMdownload.html.", "labels": [], "entities": []}, {"text": "For both Bayesian Sets and S-EM, we used their default parameters.", "labels": [], "entities": []}, {"text": "EM in S-EM ran only two iterations.", "labels": [], "entities": [{"text": "EM", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8171838521957397}]}, {"text": "For distributional similarity, we tested TF-IDF and PMI as feature values of vectors, and Cosine and Jaccard as similarity measures.", "labels": [], "entities": []}, {"text": "Due to space limitations, we only show the results of the PMI and Cosine combination as it performed the best.", "labels": [], "entities": []}, {"text": "This combination was also used in).", "labels": [], "entities": []}, {"text": "We used 10 diverse corpora to evaluate the techniques.", "labels": [], "entities": []}, {"text": "They were obtained from a commercial company.", "labels": [], "entities": []}, {"text": "The data were crawled and extracted from multiple online message boards and blogs discussing different products and services.", "labels": [], "entities": []}, {"text": "We split each message into sentences, and the sentences were POS-tagged using Brill's tagger.", "labels": [], "entities": [{"text": "POS-tagged", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.8270370364189148}]}, {"text": "The tagged sentences were used to extract candidate entities and their contexts.", "labels": [], "entities": []}, {"text": "Table 1 shows the domains and the number of sentences in each corpus, as well as the three seed entities used in our experiments for each corpus.", "labels": [], "entities": []}, {"text": "The three seeds for each corpus were randomly selected from a set of common entities in the application domain.", "labels": [], "entities": []}, {"text": "The regular evaluation metrics for named entity recognition such as precision and recall are not suitable for our purpose as we do not have the complete sets of gold standard entities to compare with.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6454548835754395}, {"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9995410442352295}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9992337226867676}]}, {"text": "We adopt rank precision, which is commonly used for evaluation of entity set expansion techniques: The percentage of correct entities among the top N entities in the ranked list.", "labels": [], "entities": [{"text": "rank precision", "start_pos": 9, "end_pos": 23, "type": "METRIC", "confidence": 0.6436137855052948}, {"text": "entity set expansion", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.697766919930776}]}, {"text": "The detailed experimental results for window size 3 (w=3) are shown in for the 10 corpora.", "labels": [], "entities": []}, {"text": "We present the precisions at the top 15-, 30-and 45-ranked positions (i.e., precisions @15, 30 and 45) for each corpus, with the average given in the last column.", "labels": [], "entities": [{"text": "precisions", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9954530000686646}]}, {"text": "For distributional similarity, to save space only shows the results of Distr-Sim-freq, which is the distributional similarity method with term frequency considered in the same way as for Bayesian Sets and S-EM, instead of the original distributional similarity, which is denoted by Distr-Sim.", "labels": [], "entities": [{"text": "distributional similarity", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.773005485534668}]}, {"text": "This is because on average, Distr-Sim-freq performs better than Distr-Sim.", "labels": [], "entities": []}, {"text": "However, the summary results of both Distr-Sim-freq and Distr-Sim are given in.", "labels": [], "entities": []}, {"text": "From, we observe that on average S-EM outperforms Distr-Sim-freq by about 12 -20% in terms of Precision @ N. Bayesian-Sets is also more accurate than Distr-Sim-freq, but S-EM outperforms Bayesian-Sets by 9 -10%.", "labels": [], "entities": [{"text": "Precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9962598085403442}]}, {"text": "To test the sensitivity of window size w, we also experimented with w = 6 and w = 9.", "labels": [], "entities": []}, {"text": "Due to space constraints, we present only their average results in.", "labels": [], "entities": []}, {"text": "Again, we can seethe same performance pattern as in (w = 3): S-EM performs the best, Bayesian-Sets the second, and the two distributional similarity methods the third and the fourth, with Distr-Sim-freq slightly better than Distr-Sim.", "labels": [], "entities": []}], "tableCaptions": []}