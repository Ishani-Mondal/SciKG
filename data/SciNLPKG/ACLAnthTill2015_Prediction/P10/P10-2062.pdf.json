{"title": [{"text": "Automatically generating annotator rationales to improve sentiment classification", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.9662740528583527}]}], "abstractContent": [{"text": "One of the central challenges in sentiment-based text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document.", "labels": [], "entities": []}, {"text": "Previous research has shown that enriching the sentiment labels with human annotators' \"rationales\" can produce substantial improvements in categorization performance (Zaidan et al., 2007).", "labels": [], "entities": []}, {"text": "We explore methods to automatically generate annotator rationales for document-level sentiment classification.", "labels": [], "entities": [{"text": "document-level sentiment classification", "start_pos": 70, "end_pos": 109, "type": "TASK", "confidence": 0.7016230424245199}]}, {"text": "Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the central challenges in sentiment-based text categorization is that not every portion of a given document is equally informative for inferring its overall sentiment (e.g.,).", "labels": [], "entities": [{"text": "sentiment-based text categorization", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.6066122551759084}]}, {"text": "address this problem by asking human annotators to mark (at least some of) the relevant text spans that support each document-level sentiment decision.", "labels": [], "entities": [{"text": "document-level sentiment decision", "start_pos": 117, "end_pos": 150, "type": "TASK", "confidence": 0.7404178480307261}]}, {"text": "The text spans of these \"rationales\" are then used to construct additional training examples that can guide the learning algorithm toward better categorization models.", "labels": [], "entities": []}, {"text": "But could we perhaps enjoy the performance gains of rationale-enhanced learning models without any additional human effort whatsoever (beyond the document-level sentiment label)?", "labels": [], "entities": []}, {"text": "We hypothesize that in the area of sentiment analysis, where there has been a great deal of recent research attention given to various aspects of the task, this might be possible: using existing resources for sentiment analysis, we might be able to construct annotator rationales automatically.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.9523622989654541}, {"text": "sentiment analysis", "start_pos": 209, "end_pos": 227, "type": "TASK", "confidence": 0.9140512943267822}]}, {"text": "In this paper, we explore a number of methods to automatically generate rationales for documentlevel sentiment classification.", "labels": [], "entities": [{"text": "documentlevel sentiment classification", "start_pos": 87, "end_pos": 125, "type": "TASK", "confidence": 0.6835925976435343}]}, {"text": "In particular, we investigate the use of off-the-shelf sentiment analysis components and lexicons for this purpose.", "labels": [], "entities": [{"text": "off-the-shelf sentiment analysis", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.671257217725118}]}, {"text": "Our approaches for generating annotator rationales can be viewed as mostly unsupervised in that we do not require manually annotated rationales for training.", "labels": [], "entities": []}, {"text": "Rather unexpectedly, our empirical results show that automatically generated rationales (91.78%) are just as good as human rationales (91.61%) for document-level sentiment classification of movie reviews.", "labels": [], "entities": [{"text": "document-level sentiment classification of movie reviews", "start_pos": 147, "end_pos": 203, "type": "TASK", "confidence": 0.798494502902031}]}, {"text": "In addition, complementing the human annotator rationales with automatic rationales boosts the performance even further for this domain, achieving 92.5% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9979945421218872}]}, {"text": "We further evaluate our rationale-generation approaches on product review data for which human rationales are not available: here we find that even randomly generated rationales can improve the classification accuracy although rationales generated from sentiment resources are not as effective as for movie reviews.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.9002538919448853}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first briefly summarize the SVM-based learning approach of that allows the incorporation of rationales (Section 2).", "labels": [], "entities": []}, {"text": "We next introduce three methods for the automatic generation of rationales (Section 3).", "labels": [], "entities": [{"text": "automatic generation of rationales", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.7849214226007462}]}, {"text": "The experimental results are presented in Section 4, followed by related work (Section 5) and conclusions (Section 6).", "labels": [], "entities": []}, {"text": "first introduced the notion of annotator rationales -text spans highlighted by human annotators as support or evidence for each document-level sentiment decision.", "labels": [], "entities": []}, {"text": "These rationales, of course, are only useful if the sentiment categorization algorithm can be extended to exploit the rationales effectively.", "labels": [], "entities": []}, {"text": "With this in mind, propose the following con-trastive learning extension to the standard SVM learning algorithm.", "labels": [], "entities": []}, {"text": "Let xi be movie review i, and let { r ij } be the set of annotator rationales that support the positive or negative sentiment decision for xi . For each such rationale r ij in the set, construct a contrastive training example v ij , by removing the text span associated with the rationale r ij from the original review xi . Intuitively, the contrastive example v ij should not be as informative to the learning algorithm as the original review xi , since one of the supporting regions identified by the human annotator has been deleted.", "labels": [], "entities": []}, {"text": "That is, the correct learned model should be less confident of its classification of a contrastive example vs. the corresponding original example, and the classification boundary of the model should be modified accordingly.", "labels": [], "entities": []}, {"text": "formulate exactly this intuition as SVM constraints as follows:", "labels": [], "entities": []}], "datasetContent": [{"text": "For our contrastive learning experiments we use SV M light five different datasets.", "labels": [], "entities": []}, {"text": "The first is the movie review data of, which was manually annotated with rationales by ; the remaining are four product review datasets from.", "labels": [], "entities": [{"text": "movie review data", "start_pos": 17, "end_pos": 34, "type": "DATASET", "confidence": 0.6556778252124786}]}, {"text": "Only the movie review dataset contains human annotator rationales.", "labels": [], "entities": [{"text": "movie review dataset", "start_pos": 9, "end_pos": 29, "type": "DATASET", "confidence": 0.7611536184946696}]}, {"text": "We replicate the same feature set and experimental set-up as in to facilitate comparison with their work.", "labels": [], "entities": []}, {"text": "The contrastive learning method introduced in requires three parameters: (C, \u00b5, C contrast ).", "labels": [], "entities": [{"text": "C contrast", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.8239842355251312}]}, {"text": "To set the parameters, we use a grid search with step 0.1 for the range of values of each parameter around the point).", "labels": [], "entities": []}, {"text": "In total, we try around 3000 different parameter triplets for each type of rationales.", "labels": [], "entities": []}, {"text": "We follow for the training/test data splits.", "labels": [], "entities": [{"text": "training/test data splits", "start_pos": 18, "end_pos": 43, "type": "DATASET", "confidence": 0.6580099999904633}]}, {"text": "The top half of shows the performance of a system trained with no annotator rationales vs. two variations of human annotator rationales.", "labels": [], "entities": []}, {"text": "HUMANR treats each rationale in the same way as.", "labels": [], "entities": [{"text": "HUMANR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6190801858901978}]}, {"text": "HU-MANR@SENTENCE extends the human annotator rationales to sentence boundaries, and then treats each such sentence as a separate rationale.", "labels": [], "entities": []}, {"text": "As shown in, we get almost the same performance from these two variations (91.33% and 91.61%).", "labels": [], "entities": []}, {"text": "8 This result demonstrates that locking rationales to sentence boundaries was a reasonable 5 Available at http://www.cs.jhu.edu/\u223cozaidan/rationales/.", "labels": [], "entities": []}, {"text": "6 http://www.cs.jhu.edu/\u223cmdredze/datasets/sentiment/.", "labels": [], "entities": []}, {"text": "We use binary unigram features corresponding to the unstemmed words or punctuation marks with count greater or equal to 4 in the full 2000 documents, then we normalize the examples to the unit length.", "labels": [], "entities": []}, {"text": "When computing the pseudo examples xij = we first compute ( xi \u2212 vij) using the binary representation.", "labels": [], "entities": []}, {"text": "As a result, features (unigrams) that appeared in both vectors will be zeroed out in the resulting vector.", "labels": [], "entities": []}, {"text": "We then normalize the resulting vector to a unit vector.", "labels": [], "entities": []}, {"text": "The performance of HUMANR reported by is 92.2% which lies between the performance we get (91.61%) and the oracle accuracy we get if we knew the best parameters for the test set (92.67%).", "labels": [], "entities": []}, {"text": "-The numbers marked with \u2022 (or * ) are statistically significantly better than NORATIONALES according to a paired t-test with p < 0.001 (or p < 0.01).", "labels": [], "entities": [{"text": "NORATIONALES", "start_pos": 79, "end_pos": 91, "type": "METRIC", "confidence": 0.9122672080993652}]}, {"text": "-The numbers marked with are statistically significantly better than HUMANR according to a paired t-test with p < 0.01.", "labels": [], "entities": [{"text": "HUMANR", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9520522952079773}]}, {"text": "-The numbers marked with \u2020 are not statistically significantly worse than HUMANR according to a paired t-test with p > 0.1.", "labels": [], "entities": [{"text": "HUMANR", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9838332533836365}]}, {"text": "We next evaluate our approaches on datasets for which human annotator rationales do not exist.", "labels": [], "entities": []}, {"text": "For this, we use some of the product review data from: reviews for Books, DVDs, Videos and Kitchen appliances.", "labels": [], "entities": [{"text": "Books", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.9645054936408997}]}, {"text": "Each dataset contains 1000 positive and 1000 negative reviews.", "labels": [], "entities": []}, {"text": "The reviews, however, are substantially shorter than those in the movie review dataset: the average number of sentences in each review is 9.20/9.13/8.12/6.37 respectively vs. 30.86 for the movie reviews.", "labels": [], "entities": [{"text": "movie review dataset", "start_pos": 66, "end_pos": 86, "type": "DATASET", "confidence": 0.666045069694519}]}, {"text": "We perform 10-fold crossvalidation, where 8 folds are used for training, 1 fold for tuning parameters, and 1 fold for testing.", "labels": [], "entities": []}, {"text": "Rationale-based methods perform statistically significantly better than NORATIONALES for all but the Kitchen dataset.", "labels": [], "entities": [{"text": "NORATIONALES", "start_pos": 72, "end_pos": 84, "type": "METRIC", "confidence": 0.8501270413398743}, {"text": "Kitchen dataset", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.9775333106517792}]}, {"text": "An interesting trend in product review datasets is that RANDOM rationales are just as good as other more sophisticated rationales.", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 56, "end_pos": 62, "type": "TASK", "confidence": 0.5419480800628662}]}, {"text": "We suspect that this is because product reviews are generally shorter and more focused than the movie reviews, thereby any randomly selected sentence is likely to be a good rationale.", "labels": [], "entities": []}, {"text": "Quantitatively, subjective sentences in the product reviews amount to 78%, while subjective sentences in the movie review dataset are only about 25% ().", "labels": [], "entities": [{"text": "movie review dataset", "start_pos": 109, "end_pos": 129, "type": "DATASET", "confidence": 0.6250432332356771}]}], "tableCaptions": [{"text": " Table 1: Comparison of Automatic vs. Human-annotated Rationales.", "labels": [], "entities": []}]}