{"title": [{"text": "Bayesian Synchronous Tree-Substitution Grammar Induction and its Application to Sentence Compression", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing.", "labels": [], "entities": [{"text": "tree-to-tree synchronous tree-substitution grammar (STSG)", "start_pos": 57, "end_pos": 114, "type": "TASK", "confidence": 0.7667605025427682}, {"text": "sentence compression", "start_pos": 157, "end_pos": 177, "type": "TASK", "confidence": 0.717463806271553}]}, {"text": "These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments , yet the unavailability of large-scale data, calling fora Bayesian tree-to-tree formalism.", "labels": [], "entities": []}, {"text": "We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression.", "labels": [], "entities": [{"text": "extractive sentence compression", "start_pos": 171, "end_pos": 202, "type": "TASK", "confidence": 0.6218544940153757}]}, {"text": "We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given an aligned corpus of tree pairs, we might want to learn a mapping between the paired trees.", "labels": [], "entities": []}, {"text": "Such induction of tree mappings has application in a variety of natural-language-processing tasks including machine translation, paraphrase, and sentence compression.", "labels": [], "entities": [{"text": "induction of tree mappings", "start_pos": 5, "end_pos": 31, "type": "TASK", "confidence": 0.6722330376505852}, {"text": "machine translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.8020466566085815}, {"text": "sentence compression", "start_pos": 145, "end_pos": 165, "type": "TASK", "confidence": 0.7539667189121246}]}, {"text": "The induced tree mappings can be expressed by synchronous grammars.", "labels": [], "entities": []}, {"text": "Where the tree pairs are isomorphic, synchronous context-free grammars (SCFG) may suffice, but in general, non-isomorphism can make the problem of rule extraction difficult (.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 147, "end_pos": 162, "type": "TASK", "confidence": 0.7799683511257172}]}, {"text": "More expressive formalisms such as synchronous tree-substitution or treeadjoining grammars may better capture the pairings.", "labels": [], "entities": []}, {"text": "In this work, we explore techniques for inducing synchronous tree-substitution grammars (STSG) using as a testbed application extractive sentence compression.", "labels": [], "entities": [{"text": "synchronous tree-substitution grammars (STSG)", "start_pos": 49, "end_pos": 94, "type": "TASK", "confidence": 0.7068056563536326}, {"text": "extractive sentence compression", "start_pos": 126, "end_pos": 157, "type": "TASK", "confidence": 0.6524363160133362}]}, {"text": "Learning an STSG from aligned trees is tantamount to determining a segmentation of the trees into elementary trees of the grammar along with an alignment of the elementary trees (see for an example of such a segmentation), followed by estimation of the weights for the extracted tree pairs.", "labels": [], "entities": []}, {"text": "1 These elementary tree pairs serve as the rules of the extracted grammar.", "labels": [], "entities": []}, {"text": "For SCFG, segmentation is trivial -each parent with its immediate children is an elementary tree -but the formalism then restricts us to deriving isomorphic tree pairs.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9587603807449341}]}, {"text": "STSG is much more expressive, especially if we allow some elementary trees on the source or target side to be unsynchronized, so that insertions and deletions can be modeled, but the segmentation and alignment problems become nontrivial.", "labels": [], "entities": [{"text": "STSG", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.8301250338554382}]}, {"text": "Previous approaches to this problem have treated the two steps -grammar extraction and weight estimation -with a variety of methods.", "labels": [], "entities": [{"text": "grammar extraction", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7293484956026077}, {"text": "weight estimation", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.6588526219129562}]}, {"text": "One approach is to use word alignments (where these can be reliably estimated, as in our testbed application) to align subtrees and extract rules ( but this leaves open the question of finding the right level of generality of the rules -how deep the rules should be and how much lexicalization they should involve -necessitating resorting to heuristics such as minimality of rules, and leading to large grammars.", "labels": [], "entities": []}, {"text": "Once a given set of rules is extracted, weights can be imputed using a discriminative approach to maximize the (joint or conditional) likelihood or the classification margin in the training data (taking or not taking into account the derivational ambiguity).", "labels": [], "entities": []}, {"text": "This option leverages a large amount of manual domain knowledge engineering and is not in general amenable to latent variable problems.", "labels": [], "entities": []}, {"text": "A simpler alternative to this two step approach is to use a generative model of synchronous derivation and simultaneously segment and weight the elementary tree pairs to maximize the probability of the training data under that model; the simplest exemplar of this approach uses expectation maximization (EM).", "labels": [], "entities": []}, {"text": "This approach has two frailties.", "labels": [], "entities": []}, {"text": "First, EM search over the space of all possible rules is computationally impractical.", "labels": [], "entities": [{"text": "EM search", "start_pos": 7, "end_pos": 16, "type": "TASK", "confidence": 0.9759178459644318}]}, {"text": "Second, even if such a search were practical, the method is degenerate, pushing the probability mass towards larger rules in order to better approximate the empirical distribution of the data).", "labels": [], "entities": []}, {"text": "Indeed, the optimal grammar would be one in which each tree pair in the training data is its own rule.", "labels": [], "entities": []}, {"text": "Therefore, proposals for using EM for this task start with a precomputed subset of rules, and with EM used just to assign weights within this grammar.", "labels": [], "entities": [{"text": "EM", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.8806430101394653}]}, {"text": "In summary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars.", "labels": [], "entities": []}, {"text": "We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting problems.", "labels": [], "entities": []}, {"text": "Such models have been used as generative solutions to several other segmentation problems, ranging from word segmentation), to parsing and machine translation).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7304149866104126}, {"text": "parsing", "start_pos": 127, "end_pos": 134, "type": "TASK", "confidence": 0.9752715229988098}, {"text": "machine translation", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.6880062967538834}]}, {"text": "Segmentation is achieved by introducing a prior bias towards grammars that are compact representations of the data, namely by enforcing simplicity and sparsity: preferring simple rules (smaller segments) unless the use of a complex rule is evidenced by the data (through repetition), and thus mitigating the overfitting problem.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 136, "end_pos": 146, "type": "METRIC", "confidence": 0.9973771572113037}]}, {"text": "A Dirichlet process (DP) prior is typically used to achieve this interplay.", "labels": [], "entities": []}, {"text": "Interestingly, samplingbased nonparametric inference further allows the possibility of searching over the infinite space of grammars (and, in machine translation, possible word alignments), thus side-stepping the narrowness problem outlined above as well.", "labels": [], "entities": [{"text": "machine translation, possible word alignments", "start_pos": 142, "end_pos": 187, "type": "TASK", "confidence": 0.6780574470758438}]}, {"text": "In this work, we use an extension of the aforementioned models of generative segmentation for STSG induction, and describe an algorithm for posterior inference under this model that is tailored to the task of extractive sentence compression.", "labels": [], "entities": [{"text": "generative segmentation", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.9627745747566223}, {"text": "STSG induction", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.985964298248291}, {"text": "extractive sentence compression", "start_pos": 209, "end_pos": 240, "type": "TASK", "confidence": 0.6392003893852234}]}, {"text": "This task is characterized by the availability of word alignments, providing a clean testbed for investigating the effects of grammar extraction.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7272177189588547}, {"text": "grammar extraction", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7341926246881485}]}, {"text": "We achieve substantial improvements against a number of baselines including EM, support vector machine (SVM) based discriminative training, and variational Bayes (VB).", "labels": [], "entities": [{"text": "variational Bayes (VB)", "start_pos": 144, "end_pos": 166, "type": "METRIC", "confidence": 0.9071881413459778}]}, {"text": "By comparing our method to a range of other methods that are subject differentially to the two problems, we can show that both play an important role in performance limitations, and that our method helps address both as well.", "labels": [], "entities": []}, {"text": "Our results are thus not only encouraging for grammar estimation using sparse priors but also illustrate the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar.", "labels": [], "entities": [{"text": "grammar estimation", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.684407502412796}]}, {"text": "In the following, we define the task of extractive sentence compression and the Bayesian STSG model, and algorithms we used for inference and prediction.", "labels": [], "entities": [{"text": "extractive sentence compression", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.6101636091868082}]}, {"text": "We then describe the experiments in extractive sentence compression and present our results in contrast with alternative algorithms.", "labels": [], "entities": [{"text": "extractive sentence compression", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.6176387270291647}]}, {"text": "We conclude by giving examples of compression patterns learned by the Bayesian method.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compared the Gibbs sampling compressor (GS) against aversion of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training) (SVM).", "labels": [], "entities": [{"text": "a posteriori EM", "start_pos": 75, "end_pos": 90, "type": "METRIC", "confidence": 0.6930823922157288}]}, {"text": "EM is a natural benchmark, while SVM is also appropriate since it can betaken as the state of the art for our task.", "labels": [], "entities": [{"text": "EM", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7516207098960876}]}, {"text": "We used a publicly available extractive sentence compression corpus: the Broadcast News compressions corpus (BNC) of: Average grammar and importance scores for various systems on the 20-sentence subsample.", "labels": [], "entities": [{"text": "Broadcast News compressions corpus (BNC)", "start_pos": 73, "end_pos": 113, "type": "DATASET", "confidence": 0.8258848871503558}, {"text": "grammar and importance scores", "start_pos": 126, "end_pos": 155, "type": "METRIC", "confidence": 0.8357885777950287}]}, {"text": "Scores marked with * are significantly different than the corresponding GS score at \u03b1 < .05 and with \u2020 at \u03b1 < .01 according to post-hoc Tukey tests.", "labels": [], "entities": [{"text": "GS score", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.7959713339805603}]}, {"text": "ANOVA was significant at p < .01 both for grammar and importance.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6834283471107483}, {"text": "grammar", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9561212062835693}, {"text": "importance", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9289922714233398}]}, {"text": "170, and 200 pairs, respectively.", "labels": [], "entities": []}, {"text": "The corpus was parsed using the Stanford parser.", "labels": [], "entities": []}, {"text": "In our experiments with the publicly available SVM system we used all except paraphrasal rules extracted from bilingual corpora).", "labels": [], "entities": []}, {"text": "The model chosen for testing had parameter for trade-off between training error and margin set to C = 0.001, used margin rescaling, and Hamming distance over bags of tokens with brevity penalty for loss function.", "labels": [], "entities": [{"text": "training error", "start_pos": 65, "end_pos": 79, "type": "METRIC", "confidence": 0.8968187570571899}, {"text": "margin", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.8956180214881897}, {"text": "margin rescaling", "start_pos": 114, "end_pos": 130, "type": "METRIC", "confidence": 0.8904546797275543}, {"text": "Hamming distance", "start_pos": 136, "end_pos": 152, "type": "METRIC", "confidence": 0.7346191704273224}]}, {"text": "EM used a subset of the rules extracted by SVM, namely all rules except non-head deleting compression rules, and was initialized uniformly.", "labels": [], "entities": [{"text": "EM", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8924345374107361}]}, {"text": "Each EM instance was characterized by two parameters: \u03b1, the smoothing parameter for MAP-EM, and \u03b4, the smoothing parameter for augmenting the learned grammar with rules extracted from unseen data (add-(\u03b4 \u2212 1) smoothing was used), both of which were fit to the development set using grid-search over.", "labels": [], "entities": []}, {"text": "The model chosen for testing was (\u03b1, \u03b4) = (1.0001, 1.01).", "labels": [], "entities": []}, {"text": "GS was initialized at a random derivation.", "labels": [], "entities": []}, {"text": "We sampled the alignments of the source nodes in random order.", "labels": [], "entities": []}, {"text": "The sampler was run for 5000 iterations with annealing.", "labels": [], "entities": []}, {"text": "All hyperparameters \u03b1 c , \u03b2 c were held constant at \u03b1, \u03b2 for simplicity and were fit using grid-search over \u03b1 \u2208 [10 \u22126 , 10 6 ], \u03b2 \u2208 [10 \u22123 , 0.5].", "labels": [], "entities": []}, {"text": "The model chosen for testing was (\u03b1, \u03b2) = (100, 0.1).", "labels": [], "entities": []}, {"text": "As an automated metric of quality, we compute F-score based on grammatical relations (relational F1, or RelF1) (, by which the consistency between the set of predicted grammatical relations and those from the gold standard is measured, which has been shown by to correlate reliably with human judgments.", "labels": [], "entities": [{"text": "F-score", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9905984997749329}, {"text": "relational F1", "start_pos": 86, "end_pos": 99, "type": "METRIC", "confidence": 0.618945300579071}, {"text": "RelF1", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.9551582932472229}]}, {"text": "We also conducted a small human subjective evaluation of the grammaticality and informativeness of the compressions generated by the various methods.", "labels": [], "entities": []}, {"text": "For all three systems we obtained predictions for the test set and used the Stanford parser to extract grammatical relations from predicted trees and the gold standard.", "labels": [], "entities": []}, {"text": "We computed precision, recall, RelF1 (all based on grammatical relations), and compression rate (percentage of the words that are retained), which we report in.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9993860721588135}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9994755387306213}, {"text": "RelF1", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9936342239379883}, {"text": "compression rate", "start_pos": 79, "end_pos": 95, "type": "METRIC", "confidence": 0.9851529002189636}]}, {"text": "The results for GS are averages over five independent runs.", "labels": [], "entities": [{"text": "GS", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.6251530051231384}]}, {"text": "EM gives a strong baseline since it already uses rules that are limited in depth and number of frontier nodes by stipulation, helping with the overfitting we have mentioned, surprisingly outperforming its discriminative counterpart in both precision and recall (and consequently RelF1).", "labels": [], "entities": [{"text": "EM", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8456531763076782}, {"text": "precision", "start_pos": 240, "end_pos": 249, "type": "METRIC", "confidence": 0.9991636276245117}, {"text": "recall", "start_pos": 254, "end_pos": 260, "type": "METRIC", "confidence": 0.9966431856155396}]}, {"text": "GS however maintains the same level of precision as EM while improving recall, bringing an overall improvement in RelF1.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9995326995849609}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9996088147163391}, {"text": "RelF1", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.9307030439376831}]}, {"text": "We randomly subsampled our 200-sentence test set for 20 sentences to be evaluated by human judges through Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 106, "end_pos": 128, "type": "DATASET", "confidence": 0.9553059935569763}]}, {"text": "We asked 15 self-reported native English speakers for their judgments of GS, EM, and SVM output sentences and the gold standard in terms of grammaticality (how fluent the compression is) and importance (how much of the meaning of and important information from the original sentence is retained) on a scale of 1 (worst) to 5 (best).", "labels": [], "entities": []}, {"text": "We report in the average scores.", "labels": [], "entities": [{"text": "average scores", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.9383559226989746}]}, {"text": "EM and SVM perform at very similar levels, which we attribute to using the same set of rules, while GS performs at a level substantially better than both, and much closer to human performance in both criteria.", "labels": [], "entities": []}, {"text": "The human evaluation indicates that the superiority of the Bayesian nonparametric method is underappreciated by the automated evaluation metric.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision, recall, relational F1 and com- pression rate (%) for various systems on the 200- sentence BNC test set. The compression rate for  the gold standard was 65.67%.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9984002709388733}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9987645149230957}, {"text": "F1", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.7931736707687378}, {"text": "com- pression rate", "start_pos": 47, "end_pos": 65, "type": "METRIC", "confidence": 0.957597404718399}, {"text": "BNC test set", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.9647440711657206}, {"text": "compression rate", "start_pos": 129, "end_pos": 145, "type": "METRIC", "confidence": 0.9763103723526001}, {"text": "gold standard", "start_pos": 155, "end_pos": 168, "type": "DATASET", "confidence": 0.8922517597675323}]}, {"text": " Table 2: Average grammar and importance scores  for various systems on the 20-sentence subsam- ple. Scores marked with  *  are significantly dif- ferent than the corresponding GS score at \u03b1 < .05  and with  \u2020 at \u03b1 < .01 according to post-hoc Tukey  tests. ANOVA was significant at p < .01 both for  grammar and importance.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 257, "end_pos": 262, "type": "METRIC", "confidence": 0.9915493726730347}]}]}