{"title": [{"text": "Identifying Text Polarity Using Random Walks", "labels": [], "entities": [{"text": "Identifying Text Polarity", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9196758270263672}]}], "abstractContent": [{"text": "Automatically identifying the polarity of words is a very important task in Natural Language Processing.", "labels": [], "entities": [{"text": "Automatically identifying the polarity of words", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7505780756473541}, {"text": "Natural Language Processing", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.6523303786913554}]}, {"text": "It has applications in text classification, text filtering, analysis of product review, analysis of responses to surveys, and mining online discussions.", "labels": [], "entities": [{"text": "text classification", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7644129991531372}, {"text": "text filtering", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.8138748705387115}, {"text": "analysis of product review", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.855989545583725}]}, {"text": "We propose a method for identifying the polarity of words.", "labels": [], "entities": [{"text": "identifying the polarity of words", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.862135648727417}]}, {"text": "We apply a Markov random walk model to a large word related-ness graph, producing a polarity estimate for any given word.", "labels": [], "entities": []}, {"text": "A key advantage of the model is its ability to accurately and quickly assign a polarity sign and magnitude to any word.", "labels": [], "entities": []}, {"text": "The method could be used both in a semi-supervised setting where a training set of labeled words is used, and in an unsupervised setting where a handful of seeds is used to define the two polarity classes.", "labels": [], "entities": []}, {"text": "The method is experimentally tested using a manually labeled set of positive and negative words.", "labels": [], "entities": []}, {"text": "It out-performs the state of the art methods in the semi-supervised setting.", "labels": [], "entities": []}, {"text": "The results in the unsupervised setting is comparable to the best reported values.", "labels": [], "entities": []}, {"text": "However, the proposed method is faster and does not need a large corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Identifying emotions and attitudes from unstructured text is a very important task in Natural Language Processing.", "labels": [], "entities": [{"text": "Identifying emotions and attitudes from unstructured text", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.8713090760367257}, {"text": "Natural Language Processing", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.6490597526232401}]}, {"text": "This problem has a variety of possible applications.", "labels": [], "entities": []}, {"text": "For example, there has been a great body of work for mining product reputation on the Web ().", "labels": [], "entities": [{"text": "mining product reputation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.8521896799405416}]}, {"text": "Knowing the reputation of a product is very important for marketing and customer relation management ().", "labels": [], "entities": [{"text": "customer relation management", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.7824530800183614}]}, {"text": "Manually handling reviews to identify reputation is a very costly, and time consuming process given the overwhelming amount of reviews on the Web.", "labels": [], "entities": []}, {"text": "A list of words with positive/negative polarity is a very valuable resource for such an application.", "labels": [], "entities": []}, {"text": "Another interesting application is mining online discussions.", "labels": [], "entities": [{"text": "mining online discussions", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.8682328462600708}]}, {"text": "A threaded discussion is an electronic discussion in which software tools are used to help individuals post messages and respond to other messages.", "labels": [], "entities": []}, {"text": "Threaded discussions include e-mails, e-mail lists, bulletin boards, newsgroups, or Internet forums.", "labels": [], "entities": []}, {"text": "Threaded discussions act as a very important tool for communication and collaboration in the Web.", "labels": [], "entities": []}, {"text": "An enormous number of discussion groups exists on the Web.", "labels": [], "entities": []}, {"text": "Millions of users post content to these groups covering pretty much every possible topic.", "labels": [], "entities": []}, {"text": "Tracking participant attitude towards different topics and towards other participants is a very interesting task.", "labels": [], "entities": [{"text": "Tracking participant attitude", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8466763496398926}]}, {"text": "For example, presented the concept of sentiment timelines.", "labels": [], "entities": [{"text": "sentiment timelines", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8847154676914215}]}, {"text": "His system classifies discussion posts about movies as either positive or negative.", "labels": [], "entities": []}, {"text": "This is used to produce a plot of the number of positive and negative sentiment messages overtime.", "labels": [], "entities": []}, {"text": "All those applications could benefit much from an automatic way of identifying semantic orientation of words.", "labels": [], "entities": [{"text": "identifying semantic orientation of words", "start_pos": 67, "end_pos": 108, "type": "TASK", "confidence": 0.8811416745185852}]}, {"text": "In this paper, we study the problem of automatically identifying semantic orientation of any word by analyzing its relations to other words.", "labels": [], "entities": [{"text": "automatically identifying semantic orientation of any word", "start_pos": 39, "end_pos": 97, "type": "TASK", "confidence": 0.7692453563213348}]}, {"text": "Automatically classifying words as either positive or negative enables us to automatically identify the polarity of larger pieces of text.", "labels": [], "entities": []}, {"text": "This could be a very useful building block for mining surveys, product reviews and online discussions.", "labels": [], "entities": []}, {"text": "We apply a Markov random walk model to a large semantic word graph, producing a polarity estimate for any given word.", "labels": [], "entities": []}, {"text": "Previous work on identifying the semantic orientation of words has addressed the problem as both a semi-supervised () and an unsupervised) learning problem.", "labels": [], "entities": [{"text": "identifying the semantic orientation of words", "start_pos": 17, "end_pos": 62, "type": "TASK", "confidence": 0.8788917660713196}]}, {"text": "In the semisupervised setting, a training set of labeled words is used to train the model.", "labels": [], "entities": []}, {"text": "In the unsupervised setting, only a handful of seeds is used to define the two polarity classes.", "labels": [], "entities": []}, {"text": "The proposed method could be used both in a semi-supervised and in an unsupervised setting.", "labels": [], "entities": []}, {"text": "Empirical experiments on a labeled set of words show that the proposed method outperforms the state of the art methods in the semi-supervised setting.", "labels": [], "entities": []}, {"text": "The results in the unsupervised setting are comparable to the best reported values.", "labels": [], "entities": []}, {"text": "The proposed method has the advantages that it is faster and it does not need a large training corpus.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss related work.", "labels": [], "entities": []}, {"text": "Section 3 presents our method for identifying word polarity.", "labels": [], "entities": [{"text": "identifying word polarity", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.793887734413147}]}, {"text": "Section 4 describes our experimental setup.", "labels": [], "entities": []}, {"text": "We conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments on the General Inquirer lexicon).", "labels": [], "entities": [{"text": "General Inquirer lexicon", "start_pos": 32, "end_pos": 56, "type": "DATASET", "confidence": 0.9229583541552225}]}, {"text": "We used it as a gold standard data set for positive/negative words.", "labels": [], "entities": []}, {"text": "The dataset contains 4206 words, 1915 of which are positive and 2291 are negative.", "labels": [], "entities": []}, {"text": "Some of the ambiguous words were removed like).", "labels": [], "entities": []}, {"text": "We use WordNet as a source of synonyms and hypernyms for the word relatedness graph.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9540543556213379}]}, {"text": "We used 10-fold cross validation for all tests.", "labels": [], "entities": []}, {"text": "We evaluate our results in terms of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9994434714317322}]}, {"text": "Statistical significance was tested using a 2-tailed paired t-test.", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.8020097017288208}]}, {"text": "All reported results are statistically significant at the 0.05 level.", "labels": [], "entities": []}, {"text": "We perform experiments varying the parameters and the network.", "labels": [], "entities": []}, {"text": "We also look at the performance of the proposed method for different parts of speech, and for different confidence levels We compare our method to the Semantic Orientation from PMI (SO-PMI) method described in), the Spin model (Spin) described in (), the shortest path (short-path) described in (), and the bootstrapping (bootstrap) method described in ().", "labels": [], "entities": []}, {"text": "We now measure the performance of the proposed method when the system is allowed to abstain from classifying the words for which it have low confidence.", "labels": [], "entities": []}, {"text": "We regard the ratio between the hitting time to positive words and hitting time to negative words as a confidence measure and evaluate the top words with the highest confidence level at different values of threshold.", "labels": [], "entities": []}, {"text": "shows the accuracy for 10-fold cross validation and for using only 14 seeds at different thresholds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996271133422852}]}, {"text": "We notice that the accuracy improves by abstaining from classifying the difficult words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.999420166015625}]}, {"text": "The figure shows that the top 60% words are classified with an accuracy greater than 99% for 10-fold cross validation and 92% with 14 seed words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9989811778068542}]}, {"text": "This maybe compared to the work descibed in () where they achieve the 92% level when they only consider the top 1000 words (28%).", "labels": [], "entities": []}, {"text": "shows a learning curve displaying how the performance of the proposed method is affected with varying the labeled set size (i.e., the number of seeds).", "labels": [], "entities": []}, {"text": "We notice that the accuracy exceeds 90% when the training set size rises above 20%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9997729659080505}]}, {"text": "The accuracy steadily increases as the labeled data increases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996519088745117}]}, {"text": "We also looked at the classification accuracy for different parts of speech in. we notice that, in the case of 10-fold cross validation, the performance is consistent across parts of speech.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.8480174541473389}]}, {"text": "However, when we only use 14 seeds all of which are adjectives, similar to, we notice that the performance on adjectives is much better than other parts of speech.", "labels": [], "entities": []}, {"text": "When we use 14 seeds but replace some of the adjectives with verbs and nouns like (love, harm, friend, enemy), the performance for nouns and verbs improves considerably at the cost of losing a little bit of the performance on adjectives.", "labels": [], "entities": []}, {"text": "We had a closer look at the results to find out what are the reasons behind incorrect predictions.", "labels": [], "entities": []}, {"text": "We found two main reasons.", "labels": [], "entities": []}, {"text": "First, some words are ambiguous and has more than one sense, possible with different orientations.", "labels": [], "entities": []}, {"text": "Disambiguating the sense of words given their context before trying to predict their polarity should solve this problem.", "labels": [], "entities": []}, {"text": "The second reason is that some words have very few connection in thesaurus.", "labels": [], "entities": []}, {"text": "A possible solution to this might be identifying those words and adding more links to them from glosses of co-occurrence statistics in corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy for adjectives only for the spin  model, the bootstrap method, and the random  walk model.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985358715057373}]}, {"text": " Table 2: Accuracy for SO-PMI with different  dataset sizes, the spin model, and the random  walks model for 10-fold cross validation and 14  seeds.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9949232935905457}]}]}