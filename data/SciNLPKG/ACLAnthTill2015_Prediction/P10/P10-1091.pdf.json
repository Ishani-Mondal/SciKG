{"title": [], "abstractContent": [{"text": "Strictly Piecewise (SP) languages area subclass of regular languages which encode certain kinds of long-distance dependencies that are found in natural languages.", "labels": [], "entities": []}, {"text": "Like the classes in the Chom-sky and Subregular hierarchies, there are many independently converging characterizations of the SP class (Rogers et al., to appear).", "labels": [], "entities": []}, {"text": "Here we define SP distributions and show that they can be efficiently estimated from positive data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Long-distance dependencies in natural language are of considerable interest.", "labels": [], "entities": []}, {"text": "Although much attention has focused on long-distance dependencies which are beyond the expressive power of models with finitely many states), there are some long-distance dependencies in natural language which permit finite-state characterizations.", "labels": [], "entities": []}, {"text": "For example, although it is well-known that vowel and consonantal harmony applies across any arbitrary number of intervening segments and that phonological patterns are regular, it is less well-known that harmony patterns are largely characterizable by the Strictly Piecewise languages, a subregular class of languages with independently-motivated, converging characterizations (see Heinz (2007, to appear) and especially).", "labels": [], "entities": []}, {"text": "As shown by (to appear), the Strictly Piecewise (SP) languages, which make distinctions on the basis of (potentially) discontiguous subsequences, are precisely analogous to the Strictly Local (SL) languages; Rogers and Pullum, to appear), which make distinctions on the basis of contiguous subsequences.", "labels": [], "entities": []}, {"text": "The Strictly Local languages are the formal-language theoretic foundation for n-gram models (, which are widely used in natural language processing (NLP) in part because such distributions can be estimated from positive data (i.e. a corpus).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 120, "end_pos": 153, "type": "TASK", "confidence": 0.7400186955928802}]}, {"text": "N -gram models describe probability distributions overall strings on the basis of the Markov assumption: that the probability of the next symbol only depends on the previous contiguous sequence of length n \u2212 1.", "labels": [], "entities": []}, {"text": "From the perspective of formal language theory, these distributions are perhaps properly called Strictly k-Local distributions (SL k ) where k = n.", "labels": [], "entities": []}, {"text": "It is well-known that one limitation of the Markov assumption is its inability to express any kind of long-distance dependency.", "labels": [], "entities": []}, {"text": "This paper defines Strictly k-Piecewise (SP k ) distributions and shows how they too can be efficiently estimated from positive data.", "labels": [], "entities": []}, {"text": "In contrast with the Markov assumption, our assumption is that the probability of the next symbol is conditioned on the previous set of discontiguous subsequences of length k \u2212 1 in the string.", "labels": [], "entities": []}, {"text": "While this suggests the model has too many parameters (one for each subset of all possible subsequences), in fact the model has on the order of |\u03a3| k+1 parameters because of an independence assumption: there is no interaction between different subsequences.", "labels": [], "entities": []}, {"text": "As a result, SP distributions are efficiently computable even though they condition the probability of the next symbol on the occurrences of earlier (possibly very distant) discontiguous subsequences.", "labels": [], "entities": []}, {"text": "Essentially, these SP distributions reflect a kind of long-term memory.", "labels": [], "entities": []}, {"text": "On the other hand, SP models have no shortterm memory and are unable to make distinctions on the basis of contiguous subsequences.", "labels": [], "entities": []}, {"text": "We do not intend SP models to replace n-gram models, but instead expect them to be used alongside of them.", "labels": [], "entities": []}, {"text": "Exactly how this is to be done is beyond the scope of this paper and is left for future research.", "labels": [], "entities": []}, {"text": "Since SP languages are the analogue of SL languages, which are the formal-language theoretical foundation for n-gram models, which are widely used in NLP, it is expected that SP distributions and their estimation will also find wide application.", "labels": [], "entities": []}, {"text": "Apart from their interest to problems in theoretical phonology such as phonotactic learning; Heinz, to appear), it is expected that their use will have application, in conjunction with n-gram models, in areas that currently use them; e.g. augmentative communication (, part of speech tagging, and speech recognition.", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 277, "end_pos": 291, "type": "TASK", "confidence": 0.6964233219623566}, {"text": "speech recognition", "start_pos": 297, "end_pos": 315, "type": "TASK", "confidence": 0.8125419020652771}]}, {"text": "\u00a72 provides basic mathematical notation.", "labels": [], "entities": []}, {"text": "\u00a73 provides relevant background on the subregular hierarchy.", "labels": [], "entities": []}, {"text": "\u00a74 describes automata-theoretic characterizations of SP languages.", "labels": [], "entities": []}, {"text": "\u00a75 defines SP distributions.", "labels": [], "entities": []}, {"text": "\u00a76 shows how these distributions can be efficiently estimated from positive data and provides a demonstration.", "labels": [], "entities": []}, {"text": "\u00a77 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 6. The results  clearly demonstrate the effectiveness of the model:  the probability of a [\u03b1 anterior] sibilant given  P \u22641 (", "labels": [], "entities": []}, {"text": " Table 1: Results of SP 2 estimation on the Samala  corpus. Only sibilants are shown.", "labels": [], "entities": [{"text": "Samala  corpus", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.8654893636703491}]}]}