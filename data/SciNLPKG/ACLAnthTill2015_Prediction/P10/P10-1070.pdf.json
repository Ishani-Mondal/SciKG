{"title": [{"text": "A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation", "labels": [], "entities": [{"text": "Automatic Noun Compound Interpretation", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.6576362326741219}]}], "abstractContent": [{"text": "The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest.", "labels": [], "entities": [{"text": "automatic interpretation of noun-noun compounds", "start_pos": 4, "end_pos": 51, "type": "TASK", "confidence": 0.7913940906524658}]}, {"text": "The problem is difficult, with disagreement regarding the number and nature of the relations, low inter-annotator agreement, and limited annotated data.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel taxonomy of relations that integrates previous relations , the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation.", "labels": [], "entities": [{"text": "noun compound interpretation", "start_pos": 196, "end_pos": 224, "type": "TASK", "confidence": 0.7181469897429148}]}], "introductionContent": [{"text": "Noun compounds (e.g., 'maple leaf') occur very frequently in text, and their interpretationdetermining the relationships between adjacent nouns as well as the hierarchical dependency structure of the NP in which they occur-is an important problem within a wide variety of natural language processing (NLP) applications, including machine translation () and question answering ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 330, "end_pos": 349, "type": "TASK", "confidence": 0.7789493501186371}, {"text": "question answering", "start_pos": 357, "end_pos": 375, "type": "TASK", "confidence": 0.9042158126831055}]}, {"text": "The interpretation of noun compounds is a difficult problem for various reasons).", "labels": [], "entities": [{"text": "interpretation of noun compounds", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.8735173493623734}]}, {"text": "Among them is the fact that no set of relations proposed to date has been accepted as complete and appropriate for general-purpose text.", "labels": [], "entities": []}, {"text": "Regardless, automatic noun compound interpretation is the focus of an upcoming SEMEVAL task (.", "labels": [], "entities": [{"text": "noun compound interpretation", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.7008926967779795}, {"text": "SEMEVAL task", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.8748767971992493}]}, {"text": "Leaving aside the problem of determining the dependency structure among strings of three or more nouns-a problem we do not address in this paper-automatic noun compound interpretation requires a taxonomy of noun-noun relations, an automatic method for accurately assigning the relations to noun compounds, and, in the case of supervised classification, a sufficiently large dataset for training.", "labels": [], "entities": [{"text": "noun compound interpretation", "start_pos": 155, "end_pos": 183, "type": "TASK", "confidence": 0.6855833927790324}]}, {"text": "Earlier work has often suffered from using taxonomies with coarse-grained, highly ambiguous predicates, such as prepositions, as various labels and/or unimpressive inter-annotator agreement among human judges).", "labels": [], "entities": []}, {"text": "In addition, the datasets annotated according to these various schemes have often been too small to provide wide coverage of the noun compounds likely to occur in general text.", "labels": [], "entities": []}, {"text": "In this paper, we present a large, fine-grained taxonomy of 43 noun compound relations, a dataset annotated according to this taxonomy, and a supervised, automatic classification method for determining the relation between the head and modifier words in a noun compound.", "labels": [], "entities": []}, {"text": "We compare and map our relations to those in other taxonomies and report the promising results of an inter-annotator agreement study as well as an automatic classification experiment.", "labels": [], "entities": []}, {"text": "We examine the various features used for classification and identify one very useful, novel family of features.", "labels": [], "entities": []}, {"text": "Our dataset is, to the best of our knowledge, the largest noun compound dataset yet produced.", "labels": [], "entities": []}, {"text": "We will make it available via http://www.isi.edu.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our noun compound dataset was created from two principal sources: an in-house collection of terms extracted from a large corpus using partof-speech tagging and mutual information and the Wall Street Journal section of the Penn Treebank.", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 134, "end_pos": 155, "type": "TASK", "confidence": 0.7009432911872864}, {"text": "Wall Street Journal section of the Penn Treebank", "start_pos": 187, "end_pos": 235, "type": "DATASET", "confidence": 0.9288649335503578}]}, {"text": "Compounds including one or more proper nouns were ignored.", "labels": [], "entities": []}, {"text": "In total, the dataset contains 17509 unique, out-of-context examples, making it by far the largest hand-annotated compound noun dataset in existence that we are aware of.", "labels": [], "entities": []}, {"text": "Proper nouns were not included.", "labels": [], "entities": []}, {"text": "The next largest available datasets have a variety of drawbacks for noun compound interpretation in general text.", "labels": [], "entities": [{"text": "noun compound interpretation", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.8334397872289022}]}, {"text": "dataset is the second largest available dataset, but inter-annotator agreement was only 52.3%, and the annotations had an usually lopsided distribution; 42% of the data has TOPIC labels.", "labels": [], "entities": []}, {"text": "Most (73.23%) of dataset consists of noun-preposition-noun constructions.", "labels": [], "entities": []}, {"text": "dataset is specific to the biomedical domain, while \u00d3 S\u00e9aghdha and Copestake's (2009) data is labeled with only 5 extremely coarse-grained categories.", "labels": [], "entities": [{"text": "Copestake's (2009) data", "start_pos": 67, "end_pos": 90, "type": "DATASET", "confidence": 0.7843737204869589}]}, {"text": "The remaining datasets are too small to provide wide coverage.", "labels": [], "entities": []}, {"text": "See  We performed 10-fold cross validation on our dataset, and, for the purpose of comparison, we also performed 5-fold cross validation on \u00d3 S\u00e9aghdha's (2007) dataset using his folds.", "labels": [], "entities": [{"text": "\u00d3 S\u00e9aghdha's (2007) dataset", "start_pos": 140, "end_pos": 167, "type": "DATASET", "confidence": 0.9559205429894584}]}, {"text": "Our classification accuracy results are 79.3% on our data and 63.6% on the \u00d3 S\u00e9aghdha data.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.8463225960731506}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9614499807357788}, {"text": "\u00d3 S\u00e9aghdha data", "start_pos": 75, "end_pos": 90, "type": "DATASET", "confidence": 0.9562389055887858}]}, {"text": "We used the \u03c7 2 measure to limit our experiments to the most useful 35000 features, which is the point where we obtain the highest results on \u00d3 S\u00e9aghdha's data.", "labels": [], "entities": [{"text": "\u00d3 S\u00e9aghdha's data", "start_pos": 142, "end_pos": 159, "type": "DATASET", "confidence": 0.9445649832487106}]}, {"text": "The 63.6% figure is similar to the best previously reported accuracy for this dataset of 63.1%, which was obtained by \u00d3 S\u00e9aghdha and Copestake (2009) using kernel methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9975860118865967}, {"text": "\u00d3 S\u00e9aghdha and Copestake (2009)", "start_pos": 118, "end_pos": 149, "type": "DATASET", "confidence": 0.7578479860510144}]}, {"text": "For comparison with SVMs, we used Thorsten Joachims' SVM multiclass , which implements an optimization solution to multiclass SVM formulation.", "labels": [], "entities": [{"text": "SVM formulation", "start_pos": 126, "end_pos": 141, "type": "TASK", "confidence": 0.774172306060791}]}, {"text": "The best results were similar, with 79.4% on our dataset and 63.1% on \u00d3 S\u00e9aghdha's.", "labels": [], "entities": [{"text": "\u00d3 S\u00e9aghdha's", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.9519611795743307}]}, {"text": "SVM multiclass was, however, observed to be very sensitive to the tuning of the C parameter, which determines the tradeoff between training error and margin width.", "labels": [], "entities": [{"text": "training error", "start_pos": 131, "end_pos": 145, "type": "METRIC", "confidence": 0.8763611614704132}, {"text": "margin width", "start_pos": 150, "end_pos": 162, "type": "METRIC", "confidence": 0.8358794748783112}]}, {"text": "The best results for the datasets were produced with C set to 5000 and 375 respectively.: Patterns for extracting trigram and 4-Gram features from the Web 1T Corpus fora given noun compound (n 1 n 2 ).", "labels": [], "entities": [{"text": "C set", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9721702337265015}]}, {"text": "To assess the impact of the various features, we ran the cross validation experiments for each feature type, alternating between including only one feature type and including all feature types except that one.", "labels": [], "entities": []}, {"text": "The results for these runs using the Maximum Entropy classifier are presented in.", "labels": [], "entities": []}, {"text": "There are several points of interest in these results.", "labels": [], "entities": []}, {"text": "The WordNet gloss terms had a surprisingly strong influence.", "labels": [], "entities": [{"text": "WordNet gloss terms", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9597672820091248}]}, {"text": "In fact, by themselves they proved roughly as useful as the hypernym features, and their removal had the single strongest negative impact on accuracy for our dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9990911483764648}]}, {"text": "As far as we know, this is the first time that WordNet definition words have been used as features for noun compound interpretation.", "labels": [], "entities": [{"text": "noun compound interpretation", "start_pos": 103, "end_pos": 131, "type": "TASK", "confidence": 0.808485746383667}]}, {"text": "In the future, it maybe valuable to add definition words from other machinereadable dictionaries.", "labels": [], "entities": []}, {"text": "The influence of the Web 1T n-gram features was somewhat mixed.", "labels": [], "entities": []}, {"text": "They had a positive impact on the \u00d3 S\u00e9aghdha data, but their affect upon our dataset was limited and mixed, with the removal of the 4-gram features actually improving performance slightly.: Impact of features; cross validation accuracy for only one feature type and all but one feature type experiments, denoted by 1 and M-1 respectively.", "labels": [], "entities": [{"text": "\u00d3 S\u00e9aghdha data", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.8979411323865255}, {"text": "accuracy", "start_pos": 227, "end_pos": 235, "type": "METRIC", "confidence": 0.9860953688621521}]}, {"text": "\u2229-features shared by both n 1 and n 2 ; \u2227-n 1 and n 2 features conjoined by logical AND (e.g., n 1 is a 'substance' \u2227 n 2 is a 'artifact')   To assess the quality of our taxonomy and classification method, we performed an inter-annotator agreement study using 150 noun compounds extracted from a random subset of articles taken from New York Times articles dating back to 1987.", "labels": [], "entities": []}, {"text": "The terms were selected based upon their frequency (i.e., a compound occurring twice as often as another is twice as likely to be selected) to label for testing purposes.", "labels": [], "entities": []}, {"text": "Using a heuristic similar to that used by Lauer (1995), we only extracted binary noun compounds not part of a larger sequence.", "labels": [], "entities": []}, {"text": "Before reaching the 150 mark, we discarded 94 of the drawn examples because they were included in the training set.", "labels": [], "entities": []}, {"text": "Thus, our training set covers roughly 38.5% of the binary noun compound instances in recent New York Times articles.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Size of various available noun compound  datasets labeled with relation annotations. Ital- ics indicate that the dataset contains n-prep-n con- structions and/or non-nouns.", "labels": [], "entities": []}, {"text": " Table 4: Impact of features; cross validation ac- curacy for only one feature type and all but one  feature type experiments, denoted by 1 and M-1  respectively. \u2229-features shared by both n 1 and n 2 ;  \u2227-n 1 and n 2 features conjoined by logical AND  (e.g., n 1 is a 'substance' \u2227 n 2 is a 'artifact')", "labels": [], "entities": []}, {"text": " Table 5: Annotation results. Id -annotator id; N  -number of annotations; Weight -voting weight;  Agree -raw agreement versus the author's annota- tions; \u03ba -Cohen's \u03ba agreement; \u03ba* and \u03ba** -Co- hen's \u03ba results after conflating certain categories.  Voted -combined annotation set using weighted  voting; Auto -automatic classification output.", "labels": [], "entities": [{"text": "Agree -raw agreement", "start_pos": 99, "end_pos": 119, "type": "METRIC", "confidence": 0.9051242172718048}]}]}