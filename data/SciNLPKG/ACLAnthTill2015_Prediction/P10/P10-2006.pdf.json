{"title": [{"text": "Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices", "labels": [], "entities": [{"text": "Efficient Path Counting Transducers", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6785366758704185}, {"text": "Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices", "start_pos": 40, "end_pos": 111, "type": "TASK", "confidence": 0.7375166825950146}]}], "abstractContent": [{"text": "This paper presents an efficient implementation of linearised lattice minimum Bayes-risk decoding using weighted finite state transducers.", "labels": [], "entities": []}, {"text": "We introduce transducers to efficiently count lattice paths containing n-grams and use these to gather the required statistics.", "labels": [], "entities": []}, {"text": "We show that these procedures can be implemented exactly through simple transformations of word sequences to sequences of n-grams.", "labels": [], "entities": []}, {"text": "This yields a novel implementation of lattice minimum Bayes-risk decoding which is fast and exact even for very large lattices.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper focuses on an exact implementation of the linearised form of lattice minimum Bayesrisk (LMBR) decoding using general purpose weighted finite state transducer (WFST) operations . The LMBR decision rule in has the form\u02c6E form\u02c6 form\u02c6E = argmax (1) where E is a lattice of translation hypotheses, N is the set of all n-grams in the lattice (typically, n = 1 . .", "labels": [], "entities": [{"text": "argmax", "start_pos": 245, "end_pos": 251, "type": "METRIC", "confidence": 0.9835500717163086}]}, {"text": "4), and the parameters \u03b8 are constants estimated on held-out data.", "labels": [], "entities": []}, {"text": "The quantity p(u|E) we refer to as the path posterior probability of the n-gram u.", "labels": [], "entities": []}, {"text": "This particular posterior is defined as p(u|E) = p(E u |E) = E\u2208Eu P (E|F ), where Eu = {E \u2208 E : # u (E) > 0} is the subset of lattice paths containing the n-gram u at least once.", "labels": [], "entities": []}, {"text": "It is the efficient computation of these path posterior n-gram probabilities that is the primary focus of this paper.", "labels": [], "entities": []}, {"text": "We will show how general purpose WFST algorithms can be employed to efficiently compute p(u|E) for all u \u2208 N . use Equation as an approximation to the general form of statistical machine translation MBR decoder (: The approximation replaces the sum overall paths in the lattice by a sum over lattice n-grams.", "labels": [], "entities": [{"text": "Equation", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.961858332157135}, {"text": "statistical machine translation MBR decoder", "start_pos": 167, "end_pos": 210, "type": "TASK", "confidence": 0.6942402064800263}]}, {"text": "Even though a lattice may have many n-grams, it is possible to extract and enumerate them exactly whereas this is often impossible for individual paths.", "labels": [], "entities": []}, {"text": "Therefore, while the linearisation of the gain function in the decision rule is an approximation, Equation (1) can be computed exactly even over very large lattices.", "labels": [], "entities": [{"text": "Equation", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.999305248260498}]}, {"text": "The challenge is to do so efficiently.", "labels": [], "entities": []}, {"text": "If the quantity p(u|E) had the form of a conditional expected count it could be computed efficiently using counting transducers (.", "labels": [], "entities": []}, {"text": "The statistic c(u|E) counts the number of times an n-gram occurs on each path, accumulating the weighted count overall paths.", "labels": [], "entities": []}, {"text": "By contrast, what is needed by the approximation in Equation (1) is to identify all paths containing an n-gram and accumulate their probabilities.", "labels": [], "entities": []}, {"text": "The accumulation of probabilities at the path level, rather than the n-gram level, makes the exact computation of p(u|E) hard.", "labels": [], "entities": []}, {"text": "approach this problem by building a separate word sequence acceptor for each n-gram in N and intersecting this acceptor with the lattice to discard all paths that do not contain the n-gram; they then sum the probabilities of all paths in the filtered lattice.", "labels": [], "entities": []}, {"text": "We refer to this as the sequential method, since p(u|E) is calculated separately for each u in sequence.", "labels": [], "entities": []}, {"text": "introduce a transducer for simultaneous calculation of p(u|E) for all unigrams u \u2208 N 1 in a lattice.", "labels": [], "entities": []}, {"text": "This transducer is effective for finding path posterior probabilities of unigrams because there are relatively few unique unigrams in the lattice.", "labels": [], "entities": []}, {"text": "As we will show, however, it is less efficient for higher-order n-grams.", "labels": [], "entities": []}, {"text": "use exact statistics for the unigram path posterior probabilities in Equation (1), but use the conditional expected counts of Equation for higher-order n-grams.", "labels": [], "entities": []}, {"text": "Their hybrid MBR decoder has the form\u02c6E where k determines the range of n-gram orders at which the path posterior probabilities p(u|E) of Equation and conditional expected counts c(u|E) of Equation (4) are used to compute the expected gain.", "labels": [], "entities": []}, {"text": "For k < 4, Equation (5) is thus an approximation to the approximation.", "labels": [], "entities": [{"text": "Equation (5)", "start_pos": 11, "end_pos": 23, "type": "METRIC", "confidence": 0.9546913057565689}]}, {"text": "In many cases it will be perfectly fine, depending on how closely p(u|E) and c(u|E) agree for higher-order n-grams.", "labels": [], "entities": []}, {"text": "Experimentally, find this approximation works well at k = 1 for MBR decoding of statistical machine translation lattices.", "labels": [], "entities": [{"text": "MBR decoding of statistical machine translation lattices", "start_pos": 64, "end_pos": 120, "type": "TASK", "confidence": 0.6742875448295048}]}, {"text": "However, there maybe scenarios in which p(u|E) and c(u|E) differ so that Equation (5) is no longer useful in place of the original approximation.", "labels": [], "entities": [{"text": "Equation (5)", "start_pos": 73, "end_pos": 85, "type": "METRIC", "confidence": 0.9511817544698715}]}, {"text": "In the following sections, we present an efficient method for simultaneous calculation of p(u|E) for n-grams of a fixed order.", "labels": [], "entities": []}, {"text": "While other fast MBR approximations are possible (, we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation for efficient MBR decoding over lattices.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: BLEU scores for Arabic\u2192English maximum likelihood translation (ML), MBR decoding using  the hybrid decision rule of Equation (5) at 0 \u2264 k \u2264 3, and regular linearised lattice MBR (LMBR).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985935091972351}, {"text": "Arabic\u2192English maximum likelihood translation (ML)", "start_pos": 26, "end_pos": 76, "type": "TASK", "confidence": 0.6632850666840872}]}, {"text": " Table 2: Time in seconds required for path posterior n-gram probability calculation and LMBR decoding  using sequential method and left-most (\u03a8 L  n ) or right-most (\u03a8 R  n ) counting transducer implementations.", "labels": [], "entities": [{"text": "Time", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9645044803619385}, {"text": "path posterior n-gram probability calculation", "start_pos": 39, "end_pos": 84, "type": "TASK", "confidence": 0.5757555842399598}, {"text": "LMBR decoding", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.6564481109380722}]}]}