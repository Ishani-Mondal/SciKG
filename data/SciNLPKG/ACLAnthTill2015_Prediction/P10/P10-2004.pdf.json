{"title": [{"text": "Filtering Syntactic Constraints for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.8639534910519918}]}], "abstractContent": [{"text": "Source language parse trees offer very useful but imperfect reordering constraints for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.725183516740799}]}, {"text": "A lot of effort has been made for soft applications of syntactic constraints.", "labels": [], "entities": []}, {"text": "We alternatively propose the selective use of syntactic constraints.", "labels": [], "entities": []}, {"text": "A classifier is built automatically to decide whether anode in the parse trees should be used as a reordering constraint or not.", "labels": [], "entities": []}, {"text": "Using this information yields a 0.8 BLEU point improvement over a full constraint-based system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9993880987167358}]}], "introductionContent": [{"text": "In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.8320030868053436}]}, {"text": "Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality.", "labels": [], "entities": []}, {"text": "The most widely used reordering constraints are IBM constraints), ITG constraints and syntactic constraints (;; and numerous others).", "labels": [], "entities": []}, {"text": "Syntactic constraints can be imposed from the source side or target side.", "labels": [], "entities": []}, {"text": "This work will focus on syntactic constraints from source parse trees.", "labels": [], "entities": []}, {"text": "Linguistic parse trees can provide very useful reordering constraints for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9956146478652954}]}, {"text": "However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.9632641673088074}]}, {"text": "The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the strengths of formal phrases.", "labels": [], "entities": []}, {"text": "Recent efforts attack this problem by using the constraints softly.", "labels": [], "entities": []}, {"text": "In their methods, a candidate translation gets an extra credit if it respects the parse tree but may incur a cost if it violates a constituent boundary.", "labels": [], "entities": []}, {"text": "In this paper, we address this challenge from a less explored direction.", "labels": [], "entities": []}, {"text": "Rather than use all constraints offered by the parse trees, we propose using them selectively.", "labels": [], "entities": []}, {"text": "Based on parallel training data, a classifier is built automatically to decide whether anode in the parse trees should be used as a reordering constraint or not.", "labels": [], "entities": []}, {"text": "As a result, we obtain a 0.8 BLEU point improvement over a full constraint-based system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9994082450866699}]}], "datasetContent": [{"text": "Our SMT system is based on a fairly typical phrase-based model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9911810159683228}]}, {"text": "For the training of our SMT model, we use a modified training toolkit adapted from the 2 http://www.cs.huji.ac.il/~shais/code/index.html MOSES decoder.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9925011992454529}, {"text": "MOSES decoder", "start_pos": 137, "end_pos": 150, "type": "DATASET", "confidence": 0.8611294627189636}]}, {"text": "Our decoder can operate on the same principles as the MOSES decoder.", "labels": [], "entities": [{"text": "MOSES decoder", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.927649199962616}]}, {"text": "Minimum error rate training (MERT) with respect to BLEU score is used to tune the decoder's parameters, and it is performed using the standard technique of.", "labels": [], "entities": [{"text": "Minimum error rate training (MERT)", "start_pos": 0, "end_pos": 34, "type": "METRIC", "confidence": 0.8603409358433315}, {"text": "BLEU score", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9834803938865662}]}, {"text": "A lexical reordering model was used in our experiments.", "labels": [], "entities": []}, {"text": "The translation model was created from the FBIS corpus.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9554376006126404}]}, {"text": "We used a 5-gram language model trained with modified Knesser-Ney smoothing.", "labels": [], "entities": []}, {"text": "The language model was trained on the target side of FBIS corpus and the Xinhua news in GI-GAWORD corpus.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.9369001090526581}, {"text": "Xinhua news in GI-GAWORD corpus", "start_pos": 73, "end_pos": 104, "type": "DATASET", "confidence": 0.8449707984924316}]}, {"text": "The development and test sets are from NIST MT08 evaluation campaign.", "labels": [], "entities": [{"text": "NIST MT08 evaluation campaign", "start_pos": 39, "end_pos": 68, "type": "DATASET", "confidence": 0.9087829440832138}]}, {"text": "shows the statistics of the corpora used in our experiments.", "labels": [], "entities": []}, {"text": "We extracted about 3.9 million example nodes from the training data, i.e. the FBIS corpus.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.9457173645496368}]}, {"text": "There were 2.37 million frontier nodes and 1.59 million interior nodes in these examples, give rise to about 4.4 million features.", "labels": [], "entities": []}, {"text": "To test the performance of our classifier, we simply use the last ten thousand examples as a test set, and the rest being used as Pegasos training data.", "labels": [], "entities": [{"text": "Pegasos training data", "start_pos": 130, "end_pos": 151, "type": "DATASET", "confidence": 0.761806865533193}]}, {"text": "All the parameters in Pegasos were set as default values.", "labels": [], "entities": [{"text": "Pegasos", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.8433371782302856}]}, {"text": "In this way, the accuracy of the classifier was 71.59%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9998227953910828}]}, {"text": "Then we retrained our classifier by using all of the examples.", "labels": [], "entities": []}, {"text": "The nodes in the automatically parsed NIST MT08 test set were labeled by the classifier.", "labels": [], "entities": [{"text": "NIST MT08 test set", "start_pos": 38, "end_pos": 56, "type": "DATASET", "confidence": 0.8923216611146927}]}, {"text": "As a result, 17,240 nodes were labeled as frontier nodes and 5,736 nodes were labeled as interior nodes.", "labels": [], "entities": []}, {"text": "In order to confirm that it is advantageous to distinguish between frontier nodes and interior nodes, we performed four translation experiments.", "labels": [], "entities": []}, {"text": "The first one was atypical beam search decoding without any syntactic constraints.", "labels": [], "entities": []}, {"text": "All the other three experiments were based on the IST-ITG method which makes use of syntac-tic constraints.", "labels": [], "entities": []}, {"text": "The difference between these three experiments lies in what constraints are used.", "labels": [], "entities": []}, {"text": "In detail, the second one used all nodes recognized by the parser; the third one only used frontier nodes labeled by the classifier; the fourth one only used interior nodes labeled by the classifier.", "labels": [], "entities": []}, {"text": "With the exception of the above differences, all the other settings were the same in the four experiments.", "labels": [], "entities": []}, {"text": "Clearly, we obtain the best performance if we constrain the search with only frontier nodes.", "labels": [], "entities": []}, {"text": "Using just frontier yields a 0.8 BLEU point improvement over the baseline constraint-based system which uses all the constraints.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9992526173591614}]}, {"text": "On the other hand, constraints from interior nodes result in the worst performance.", "labels": [], "entities": []}, {"text": "This comparison shows it is necessary to explicitly distinguish nodes in the source parse trees when they are used as reordering constraints.", "labels": [], "entities": []}, {"text": "The improvement over the system without constraints is only modest.", "labels": [], "entities": []}, {"text": "It maybe too coarse to use pare trees as hard constraints.", "labels": [], "entities": []}, {"text": "We believe a greater improvement can be expected if we apply our idea to finer-grained approaches that use constraints softly and).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of different constraints by  SMT quality", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.97906494140625}]}]}