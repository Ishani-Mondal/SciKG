{"title": [{"text": "Intelligent Selection of Language Model Training Data", "labels": [], "entities": [{"text": "Intelligent Selection of Language Model Training", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8033175468444824}]}], "abstractContent": [{"text": "We address the problem of selecting non-domain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.83189457654953}]}, {"text": "Our approach is based on comparing the cross-entropy, according to domain-specific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.", "labels": [], "entities": []}, {"text": "We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical N-gram language models are widely used in applications that produce natural-language text as output, particularly speech recognition and machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7453790009021759}, {"text": "machine translation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.7852469682693481}]}, {"text": "It seems to be a universal truth that output quality can always be improved by using more language model training data, but only if the training data is reasonably well-matched to the desired output.", "labels": [], "entities": []}, {"text": "This presents a problem, because in virtually any particular application the amount of in-domain data is limited.", "labels": [], "entities": []}, {"text": "Thus it has become standard practice to combine in-domain data with other data, either by combining N-gram counts from in-domain and other data (usually weighting the counts in some way), or building separate language models from different data sources, interpolating the language model probabilities either linearly or log-linearly.", "labels": [], "entities": []}, {"text": "Log-linear interpolation is particularly popular in statistical machine translation (e.g.,, because the interpolation weights can easily be discriminatively trained to optimize an end-to-end translation objective function (such as BLEU) by making the log probability according to each language model a separate feature function in the overall translation model.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.649145632982254}, {"text": "BLEU", "start_pos": 231, "end_pos": 235, "type": "METRIC", "confidence": 0.9951351284980774}]}, {"text": "The normal practice when using multiple languages models in machine translation seems to be to train models on as much data as feasible from each source, and to depend on feature weight optimization to down-weight the impact of data that is less well-matched to the translation application.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.740504115819931}]}, {"text": "In this paper, however, we show that fora data source that is not entirely in-domain, we can improve the match between the language model from that data source and the desired application output by intelligently selecting a subset of the available data as language model training data.", "labels": [], "entities": []}, {"text": "This not only produces a language model better matched to the domain of interest (as measured in terms of perplexity on held-out in-domain data), but it reduces the computational resources needed to exploit a large amount of non-domain-specific data, since the resources needed to filter a large amount of data are much less (especially in terms of memory) than those required to build a language model from all the data.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Corpus size statistics", "labels": [], "entities": [{"text": "Corpus size", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8668242394924164}]}, {"text": " Table 2: Results adjusted for vocabulary coverage", "labels": [], "entities": []}]}