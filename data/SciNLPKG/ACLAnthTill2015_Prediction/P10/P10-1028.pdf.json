{"title": [{"text": "Learning Phrase-Based Spelling Error Models from Clickthrough Data", "labels": [], "entities": [{"text": "Phrase-Based Spelling Error", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.5980924268563589}]}], "abstractContent": [{"text": "This paper explores the use of clickthrough data for query spelling correction.", "labels": [], "entities": [{"text": "query spelling correction", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.8257580796877543}]}, {"text": "First, large amounts of query-correction pairs are derived by analyzing users' query reformulation behavior encoded in the clickthrough data.", "labels": [], "entities": []}, {"text": "Then, a phrase-based error model that accounts for the transformation probability between multi-term phrases is trained and integrated into a query speller system.", "labels": [], "entities": []}, {"text": "Experiments are carried out on a human-labeled data set.", "labels": [], "entities": []}, {"text": "Results show that the system using the phrase-based error model outperforms significantly its baseline systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Search queries present a particular challenge for traditional spelling correction methods for three main reasons ().", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7878148555755615}]}, {"text": "First, spelling errors are more common in search queries than in regular written text: roughly 10-15% of queries contain misspelled terms ().", "labels": [], "entities": []}, {"text": "Second, most search queries consist of a few key words rather than grammatical sentences, making a grammar-based approach inappropriate.", "labels": [], "entities": []}, {"text": "Most importantly, many queries contain search terms, such as proper nouns and names, which are not well established in the language.", "labels": [], "entities": []}, {"text": "For example, reported that 16.5% of valid search terms do not occur in their 200K-entry spelling lexicon.", "labels": [], "entities": []}, {"text": "Therefore, recent research has focused on the use of Web corpora and query logs, rather than human-compiled lexicons, to infer knowledge about misspellings and word usage in search queries (e.g.,.", "labels": [], "entities": []}, {"text": "Another important data source that would be useful for this purpose is clickthrough data.", "labels": [], "entities": []}, {"text": "Although it is well-known that clickthrough data contain rich information about users' search behavior, e.g., how a user (re-) formulates a query in order to find the relevant document, there has been little research on exploiting the data for the development of a query speller system.", "labels": [], "entities": []}, {"text": "In this paper we present a novel method of extracting large amounts of query-correction pairs from the clickthrough data.", "labels": [], "entities": []}, {"text": "These pairs, implicitly judged by millions of users, are used to train a set of spelling error models.", "labels": [], "entities": []}, {"text": "Among these models, the most effective one is a phrase-based error model that captures the probability of transforming one multi-term phrase into another multi-term phrase.", "labels": [], "entities": []}, {"text": "Comparing to traditional error models that account for transformation probabilities between single characters () or sub-word strings), the phrase-based model is more powerful in that it captures some contextual information by retaining inter-term dependencies.", "labels": [], "entities": []}, {"text": "We show that this information is crucial to detect the correction of a query term, because unlike in regular written text, any query word can be a valid search term and in many cases the only way fora speller system to make the judgment is to explore its usage according to the contextual information.", "labels": [], "entities": []}, {"text": "We conduct a set of experiments on a large data set, consisting of human-labeled query-correction pairs.", "labels": [], "entities": []}, {"text": "Results show that the error models learned from clickthrough data lead to significant improvements on the task of query spelling correction.", "labels": [], "entities": [{"text": "query spelling correction", "start_pos": 114, "end_pos": 139, "type": "TASK", "confidence": 0.8179113666216532}]}, {"text": "In particular, the speller system incorporating a phrase-based error model significantly outperforms its baseline systems.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first extensive study of learning phase-based error models from clickthrough data for query spelling correction.", "labels": [], "entities": [{"text": "query spelling correction", "start_pos": 128, "end_pos": 153, "type": "TASK", "confidence": 0.7169190843900045}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews related work.", "labels": [], "entities": []}, {"text": "Section 3 presents the way query-correction pairs are extracted from the clickthrough data.", "labels": [], "entities": []}, {"text": "Section 4 presents the baseline speller system used in this study.", "labels": [], "entities": []}, {"text": "Section 5 describes in detail the phrasebased error model.", "labels": [], "entities": []}, {"text": "Section 6 presents the experiments.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the spelling error models on a large scale real world data set containing 24,172 queries sampled from one year's worth of query logs from a commercial search engine.", "labels": [], "entities": []}, {"text": "The spelling of each query is judged and corrected by four annotators.", "labels": [], "entities": [{"text": "spelling", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.7791266441345215}]}, {"text": "We divided the data set into training and test data sets.", "labels": [], "entities": []}, {"text": "The two data sets do not overlap.", "labels": [], "entities": []}, {"text": "The training data contains 8,515 query-correction pairs, among which 1,743 queries are misspelled (i.e., in these pairs, the corrections are different from the queries).", "labels": [], "entities": []}, {"text": "The test data contains 15,657 query-correction pairs, among which 2,960 queries are misspelled.", "labels": [], "entities": []}, {"text": "The average length of queries in the training and test data is 2.7 words.", "labels": [], "entities": []}, {"text": "The speller systems we developed in this study are evaluated using the following three metrics.", "labels": [], "entities": []}, {"text": "\u2022 Accuracy: The number of correct outputs generated by the system divided by the total number of queries in the test set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9992687106132507}]}, {"text": "\u2022 Precision: The number of correct spelling corrections for misspelled queries generated by the system divided by the total number of corrections generated by the system.", "labels": [], "entities": [{"text": "Precision", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9965289235115051}]}, {"text": "\u2022 Recall: The number of correct spelling corrections for misspelled queries generated by the system divided by the total number of misspelled queries in the test set.", "labels": [], "entities": [{"text": "Recall", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9988638162612915}]}, {"text": "We also perform a significance test, i.e., a t-test with a significance level of 0.05.", "labels": [], "entities": [{"text": "significance test", "start_pos": 18, "end_pos": 35, "type": "METRIC", "confidence": 0.9530396461486816}, {"text": "significance level", "start_pos": 59, "end_pos": 77, "type": "METRIC", "confidence": 0.9652070701122284}]}, {"text": "A significant difference should be read as significant at the 95% level.", "labels": [], "entities": []}, {"text": "In our experiments, all the speller systems are ranker-based.", "labels": [], "entities": []}, {"text": "In most cases, other than the baseline system (a linear neural net), the ranker is a two-layer neural net with 5 hidden nodes.", "labels": [], "entities": []}, {"text": "The free parameters of the neural net are trained to optimize accuracy on the training data using the back propagation algorithm, running for 500 iterations with a very small learning rate (0.1) to avoid overfitting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9979977011680603}]}, {"text": "We did not adjust the neural net structure (e.g., the number of hidden nodes) or any training parameters for different speller systems.", "labels": [], "entities": []}, {"text": "Neither did we try to seek the best tradeoff between precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.999212384223938}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9889684915542603}]}, {"text": "Since all the systems are optimized for accuracy, we use accuracy as the primary metric for comparison.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9973222613334656}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9993040561676025}]}, {"text": "summarizes the main spelling correction results.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.6644264608621597}]}, {"text": "Row 1 is the baseline speller system where the source-channel model of Equations and is used.", "labels": [], "entities": [{"text": "Equations", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9420580863952637}]}, {"text": "In our implementation, we use a linear ranker with only two features, derived respectively from the language model and the error model models.", "labels": [], "entities": []}, {"text": "The error model is based on the edit distance function.", "labels": [], "entities": []}, {"text": "Row 2 is the ranker-based spelling system that uses all 96 ranking features, as described in Section 4.", "labels": [], "entities": []}, {"text": "Note that the system uses the features derived from two error models.", "labels": [], "entities": []}, {"text": "One is the edit distance model used for candidate generation.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.8633630573749542}]}, {"text": "The other is a phonetic model that measures the edit distance between the metaphones (Philips, 1990) of a query word and its aligned correction word.", "labels": [], "entities": [{"text": "Philips, 1990)", "start_pos": 86, "end_pos": 100, "type": "DATASET", "confidence": 0.8660541623830795}]}, {"text": "Row 3 is the same system as Row 2, with an additional set of features derived from a word-based error model.", "labels": [], "entities": []}, {"text": "This model is a special case of the phrase-based error model described in Section 5 with the maximum phrase length set to one.", "labels": [], "entities": []}, {"text": "Row 4 is the system that uses the additional 5 features derived from the phrase-based error models with a maximum bi-phrase length of 3.", "labels": [], "entities": []}, {"text": "In phrase based error model, L is the maximum length of a bi-phrase).", "labels": [], "entities": []}, {"text": "This value is important for the spelling performance.", "labels": [], "entities": [{"text": "spelling", "start_pos": 32, "end_pos": 40, "type": "TASK", "confidence": 0.9455562233924866}]}, {"text": "We perform experiments to study the impact of L; the results are displayed in.", "labels": [], "entities": []}, {"text": "Moreover, since we proposed to use clickthrough data for spelling correction, it is interesting to study the impact on spelling performance from the size of clickthrough data used for training.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.9417544305324554}]}, {"text": "We varied the size of clickthrough data and the experimental results are presented in.", "labels": [], "entities": []}, {"text": "The results show first and foremost that the ranker-based system significantly outperforms the spelling system based solely on the source-channel model, largely due to the richer set of features used (Row 1 vs. Row 2).", "labels": [], "entities": []}, {"text": "Second, the error model learned from clickthrough data leads to significant improvements (Rows 3 and 4 vs. Row 2).", "labels": [], "entities": []}, {"text": "The phrase-based error model, due to its capability of capturing contextual information, outperforms the word-based model with a small but statistically significant margin (Row 4 vs. Row 3), though using phrases longer (L > 3) does not lead to further significant improvement (Rows 6 and 7 vs. Rows 8 and 9).", "labels": [], "entities": []}, {"text": "Finally, using more clickthrough data leads to significant improvement (Row 13 vs. Rows 10 to 12).", "labels": [], "entities": []}, {"text": "The benefit does not appear to have peaked -further improvements are likely given a larger data set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Summary of spelling correction results.", "labels": [], "entities": [{"text": "Summary of spelling correction", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.5364589691162109}]}, {"text": " Table 2. Variations of spelling performance as a func- tion of phrase length.", "labels": [], "entities": []}, {"text": " Table 3. Variations of spelling performance as a func- tion of the size of clickthrough data used for training.", "labels": [], "entities": []}]}