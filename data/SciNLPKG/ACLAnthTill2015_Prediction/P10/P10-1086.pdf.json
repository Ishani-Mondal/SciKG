{"title": [{"text": "Bilingual Sense Similarity for Statistical Machine Translation", "labels": [], "entities": [{"text": "Bilingual Sense Similarity", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7699357469876608}, {"text": "Statistical Machine Translation", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.8096123536427816}]}], "abstractContent": [{"text": "This paper proposes new algorithms to compute the sense similarity between two units (words, phrases, rules, etc.) from parallel corpora.", "labels": [], "entities": []}, {"text": "The sense similarity scores are computed by using the vector space model.", "labels": [], "entities": []}, {"text": "We then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of translation rule pairs.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.6746096710364023}]}, {"text": "Similarity scores are used as additional features of the translation model to improve translation performance.", "labels": [], "entities": [{"text": "Similarity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.8439577221870422}, {"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9662550687789917}]}, {"text": "Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.6551676392555237}]}], "introductionContent": [{"text": "The sense of a term can generally be inferred from its context.", "labels": [], "entities": []}, {"text": "The underlying idea is that a term is characterized by the contexts it co-occurs with.", "labels": [], "entities": []}, {"text": "This is also well known as the Distributional Hypothesis: terms occurring in similar contexts tend to have similar meanings.", "labels": [], "entities": []}, {"text": "There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as;.", "labels": [], "entities": []}, {"text": "In the work just cited, a common procedure is followed.", "labels": [], "entities": []}, {"text": "Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions.", "labels": [], "entities": []}, {"text": "The features include words within a surface window of a fixed size, grammatical dependencies, etc.", "labels": [], "entities": []}, {"text": "The similarity function which has been most widely used is cosine distance; other similarity functions include Euclidean distance, City Block distance (, and Dice and Jaccard coefficients, etc.", "labels": [], "entities": []}, {"text": "Measures of monolingual sense similarity have been widely used in many applications, such as synonym recognizing, word clustering (, word sense disambiguation, etc.", "labels": [], "entities": [{"text": "synonym recognizing", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.9656479358673096}, {"text": "word clustering", "start_pos": 114, "end_pos": 129, "type": "TASK", "confidence": 0.7772393226623535}, {"text": "word sense disambiguation", "start_pos": 133, "end_pos": 158, "type": "TASK", "confidence": 0.6645337243874868}]}, {"text": "Use of the vector space model to compute sense similarity has also been adapted to the multilingual condition, based on the assumption that two terms with similar meanings often occur in comparable contexts across languages. and Rapp (1999) adopted VSM for the application of extracting translation pairs from comparable or even unrelated corpora.", "labels": [], "entities": [{"text": "extracting translation pairs from comparable or even unrelated corpora", "start_pos": 276, "end_pos": 346, "type": "TASK", "confidence": 0.8075848089324104}]}, {"text": "The vectors in different languages are first mapped to a common space using an initial bilingual dictionary, and then compared.", "labels": [], "entities": []}, {"text": "However, there is no previous work that uses the VSM to compute sense similarity for terms from parallel corpora.", "labels": [], "entities": [{"text": "VSM", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.6690526604652405}]}, {"text": "The sense similarities, i.e. the translation probabilities in a translation model, for units from parallel corpora are mainly based on the co-occurrence counts of the two units.", "labels": [], "entities": []}, {"text": "Therefore, questions emerge: how good is the sense similarity computed via VSM for two units from parallel corpora?", "labels": [], "entities": []}, {"text": "Is it useful for multilingual applications, such as statistical machine translation (SMT)?", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 52, "end_pos": 89, "type": "TASK", "confidence": 0.8328309754530588}]}, {"text": "In this paper, we try to answer these questions, focusing on sense similarity applied to the SMT task.", "labels": [], "entities": [{"text": "SMT task", "start_pos": 93, "end_pos": 101, "type": "TASK", "confidence": 0.9276899993419647}]}, {"text": "For this task, translation rules are heuristically extracted from automatically word-aligned sentence pairs.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9621095061302185}]}, {"text": "Due to noise in the training corpus or wrong word alignment, the source and target sides of some rules are not semantically equivalent, as can be seen from the following real examples which are taken from the rule table built on our training data (Section 5.1): The source and target sides of the rules with (*) at the end are not semantically equivalent; it seems likely that measuring the semantic similarity from their context between the source and target sides of rules might be helpful to machine translation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.7358652949333191}, {"text": "machine translation", "start_pos": 495, "end_pos": 514, "type": "TASK", "confidence": 0.7467733323574066}]}, {"text": "In this work, we first propose new algorithms to compute the sense similarity between two units (unit here includes word, phrase, rule, etc.) in different languages by using their contexts.", "labels": [], "entities": []}, {"text": "Second, we use the sense similarities between the source and target sides of a translation rule to improve statistical machine translation performance.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 107, "end_pos": 138, "type": "TASK", "confidence": 0.6472236712773641}]}, {"text": "This work attempts to measure directly the sense similarity for units from different languages by comparing their contexts 1 . Our contribution includes proposing new bilingual sense similarity algorithms and applying them to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 226, "end_pos": 245, "type": "TASK", "confidence": 0.8037896752357483}]}, {"text": "We chose a hierarchical phrase-based SMT system as our baseline; thus, the units involved in computation of sense similarities are hierarchical rules.", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9111384749412537}]}], "datasetContent": [{"text": "We evaluate the algorithm of bilingual sense similarity via machine translation.", "labels": [], "entities": [{"text": "bilingual sense similarity", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.5931412875652313}]}, {"text": "The sense similarity scores are used as feature functions in the translation model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of training, dev, and test sets for  Chinese-to-English task.", "labels": [], "entities": []}, {"text": " Table 2: Results (BLEU%) of small data Chinese-to- English NIST task. Alg1 represents the original simi- larity functions as in Equation", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9990838766098022}]}, {"text": " Table 3: Results (BLEU%) of large data Chinese-to- English NIST task and German-to-English WMT  task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9993891716003418}, {"text": "WMT  task", "start_pos": 92, "end_pos": 101, "type": "TASK", "confidence": 0.690262645483017}]}, {"text": " Table 4: Results (BLEU%) of Chinese-to-English  large data (CE_LD) and small data (CE_SD) NIST  task by applying one feature.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9987301230430603}]}, {"text": " Table 5: Results (BLEU%) for combination of two  similarity scores. Further improvement was only ob- tained on dev set but not on test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9995372295379639}]}, {"text": " Table 6: Results (BLEU%) of using simple features  based on context on small data NIST task. Some im- provements are obtained on dev set, but there was no  significant effect on the test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9995087385177612}]}]}