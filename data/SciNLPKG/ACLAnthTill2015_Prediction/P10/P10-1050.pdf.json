{"title": [{"text": "Efficient Staggered Decoding for Sequence Labeling", "labels": [], "entities": [{"text": "Sequence Labeling", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.958524763584137}]}], "abstractContent": [{"text": "The Viterbi algorithm is the conventional decoding algorithm most widely adopted for sequence labeling.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.6381862461566925}]}, {"text": "Viterbi decoding is, however, prohibitively slow when the label set is large, because its time complexity is quadratic in the number of labels.", "labels": [], "entities": []}, {"text": "This paper proposes an exact decoding algorithm that overcomes this problem.", "labels": [], "entities": []}, {"text": "A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution.", "labels": [], "entities": []}, {"text": "Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertag-ging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algorithm , CARPEDIEM (Esposito and Radi-cioni, 2009).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.7353659272193909}, {"text": "POS tagging and chunking", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.716720461845398}]}], "introductionContent": [{"text": "In the past decade, sequence labeling algorithms such as HMMs, CRFs, and Collins' perceptrons have been extensively studied in the field of NLP).", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.6425106376409531}]}, {"text": "Now they are indispensable in a wide range of NLP tasks including chunking, POS tagging, NER and soon.", "labels": [], "entities": [{"text": "chunking", "start_pos": 66, "end_pos": 74, "type": "TASK", "confidence": 0.9640834927558899}, {"text": "POS tagging", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.8127371966838837}]}, {"text": "One important task in sequence labeling is how to find the most probable label sequence from among all possible ones.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7725391685962677}]}, {"text": "This task, referred to as decoding, is usually carried out using the Viterbi algorithm.", "labels": [], "entities": []}, {"text": "The Viterbi algorithm has O(NL 2 ) time complexity, 1 where N is the input size and L is the number of labels.", "labels": [], "entities": [{"text": "O(NL 2 ) time complexity", "start_pos": 26, "end_pos": 50, "type": "METRIC", "confidence": 0.6720831436770303}]}, {"text": "Although the Viterbi algorithm is generally efficient, it becomes prohibitively slow when dealing with a large number of labels, since its computational cost is quadratic in L (.", "labels": [], "entities": []}, {"text": "Unfortunately, several sequence-labeling problems in NLP involve a large number of labels.", "labels": [], "entities": []}, {"text": "For example, there are more than 40 and 2000 labels in POS tagging and supertagging, respectively).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.696740910410881}]}, {"text": "These tasks incur much higher computational costs than simpler tasks like NP chunking.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.7387225031852722}]}, {"text": "What is worse, the number of labels grows drastically if we jointly perform multiple tasks.", "labels": [], "entities": []}, {"text": "As we shall see later, we need over 300 labels to reduce joint POS tagging and chunking into the single sequence labeling problem.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.8628721535205841}]}, {"text": "Although joint learning has attracted much attention in recent years, how to perform decoding efficiently still remains an open problem.", "labels": [], "entities": []}, {"text": "In this paper, we present anew decoding algorithm that overcomes this problem.", "labels": [], "entities": []}, {"text": "The proposed algorithm has three distinguishing properties: (1) It is much more efficient than the Viterbi algorithm when dealing with a large number of labels.", "labels": [], "entities": []}, {"text": "(2) It is an exact algorithm, that is, the optimality of the solution is always guaranteed unlike approximate algorithms.", "labels": [], "entities": []}, {"text": "(3) It is automatic, requiring no taskdependent hyperparameters that have to be manually adjusted.", "labels": [], "entities": []}, {"text": "Experiments evaluate our algorithm on three tasks: POS tagging, joint POS tagging and chunking, and supertagging 2 . The results demonstrate that our algorithm is up to several orders of magnitude faster than the basic Viterbi algorithm and a state-of-the-art algorithm; it makes exact decoding practical even in labeling problems with a large label set.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.7730072736740112}, {"text": "POS tagging", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.6312264949083328}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Decoding speed (sent./sec).", "labels": [], "entities": []}, {"text": " Table 2: The average number of iterations.", "labels": [], "entities": []}, {"text": " Table 4: Comparison with beam search (sent./sec).", "labels": [], "entities": []}]}