{"title": [{"text": "Conditional Random Fields for Word Hyphenation", "labels": [], "entities": []}], "abstractContent": [{"text": "Finding allowable places in words to insert hyphens is an important practical problem.", "labels": [], "entities": []}, {"text": "The algorithm that is used most often nowadays has remained essentially unchanged for 25 years.", "labels": [], "entities": []}, {"text": "This method is the T E X hyphenation algorithm of Knuth and Liang.", "labels": [], "entities": []}, {"text": "We present here a hyphenation method that is clearly more accurate.", "labels": [], "entities": []}, {"text": "The new method is an application of conditional random fields.", "labels": [], "entities": []}, {"text": "We create new training sets for English and Dutch from the CELEX European lexical resource, and achieve error rates for English of less than 0.1% for correctly allowed hyphens, and less than 0.01% for Dutch.", "labels": [], "entities": [{"text": "CELEX European lexical resource", "start_pos": 59, "end_pos": 90, "type": "DATASET", "confidence": 0.9729099869728088}, {"text": "error rates", "start_pos": 104, "end_pos": 115, "type": "METRIC", "confidence": 0.9750646352767944}]}, {"text": "Experiments show that both the Knuth/Liang method and a leading current commercial alternative have error rates several times higher for both languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task that we investigate is learning to split words into parts that are conventionally agreed to be individual written units.", "labels": [], "entities": []}, {"text": "In many languages, it is acceptable to separate these units with hyphens, but it is not acceptable to split words arbitrarily.", "labels": [], "entities": []}, {"text": "Another way of stating the task is that we want to learn to predict for each letter in a word whether or not it is permissible for the letter to be followed by a hyphen.", "labels": [], "entities": []}, {"text": "This means that we tag each letter with either 1, for hyphen allowed following this letter, or 0, for hyphen not allowed after this letter.", "labels": [], "entities": []}, {"text": "The hyphenation task is also called orthographic syllabification (.", "labels": [], "entities": []}, {"text": "It is an important issue in real-world text processing, as described further in Section 2 below.", "labels": [], "entities": [{"text": "text processing", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7760798335075378}]}, {"text": "It is also useful as a preprocessing step to improve letter-tophoneme conversion, and more generally for textto-speech conversion.", "labels": [], "entities": [{"text": "letter-tophoneme conversion", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.7139122933149338}, {"text": "textto-speech conversion", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.8242826461791992}]}, {"text": "In the well-known NETtalk system, for example, syllable boundaries are an input to the neural network in addition to letter identities (.", "labels": [], "entities": []}, {"text": "Of course, orthographic syllabification is not a fundamental scientific problem in linguistics.", "labels": [], "entities": [{"text": "orthographic syllabification", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.7420186996459961}]}, {"text": "Nevertheless, it is a difficult engineering task that is worth studying for both practical and intellectual reasons.", "labels": [], "entities": []}, {"text": "The goal in performing hyphenation is to predict a sequence of 0/1 values as a function of a sequence of input characters.", "labels": [], "entities": []}, {"text": "This sequential prediction task is significantly different from a standard (non-sequential) supervised learning task.", "labels": [], "entities": [{"text": "sequential prediction task", "start_pos": 5, "end_pos": 31, "type": "TASK", "confidence": 0.8317131996154785}]}, {"text": "There are at least three important differences that make sequence prediction difficult.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.889057457447052}]}, {"text": "First, the set of all possible sequences of labels is an exponentially large set of possible outputs.", "labels": [], "entities": []}, {"text": "Second, different inputs have different lengths, so it is not obvious how to represent every input by a vector of the same fixed length, as is almost universal in supervised learning.", "labels": [], "entities": []}, {"text": "Third and most important, too much information is lost if we learn a traditional classifier that makes a prediction for each letter separately.", "labels": [], "entities": []}, {"text": "Even if the traditional classifier is a function of the whole input sequence, this remains true.", "labels": [], "entities": []}, {"text": "In order to achieve high accuracy, correlations between neighboring predicted labels must betaken into account.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9966956377029419}]}, {"text": "Learning to predict a sequence of output labels, given a sequence of input data items, is an instance of a structured learning problem.", "labels": [], "entities": []}, {"text": "In general, structured learning means learning to predict outputs that have internal structure.", "labels": [], "entities": []}, {"text": "This structure can be modeled; to achieve high predictive accuracy, when there are dependencies between parts of an output, it must be modeled.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9022846221923828}]}, {"text": "Research on structured learning has been highly successful, with sequence classification as its most important and successful subfield, and with conditional random fields (CRFs) as the most influential approach to learning sequence classifiers.", "labels": [], "entities": [{"text": "sequence classification", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.7861957550048828}]}, {"text": "In the present paper, we show that CRFs can achieve extremely good performance on the hyphenation task.", "labels": [], "entities": []}], "datasetContent": [{"text": "We start with the lexicon for English published by the Dutch Centre for Lexical Information at http://www.mpi.nl/world/celex.", "labels": [], "entities": []}, {"text": "We download all English word forms with legal hyphenation points indicated by hyphens.", "labels": [], "entities": []}, {"text": "These include plurals of nouns, conjugated forms of verbs, and compound words such as \"off-line\".", "labels": [], "entities": []}, {"text": "We separate the components of compound words and phrases, leading to 204,466 words, of which 68,744 are unique.", "labels": [], "entities": []}, {"text": "In order to eliminate abbreviations and proper names which may not be English, we remove all words that are not fully lower-case.", "labels": [], "entities": []}, {"text": "In particular, we exclude words that contain capital letters, apostrophes, and/or periods.", "labels": [], "entities": []}, {"text": "Among these words, 86 have two different hyphenations, and one has three hyphenations.", "labels": [], "entities": []}, {"text": "For most of the 86 words with alternative hyphenations, these alternatives exist because different meanings of the words have different pronunciations, and the different pronunciations have different boundaries between syllables.", "labels": [], "entities": []}, {"text": "This fact implies that no algorithm that operates on words in isolation can be a complete solution for the hyphenation task.", "labels": [], "entities": []}, {"text": "We exclude the few words that have two or more different hyphenations from the dataset.", "labels": [], "entities": []}, {"text": "Finally, we obtain 65,828 spellings.", "labels": [], "entities": []}, {"text": "These have 550,290 letters and 111,228 hyphens, so the average is 8.36 letters and 1.69 hyphens per word.", "labels": [], "entities": []}, {"text": "Informal inspection suggests that the 65,828 spellings contain no mistakes.", "labels": [], "entities": []}, {"text": "However, about 1000 words follow British as opposed to American spelling.", "labels": [], "entities": []}, {"text": "The Dutch dataset of 293,681 words is created following the same procedure as for the English dataset, except that all entries from CELEX that are compound words containing dashes are discarded instead of being split into parts, since many of these are not in fact Dutch words.", "labels": [], "entities": [{"text": "Dutch dataset of 293,681 words", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.9146741628646851}, {"text": "CELEX", "start_pos": 132, "end_pos": 137, "type": "DATASET", "confidence": 0.940248966217041}]}, {"text": "We use ten-fold cross validation for the experiments.", "labels": [], "entities": []}, {"text": "In order to measure accuracy, we compute the confusion matrix for each method, and from this we compute error rates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.999320387840271}]}, {"text": "We report both word-level and letter-level error rates.", "labels": [], "entities": [{"text": "letter-level error rates", "start_pos": 30, "end_pos": 54, "type": "METRIC", "confidence": 0.7339720328648885}]}, {"text": "The wordlevel error rate is the fraction of words on which a method makes at least one mistake.", "labels": [], "entities": [{"text": "wordlevel error rate", "start_pos": 4, "end_pos": 24, "type": "METRIC", "confidence": 0.7132474184036255}]}, {"text": "The letterlevel error rate is the fraction of letters for which the method predicts incorrectly whether or not a hyphen is legal after this letter.", "labels": [], "entities": [{"text": "letterlevel error rate", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.7329946756362915}]}, {"text": "explains the terminology that we use in presenting our results.", "labels": [], "entities": []}, {"text": "Precision, recall, and F1 can be computed easily from the reported confusion matrices.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9940283298492432}, {"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9993364214897156}, {"text": "F1", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9997492432594299}]}, {"text": "As an implementation of Liang's method we use T E X Hyphenator in Java software available at http://texhyphj.sourceforge.net.", "labels": [], "entities": []}, {"text": "We evaluate this algorithm on our entire English and Dutch datasets using the appropriate language pattern files, and not allowing a hyphen to be placed between the first lefthyphenmin and last righthyphenmin letters of each word.", "labels": [], "entities": []}, {"text": "For The single word with more than two alternative hyphenations is \"invalid\" whose three hyphenations are in-va-lid in-val-id and in-valid.", "labels": [], "entities": []}, {"text": "Interestingly, the Merriam-Webster online dictionary also gives three hyphenations for this word, but not the same ones: in-va-lid in-val-id invalid.", "labels": [], "entities": [{"text": "Merriam-Webster online dictionary", "start_pos": 19, "end_pos": 52, "type": "DATASET", "confidence": 0.9639183878898621}]}, {"text": "The American Heritage dictionary agrees with Merriam-Webster.", "labels": [], "entities": [{"text": "American Heritage dictionary", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.935255209604899}, {"text": "Merriam-Webster", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.9454891085624695}]}, {"text": "The disagreement illustrates that there is a certain irreducible ambiguity or subjectivity concerning the correctness of hyphenations.", "labels": [], "entities": []}, {"text": "The hyphenation patterns used by TeXHyphenator, which are those currently used by essentially all variants of T E X, may not be optimal for our new English and Dutch datasets.", "labels": [], "entities": [{"text": "Dutch datasets", "start_pos": 160, "end_pos": 174, "type": "DATASET", "confidence": 0.6881845444440842}]}, {"text": "Therefore, we also do experiments with the PATGEN tool (.", "labels": [], "entities": [{"text": "PATGEN tool", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.7521145939826965}]}, {"text": "These are learning experiments so we also use ten-fold cross validation in the same way as with CRF++.", "labels": [], "entities": []}, {"text": "Specifically, we create a pattern file from 90% of the dataset using PATGEN, and then hyphenate the remaining 10% of the dataset using Liang's algorithm and the learned pattern file.", "labels": [], "entities": [{"text": "PATGEN", "start_pos": 69, "end_pos": 75, "type": "DATASET", "confidence": 0.6368233561515808}]}, {"text": "The PATGEN tool has many user-settable parameters.", "labels": [], "entities": []}, {"text": "As is the case with many machine learning methods, no strong guidance is available for choosing values for these parameters.", "labels": [], "entities": []}, {"text": "For English we use the parameters reported in.", "labels": [], "entities": []}, {"text": "For Dutch we use the parameters reported in.", "labels": [], "entities": []}, {"text": "Preliminary informal experiments found that these parameters work better than alternatives.", "labels": [], "entities": []}, {"text": "We also disallow hyphens in the first two letters of every word, and the last three letters for English, or last two for Dutch.", "labels": [], "entities": []}, {"text": "We also evaluate the TALO commercial software).", "labels": [], "entities": []}, {"text": "We know of one other commercial hyphenation application, which is named Dashes.", "labels": [], "entities": []}, {"text": "Unfortunately we do not have access to it for evaluation.", "labels": [], "entities": []}, {"text": "We also cannot do a precise comparison with the method of ().", "labels": [], "entities": []}, {"text": "We do know that their training set was also derived from CELEX, and their maximum reported accuracy is slightly lower.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.9040554165840149}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9723854660987854}]}, {"text": "Specifically, for English our word-level accuracy (\"ower\") is 96.33% while their best (\"WA\") is 95.65%.", "labels": [], "entities": [{"text": "accuracy (\"ower\")", "start_pos": 41, "end_pos": 58, "type": "METRIC", "confidence": 0.8015990853309631}, {"text": "WA\")", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9780445992946625}]}, {"text": "3 http://www.circlenoetics.com/dashes.", "labels": [], "entities": []}, {"text": "aspx  In we report the performance of the different methods on the English and Dutch datasets respectively.", "labels": [], "entities": [{"text": "English and Dutch datasets", "start_pos": 67, "end_pos": 93, "type": "DATASET", "confidence": 0.6585413813591003}]}, {"text": "shows how the error rate is affected by increasing the CRF probability threshold for each language.", "labels": [], "entities": [{"text": "error rate", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9269265532493591}, {"text": "CRF probability threshold", "start_pos": 55, "end_pos": 80, "type": "METRIC", "confidence": 0.8657834529876709}]}, {"text": "shows confidence intervals for the error rates.", "labels": [], "entities": []}, {"text": "These are computed as follows.", "labels": [], "entities": []}, {"text": "For a single Bernoulli trial the mean is p and the variance is p(1 \u2212 p).", "labels": [], "entities": []}, {"text": "If N such trials are taken, then the observed success rate f = S/N is a random variable with mean p and variance p(1 \u2212 p)/N . For large N , the distribution of the random variable f approaches the normal distribution.", "labels": [], "entities": []}, {"text": "Hence we can derive a confidence interval for p using the formula where fora 95% confidence interval, i.e. for c = 0.95, we set z = 1.96.", "labels": [], "entities": []}, {"text": "All differences between rows in are significant, with one exception: the serious error rates for PATGEN and TALO are not statistically significantly different.", "labels": [], "entities": [{"text": "error", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.5080031156539917}, {"text": "PATGEN", "start_pos": 97, "end_pos": 103, "type": "DATASET", "confidence": 0.49336546659469604}, {"text": "TALO", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.8788265585899353}]}, {"text": "A similar conclusion applies to.", "labels": [], "entities": []}, {"text": "For the English language, the CRF using the Viterbi path has overall error rate of 0.84%, compared to 6.81% for the T E X algorithm using American English patterns, which is eight times worse.", "labels": [], "entities": [{"text": "error rate", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9940668344497681}]}, {"text": "However, the serious error rate for the CRF is less good: 0.41% compared to 0.24%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.8928355872631073}, {"text": "CRF", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.5403600931167603}]}, {"text": "This weakness is remedied by predicting that a hyphen is allowable only if it has high probability.", "labels": [], "entities": []}, {"text": "shows that the CRF can use a probability threshold up to 0.99, and still have lower overall error rate than the T E X algorithm.", "labels": [], "entities": [{"text": "error rate", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9747438430786133}]}, {"text": "Fixing the probability threshold at 0.99, the CRF serious error rate is 0.04% (224 false positives) compared to 0.24% (1343 false positives) for the T E X algorithm.", "labels": [], "entities": [{"text": "CRF serious error rate", "start_pos": 46, "end_pos": 68, "type": "METRIC", "confidence": 0.8780441284179688}]}, {"text": "For the English language, TALO yields overall error rate 1.99% with serious error rate 0.72%, so the standard CRF using the Viterbi path is better on both measures.", "labels": [], "entities": [{"text": "TALO", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9971056580543518}, {"text": "error rate", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9815187752246857}, {"text": "serious error rate", "start_pos": 68, "end_pos": 86, "type": "METRIC", "confidence": 0.8956892887751261}]}, {"text": "The dominance of the CRF method can be increased further by using a probability threshold.", "labels": [], "entities": []}, {"text": "shows that the CRF can use a probability threshold up to 0.94, and still have lower overall error rate than TALO.", "labels": [], "entities": [{"text": "error rate", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9609584510326385}, {"text": "TALO", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9845213294029236}]}, {"text": "Using this threshold, the CRF serious error rate is 0.12% (657 false positives) compared to 0.72% (3970 false positives) for TALO.", "labels": [], "entities": [{"text": "CRF serious error rate", "start_pos": 26, "end_pos": 48, "type": "METRIC", "confidence": 0.8699552714824677}, {"text": "TALO", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.5905327200889587}]}, {"text": "For the Dutch language, the standard CRF using the Viterbi path has overall error rate 0.08%, compared to 0.81% for the T E X algorithm.", "labels": [], "entities": [{"text": "error rate", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9910379946231842}]}, {"text": "The serious error rate for the CRF is 0.04% while for T E X it is 0.18%.", "labels": [], "entities": [{"text": "serious error rate", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.8224296768506368}, {"text": "CRF", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.5742809176445007}]}, {"text": "shows that any probability threshold for the CRF of 0.99 or below yields lower error rates than the T E X algorithm.", "labels": [], "entities": [{"text": "error", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9516232013702393}]}, {"text": "Using the threshold 0.99, the CRF has serious error rate only 0.005%.", "labels": [], "entities": [{"text": "CRF", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.55750572681427}, {"text": "error rate", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9914489388465881}]}, {"text": "For the Dutch language, the TALO method has overall error rate 0.61%.", "labels": [], "entities": [{"text": "TALO", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9204967021942139}, {"text": "error rate", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9900243580341339}]}, {"text": "The serious error rate for TALO is 0.11%.", "labels": [], "entities": [{"text": "serious error rate", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.8853373328844706}, {"text": "TALO", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.7867673635482788}]}, {"text": "The CRF dominance can again be increased via a high probability threshold.", "labels": [], "entities": []}, {"text": "shows that this threshold can range up to 0.98, and still give lower overall error rate than TALO.", "labels": [], "entities": [{"text": "error rate", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.981490820646286}, {"text": "TALO", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.992672860622406}]}, {"text": "Using the 0.98 threshold, the CRF has serious error rate 0.006% (206 false positives); in comparison the serious error rate of TALO is 0.11% (3638 false positives).", "labels": [], "entities": [{"text": "serious error rate", "start_pos": 38, "end_pos": 56, "type": "METRIC", "confidence": 0.8220230937004089}, {"text": "serious error rate", "start_pos": 105, "end_pos": 123, "type": "METRIC", "confidence": 0.8597228527069092}, {"text": "TALO", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9396006464958191}]}, {"text": "For both languages, PATGEN has higher serious letter-level and word-level error rates than T E X using the existing pattern files.", "labels": [], "entities": [{"text": "PATGEN", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.5139244794845581}, {"text": "word-level error rates", "start_pos": 63, "end_pos": 85, "type": "METRIC", "confidence": 0.7602991461753845}]}, {"text": "This is expected since the pattern collections included in T E X distributions have been tuned over the years to minimize objectionable errors.", "labels": [], "entities": []}, {"text": "The difference is especially pronounced for American English, for which the standard pattern collection has been manually improved over more than two decades by many people).", "labels": [], "entities": []}, {"text": "Initially, Liang optimized this pattern collection extensively by upweighting the most common words and by iteratively adding exception words found by testing the algorithm against a large dictionary from an unknown publisher.", "labels": [], "entities": []}, {"text": "One can tune PATGEN to yield either better overall error rate, or better serious error rate, but not both simultaneously, compared to the T E X algorithm using the existing pattern files for both languages.", "labels": [], "entities": [{"text": "PATGEN", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.575013279914856}, {"text": "error rate", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.8773810267448425}, {"text": "serious error rate", "start_pos": 73, "end_pos": 91, "type": "METRIC", "confidence": 0.8736937642097473}]}, {"text": "For the English dataset, if we use Liang's parameters for PATGEN as reported in, we obtain overall error rate of 6.05% and serious error rate of 0.85%.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.844264417886734}, {"text": "PATGEN", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.6760255098342896}, {"text": "error rate", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9900656044483185}, {"text": "serious error rate", "start_pos": 123, "end_pos": 141, "type": "METRIC", "confidence": 0.9276501139005026}]}, {"text": "It is possible that the specific patterns used in T E X implementations today have been tuned by hand to be better than anything the PATGEN software is capable of.", "labels": [], "entities": []}, {"text": "This section presents empirical results following two experimental designs that are less standard, but that maybe more appropriate for the hyphenation task.", "labels": [], "entities": []}, {"text": "First, the experimental design used above has an issue shared by many CELEX-based tagging or transduction evaluations: words are randomly divided into training and test sets without being grouped by stem.", "labels": [], "entities": [{"text": "CELEX-based tagging or transduction evaluations", "start_pos": 70, "end_pos": 117, "type": "TASK", "confidence": 0.7811920046806335}]}, {"text": "This means that a method can get credit for hyphenating \"accents\" correctly, when \"accent\" appears in the training data.", "labels": [], "entities": []}, {"text": "Therefore, we do further experiments where the folds for evaluation are divided by stem, and not byword; that is, all versions of abase form of a word appear in the same fold.", "labels": [], "entities": []}, {"text": "Stemming uses the English and Dutch versions of the Porter stemmer.", "labels": [], "entities": [{"text": "Porter stemmer", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.9059003293514252}]}, {"text": "The 65,828 English words in our dictionary produce 27,100 unique stems, while the 293,681 Dutch words produce 169,693 unique stems.", "labels": [], "entities": []}, {"text": "The results of these experiments are shown in.", "labels": [], "entities": []}, {"text": "The main evaluation in the previous section is based on a list of unique words, which means that in the results each word is equally weighted.", "labels": [], "entities": []}, {"text": "Because cross validation is applied, errors are always measured on testing subsets that are disjoint from the corresponding training subsets.", "labels": [], "entities": []}, {"text": "Hence, the accuracy achieved can be interpreted as the performance expected when hyphenating unknown words, i.e. rare future words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9990978240966797}]}, {"text": "However, in real documents common words appear repeatedly.", "labels": [], "entities": []}, {"text": "Therefore, the second lessstandard experimental design for which we report results restricts attention to the most common English words.", "labels": [], "entities": []}, {"text": "Specifically, we consider the top 4000 words that makeup about three quarters of all word appearances in the American National Corpus, which consists of 18,300,430 words from written texts of all genres.", "labels": [], "entities": [{"text": "American National Corpus", "start_pos": 109, "end_pos": 133, "type": "DATASET", "confidence": 0.9510192275047302}]}, {"text": "From the 4,471 most frequent words in this list, if we omit the words not in our dataset of 89,019 hyphenated English words from CELEX, we get 4,000 words.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 129, "end_pos": 134, "type": "DATASET", "confidence": 0.8745180368423462}]}, {"text": "The words that are omitted are proper names, contractions, incomplete words containing apostrophes, and abbreviations such as DNA.", "labels": [], "entities": []}, {"text": "These 4,000 most frequent words makeup 74.93% of the whole corpus.", "labels": [], "entities": []}, {"text": "We evaluate the following methods on the 4000 words: Liang's method using the American patterns file hyphen.tex, Liang's method using the patterns derived from PATGEN when trained on the whole English dataset, our CRF trained on the whole English dataset, and the same CRF with a probability threshold of 0.9.", "labels": [], "entities": [{"text": "American patterns file hyphen.tex", "start_pos": 78, "end_pos": 111, "type": "DATASET", "confidence": 0.913591593503952}, {"text": "English dataset", "start_pos": 193, "end_pos": 208, "type": "DATASET", "confidence": 0.7570275664329529}, {"text": "English dataset", "start_pos": 239, "end_pos": 254, "type": "DATASET", "confidence": 0.7602826654911041}]}, {"text": "In summary, T E X and PATGEN make serious errors on 43 and 113 of the 4000 words, respectively.", "labels": [], "entities": [{"text": "T E X", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.7773844003677368}, {"text": "PATGEN", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9025856852531433}]}, {"text": "With a threshold of 0.9, the CRF approach makes zero serious errors on these words.", "labels": [], "entities": []}, {"text": "shows the speed of the alternative methods for the English dataset.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.8756174147129059}]}, {"text": "The column \"Features/Patterns\" in the table reports the number of feature-functions used for the CRF, or the number of patterns used for the T E X algorithm.", "labels": [], "entities": []}, {"text": "Overall, the CRF approach is about ten times slower than the T E X algorithm, but its performance is still acceptable on a standard personal computer.", "labels": [], "entities": [{"text": "CRF", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.8741452097892761}]}, {"text": "All experiments use a machine having a Pentium 4 CPU at 3.20GHz and 2GB memory.", "labels": [], "entities": []}, {"text": "Moreover, informal experiments show that CRF training would be about eight times faster if we used CRFSGD rather than CRF++.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance on the English dataset.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.9293370246887207}]}, {"text": " Table 3: Performance on the Dutch dataset.", "labels": [], "entities": [{"text": "Dutch dataset", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9763034582138062}]}, {"text": " Table 4: Performance on the English dataset (10-fold cross validation dividing by stem).", "labels": [], "entities": [{"text": "English dataset", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.8782169222831726}]}, {"text": " Table 5: Performance on the Dutch dataset (10-fold cross validation dividing by stem).", "labels": [], "entities": [{"text": "Dutch dataset", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9717003107070923}]}, {"text": " Table 6: Performance on the 4000 most frequent English words.", "labels": [], "entities": []}, {"text": " Table 7: Timings for the English dataset (training  and testing on the whole dataset that consists of  65,828 words).", "labels": [], "entities": [{"text": "English dataset", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.8407774865627289}]}]}