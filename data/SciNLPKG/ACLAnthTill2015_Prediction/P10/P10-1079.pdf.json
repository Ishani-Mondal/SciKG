{"title": [{"text": "A hybrid rule/model-based finite-state framework for normalizing SMS messages", "labels": [], "entities": [{"text": "normalizing SMS", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.8782402276992798}]}], "abstractContent": [{"text": "In recent years, research in natural language processing has increasingly focused on normalizing SMS messages.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.6707955002784729}, {"text": "normalizing SMS messages", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.8486502170562744}]}, {"text": "Different well-defined approaches have been proposed, but the problem remains far from being solved: best systems achieve a 11% Word Error Rate.", "labels": [], "entities": [{"text": "Word Error Rate", "start_pos": 128, "end_pos": 143, "type": "METRIC", "confidence": 0.8235353032747904}]}, {"text": "This paper presents a method that shares similarities with both spell checking and machine translation approaches.", "labels": [], "entities": [{"text": "spell checking", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.8973798453807831}, {"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7160756140947342}]}, {"text": "The normalization part of the system is entirely based on models trained from a corpus.", "labels": [], "entities": [{"text": "normalization", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.967766523361206}]}, {"text": "Evaluated in French by 10-fold-cross validation, the system achieves a 9.3% Word Error Rate and a 0.83 BLEU score.", "labels": [], "entities": [{"text": "Word Error Rate", "start_pos": 76, "end_pos": 91, "type": "METRIC", "confidence": 0.8376693725585938}, {"text": "BLEU score", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.982324481010437}]}], "introductionContent": [{"text": "Introduced a few years ago, Short Message Service (SMS) offers the possibility of exchanging written messages between mobile phones.", "labels": [], "entities": [{"text": "Short Message Service (SMS)", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.536686529715856}]}, {"text": "SMS has quickly been adopted by users.", "labels": [], "entities": []}, {"text": "These messages often greatly deviate from traditional spelling conventions.", "labels": [], "entities": []}, {"text": "As shown by specialists (, this variability is due to the simultaneous use of numerous coding strategies, like phonetic plays (2m1 read 'demain', \"tomorrow\"), phonetic transcriptions (kom instead of 'comme', \"like\"), consonant skeletons (tjrs for 'toujours', \"always\"), misapplied, missing or incorrect separators (j esper for 'j'esp\u00e8re', \"I hope\"; j'croibi1k, instead of 'je crois bien que', \"I am pretty sure that\"), etc.", "labels": [], "entities": []}, {"text": "These deviations are due to three main factors: the small number of characters allowed per text message by the service (140 bytes), the constraints of the small phones' keypads and, last but not least, the fact that people mostly communicate between friends and relatives in an informal register.", "labels": [], "entities": []}, {"text": "Whatever their causes, these deviations considerably hamper any standard natural language processing (NLP) system, which stumbles against so many Out-Of-Vocabulary words.", "labels": [], "entities": []}, {"text": "For this reason, as noted by, an SMS normalization must be performed before a more conventional NLP process can be applied.", "labels": [], "entities": [{"text": "SMS normalization", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8898734450340271}]}, {"text": "As defined by, \"SMS normalization consists in rewriting an SMS text using a more conventional spelling, in order to make it more readable fora human or fora machine.\"", "labels": [], "entities": [{"text": "SMS normalization", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.8749510645866394}]}, {"text": "The SMS normalization we present here was developed in the general framework of an SMSto-speech synthesis system 1 . This paper, however, only focuses on the normalization process.", "labels": [], "entities": [{"text": "SMS normalization", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7990220189094543}]}, {"text": "Evaluated in French, our method shares similarities with both spell checking and machine translation.", "labels": [], "entities": [{"text": "spell checking", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.8762568235397339}, {"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7081559896469116}]}, {"text": "The machine translation-like module of the system performs the true normalization task.", "labels": [], "entities": []}, {"text": "It is entirely based on models learned from an SMS corpus and its transcription, aligned at the character-level in order to get parallel corpora.", "labels": [], "entities": []}, {"text": "Two spell checking-like modules surround the normalization module.", "labels": [], "entities": [{"text": "spell checking-like", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7766938805580139}]}, {"text": "The first one detects unambiguous tokens, like URLs or phone numbers, to keep them out of the normalization.", "labels": [], "entities": []}, {"text": "The second one, applied on the normalized parts only, identifies non-alphabetic sequences, like punctuations, and labels them with the corresponding token.", "labels": [], "entities": []}, {"text": "This greatly helps the system's print module to follow the basic rules of typography.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 proposes an overview of the state of the art.", "labels": [], "entities": []}, {"text": "Section 3 presents the general architecture of our system, while Section 4 focuses on how we learn and combine our normalization models.", "labels": [], "entities": []}, {"text": "Section 5 evaluates the system and compares it to The Vocalise project.", "labels": [], "entities": []}, {"text": "See cental.fltr.ucl.ac.be/team/projects/vocalise/.", "labels": [], "entities": []}, {"text": "Section 6 draws conclusions and considers some future possible improvements of the method.", "labels": [], "entities": []}], "datasetContent": [{"text": "The performance and the efficiency of our system were evaluated on a MacBook Pro with a 2.4 GHz Intel Core 2 Duo CPU, 4 GB 667 MHz DDR2 SDRAM, running Mac OS X version 10.5.8.", "labels": [], "entities": []}, {"text": "The evaluation was performed on the corpus of 30,000 French SMS presented in Section 4.2, by ten-fold cross-validation.", "labels": [], "entities": [{"text": "French SMS presented in Section 4.2", "start_pos": 53, "end_pos": 88, "type": "DATASET", "confidence": 0.8603494465351105}]}, {"text": "The principle of this method of evaluation is to split the initial corpus into 10 subsets of equal size.", "labels": [], "entities": []}, {"text": "The system is then trained 10 times, each time leaving out one of the subsets from the training corpus, but using only this omitted subset as test corpus.", "labels": [], "entities": []}, {"text": "The language model of the evaluation is a 3-gram.", "labels": [], "entities": []}, {"text": "We did not try a 4-gram.", "labels": [], "entities": []}, {"text": "This choice was motivated by the experiments of, who showed on a French corpus comparable to ours that, if using a larger language model is always rewarded, the improvement quickly decreases with every higher level and is already quite small between 2-gram and 3-gram.", "labels": [], "entities": []}, {"text": "presents the results in terms of efficiency.", "labels": [], "entities": []}, {"text": "The system seems efficient, while we cannot compare it with other methods, which did not provide us with this information., part 1, presents the performance of our approach (Hybrid) and compares it to a trivial copy-paste (Copy).", "labels": [], "entities": []}, {"text": "The system was evaluated in terms of BLEU score (), Word Error Rate (WER) and Sentence Error Rate (SER).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9873070418834686}, {"text": "Word Error Rate (WER)", "start_pos": 52, "end_pos": 73, "type": "METRIC", "confidence": 0.9337610205014547}, {"text": "Sentence Error Rate (SER)", "start_pos": 78, "end_pos": 103, "type": "METRIC", "confidence": 0.8575964073340098}]}, {"text": "Concerning WER, the table presents the distribution between substitutions (Sub), deletions (Del) and insertions (Ins).", "labels": [], "entities": [{"text": "WER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.39066940546035767}, {"text": "insertions (Ins)", "start_pos": 101, "end_pos": 117, "type": "METRIC", "confidence": 0.8572916388511658}]}, {"text": "The copy-paste results just inform about the real deviation of our corpus from the traditional spelling conventions, and highlight the fact that our system is still at pains to significantly reduce the SER, while results in terms of WER and BLEU score are quite encouraging., part 2, provides the results of the state-of-the-art approaches.", "labels": [], "entities": [{"text": "SER", "start_pos": 202, "end_pos": 205, "type": "METRIC", "confidence": 0.9934782981872559}, {"text": "WER", "start_pos": 233, "end_pos": 236, "type": "METRIC", "confidence": 0.9980379939079285}, {"text": "BLEU score", "start_pos": 241, "end_pos": 251, "type": "METRIC", "confidence": 0.9848148822784424}]}, {"text": "The only results truly comparable to ours are those of Guimier de, who evaluated their approach on the same corpus as ours ; clearly, our method They performed an evaluation without ten-fold cross- The analysis of the normalizations produced by our system pointed out that, most often, errors are contextual and concern the gender (quel(le), \"what\"), the number (bisou(s), \"kiss\"), the person ([tu t']inqui\u00e8te(s), \"you are worried\") or the tense (arriv\u00e9/arriver, \"arrived\"/\"to arrive\").", "labels": [], "entities": []}, {"text": "That contextual errors are frequent is not surprising.", "labels": [], "entities": []}, {"text": "In French, as mentioned by, ngram models are unable to catch this information, as it is generally out of their scope.", "labels": [], "entities": []}, {"text": "On the other hand, this analysis confirmed our initial assumptions.", "labels": [], "entities": []}, {"text": "First, special tokens (URLs, phones, etc.) are not modified.", "labels": [], "entities": []}, {"text": "Second, agglutinated words are generally split (Pensa ms \u2192 Pense \u00e0 mes, \"think to my\"), while misapplied separators tend to be deleted (G t \u2192 J'\u00e9tais, \"I was\").", "labels": [], "entities": [{"text": "Pensa ms \u2192 Pense \u00e0 mes", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.6644419183333715}]}, {"text": "Of course, we also found some errors at word boundaries ([il] l'arrange \u2192 [il] la range, \"[he] arranges\" \u2192 \"[he] pits in order\"), but they were fairly rare.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Efficiency of the system.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9932602047920227}]}, {"text": " Table 2: Performance of the system. ( * ) Kobus 2008-1 corresponds to the ASR-like system, while  Kobus 2008-2 is a combination of this system with a series of open-source machine translation toolkits.  ( *  * ) Scores obtained on noisy data only, out of the sentence's context.", "labels": [], "entities": [{"text": "Kobus 2008-1", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.9704121053218842}, {"text": "Kobus 2008-2", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.957223653793335}]}]}