{"title": [{"text": "Unsupervised Event Coreference Resolution with Rich Linguistic Features", "labels": [], "entities": [{"text": "Unsupervised Event Coreference Resolution", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.6366856396198273}]}], "abstractContent": [{"text": "This paper examines how anew class of nonparametric Bayesian models can be effectively applied to an open-domain event coreference task.", "labels": [], "entities": [{"text": "open-domain event coreference task", "start_pos": 101, "end_pos": 135, "type": "TASK", "confidence": 0.7141744270920753}]}, {"text": "Designed with the purpose of clustering complex linguistic objects , these models consider a potentially infinite number of features and categorical outcomes.", "labels": [], "entities": []}, {"text": "The evaluation performed for solving both within-and cross-document event coreference shows significant improvements of the models when compared against two baselines for this task.", "labels": [], "entities": [{"text": "cross-document event coreference", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.5989197790622711}]}], "introductionContent": [{"text": "The event coreference task consists of finding clusters of event mentions that refer to the same event.", "labels": [], "entities": []}, {"text": "Although it has not been extensively studied in comparison with the related problem of entity coreference resolution, solving event coreference has already proved its usefulness in various applications such as topic detection and tracking (), information extraction (), question answering (), textual entailment (), and contradiction detection (.", "labels": [], "entities": [{"text": "entity coreference resolution", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.7578112781047821}, {"text": "topic detection and tracking", "start_pos": 210, "end_pos": 238, "type": "TASK", "confidence": 0.8327511250972748}, {"text": "information extraction", "start_pos": 243, "end_pos": 265, "type": "TASK", "confidence": 0.8027689456939697}, {"text": "question answering", "start_pos": 270, "end_pos": 288, "type": "TASK", "confidence": 0.9004983305931091}, {"text": "textual entailment", "start_pos": 293, "end_pos": 311, "type": "TASK", "confidence": 0.7193455100059509}, {"text": "contradiction detection", "start_pos": 320, "end_pos": 343, "type": "TASK", "confidence": 0.7219180315732956}]}, {"text": "Previous approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (.", "labels": [], "entities": [{"text": "solving event coreference", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.6801210542519888}]}, {"text": "In spite of being successful fora particular labeled corpus, these pairwise models are dependent on the domain or language that they are trained on.", "labels": [], "entities": []}, {"text": "Moreover, since event coreference resolution is a complex task that involves exploring a rich set of linguistic features, annotating a large corpus with event coreference information fora new language or domain of interest requires a substantial amount of manual effort.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.7900384068489075}]}, {"text": "Also, since these models are dependent on local pairwise decisions, they are unable to capture a global event distribution at topic or document collection level.", "labels": [], "entities": []}, {"text": "To address these limitations and to provide a more flexible representation for modeling observable data with rich properties, we present two novel, fully generative, nonparametric Bayesian models for unsupervised within-and crossdocument event coreference resolution.", "labels": [], "entities": [{"text": "crossdocument event coreference resolution", "start_pos": 224, "end_pos": 266, "type": "TASK", "confidence": 0.6692262664437294}]}, {"text": "The first model extends the hierarchical Dirichlet process) to take into account additional properties associated with observable objects (i.e., event mentions).", "labels": [], "entities": []}, {"text": "The second model overcomes some of the limitations of the first model.", "labels": [], "entities": []}, {"text": "It uses the infinite factorial hidden Markov model) coupled to the infinite hidden Markov model) in order to (1) consider a potentially infinite number of features associated with observable objects, (2) perform an automatic selection of the most salient features, and (3) capture the structural dependencies of observable objects at the discourse level.", "labels": [], "entities": []}, {"text": "Furthermore, both models are designed to account fora potentially infinite number of categorical outcomes (i.e., events).", "labels": [], "entities": []}, {"text": "These models provide additional details and experimental results to our preliminary work on unsupervised event coreference resolution ().", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.7590235769748688}]}], "datasetContent": [{"text": "Datasets One dataset we employed is the automatic content extraction (ACE)).", "labels": [], "entities": [{"text": "automatic content extraction (ACE))", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.7578226874272028}]}, {"text": "However, the utilization of the ACE corpus for the task of solving event coreference is limited because this resource provides only withindocument event coreference annotations using a restricted set of event types such as LIFE, BUSI-NESS, CONFLICT, and JUSTICE.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.8707015514373779}, {"text": "solving event coreference", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.7515597144762675}, {"text": "BUSI-NESS", "start_pos": 229, "end_pos": 238, "type": "METRIC", "confidence": 0.8850626349449158}]}, {"text": "Therefore, as a second dataset, we created the EventCorefBank (ECB) corpus to increase the diversity of event types and to be able to evaluate our models for both within-and cross-document event coreference resolution.", "labels": [], "entities": [{"text": "EventCorefBank (ECB) corpus", "start_pos": 47, "end_pos": 74, "type": "DATASET", "confidence": 0.658628273010254}, {"text": "cross-document event coreference resolution", "start_pos": 174, "end_pos": 217, "type": "TASK", "confidence": 0.6743444874882698}]}, {"text": "One important step in the creation process of this corpus consists in finding sets of related documents that describe the same seminal event such that the annotation of coreferential event mentions across documents is possible.", "labels": [], "entities": []}, {"text": "For this purpose, we selected from the GoogleNews archive 7 various topics whose description contains keywords such as commercial transaction, attack, death, sports, terrorist act, election, arrest, natural disaster, etc.", "labels": [], "entities": [{"text": "GoogleNews archive", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.9070446789264679}]}, {"text": "The entire annotation process for creating the ECB resource is described in. lists several basic statistics extracted from these two corpora.", "labels": [], "entities": [{"text": "ECB resource", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.8910156786441803}]}, {"text": "Evaluation For a more realistic approach, we not only trained the models on the manually annotated event mentions (i.e., true mentions), but also on all the possible mentions encoded in the two datasets.", "labels": [], "entities": []}, {"text": "To extract all event mentions, we ran the event identifier described in.", "labels": [], "entities": []}, {"text": "The mentions extracted by this system (i.e., system men-6 ECB is available at http://www.hlt.utdallas.edu/\u223cady.", "labels": [], "entities": []}, {"text": "tions) were able to coverall the true mentions from both datasets.", "labels": [], "entities": []}, {"text": "As shown in, we extracted from ACE and ECB corpora 45289 and 21175 system mentions, respectively.", "labels": [], "entities": [{"text": "ACE", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.9485552310943604}, {"text": "ECB corpora 45289", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.9167844653129578}]}, {"text": "We report results in terms of recall (R), precision (P), and F-score (F) by employing the mention-based B 3 metric (, the entity-based CEAF metric (, and the pairwise F1 (PW) metric.", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9601070731878281}, {"text": "precision (P)", "start_pos": 42, "end_pos": 55, "type": "METRIC", "confidence": 0.9486453533172607}, {"text": "F-score (F)", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9622171223163605}, {"text": "mention-based B 3 metric", "start_pos": 90, "end_pos": 114, "type": "METRIC", "confidence": 0.8198011517524719}, {"text": "F1 (PW) metric", "start_pos": 167, "end_pos": 181, "type": "METRIC", "confidence": 0.9130219459533692}]}, {"text": "All the results are averaged over 5 runs of the generative models.", "labels": [], "entities": []}, {"text": "In the evaluation process, we considered only the true mentions of the ACE test dataset, and the event mentions of the test sets derived from a 5-fold cross validation scheme on the ECB dataset.", "labels": [], "entities": [{"text": "ACE test dataset", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.9578465024630228}, {"text": "ECB dataset", "start_pos": 182, "end_pos": 193, "type": "DATASET", "confidence": 0.9889264106750488}]}, {"text": "For evaluating the cross-document coreference annotations, we adopted the same approach as described in () by merging all the documents from the same topic into a meta-document and then scoring this document as performed for within-document evaluation.", "labels": [], "entities": []}, {"text": "For both corpora, we considered a set of 132 feature types, where each feature type consists on average of 3900 distinct feature values.", "labels": [], "entities": []}, {"text": "Baselines We consider two baselines for event coreference resolution (rows 1&2 in One baseline groups each event mention by its event class (BL eclass ).", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.8510046601295471}]}, {"text": "Therefore, for this baseline, we cluster mentions according to their corresponding EC feature value.", "labels": [], "entities": [{"text": "EC feature value", "start_pos": 83, "end_pos": 99, "type": "METRIC", "confidence": 0.9357597629229227}]}, {"text": "Similarly, the second baseline uses as grouping criteria for event mentions their corresponding WNS feature value (BL syn ).", "labels": [], "entities": []}, {"text": "HDP Extensions Due to memory limitations, we evaluated the HDP models on a restricted set of manually selected feature types.", "labels": [], "entities": []}, {"text": "In general, the HDP 1f model with the feature type HL, which plays the role of a baseline for the HDP flat and HDP struct models, outperforms both baselines on the ACE and ECB datasets.", "labels": [], "entities": [{"text": "ACE and ECB datasets", "start_pos": 164, "end_pos": 184, "type": "DATASET", "confidence": 0.7579183578491211}]}, {"text": "For the HDP flat models (rows 4-7 in Tables 2&3), we classified the experiments according to the set of feature types described in Section 2.", "labels": [], "entities": []}, {"text": "Our experiments reveal that the best configuration of features for this model  consists of a combination of feature types from all the categories of features (row 7).", "labels": [], "entities": []}, {"text": "For the HDP struct experiments, we considered the set of features of the best HDP flat experiment as well as the dependencies between HL, FR, and FEA.", "labels": [], "entities": [{"text": "FR", "start_pos": 138, "end_pos": 140, "type": "METRIC", "confidence": 0.9776124358177185}, {"text": "FEA", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.9849547147750854}]}, {"text": "Overall, we can assert that HDP flat achieved the best performance results on the ACE test dataset (Table 3), whereas HDP struct proved to be more effective on the ECB dataset.", "labels": [], "entities": [{"text": "ACE test dataset", "start_pos": 82, "end_pos": 98, "type": "DATASET", "confidence": 0.9725853800773621}, {"text": "ECB dataset", "start_pos": 164, "end_pos": 175, "type": "DATASET", "confidence": 0.985723227262497}]}, {"text": "Moreover, the results of the HDP flat and HDP struct models show an F-score increase by 4-10% over HDP 1f , and therefore, the results prove that the HDP extension provides a more flexible representation for clustering objects with rich properties.", "labels": [], "entities": [{"text": "F-score", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.9990948438644409}]}, {"text": "We also plot the evolution of our generative processes.", "labels": [], "entities": []}, {"text": "For instance, shows that the HDP flat model corresponding to row 7 in Table 3 converges in 350 iteration steps to a posterior distribution over event mentions from ACE with around 2000 latent events.", "labels": [], "entities": []}, {"text": "Additionally, our experiments with different values of the \u03bb parameter for the Lidstone's smoothing method indicate that this smoothing method is useful for improving the performance of the HDP models.", "labels": [], "entities": []}, {"text": "However, we could not find a \u03bb value in our experiments that brings a major improvement over the non-smoothed HDP models.", "labels": [], "entities": []}, {"text": "To show the usefulness of the sampling schemes considered for this model, we also compare in the results obtained by an iFHMMiHMM model that considers all the feature values associated with an observable object (iFHMMiHMM all ) against the iFHMM-iHMM models that employ the mIBP sampling scheme together with the unf iltered, discrete, median, and uniform filtering schemes.", "labels": [], "entities": []}, {"text": "Because of the memory limitation constraints, we performed the experiments listed in by selecting only a subset from  the feature types which proved to be salient in the HDP experiments.", "labels": [], "entities": []}, {"text": "As listed in, all the iFHMM-iHMM models that used a feature sampling scheme significantly outperform the iFHMM-iHMM all model; this proves that all the sampling schemes considered in the iFHMMiHMM framework are able to successfully filter out noisy and redundant feature values.", "labels": [], "entities": []}, {"text": "The closest comparison to prior work is the supervised approach described in) that achieved a 92.2% B 3 F-measure on the ACE corpus.", "labels": [], "entities": [{"text": "B 3", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9159174561500549}, {"text": "F-measure", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.5375432968139648}, {"text": "ACE corpus", "start_pos": 121, "end_pos": 131, "type": "DATASET", "confidence": 0.976914644241333}]}, {"text": "However, for this result, ground truth event mentions as well as a manually tuned coreference threshold were employed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the ACE and ECB corpora.", "labels": [], "entities": [{"text": "ACE and ECB corpora", "start_pos": 28, "end_pos": 47, "type": "DATASET", "confidence": 0.767035037279129}]}, {"text": " Table 2: Results for within-document (WD) and cross-document (WD) coreference resolution on the ECB dataset.", "labels": [], "entities": [{"text": "cross-document (WD) coreference resolution", "start_pos": 47, "end_pos": 89, "type": "TASK", "confidence": 0.5892729659875234}, {"text": "ECB dataset", "start_pos": 97, "end_pos": 108, "type": "DATASET", "confidence": 0.9781116843223572}]}, {"text": " Table 3: Results for WD coreference resolution on ACE.", "labels": [], "entities": [{"text": "WD coreference resolution", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.9450482924779257}, {"text": "ACE", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.901059091091156}]}, {"text": " Table 4: Feature non-sampling vs. feature sampling in the  iFHMM-iHMM model.", "labels": [], "entities": []}]}