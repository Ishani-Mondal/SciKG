{"title": [], "abstractContent": [{"text": "We present an efficient algorithm for computing the weakest readings of semantically ambiguous sentences.", "labels": [], "entities": []}, {"text": "A corpus-based evaluation with a large-scale grammar shows that our algorithm reduces over 80% of sentences to one or two readings, in negligible runtime, and thus makes it possible to work with semantic representations derived by deep large-scale grammars.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past few years, there has been considerable progress in the ability of manually created large-scale grammars, such as the English Resource Grammar (ERG,) or the ParGram grammars), to parse wide-coverage text and assign it deep semantic representations.", "labels": [], "entities": []}, {"text": "While applications should benefit from these very precise semantic representations, their usefulness is limited by the presence of semantic ambiguity: On the Rondane Treebank (), the ERG computes an average of several million semantic representations for each sentence, even when the syntactic analysis is fixed.", "labels": [], "entities": [{"text": "Rondane Treebank", "start_pos": 158, "end_pos": 174, "type": "DATASET", "confidence": 0.815453052520752}]}, {"text": "The problem of appropriately selecting one of them to work with would ideally be solved by statistical methods or knowledge-based inferences.", "labels": [], "entities": []}, {"text": "However, no such approach has been worked out in sufficient detail to support the disambiguation of treebank sentences.", "labels": [], "entities": []}, {"text": "As an alternative, proposes to compute the weakest reading of each sentence and then use it instead of the \"true\" reading of the sentence.", "labels": [], "entities": []}, {"text": "This is based on the observation that the readings of a semantically ambiguous sentence are partially ordered with respect to logical entailment, and the weakest readings -the minimal (least informative) readings with respect to this order -only express \"safe\" information that is common to all other readings as well.", "labels": [], "entities": []}, {"text": "However, when a sentence has millions of readings, finding the weakest reading is a hard problem.", "labels": [], "entities": []}, {"text": "It is of course completely infeasible to compute all readings and compare all pairs for entailment; but even the best known algorithm in the literature () is only an optimization of this basic strategy, and would take months to compute the weakest readings for the sentences in the Rondane Treebank.", "labels": [], "entities": [{"text": "Rondane Treebank", "start_pos": 282, "end_pos": 298, "type": "DATASET", "confidence": 0.8725895583629608}]}, {"text": "In this paper, we propose anew, efficient approach to the problem of computing weakest readings.", "labels": [], "entities": []}, {"text": "We follow an underspecification approach to managing ambiguity: Rather than deriving all semantic representations from the syntactic analysis, we work with a single, compact underspecified semantic representation, from which the semantic representations can then be extracted by need.", "labels": [], "entities": []}, {"text": "We then approximate entailment with a rewrite system that rewrites readings into logically weaker readings; the weakest readings are exactly those readings that cannot be rewritten into some other reading anymore (the relative normal forms).", "labels": [], "entities": []}, {"text": "We present an algorithm that computes the relative normal forms, and evaluate it on the underspecified descriptions that the ERG derives on a 624-sentence subcorpus of the Rondane Treebank.", "labels": [], "entities": [{"text": "Rondane Treebank", "start_pos": 172, "end_pos": 188, "type": "DATASET", "confidence": 0.7306776493787766}]}, {"text": "While the mean number of scope readings in the subcorpus is in the millions, our system computes on average 4.5 weakest readings for each sentence, in less than twenty milliseconds; over 80% of all sentences are reduced to at most two weakest readings.", "labels": [], "entities": []}, {"text": "In other words, we make it feasible for the first time to build an application that uses the individual (weakest) semantic representations computed by the ERG, both in terms of the remaining ambiguity and in terms of performance.", "labels": [], "entities": []}, {"text": "Our technique is not limited to the ERG, but should be applicable to other underspecification-based grammars as well.", "labels": [], "entities": []}, {"text": "Technically, we use underspecified descriptions that are regular tree grammars derived from dominance graphs ().", "labels": [], "entities": []}, {"text": "We compute the weakest readings by intersecting these grammars with other grammars representing the rewrite rules.", "labels": [], "entities": []}, {"text": "This approach can be used much more generally than just for the computation of weakest readings; we illustrate this by showing how a more general version of the redundancy elimination algorithm by can be seen as a special case of our construction.", "labels": [], "entities": [{"text": "redundancy elimination", "start_pos": 161, "end_pos": 183, "type": "TASK", "confidence": 0.7474715113639832}]}, {"text": "Thus our system can serve as a general framework for removing unintended readings from an underspecified representation.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 starts by reviewing related work.", "labels": [], "entities": []}, {"text": "We recall dominance graphs, regular tree grammars, and the basic ideas of underspecification in Section 3, before we show how to compute weakest readings (Section 4) and logical equivalences (Section 5).", "labels": [], "entities": []}, {"text": "In Section 6, we define a weakening rewrite system for the ERG and evaluate it on the Rondane Treebank.", "labels": [], "entities": [{"text": "Rondane Treebank", "start_pos": 86, "end_pos": 102, "type": "DATASET", "confidence": 0.9289226233959198}]}, {"text": "Section 7 concludes and points to future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the effectiveness and efficiency of our weakest readings algorithm on a treebank.", "labels": [], "entities": []}, {"text": "We compute RTGs for all sentences in the treebank and measure how many weakest readings remain after the intersection, and how much time this computation takes.", "labels": [], "entities": []}, {"text": "For our experiment, we use the Rondane treebank (version of January 2006), a \"Redwoods style\") treebank containing underspecified representations (USRs) in the MRS formalism) for sentences from the tourism domain.", "labels": [], "entities": [{"text": "Rondane treebank (version of January 2006", "start_pos": 31, "end_pos": 72, "type": "DATASET", "confidence": 0.862863268171038}]}, {"text": "Our implementation of the relative normal forms algorithm is based on Utool ( ), which (among other things) can translate a large class of MRS descriptions into hypernormally connected dominance graphs and further into RTGs as in Section 3.", "labels": [], "entities": [{"text": "MRS descriptions", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.9005832970142365}]}, {"text": "The implementation exploits certain properties of RTGs computed from dominance graphs to maximize efficiency.", "labels": [], "entities": []}, {"text": "We will make this implementation publically available as part of the next Utool release.", "labels": [], "entities": [{"text": "Utool release", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.8328498005867004}]}, {"text": "We use Utool to automatically translate the 999 MRS descriptions for which this is possible into RTGs.", "labels": [], "entities": [{"text": "999 MRS descriptions", "start_pos": 44, "end_pos": 64, "type": "DATASET", "confidence": 0.8229297200838724}]}, {"text": "To simplify the specification of the rewrite systems, we restrict ourselves to the subcorpus in which all scope-taking operators (labels with arity > 0) occur at least ten times.", "labels": [], "entities": []}, {"text": "This subset contains 624 dominance graphs.", "labels": [], "entities": []}, {"text": "We refer to this subset as \"RON10.\"", "labels": [], "entities": [{"text": "RON10", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.9005809426307678}]}, {"text": "For each dominance graph D that we obtain by converting an MRS description, we take G D as a grammar over the signature That is, we distinguish possible different occurrences of the same symbol in D by marking each occur-rence with the name of the node.", "labels": [], "entities": []}, {"text": "This makes G D a deterministic grammar.", "labels": [], "entities": []}, {"text": "We then specify an annotator over \u03a3 that assigns polarities for the weakening rewrite system.", "labels": [], "entities": []}, {"text": "We distinguish three polarities: + for positive occurrences, \u2212 for negative occurrences (as in predicate logic), and \u22a5 for contexts in which a weakening rule neither weakens or strengthens the entire formula.", "labels": [], "entities": []}, {"text": "The starting annotation is +.", "labels": [], "entities": []}, {"text": "Finally, we need to decide upon each scopetaking operator's effects on these annotations.", "labels": [], "entities": []}, {"text": "To this end, we build upon classification of the monotonicity properties of determiners.", "labels": [], "entities": []}, {"text": "A determiner is upward (downward) monotonic if making the denotation of the determiner's argument bigger (smaller) makes the sentence logically weaker.", "labels": [], "entities": []}, {"text": "For instance, every is downward monotonic in its first argument and upward monotonic in its second argument, i.e. every girl kissed a boy entails every blond girl kissed someone.", "labels": [], "entities": []}, {"text": "Thus ann(every u , a, 1) = \u2212a and ann(every u , a, 2) = a (where u is anode name as above).", "labels": [], "entities": []}, {"text": "There are also determiners with nonmonotonic argument positions, which assign the annotation \u22a5 to this argument.", "labels": [], "entities": []}, {"text": "Negation reverses positive and negative polarity, and all other nonquantifiers simply pass on their annotation to the arguments.", "labels": [], "entities": []}, {"text": "We use the following weakening rewrite system for our experiment, where i \u2208 {1, 2}: Here the symbols E, D, etc.", "labels": [], "entities": []}, {"text": "stand for classes of labels in \u03a3, and a rule schema (C/i, C /k) is to be read as shorthand fora set of rewrite rules which rearrange a tree where the i-th child of a symbol from C is a symbol from C into a tree where the symbol from C becomes the k-th child of the symbol from C . For example, because we have all u \u2208 A and not v \u2208 N, Schema 4 licenses the following annotated rewrite rule: We write E and D for existential and definite determiners.", "labels": [], "entities": []}, {"text": "P stands for proper names and pronouns, A stands for universal determiners like all and each, N for the negation not, and M for modal operators like can or would.", "labels": [], "entities": []}, {"text": "M also includes intensional verbs like have to and want.", "labels": [], "entities": []}, {"text": "Notice that while the reverse rules are applicable in negative polarities, no rules are applicable in polarity \u22a5.", "labels": [], "entities": []}, {"text": "Rule schema 1 states, for instance, that the specific (wide-scope) reading of the indefinite in the president of a company is logically stronger than the reading in which a company is within the restriction of the definite determiner.", "labels": [], "entities": []}, {"text": "The schema is intuitively plausible, and it can also be proved to be logically sound if we make the standard assumption that the definite determiner the means \"exactly one\".", "labels": [], "entities": []}, {"text": "A similar argument applies to rule schema 2.", "labels": [], "entities": []}, {"text": "Rule schema 3 encodes the classical entailment (1).", "labels": [], "entities": []}, {"text": "Schema 4 is similar to the rule (2).", "labels": [], "entities": []}, {"text": "Notice that it is not, strictly speaking, logically sound; however, because strong determiners like all or every carry a presupposition that their restrictions have a non-empty denotation, the schema becomes sound for all instances that can be expressed in natural language.", "labels": [], "entities": []}, {"text": "Similar arguments apply to rule schemas 5 and 6, which are potentially unsound for subtle reasons involving the logical interpretation of intensional expressions.", "labels": [], "entities": []}, {"text": "However, these cases of unsoundness did not occur in our test corpus.", "labels": [], "entities": []}, {"text": "In addition, we assume the following equation system for redundancy elimination for i, j \u2208 {1, 2} and k \u2208 N (again written in an analogous shorthand as above): These rule schemata state that permuting existential determiners with each other is an equivalence transformation, and so is permuting definite determiners with existential and definite determiners if one determiner is the second argument (in the scope) of a definite.", "labels": [], "entities": [{"text": "redundancy elimination", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.7457983195781708}]}, {"text": "Schema 10 states that proper names and pronouns, which the ERG analyzes as scope-bearing operators, can permute with any other label.", "labels": [], "entities": []}, {"text": "We orient these equalities into rewrite rules by ordering symbols in P before symbols that are not  in P, and otherwise ordering a symbol f u before a symbol g v if u < v by comparison of the (arbitrary) node names.", "labels": [], "entities": []}, {"text": "We used these rewrite systems to compute, for each USR in RON10, the number of all configurations, the number of configurations that remain after redundancy elimination, and the number of weakest readings (i.e., the relative normal forms of the combined equivalence and weakening rewrite systems).", "labels": [], "entities": [{"text": "USR", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.8064166307449341}, {"text": "RON10", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.659711480140686}]}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "By computing weakest readings (WR), we reduce the ambiguity of over 80% of all sentences to one or two readings; this is a clear improvement even over the results of the redundancy elimination (RE).", "labels": [], "entities": [{"text": "weakest readings (WR)", "start_pos": 13, "end_pos": 34, "type": "METRIC", "confidence": 0.6798371613025666}, {"text": "redundancy elimination (RE)", "start_pos": 170, "end_pos": 197, "type": "METRIC", "confidence": 0.7006064593791962}]}, {"text": "Computing weakest readings reduces the mean number of readings from several million to 4.5, and improves over the RE results by a factor of 30.", "labels": [], "entities": [{"text": "mean number of readings", "start_pos": 39, "end_pos": 62, "type": "METRIC", "confidence": 0.9467681497335434}, {"text": "RE", "start_pos": 114, "end_pos": 116, "type": "METRIC", "confidence": 0.9892850518226624}]}, {"text": "Notice that the RE algorithm from Section 5 is itself an improvement over system (\"KRT08\" in the table), which could not process the rule schema 10.", "labels": [], "entities": []}, {"text": "Finally, computing the weakest readings takes only a tiny amount of extra runtime compared to the RE elimination or even the computation of the RTGs (reported as the runtime for \"All\").", "labels": [], "entities": [{"text": "RE elimination", "start_pos": 98, "end_pos": 112, "type": "METRIC", "confidence": 0.9178104996681213}]}, {"text": "1 This remains true on the entire Rondane corpus (although the reduction factor is lower because we have no rules for the rare scope-bearers): RE+WR computation takes 32 seconds, compared to 30 seconds for RE.", "labels": [], "entities": [{"text": "Rondane corpus", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.8351527452468872}, {"text": "RE", "start_pos": 143, "end_pos": 145, "type": "METRIC", "confidence": 0.830754816532135}, {"text": "WR", "start_pos": 146, "end_pos": 148, "type": "METRIC", "confidence": 0.5232577919960022}, {"text": "RE", "start_pos": 206, "end_pos": 208, "type": "METRIC", "confidence": 0.7977417707443237}]}, {"text": "In other words, our algorithm brings the semantic ambiguity in the Rondane Treebank down to practically useful levels at a mean runtime investment of a few milliseconds per sentence.", "labels": [], "entities": [{"text": "Rondane Treebank", "start_pos": 67, "end_pos": 83, "type": "DATASET", "confidence": 0.8982719779014587}]}, {"text": "It is interesting to note how the different rule schemas contribute to this reduction.", "labels": [], "entities": []}, {"text": "While the instances of Schemata 1 and 2 are applicable in 340 sentences, the other schemas 3-6 together are only", "labels": [], "entities": []}], "tableCaptions": []}