{"title": [{"text": "Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining", "labels": [], "entities": [{"text": "Generating Templates of Entity Summaries", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7409199953079224}]}], "abstractContent": [{"text": "In this paper, we propose a novel approach to automatic generation of summary templates from given collections of summary articles.", "labels": [], "entities": [{"text": "automatic generation of summary templates from given collections of summary articles", "start_pos": 46, "end_pos": 130, "type": "TASK", "confidence": 0.7869805097579956}]}, {"text": "This kind of summary templates can be useful in various applications.", "labels": [], "entities": []}, {"text": "We first develop an entity-aspect LDA model to simultaneously cluster both sentences and words into aspects.", "labels": [], "entities": []}, {"text": "We then apply frequent subtree pattern mining on the dependency parse trees of the clustered and labeled sentences to discover sentence patterns that well represent the aspects.", "labels": [], "entities": []}, {"text": "Key features of our method include automatic grouping of semantically related sentence patterns and automatic identification of template slots that need to be filled in.", "labels": [], "entities": [{"text": "grouping of semantically related sentence patterns", "start_pos": 45, "end_pos": 95, "type": "TASK", "confidence": 0.8325581649939219}]}, {"text": "We apply our method on five Wikipedia entity categories and compare our method with two baseline methods.", "labels": [], "entities": []}, {"text": "Both quantitative evaluation based on human judgment and qualitative comparison demonstrate the effectiveness and advantages of our method.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we study the task of automatically generating templates for entity summaries.", "labels": [], "entities": []}, {"text": "An entity summary is a short document that gives the most important facts about an entity.", "labels": [], "entities": []}, {"text": "In Wikipedia, for instance, most articles have an introduction section that summarizes the subject entity before the table of contents and other elaborate sections.", "labels": [], "entities": []}, {"text": "These introduction sections are examples of entity summaries we consider.", "labels": [], "entities": [{"text": "entity summaries", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.4899936318397522}]}, {"text": "Summaries of entities from the same category usually share some common structure.", "labels": [], "entities": []}, {"text": "For example, biographies of physicists usually contain facts about the nationality, educational background, affiliation and major contributions of the physicist, whereas introductions of companies usually list information such as the industry, founder and headquarter of the company.", "labels": [], "entities": []}, {"text": "Our goal is to automatically construct a summary template that outlines the most salient types of facts for an entity category, given a collection of entity summaries from this category.", "labels": [], "entities": []}, {"text": "Such kind of summary templates can be very useful in many applications.", "labels": [], "entities": []}, {"text": "First of all, they can uncover the underlying structures of summary articles and help better organize the information units, much in the same way as infoboxes do in Wikipedia.", "labels": [], "entities": []}, {"text": "In fact, automatic template generation provides a solution to induction of infobox structures, which are still highly incomplete in Wikipedia (.", "labels": [], "entities": [{"text": "template generation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7270926684141159}]}, {"text": "A template can also serve as a starting point for human editors to create new summary articles.", "labels": [], "entities": []}, {"text": "Furthermore, with summary templates, we can potentially apply information retrieval and extraction techniques to construct summaries for new entities automatically on the fly, improving the user experience for search engine and question answering systems.", "labels": [], "entities": [{"text": "information retrieval and extraction", "start_pos": 62, "end_pos": 98, "type": "TASK", "confidence": 0.6952768564224243}, {"text": "question answering", "start_pos": 228, "end_pos": 246, "type": "TASK", "confidence": 0.7939348220825195}]}, {"text": "Despite its usefulness, the problem has not been well studied.", "labels": [], "entities": []}, {"text": "The most relevant work is by on automatic creation of domain templates, where the defintion of a domain is similar to our notion of an entity category.", "labels": [], "entities": []}, {"text": "first identify the important verbs fora domain using corpus statistics, and then find frequent parse tree patterns from sentences containing these verbs to construct a domain template.", "labels": [], "entities": []}, {"text": "There are two major limitations of their approach.", "labels": [], "entities": []}, {"text": "First, the focus on verbs restricts the template patterns that can be found.", "labels": [], "entities": []}, {"text": "Second, redundant or related patterns using different verbs to express the same or similar facts cannot be grouped together.", "labels": [], "entities": []}, {"text": "For example, \"won X award\" and \"received X prize\" are considered two different patterns by this approach.", "labels": [], "entities": []}, {"text": "We propose a method that can overcome these two limitations.", "labels": [], "entities": []}, {"text": "Automatic template generation is also related to a number of other problems that have been studied before, in-cluding unsupervised IE pattern discovery ( and automatic generation of Wikipedia articles.", "labels": [], "entities": [{"text": "Automatic template generation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6434404850006104}, {"text": "IE pattern discovery", "start_pos": 131, "end_pos": 151, "type": "TASK", "confidence": 0.9115942319234213}, {"text": "automatic generation of Wikipedia articles", "start_pos": 158, "end_pos": 200, "type": "TASK", "confidence": 0.8362982392311096}]}, {"text": "We discuss the differences of our work from existing related work in Section 6.", "labels": [], "entities": []}, {"text": "In this paper we propose a novel approach to the task of automatically generating entity summary templates.", "labels": [], "entities": []}, {"text": "We first develop an entity-aspect model that extends standard LDA to identify clusters of words that can represent different aspects of facts that are salient in a given summary collection (Section 3).", "labels": [], "entities": []}, {"text": "For example, the words \"received,\" \"award,\" \"won\" and \"Nobel\" maybe clustered together from biographies of physicists to represent one aspect, even though they may appear in different sentences from different biographies.", "labels": [], "entities": [{"text": "Nobel", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9774304032325745}]}, {"text": "Simultaneously, the entity-aspect model separates words in each sentence into background words, document words and aspect words, and sentences likely about the same aspect are naturally clustered together.", "labels": [], "entities": []}, {"text": "After this aspect identification step, we mine frequent subtree patterns from the dependency parse trees of the clustered sentences (Section 4).", "labels": [], "entities": [{"text": "aspect identification", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.6942298114299774}]}, {"text": "Different from previous work, we leverage the word labels assigned by the entity-aspect model to prune the patterns and to locate template slots to be filled in.", "labels": [], "entities": []}, {"text": "We evaluate our method on five entity categories using Wikipedia articles (Section 5).", "labels": [], "entities": []}, {"text": "Because the task is new and thus there is no standard evaluation criteria, we conduct both quantitative evaluation using our own human judgment and qualitative comparison.", "labels": [], "entities": []}, {"text": "Our evaluation shows that our method can obtain better sentence patterns in terms of f1 measure compared with two baseline methods, and it can also achieve reasonably good quality of aspect clusters in terms of purity.", "labels": [], "entities": [{"text": "purity", "start_pos": 211, "end_pos": 217, "type": "METRIC", "confidence": 0.9759179353713989}]}, {"text": "Compared with standard LDA and K-means sentence clustering, the aspects identified by our method are also more meaningful.", "labels": [], "entities": [{"text": "K-means sentence clustering", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.5815122028191885}]}], "datasetContent": [{"text": "Because we study a non-standard task, there is no existing annotated data set.", "labels": [], "entities": []}, {"text": "We therefore created a small data set and made our own human judgment for quantitative evaluation purpose.", "labels": [], "entities": []}, {"text": "To quantitatively evaluate the summary templates, we want to check (1) whether our sentence patterns are meaningful and can represent the corresponding entity categories well, and (2) whether semantically related sentence patterns are grouped into the same aspect.", "labels": [], "entities": []}, {"text": "It is hard to evaluate both together.", "labels": [], "entities": []}, {"text": "We therefore separate these two criteria.", "labels": [], "entities": []}, {"text": "We also conducted qualitative comparison between our entity-aspect model and standard LDA model as well as a K-means sentence clustering method.", "labels": [], "entities": [{"text": "K-means sentence clustering", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.5799392759799957}]}, {"text": "In, we show the top 5 frequent words of three sample aspects as found by our method, standard LDA, and K-means.", "labels": [], "entities": []}, {"text": "Note that although we try to align the aspects, there is   no correspondence between clusters numbered the same but generated by different methods.", "labels": [], "entities": []}, {"text": "We can see that our method gives very meaningful aspect clusters.", "labels": [], "entities": []}, {"text": "Standard LDA also gives meaningful words, but background words such as \"physics\" and \"physicist\" are mixed with aspect words.", "labels": [], "entities": []}, {"text": "Entity-specific words such as \"john\" also appear mixed with aspect words.", "labels": [], "entities": []}, {"text": "K-means clusters are much less meaningful, with too many background words mixed with aspect words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The number of documents (D), total  number of sentences (S) and minimum, maximum  and average numbers of sentences per document  (S d ) of the data set.", "labels": [], "entities": []}, {"text": " Table 5: The true numbers of aspects as judged  by the human annotator (B), and the purity of the  clusters.", "labels": [], "entities": [{"text": "purity", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9893503785133362}]}, {"text": " Table 4: Quality of sentence patterns in terms of precision, recall and f1.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9995191097259521}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9996258020401001}, {"text": "f1", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.983681321144104}]}, {"text": " Table 6: Comparison of the top 5 words of three  sample aspects using different methods.", "labels": [], "entities": []}]}