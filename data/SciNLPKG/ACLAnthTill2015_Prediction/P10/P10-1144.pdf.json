{"title": [{"text": "Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9085291028022766}]}], "abstractContent": [{"text": "This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B 3 , and CEAF.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.9415738880634308}, {"text": "MUC", "start_pos": 143, "end_pos": 146, "type": "DATASET", "confidence": 0.6695369482040405}, {"text": "B 3", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.8406761288642883}]}, {"text": "By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.7593052089214325}]}, {"text": "The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features.", "labels": [], "entities": [{"text": "coreference resolution evaluation", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.9524662494659424}, {"text": "task definition", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.7154041528701782}]}, {"text": "They also expose systematic biases in the coreference evaluation metrics.", "labels": [], "entities": [{"text": "coreference evaluation", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.8953314125537872}]}, {"text": "We show that system comparison is only possible when corpus parameters are inexact agreement.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of coreference resolution, which aims to automatically identify the expressions in a text that refer to the same discourse entity, has been an increasing research topic in NLP ever since MUC-6 made available the first coreferentially annotated corpus in 1995.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.9446044564247131}]}, {"text": "Most research has centered around the rules by which mentions are allowed to corefer, the features characterizing mention pairs, the algorithms for building coreference chains, and coreference evaluation methods.", "labels": [], "entities": []}, {"text": "The surprisingly important role played by different aspects of the corpus, however, is an issue to which little attention has been paid.", "labels": [], "entities": []}, {"text": "We demonstrate the extent to which a system will be evaluated as performing differently depending on parameters such as the corpus language, the way coreference relations are defined in the corresponding coding scheme, and the nature and source of preprocessing information.", "labels": [], "entities": []}, {"text": "This paper unpacks these issues by running the same system-a prototype entity-based architecture called CISTELL-on different corpus configurations, varying three parameters.", "labels": [], "entities": []}, {"text": "First, we show how much language-specific issues affect performance when trained and tested on English and Spanish.", "labels": [], "entities": []}, {"text": "Second, we demonstrate the extent to which the specific annotation scheme (used on the same corpus) makes evaluated performance vary.", "labels": [], "entities": []}, {"text": "Third, we compare the performance using goldstandard preprocessing information with that using automatic preprocessing tools.", "labels": [], "entities": []}, {"text": "Throughout, we apply the three principal coreference evaluation measures in use today: MUC, B 3 , and CEAF.", "labels": [], "entities": [{"text": "MUC", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.7108467817306519}, {"text": "B 3", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9326673150062561}, {"text": "CEAF", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.5740676522254944}]}, {"text": "We highlight the systematic preferences of each measure to reward different configurations.", "labels": [], "entities": []}, {"text": "This raises the difficult question of why one should use one or another evaluation measure, and how one should interpret their differences in reporting changes of performance score due to 'secondary' factors like preprocessing information.", "labels": [], "entities": []}, {"text": "To this end, we employ three corpora: ACE (),, and AnCora (.", "labels": [], "entities": [{"text": "AnCora", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9914448857307434}]}, {"text": "In order to isolate the three parameters as far as possible, we benefit from a 100k-word portion (from the TDT collection) that is common to both ACE and OntoNotes.", "labels": [], "entities": [{"text": "TDT collection", "start_pos": 107, "end_pos": 121, "type": "DATASET", "confidence": 0.8654961884021759}]}, {"text": "We apply the same coreference resolution system in all cases.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.8712653815746307}]}, {"text": "The results show that a system's score is not informative by itself, as different corpora or corpus parameters lead to different scores.", "labels": [], "entities": []}, {"text": "Our goal is not to achieve the best performance to date, but rather to expose various issues raised by the choices of corpus preparation and evaluation measure and to shed light on the definition, methods, evaluation, and complexities of the coreference resolution task.", "labels": [], "entities": [{"text": "coreference resolution task", "start_pos": 242, "end_pos": 269, "type": "TASK", "confidence": 0.9460691213607788}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 sets our work in context and provides the motivations for undertaking this study.", "labels": [], "entities": []}, {"text": "Section 3 presents the architecture of CISTELL, the system used in the experimental evaluation.", "labels": [], "entities": []}, {"text": "In Sections 4, 5, and 6, we describe the experiments on three different datasets and discuss the results.", "labels": [], "entities": []}, {"text": "We conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since they sometimes provide quite different results, we evaluate using three coreference measures, as there is no agreement on a standard.", "labels": [], "entities": []}, {"text": "It computes the number of links common between the true and system partitions.", "labels": [], "entities": []}, {"text": "Recall (R) and precision (P) result from dividing it by the minimum number of links required to specify the true and the system partitions, respectively.", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.955846756696701}, {"text": "precision (P)", "start_pos": 15, "end_pos": 28, "type": "METRIC", "confidence": 0.9666668325662613}]}, {"text": "\u2022 B 3 (Bagga and Baldwin, 1998).", "labels": [], "entities": [{"text": "Bagga and Baldwin, 1998)", "start_pos": 7, "end_pos": 31, "type": "DATASET", "confidence": 0.7135572185118993}]}, {"text": "Rand P are computed for each mention and averaged at the end.", "labels": [], "entities": [{"text": "Rand P", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9096677303314209}]}, {"text": "For each mention, the number of common mentions between the true and the system entity is divided by the number of mentions in the true entity or in the system entity to obtain Rand P, respectively.", "labels": [], "entities": [{"text": "Rand P", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.7788761854171753}]}, {"text": "It finds the best one-toone alignment between true and system entities.", "labels": [], "entities": []}, {"text": "Using true mentions and the \u03c6 3 similarity function, Rand P are the same and correspond to the number of common mentions between the aligned entities divided by the total number of mentions.", "labels": [], "entities": [{"text": "\u03c6 3 similarity function", "start_pos": 28, "end_pos": 51, "type": "METRIC", "confidence": 0.8597729653120041}]}], "tableCaptions": [{"text": " Table 2: Mention types (%) in Table 1 datasets.", "labels": [], "entities": []}, {"text": " Table 1: Corpus statistics for the large portion of OntoNotes and AnCora.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.8984410762786865}, {"text": "AnCora", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.6942847967147827}]}, {"text": " Table 3: CISTELL results varying the corpus language.", "labels": [], "entities": []}, {"text": " Table 4: Corpus statistics for the aligned portion of ACE and OntoNotes on gold-standard data.", "labels": [], "entities": [{"text": "ACE", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.7083730697631836}, {"text": "OntoNotes", "start_pos": 63, "end_pos": 72, "type": "DATASET", "confidence": 0.8523171544075012}, {"text": "gold-standard data", "start_pos": 76, "end_pos": 94, "type": "DATASET", "confidence": 0.8449176549911499}]}, {"text": " Table 5: CISTELL results varying the annotation scheme on gold-standard data.", "labels": [], "entities": [{"text": "CISTELL", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.6805387735366821}]}, {"text": " Table 6: Corpus statistics for the aligned portion of ACE and OntoNotes on automatically parsed data.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 63, "end_pos": 72, "type": "DATASET", "confidence": 0.8441368341445923}]}, {"text": " Table 7: CISTELL results varying the annotation scheme on automatically preprocessed data.", "labels": [], "entities": []}]}