{"title": [{"text": "Wrapping up a Summary: from Representation to Generation", "labels": [], "entities": [{"text": "Wrapping up a Summary", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7332609444856644}]}], "abstractContent": [{"text": "The main focus of this work is to investigate robust ways for generating summaries from summary representations without recurring to simple sentence extraction and aiming at more human-like summaries.", "labels": [], "entities": [{"text": "summaries from summary representations", "start_pos": 73, "end_pos": 111, "type": "TASK", "confidence": 0.7816920280456543}, {"text": "sentence extraction", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.6921691298484802}]}, {"text": "This is motivated by empirical evidence from TAC 2009 data showing that human summaries contain on average more and shorter sentences than the system summaries.", "labels": [], "entities": [{"text": "TAC 2009 data", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9100641012191772}, {"text": "summaries", "start_pos": 78, "end_pos": 87, "type": "TASK", "confidence": 0.8902661204338074}]}, {"text": "We report encouraging preliminary results comparable to those attained by participating systems at TAC 2009.", "labels": [], "entities": [{"text": "TAC 2009", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.9082537591457367}]}], "introductionContent": [{"text": "In this paper we adopt the general framework for summarization put forward by Sp\u00e4rck-Jones (1999) -which views summarization as a threefold process: interpretation, transformation and generation -and attempt to provide a clean instantiation for each processing phase, with a particular emphasis on the last, summary-generation phase often omitted or over-simplified in the mainstream work on summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.9863865375518799}, {"text": "summarization", "start_pos": 111, "end_pos": 124, "type": "TASK", "confidence": 0.9720540642738342}, {"text": "summarization", "start_pos": 392, "end_pos": 405, "type": "TASK", "confidence": 0.975162148475647}]}, {"text": "The advantages of looking at the summarization problem in terms of distinct processing phases are numerous.", "labels": [], "entities": [{"text": "summarization problem", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.9250868260860443}]}, {"text": "It not only serves as a common ground for comparing different systems and understanding better the underlying logic and assumptions, but it also provides a neat framework for developing systems based on clean and extendable designs.", "labels": [], "entities": []}, {"text": "For instance, proposed a method based on Latent Semantic Analysis (LSA) and later J. showed that solely by enhancing the first source interpretation phase, one is already able to produce better summaries.", "labels": [], "entities": []}, {"text": "There has been limited work on the last summary generation phase due to the fact that it is unarguably a very challenging problem.", "labels": [], "entities": [{"text": "summary generation phase", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.8244369427363077}]}, {"text": "The vast amount of approaches assume simple sentence selection, a type of extractive summarization, where often the summary representation and the end summary are, indeed, conflated.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7238436788320541}]}, {"text": "The main focus of this work is, thus, to investigate robust ways for generating summaries from summary representations without recurring to simple sentence extraction and aiming at more human-like summaries.", "labels": [], "entities": [{"text": "summaries from summary representations", "start_pos": 80, "end_pos": 118, "type": "TASK", "confidence": 0.7916412353515625}, {"text": "sentence extraction", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.6644322276115417}]}, {"text": "This decision is also motivated by empirical evidence from TAC 2009 data (see table 1) showing that human summaries contain on average more and shorter sentences than the system summaries.", "labels": [], "entities": [{"text": "TAC 2009 data", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.9175655245780945}, {"text": "summaries", "start_pos": 106, "end_pos": 115, "type": "TASK", "confidence": 0.8962126970291138}]}, {"text": "The intuition behind this is that, by containing more sentences, a summary is able to capture more of the important content from the source.", "labels": [], "entities": []}, {"text": "Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data 1 yield ROUGE scores ( comparable to the participating systems in the Summarization task at TAC 2009.", "labels": [], "entities": [{"text": "TAC 2009 data", "start_pos": 131, "end_pos": 144, "type": "DATASET", "confidence": 0.9409678181012472}, {"text": "ROUGE", "start_pos": 153, "end_pos": 158, "type": "METRIC", "confidence": 0.9968019723892212}, {"text": "Summarization task at TAC 2009", "start_pos": 215, "end_pos": 245, "type": "TASK", "confidence": 0.7264249503612519}]}, {"text": "Taking into account that our approach is completely unsupervised and language-independent, we find our preliminary results encouraging.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows: in the next section we briefly survey the related work, in \u00a73 we describe our approach to summarization, in \u00a74 we explain how we tackle the generation step, in \u00a75 we present and discuss our experimental results and towards the end we conclude and give pointers to future work.", "labels": [], "entities": [{"text": "summarization", "start_pos": 142, "end_pos": 155, "type": "TASK", "confidence": 0.9850413203239441}]}], "datasetContent": [{"text": "For our experiments we made use of the TAC 2009 data which conveniently contains humanproduced summaries against which we could evaluate the output of our system.", "labels": [], "entities": [{"text": "TAC 2009 data", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.959195593992869}]}, {"text": "To begin our inquiry we carried out a phase of exploratory data analysis, in which we measured the average number of sentences per summary, words per sentence and words per summary inhuman vs. system summaries in the TAC 2009 data.", "labels": [], "entities": [{"text": "TAC 2009 data", "start_pos": 217, "end_pos": 230, "type": "DATASET", "confidence": 0.9808501402537028}]}, {"text": "Additionally, we also measured these statistics of summaries produced by our system at five different percentage cutoffs: 100%, 15%, 10%, 5% and 1%.", "labels": [], "entities": []}, {"text": "The results from this exploration are summarised in table 1.", "labels": [], "entities": []}, {"text": "The most notable thing is that human summaries contain on average more and shorter sentences than the system summaries (see 2nd and 3rd column from left to right).", "labels": [], "entities": [{"text": "summaries", "start_pos": 37, "end_pos": 46, "type": "TASK", "confidence": 0.8362897038459778}]}, {"text": "Secondly, we note that as the percentage cutoff decreases (from 4th column rightwards) the characteristics of the summaries produced by our system are increasingly more similar to those of the human summaries.", "labels": [], "entities": []}, {"text": "In other words, within the 100-word window imposed by the TAC guidelines, our system is able to fit more (and hence shorter) sentences as we decrease the percentage cutoff.", "labels": [], "entities": [{"text": "TAC", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.7156110405921936}]}, {"text": "Summarization performance results are shown in table 2.", "labels": [], "entities": []}, {"text": "We used the standard ROUGE evaluation () which has been also used for TAC.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9751113653182983}, {"text": "TAC", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.7469445466995239}]}, {"text": "We include the usual ROUGE metrics: R 1 is the maximum number of co-occurring unigrams, R 2 is the maximum number of cooccurring bigrams and R SU 4 is the skip bigram measure with the addition of unigrams as counting unit.", "labels": [], "entities": []}, {"text": "The last five columns of table 2 (from left to right) correspond to summaries produced by our system at various percentage cutoffs.", "labels": [], "entities": []}, {"text": "The 2nd column, LSA extract , corresponds to the performance of our system at producing summaries by sentence extraction only.", "labels": [], "entities": [{"text": "LSA extract", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.7692471444606781}, {"text": "sentence extraction", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.6833087652921677}]}, {"text": "In the light of the above, the decrease in performance from column LSA extract to column 'At 100%' can be regarded as reconstruction error.", "labels": [], "entities": [{"text": "reconstruction error", "start_pos": 118, "end_pos": 138, "type": "METRIC", "confidence": 0.8473461270332336}]}, {"text": "Then, as we decrease the percentage cutoff (from 4th column rightwards) we are increasingly covering more of the content comprised by the human summaries (as far as the ROUGE metrics are able to gauge this, of course).", "labels": [], "entities": []}, {"text": "In other words, the improvement of content coverage makes up for the reconstruction error, and at 5% cutoff we already obtain ROUGE scores comparable to LSA extract . This suggests that if we improve the quality of our sentence reconstruction we would potentially end up with a better performing system than atypical LSA system based on sentence selection.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 126, "end_pos": 131, "type": "METRIC", "confidence": 0.9988000392913818}, {"text": "sentence reconstruction", "start_pos": 219, "end_pos": 242, "type": "TASK", "confidence": 0.709187462925911}]}, {"text": "Hence, we find these results very encouraging.", "labels": [], "entities": []}, {"text": "Finally, we admittedly note that by applying a percentage cutoff on the initial term set and further performing the sentence reconstruction we gain in content coverage, to a certain extent, on the expense of sentence readability.", "labels": [], "entities": [{"text": "sentence reconstruction", "start_pos": 116, "end_pos": 139, "type": "TASK", "confidence": 0.7299069166183472}]}], "tableCaptions": [{"text": " Table 1: Summary statistics on TAC'09 data (initial summaries).", "labels": [], "entities": [{"text": "TAC'09 data", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.8400451838970184}]}, {"text": " Table 2: Summarization results on TAC'09 data (initial summaries).", "labels": [], "entities": [{"text": "TAC'09 data", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.8696099817752838}]}]}