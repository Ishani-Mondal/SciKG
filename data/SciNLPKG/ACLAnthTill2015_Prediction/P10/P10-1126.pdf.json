{"title": [{"text": "How Many Words is a Picture Worth? Automatic Caption Generation for News Images", "labels": [], "entities": [{"text": "Automatic Caption Generation", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.7787083685398102}]}], "abstractContent": [{"text": "In this paper we tackle the problem of automatic caption generation for news images.", "labels": [], "entities": [{"text": "automatic caption generation", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.7192214926083883}]}, {"text": "Our approach leverages the vast resource of pictures available on the web and the fact that many of them are cap-tioned.", "labels": [], "entities": []}, {"text": "Inspired by recent work in sum-marization, we propose extractive and ab-stractive caption generation models.", "labels": [], "entities": [{"text": "ab-stractive caption generation", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.6324856281280518}]}, {"text": "They both operate over the output of a proba-bilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content.", "labels": [], "entities": []}, {"text": "Experimental results show that an abstractive model defined over phrases is superior to extractive methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have witnessed an unprecedented growth in the amount of digital information available on the Internet.", "labels": [], "entities": []}, {"text": "Flickr, one of the best known photo sharing websites, hosts more than three billion images, with approximately 2.5 million images being uploaded everyday.", "labels": [], "entities": [{"text": "Flickr", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.966313362121582}]}, {"text": "1 Many on-line news sites like CNN, Yahoo!, and BBC publish images with their stories and even provide photo feeds related to current events.", "labels": [], "entities": []}, {"text": "Browsing and finding pictures in large-scale and heterogeneous collections is an important problem that has attracted much interest within information retrieval.", "labels": [], "entities": [{"text": "Browsing and finding pictures", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6785801276564598}, {"text": "information retrieval", "start_pos": 139, "end_pos": 160, "type": "TASK", "confidence": 0.7464672327041626}]}, {"text": "Many of the search engines deployed on the web retrieve images without analyzing their content, simply by matching user queries against collocated textual information.", "labels": [], "entities": []}, {"text": "Examples include meta-data (e.g., the image's filename and format), user-annotated tags, captions, and generally text surrounding the image.", "labels": [], "entities": []}, {"text": "As this limits the applicability of search engines (images that do not coincide with textual data cannot be retrieved), a great deal of work has focused on the development of methods that generate description words fora picture automatically.", "labels": [], "entities": []}, {"text": "The literature is littered with various attempts to learn the associations between image features and words using supervised classification (;), instantiations of the noisychannel model (), latent variable models;), and models inspired by information retrieval (.", "labels": [], "entities": []}, {"text": "In this paper we go one step further and generate captions for images rather than individual keywords.", "labels": [], "entities": []}, {"text": "Although image indexing techniques based on keywords are popular and the method of choice for image retrieval engines, there are good reasons for using more linguistically meaningful descriptions.", "labels": [], "entities": [{"text": "image indexing", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7785913646221161}]}, {"text": "A list of keywords is often ambiguous.", "labels": [], "entities": []}, {"text": "An image annotated with the words blue, sky, car could depict a blue car or a blue sky, whereas the caption \"car running under the blue sky\" would make the relations between the words explicit.", "labels": [], "entities": []}, {"text": "Automatic caption generation could improve image retrieval by supporting longer and more targeted queries.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8988348543643951}, {"text": "image retrieval", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7880921065807343}]}, {"text": "It could also assist journalists in creating descriptions for the images associated with their articles.", "labels": [], "entities": []}, {"text": "Beyond image retrieval, it could increase the accessibility of the web for visually impaired (blind and partially sighted) users who cannot access the content of many sites in the same ways as sighted users can).", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.7301704734563828}]}, {"text": "We explore the feasibility of automatic caption generation in the news domain, and create descriptions for images associated with on-line articles.", "labels": [], "entities": [{"text": "automatic caption generation", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.7279228170712789}]}, {"text": "Obtaining training data in this setting does not require expensive manual annotation as many articles are published together with captioned images.", "labels": [], "entities": []}, {"text": "Inspired by recent work in summarization, we propose extractive and abstractive caption gen-eration models.", "labels": [], "entities": [{"text": "summarization", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.9883803129196167}]}, {"text": "The backbone for both approaches is a probabilistic image annotation model that suggests keywords for an image.", "labels": [], "entities": []}, {"text": "We can then simply identify (and rank) the sentences in the documents that share these keywords or create anew caption that is potentially more concise but also informative and fluent.", "labels": [], "entities": []}, {"text": "Our abstractive model operates over image description keywords and document phrases.", "labels": [], "entities": []}, {"text": "Their combination gives rise to many caption realizations which we select probabilistically by taking into account dependency and word order constraints.", "labels": [], "entities": [{"text": "caption realizations", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8655176162719727}]}, {"text": "Experimental results show that the model's output compares favorably to handwritten captions and is often superior to extractive methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we discuss our experimental design for assessing the performance of the caption generation models presented above.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.873119056224823}]}, {"text": "We give details on our training procedure, parameter estimation, and present the baseline methods used for comparison with our models.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.6298625767230988}]}, {"text": "Data All our experiments were conducted on the corpus created by, following their original partition of the data (2,881 image-caption-document tuples for training, 240 tuples for development and 240 for testing).", "labels": [], "entities": []}, {"text": "Documents and captions were parsed with the Stanford parser () in order to obtain dependencies for the phrase-based abstractive model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: TER results for extractive, abstractive  models, and lead sentence baseline;  *  : sig. dif- ferent from lead sentence;  \u2020 : sig. different from  KL and JS divergence.", "labels": [], "entities": [{"text": "TER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9935179948806763}]}]}