{"title": [{"text": "Predicate Argument Structure Analysis using Transformation-based Learning", "labels": [], "entities": [{"text": "Predicate Argument Structure Analysis", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8746083974838257}]}], "abstractContent": [{"text": "Maintaining high annotation consistency in large corpora is crucial for statistical learning; however, such work is hard, especially for tasks containing semantic elements.", "labels": [], "entities": [{"text": "statistical learning", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.8449927866458893}]}, {"text": "This paper describes predicate argument structure analysis using \ud97b\udf59 transformation-based learning.", "labels": [], "entities": [{"text": "predicate argument structure analysis", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.9143766313791275}]}, {"text": "An advantage of transformation-based learning is the readability of learned rules.", "labels": [], "entities": []}, {"text": "A disadvantage is that the rule extraction procedure is time-consuming.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.8720649182796478}]}, {"text": "We present incremental-based, transformation-based learning for semantic processing tasks.", "labels": [], "entities": []}, {"text": "As an example, we deal with Japanese predicate argument analysis and show some tendencies of annotators for constructing a corpus with our method.", "labels": [], "entities": [{"text": "Japanese predicate argument analysis", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.6374027952551842}]}], "introductionContent": [{"text": "Automatic predicate argument structure analysis (PAS) provides information of \"who did what to whom\" and is an important base tool for such various text processing tasks as machine translation information extraction), question answering (, and summarization ().", "labels": [], "entities": [{"text": "Automatic predicate argument structure analysis (PAS)", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7490453906357288}, {"text": "machine translation information extraction", "start_pos": 173, "end_pos": 215, "type": "TASK", "confidence": 0.8713322579860687}, {"text": "question answering", "start_pos": 218, "end_pos": 236, "type": "TASK", "confidence": 0.934940367937088}, {"text": "summarization", "start_pos": 244, "end_pos": 257, "type": "TASK", "confidence": 0.9875828623771667}]}, {"text": "Most recent approaches to predicate argument structure analysis are statistical machine learning methods such as support vector machines (SVMs)().", "labels": [], "entities": [{"text": "predicate argument structure analysis", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.9412705451250076}]}, {"text": "For predicate argument structure analysis, we have the following representative large corpora: FrameNet (),), and NomBank () in English, the Chinese PropBank in Chinese, the GDA Corpus (), Kyoto Text Corpus Ver.4.0 (), and the NAIST Text Corpus () in Japanese.", "labels": [], "entities": [{"text": "predicate argument structure analysis", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.9018708914518356}, {"text": "GDA Corpus", "start_pos": 174, "end_pos": 184, "type": "DATASET", "confidence": 0.9735627174377441}, {"text": "Kyoto Text Corpus Ver.4.0", "start_pos": 189, "end_pos": 214, "type": "DATASET", "confidence": 0.9299010038375854}, {"text": "NAIST Text Corpus", "start_pos": 227, "end_pos": 244, "type": "DATASET", "confidence": 0.9835538069407145}]}, {"text": "The construction of such large corpora is strenuous and time-consuming.", "labels": [], "entities": []}, {"text": "Additionally, maintaining high annotation consistency in such corpora is crucial for statistical learning; however, such work is hard, especially for tasks containing semantic elements.", "labels": [], "entities": []}, {"text": "For example, in Japanese corpora, distinguishing true dative (or indirect object) arguments from time-type argument is difficult because the arguments of both types are often accompanied with the 'ni' case marker.", "labels": [], "entities": []}, {"text": "A problem with such statistical learners as SVM is the lack of interpretability; if accuracy is low, we cannot identify the problems in the annotations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9981170892715454}]}, {"text": "We are focusing on transformation-based learning (TBL).", "labels": [], "entities": [{"text": "transformation-based learning (TBL)", "start_pos": 19, "end_pos": 54, "type": "TASK", "confidence": 0.6774343490600586}]}, {"text": "An advantage for such learning methods is that we can easily interpret the learned model.", "labels": [], "entities": []}, {"text": "The tasks inmost previous research are such simple tagging tasks as part-of-speech tagging, insertion and deletion of parentheses in syntactic parsing, and chunking.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7188001573085785}, {"text": "insertion and deletion of parentheses in syntactic parsing", "start_pos": 92, "end_pos": 150, "type": "TASK", "confidence": 0.6763685494661331}]}, {"text": "Here we experiment with a complex task: Japanese PASs.", "labels": [], "entities": [{"text": "Japanese PASs", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.7604015469551086}]}, {"text": "TBL can be slow, so we proposed an incremental training method to speedup the training.", "labels": [], "entities": [{"text": "TBL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.4894619286060333}]}, {"text": "We experimented with a Japanese PAS corpus with a graph-based TBL.", "labels": [], "entities": [{"text": "Japanese PAS corpus", "start_pos": 23, "end_pos": 42, "type": "DATASET", "confidence": 0.5983210404713949}]}, {"text": "From the experiments, we interrelated the annotation tendency on the dataset.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes Japanese predicate structure, our graph expression of it, and our improved method.", "labels": [], "entities": [{"text": "Japanese predicate structure", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.690377672513326}]}, {"text": "The results of experiments using the NAIST Text Corpus, which is our target corpus, are reported in Section 3, and our conclusion is provided in Section 4.", "labels": [], "entities": [{"text": "NAIST Text Corpus", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.9812360405921936}]}], "datasetContent": [{"text": "We used the articles in the NAIST Text Corpus version 1.4\u03b2 () based on the Mainichi Shinbun Corpus, which were taken from news articles published in the Japanese Mainichi Shinbun newspaper.", "labels": [], "entities": [{"text": "NAIST Text Corpus version 1.4\u03b2", "start_pos": 28, "end_pos": 58, "type": "DATASET", "confidence": 0.9671464443206788}, {"text": "Mainichi Shinbun Corpus", "start_pos": 75, "end_pos": 98, "type": "DATASET", "confidence": 0.9476950963338217}, {"text": "Mainichi Shinbun newspaper", "start_pos": 162, "end_pos": 188, "type": "DATASET", "confidence": 0.9341965317726135}]}, {"text": "We used articles published on January 1st for training examples and on January 3rd for test examples.", "labels": [], "entities": []}, {"text": "Three original argument types are defined in the NAIST Text Corpus: nominative (or subjective), accusative (or direct object), and dative (or indirect object).", "labels": [], "entities": [{"text": "NAIST Text Corpus", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.9778812527656555}]}, {"text": "For evaluation of the difficult annotation cases, we also added annotations for 'time' and 'location' types by ourselves.", "labels": [], "entities": []}, {"text": "We show the dataset distribution in.", "labels": [], "entities": []}, {"text": "We extracted the BP units and dependencies among these BPs from the dataset using Cabocha, a Japanese dependency parser, as pre-processing.", "labels": [], "entities": []}, {"text": "After that, we adapted our incremental learning to the training data.", "labels": [], "entities": []}, {"text": "We used two constraint templates in for predicate nodes and edges when extracting the rule candidates.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Total performances (F1-measure (%))", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9991496801376343}]}, {"text": " Table 5: Results for every arg. type (F-measure  (%))", "labels": [], "entities": [{"text": "F-measure", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.987247884273529}]}]}