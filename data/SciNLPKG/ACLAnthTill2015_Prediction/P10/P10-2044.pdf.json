{"title": [{"text": "Optimizing Question Answering Accuracy by Maximizing Log-Likelihood", "labels": [], "entities": [{"text": "Optimizing Question Answering Accuracy", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7237735390663147}]}], "abstractContent": [{"text": "In this paper we demonstrate that there is a strong correlation between the Question Answering (QA) accuracy and the log-likelihood of the answer typing component of our statistical QA model.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.7512002050876617}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.5389072299003601}]}, {"text": "We exploit this observation in a clustering algorithm which optimizes QA accuracy by maximizing the log-likelihood of a set of question-and-answer pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9429927468299866}]}, {"text": "Experimental results show that we achieve better QA accuracy using the resulting clusters than by using manually derived clusters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9100012183189392}]}], "introductionContent": [{"text": "Question Answering (QA) distinguishes itself from other information retrieval tasks in that the system tries to return accurate answers to queries posed in natural language.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8829789757728577}, {"text": "information retrieval", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.7521817982196808}]}, {"text": "Factoid QA limits itself to questions that can usually be answered with a few words.", "labels": [], "entities": [{"text": "Factoid QA", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.6716216206550598}]}, {"text": "Typically factoid QA systems employ some form of question type analysis, so that a question such as What is the capital of Japan?", "labels": [], "entities": [{"text": "question type analysis", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.74736487865448}]}, {"text": "will be answered with a geographical term.", "labels": [], "entities": []}, {"text": "While many QA systems use hand-crafted rules for this task, such an approach is time-consuming and doesn't generalize well to other languages.", "labels": [], "entities": []}, {"text": "Machine learning methods have been proposed, such as question classification using support vector machines ( and language modeling (.", "labels": [], "entities": [{"text": "question classification", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.9063729047775269}]}, {"text": "In these approaches, question categories are predefined and a classifier is trained on manually labeled data.", "labels": [], "entities": []}, {"text": "This is an example of supervised learning.", "labels": [], "entities": []}, {"text": "In this paper we present an unsupervised method, where we attempt to cluster question-and-answer (q-a) pairs without any predefined question categories, hence no manually class-labeled questions are used.", "labels": [], "entities": []}, {"text": "We use a statistical QA framework, described in Section 2, where the system is trained with clusters of q-a pairs.", "labels": [], "entities": []}, {"text": "This framework was used in several TREC evaluations where it placed in the top 10 of participating systems).", "labels": [], "entities": [{"text": "TREC evaluations", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.649670422077179}]}, {"text": "In Section 3 we show that answer accuracy is strongly correlated with the log-likelihood of the q-a pairs computed by this statistical model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.956405520439148}]}, {"text": "In Section 4 we propose an algorithm to cluster q-a pairs by maximizing the log-likelihood of a disjoint set of q-a pairs.", "labels": [], "entities": []}, {"text": "In Section 5 we evaluate the QA accuracy by training the QA system with the resulting clusters.", "labels": [], "entities": [{"text": "QA", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.8001697659492493}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9754881858825684}]}], "datasetContent": [{"text": "For our data sets, we restrict ourselves to questions that start with who, when or where.", "labels": [], "entities": []}, {"text": "Furthermore, we only use q-a pairs which can be answered with a single word.", "labels": [], "entities": []}, {"text": "As training data we use questions and answers from the Knowledge-Master collection . Development/evaluation questions are the questions from TREC QA evaluations from TREC 2002 to TREC 2006, the answers to which are to be retrieved from the AQUAINT corpus.", "labels": [], "entities": [{"text": "TREC QA evaluations from TREC 2002 to TREC 2006", "start_pos": 141, "end_pos": 188, "type": "DATASET", "confidence": 0.8495540155304803}, {"text": "AQUAINT corpus", "start_pos": 240, "end_pos": 254, "type": "DATASET", "confidence": 0.9605385363101959}]}, {"text": "In total we have 2016 q-a pairs for training and 568 questions for development/evaluation.", "labels": [], "entities": []}, {"text": "We are able to retrieve the correct answer for 317 of the development/evaluation questions, thus the theoretical upper bound for our experiments is an answer accuracy of M RR = 0.558.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.6644811034202576}, {"text": "M RR", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9226078689098358}]}, {"text": "Accuracy is evaluated using 5-fold (rotating) cross-validation, wherein each fold the TREC QA data is partitioned into a development set of 4 years' data and an evaluation set of one year's data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9890388250350952}, {"text": "TREC QA data", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.7561314205328623}]}, {"text": "For each TREC question the top 50 documents from the AQUAINT corpus are retrieved using Lucene 2 . We use the QA system described in Section 2 for QA evaluation.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9529249370098114}, {"text": "Lucene 2", "start_pos": 88, "end_pos": 96, "type": "DATASET", "confidence": 0.9280268549919128}]}, {"text": "Our evaluation metric is M RR eval , and LL dev is our optimization criterion, as motivated in Section 3.", "labels": [], "entities": []}, {"text": "Our baseline system uses manual clusters.", "labels": [], "entities": []}, {"text": "These clusters are obtained by putting all who q-a pairs in one cluster, all when pairs in a second and all where pairs in a third.", "labels": [], "entities": []}, {"text": "We compare this baseline with using clusters resulting from the algorithm described in Section 4.", "labels": [], "entities": []}, {"text": "We run this algorithm until there are no further improvements in LL dev . Two other cluster configurations are also investigated: all q-a pairs in one cluster (all-in-one), and each qa pair in its own cluster (one-in-each).", "labels": [], "entities": []}, {"text": "The all-inone configuration is equivalent to not using the filter model, i.e. answer candidates are ranked solely by the retrieval model.", "labels": [], "entities": []}, {"text": "The one-in-each configuration was shown to perform well in the TREC 2006 QA evaluation), where it ranked 9th among 27 participants on the factoid QA task.", "labels": [], "entities": [{"text": "TREC 2006 QA evaluation", "start_pos": 63, "end_pos": 86, "type": "DATASET", "confidence": 0.8539989143610001}]}], "tableCaptions": [{"text": " Table 1: LL eval (average per q-a pair) and  M RR eval (over all held-out TREC years), and  number of clusters (median of the cross-evaluation  folds) for the various configurations.", "labels": [], "entities": [{"text": "LL eval", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8333798050880432}, {"text": "M RR eval", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9136257966359457}]}]}