{"title": [{"text": "Online Sentence Novelty Scoring for Topical Document Streams", "labels": [], "entities": [{"text": "Sentence Novelty Scoring", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.8153552810351054}, {"text": "Topical Document Streams", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7282890280087789}]}], "abstractContent": [{"text": "The enormous amount of information on the Internet has raised the challenge of highlighting new information in the context of already viewed content.", "labels": [], "entities": []}, {"text": "This type of intelligent interface can save users time and prevent frustration.", "labels": [], "entities": []}, {"text": "Our goal is to scale up novelty detection to large web properties like Google News and Yahoo News.", "labels": [], "entities": [{"text": "novelty detection", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.9011960029602051}]}, {"text": "We present a set of lightweight features for online novelty scoring and fast nonlinear feature transformation methods.", "labels": [], "entities": [{"text": "online novelty scoring", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.6471048990885416}]}, {"text": "Our experimental results on the TREC 2004 shared task datasets show that the proposed method is not only efficient but also very powerful , significantly surpassing the best system at TREC 2004.", "labels": [], "entities": [{"text": "TREC 2004 shared task datasets", "start_pos": 32, "end_pos": 62, "type": "DATASET", "confidence": 0.8847696542739868}, {"text": "TREC 2004", "start_pos": 184, "end_pos": 193, "type": "DATASET", "confidence": 0.8787191212177277}]}], "introductionContent": [{"text": "The Internet supplies a wealth of news content with a corresponding problem: finding the right content for different users.", "labels": [], "entities": []}, {"text": "Search engines are helpful if a user is looking for something specific that can be cast as a keyword query.", "labels": [], "entities": []}, {"text": "If a user does not know what to look for, recommendation engines can make personalized suggestions for stories that may interest the user.", "labels": [], "entities": []}, {"text": "But both types of systems frequently represent content that the user has already consumed, leading to delay and frustration.", "labels": [], "entities": [{"text": "delay", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.9689590334892273}]}, {"text": "Consequently, identifying novel information has been an essential aspect of studies on news information retrieval.", "labels": [], "entities": [{"text": "news information retrieval", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.5951341589291891}]}, {"text": "Newsjunkie (), for instance, describes a system that personalizes a newsfeed based on a measure of information novelty: the user can be presented custom tailored news feeds that are novel in the context of documents that have already been reviewed.", "labels": [], "entities": [{"text": "Newsjunkie", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8987004160881042}]}, {"text": "This will spare the user from hunting through duplicate and redundant content for new nuggets of information.", "labels": [], "entities": []}, {"text": "Identifying genuinely novel information is also an essential aspect of update summarization).", "labels": [], "entities": [{"text": "update summarization", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.7009134292602539}]}, {"text": "But the temporal dynamics of a document stream are not generally the focus.", "labels": [], "entities": []}, {"text": "Novelty detection has also been studied in Topic Detection and Tracking field for the First Story Detection task) where the aim is to detect novel documents given previously seen documents.", "labels": [], "entities": [{"text": "Novelty detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7967657446861267}, {"text": "Topic Detection and Tracking", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.8561489582061768}, {"text": "First Story Detection task", "start_pos": 86, "end_pos": 112, "type": "TASK", "confidence": 0.7031418234109879}]}, {"text": "In this paper, we examine a slightly different problem; we perform novelty detection at the sentence level to highlight sentences that contain novel information.", "labels": [], "entities": [{"text": "novelty detection", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.6801742166280746}]}, {"text": "The novelty track in TREC was designed to serve as a shared task for exactly this type of research: finding novel, on-topic sentences from a news stream).", "labels": [], "entities": []}, {"text": "There were four tasks in the novelty track but we only focus on task 2 in this paper: \"given relevant sentences in all documents, identify all novel sentences.\"", "labels": [], "entities": []}, {"text": "The track changed slightly from year to year.", "labels": [], "entities": []}, {"text": "The data of the first run in 2002) used old topics and judgments which proved to be problematic due to the small percentage of relevant sentences.) included 50 new topics with an improved balance of relevant and novel sentences and chronologically ordered documents.", "labels": [], "entities": []}, {"text": "used the same task settings and the same number of topics, but made a major change through the inclusion of irrelevant documents.", "labels": [], "entities": []}, {"text": "Although the participants in the novelty track of TREC and many followup studies have investigated a wide ranging set of features and algorithms), almost none were specifically focused on scalability.", "labels": [], "entities": []}, {"text": "However, modern news aggregators are usually visited by millions of unique users and consume millions of stories each day.", "labels": [], "entities": []}, {"text": "Moreover, every few minutes item churn takes place and the stories of interest are likely to be the ones that appeared in the last couple of hours.", "labels": [], "entities": []}, {"text": "As real-time processing on a large scale gains more attention (), we investigate features that are both effective and efficient, and so could be used in a scalable online novelty scoring engine for making personalized newsfeeds on large web properties like Google News and Yahoo News.", "labels": [], "entities": []}, {"text": "To achieve this goal, our contributions are two-fold.", "labels": [], "entities": []}, {"text": "First, we present a set of effective, light-weight features: KL divergence with asymmetric smoothing, nonlinear transformation of unseen word count, relative sentence position and word embedding-based similarity.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.7644607722759247}]}, {"text": "Note that we restrict ourselves to only surface-level text features and algorithms that have time complexity of O(W ) where Wis the number of unique words seen so far (previous studies often employed quite expensive features and algorithms that have time complexity of at least O(W T ) where T is the number of sentences so far).", "labels": [], "entities": [{"text": "O", "start_pos": 112, "end_pos": 113, "type": "METRIC", "confidence": 0.891024112701416}]}, {"text": "To fully comply with the online setting, we also exclude very popular methods for measuring similarity such as tf-idf, since we are not allowed to seethe entire corpus.", "labels": [], "entities": []}, {"text": "Second, we propose efficient feature transformation methods: recursive feature averaging and Deep Neural Network (DNN)-based nonlinear transformation.", "labels": [], "entities": [{"text": "feature transformation", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.7667196393013}, {"text": "recursive feature averaging", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.6379297276337942}]}, {"text": "We evaluate our system on task 2 of the 2004 TREC novelty track.", "labels": [], "entities": [{"text": "2004 TREC novelty track", "start_pos": 40, "end_pos": 63, "type": "DATASET", "confidence": 0.8184856325387955}]}, {"text": "Interestingly, our experiment results indicate that our light-weight features are actually very powerful when used in conjunction with the proposed feature transformation; we obtain a significant performance improvement over the best challenge system.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows: Section 2 presents a brief summary of related work.", "labels": [], "entities": []}, {"text": "Section 3 describes our algorithm and features.", "labels": [], "entities": []}, {"text": "Section 4 outlines the experimental setup and reports the results of comparative analysis with challenge systems.", "labels": [], "entities": []}, {"text": "We finish with some conclusions and future directions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following the guidelines of task 2 for the TREC 2004 novelty detection track, we used the TREC 2003 dataset as training data and the TREC 2004 dataset as test data.", "labels": [], "entities": [{"text": "TREC 2004 novelty detection track", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.707028865814209}, {"text": "TREC 2003 dataset", "start_pos": 90, "end_pos": 107, "type": "DATASET", "confidence": 0.9230516950289408}, {"text": "TREC 2004 dataset", "start_pos": 133, "end_pos": 150, "type": "DATASET", "confidence": 0.9527855118115743}]}, {"text": "The training data includes 10,226 novel sentences out of 15,557 sentences.", "labels": [], "entities": []}, {"text": "The test data includes 3,454 novel ones out of 8,343 sentences.", "labels": [], "entities": []}, {"text": "We trained a DNN-based classifier and several logistic regression classifiers (which are the same model with the DNN model except without the hidden layers) using the Theano toolkit () to verify the effectiveness of each feature and feature transformation.", "labels": [], "entities": [{"text": "Theano toolkit", "start_pos": 167, "end_pos": 181, "type": "DATASET", "confidence": 0.9533897340297699}]}, {"text": "We optimized all models by minimizing logloss with the stochastic gradient decent algorithm with momentum.", "labels": [], "entities": []}, {"text": "We classified a sentence as novel if the posterior probability is greater than 0.5.", "labels": [], "entities": []}, {"text": "We performed a search based on five-fold cross validation to identify optimal values for the parameters defined in Section 3, and obtained the following values: s = 10, u = 0.1, \u03b1 = 0.5, \u03b2 = 0, \u03b3 = 1.5 and \u03b7 = 0.5.", "labels": [], "entities": []}, {"text": "For the DNN classifier, we used a set of five bottleneck hidden layers.", "labels": [], "entities": []}, {"text": "The number of nodes for each hidden layer were set to 10, 5, 3, 5 and 10, respectively.", "labels": [], "entities": []}, {"text": "Comparative evaluation results in F-score (following the TREC protocol) are shown in.", "labels": [], "entities": [{"text": "F-score", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9961573481559753}, {"text": "TREC", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.6423813104629517}]}, {"text": "In, the first four entries refer to the best top systems from TREC 2004 and followup studies, KLdiv to a system using only KL divergence features, TransCount to a system using only nonlinear transformation of unseen word count features, RelPos to a system using only relative position features, Word2Vec to a system using only word embedding features, All to a system using all features, All + Recursive to All with recursive feature averaging applied, All + DNN to All with DNNbased feature transformation applied and All + Recursive + DNN to All + Recursive with DNN-based feature transformation applied.", "labels": [], "entities": [{"text": "TREC 2004", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.8192262947559357}]}, {"text": "The best result (in bold) is significantly better than the best system results from TREC 2004, while still being very computationally efficient and therefore scalable.", "labels": [], "entities": [{"text": "TREC 2004", "start_pos": 84, "end_pos": 93, "type": "DATASET", "confidence": 0.7901384830474854}]}, {"text": "In terms of individual features, KLdiv (ties for 5th place at TREC 2004) and TransCount (outperforms the 6th entry) showed very strong results.", "labels": [], "entities": [{"text": "TREC 2004", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.7120752930641174}, {"text": "TransCount", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.9311346411705017}]}, {"text": "Although RelPos and Word2Vec did not yield good results, we found them complementary to other features; performance was degraded to 0.621 and 0.624, respectively, when they were excluded from All + Recursive + DNN.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.9204712510108948}]}, {"text": "The DNN-based feature transformation generally yielded better results.", "labels": [], "entities": [{"text": "DNN-based feature transformation", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.5985822180906931}]}, {"text": "In particular, it becomes very effective in conjunction with recursive feature averaging.", "labels": [], "entities": [{"text": "recursive feature averaging", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.6895643671353658}]}, {"text": "This result indicates that the DNNbased transformation allows the system to capture the non-trivial interactions between previous sentences and the current one.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance breakdown. The best result is  significantly better than the other configurations (p <  0.01) based on the McNemar test. Since the systems'  output is not available, we are not able to calculate sta- tistical significance against TREC systems.", "labels": [], "entities": [{"text": "McNemar test", "start_pos": 129, "end_pos": 141, "type": "DATASET", "confidence": 0.8141773641109467}]}]}