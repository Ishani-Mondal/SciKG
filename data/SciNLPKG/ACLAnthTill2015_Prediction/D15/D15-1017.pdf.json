{"title": [], "abstractContent": [{"text": "Opinion summarization is the task of producing the summary of a text, such that the summary also preserves the sentiment of the text.", "labels": [], "entities": [{"text": "Opinion summarization", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9030900597572327}]}, {"text": "Opinion Summarization is thus a trade-off between summarization and sentiment analysis.", "labels": [], "entities": [{"text": "Opinion Summarization", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8560411930084229}, {"text": "summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9685645699501038}, {"text": "sentiment analysis", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.9166605770587921}]}, {"text": "The demand of compression may drop sentiment bearing sentences , and the demand of sentiment detection may bring in redundant sentences.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.9196702837944031}]}, {"text": "We harness the power of submodularity to strike a balance between two conflicting requirements.", "labels": [], "entities": []}, {"text": "We investigate an incipient class of submodular functions for the problem, and a partial enumeration based greedy algorithm that has performance guarantee of 63%.", "labels": [], "entities": []}, {"text": "Our functions generate summaries such that there is good correlation between document sentiment and summary sentiment along with good ROUGE score, which outperforms the-state-of-the-art algorithms.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 134, "end_pos": 145, "type": "METRIC", "confidence": 0.9854013025760651}]}], "introductionContent": [{"text": "Sentiment Analysis is often addressed as a classification task, which aims at determining the sentiment of a word, sentence, paragraph or a document as a whole into positive, negative or neutral classes ().", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9564245641231537}, {"text": "determining the sentiment of a word, sentence, paragraph or a document as a whole", "start_pos": 78, "end_pos": 159, "type": "TASK", "confidence": 0.7058521844446659}]}, {"text": "Summarization, on the other hand is the task of aggregating and representing information content from a single document or multiple documents in a brief and fluent manner.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.976095974445343}]}, {"text": "Due to the explosive growth of data, fine grained sentiment analysis as well as summarization on the whole chunk of data can be a very time-consuming task.", "labels": [], "entities": [{"text": "fine grained sentiment analysis", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.6658706218004227}, {"text": "summarization", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.9823513031005859}]}, {"text": "Sentiment Analysis also requires filtering of text portions as either objective (factual information) or subjective (expressing some sentiment or opinion) during pre-processing and then, classifying the subjective extracts as positive or negative.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9572668075561523}]}, {"text": "Subjective extracts can also be provided to users as a summary of the sentiment-oriented content of the reviews in search engines.", "labels": [], "entities": []}, {"text": "In this paper, we address the problem of generic extractive summarization of reviews, a task commonly known as Opinion Summarization (.", "labels": [], "entities": [{"text": "generic extractive summarization of reviews", "start_pos": 41, "end_pos": 84, "type": "TASK", "confidence": 0.9040769100189209}, {"text": "Opinion Summarization", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.695037379860878}]}, {"text": "The goals of opinion summarization are: 1.", "labels": [], "entities": [{"text": "opinion summarization", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.7293031215667725}]}, {"text": "Present a short summary that conveys the essence as well as the sentiment of the review 2.", "labels": [], "entities": []}, {"text": "Provide a short subjective extract to NLP pipeline for faster execution (e.g. sentiment analysis, review clustering etc.).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.9099996089935303}, {"text": "review clustering", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.6578052937984467}]}, {"text": "In this paper, we use movie reviews for opinion summarization task as they often have the following parts: 1.", "labels": [], "entities": [{"text": "opinion summarization", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.637169674038887}]}, {"text": "Plot -Description of the story, which is factual in nature 2.", "labels": [], "entities": []}, {"text": "Critique -Opinion about the movie, which is sentiment bearing Clearly, opinion summary to be generated will have a trade-off between the two opposing parts -subjective critique and objective plot.", "labels": [], "entities": []}, {"text": "Our goal is to strike a balance through linear combination of suitable submodular functions in our paper.", "labels": [], "entities": []}, {"text": "Joint models of relevance and subjectivity have a great benefit in that they have a large degree of freedom as far as controlling redundancy goes.", "labels": [], "entities": []}, {"text": "In contrast, conventional two-stage approach, which first generate candidate subjective sentences using min-cut and then selects top subjective sentences within budget to generate a summary, have less computational complexity than joint models.", "labels": [], "entities": []}, {"text": "However, two-stage approaches are suboptimal for text summarization.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7595394253730774}]}, {"text": "For example, when we select subjective sentences first, the sentiment as well information content may become redundant fora particular aspect.", "labels": [], "entities": []}, {"text": "On the other hand, when we extract sentences first, an important subjective sentence may fail to be selected, simply because it is long.", "labels": [], "entities": []}, {"text": "The two stage conflict in the sense that the demand of compression may drop sentiment bearing sentences, and the demand of sentiment detection may bring in redundant sentences.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.8837553560733795}]}, {"text": "We then, use partial enumeration based greedy algorithm (), which gives performance guarantee of (1 \u2212 e \u22121 ) \u2248 0.632).", "labels": [], "entities": []}, {"text": "The performance guarantee reported is better than simple greedy algorithm, used by as their proof is erroneous (.", "labels": [], "entities": []}, {"text": "Further, the same greedy algorithm, which was used again in gives only performance guarantee of 1 2 (1 \u2212 e 1 ) \u2248 0.316 ().", "labels": [], "entities": []}, {"text": "The rest of the paper is as follows -in the next section, we look at previous work and establish further motivation for our work.", "labels": [], "entities": []}, {"text": "Following that, we build the theory and formulate suitable objectives for opinion summarization task.", "labels": [], "entities": [{"text": "opinion summarization", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.8280761539936066}]}, {"text": "In the final section, we present results based on implementation and testing of the functions.", "labels": [], "entities": []}, {"text": "Experimental results show that the functions outperform the-stateof-the-art methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have created Movie ontology tree manually (figure 1).", "labels": [], "entities": [{"text": "Movie ontology tree", "start_pos": 16, "end_pos": 35, "type": "DATASET", "confidence": 0.7493587136268616}]}, {"text": "Further the ontology is enriched by adding clue words to all aspects using wordnet sense propagation algorithm () for three iterations.", "labels": [], "entities": [{"text": "wordnet sense propagation", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.5931316415468851}]}, {"text": "The algorithm does a hard clustering of the sentences by assigning the sentence aspect, which has maximum number of clue words in that sentence.", "labels": [], "entities": []}, {"text": "Clue words for 'Plot' aspect are story, script, storyline, chief, communicative, explain, narrate, narration, narrative, narrator, report, reporter, scheme, schemer, script, scriptural, storyteller, tell, write up..,.", "labels": [], "entities": []}, {"text": "For the experiments, we have used the polarity dataset from.", "labels": [], "entities": []}, {"text": "The dataset contains 1000 positive and 1000 negative movie reviews with size varying between 700 to 1000 words.", "labels": [], "entities": []}, {"text": "As summary generation is time consuming task (DUC 4 only used 25 summaries to evaluate the performance of systems), we picked 100 positive and 100 negative reviews randomly from the dataset and their abstract summaries are generated manually with 200 words limit as budget for In the experiment, the partial enumeration based greedy algorithm () is used for summary generation of 200 test documents within budget of 200 words.", "labels": [], "entities": [{"text": "summary generation", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8203232884407043}]}, {"text": "The algorithm has two parts.", "labels": [], "entities": []}, {"text": "In the first part, the algorithm compares function values of all feasible solutions (sets) of cardinality one or two.", "labels": [], "entities": []}, {"text": "Let Summ 1 be a feasible set of cardinality one or two that has the largest value of the objective function F (S).", "labels": [], "entities": [{"text": "Summ", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.9104595184326172}]}, {"text": "In the second part, the algorithm enumerates all feasible sets of cardinality three.", "labels": [], "entities": []}, {"text": "The algorithm, then completes each such set greedily and keeps the current solution feasible with respect to the knapsack constraint.", "labels": [], "entities": []}, {"text": "Let Summ 2 be the solution obtained in the second part that has the largest value of objective function overall choices of the starting set for the greedy algorithm.", "labels": [], "entities": [{"text": "Summ", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.7827966213226318}]}, {"text": "Finally, the algorithm outputs Summ 1 if F (Summ 1 ) > F (Summ 2 ) else Summ 2 otherwise.", "labels": [], "entities": []}, {"text": "The algorithm does O(n 2 ) function calculations in first part, while O(n 5 ) in second part.", "labels": [], "entities": [{"text": "O", "start_pos": 70, "end_pos": 71, "type": "METRIC", "confidence": 0.8479398488998413}]}, {"text": "This algorithm gives a performance guarantee of (1\u2212e \u22121 ) for solving monotone submodular objective with knapsack constraint ().", "labels": [], "entities": []}, {"text": "As far as we know, the algorithm has not been implemented for such problems because of complexity constraints ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE F-score and sentiment correla- tion for optimal values of \u03b1 with baselines 1-5", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9900814890861511}, {"text": "F-score", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.8091124892234802}, {"text": "sentiment correla- tion", "start_pos": 28, "end_pos": 51, "type": "METRIC", "confidence": 0.9274982362985611}]}, {"text": " Table 2: ROUGE F-score and sentiment correla- tion for optimal values of \u03b1 with baseline 6", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9915068745613098}, {"text": "F-score", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.804494321346283}, {"text": "sentiment correla- tion", "start_pos": 28, "end_pos": 51, "type": "METRIC", "confidence": 0.9340250641107559}]}, {"text": " Table 3: Maximum ROUGE F-score and their cor- responding sentiment correlation", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.9860921502113342}, {"text": "F-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.7777475714683533}]}, {"text": " Table 4: Maximum sentiment correlation and cor- responding ROUGE F-Score", "labels": [], "entities": [{"text": "Maximum sentiment correlation", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.6531482140223185}, {"text": "cor- responding ROUGE F-Score", "start_pos": 44, "end_pos": 73, "type": "METRIC", "confidence": 0.8239137411117554}]}]}