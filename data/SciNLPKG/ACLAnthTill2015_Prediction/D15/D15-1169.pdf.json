{"title": [{"text": "Joint A * CCG Parsing and Semantic Role Labeling", "labels": [], "entities": [{"text": "A", "start_pos": 6, "end_pos": 7, "type": "METRIC", "confidence": 0.9559640884399414}, {"text": "Semantic Role Labeling", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.6033527652422587}]}], "abstractContent": [{"text": "Joint models of syntactic and semantic parsing have the potential to improve performance on both tasks-but to date, the best results have been achieved with pipelines.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.729517936706543}]}, {"text": "We introduce a joint model using CCG, which is motivated by the close link between CCG syntax and semantics.", "labels": [], "entities": []}, {"text": "Semantic roles are recovered by labelling the deep dependency structures produced by the grammar.", "labels": [], "entities": []}, {"text": "Furthermore, because CCG is lexicalized, we show it is possible to factor the parsing model over words and introduce anew A * parsing algorithm-which we demonstrate is faster and more accurate than adaptive supertagging.", "labels": [], "entities": []}, {"text": "Our joint model is the first to substantially improve both syntactic and semantic accuracy over a comparable pipeline, and also achieves state-of-the-art results fora non-ensemble semantic role labelling model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.973407506942749}]}], "introductionContent": [{"text": "Joint models of syntactic and semantic parsing are attractive; they can potentially avoid the error propagation that is inherent in pipelines by using semantic models to inform syntactic attachments.", "labels": [], "entities": [{"text": "syntactic and semantic parsing", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.6810614541172981}]}, {"text": "However, in practice, the performance of joint systems for semantic role labelling (SRL) has been substantially beneath that of pipelines.", "labels": [], "entities": [{"text": "semantic role labelling (SRL)", "start_pos": 59, "end_pos": 88, "type": "TASK", "confidence": 0.7784603834152222}]}, {"text": "In this paper, we present the first approach to break this trend, by building on the close relationship of syntax and semantics in CCG grammars to enable both (1) a simple but highly effective joint model and (2) an efficient A * parsing algorithm.", "labels": [], "entities": [{"text": "A * parsing", "start_pos": 226, "end_pos": 237, "type": "TASK", "confidence": 0.5826366941134135}]}, {"text": "Semantic dependencies can span an unbounded number of syntactic dependencies, causing significant inference and sparsity challenges for joint Figure 2: Dependencies produced by a CCG parse.", "labels": [], "entities": []}, {"text": "SRL dependencies can be recovered by labelling the edges with a semantic role or \u2205.", "labels": [], "entities": []}, {"text": "shows a CCG derivation for these dependencies. models.", "labels": [], "entities": []}, {"text": "For example, in the, the semantic dependency between He and deny spans three syntactic edges.", "labels": [], "entities": []}, {"text": "This fact makes it difficult to jointly parse syntactic and semantic dependencies with dynamic programs, and means that dependency path features can be sparse.", "labels": [], "entities": []}, {"text": "Syntactic dependencies also often have ambiguous semantic interpretations-for example in He opened the door and The door opened, the syntactic subject corresponds to different semantic roles.", "labels": [], "entities": []}, {"text": "We address these challenges with anew joint model of CCG syntactic parsing and semantic role labelling.", "labels": [], "entities": [{"text": "CCG syntactic parsing", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.7655430833498637}, {"text": "semantic role labelling", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.6408891677856445}]}, {"text": "The CCG formalism is particularly well suited; it models both short-and longrange syntactic dependencies which correspond directly to the semantic roles we aim to recover.", "labels": [], "entities": []}, {"text": "The joint model simply involves labelling a subset of these dependencies with the appropriate roles, as seen in.", "labels": [], "entities": []}, {"text": "This labelling decision can be easily integrated into existing parsing algorithms.", "labels": [], "entities": []}, {"text": "CCG also helps resolve cases where interpretation depends on the valency of the pred-he refused to confirm or deny reports NP he (S refuse \\NPi )/(Sj \\NPi )j Sz /Sz (S confirm \\NPu )/NPv conj (S deny \\NPx )/NPy NPreports {} {ref use: Jointly building a CCG parse and semantic dependencies representation.", "labels": [], "entities": []}, {"text": "Subscripts beneath categories denote heads, which are unified when spans combine.", "labels": [], "entities": []}, {"text": "One dependency is created for each argument of each lexical category.", "labels": [], "entities": []}, {"text": "In our approach, dependency labels are initially underspecified (represented f ? \u2212 \u2192a) until an attachment is determined by the derivation and a label is chosen by the model.", "labels": [], "entities": []}, {"text": "icate, such as ergative verbs, by learning lexical entries that pair syntactic arguments with semantic roles, such as open : S \\NP ARG1 and open : (S \\NP ARG0 )/NP ARG1 . shows a detailed trace of how the example from is parsed with our model.", "labels": [], "entities": [{"text": "NP ARG0 )/NP ARG1", "start_pos": 151, "end_pos": 168, "type": "DATASET", "confidence": 0.5917623996734619}]}, {"text": "We also present anew A * algorithm for the joint model.", "labels": [], "entities": []}, {"text": "Because CCG is strongly lexicalized, we are able to introduce anew type of extended lexical entries that allows us to factor the model over words and develop effective new upper bounds on the Viterbi outside parse score.", "labels": [], "entities": []}, {"text": "A * parsing algorithms have previously been developed for models with tree-structured syntactic dependencies (), and models with no bi-lexical dependencies, including supertag-factored CCGs ().", "labels": [], "entities": [{"text": "A * parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.506499449412028}]}, {"text": "We generalize these techniques to SRL-style graph-structured dependencies.", "labels": [], "entities": [{"text": "SRL-style graph-structured dependencies", "start_pos": 34, "end_pos": 73, "type": "TASK", "confidence": 0.805869996547699}]}, {"text": "Experiments demonstrate that our model not only outperforms pipeline semantic role labelling models, but improves the quality of the syntactic parser.", "labels": [], "entities": []}, {"text": "PropBank SRL performance is 1.6 points higher than comparable existing work, and semantic features improve syntactic accuracy by 1.6 points.", "labels": [], "entities": [{"text": "SRL", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.5024662613868713}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.978955864906311}]}, {"text": "Our A * algorithm is 5 times faster than CKY parsing, with no loss inaccuracy.", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.5998598635196686}]}, {"text": "The combination of CCG-based joint modelling and A * decoding gives an efficient, accurate, and linguistically principled parser.", "labels": [], "entities": []}, {"text": "1 The parser is available from: https://github.com/ mikelewis0/EasySRL", "labels": [], "entities": [{"text": "EasySRL", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.5453644394874573}]}], "datasetContent": [{"text": "We used PropBank Section 00 for development, Sections 02-21 for training, and Section 23 for testing.", "labels": [], "entities": [{"text": "PropBank Section 00", "start_pos": 8, "end_pos": 27, "type": "DATASET", "confidence": 0.8942022323608398}]}, {"text": "The Pipeline baseline first parses with a supertag-factored A * model, and chooses a semantic role for each CCG dependency with a log-linear classifier.", "labels": [], "entities": [{"text": "Pipeline baseline", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9293517470359802}]}, {"text": "The classifier uses the role and attachment features used by the parser.", "labels": [], "entities": []}, {"text": "We explore whether our A * decoding is more efficient than alternatives, including both algorithmic and feature computation.", "labels": [], "entities": []}, {"text": "While the A * search builds very small charts, the features must be precomputed for the heuristic.", "labels": [], "entities": []}, {"text": "In CKY parsing, features can be computed lazily.", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.7016879916191101}]}, {"text": "We compare with a CKY parsing over the same space.", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.6384550631046295}]}, {"text": "If no parse is found, or the chart size exceeds 400000 nodes, we back off to the pipeline (tuned so that the backoff is used roughly as often as for the A * parser).", "labels": [], "entities": []}, {"text": "We also compare with adaptive supertagging), which is the same except for first attempting to parse with a restrictive supertagger beam \u03b2 = 0.1.", "labels": [], "entities": []}, {"text": "The A * pipeline is fast, but inaccurate.", "labels": [], "entities": []}, {"text": "CKY is 5 times slower than A * in the same space, whereas adaptive supertagging trades accuracy for speed.", "labels": [], "entities": [{"text": "A", "start_pos": 27, "end_pos": 28, "type": "METRIC", "confidence": 0.9810094833374023}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9991663694381714}, {"text": "speed", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.949751615524292}]}, {"text": "The best previously reported speed improvement using A * parsing is a factor of 1.2 times faster (.", "labels": [], "entities": [{"text": "speed", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9657427668571472}, {"text": "A * parsing", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.5647504230340322}]}, {"text": "Our new A * algorithm dominates existing alternatives in both speed and accuracy.", "labels": [], "entities": [{"text": "speed", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9972622394561768}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9968011379241943}]}], "tableCaptions": [{"text": " Table 1: Comparison with the best single-parser  SRL models on PropBank from CoNLL-2008.  The comparison models are Vickrey and Koller  (2008), Che et al. (2008),", "labels": [], "entities": [{"text": "PropBank from CoNLL-2008", "start_pos": 64, "end_pos": 88, "type": "DATASET", "confidence": 0.8715537985165914}]}, {"text": " Table 2: Parser speed on PropBank Section 23", "labels": [], "entities": [{"text": "Parser speed", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.7986492216587067}, {"text": "PropBank Section", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.9613942205905914}]}, {"text": " Table 3: Labelled F1 for CCGbank dependencies  on CCGrebank Section 23", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.8507448434829712}, {"text": "CCGrebank Section", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.8258634805679321}]}, {"text": " Table 5: Error analysis of recall errors from 100  development set sentences.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9767553210258484}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9098397493362427}]}]}