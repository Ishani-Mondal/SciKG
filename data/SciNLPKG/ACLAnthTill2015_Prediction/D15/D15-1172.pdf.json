{"title": [{"text": "Do You See What I Mean? Visual Resolution of Linguistic Ambiguities", "labels": [], "entities": [{"text": "Visual Resolution of Linguistic Ambiguities", "start_pos": 24, "end_pos": 67, "type": "TASK", "confidence": 0.8415624260902405}]}], "abstractContent": [{"text": "Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception.", "labels": [], "entities": []}, {"text": "In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence.", "labels": [], "entities": [{"text": "grounded language understanding", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.6981213688850403}]}, {"text": "To this end, we introduce anew multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence.", "labels": [], "entities": []}, {"text": "We address this task by extending a vision model which determines if a sentence is depicted by a video.", "labels": [], "entities": []}, {"text": "We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a unified fashion across the different ambiguity types.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ambiguity is one of the defining characteristics of human languages, and language understanding crucially relies on the ability to obtain unambiguous representations of linguistic content.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.7585419416427612}]}, {"text": "While some ambiguities can be resolved using intra-linguistic contextual cues, the disambiguation of many linguistic constructions requires integration of world knowledge and perceptual information obtained from other modalities.", "labels": [], "entities": []}, {"text": "In this work, we focus on the problem of grounding language in the visual modality, and introduce a novel task for language understanding which requires resolving linguistic ambiguities by utilizing the visual context in which the linguistic content is expressed.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.744589775800705}]}, {"text": "This type of inference is frequently called for inhuman communication that occurs in a visual environment, and is crucial for language acquisition, when much of the linguistic content refers to the visual surroundings of the child.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.7457068860530853}]}, {"text": "Our task is also fundamental to the problem of grounding vision in language, by focusing on phenomena of linguistic ambiguity, which are prevalent in language, but typically overlooked when using language as a medium for expressing understanding of visual content.", "labels": [], "entities": []}, {"text": "Due to such ambiguities, a superficially appropriate description of a visual scene may in fact not be sufficient for demonstrating a correct understanding of the relevant visual content.", "labels": [], "entities": []}, {"text": "Our task addresses this issue by introducing a deep validation protocol for visual understanding, requiring not only providing a surface description of a visual activity but also demonstrating structural understanding at the levels of syntax, semantics and discourse.", "labels": [], "entities": []}, {"text": "To enable the systematic study of visually grounded processing of ambiguous language, we create anew corpus, LAVA (Language and Vision Ambiguities).", "labels": [], "entities": [{"text": "LAVA", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.799231767654419}, {"text": "Language and Vision Ambiguities)", "start_pos": 115, "end_pos": 147, "type": "TASK", "confidence": 0.5842479169368744}]}, {"text": "This corpus contains sentences with linguistic ambiguities that can only be resolved using external information.", "labels": [], "entities": []}, {"text": "The sentences are paired with short videos that visualize different interpretations of each sentence.", "labels": [], "entities": []}, {"text": "Our sentences encompass a wide range of syntactic, semantic and dis-course ambiguities, including ambiguous prepositional and verb phrase attachments, conjunctions, logical forms, anaphora and ellipsis.", "labels": [], "entities": []}, {"text": "Overall, the corpus contains 237 sentences, with 2 to 3 interpretations per sentence, and an average of 3.37 videos that depict visual variations of each sentence interpretation, corresponding to a total of 1679 videos.", "labels": [], "entities": []}, {"text": "Using this corpus, we address the problem of selecting the interpretation of an ambiguous sentence that matches the content of a given video.", "labels": [], "entities": []}, {"text": "Our approach for tackling this task extends the sentence tracker introduced in ().", "labels": [], "entities": [{"text": "sentence tracker", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7148350477218628}]}, {"text": "The sentence tracker produces a score which determines if a sentence is depicted by a video.", "labels": [], "entities": []}, {"text": "This earlier work had no concept of ambiguities; it assumed that every sentence had a single interpretation.", "labels": [], "entities": []}, {"text": "We extend this approach to represent multiple interpretations of a sentence, enabling us to pick the interpretation that is most compatible with the video.", "labels": [], "entities": []}, {"text": "To summarize, the contributions of this paper are threefold.", "labels": [], "entities": []}, {"text": "First, we introduce anew task for visually grounded language understanding, in which an ambiguous sentence has to be disambiguated using a visual depiction of the sentence's content.", "labels": [], "entities": [{"text": "visually grounded language understanding", "start_pos": 34, "end_pos": 74, "type": "TASK", "confidence": 0.779456228017807}]}, {"text": "Second, we release a multimodal corpus of sentences coupled with videos which covers a wide range of linguistic ambiguities, and enables a systematic study of linguistic ambiguities in visual contexts.", "labels": [], "entities": []}, {"text": "Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.999290943145752}]}], "datasetContent": [{"text": "We tested the performance of the model described in the previous section on the LAVA dataset presented in section 5.", "labels": [], "entities": [{"text": "LAVA dataset", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.9646230638027191}]}, {"text": "Each video in the dataset was pre-processed with object detectors for humans, bags, chairs, and telescopes.", "labels": [], "entities": []}, {"text": "We employed a mixture of CNN () and DPM) detectors, trained on held out sections of our corpus.", "labels": [], "entities": []}, {"text": "For each object class we generated proposals from both the CNN and the DPM detectors, and trained a scoring function to map both results into the same space.", "labels": [], "entities": []}, {"text": "The scoring function consisted of a sigmoid over the confidence of the detectors trained on the same held out portion of the training set.", "labels": [], "entities": []}, {"text": "As none of the disambiguation examples discussed here rely on the specific identity of the actors, we did not detect their identity.", "labels": [], "entities": []}, {"text": "Instead, any sentence which contains names was automatically converted to one which contains arbitrary \"person\" labels.", "labels": [], "entities": []}, {"text": "The sentences in our corpus have either two or three interpretations.", "labels": [], "entities": []}, {"text": "Each interpretation has one or more associated videos where the scene was shot from a different angle, carried out either by different actors, with different objects, or in different directions of motion.", "labels": [], "entities": []}, {"text": "For each sentence-video pair, we performed a 1-out-of-2 or 1-out-of-3 classification task to determine which of the interpretations of the corresponding sentence best fits that video.", "labels": [], "entities": []}, {"text": "Overall chance performance on our dataset is 49.04%, slightly lower than 50% due to the 1-out-of-3 classification examples.", "labels": [], "entities": [{"text": "chance", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9804239273071289}]}, {"text": "The model presented here achieved an accuracy of 75.36% over the entire corpus averaged across all error categories.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9995278120040894}]}, {"text": "This demonstrates that the model is largely capable of capturing the underlying task and that similar compositional crossmodal models may do the same.", "labels": [], "entities": []}, {"text": "For each of the 3 major ambiguity classes we had an accuracy of 84.26% for syntactic ambiguities, 72.28% for semantic ambiguities, and 64.44% for discourse ambiguities.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9995692372322083}]}, {"text": "The most significant source of model failures are poor object detections.", "labels": [], "entities": [{"text": "object detections", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7703291177749634}]}, {"text": "Objects are often rotated and presented at angles that are difficult to recognize.", "labels": [], "entities": []}, {"text": "Certain object classes like the telescope are much more difficult to recognize due to their small size and the fact that hands tend to largely occlude them.", "labels": [], "entities": [{"text": "telescope", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.8648549914360046}]}, {"text": "This accounts for the degraded performance of the semantic ambiguities relative to the syntactic ambiguities, as many more semantic ambiguities involved the telescope.", "labels": [], "entities": []}, {"text": "Object detector performance is similarly responsible for the lower performance of the discourse ambiguities which relied much more on the accuracy of the person detector as many sentences involve only people interacting with each other without any additional objects.", "labels": [], "entities": [{"text": "Object detector", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6719586402177811}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9991262555122375}]}, {"text": "This degrades performance by removing a helpful constraint for inference, according to which people tend to be close to the objects they are manipulating.", "labels": [], "entities": []}, {"text": "In addition, these sentences introduced more visual uncertainty as they often involved three actors.", "labels": [], "entities": []}, {"text": "The remaining errors are due to the event models.", "labels": [], "entities": [{"text": "errors", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9611031413078308}]}, {"text": "HMMs can fixate on short sequences of events which seem as if they are part of an action, but in fact are just noise or the prefix of another action.", "labels": [], "entities": []}, {"text": "Ideally, one would want an event model which has a global view of the action, if an object went up from the beginning to the end of the video while a person was holding it, it's likely that the object was being picked up.", "labels": [], "entities": []}, {"text": "The event models used here cannot enforce this constraint, they merely assert that the object was moving up for some number of frames; an event which can happen due to noise in the object detectors.", "labels": [], "entities": []}, {"text": "Enforcing such local constraints instead of the global constraint of the motion of the object over the video makes joint tracking and event recognition tractable in the framework presented here but can lead to errors.", "labels": [], "entities": [{"text": "joint tracking", "start_pos": 115, "end_pos": 129, "type": "TASK", "confidence": 0.6769810765981674}, {"text": "event recognition", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.7582865357398987}]}, {"text": "Finding models which strike a better balance between local information and global constraints while maintaining tractable inference remains an area of future work.", "labels": [], "entities": []}], "tableCaptions": []}