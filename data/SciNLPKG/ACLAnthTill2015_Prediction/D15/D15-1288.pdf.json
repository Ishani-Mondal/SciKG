{"title": [{"text": "Compact, Efficient and Unlimited Capacity: Language Modeling with Compressed Suffix Trees", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.6888734996318817}]}], "abstractContent": [{"text": "Efficient methods for storing and querying language models are critical for scaling to large corpora and high Markov orders.", "labels": [], "entities": []}, {"text": "In this paper we propose methods for mod-eling extremely large corpora without imposing a Markov condition.", "labels": [], "entities": []}, {"text": "At its core, our approach uses a succinct index-a compressed suffix tree-which provides near optimal compression while supporting efficient search.", "labels": [], "entities": []}, {"text": "We present algorithms for on-the-fly computation of probabilities under a Kneser-Ney language model.", "labels": [], "entities": []}, {"text": "Our technique is exact and although slower than leading LM toolkits, it shows promising scaling properties, which we demonstrate through \u221e-order modeling over the full Wikipedia collection.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models (LMs) are critical components in many modern NLP systems, including machine translation and automatic speech recognition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.817218542098999}, {"text": "automatic speech recognition", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.6420906484127045}]}, {"text": "The most widely used LMs are mgram models, based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets.", "labels": [], "entities": []}, {"text": "To be useful, LMs need to be not only accurate but also fast and compact.", "labels": [], "entities": []}, {"text": "Depending on the order and the training corpus size, atypical mgram LM may contain as many as several hundred billions of mgrams (, raising challenges of efficient storage and retrieval.", "labels": [], "entities": []}, {"text": "As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (, or loss-less LMs backed by tries), or related compressed structures (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9993003606796265}]}, {"text": "However, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements.", "labels": [], "entities": [{"text": "memory", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9542521238327026}]}, {"text": "An important exception is, who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20\u00d7 the training corpus).", "labels": [], "entities": []}, {"text": "In contrast, we 1 make use of recent advances in compressed suffix trees (CSTs) to build compact indices with much more modest memory requirements (\u2248 the size of the corpus).", "labels": [], "entities": []}, {"text": "We present methods for extracting frequency and unique context count statistics for mgram queries from CSTs, and two algorithms for computing Kneser-Ney LM probabilities on the fly using these statistics.", "labels": [], "entities": []}, {"text": "The first method uses two CSTs (over the corpus and the reversed corpus), which allow for efficient computation of the number of unique contexts to the left and right of an mgram, but is inefficient in several ways, most notably when computing the number of unique contexts to both sides.", "labels": [], "entities": []}, {"text": "Our second method addresses this problem using a single CST backed by a wavelet tree based FM-index, which results in better time complexity and considerably faster runtime performance.", "labels": [], "entities": []}, {"text": "Our experiments show that our method is practical for large-scale language modelling, although querying is substantially slower than a SRILM benchmark.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.6456058472394943}]}, {"text": "However our technique scales much more gracefully with Markov order m, allowing unbounded 'non-Markov' application, and enables training on large corpora as we demonstrate on the complete Wikipedia dump.", "labels": [], "entities": []}, {"text": "Overall this paper illustrates the vast potential succinct indexes have for language modelling and other 'big data' problems in language processing.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.8050150573253632}]}], "datasetContent": [{"text": "We used Europarl dataset and the data was numberized after tokenizing, splitting, and excluding XML markup.", "labels": [], "entities": [{"text": "Europarl dataset", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.9931820929050446}]}, {"text": "The first 10k sentences were used as the test data, and the last 80% as the training data, giving rise to training corpora of between 8M and 50M tokens and uncompressed size of up to 200 MiB (see for detailed corpus statistics).", "labels": [], "entities": []}, {"text": "We also processed the full 52 GiB uncompressed \"20150205\" English Wikipedia articles dump to create a character level language model consisting of 72M sentences.", "labels": [], "entities": [{"text": "20150205\" English Wikipedia articles dump", "start_pos": 48, "end_pos": 89, "type": "DATASET", "confidence": 0.9374789992968241}]}, {"text": "We excluded 10k random sentences from the collection as test data.", "labels": [], "entities": []}, {"text": "We use the SDSL library () to implement all our structures and compare our indexes to SRILM).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.7046114802360535}]}, {"text": "We refer to our dual-CST approach as D-CST, and the single-CST as S-CST.", "labels": [], "entities": []}, {"text": "We evaluated the perplexity across different languages and using mgrams of varying order from m = 2 to \u221e (unbounded), as shown on  SRILM (for smaller values of min which SRILM training was feasible, m \u2264 10).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.9238056540489197}]}, {"text": "Note that perplexity drops dramatically from m = 2 . .", "labels": [], "entities": [{"text": "perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9511251449584961}]}, {"text": "5 however the gains thereafter are modest for most languages.", "labels": [], "entities": []}, {"text": "Despite this, several large mgram matches were found ranging in size up to a 34-gram match.", "labels": [], "entities": []}, {"text": "We speculate that the perplexity plateau is due to the simplistic Kneser-Ney discounting formula which is not designed for higher order mgram LMs and appear to discount large mgrams too aggressively.", "labels": [], "entities": []}, {"text": "We leave further exploration of richer discounting techniques such as Modified Kneser-Ney or the Sequence Memoizer) to our future work.", "labels": [], "entities": []}, {"text": "compares space and time of our indexes with SRILM on the German part of Europarl.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.5858864188194275}, {"text": "German part of Europarl", "start_pos": 57, "end_pos": 80, "type": "DATASET", "confidence": 0.8291953504085541}]}, {"text": "The construction cost of our indexes in terms of both space and time is comparable to that of a 3/4-gram SRILM index.", "labels": [], "entities": []}, {"text": "The space usage of D-CST index is comparable to a compact 3-gram SRILM index.", "labels": [], "entities": []}, {"text": "Our S-CST index uses only 177 MiB RAM at query time, which is comparable to the size of the collection (172 MiB).", "labels": [], "entities": [{"text": "RAM", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.931709885597229}]}, {"text": "However, query processing is significantly slower for both our structures.", "labels": [], "entities": [{"text": "query processing", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.9302512407302856}]}, {"text": "For 2-grams, D-CST is 3 times slower than a 2-gram SRILM index as the expensive N 1+ ( \u00b7 \u03b1 \u00b7 ) is not computed.", "labels": [], "entities": [{"text": "D-CST", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.8991779088973999}, {"text": "SRILM", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.8364460468292236}]}, {"text": "However, for large mgrams, our indexes are much slower than SRILM.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.7495224475860596}]}, {"text": "For m > 2, the D-CST index is roughly six times slower than S-CST.", "labels": [], "entities": []}, {"text": "Our fastest index, is 10 times slower than the slowest SRILM 10-gram index.", "labels": [], "entities": [{"text": "fastest index", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9284580051898956}, {"text": "SRILM 10-gram index", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.48098326722780865}]}, {"text": "However, our run-time is independent of m.", "labels": [], "entities": []}, {"text": "Thus, as m increases, our index will become more competitive to SRILM while using a constant amount of space.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.5674986243247986}]}, {"text": "Next we analyze the performance of our index on the large Wikipedia dataset.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.977649986743927}]}, {"text": "The S-CST, character level index for the data set requires 22 GiB RAM at query time whereas the D-CST requires 43 GiB.", "labels": [], "entities": []}, {"text": "shows the run-time performance of both indexes for different mgrams, broken down by the different components of the computation.", "labels": [], "entities": []}, {"text": "As discussed above, 2-gram performance is much faster.", "labels": [], "entities": []}, {"text": "For both indexes, most time is spent computing N1PFRONTBACK (i.e., N 1+ ( \u00b7 \u03b1 \u00b7 )) for all m > 2.", "labels": [], "entities": []}, {"text": "However, the wavelet tree traversal used in S-CST roughly reduces the running time by a factor of three.", "labels": [], "entities": []}, {"text": "The complexity of N1PFRONTBACK depends on the number of contexts, which is likely small for larger mgrams, but can be large for small mgrams, which suggest partial precomputation could significantly increase the query performance of our indexes.", "labels": [], "entities": []}, {"text": "Exploring the myraid of different CST and CSA configurations available could also lead to significant improvements in runtime and space usage also remains future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset statistics, showing total un- compressed size; and tokens, types and sentence  counts for the training partition. For Wikipedia  the Word Types, and Tokens are computed based  on characters.", "labels": [], "entities": []}]}