{"title": [{"text": "Online Updating of Word Representations for Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Updating of Word Representations", "start_pos": 7, "end_pos": 39, "type": "TASK", "confidence": 0.6354912742972374}, {"text": "Part-of-Speech Tagging", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7688229084014893}]}], "abstractContent": [{"text": "We propose online unsupervised domain adaptation (DA), which is performed in-crementally as data comes in and is applicable when batch DA is not possible.", "labels": [], "entities": [{"text": "online unsupervised domain adaptation (DA)", "start_pos": 11, "end_pos": 53, "type": "TASK", "confidence": 0.7287435872214181}]}, {"text": "Ina part-of-speech (POS) tagging evaluation, we find that online unsupervised DA performs as well as batch DA.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6419801115989685}]}], "introductionContent": [{"text": "Unsupervised domain adaptation is a scenario that practitioners often face when having to build robust NLP systems.", "labels": [], "entities": [{"text": "Unsupervised domain adaptation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5937463343143463}]}, {"text": "They have labeled data in the source domain, but wish to improve performance in the target domain by making use of unlabeled data alone.", "labels": [], "entities": []}, {"text": "Most work on unsupervised domain adaptation in NLP uses batch learning: It assumes that a large corpus of unlabeled data of the target domain is available before testing.", "labels": [], "entities": [{"text": "unsupervised domain adaptation", "start_pos": 13, "end_pos": 43, "type": "TASK", "confidence": 0.7378758986790975}]}, {"text": "However, batch learning is not possible in many real-world scenarios where incoming data from anew target domain must be processed immediately.", "labels": [], "entities": [{"text": "batch learning", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.8556498289108276}]}, {"text": "More importantly, in many real-world scenarios the data does not come with neat domain labels and it may not be immediately obvious that an input stream is suddenly delivering data from anew domain.", "labels": [], "entities": []}, {"text": "Consider an NLP system that analyzes emails at an enterprise.", "labels": [], "entities": []}, {"text": "There is a constant stream of incoming emails and it changes overtime -without any clear indication that the models in use should be adapted to the new data distribution.", "labels": [], "entities": [{"text": "overtime", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9779154062271118}]}, {"text": "Because the system needs to work in real-time, it is also desirable to do any adaptation of the system online, without the need of stopping the system, changing it and restarting it as is done in batch mode.", "labels": [], "entities": []}, {"text": "In this paper, we propose online unsupervised domain adaptation as an extension to traditional unsupervised DA.", "labels": [], "entities": [{"text": "online unsupervised domain adaptation", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.5865404456853867}]}, {"text": "In online unsupervised DA, domain adaptation is performed incrementally as data comes in.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7201612442731857}]}, {"text": "Specifically, we adopt a form of representation learning.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.9259834289550781}]}, {"text": "In our experiments, the incremental updating will be performed for representations of words.", "labels": [], "entities": []}, {"text": "Each time a word is encountered in the stream of data attest time, its representation is updated.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, the work reported here is the first study of online unsupervised DA.", "labels": [], "entities": [{"text": "online unsupervised DA", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.5654093623161316}]}, {"text": "More specifically, we evaluate online unsupervised DA for the task of POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.8994345963001251}]}, {"text": "We compare POS tagging results for three distinct approaches: static (the baseline), batch learning and online unsupervised DA.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.8438602983951569}]}, {"text": "Our results show that online unsupervised DA is comparable in performance to batch learning while requiring no retraining or prior data in the target domain.", "labels": [], "entities": [{"text": "DA", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.6264650821685791}]}], "datasetContent": [{"text": "We reimplemented the FLORS tagger (), a fast and simple tagger that performs well in DA.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.8854764699935913}]}, {"text": "It treats POS tagging as a window-based (as opposed to sequence classification), multilabel classification problem.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8003183901309967}, {"text": "sequence classification), multilabel classification", "start_pos": 55, "end_pos": 106, "type": "TASK", "confidence": 0.696407695611318}]}, {"text": "FLORS is ideally suited for online unsupervised DA because its representation of words includes distributional vectors -these vectors can be easily updated in both batch learning and online unsupervised DA.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7685028910636902}]}, {"text": "More specifically, a word's representation in FLORS consists of four feature vectors: one each for its suffix, its shape and its left and right distributional neighbors.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.77134108543396}]}, {"text": "Suffix and shape features are standard features used in the literature; our use of them is exactly as described by.", "labels": [], "entities": [{"text": "Suffix", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8577492833137512}]}, {"text": "The i th entry xi of the left distributional vector of w is the weighted number of times the indicator word c i occurs immediately to the left of w: xi = tf (freq (bigram(c i , w))) where c i is the word with frequency rank i in the corpus, freq (bigram(c i , w)) is the number of occurrences of the bigram \"c i w\" and we weight non- zero frequencies logarithmically: tf(x) = 1 + log(x).", "labels": [], "entities": []}, {"text": "The right distributional vector is defined analogously.", "labels": [], "entities": []}, {"text": "We restrict the set of indicator words to then = 500 most frequent words.", "labels": [], "entities": []}, {"text": "To avoid zero vectors, we add an entry x n+1 to each vector that counts omitted contexts: be the concatentation of the two distributional and suffix and shape vectors of word w.", "labels": [], "entities": []}, {"text": "Then FLORS represents token vi as follows: where \u2295 is vector concatenation.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 5, "end_pos": 10, "type": "METRIC", "confidence": 0.8363363742828369}]}, {"text": "FLORS then tags token vi based on this representation.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5901038646697998}]}, {"text": "FLORS assumes that the association between distributional features and labels does not change fundamentally when going from source to target.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.535119354724884}]}, {"text": "This is in contrast to other work, notably, that carefully selects \"stable\" distributional features and discards \"unstable\" distributional features.", "labels": [], "entities": []}, {"text": "The hypothesis underlying FLORS is that basic distributional POS properties are relatively stable across domains -in contrast to semantic and other more complex tasks.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.5364883542060852}]}, {"text": "The high performance of FLORS ( suggests this hypothesis is true.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9288143515586853}]}, {"text": "We evaluate on the development sets of six different TDs: five SANCL () domains -newsgroups, weblogs, reviews, answers, emails -and sections 22-23 of WSJ for in-domain testing.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 150, "end_pos": 153, "type": "DATASET", "confidence": 0.9189610481262207}]}, {"text": "We use two training sets of different sizes.", "labels": [], "entities": []}, {"text": "In condition l:big (big labeled data set), we train FLORS on sections 2-21 of Wall Street Journal (WSJ).", "labels": [], "entities": [{"text": "FLORS", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9805147051811218}, {"text": "Wall Street Journal (WSJ)", "start_pos": 78, "end_pos": 103, "type": "DATASET", "confidence": 0.9645444949467977}]}, {"text": "Condition l:small uses 10% of l:big.", "labels": [], "entities": []}, {"text": "We also vary the size of the datasets that are used to compute the word representations before the FLORS model is trained on the training set.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.7948117256164551}]}, {"text": "In condition u:big, we compute distributional vectors on the joint corpus of all labeled and unlabeled text of source and target domains (except for the test sets).", "labels": [], "entities": []}, {"text": "We also include 100,000 WSJ sentences from 1988 and 500,000 sentences from Gigaword.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9742558598518372}]}, {"text": "In condition u:0, only labeled training data is used.", "labels": [], "entities": []}, {"text": "We implemented the following modification compared to the setup in (): distributional vectors are kept in memory as count vectors.", "labels": [], "entities": []}, {"text": "This allows us to increase the counts during online tagging.", "labels": [], "entities": [{"text": "counts", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9865912795066833}]}, {"text": "We run experiments with three versions of FLORS: STATIC, BATCH and ONLINE.", "labels": [], "entities": [{"text": "FLORS", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.8471640348434448}, {"text": "STATIC", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.973217785358429}, {"text": "BATCH", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9914776682853699}, {"text": "ONLINE", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9962589740753174}]}, {"text": "All three methods compute word representations on \"data for word representations\" (described above) before the model is trained on one of the two \"training sets\" (described above).", "labels": [], "entities": []}, {"text": "Word representations are not changed during testing.", "labels": [], "entities": []}, {"text": "Before testing, we update count vectors by freq (bigram(c i , w)) += freq * (bigram(c i , w)), where freq * (\u00b7) denotes the number of occurrences of the bigram \"c i w\" in the entire test set.", "labels": [], "entities": []}, {"text": "Before tagging a test sentence, both left and right distributional vectors are updated via freq (bigram(c i , w)) += 1 for each appearance of bigram \"c i w\" in the sentence.", "labels": [], "entities": [{"text": "freq", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9730433821678162}]}, {"text": "Then the sentence is tagged using the updated word representations.", "labels": [], "entities": []}, {"text": "As tagging progresses, the distributional representations become increasingly specific to the target domain (TD), converging to the representations that BATCH uses at the end of the tagging process.", "labels": [], "entities": [{"text": "tagging", "start_pos": 3, "end_pos": 10, "type": "TASK", "confidence": 0.9601998925209045}, {"text": "BATCH", "start_pos": 153, "end_pos": 158, "type": "DATASET", "confidence": 0.7731030583381653}]}, {"text": "In all three modes, suffix and shape features are always fully specified, for both known and unknown words.", "labels": [], "entities": []}, {"text": "investigates the effect of sizes of labeled and unlabeled data on performance of ONLINE and BATCH.", "labels": [], "entities": [{"text": "ONLINE", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.857632577419281}, {"text": "BATCH", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.6023697257041931}]}, {"text": "We report accuracy for all (ALL) tokens, for tokens occurring in both l:big and l:small (KN), tokens occurring in neither l:big nor l:small (OOV) and tokens ocurring in l:big, but not in l:small (SHFT).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994800686836243}]}, {"text": "1 Except for some minor variations in a few cases, both using more labeled data and using more unlabeled data improves tagging accuracy for both ONLINE and BATCH.", "labels": [], "entities": [{"text": "tagging", "start_pos": 119, "end_pos": 126, "type": "TASK", "confidence": 0.9529237747192383}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9808170199394226}, {"text": "ONLINE", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.8927255272865295}]}, {"text": "ONLINE and BATCH are generally better or as good as STATIC (in bold), always on ALL and OOV, and with a few exceptions also on KN and SHFT.", "labels": [], "entities": [{"text": "ONLINE", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9715148210525513}, {"text": "BATCH", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9779404401779175}, {"text": "STATIC", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9794329404830933}, {"text": "OOV", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9290416836738586}, {"text": "KN", "start_pos": 127, "end_pos": 129, "type": "DATASET", "confidence": 0.8985592126846313}, {"text": "SHFT", "start_pos": 134, "end_pos": 138, "type": "DATASET", "confidence": 0.7896872758865356}]}, {"text": "ONLINE performance is comparable to BATCH performance: it is slightly worse than BATCH on u:0 (largest ALL difference is .29) and at most .02 different from BATCH for ALL on u:big.", "labels": [], "entities": [{"text": "ONLINE", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9647531509399414}, {"text": "BATCH", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.7460173964500427}, {"text": "BATCH", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.7944894433021545}]}, {"text": "We ex-: Error rates (err) and standard deviations (std) for tagging.", "labels": [], "entities": [{"text": "Error rates (err)", "start_pos": 8, "end_pos": 25, "type": "METRIC", "confidence": 0.94373619556427}, {"text": "standard deviations (std)", "start_pos": 30, "end_pos": 55, "type": "METRIC", "confidence": 0.9405182003974915}, {"text": "tagging", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9681710004806519}]}, {"text": "\u2020 (resp. * ): significantly different from ONLINE error rate above&below (resp. from \"u:0\" error rate to the left).", "labels": [], "entities": [{"text": "ONLINE error rate", "start_pos": 43, "end_pos": 60, "type": "METRIC", "confidence": 0.9093856811523438}]}, {"text": "plain below why ONLINE is sometimes (slightly) better than BATCH, e.g., for ALL and condition l:small/u:big.", "labels": [], "entities": [{"text": "ONLINE", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.967279314994812}, {"text": "BATCH", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.986343502998352}, {"text": "ALL", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.6130479574203491}]}], "tableCaptions": [{"text": " Table 1: BATCH and ONLINE accuracies are comparable and state-of-the-art. Best number in each column is bold.", "labels": [], "entities": [{"text": "BATCH", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9475553631782532}, {"text": "ONLINE", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9849317073822021}]}, {"text": " Table 2: ONLINE / BATCH accuracies are generally better than STATIC (see bold numbers) and improve with both more training  data and more unlabeled data.", "labels": [], "entities": [{"text": "ONLINE", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9715933799743652}, {"text": "BATCH", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.7223681807518005}, {"text": "accuracies", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.6706070899963379}, {"text": "STATIC", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.8907601237297058}]}, {"text": " Table 3: Error rates (err) and standard deviations (std) for tagging.  \u2020 (resp.  * ): significantly different from ONLINE error rate  above&below (resp. from \"u:0\" error rate to the left).", "labels": [], "entities": [{"text": "Error rates (err)", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.7845731258392334}, {"text": "standard deviations (std)", "start_pos": 32, "end_pos": 57, "type": "METRIC", "confidence": 0.9208493113517762}, {"text": "tagging", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.9691214561462402}, {"text": "\u2020", "start_pos": 72, "end_pos": 73, "type": "METRIC", "confidence": 0.969798743724823}, {"text": "ONLINE error rate", "start_pos": 116, "end_pos": 133, "type": "METRIC", "confidence": 0.8814722100893656}]}]}