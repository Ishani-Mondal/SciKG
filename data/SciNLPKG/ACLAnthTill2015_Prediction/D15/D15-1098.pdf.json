{"title": [], "abstractContent": [{"text": "Distributed word representations are very useful for capturing semantic information and have been successfully applied in a variety of NLP tasks, especially on En-glish.", "labels": [], "entities": []}, {"text": "In this work, we innovatively develop two component-enhanced Chinese character embedding models and their bi-gram extensions.", "labels": [], "entities": []}, {"text": "Distinguished from En-glish word embeddings, our models explore the compositions of Chinese characters , which often serve as semantic in-dictors inherently.", "labels": [], "entities": []}, {"text": "The evaluations on both word similarity and text classification demonstrate the effectiveness of our models .", "labels": [], "entities": [{"text": "word similarity", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.7500637471675873}, {"text": "text classification", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7507062256336212}]}], "introductionContent": [{"text": "Due to its advantage over traditional one-hot representation, distributed word representation has demonstrated its benefit for semantic representation in various NLP tasks.", "labels": [], "entities": [{"text": "distributed word representation", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.6290876766045889}, {"text": "semantic representation", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.7177129685878754}]}, {"text": "Among the existing approaches (, the continuous bag-of-words model (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently ().", "labels": [], "entities": [{"text": "SkipGram", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.847058892250061}]}, {"text": "These two models learn the distributed representation of a word based on its context.", "labels": [], "entities": []}, {"text": "The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (.", "labels": [], "entities": []}, {"text": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates () or directly keeping the order features ().", "labels": [], "entities": []}, {"text": "As to another line, captures morphological composition by using neural networks and Qiu et al introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework.", "labels": [], "entities": []}, {"text": "While most previous work focuses on English, there is a little work on Chinese.", "labels": [], "entities": []}, {"text": "extracts the syntactical morphemes and incorporates the POS tags and dependency relations.", "labels": [], "entities": []}, {"text": "Basically, the work in Chinese follows the same ideas as in English.", "labels": [], "entities": []}, {"text": "Distinguished from English, Chinese characters are logograms, of which over 80% are phonosemantic compounds, with a semantic component giving abroad category of meaning and a phonetic component suggesting the sound . For example, the semantic component \u4ebb (human) of the Chinese character \u4ed6 (he) provides the meaning connected with human.", "labels": [], "entities": []}, {"text": "In fact, the components of most Chinese characters inherently bring with certain levels of semantics regardless of the contexts.", "labels": [], "entities": []}, {"text": "Being aware that the components of Chinese characters are finer grained semantic units, then an important question arises before slipping to the applications of word embeddings-would it be better to learn the semantic representations from the character components in Chinese?", "labels": [], "entities": []}, {"text": "We approach this question from both the practical and the cognitive points of view.", "labels": [], "entities": []}, {"text": "In practice, we expect the representations to be optimized for good generalization.", "labels": [], "entities": []}, {"text": "As analyzed before, the components are more generic unit inside Chinese characters that provides semantics.", "labels": [], "entities": []}, {"text": "Such inherent information somehow alleviates the shortcoming of the external contexts.", "labels": [], "entities": []}, {"text": "From the cognitive point of view, it has been found that the knowledge of semantic components significantly corre-late to Chinese word reading and sentence comprehension ().", "labels": [], "entities": [{"text": "Chinese word reading", "start_pos": 122, "end_pos": 142, "type": "TASK", "confidence": 0.5899601777394613}]}, {"text": "These evidences inspire us to explore novel Chinese character embedding models.", "labels": [], "entities": []}, {"text": "Different from word embeddings, character embeddings relate Chinese characters that occur in similar contexts with their component information.", "labels": [], "entities": []}, {"text": "Chinese characters convey the meanings from their components, and beyond that, the meanings of most Chinese words also take roots in their composite characters.", "labels": [], "entities": []}, {"text": "For example, the meaning of the Chinese word \u6447\u7bee (cradle) can be interpreted in terms of its composite characters \u6447 (sway) and \u7bee (basket).", "labels": [], "entities": []}, {"text": "Considering this, we further extend character embeddings from uni-gram models to bi-gram models.", "labels": [], "entities": []}, {"text": "At the core of our work is the exploration of Chinese semantic representations from a novel character-based perspective.", "labels": [], "entities": [{"text": "Chinese semantic representations", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.6841155389944712}]}, {"text": "Our proposed Chinese character embeddings incorporate the finergrained semantics from the components of characters and in turn enrich the representations inherently in addition to utilizing the external contexts.", "labels": [], "entities": []}, {"text": "The evaluations on both intrinsic word similarity and extrinsic text classification demonstrate the effectiveness and potentials of the new models.", "labels": [], "entities": [{"text": "extrinsic text classification", "start_pos": 54, "end_pos": 83, "type": "TASK", "confidence": 0.6368663807710012}]}], "datasetContent": [{"text": "We examine the quality of the proposed two Chinese character embedding models as well as their corresponding extensions on both intrinsic word similarity evaluation and extrinsic text classification evaluation.", "labels": [], "entities": [{"text": "word similarity evaluation", "start_pos": 138, "end_pos": 164, "type": "TASK", "confidence": 0.6993511319160461}, {"text": "extrinsic text classification evaluation", "start_pos": 169, "end_pos": 209, "type": "TASK", "confidence": 0.7327595576643944}]}, {"text": "we start from developing appropriate Chinese synonym sets.", "labels": [], "entities": []}, {"text": "Two candidate choices are Chinese dictionaries HowNet () and HIT-CIR's Extended Tongyici Cilin (denoted as E-TC) . As HowNet contains less modern words, such as \u8c37 \u6b4c (Google), we select E-TC as our benchmark for word similarity evaluation.  and extract all their component lists.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.7801940441131592}, {"text": "HIT-CIR", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9199795126914978}, {"text": "word similarity evaluation.", "start_pos": 211, "end_pos": 238, "type": "TASK", "confidence": 0.7093762457370758}]}, {"text": "It is easy to obtain the first components (i.e., the radicals), as they are readily available in the online Xinhua Dictionary 6 . For the rest radical-like components, we extract them by matching the patterns like \"\u4ece (from)+X\" in the Xinhua dictionary.", "labels": [], "entities": [{"text": "Xinhua Dictionary 6", "start_pos": 108, "end_pos": 127, "type": "DATASET", "confidence": 0.9484707117080688}, {"text": "Xinhua dictionary", "start_pos": 234, "end_pos": 251, "type": "DATASET", "confidence": 0.9640547037124634}]}, {"text": "Such a pattern indicates that a character has a component of X.", "labels": [], "entities": []}, {"text": "We also enrich the component lists by matching the pattern \"X is only seen\" in Hong Kong Computer Chinese Basic Component Reference . It is observed that nearly 65% Chinese characters have only one component (their radicals), and 95% Chinese characters have two components (including their radicals).", "labels": [], "entities": [{"text": "Hong Kong Computer Chinese Basic Component Reference", "start_pos": 79, "end_pos": 131, "type": "DATASET", "confidence": 0.936273319380624}]}, {"text": "Thus, we decide to maintain up to two extracted components to build the character embeddings according to the frequency of their occurrences.", "labels": [], "entities": []}, {"text": "To cope with the radical variation problem, we transform 24 radical variants to their origins, such as \u4ebb to \u4eba (human), \u624c to \u624b (hand), \u6c35 to \u6c34 (water) and \u8fb6 to \u8fb5 (foot).", "labels": [], "entities": []}, {"text": "The complete list of the transformations is provided in http://xh.5156edu.com/ 7 http://www.ogcio.gov.hk/tc/business/tech_ promotion/ccli/cliac/glyphs_guidelines.htm Appendix for easy reference.", "labels": [], "entities": []}, {"text": "We adopt Chinese Wikipedia Dump 8 to train our models as well as the original CBOW and SkipGram, implemented in the Word2Vec tool 9 for comparison.", "labels": [], "entities": [{"text": "Chinese Wikipedia Dump 8", "start_pos": 9, "end_pos": 33, "type": "DATASET", "confidence": 0.8729563504457474}, {"text": "CBOW", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.8720413446426392}, {"text": "SkipGram", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.8704359531402588}, {"text": "Word2Vec tool 9", "start_pos": 116, "end_pos": 131, "type": "DATASET", "confidence": 0.9252832333246866}]}, {"text": "The corpus in total contains 232,894 articles.", "labels": [], "entities": []}, {"text": "In preprocessing, we remove pure digit words and non-Chinese characters, and ignore the words less than 10 occurrences during training.", "labels": [], "entities": []}, {"text": "We set the context window size T as 2 and use 5 negative samples experimentally.", "labels": [], "entities": []}, {"text": "All the embedding dimensions K are set to 50.", "labels": [], "entities": []}, {"text": "In the word similarity evaluation, we compute the Spearman's rank correlation between the similarity scores based on the learned embedding models and the E-TC similarity scores computed by following.", "labels": [], "entities": [{"text": "word similarity evaluation", "start_pos": 7, "end_pos": 33, "type": "TASK", "confidence": 0.6733290751775106}, {"text": "Spearman's rank correlation", "start_pos": 50, "end_pos": 77, "type": "METRIC", "confidence": 0.7244304493069649}]}, {"text": "The bi-character embeddings are concatenation of the composite character embeddings.", "labels": [], "entities": []}, {"text": "For the text classification evaluation, we average the composite single character embeddings for each bi-gram.", "labels": [], "entities": [{"text": "text classification", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.8309232294559479}]}, {"text": "And each bi-gram overlaps with the previous one.", "labels": [], "entities": []}, {"text": "The titles are represented by averaging the embeddings of their composite grams . presents the word similarity evaluation results of the eight embedding models mentioned above, where A-L denote the twelve categories in E-TC.", "labels": [], "entities": []}, {"text": "The first four rows are the results with the uni-character inputs, and the last four rows correspond to the bi-character embeddings results.", "labels": [], "entities": []}, {"text": "We can see that both CBOW and CBOW-bi perform worse than the corresponding SkipGram and SkipGram-bi.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.7699943780899048}, {"text": "SkipGram", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9565707445144653}, {"text": "SkipGram-bi", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.9598110914230347}]}, {"text": "This result is consistent with the finding in the previous work).", "labels": [], "entities": []}, {"text": "To some extent, CBOW and its extension CBOWbi are the most different among the eight (the first four models in and the first four models in).", "labels": [], "entities": [{"text": "CBOW", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.8665457367897034}, {"text": "CBOWbi", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.9092934727668762}]}, {"text": "They tie together the characters in each context window by representing the context vector as the sum of their characters' vectors.", "labels": [], "entities": []}, {"text": "Although they have a potential of deriving better representations (, they lose some particular information from each unit of input in the average operations.", "labels": [], "entities": []}, {"text": "Although the performance on twelve different categories varies, in overall charCBOW, charSkipGram and their extensions consistently better correlate to E-TC.", "labels": [], "entities": []}, {"text": "It provides the evidence that the component information in Chinese characters is of significance.", "labels": [], "entities": []}, {"text": "Clearly, the bi-character models achieve higher rank correlations.", "labels": [], "entities": []}, {"text": "These results are not surprised.", "labels": [], "entities": []}, {"text": "As a matter of fact, a majority of Chinese words are compounds of two characters.", "labels": [], "entities": []}, {"text": "Thus, in many cases two characters together is equivalent to a Chinese word.", "labels": [], "entities": []}, {"text": "Considering the superiority of the bi-character models, we only apply them in the text classification evaluations.", "labels": [], "entities": [{"text": "text classification", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.801410585641861}]}, {"text": "The results shown in the first four rows of Table 2 are similar to those in the word similarity evaluation.", "labels": [], "entities": [{"text": "word similarity evaluation", "start_pos": 80, "end_pos": 106, "type": "TASK", "confidence": 0.7664409875869751}]}, {"text": "Please notice the significant improvement of charCBOW and charCBOW-bi.", "labels": [], "entities": []}, {"text": "We conjecture this as a hint of the importance of the order information, which is introduced by the extra parameter in the output matrixes.", "labels": [], "entities": []}, {"text": "Their better performances verify our assumption that the radicals are more important than non-radicals.", "labels": [], "entities": []}, {"text": "This is also attributed to the benefit from the order of the characters in the contexts.", "labels": [], "entities": []}, {"text": "Actually, we also conduct an additional experiment to combine the uni-gram and the bi-gram embeddings for text classification and notice in aver- We do not compare the uni-formed characters with biformed compound characters.", "labels": [], "entities": [{"text": "text classification", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7762536406517029}]}, {"text": "The word pairs that cannot be found in the vocabulary are removed.", "labels": [], "entities": []}, {"text": "age about 8.4% of gain over the bi-gram embeddings alone.", "labels": [], "entities": []}, {"text": "The detailed results are presented in the last four rows of.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word Similarity Results of Embedding Models", "labels": [], "entities": [{"text": "Word Similarity", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6452909409999847}]}, {"text": " Table 2: Text Classification Results of Embedding Models", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.6801955848932266}]}]}