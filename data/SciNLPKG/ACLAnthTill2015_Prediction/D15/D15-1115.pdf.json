{"title": [{"text": "Semantic Framework for Comparison Structures in Natural Language", "labels": [], "entities": []}], "abstractContent": [{"text": "Comparison is one of the most important phenomena in language for expressing objective and subjective facts about various entities.", "labels": [], "entities": []}, {"text": "Systems that can understand and reason over comparative structure can play a major role in the applications which require deeper understanding of language.", "labels": [], "entities": []}, {"text": "In this paper we present a novel semantic framework for representing the meaning of comparative structures in natural language, which models comparisons as predicate-argument pairs interconnected with semantic roles.", "labels": [], "entities": []}, {"text": "Our framework supports not only adjectival, but also adverbial, nominal, and verbal compara-tives.", "labels": [], "entities": []}, {"text": "With this paper, we provide a novel dataset of gold-standard comparison structures annotated according to our semantic framework.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing the meaning of text has long been a focus in linguistics and deriving computational models of meaning has been pursued by various semantic tasks such as semantic parsing.", "labels": [], "entities": [{"text": "Representing the meaning of text", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9139290928840638}, {"text": "semantic parsing", "start_pos": 166, "end_pos": 182, "type": "TASK", "confidence": 0.7143303751945496}]}, {"text": "Deep semantic parsing (as opposed to shallow semantic parsing, such as semantic role labeling) aims to map a sentence in natural language into its corresponding formal meaning representation (.", "labels": [], "entities": [{"text": "Deep semantic parsing", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6279765864213308}, {"text": "semantic role labeling", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.6752269268035889}]}, {"text": "There has been a renewed interest in deeper semantic representations of natural language) in NLP community.", "labels": [], "entities": []}, {"text": "Opendomain semantic representations enable inference and reasoning, which is required for many language understanding tasks such as reading comprehension tests and open-domain question answering.", "labels": [], "entities": [{"text": "open-domain question answering", "start_pos": 164, "end_pos": 194, "type": "TASK", "confidence": 0.5698361098766327}]}, {"text": "Comparison is a common way for expressing differences in sentiment and other properties towards some entity.", "labels": [], "entities": []}, {"text": "Comparison can happen in very simple structures such as 'John is taller than Sam', or more complicated constructions such as 'The table is longer than the sofa is wide'.", "labels": [], "entities": []}, {"text": "So far the computational semantics of comparatives and how they affect the meaning of text has not been studied effectively.", "labels": [], "entities": []}, {"text": "That is, the difference between the existing semantic and syntactic representation of comparatives is not distinctive enough for enabling deeper understanding of a sentence.", "labels": [], "entities": []}, {"text": "For instance, the general logical form representation of the sentence 'John is taller than Susan' using the Boxer system) is the following: The above meaning representation does not fully capture the underlying semantics of the adjective 'tall' and what it means to be 'taller'.", "labels": [], "entities": []}, {"text": "A human reader can easily infer that actually the height of John is greater than the height of Susan.", "labels": [], "entities": []}, {"text": "Another example to consider is the sentence 'John is tall', which basically has the typical logical form tall(john) -which is a very superficial representation for the meaning of the predicate 'tall'.", "labels": [], "entities": []}, {"text": "Likewise, a human reader can infer that defining someone as 'tall' in some domain of discourse entails that this person is somehow 'taller' than some other population (say their average), however, the earlier typical logical form representation does not enable such inferences.", "labels": [], "entities": []}, {"text": "In this paper we introduce a novel framework for semantic representation and computational analysis of the structure of comparison in natural language.", "labels": [], "entities": [{"text": "semantic representation", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.7366477251052856}]}, {"text": "This framework enables deeper representation of semantics of comparatives, including all different types of comparison within compara-tives, superlatives, equatives, excessives, and assetives, and the way they are related to their corresponding semantic roles.", "labels": [], "entities": []}, {"text": "Together with this paper, we provide a dataset of gold-annotated comparative structures using our meaning representation, which enables training models on comparison constructions.", "labels": [], "entities": []}, {"text": "We propose anew approach for automatic extraction of comparison structures from a given text.", "labels": [], "entities": [{"text": "automatic extraction of comparison structures from a given text", "start_pos": 29, "end_pos": 92, "type": "TASK", "confidence": 0.8100532227092319}]}, {"text": "A semantic representation of the comparison expressed by the sentence 'The equipment is too old to be much of use to us.'", "labels": [], "entities": []}, {"text": "augmented under our representation would be the following: Throughout this paper we define a comparison to be any statement comparing two or more entities, expressing some kind of measurement on a scale, or indicating some degree of having a measurable property.", "labels": [], "entities": []}, {"text": "The details of these variations will be discussed in Section 3.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to make our gold-annotated dataset we used OntoNotes () release 5.0 corpus.", "labels": [], "entities": [{"text": "OntoNotes () release 5.0 corpus", "start_pos": 52, "end_pos": 83, "type": "DATASET", "confidence": 0.8923251986503601}]}, {"text": "OntoNotes covers various genres such as conversations, news-wire, and Weblogs, which provides distinctive variations of comparison structures in natural language.", "labels": [], "entities": []}, {"text": "Furtheremore, we think our annotations can potentially provide augmentations on OntoNotes, so using the original OntoNotes sentences can be beneficial.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 80, "end_pos": 89, "type": "DATASET", "confidence": 0.8685312867164612}]}, {"text": "One approach for pinpointing comparison sentences is to mine for some known patterns and train a classifier for distinguishing comparison and non-comparison sentences).", "labels": [], "entities": []}, {"text": "However, as demonstrated earlier, the variety of comparison structures is so vast that being limited to some specific patterns or syntactic structures will not serve our purpose.", "labels": [], "entities": []}, {"text": "In order to address this issue, we randomly selected 2000 sentences from OntoNotes which contained an adjective, an adverb, or any of the comparison morphemes.", "labels": [], "entities": []}, {"text": "This set contained some non-comparison sentences, such as 'John admitted to the crime too'.", "labels": [], "entities": []}, {"text": "In order to make the final set of comparison sentences we performed the following task: we define a comparative sentence as a sentence that contains at least one predicate operator as defined in Section 3.", "labels": [], "entities": []}, {"text": "Hence, we provided three human experts with a full predicate operator types table and asked each of them to annotate any predicate operator found in the given sentences.", "labels": [], "entities": []}, {"text": "Then we retained any sentences with at least one predicate operator which was annotated by at least two of the three judges.", "labels": [], "entities": []}, {"text": "We further refined the set to include equal number of predicate types.", "labels": [], "entities": []}, {"text": "This resulted in 531 sentences.", "labels": [], "entities": []}, {"text": "After collecting the comparison sentences, we asked the annotators to provide gold-standard annotation of predicate-argument structure of the sentences.", "labels": [], "entities": []}, {"text": "This involves the annotator to read the annotation guideline and basically understand the semantic framework for comparison structures that we introduced in Section 3.", "labels": [], "entities": []}, {"text": "Initially, we ran a pilot study on a set of 50 sentences where each sentence was annotated by two of the experts.", "labels": [], "entities": []}, {"text": "We used pilot results for iterating over the annotation schema and guideline and resolving issues regarding low agreement predicates and argument types , until getting to average agreement \u03ba = 0.80.", "labels": [], "entities": []}, {"text": "We split the dataset into 30% and 70% for testing and training respectively.", "labels": [], "entities": []}, {"text": "Here we evaluate the performance of our proposed predicate-argument structure prediction.", "labels": [], "entities": [{"text": "predicate-argument structure prediction", "start_pos": 49, "end_pos": 88, "type": "TASK", "confidence": 0.7456947366396586}]}, {"text": "We present the following two methods: \u2022 ILP Method: Our full approach as described in Section 4.", "labels": [], "entities": []}, {"text": "Here we used the Gurobi 7 optimization package for finding an exact solution for our ILP formalization.", "labels": [], "entities": []}, {"text": "\u2022 Baseline: A simple pattern-based method which uses lexical patterns for predicting predicate type and argument types.", "labels": [], "entities": [{"text": "predicting predicate type and argument types", "start_pos": 74, "end_pos": 118, "type": "TASK", "confidence": 0.8363564709822336}]}, {"text": "This method uses the generic comparative morphemes such as 'er', 'est', 'more' and 'less' for detecting any specific type of predicate.", "labels": [], "entities": []}, {"text": "For identifying predicate arguments it relies on rules which use syntactic structure, e.g., fora 'greater' predicate identified by 'er' morpheme, the 'left' argument is always the main subject of the sentence.", "labels": [], "entities": []}, {"text": "This method annotates anything not recognized by patterns as 'None'.", "labels": [], "entities": []}, {"text": "Here with compare their predictions on test set to the gold standard annotations and compute micro-averaged precision, recall and F1 score.", "labels": [], "entities": [{"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9329327940940857}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9996285438537598}, {"text": "F1 score", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9844262003898621}]}, {"text": "For this analysis we remove the 'equative' predicate type, given its very low frequency in our training set.", "labels": [], "entities": []}, {"text": "Moreover, here we do not include the positive and negative predicate types, as these take only one role argument which is 'figure', making the prediction task trivial.", "labels": [], "entities": []}, {"text": "shows the results of predicate type prediction.", "labels": [], "entities": [{"text": "predicate type prediction", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.8580139875411987}]}, {"text": "The final reported average in this table excludes the type 'None'.", "labels": [], "entities": [{"text": "None", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.8061236143112183}]}, {"text": "The best performing category in both methods is 'superlative', which is because of its more typical structure which makes it easier to be predicted.", "labels": [], "entities": []}, {"text": "In general, the precision of predicate prediction is very high in ILP method, which is due to the fact that our predicates are the comparison operators indicated by the comparison morphemes.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9993745684623718}, {"text": "predicate prediction", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.8379334211349487}]}, {"text": "The baseline performs considerably weaker than ILP method for predicting less and greater predicates.", "labels": [], "entities": []}, {"text": "This is because predicting these types requires a more complicated analysis where simple morphological and syntactic patterns can result in many false positives.", "labels": [], "entities": []}, {"text": "depicts the results of the role type prediction.", "labels": [], "entities": [{"text": "role type prediction", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7863439122835795}]}, {"text": "The weighted average in this table is based on frequency, excluding the type 'None'.", "labels": [], "entities": []}, {"text": "The precision on role prediction varies across different types.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9991957545280457}, {"text": "role prediction", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.9500367343425751}]}, {"text": "Overall, the baseline performs weakly on predicting role types, which is dues to the complicated structure of roles.", "labels": [], "entities": [{"text": "predicting role types", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.9013834198315939}]}, {"text": "The best prediction of ILP method is on scales, which has benefited from the attribute concept feature.", "labels": [], "entities": []}, {"text": "The weaker performing types have been affected by the low-frequency occurrence in the training set.", "labels": [], "entities": []}, {"text": "There are many cases of very long and complex sentences in our dataset.", "labels": [], "entities": []}, {"text": "One major reason behind some of the false predictions is incorrect dependency parse for long sentences.", "labels": [], "entities": []}, {"text": "One notable issue here is that for easier prediction and analysis, we had asked our annotators to mark only the head words for phrasal arguments.", "labels": [], "entities": [{"text": "prediction and analysis", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.6965652406215668}]}, {"text": "This had often caused lower agreement among annotators and hence worse predictions on the system trained on the dataset.", "labels": [], "entities": [{"text": "agreement", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9820778965950012}]}, {"text": "In future, we are going to switch to span-based argument identification.", "labels": [], "entities": [{"text": "span-based argument identification", "start_pos": 37, "end_pos": 71, "type": "TASK", "confidence": 0.5939758519331614}]}], "tableCaptions": [{"text": " Table 4: The evaluation results on predicate type  prediction.", "labels": [], "entities": [{"text": "predicate type  prediction", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.8046802679697672}]}, {"text": " Table 5: The evaluation result on role type predic- tion.", "labels": [], "entities": []}]}