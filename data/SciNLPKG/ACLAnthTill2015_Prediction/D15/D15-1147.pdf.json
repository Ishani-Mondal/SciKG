{"title": [{"text": "How to Avoid Unwanted Pregnancies: Domain Adaptation using Neural Network Models", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7853752076625824}]}], "abstractContent": [{"text": "We present novel models for domain adaptation based on the neural network joint model (NNJM).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7602617740631104}]}, {"text": "Our models maximize the cross entropy by regularizing the loss function with respect to in-domain model.", "labels": [], "entities": []}, {"text": "Domain adaptation is carried out by assigning higher weight to out-domain sequences that are similar to the in-domain data.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8111317455768585}]}, {"text": "In our alternative model we take a more restrictive approach by additionally penalizing sequences similar to the out-domain data.", "labels": [], "entities": []}, {"text": "Our models achieve better perplexities than the baseline NNJM models and give improvements of up to 0.5 and 0.6 BLEU points in Arabic-to-English and English-to-German language pairs, on a standard task of translating TED talks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9987781643867493}, {"text": "translating TED talks", "start_pos": 205, "end_pos": 226, "type": "TASK", "confidence": 0.8901095390319824}]}], "introductionContent": [{"text": "Rapid influx of digital data has galvanized the use of empirical methods in many fields including Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.8653874039649964}]}, {"text": "The increasing availability of bilingual corpora has made it possible to automatically learn translation rules that required years of linguistic analysis previously.", "labels": [], "entities": []}, {"text": "While additional data is often beneficial fora general purpose Statistical Machine Translation (SMT) system, a problem arises when translating new domains such as lectures (), patents () or medical text (, where either the bilingual text does not exist or is available in small quantity.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 63, "end_pos": 100, "type": "TASK", "confidence": 0.8109410206476847}]}, {"text": "All domains have their own vocabulary and stylistic preferences which cannot be fully encompassed by a system trained on the general domain.", "labels": [], "entities": []}, {"text": "Machine translation systems trained from a simple concatenation of small in-domain and large out-domain data often perform below par because the out-domain data is distant or overwhelmingly larger than the in-domain data.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7949305176734924}]}, {"text": "Additional data increases lexical ambiguity by introducing new senses to the existing in-domain vocabulary.", "labels": [], "entities": []}, {"text": "For example, an Arabic-to-English SMT system trained by simply concatenating inand out-domain data translates the Arabic phrase \" \" to \"about the problem of unwanted pregnancy\".", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9551816582679749}]}, {"text": "This translation is incorrect in the context of the in-domain data, where it should be translated to \"about the problem of choice overload\".", "labels": [], "entities": []}, {"text": "The sense of the Arabic phrase taken from out-domain data completely changes the meaning of the sentence.", "labels": [], "entities": []}, {"text": "In this paper, we tackle this problem by proposing domain adaptation models that make use of all the data while preserving the in-domain preferences.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7120235562324524}]}, {"text": "A significant amount of research has been carried out recently in domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7983024716377258}]}, {"text": "The complexity of the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carryout domain specific adaptations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9944508671760559}, {"text": "corpus preparation", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.6923502534627914}]}, {"text": "This is typically done using either data selection) or model adaptation.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.6988140046596527}]}, {"text": "In this paper, we further research in model adaptation using the neural network framework.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7542072832584381}]}, {"text": "In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems.", "labels": [], "entities": []}, {"text": "A notably successful attempt on the SMT frontier was recently made by.", "labels": [], "entities": [{"text": "SMT frontier", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.7293516248464584}]}, {"text": "They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams.", "labels": [], "entities": []}, {"text": "The model is then integrated into the decoder and used as an additional language model feature.", "labels": [], "entities": []}, {"text": "Our aim in this paper is to advance the state-ofthe-art in SMT by extending NNJM for domain adaptation to leverage the huge amount of out-domain data coming from heterogeneous sources.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9959185719490051}, {"text": "NNJM", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.7141833305358887}, {"text": "domain adaptation", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7136426568031311}]}, {"text": "We hypothesize that the distributed vector representation of NNJM helps to bridge the lexical differences between the in-domain and the outdomain data, and adaptation is necessary to avoid deviation of the model from the in-domain data, which otherwise happens because of the large outdomain data.", "labels": [], "entities": [{"text": "NNJM", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.9188278317451477}]}, {"text": "To this end, we propose two novel extensions of NNJM for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7154689580202103}]}, {"text": "Our first model minimizes the cross entropy by regularizing the loss function with respect to the in-domain model.", "labels": [], "entities": []}, {"text": "The regularizer gives higher weight to the training instances that are similar to the in-domain data.", "labels": [], "entities": []}, {"text": "Our second model takes a more conservative approach by additionally penalizing data instances similar to the out-domain data.", "labels": [], "entities": []}, {"text": "We evaluate our models on the standard task of translating Arabic-English and English-German language pairs.", "labels": [], "entities": [{"text": "translating Arabic-English", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.8727795481681824}]}, {"text": "Our adapted models achieve better perplexities) than the models trained on in-and in+out-domain data.", "labels": [], "entities": []}, {"text": "Improvements are also reflected in BLEU scores () as we compare these models within the SMT pipeline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9983996748924255}, {"text": "SMT pipeline", "start_pos": 88, "end_pos": 100, "type": "TASK", "confidence": 0.8921804130077362}]}, {"text": "We obtain gains of up to 0.5 and 0.6 on Arabic-English and EnglishGerman pairs over a competitive baseline system.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2 gives an account on related work.", "labels": [], "entities": []}, {"text": "Section 3 revisits NNJM model and Section 4 discusses our models.", "labels": [], "entities": [{"text": "NNJM model", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.9046777784824371}]}, {"text": "Section 5 presents the experimental setup and the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the experimental setup (i.e., data, settings for NN models and MT pipeline) and the results.", "labels": [], "entities": []}, {"text": "First we evaluate our models intrinsically by comparing the perplexities on a held-out in-domain testset against the baseline NNJM model.", "labels": [], "entities": []}, {"text": "Then we carryout an extrinsic evaluation by using the NNJM and NDAM models as features in machine translation and compare the BLEU scores.", "labels": [], "entities": [{"text": "NNJM", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.9549437761306763}, {"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.6702727377414703}, {"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9984099864959717}]}, {"text": "Initial developmental experiments were done on the Arabic-to-English language pair.", "labels": [], "entities": []}, {"text": "We carried out further experiments on the Englishto-German pair to validate our models.", "labels": [], "entities": []}, {"text": "In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010).", "labels": [], "entities": []}, {"text": "We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the relatedness of each domain with respect to the in-domain.", "labels": [], "entities": []}, {"text": "We later replicated selective experiments for the English-German language pair.", "labels": [], "entities": []}, {"text": "The first part of  shows results of the models trained from concatenating each domain to the in-domain data.", "labels": [], "entities": []}, {"text": "The perplexity numbers improved significantly in each case showing that there is useful information available in each domain which can be utilized to improve the baseline.", "labels": [], "entities": []}, {"text": "It also shows the robustness of neural network models.", "labels": [], "entities": []}, {"text": "Unlike the n-gram model, the NN-based model improves generalization with the increase in data without completely skewing towards the dominating part of the data.", "labels": [], "entities": []}, {"text": "Concatenating in-domain with the NEWS data gave better perplexities than other domains.", "labels": [], "entities": [{"text": "NEWS data", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.9673987627029419}]}, {"text": "Best results were obtained by concatenating all the data together (See row ALL).", "labels": [], "entities": [{"text": "ALL", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.7828092575073242}]}, {"text": "The third and fourth columns show results of our models (NDAM v * ).", "labels": [], "entities": []}, {"text": "Both give better perplexities than NNJM cat in all cases.", "labels": [], "entities": [{"text": "NNJM", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.834347128868103}]}, {"text": "However, it is unclear which of the two is better.", "labels": [], "entities": []}, {"text": "Similar observations were made for the English-to-German pair, where we only did experiments on the concatenation of all domains.", "labels": [], "entities": []}, {"text": "Arabic-to-English: For most language pairs, the conventional wisdom is to train the system with all available data.", "labels": [], "entities": []}, {"text": "However, previously reported MT results on Arabic-to-English ( show that this is not optimal and the results are often worse than only using indomain data.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9833266139030457}]}, {"text": "The reason for this is that the UN domain is found to be distant and overwhelmingly large as compared to the in-domain IWSLT data.", "labels": [], "entities": [{"text": "UN domain", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.8573476076126099}, {"text": "IWSLT data", "start_pos": 119, "end_pos": 129, "type": "DATASET", "confidence": 0.9030902683734894}]}, {"text": "We carried out domain-wise experiments and also found this to be true.", "labels": [], "entities": []}, {"text": "We considered three baseline systems: (i) B in , shows results of the MT systems S v1 and S v2 using our adapted models NDAM v1 and NDAM v2 . We compare them to the baseline system B cat , which uses the non-adapted NNJM cat as a feature.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9176343083381653}]}, {"text": "S v1 achieved an improvement of up to +0.4 and S v2 achieved an improvement of up to +0.5 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9988123178482056}]}, {"text": "However, S v2 performs slightly worse than S v1 on individual domains.", "labels": [], "entities": []}, {"text": "We speculate this is because of the nature of the NDAM v2 , which gives high weight to out-domain sequences that are liked by the in-domain model and disliked by the out-domain model.", "labels": [], "entities": [{"text": "NDAM", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.8145352602005005}]}, {"text": "In the case of individual domains, NDAM v2 might be over penalizing out-domain since the out-domain model is only built on that particular domain and always prefers it more than the in-domain model.", "labels": [], "entities": []}, {"text": "In case of ALL, the out-domain model is more diverse and has different level of likeness for each domain.", "labels": [], "entities": [{"text": "ALL", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9167356491088867}]}, {"text": "We analyzed the output of the baseline system (S cat ) and spotted several cases of lexical ambiguity caused by out-domain data.", "labels": [], "entities": []}, {"text": "For example, the Arabic phrase can be translated to choice overload or unwanted pregnancy.", "labels": [], "entities": []}, {"text": "The latter translation is incorrect in the context of in-domain.", "labels": [], "entities": []}, {"text": "The bias created due to the out-domain data caused S cat to choose the contextually incorrect translation unwanted pregnancy.", "labels": [], "entities": []}, {"text": "However, the adapted systems S v * were able to translate it   to proprietary by S cat , a translation frequently observed in the out-domain data.", "labels": [], "entities": []}, {"text": "S v * translated it correctly to fitness, as preferred by the in-domain.", "labels": [], "entities": []}, {"text": "English-to-German: Concatenating all training data to train the MT pipeline has been shown to give the best results for English-to-German ().", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9271965026855469}]}, {"text": "Therefore, we did not do domainwise experiments, except for training a system on the in-domain IWSLT data for the sake of completeness.", "labels": [], "entities": [{"text": "IWSLT data", "start_pos": 95, "end_pos": 105, "type": "DATASET", "confidence": 0.8965483605861664}]}, {"text": "We also tried B cat,in variation, i.e. training an MT system on the entire data and using in-domain data to train the baseline NNJM.", "labels": [], "entities": [{"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9446115493774414}, {"text": "NNJM", "start_pos": 127, "end_pos": 131, "type": "DATASET", "confidence": 0.9446470141410828}]}, {"text": "The baseline system B cat gave better results and was used as our reference for comparison.", "labels": [], "entities": []}, {"text": "shows the results of our systems, S v1 and S v2 , compared to the baselines, B in and B cat . Unlike Arabic-to-English, the baseline system B in is much worse than B cat . Our adapted MT systems S v1 and S v2 both outperformed the best baseline system (B cat ) with an improvement of up to 0.6 points.", "labels": [], "entities": [{"text": "MT", "start_pos": 184, "end_pos": 186, "type": "TASK", "confidence": 0.9443389177322388}]}, {"text": "S v2 performed slightly better than S v1 on one occasion and slightly worse in others.", "labels": [], "entities": []}, {"text": "Comparison with Data Selection: We also compared our results with the MML-based data.", "labels": [], "entities": [{"text": "MML-based data", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.892348051071167}]}, {"text": "The MML-based baseline systems (B mml ) used 20% selected data for training the MT system and the NNJM.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.8802328109741211}, {"text": "NNJM", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.9812418222427368}]}, {"text": "On Arabic-English, both MML-based selection and our model (S v1 ) gave similar gains on top of the baseline system (B cat ).", "labels": [], "entities": [{"text": "MML-based selection", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.794850081205368}]}, {"text": "Further results showed that both approaches are complementary.", "labels": [], "entities": []}, {"text": "We were able to obtain an average gain of +0.3 BLEU points by training an NDAM v1 model over the selected data (see S v1+mml ).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9990216493606567}]}, {"text": "However, on English-German, the MML-based selection caused a drop in the performance (see).", "labels": [], "entities": []}, {"text": "Training an adapted NDAM v1 model over selected data gave improvements over MML in two test sets but could not restore the baseline performance, probably because the useful data has already been filtered by the selection process.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparing the perplexity of NNJM  and NDAM models. NNJM b represents the model  trained on each individual domain separately.", "labels": [], "entities": []}, {"text": " Table 3: Results of the baseline Arabic-to-English  MT systems. The numbers are averaged over  tst2011-2013.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.7941251993179321}]}, {"text": " Table 4: Arabic-to-English MT Results", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.7921159863471985}]}, {"text": " Table 6: Comparison with Modified Moore-Lewis", "labels": [], "entities": []}, {"text": " Table 6. The  MML-based baseline systems (B mml ) used 20%  selected data for training the MT system and the  NNJM. On Arabic-English, both MML-based se- lection and our model (S v1 ) gave similar gains on  top of the baseline system (B cat ). Further results  showed that both approaches are complementary.  We were able to obtain an average gain of +0.3  BLEU points by training an NDAM v1 model over  the selected data (see S v1+mml ).  However, on English-German, the MML-based  selection caused a drop in the performance (see", "labels": [], "entities": [{"text": "MT", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.8912672400474548}, {"text": "NNJM", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.9779725074768066}, {"text": "BLEU", "start_pos": 358, "end_pos": 362, "type": "METRIC", "confidence": 0.9973913431167603}]}]}