{"title": [{"text": "The Rating Game: Sentiment Rating Reproducibility from Text", "labels": [], "entities": [{"text": "Sentiment Rating Reproducibility", "start_pos": 17, "end_pos": 49, "type": "TASK", "confidence": 0.9589866399765015}]}], "abstractContent": [{"text": "Sentiment analysis models often use ratings as labels, assuming that these ratings reflect the sentiment of the accompanying text.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9248683452606201}]}, {"text": "We investigate (i) whether human readers can infer ratings from review text, (ii) how human performance compares to a regression model, and (iii) whether model performance is affected by the rating \"source\" (i.e. original author vs. annotator).", "labels": [], "entities": []}, {"text": "We collect IMDb movie reviews with author-provided ratings, and have them re-annotated by crowdsourced and trained annotators.", "labels": [], "entities": []}, {"text": "Annotators reproduce the original ratings better than a model, but are still far off in more than 5% of the cases.", "labels": [], "entities": []}, {"text": "Models trained on annotator-labels outperform those trained on author-labels, questioning the usefulness of author-rated reviews as training data for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.9608574509620667}]}], "introductionContent": [{"text": "Machine learning approaches have become the dominant paradigm for sentiment analysis since introduced by.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.9692595601081848}]}, {"text": "While these approaches produce good results, they need to be trained on sufficiently large labeled data sets.", "labels": [], "entities": []}, {"text": "Since human annotation can be both slow and expensive, many studies use data with an inherent subjectivity indicator, such as movie or product reviews with user ratings (.", "labels": [], "entities": []}, {"text": "While it is a fair assumption that the rating expresses the author's attitude towards the subject, it is less obvious to what extent the review text reflects this attitude, and hence what the relation between text and rating is.", "labels": [], "entities": []}, {"text": "In this study, we ask (i) whether readers are able to infer the author's numerical rating based on the author's review text, (ii) how well learning algorithms perform on the task compared to human readers, and (iii) whether model performance is affected by the rating source used for labeling (i.e. how the numerical rating is obtained) . In order to investigate these questions, we compile a data set of user-generated movie reviews with author ratings and collect both crowdsourced annotator ratings and trained annotator ratings.", "labels": [], "entities": []}, {"text": "This setup allows us to evaluate the reproducibility of ratings for both humans and models.", "labels": [], "entities": []}, {"text": "We address (i) by comparing author ratings to crowdsourced and trained annotator ratings.", "labels": [], "entities": []}, {"text": "Author ratings supposedly capture the essence of the author's sentiment, but we do not expect annotators to perfectly reproduce these ratings based on text alone.", "labels": [], "entities": []}, {"text": "We investigate (ii) by evaluating a linear regression model on author-labeled data.", "labels": [], "entities": []}, {"text": "Sentiment analysis models supposedly emulate the cognitive process of text-based rating inference.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9151345193386078}]}, {"text": "The gap between human and model performance is interesting, because if human annotators are unable to consistently infer author ratings, we cannot expect learning algorithms to achieve this goal.", "labels": [], "entities": []}, {"text": "Finally, we address (iii) by comparing regression models trained on data labeled with crowdsourced and author ratings.", "labels": [], "entities": []}, {"text": "Existing work treats both labeling sources as ontologically interchangeable.", "labels": [], "entities": []}, {"text": "That is, it does not matter whether a text was labeled by the author in the process of writing said text, or by an annotator who has been paid to label the text a posteriori.", "labels": [], "entities": []}, {"text": "This is not at all self-evident.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, no previous study has investigated the assumption that the sentiment of a text can be objectively inferred.", "labels": [], "entities": []}, {"text": "Since sentiment analysis is still far from being solved, investigating this core bias can help address current limitations.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.965431272983551}]}], "datasetContent": [{"text": "We want to establish the reproducibility of author ratings from text by human annotators and statistical models.", "labels": [], "entities": []}, {"text": "In order to measure performance of the different methods, we use mean absolute error (MAE) and root mean squared error (RMSE).", "labels": [], "entities": [{"text": "mean absolute error (MAE)", "start_pos": 65, "end_pos": 90, "type": "METRIC", "confidence": 0.9403846263885498}, {"text": "root mean squared error (RMSE)", "start_pos": 95, "end_pos": 125, "type": "METRIC", "confidence": 0.8806300759315491}]}, {"text": "While RMSE is more common, MAE is more directly interpretable, as it does not emphasize outliers.", "labels": [], "entities": []}, {"text": "For this reason, we focus on MAE in our analysis.", "labels": [], "entities": [{"text": "MAE", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.7345855832099915}]}, {"text": "MAE and RMSE measure the proximity between two sets of observations, but we also need a measure of the relative movement between observations.", "labels": [], "entities": [{"text": "MAE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8259504437446594}, {"text": "RMSE", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.8989163041114807}]}, {"text": "For this purpose, we use mainly Spearman's \u03c1, but also report Krippendorff's \u03b1 and Cohen's \u03ba.", "labels": [], "entities": []}, {"text": "The latter is a standard agreement measure, but does notwork as well for ordinal ratings such as these, since it assumes a uniform distribution to compute chance agreement.", "labels": [], "entities": []}, {"text": "We have two sources of human annotations, namely three trained annotators and five crowdsource annotators per review.", "labels": [], "entities": []}, {"text": "In order to obtain our final ratings, we average over each of those annotation sources.", "labels": [], "entities": []}, {"text": "1 This result is more robust towards individual biases and misinterpretations.", "labels": [], "entities": []}, {"text": "This effect is known as wisdom of the crowd and well-documented in the literature, e.g..", "labels": [], "entities": []}, {"text": "However, we also wish to investigate how well individual annotators perform.", "labels": [], "entities": []}, {"text": "Therefore, we also compute error and pairwise correlation for each individual annotator with the authors or other annotators, and then average over the pairwise comparisons for each annotator type.", "labels": [], "entities": [{"text": "error", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.978415846824646}, {"text": "pairwise correlation", "start_pos": 37, "end_pos": 57, "type": "METRIC", "confidence": 0.762752503156662}]}, {"text": "This measure is equivalent to a macro-score and captures the average influence of individual annotators.", "labels": [], "entities": []}, {"text": "When comparing across the two groups of annotators, we use all possible 3x5 combinations.", "labels": [], "entities": []}, {"text": "We use the same measures as outlined above to compare the different annotators to each other within the two groups.", "labels": [], "entities": []}, {"text": "Hence, we compute both MAE, RMSE and correlation calculated between the individual crowdsource and trained annotators, respectively.", "labels": [], "entities": [{"text": "MAE", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9896252751350403}, {"text": "RMSE", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.969996452331543}, {"text": "correlation", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.997401237487793}]}, {"text": "In order to control for different levels variance in the rating distributions, we align the crowdsource annotator and author distribution by sub-sampling.", "labels": [], "entities": []}, {"text": "The number of reviews per rating is determined by the distribution with fewer reviews for the given rating.", "labels": [], "entities": []}, {"text": "The resulting two data sets contain the same number of reviews per rating, and a total of 1,319 reviews.", "labels": [], "entities": []}, {"text": "The main implication of aligning the distributions, is that variance for both distributions will be identical, thus making the comparison more appropriate.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Pairwise comparisons between author  (aut), trained (tr), and crowdsource (cs) ratings.", "labels": [], "entities": []}]}