{"title": [{"text": "Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs", "labels": [], "entities": [{"text": "Improved Transition-Based Parsing", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8687815070152283}]}], "abstractContent": [{"text": "We present extensions to a continuous-state dependency parsing method that makes it applicable to morphologically rich languages.", "labels": [], "entities": [{"text": "continuous-state dependency parsing", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.6876639922459921}]}, {"text": "Starting with a high-performance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the or-thographic representations of the words, also using LSTMs.", "labels": [], "entities": []}, {"text": "This allows statistical sharing across word forms that are similar on the surface.", "labels": [], "entities": []}, {"text": "Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words.", "labels": [], "entities": []}], "introductionContent": [{"text": "At the heart of natural language parsing is the challenge of representing the \"state\" of an algorithmwhat parts of a parse have been built and what parts of the input string are not yet accounted foras it incrementally constructs a parse.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.6648348967234293}]}, {"text": "Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable.", "labels": [], "entities": []}, {"text": "Continuous-state parsers have been proposed, in which the state is embedded as a vector).", "labels": [], "entities": []}, {"text": "Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions.", "labels": [], "entities": []}, {"text": "The primary contribution made in this work is to take the idea of continuous-state parsing a step further by making the word embeddings that are used to construct the parse state sensitive to the morphology of the words.", "labels": [], "entities": [{"text": "continuous-state parsing", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.6515262126922607}]}, {"text": "Since it it is well known that a word's form often provides strong evidence regarding its grammatical role in morphologically rich languages, this has promise to improve accuracy and statistical efficiency relative to traditional approaches that treat each word type as opaque and independently modeled.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9983735084533691}]}, {"text": "In the traditional parameterization, words with similar grammatical roles will only be embedded near each other if they are observed in similar contexts with sufficient frequency.", "labels": [], "entities": []}, {"text": "Our approach reparameterizes word embeddings using the same RNN machinery used in the parser: a word's vector is calculated based on the sequence of orthographic symbols representing it ( \u00a73).", "labels": [], "entities": []}, {"text": "Although our model is provided no supervision in the form of explicit morphological annotation, we find that it gives a large performance increase when parsing morphologically rich languages in the SPMRL datasets, especially in agglutinative languages and the ones that present extensive case systems ( \u00a74).", "labels": [], "entities": [{"text": "SPMRL datasets", "start_pos": 198, "end_pos": 212, "type": "DATASET", "confidence": 0.7673318088054657}]}, {"text": "In languages that show little morphology, performance remains good, showing that the RNN composition strategy is capable of capturing both morphological regularities and arbitrariness in the sense of.", "labels": [], "entities": []}, {"text": "Finally, a particularly noteworthy result is that we find that character-based word embeddings in some cases obviate explicit POS information, which is usually found to be indispensable for accurate parsing.", "labels": [], "entities": []}, {"text": "A secondary contribution of this work is to show that the continuous-state parser of  can learn to generate nonprojective trees.", "labels": [], "entities": []}, {"text": "We do this by augmenting its transition operations with a SWAP operation (Nivre, 2009) ( \u00a72.4), enabling the parser to produce nonprojective dependencies which are often found in morphologically rich languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied our parsing model and several variations of it to several parsing tasks and report re- sults below.", "labels": [], "entities": [{"text": "parsing", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.9735172390937805}]}, {"text": "In order to isolate the improvements provided by the LSTM encodings of characters, we run the stack LSTM parser in the following configurations: \u2022 Words: words only, as in \u00a73.1 (but without POS tags) \u2022 Chars: character-based representations of words with bidirectional LSTMs, as in \u00a73.2 (but without POS tags) \u2022 Words + POS: words and POS tags ( \u00a73.1) \u2022 Chars + POS: character-based representations of words with bidirectional LSTMs plus POS tags ( \u00a73.2) None of the experimental configurations include pretrained word-embeddings or any additional data resources.", "labels": [], "entities": []}, {"text": "All experiments include the SWAP transition, meaning that nonprojective trees can be produced in any language.", "labels": [], "entities": []}, {"text": "The full version of our parsing model sets dimensionalities as follows.", "labels": [], "entities": []}, {"text": "LSTM hidden states are of size 100, and we use two layers of LSTMs for each stack.", "labels": [], "entities": []}, {"text": "Embeddings of the parser actions used in the composition functions have 20 dimensions, and the output embedding size is 20 dimensions.", "labels": [], "entities": []}, {"text": "The learned word representations embeddings have 32 dimensions when used, while the character-based representations have The POS tags are predicted by using the Stanford Tagger () with an accuracy of 97.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.9981662631034851}]}], "tableCaptions": [{"text": " Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.", "labels": [], "entities": []}, {"text": " Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.", "labels": [], "entities": []}, {"text": " Table 3: Test-set performance of our best results (according to UAS or LAS, whichever has the larger  difference), compared to state-of-the-art greedy transition-based parsers (\"Best Greedy Result\") and best  results reported (\"Best Published Result\"). All of the systems we compare against use explicit mor- phological features and/or one of the following: pretrained word embeddings, unlabeled data and a  combination of parsers; our models do not. B'13 is Ballesteros (2013); N+'06a is Nivre et al. (2006a);  D+'15 is Dyer et al. (2015); B+'13 is Bj\u00f6rkelund et al. (2013); B+'14 is Bj\u00f6rkelund et al. (2014); K+'10  is Koo et al. (2010); W+'15 is Weiss et al. (2015).", "labels": [], "entities": [{"text": "UAS", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8884526491165161}]}]}