{"title": [{"text": "Shallow Convolutional Neural Network for Implicit Discourse Relation Recognition", "labels": [], "entities": [{"text": "Implicit Discourse Relation Recognition", "start_pos": 41, "end_pos": 80, "type": "TASK", "confidence": 0.7446976006031036}]}], "abstractContent": [{"text": "Implicit discourse relation recognition remains a serious challenge due to the absence of discourse connectives.", "labels": [], "entities": [{"text": "Implicit discourse relation recognition", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7753325700759888}]}, {"text": "In this paper , we propose a Shallow Convolutional Neural Network (SCNN) for implicit discourse relation recognition, which contains only one hidden layer but is effective in relation recognition.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 77, "end_pos": 116, "type": "TASK", "confidence": 0.5964915379881859}, {"text": "relation recognition", "start_pos": 175, "end_pos": 195, "type": "TASK", "confidence": 0.9067745506763458}]}, {"text": "The shallow structure alleviates the overfitting problem , while the convolution and nonlinear operations help preserve the recognition and generalization ability of our model.", "labels": [], "entities": []}, {"text": "Experiments on the benchmark data set show that our model achieves comparable and even better performance when comparing against current state-of-the-art systems .", "labels": [], "entities": [{"text": "benchmark data set", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.8248660763104757}]}], "introductionContent": [{"text": "As a crucial task for discourse analysis, discourse relation recognition (DRR) aims to automatically identify the internal structure and logical relationship of coherent text (e.g., TEMPORAL, CONTIN-GENCY,.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7146805077791214}, {"text": "discourse relation recognition (DRR)", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.803529421488444}]}, {"text": "It provides important information to many other natural language processing systems, such as question answering, information extraction), machine translation) and soon.", "labels": [], "entities": [{"text": "question answering", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.8575833141803741}, {"text": "information extraction", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.7670900523662567}, {"text": "machine translation", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.7839828431606293}]}, {"text": "Despite great progress in explicit DRR where the discourse connectives (e.g., \"because\", \"but\" et al.) explicitly exist in the text (, implicit DRR remains a serious challenge because of the absence of discourse connectives (.", "labels": [], "entities": []}, {"text": "Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and * Corresponding author production rules.", "labels": [], "entities": [{"text": "implicit DRR", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.6332996189594269}]}, {"text": "Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (, thus limiting the effectiveness of these methods.", "labels": [], "entities": []}, {"text": "Recently, deep learning models have achieved remarkable results in natural language processing (.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.6529730459054311}]}, {"text": "However, to the best of our knowledge, there is little deep learning work specifically for implicit DRR.", "labels": [], "entities": [{"text": "implicit DRR", "start_pos": 91, "end_pos": 103, "type": "TASK", "confidence": 0.48389412462711334}]}, {"text": "The neglect of this important domain maybe due to the following two reasons: (1) discourse relation distribution is rather unbalanced, where the generalization of deep models is relatively insufficient despite their powerful studying ability; (2) training dataset in implicit DRR is relatively small, where overfitting problems become more prominent.", "labels": [], "entities": []}, {"text": "In this paper, we propose a Shallow Convolutional Neural Network (SCNN) for implicit DRR, with only one simple convolution layer on the top of word vectors.", "labels": [], "entities": []}, {"text": "On one hand, the network structure is simple, thereby overfitting issue can be alleviated; on the other hand, the convolution operation and nonlinear transformation help preserve the recognition ability of SCNN.", "labels": [], "entities": []}, {"text": "This makes our model able to generalize better on the test dataset.", "labels": [], "entities": []}, {"text": "We performed evaluation for English implicit DRR on the PDTB-style corpus.", "labels": [], "entities": [{"text": "English implicit DRR", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.5357043743133545}, {"text": "PDTB-style corpus", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.9670450091362}]}, {"text": "Experimental results show that the proposed method can obtain comparable even better performance when compares against several baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted a series of experiments on English implicit DRR task.", "labels": [], "entities": [{"text": "English implicit DRR task", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.4337750971317291}]}, {"text": "After a brief description of the experimental setup and the baseline systems, we further investigated the effectiveness of our method with deep analysis.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of positive and negative in- stances in training (Train), development (Dev)  and test (Test) sets.  COMP.=COMPARISON,  CONT.=CONTINGENCY, EXP.=EXPANSION and  TEMP.=TEMPORAL", "labels": [], "entities": [{"text": "COMP.", "start_pos": 121, "end_pos": 126, "type": "METRIC", "confidence": 0.7688949704170227}, {"text": "TEMP.", "start_pos": 179, "end_pos": 184, "type": "METRIC", "confidence": 0.9874739646911621}, {"text": "TEMPORAL", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9064874053001404}]}, {"text": " Table 2: Performance comparison of different systems on the test set.", "labels": [], "entities": []}]}