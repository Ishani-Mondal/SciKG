{"title": [{"text": "A Computational Cognitive Model of Novel Word Generalization", "labels": [], "entities": [{"text": "Novel Word Generalization", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.6886175274848938}]}], "abstractContent": [{"text": "A key challenge in vocabulary acquisition is learning which of the many possible meanings is appropriate fora word.", "labels": [], "entities": [{"text": "vocabulary acquisition", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.8073675036430359}]}, {"text": "The word generalization problem refers to how children associate a word such as dog with a meaning at the appropriate category level in a taxonomy of objects, such as Dalma-tians, dogs, or animals.", "labels": [], "entities": [{"text": "word generalization", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6684394031763077}]}, {"text": "We present the first computational study of word generalization integrated within a word-learning model.", "labels": [], "entities": [{"text": "word generalization", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8206123411655426}]}, {"text": "The model simulates child and adult patterns of word generalization in a word-learning task.", "labels": [], "entities": [{"text": "word generalization", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7312166690826416}]}, {"text": "These patterns arise due to the interaction of type and token frequencies in the input data, an influence often observed in people's generalization of linguistic categories.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning word meanings is a challenging early step in child language acquisition.", "labels": [], "entities": [{"text": "Learning word meanings", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7308197220166525}, {"text": "child language acquisition", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.6447434822718302}]}, {"text": "Imagine a child hears the word dax for the first time while observing a white rabbit jumping around -dax might mean WHITE RABBIT, RABBIT, ANIMAL, CUTE, LOOK, etc..", "labels": [], "entities": [{"text": "WHITE RABBIT", "start_pos": 116, "end_pos": 128, "type": "METRIC", "confidence": 0.857801616191864}, {"text": "RABBIT", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.8568324446678162}, {"text": "ANIMAL", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.967427134513855}, {"text": "CUTE", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.725455641746521}, {"text": "LOOK", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.8945139050483704}]}, {"text": "How does the child learn the correct meaning of a word from a large pool of potential meanings?", "labels": [], "entities": []}, {"text": "A possible explanation is that children infer a word's meaning by identifying the commonalities across the situations in which the word occurs.", "labels": [], "entities": []}, {"text": "One mechanism for achieving this is crosssituational learning (e.g.,.", "labels": [], "entities": []}, {"text": "Recent word learning experiments confirm that both adults and children infer the correct word-meaning mappings by keeping track of cross-situational statistics across individually ambiguous learning trials ().", "labels": [], "entities": []}, {"text": "Although cross-situational learning is a general mechanism for narrowing down the meaning of a word, it does not explain how children overcome an interesting challenge in word learning: determining the correct level of a hierarchical taxonomy that a word refers to.", "labels": [], "entities": []}, {"text": "For example, children learn that the word dog refers to all kinds of dogs, and not to a specific breed, such as Dalmatians, or to a more general category, such as animalseven though some of these choices (e.g., animals) are compatible with all the cross-situational evidence available for dog (because all dogs are also animals).", "labels": [], "entities": []}, {"text": "We use the term \"word generalization\" to refer to this problem of associating a word with the meaning at an appropriate category level, given some sample of experiences with the word.", "labels": [], "entities": [{"text": "word generalization", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.713713601231575}]}, {"text": "Previous research has argued that children use a specific bias or constraint -the basic-level assumption -to focus their word generalizations appropriately.", "labels": [], "entities": [{"text": "focus their word generalizations", "start_pos": 109, "end_pos": 141, "type": "TASK", "confidence": 0.6163100004196167}]}, {"text": "According to this bias, children prefer to associate a word to a set of objects that form a basic-level category, such as dogs or trucks, and that share a significant number of attributes.", "labels": [], "entities": []}, {"text": "It is less preferred to associate anew word to much more specific subordinate categories, such as Dalmatians or bulldozers, or to more general superordinate ones, like animals or vehicles, whose members share fewer attributes.", "labels": [], "entities": []}, {"text": "It remains an important open question of whether a word learner requires such a bias to acquire appropriate mappings.", "labels": [], "entities": []}, {"text": "(X&T henceforth) studied the word generalization problem in a set of experiments in which children and adults were asked to determine which level of a taxonomy a novel word referred to.", "labels": [], "entities": [{"text": "word generalization problem", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.8004790842533112}]}, {"text": "X&T further examined this behavioral data through computational modelling.", "labels": [], "entities": []}, {"text": "They proposed a Bayesian model that, given a few exemplars of a novel word, matches human behaviour in how it maps the word to its meanings in a taxonomic category.", "labels": [], "entities": []}, {"text": "The Bayesian model of X&T is important in providing insight into how people might reason about samples of data that exemplify categories.", "labels": [], "entities": [{"text": "X&T", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.7050127983093262}]}, {"text": "However, it relies on having complete, built-in knowledge about the taxonomic hierarchy, including both the detailed composition of categories and the values for between-object similarities, drawn from adult similarity judgments.", "labels": [], "entities": []}, {"text": "Furthermore, the X&T model does not address the issue of word generalization in the broader context of word learning: While their model reasons over samples of data associated with a word label, it does not develop a meaning representation of the word overtime, as a child must do.", "labels": [], "entities": [{"text": "word generalization", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7417292296886444}]}, {"text": "It is important to understand how word generalization occurs when embedded in the natural process of learning a word meaning and in the context of more limited category knowledge.", "labels": [], "entities": [{"text": "word generalization", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7486478388309479}]}, {"text": "We address these issues by providing a unified account of word learning and word generalization within a computational model of crosssituational learning.", "labels": [], "entities": [{"text": "word learning", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.7819217443466187}, {"text": "word generalization", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7963056564331055}]}, {"text": "Unlike the X&T model, our model is an incremental learner that gradually acquires the meaning of words, and uses these developing meanings in determining the appropriate extension of a word to elements of a taxonomy.", "labels": [], "entities": []}, {"text": "Our model has general knowledge of category structure without having an elaborated taxonomy encoding known object similarities.", "labels": [], "entities": []}, {"text": "Moreover, in the absence of any bias toward generalization to particular kinds of categories, the model exhibits the observed \"basic-level bias\" due to general mechanisms of productivity that have been proposed to apply to many aspects of linguistic knowledge (e.g.,.", "labels": [], "entities": []}, {"text": "In what follows, we first describe the human experiments of X&T, and then present our computational model and the experiments that simulate the X&T data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We model X&T's behavioural experiments with our computational word learner as extended above.", "labels": [], "entities": []}, {"text": "Following X&T, we use a three-tiered category hierarchy, and the four training conditions and assessment of three types of test matches as described in.", "labels": [], "entities": []}, {"text": "In each condition, the model processes a sequence of 3 utterance-scene pairs, and updates Pt (f i |w j ) after each pair using Eqns.", "labels": [], "entities": []}, {"text": "(5) and (6).", "labels": [], "entities": []}, {"text": "The utterance-scene pair in each trial consists of the novel word coupled with the scene representation of a training object from the category hierarchy.", "labels": [], "entities": []}, {"text": "The object's scene representation is given as a set of four features, each taken from one of four feature groups: one feature corresponding to each of the subordinate, basic, and superordinate levels of the hierarchy, and a unique \"instance\" feature, as shown in  Testing the model.", "labels": [], "entities": []}, {"text": "After training on a novel word, in order to assess its level of generalization within the category hierarchy, we compare the model's learned meaning of the word to test objects that constitute various types of matches to the training conditions: i.e., subordinate matches, basic-level matches, and superordinate matches.", "labels": [], "entities": []}, {"text": "gives an example of each type of match:  To assess whether the model generalizes the learned meaning of a word w to the various types of test matches, we first consider the probability of a test object Y at time t given the learned meaning of w: where y i are the features in Y , and Pt (y i |w) is calculated using Eqn.", "labels": [], "entities": [{"text": "Pt", "start_pos": 284, "end_pos": 286, "type": "METRIC", "confidence": 0.9562541842460632}]}, {"text": "(5) for features y i observed with w during training, and using Eqn.", "labels": [], "entities": []}, {"text": "(6) for y i not observed with w.", "labels": [], "entities": []}, {"text": "(5) reduces to Eqn.", "labels": [], "entities": [{"text": "Eqn", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.8934664130210876}]}, {"text": "(6) when a feature has not been seen with the word.)", "labels": [], "entities": []}, {"text": "From Pt (Y |w), we subtract the predictive probability of the test object before the model has observed any data, P 0 (Y |w), which gives us its increase in preference attributable to the word learning trials.", "labels": [], "entities": []}, {"text": "Calculating Pt (Y |w) \u2212 P 0 (Y |w) is informative about one test object, but we need to measure generalization of the learned word to all the objects of a certain type of match -i.e., subordinate, basiclevel, or superordinate.", "labels": [], "entities": []}, {"text": "We formulate the probability of generalization to a type of test match as the relative average increase in preference for test items of that type of match, using the ShepardLuce choice rule: where m is the set of test objects at a certain level of match, and m ranges over subordinate matches, basic-level matches, and superordinate matches.", "labels": [], "entities": []}, {"text": "Using P gen (m|w) to communicate our models results has the advantage of using the learned word meanings in a very direct way to assess the preference for the various types of test matches in the X&T experiments.", "labels": [], "entities": []}, {"text": "However, the disadvantage is that this measure is not directly comparable to the reported figures from the human data, which are the percentage of test objects selected of a particular type of match.", "labels": [], "entities": []}, {"text": "Hence, in presenting our results below, we focus on the general patterns of preferences indicated by the different measures.", "labels": [], "entities": []}, {"text": "To model children, whom we assume to have no bias towards generalization to specific category levels, we equate all parameters k G and \u03b3 G across all feature groups, reflecting that all category levels are treated equivalently.", "labels": [], "entities": []}, {"text": "Here we use values of k G = 100 and initial values of \u03b3 G = 0.5 for all G as the \"child\" parameter settings.", "labels": [], "entities": []}, {"text": "In contrast, we assume that adults, through word learning experience, have accumulated biases that reflect observed differences in feature groups.", "labels": [], "entities": []}, {"text": "More specifically, we assume that the probability of observing anew feature fora group G depends on the degree of specificity of that group: That is, overtime, it is less likely to observe a completely new kind of animal, e.g., than anew breed of dog.", "labels": [], "entities": []}, {"text": "We simulate these biases by us- is the prior probability of any object instance, given parameters drawn from the Dirichlet prior, because Eqn.", "labels": [], "entities": []}, {"text": "(6) yields the value 1 k G when all assoct scores are 0 -i.e., no features from G have been observed with the word.", "labels": [], "entities": []}, {"text": "To determine the parameters for the \"child\" learner, we examined a number of settings with equal parameter values for all the feature groups, and observed similar results in these settings.", "labels": [], "entities": []}, {"text": "(We did not perform an exhaustive search over the parameter space.) ing various values for the parameter \u03b3 G , which determines the prior probability of a word being observed with new (previously unseen) features in G (cf. Eqn.).", "labels": [], "entities": []}, {"text": "We assume that the expected number of features (k G ) is the same across groups.", "labels": [], "entities": []}, {"text": "We perform a non-exhaustive search on the parameter space of \u03b3 G to select a set of values that yield the patterns of X&T's adult experiments.", "labels": [], "entities": []}, {"text": "The \"adult\" parameter values are given in  We present results of the model using both child settings () and adult settings.", "labels": [], "entities": []}, {"text": "Recall that these values do not correspond to the percentages reported in the human data; to evaluate the patterns of generalization, we look at the relative preference for the various types of test match.", "labels": [], "entities": []}, {"text": "Note also that since the generalization probabilities sum to 1.0 within each of the 4 training conditions, we can only compare the pattern of generalization across conditions (and not the actual value of the probabilities).", "labels": [], "entities": []}, {"text": "We discuss each of the child and adult sets of results in detail below.", "labels": [], "entities": []}], "tableCaptions": []}