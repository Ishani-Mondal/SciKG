{"title": [{"text": "A Dynamic Programming Algorithm for Computing N-gram Posteriors from Lattices", "labels": [], "entities": []}], "abstractContent": [{"text": "Efficient computation of n-gram posterior probabilities from lattices has applications in lattice-based minimum Bayes-risk decoding in statistical machine translation and the estimation of expected document frequencies from spoken corpora.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 135, "end_pos": 166, "type": "TASK", "confidence": 0.7014553348223368}]}, {"text": "In this paper, we present an algorithm for computing the posterior probabilities of all n-grams in a lattice and constructing a minimal deterministic weighted finite-state automaton associating each n-gram with its posterior for efficient storage and retrieval.", "labels": [], "entities": []}, {"text": "Our algorithm builds upon the best known algorithm in literature for computing n-gram posteriors from lattices and leverages the following observations to significantly improve the time and space requirements: i) the n-grams for which the posteriors will be computed typically comprises all n-grams in the lattice up to a certain length, ii) posterior is equivalent to expected count for an n-gram that do not repeat on any path, iii) there are efficient algorithms for computing n-gram expected counts from lattices.", "labels": [], "entities": []}, {"text": "We present experimental results comparing our algorithm with the best known algorithm in literature as well as a baseline algorithm based on weighted finite-state automata operations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many complex speech and natural language processing (NLP) pipelines such as Automatic Speech Recognition (ASR) and Statistical Machine Translation (SMT) systems store alternative hypotheses produced at various stages of processing as weighted acyclic automata, also known as lattices.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 76, "end_pos": 110, "type": "TASK", "confidence": 0.7876371542612711}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 115, "end_pos": 152, "type": "TASK", "confidence": 0.7774500548839569}]}, {"text": "Each lattice stores a large number of hypotheses along with the raw system scores assigned to them.", "labels": [], "entities": []}, {"text": "While single-best hypothesis is typically what is desired at the end of the processing, it is often beneficial to consider a large number of weighted hypotheses at earlier stages of the pipeline to hedge against errors introduced by various subcomponents.", "labels": [], "entities": []}, {"text": "Standard ASR and SMT techniques like discriminative training, rescoring with complex models and Minimum Bayes-Risk (MBR) decoding rely on lattices to represent intermediate system hypotheses that will be further processed to improve models or system output.", "labels": [], "entities": [{"text": "ASR", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9818516969680786}, {"text": "SMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9761744737625122}]}, {"text": "For instance, lattice based MBR decoding has been shown to give moderate yet consistent gains in performance over conventional MAP decoding in a number of speech and NLP applications including ASR) and SMT ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 193, "end_pos": 196, "type": "TASK", "confidence": 0.9513646364212036}, {"text": "SMT", "start_pos": 202, "end_pos": 205, "type": "TASK", "confidence": 0.9861220717430115}]}, {"text": "Most lattice-based techniques employed by speech and NLP systems make use of posterior quantities computed from probabilistic lattices.", "labels": [], "entities": []}, {"text": "In this paper, we are interested in two such posterior quantities: i) n-gram expected count, the expected number of occurrences of a particular n-gram in a lattice, and ii) n-gram posterior probability, the total probability of accepting paths that include a particular n-gram.", "labels": [], "entities": []}, {"text": "Expected counts have applications in the estimation of language model statistics from probabilistic input such as ASR lattices) and the estimation term frequencies from spoken corpora while posterior probabilities come up in MBR decoding of SMT lattices (, relevance ranking of spoken utterances and the estimation of document frequencies from spoken corpora.", "labels": [], "entities": [{"text": "SMT lattices", "start_pos": 241, "end_pos": 253, "type": "TASK", "confidence": 0.7609962224960327}]}, {"text": "The expected count c(x|A) of n-gram x given lattice A is defined as where # y (x) is the number of occurrences of n-gram x in hypothesis y and p(y|A) is the posterior probability of hypothesis y given lattice A.", "labels": [], "entities": []}, {"text": "Similarly, the posterior probability p(x|A) of n-gram x given lattice A is defined as where 1 y (x) is an indicator function taking the value 1 when hypothesis y includes n-gram x and 0 otherwise.", "labels": [], "entities": []}, {"text": "While it is straightforward to compute these posterior quantities from weighted nbest lists by examining each hypothesis separately and keeping a separate accumulator for each observed n-gram type, it is infeasible to do the same with lattices due to the sheer number of hypotheses stored.", "labels": [], "entities": []}, {"text": "There are efficient algorithms in literature () for computing n-gram expected counts from weighted automata that rely on weighted finite state transducer operations to reduce the computation to a sum over n-gram occurrences eliminating the need for an explicit sum over accepting paths.", "labels": [], "entities": []}, {"text": "The rather innocent looking difference between Equations 1 and 2, # y (x) vs. 1 y (x), makes it hard to develop similar algorithms for computing n-gram posteriors from weighted automata since the summation of probabilities has to be carried out over paths rather than n-gram occurrences ().", "labels": [], "entities": []}, {"text": "The problem of computing n-gram posteriors from lattices has been addressed by a number of recent works) in the context of lattice-based MBR for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 145, "end_pos": 148, "type": "TASK", "confidence": 0.9885197281837463}]}, {"text": "In these works, it has been reported that the time required for lattice MBR decoding is dominated by the time required for computing n-gram posteriors.", "labels": [], "entities": []}, {"text": "Our interest in computing n-gram posteriors from lattices stems from its potential applications in spoken content retrieval (.", "labels": [], "entities": [{"text": "spoken content retrieval", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.6399517158667246}]}, {"text": "Computation of document frequency statistics from spoken corpora relies on estimating ngram posteriors from ASR lattices.", "labels": [], "entities": []}, {"text": "In this context, a spoken document is simply a collection of ASR lattices.", "labels": [], "entities": []}, {"text": "The n-grams of interest can be word, syllable, morph or phoneme sequences.", "labels": [], "entities": []}, {"text": "Unlike in the case of lattice-based MBR for SMT where the n-grams of interest are relatively short -typically up to 4-grams -, the n-grams we are interested in are in many instances relatively long sequences of subword units.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9873433113098145}]}, {"text": "In this paper, we present an efficient algorithm for computing the posterior probabilities of all ngrams in a lattice and constructing a minimal deterministic weighted finite-state automaton associating each n-gram with its posterior for efficient storage and retrieval.", "labels": [], "entities": []}, {"text": "Our n-gram posterior computation algorithm builds upon the custom forward procedure described in and introduces a number of refinements to significantly improve the time and space requirements: \u2022 The custom forward procedure described in) computes unigram posteriors from an input lattice.", "labels": [], "entities": []}, {"text": "Higher order n-gram posteriors are computed by first transducing the input lattice to an n-gram lattice using an order mapping transducer and then running the custom forward procedure on this higher order lattice.", "labels": [], "entities": []}, {"text": "We reformulate the custom forward procedure as a dynamic programming algorithm that computes posteriors for successively longer n-grams and reuses the forward scores computed for the previous order.", "labels": [], "entities": []}, {"text": "This reformulation subsumes the transduction of input lattices to n-gram lattices and obviates the need for constructing and applying order mapping transducers.", "labels": [], "entities": []}, {"text": "2, we can observe that posterior probability and expected count are equivalent for an n-gram that do not repeat on any path of the input lattice.", "labels": [], "entities": [{"text": "expected count", "start_pos": 49, "end_pos": 63, "type": "METRIC", "confidence": 0.8058886528015137}]}, {"text": "The key idea behind our algorithm is to limit the costly posterior computation to only those ngrams that can potentially repeat on some path of the input lattice.", "labels": [], "entities": []}, {"text": "We keep track of repeating n-grams of order n and use a simple impossibility argument to significantly reduce the number of n-grams of order n + 1 for which posterior computation will be performed.", "labels": [], "entities": []}, {"text": "The posteriors for the remaining n-grams are replaced with expected counts.", "labels": [], "entities": [{"text": "posteriors", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9594278931617737}]}, {"text": "This filtering of n-grams introduces a slight bookkeeping overhead but in return dramatically reduces the runtime and memory requirements for long n-grams.", "labels": [], "entities": []}, {"text": "\u2022 We store the posteriors for n-grams that can potentially repeat on some path of the input lattice in a weighted prefix tree that we construct on the fly.", "labels": [], "entities": []}, {"text": "Once that is done, we com- pute the expected counts for all n-grams in the input lattice and represent them as a minimal deterministic weighted finite-state automaton, known as a factor automaton (, using the approach described in).", "labels": [], "entities": []}, {"text": "Finally we use general weighted automata algorithms to merge the weighted factor automaton representing expected counts with the weighted prefix tree representing posteriors to obtain a weighted factor automaton representing posteriors that can be used for efficient storage and retrieval.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we provide experiments comparing the performance of Algorithm 2 with Algorithm 1 as well as a baseline algorithm based on the approach of.", "labels": [], "entities": []}, {"text": "All algorithms were implemented in C++ using the OpenFst Library (.", "labels": [], "entities": [{"text": "OpenFst Library", "start_pos": 49, "end_pos": 64, "type": "DATASET", "confidence": 0.898416668176651}]}, {"text": "Algorithm 1 implementation is a thin wrapper around the reference implementation.", "labels": [], "entities": []}, {"text": "All experiments were conducted on the 88K ASR lattices (total size: #states + #arcs = 33M, disk size: 481MB) generated from the training subset of the IARPA Babel Turkish language pack, which includes 80 hours of conversational telephone speech.", "labels": [], "entities": [{"text": "IARPA Babel Turkish language pack", "start_pos": 151, "end_pos": 184, "type": "DATASET", "confidence": 0.8943837761878968}]}, {"text": "Lattices were generated with a speaker dependent DNN ASR system that was trained on the same data set using IBM's Attila toolkit ().", "labels": [], "entities": [{"text": "Attila toolkit", "start_pos": 114, "end_pos": 128, "type": "DATASET", "confidence": 0.8374229967594147}]}, {"text": "All lattices were pruned to a logarithmic beam width of 5.", "labels": [], "entities": []}, {"text": "gives a scatter plot of the posterior probability computation time vs. the number of lattice n-grams (up to 5-grams) where each point represents one of the 88K lattices in our data set.", "labels": [], "entities": []}, {"text": "Similarly, gives a scatter plot of the maximum memory used by the program (maximum resident set size) during the computation of posteriors vs. the number of lattice n-grams (up to 5-grams).", "labels": [], "entities": []}, {"text": "Algorithm 2 requires significantly less resources, particularly in the case of larger lattices with a large number of unique n-grams.", "labels": [], "entities": []}, {"text": "To better understand the runtime characteristics of Algorithms 1 and 2, we conducted a small experiment where we randomly selected 100 lattices (total size: #states + #arcs = 81K, disk size: 1.2MB) from our data set and analyzed the relation between the runtime and the maximum ngram length N . gives a runtime comparison between the baseline posterior computation algorithm described in (), Algorithm 1, Algorithm 2 and the expected count computation algorithm of).", "labels": [], "entities": []}, {"text": "The baseline method computes posteriors separately for each n-gram by intersecting the lattice with an automaton accepting only the paths including that n-gram and computing the total weight of the resulting automaton in log semiring.", "labels": [], "entities": []}, {"text": "Runtime complexities of the baseline method and Algorithm 1 are exponential in N due to the explicit enumeration of n-grams and we can clearly see this trend in the 3rd and 4th rows of.", "labels": [], "entities": [{"text": "Algorithm", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.964474081993103}]}, {"text": "Algorithm 2 (5th row) takes advantage of the WFSA based expected count computation algorithm (6th row) to do most of the work for long n-grams, hence does not suffer from the same exponential growth.", "labels": [], "entities": [{"text": "WFSA", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.8098593354225159}]}, {"text": "Notice the drops in the runtimes of Algorithm 2 and the WFSA based expected count computation algorithm when all n-grams are included into the computation regardless of their length.", "labels": [], "entities": [{"text": "WFSA", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.8473794460296631}]}, {"text": "These drops are due to the expected count computation algorithm that processes all n-grams simultaneously using WFSA operations.", "labels": [], "entities": []}, {"text": "Limiting the maximum n-gram length requires pruning long ngrams, which in general can increase the sizes of intermediate WFSAs used in computation and result in longer runtimes as well as larger outputs.", "labels": [], "entities": []}, {"text": "When there is no limit on the maximum n-gram length, the output of Algorithm 2 is a weighted factor automaton mapping each factor to its posterior.", "labels": [], "entities": []}, {"text": "compares the construction and storage requirements for posterior factor automata with similar factor automata structures.", "labels": [], "entities": []}, {"text": "We use the approach described in) for constructing both the unweighted and the expected count factor automata.", "labels": [], "entities": []}, {"text": "We construct the unweighted factor automata by first removing the weights on the input lattices and then applying the determinization operation on the tropical semiring so that path weights are not added together.", "labels": [], "entities": []}, {"text": "The storage requirements of the posterior factor automata produced by Algorithm 2 is similar to those of the expected count factor automata.", "labels": [], "entities": []}, {"text": "Unweighted factor automata, on the other hand, are significantly more compact than their weighted counterparts even though they accept the same set of strings.", "labels": [], "entities": []}, {"text": "This difference in size is due to accommodating path weights which in general can significantly impact the effectiveness of automata determinization and minimization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Algo- rithm 2 (5th row) takes advantage of the WFSA  based expected count computation algorithm (6th  row) to do most of the work for long n-grams,  hence does not suffer from the same exponential  growth. Notice the drops in the runtimes of Algo- rithm 2 and the WFSA based expected count com- putation algorithm when all n-grams are included  into the computation regardless of their length.  These drops are due to the expected count compu- tation algorithm that processes all n-grams simul- taneously using WFSA operations. Limiting the  maximum n-gram length requires pruning long n- grams, which in general can increase the sizes of", "labels": [], "entities": [{"text": "WFSA", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.9248864054679871}]}, {"text": " Table 3: Factor Automata Comparison", "labels": [], "entities": []}]}