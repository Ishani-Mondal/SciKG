{"title": [{"text": "Improving Semantic Parsing with Enriched Synchronous Context-Free Grammar", "labels": [], "entities": [{"text": "Improving Semantic Parsing", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8707497318585714}]}], "abstractContent": [{"text": "Semantic parsing maps a sentence in natural language into a structured meaning representation.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8236002027988434}]}, {"text": "Previous studies show that semantic parsing with synchronous context-free grammars (SCFGs) achieves favorable performance over most other alternatives.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.8211658895015717}]}, {"text": "Motivated by the observation that the performance of semantic parsing with SCFGs is closely tied to the translation rules, this paper explores extending translation rules with high quality and increased coverage in three ways.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.7173386514186859}]}, {"text": "First, we introduce structure informed non-terminals, better guiding the parsing in favor of well formed structure, instead of using a uninformed non-terminal in SCFGs.", "labels": [], "entities": [{"text": "parsing", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.965876579284668}]}, {"text": "Second, we examine the difference between word alignments for semantic parsing and statistical machine translation (SMT) to better adapt word alignment in SMT to semantic parsing.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7284073829650879}, {"text": "semantic parsing", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.724220409989357}, {"text": "statistical machine translation (SMT)", "start_pos": 83, "end_pos": 120, "type": "TASK", "confidence": 0.7713457892338434}, {"text": "word alignment", "start_pos": 137, "end_pos": 151, "type": "TASK", "confidence": 0.7049067318439484}, {"text": "SMT", "start_pos": 155, "end_pos": 158, "type": "TASK", "confidence": 0.8338487148284912}, {"text": "semantic parsing", "start_pos": 162, "end_pos": 178, "type": "TASK", "confidence": 0.7123594880104065}]}, {"text": "Finally, we address the unknown word translation issue via synthetic translation rules.", "labels": [], "entities": [{"text": "unknown word translation", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.6390127738316854}]}, {"text": "Evaluation on the standard GeoQuery benchmark dataset shows that our approach achieves the state-of-the-art across various languages, including English, German and Greek.", "labels": [], "entities": [{"text": "GeoQuery benchmark dataset", "start_pos": 27, "end_pos": 53, "type": "DATASET", "confidence": 0.8830698132514954}]}], "introductionContent": [{"text": "Semantic parsing, the task of mapping natural language (NL) sentences into a formal meaning representation language (MRL), has recently received a significant amount of attention with various models proposed over the past few years.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8646493256092072}, {"text": "mapping natural language (NL) sentences into a formal meaning representation language (MRL)", "start_pos": 30, "end_pos": 121, "type": "TASK", "confidence": 0.6172790881246328}]}, {"text": "Consider the NL sentence paired with its corresponding MRL in  naturally viewed as a statistical machine translation (SMT) task, which translates a sentence in NL (i.e., the source language in SMT) into its meaning representation in MRL (i.e., the target language in SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT) task", "start_pos": 85, "end_pos": 127, "type": "TASK", "confidence": 0.7968365507466453}]}, {"text": "Indeed, many attempts have been made to directly apply statistical machine translation (SMT) systems (or methodologies) to semantic parsing (;).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 55, "end_pos": 92, "type": "TASK", "confidence": 0.7757059832413992}, {"text": "semantic parsing", "start_pos": 123, "end_pos": 139, "type": "TASK", "confidence": 0.7500408589839935}]}, {"text": "However, although recent studies ( show that semantic parsing with SCFGs, which form the basis of most existing statistical syntax-based translation models, achieves favorable results, this approach is still behind the most recent state-of-the-art.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7908766865730286}, {"text": "statistical syntax-based translation", "start_pos": 112, "end_pos": 148, "type": "TASK", "confidence": 0.6164940794308981}]}, {"text": "For details, please see performance comparison in and.", "labels": [], "entities": []}, {"text": "The key issues behind the limited success of applying SMT systems directly to semantic parsing lie in the difference between semantic parsing and SMT: MRL is not areal natural language with different properties from natural language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9897640943527222}, {"text": "semantic parsing", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.7186135053634644}, {"text": "semantic parsing", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.7549068629741669}, {"text": "SMT", "start_pos": 146, "end_pos": 149, "type": "TASK", "confidence": 0.9716107249259949}]}, {"text": "First, MRL is machine-interpretable and thus strictly structured with the meaning representation in a nested structure of functions and arguments.", "labels": [], "entities": [{"text": "MRL", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9175333976745605}]}, {"text": "Second, the two languages are intrinsically asymmetric since each token in MRL carries specific mean-ing 1 while this does not hold in NL since auxiliary words and some function words usually have no counterparts in MRL.", "labels": [], "entities": []}, {"text": "Third and finally, the expressions in NL are more flexible with respect to lexicon selection and token ordering.", "labels": [], "entities": [{"text": "token ordering", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.7448552250862122}]}, {"text": "For example, since sentences in NL 'could you tell me the states that utah borders', 'what states does utah border', and 'utah borders what states' convey the same meaning, they should have the same expression in MRL.", "labels": [], "entities": [{"text": "MRL", "start_pos": 213, "end_pos": 216, "type": "DATASET", "confidence": 0.7142074704170227}]}, {"text": "Motivated by the above observations, we believe that semantic parsing with standard SMT components is not an ideal approach.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.870657205581665}, {"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9637324810028076}]}, {"text": "Alternatively, this paper proposes an effective, yet simple way to enrich SCFG in hierarchical phrase-based SMT for better semantic parsing.", "labels": [], "entities": [{"text": "SMT", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.747890293598175}, {"text": "semantic parsing", "start_pos": 123, "end_pos": 139, "type": "TASK", "confidence": 0.7034721076488495}]}, {"text": "Specifically, since the translation rules play a critical role in SMT, we explore to improve translation rule quality and increase its coverage in three ways.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9960783123970032}]}, {"text": "First, we enrich non-terminal symbols as to capture contextual and structured information.", "labels": [], "entities": []}, {"text": "The enrichment of non-terminal symbols not only guides the translation in favor of well formed structures, but also is beneficial to translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 133, "end_pos": 144, "type": "TASK", "confidence": 0.9734094738960266}]}, {"text": "Second, we examine the difference between word alignments for semantic parsing and SMT to better adapt word alignment in SMT to semantic parsing.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7069535404443741}, {"text": "semantic parsing", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.7177286446094513}, {"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9798822402954102}, {"text": "word alignment", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.6847662925720215}, {"text": "SMT", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.8930374383926392}, {"text": "semantic parsing", "start_pos": 128, "end_pos": 144, "type": "TASK", "confidence": 0.7101830095052719}]}, {"text": "Third, unlike most existing SMT systems that keep unknown words untranslated and intact in translation, we exploit the translation of unknown words via synthetic translation rules.", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9922423362731934}]}, {"text": "Evaluation on GeoQuery benchmark dataset shows that our approach obtains consistent improvement and achieves the state-of-the-art across various languages, including English, German and Greek.", "labels": [], "entities": [{"text": "GeoQuery benchmark dataset", "start_pos": 14, "end_pos": 40, "type": "DATASET", "confidence": 0.9080726305643717}]}], "datasetContent": [{"text": "In this section, we test our approach on the GeoQuery dataset, which is publicly available.", "labels": [], "entities": [{"text": "GeoQuery dataset", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9721620976924896}]}, {"text": "Data GeoQuery dataset consists of 880 questions paired with their corresponding tree structured semantic representations.", "labels": [], "entities": [{"text": "Data GeoQuery dataset", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.7315738797187805}]}, {"text": "Following the experimental setup in, we use the 600 question pairs to train and tune our SMT de-coder, and evaluated on the remaining 280.", "labels": [], "entities": [{"text": "SMT de-coder", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.7754330933094025}]}, {"text": "Note that there is another version of GeoQuery dataset where the semantic representation is annotated with lambda calculus expressions and which is extensively studied.", "labels": [], "entities": [{"text": "GeoQuery dataset", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.9165577292442322}]}, {"text": "Performance on the version of lambda calculus is higher than that on the tree structured version, however, the results obtained over the two versions are not directly comparable.", "labels": [], "entities": []}, {"text": "SMT Setting We use cdec () as our HPB decoder.", "labels": [], "entities": [{"text": "SMT Setting", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7106600999832153}, {"text": "HPB decoder", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.9332903325557709}]}, {"text": "As mentioned above, 600 instances are used to train and tune our decoder.", "labels": [], "entities": []}, {"text": "To get fair results, we split the 600 instances into 10 folds, each having 60 instances.", "labels": [], "entities": []}, {"text": "Then for each fold, we use it as the tuning data while the other 540 instances and the NP list are used as training data.", "labels": [], "entities": [{"text": "NP list", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.8532784581184387}]}, {"text": "We use IRSTLM toolkit) to train a 5-gram LM on the MRL side of the training data, using modified Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "We use Mira ( to tune the parameters of the system to maximize BLEU ().", "labels": [], "entities": [{"text": "Mira", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.7826616764068604}, {"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9945297837257385}]}, {"text": "When extracting translation rules from aligned training data, we include both tight and untight phrases.", "labels": [], "entities": [{"text": "extracting translation", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.6572113931179047}]}, {"text": "Evaluation We use the standard evaluation criteria for evaluation by executing both the predicted MRL and the gold standard against the database and obtaining their respective answer.", "labels": [], "entities": []}, {"text": "Specifically, we convert a translation from MRL into MRL (if exists).", "labels": [], "entities": [{"text": "MRL", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.47653090953826904}]}, {"text": "The translation then is considered correct if and only if its MRL retrieves the same answers as the gold standard MRL (), allowing fora fair comparison between our systems and previous works.", "labels": [], "entities": []}, {"text": "As in, we report accuracy, i.e. the percentage of translations with correct answers, and F1, i.e. the harmonic mean of precision (the proportion of correct answers out of translations with an answer) and recall (the proportion of correct answers out of all translations).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9992485642433167}, {"text": "F1", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.999778687953949}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9963931441307068}, {"text": "recall", "start_pos": 204, "end_pos": 210, "type": "METRIC", "confidence": 0.9995355606079102}]}, {"text": "In this section, we report our performance scores and analysis numbers averaged on our 10 SMT models.: Performance of our (non-) enriched SCFG systems with different alignment settings.", "labels": [], "entities": [{"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9785536527633667}]}, {"text": "shows the results of (non-) enriched SCFG systems over different alignment settings.", "labels": [], "entities": []}, {"text": "In Table 2, src2tgt and tgt2src indicate alignment of source to target direction and alignment of target to source direction, respectively; gdfa indicates symmetrization of alignment with growdiag-final-and strategy; src2tgt+tgt2src indicates doubling the training data with each sentence pair having both src2tgt and tgt2src alignments, similar for src2tgt+gdfa and tgt2src+gdfa; all indicates tripling the training data with each sentence pair having three alignments.", "labels": [], "entities": []}, {"text": "Finally, gold indicates using gold alignment.", "labels": [], "entities": []}, {"text": "8 Effect of Enriched SCFG From, we observe that enriched SCFG systems outperform non-enriched SCFG systems overall alignment settings, indicating the effect of enriching nonterminals.", "labels": [], "entities": []}, {"text": "In particular for tgt2src alignment, it obtains improvements of 3.5% inaccuracy and 2.7% in F1.", "labels": [], "entities": [{"text": "tgt2src alignment", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7462262809276581}, {"text": "inaccuracy", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9730985760688782}, {"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9992464780807495}]}, {"text": "As mentioned earlier, the non-enriched SCFG system may result in ill-formed translations, which cannot be converted back to tree structure.", "labels": [], "entities": []}, {"text": "One natural way to overcome this issue, as in, would be to simply filter n-best translation till a well-formed one is found.", "labels": [], "entities": []}, {"text": "However, we see very limited performance changes inaccuracy and F1, suggesting that the effect of using n-best translation is very limited.", "labels": [], "entities": [{"text": "F1", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9998138546943665}]}, {"text": "For example, after using n-best translation, the nonenriched SCFG system with all alignment obtains 82.0 inaccuracy (increased from 81.5) and 84.5 in  \u2022 Semantic parsing is substantially sensitive to alignment.", "labels": [], "entities": [{"text": "82.0 inaccuracy", "start_pos": 100, "end_pos": 115, "type": "METRIC", "confidence": 0.8469210863113403}]}, {"text": "Surprisingly, gdfa alignment, which is widely adopted in SMT, is inferior to tgt2src alignment.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9920998215675354}]}, {"text": "As expected, src2tgt alignment achieves the worst performance.", "labels": [], "entities": []}, {"text": "\u2022 Thanks to the increased coverage, doubling the training data (e.g., rows of src2tgt+tgt2src, src2tgt+gdfa, and tgt2src+gdfa) usually outperforms its corresponding single alignment.", "labels": [], "entities": [{"text": "coverage", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9532033801078796}]}, {"text": "Moreover, tripling the training data (e.g., rows of all) achieves slightly better performance than anyway of doubling the training data.", "labels": [], "entities": []}, {"text": "This is expected since the gdfa alignment actually comes from the alignments of src2tgt and tgt2src, thus doubling the training with src2tgt and tgt2src have already included most aligns in gdfa alignment.", "labels": [], "entities": []}, {"text": "\u2022 Our approach of tripling the training data achieves comparable performance to the one with gold alignment, suggesting that instead of developing a brand new algorithm for semantic parsing alignment, we can simply make use of GIZA++ alignment output.", "labels": [], "entities": [{"text": "semantic parsing alignment", "start_pos": 173, "end_pos": 199, "type": "TASK", "confidence": 0.8402929703394572}]}, {"text": "In terms of the src2tgt, tgt2src and gdfa alignments, the trend of the results is consistent over both non-enriched and enriched SCFG systems: the systems with tgt2src alignment work best while the systems with src2tgt alignment work worst.", "labels": [], "entities": []}, {"text": "Next we look at the non-enriched SCFG systems to explore the behavior differences among the three alignments.", "labels": [], "entities": []}, {"text": "We examine the alignment accuracy against the gold alignment on training data (except the NP list part).", "labels": [], "entities": [{"text": "alignment", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.8775319457054138}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8107131719589233}]}, {"text": "As shown in, src2tgt has the highest recall while tgt2src has the highest precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9995517134666443}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9976047277450562}]}, {"text": "This is partly due to: 1) In src2tgt alignment, each source word aligns to exactly one particular target word (or the null token), resulting infrequent  alignment errors for source side words that have no counterpart in target side.", "labels": [], "entities": []}, {"text": "For example, both words of the and be on source side, which play functional roles in NL, rather than semantic roles, align to 15 different target words.", "labels": [], "entities": []}, {"text": "2) Except fora few words on target side, including stateid@1, all@0 which have strong occurrence patterns (e.g., stateid@1 is always followed by a state name), each word has counterpart on source side.", "labels": [], "entities": []}, {"text": "As to have a clearer understanding on the individual contribution of using enriched nonterminals and multiple word alignments, presents two confusion matrices which show numbers of sentences that are correctly/wrongly parsed by three SMT systems on English test sentences.", "labels": [], "entities": [{"text": "SMT", "start_pos": 234, "end_pos": 237, "type": "TASK", "confidence": 0.9725819826126099}]}, {"text": "It shows that, for example, 211 sentences are correctly parsed by both non-enriched and enriched SCFG systems with gdfa alignment.", "labels": [], "entities": []}, {"text": "Moving from performance of the non-enriched SMT system with gdfa alignment to that of the enriched SMT system with all alignment, we observe that on average more than half of the improvement comes from using multiple word alignments, the rest from using enriched non-terminals.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9482069611549377}]}, {"text": "Effect of Unknown Word Translation Since each of our SMT model is actually trained on 540 instances (plus the NP list), the rate of unknown words in the test data tends to be higher than that in a system trained with the whole 600 instances.", "labels": [], "entities": [{"text": "Unknown Word Translation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.6197505990664164}, {"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9904578924179077}]}, {"text": "Based on the system of enriched SCFG with all alignment, shows the results of applying unknown word translation.", "labels": [], "entities": [{"text": "word translation", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7364577651023865}]}, {"text": "It shows that translating all unknown words into null obtains 2.4 points inaccuracy over the system without it (e.g., 85.3 vs. 82.9).", "labels": [], "entities": []}, {"text": "However, the slight improvement in F1 (e.g., 86.3 vs. 86.1) suggests that there are many scenarios that translating unknown words into null is incorrect.", "labels": [], "entities": [{"text": "F1", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9997838139533997}]}, {"text": "Fortunately, our semantic approach is partially able to generate correct translation rules for those unknown words which have translation in MRL . Actually, the effect of our approach is highly dependent on the quality of the close words found via Word2Vec.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 248, "end_pos": 256, "type": "DATASET", "confidence": 0.9613779187202454}]}, {"text": "With a manual examination  on the test data, we found that 11 out of all 17 unknown words should be translated into a corresponding token in MRL.", "labels": [], "entities": [{"text": "MRL", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.635797917842865}]}, {"text": "For 8 of them, the synthetic translation rule set returned by Algorithm 1 contains correct translation rules.", "labels": [], "entities": []}, {"text": "Effect across Different Languages We have also tested our approach on the same dataset with other three languages.", "labels": [], "entities": []}, {"text": "Specifically, while we are not aware of public resources to looking for semantically close words in German, Greek and Thai, we translate unknown words into null for the three languages.", "labels": [], "entities": []}, {"text": "Decoding Time Analysis We analyze the effect on the decoding time of our approach, which is closely related to the size of phrase tables.", "labels": [], "entities": [{"text": "Decoding Time Analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7462729016939799}]}, {"text": "Firstly, splitting non-terminal X into enriched ones increases the size of phrase tables.", "labels": [], "entities": []}, {"text": "9 This is not surprising since a phrase with non-terminal X (e.g., the X on the source side) maybe further specified as multiple phrases with various nonterminals (e.g., the C, the C/A1, etc.).", "labels": [], "entities": []}, {"text": "As a result, the average number of phrases per sentence (in English test data, hereafter) increases from 453 to 893 while the decoding time of the SMT decoder increases from 0.11 seconds to 0.19 seconds per sentence on average.", "labels": [], "entities": [{"text": "English test data", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.6774192452430725}, {"text": "SMT decoder", "start_pos": 147, "end_pos": 158, "type": "TASK", "confidence": 0.8575940132141113}]}, {"text": "Secondly, using multiple alignments also leads to larger phrase tables.", "labels": [], "entities": []}, {"text": "This is illustrated by the increase of average number of phrases per sentence from 893 to 2055 while the decoding time moves from 0.19 seconds to 0.38 seconds per sentence on average.", "labels": [], "entities": []}, {"text": "Finally, finding similar words via Word2Vec, however, is quite fast since this is bounded by the vocabulary size of our training set.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.9566245079040527}]}, {"text": "Thanks to the small size of unknown words, adding unknown word translation rules has a very limited impact on the size of phrase table, consequently negligible changes on decoding time.", "labels": [], "entities": []}, {"text": "In cdec, we generate a phrase table for each sentence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of our (non-) enriched  SCFG systems with different alignment settings.", "labels": [], "entities": []}, {"text": " Table 4: Confusion matrices of three SMT systems  on English test sentences.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9933170080184937}]}, {"text": " Table 5: Performance with unknown word translation.", "labels": [], "entities": [{"text": "unknown word translation", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.7379055023193359}]}, {"text": " Table 6: Performance for the multilingual GeoQuery test set.", "labels": [], "entities": [{"text": "GeoQuery test set", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.740617444117864}]}, {"text": " Table 7: Performance comparison for the multilingual GeoQuery test set. The performance of WASP,  HYBRIDTREE+, tsVB and UBL is taken from", "labels": [], "entities": [{"text": "GeoQuery test set", "start_pos": 54, "end_pos": 71, "type": "DATASET", "confidence": 0.7401054203510284}, {"text": "WASP", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.710439920425415}]}]}