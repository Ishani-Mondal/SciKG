{"title": [{"text": "Concept-based Summarization using Integer Linear Programming: From Concept Pruning to Multiple Optimal Solutions", "labels": [], "entities": [{"text": "Concept-based Summarization", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.48222072422504425}]}], "abstractContent": [{"text": "In concept-based summarization, sentence selection is modelled as a budgeted maximum coverage problem.", "labels": [], "entities": [{"text": "summarization", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.8817058801651001}, {"text": "sentence selection", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7802410423755646}]}, {"text": "As this problem is NP-hard, pruning low-weight concepts is required for the solver to find optimal solutions efficiently.", "labels": [], "entities": []}, {"text": "This work shows that reducing the number of concepts in the model leads to lower ROUGE scores, and more importantly to the presence of multiple optimal solutions.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9946097135543823}]}, {"text": "We address these issues by extending the model to provide a single optimal solution, and eliminate the need for concept pruning using an approximation algorithm that achieves comparable performance to exact inference.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have witnessed increased interest in global inference methods for extractive summarization.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.8358846008777618}]}, {"text": "These methods formulate summarization as a combinatorial optimization problem, i.e. selecting a subset of sentences that maximizes an objective function under a length constraint, and use Integer Linear Programming (ILP) to solve it exactly.", "labels": [], "entities": [{"text": "formulate summarization", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.6993121802806854}]}, {"text": "In this work, we focus on the concept-based ILP model for summarization introduced by ).", "labels": [], "entities": [{"text": "summarization", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.9708648324012756}]}, {"text": "In their model, a summary is generated by assembling the subset of sentences that maximizes a function of the unique concepts it covers.", "labels": [], "entities": []}, {"text": "Selecting the optimal subset of sentences is then cast as an instance of the budgeted maximum coverage problem . As this problem is NP-hard, pruning low-weight concepts is required for the ILP solver to find optimal solutions efficiently).", "labels": [], "entities": []}, {"text": "However, reducing the number of concepts in the model has two undesirable consequences.", "labels": [], "entities": []}, {"text": "First, it forces the model to only use a limited number of concepts to rank summaries, resulting in lower ROUGE scores.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9893848299980164}]}, {"text": "Second, by reducing the number of items from which sentence scores are derived, it allows different sentences to have the same score, and ultimately leads to multiple optimal summaries.", "labels": [], "entities": []}, {"text": "To our knowledge, no previous work has mentioned these problems, and only results corresponding to the first optimal solution found by the solver are reported.", "labels": [], "entities": []}, {"text": "However, as we will show through experiments, these multiple optimal solutions cause a substantial amount of variation in ROUGE scores, which, if not accounted for, could lead to incorrect conclusions.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 122, "end_pos": 134, "type": "METRIC", "confidence": 0.9765338599681854}]}, {"text": "More specifically, the contributions of this work are as follows: \u2022 We evaluate )'s summarization model at various concept pruning levels.", "labels": [], "entities": []}, {"text": "In doing so, we quantify the impact of pruning on running time, ROUGE scores and the number of optimal solutions.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.9730461835861206}]}, {"text": "\u2022 We extend the model to address the problem of multiple optimal solutions, and we sidestep the need for concept pruning by developing a fast approximation algorithm that achieves near-optimal performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments are conducted on the DUC'04 and TAC'08 datasets.", "labels": [], "entities": [{"text": "DUC'04", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.9823132157325745}, {"text": "TAC'08 datasets", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.8114199042320251}]}, {"text": "For DUC'04, we use the 50 topics from the generic multi-document summarization task (Task 2).", "labels": [], "entities": [{"text": "DUC'04", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.8509974479675293}, {"text": "multi-document summarization task", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.6165472567081451}]}, {"text": "For TAC'08, we focus only on the 48 topics from the non-update summarization task.", "labels": [], "entities": [{"text": "TAC'08", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.6402573585510254}]}, {"text": "Each topic contains 10 newswire articles for which the task is to generate a summary no longer than 100 words (whitespace-delimited tokens).", "labels": [], "entities": []}, {"text": "Summaries are evaluated against reference summaries using the ROUGE automatic evaluation measures).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.7794696688652039}]}, {"text": "We set the ROUGE parameters to those 6 that lead to highest agreement with manual evaluation (, that is, with stemming and stopwords not removed.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9900031089782715}, {"text": "agreement", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.983410656452179}]}, {"text": "presents the average number of optimal solutions at different levels of concept pruning.", "labels": [], "entities": []}, {"text": "Overall, the average number of optimal solutions increases along with the minimum document frequency, reaching 4.8 for TAC'08 at DF = 4.", "labels": [], "entities": [{"text": "DF", "start_pos": 129, "end_pos": 131, "type": "METRIC", "confidence": 0.9794801473617554}]}, {"text": "Prun-ing concepts also greatly reduces the number of variables in the ILP formulation, and consequently improves the run-time for solving the problem.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average number of optimal solutions, concepts and sentences for different minimum document  frequencies. The average time in seconds for finding the first optimal solution is also reported.", "labels": [], "entities": []}, {"text": " Table 2: Mean ROUGE recall and standard deviation for different minimum document frequencies.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9614453911781311}, {"text": "ROUGE", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.6349446773529053}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.7585827708244324}, {"text": "standard deviation", "start_pos": 32, "end_pos": 50, "type": "METRIC", "confidence": 0.9457767903804779}]}, {"text": " Table 3: ROUGE-1 recall scores for the first opti- mal solution found by different solvers along with  the number of systems that obtain significantly  lower (\u2193) or higher (\u2191) scores (p-value < 0.05).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9834818840026855}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.8862168788909912}]}, {"text": " Table 4: ROUGE recall scores of the approxima- tion. The relative difference from the mean score  of the multiple optimal solutions is also reported.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9889116287231445}, {"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.8449599146842957}]}]}