{"title": [{"text": "Learning Semantic Representations for Nonterminals in Hierarchical Phrase-Based Translation", "labels": [], "entities": [{"text": "Hierarchical Phrase-Based Translation", "start_pos": 54, "end_pos": 91, "type": "TASK", "confidence": 0.6144299109776815}]}], "abstractContent": [{"text": "In hierarchical phrase-based translation, coarse-grained nonterminal Xs may generate inappropriate translations due to the lack of sufficient information for phrasal substitution.", "labels": [], "entities": [{"text": "hierarchical phrase-based translation", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.5975343485673269}]}, {"text": "In this paper we propose a framework to refine nonterminals in hierarchical translation rules with real-valued semantic representations.", "labels": [], "entities": []}, {"text": "The semantic representations are learned via a weighted mean value and a minimum distance method using phrase vector representations obtained from large scale monolin-gual corpus.", "labels": [], "entities": []}, {"text": "Based on the learned semantic vectors, we build a semantic non-terminal refinement model to measure semantic similarities between phrasal substitutions and nonterminal Xs in translation rules.", "labels": [], "entities": []}, {"text": "Experiment results on Chinese-English translation show that the proposed model significantly improves translation quality on NIST test sets.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.6676178872585297}, {"text": "NIST test sets", "start_pos": 125, "end_pos": 139, "type": "DATASET", "confidence": 0.9291329582532247}]}], "introductionContent": [{"text": "Hierarchical phrase-based translation explores formal synchronous context free grammar (SCFG) rules for translation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.70111483335495}]}, {"text": "Two types of nonterminal symbols are used in translation rules: nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner.", "labels": [], "entities": []}, {"text": "The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules.", "labels": [], "entities": []}, {"text": "In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (, or employ syntactic information * Corresponding author from parse trees to refine nonterminals with realvalued vectors (.", "labels": [], "entities": []}, {"text": "In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals ().", "labels": [], "entities": []}, {"text": "All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules.", "labels": [], "entities": []}, {"text": "Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data.", "labels": [], "entities": []}, {"text": "In contrast, a large amount of unlabeled data are easily available.", "labels": [], "entities": []}, {"text": "Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data?", "labels": [], "entities": []}, {"text": "Or can we exploit these data to refine nonterminals for SMT?", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9937660694122314}]}, {"text": "Learning semantic representations for terminals (words, multi-word phrases or sentences) from unlabeled data has achieved substantial progress in recent years ().", "labels": [], "entities": []}, {"text": "These representations have been used successfully in various NLP tasks.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 61, "end_pos": 70, "type": "TASK", "confidence": 0.8816480338573456}]}, {"text": "However, there is no attempt to learn semantic representations for nonterminals from unlabeled data.", "labels": [], "entities": []}, {"text": "In this paper we propose a framework to learn semantic representations for nonterminal Xs in translation rules.", "labels": [], "entities": []}, {"text": "Our framework is established on the basis of realvalued vector representations learned for multiword phrases, which are substituted with nonterminal Xs during hierarchical rule extraction.", "labels": [], "entities": [{"text": "hierarchical rule extraction", "start_pos": 159, "end_pos": 187, "type": "TASK", "confidence": 0.6457963983217875}]}, {"text": "We propose a weighted mean value and a minimum distance method to obtain nonterminal representations from representations of their phrasal substitutions.", "labels": [], "entities": []}, {"text": "We further build a semantic nonterminal refinement model with semantic representations of nonterminals to compute similarities between phrasal substitutions and nonterminals.", "labels": [], "entities": []}, {"text": "In doing so, we want to enhance phrasal substitution and translation rule selection during decoding.", "labels": [], "entities": []}, {"text": "The big challenge here is that thousands of tar-get phrasal substitutions will be generated for one single nonterminal during decoding.", "labels": [], "entities": []}, {"text": "Computing vector representations for all these phrases will be very time-consuming.", "labels": [], "entities": []}, {"text": "We therefore introduce two different methods to handle it.", "labels": [], "entities": []}, {"text": "In the first method, we project representations of source phrases onto their target counterparts linearly/nonlinearly via a neural network.", "labels": [], "entities": []}, {"text": "These projected vectors are used as approximations to real target representations to compute semantic similarities.", "labels": [], "entities": []}, {"text": "In the second method, we decode sentences in two passes.", "labels": [], "entities": []}, {"text": "The first pass collects target phrase candidates from n-best translations of sentences generated by the baseline.", "labels": [], "entities": []}, {"text": "The second pass calculates vector representations of these collected target phrases and then computes similarities between them and target-side nonterminals.", "labels": [], "entities": []}, {"text": "First, we learn semantic representations for nonterminals from their phrasal substitutions with two different methods.", "labels": [], "entities": []}, {"text": "This is the first time, to the best of our knowledge, to induce semantic representations for nonterminals from unlabeled data in the context of SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 144, "end_pos": 147, "type": "TASK", "confidence": 0.9850698113441467}]}, {"text": "Second, we successfully address the issue of time-consuming target-side phrase-nonterminal similarity computation mentioned above.", "labels": [], "entities": []}, {"text": "We incorporate both source-/target-side semantic nonterminal refinement model and their combination based on learned nonterminal representations into translation system.", "labels": [], "entities": []}, {"text": "Experiment results show that our method can achieve an improvement of 1.16 BLEU points over the baseline system on NIST MT evaluation test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9987842440605164}, {"text": "NIST MT evaluation test sets", "start_pos": 115, "end_pos": 143, "type": "DATASET", "confidence": 0.7946165561676025}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly reviews related work.", "labels": [], "entities": []}, {"text": "Section 3 presents our approach of learning semantic vectors for nonterminals, followed by Section 4 describing the details of our semantic nonterminal refinement model.", "labels": [], "entities": []}, {"text": "Section 5 introduces the integration of the proposed model into SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9684984683990479}]}, {"text": "Experiment results are reported in Section 6.", "labels": [], "entities": []}, {"text": "Finally, we conclude our work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we conducted a series of experiments on Chinese-to-English translation using large-scale bilingual training data, aiming at the following questions: 1.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.6557150930166245}]}, {"text": "Which approach is better for learning nonterminal representations, weighted mean value or minimum distance?", "labels": [], "entities": []}, {"text": "2. Can the target-side semantic nonterminal refinement model improve translation quality?", "labels": [], "entities": [{"text": "target-side semantic nonterminal refinement", "start_pos": 11, "end_pos": 54, "type": "TASK", "confidence": 0.6125316470861435}, {"text": "translation", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.9596421122550964}]}, {"text": "And which method is better for integrating the target-side semantic model into translation, projection or two-pass decoding?", "labels": [], "entities": []}, {"text": "3. Does the combination of source and target semantic nonterminal refinement models provide further improvement?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores of our models against the  baseline and Syn-Mis model. /*\" and /+\" : sig- nificantly better than Baseline at significance level  p < 0.01 and p < 0.05 respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9996880292892456}]}, {"text": " Table 2: Comparison of two-pass decoding, linear  and nonlinear projection methods for integrating the  target-side semantic nonterminal refinement model  in terms of BLEU scores. /*\" and /+\" : sig- nificantly better than Baseline at significance level  p < 0.01 and p < 0.05 respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9975243210792542}]}, {"text": " Table 3: BLEU scores of the combination of the  source-and target-side semantic nonterminal re- fine model. /*\" and /+\" : significantly better  than Baseline at significance level p < 0.01 and  p < 0.05 respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994489550590515}]}]}