{"title": [{"text": "Hierarchical Recurrent Neural Network for Document Modeling", "labels": [], "entities": [{"text": "Document Modeling", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7990613579750061}]}], "abstractContent": [{"text": "This paper proposes a novel hierarchical recurrent neural network language model (HRNNLM) for document modeling.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7924520373344421}]}, {"text": "After establishing a RNN to capture the coherence between sentences in a document , HRNNLM integrates it as the sentence history information into the word level RNN to predict the word sequence with cross-sentence contextual information.", "labels": [], "entities": []}, {"text": "A two-step training approach is designed, in which sentence-level and word-level language models are approximated for the convergence in a pipeline style.", "labels": [], "entities": []}, {"text": "Examined by the standard sentence reordering scenario , HRNNLM is proved for its better accuracy in modeling the sentence coherence.", "labels": [], "entities": [{"text": "sentence reordering", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6992535144090652}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9988754391670227}]}, {"text": "And at the word level, experimental results also indicate a significant lower model perplexity, followed by a practical better translation result when applied to a Chinese-English document translation reranking task.", "labels": [], "entities": [{"text": "Chinese-English document translation reranking task", "start_pos": 164, "end_pos": 215, "type": "TASK", "confidence": 0.7505632162094116}]}], "introductionContent": [{"text": "Deep Neural Network (DNN), a neural network with multiple layers, has been proven powerful in many different domains, such as visual recognition () and speech recognition (), ever since formulated an efficient training method for it.", "labels": [], "entities": [{"text": "visual recognition", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7310805469751358}, {"text": "speech recognition", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.8090522289276123}]}, {"text": "In addition to the applications mentioned above, many neural network based methods have also been applied to natural language processing (NLP) tasks with great success.", "labels": [], "entities": [{"text": "natural language processing (NLP) tasks", "start_pos": 109, "end_pos": 148, "type": "TASK", "confidence": 0.7759010195732117}]}, {"text": "For example, propose a generalized DNN framework fora variety of fundamental NLP tasks, including part-of-speech tagging (postag), chunking, named * Contribution during internship at Microsoft Research.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.6969001740217209}, {"text": "chunking", "start_pos": 131, "end_pos": 139, "type": "TASK", "confidence": 0.9464045166969299}]}, {"text": "entity recognition (NER), and semantic role labeling.", "labels": [], "entities": [{"text": "entity recognition (NER)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8467221438884736}, {"text": "semantic role labeling", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.6718048254648844}]}, {"text": "DNN is successfully introduced to do wordlevel language modeling, aka., to predict the next word given the history words.", "labels": [], "entities": [{"text": "DNN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8225098252296448}, {"text": "wordlevel language modeling", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6944833000500997}]}, {"text": "propose a feedforward neural network to train a word-level language model with a limited n-gram history.", "labels": [], "entities": []}, {"text": "To leverage as much history as possible, apply recurrent neural network to word-level language modeling.", "labels": [], "entities": [{"text": "word-level language modeling", "start_pos": 75, "end_pos": 103, "type": "TASK", "confidence": 0.7002741098403931}]}, {"text": "The model absorbs one word each time, keeps the information in a history vector, and predicts the next word with all the word history in the vector.", "labels": [], "entities": []}, {"text": "Word-level language model can only learn the relationship between words in one sentence.", "labels": [], "entities": []}, {"text": "For sentences in one document which talks about one or several specific topics, the words in the next sentence are chosen partially in accordance with the previous sentences.", "labels": [], "entities": []}, {"text": "To model this kind of coherence of sentences, extend word embedding learning network ( to learn the paragraph embedding as a fixed-length vector representation for paragraph or sentence.", "labels": [], "entities": []}, {"text": "propose a neural network coherence model which employs distributed sentence representation and then predict the probability of whether a sequence of sentences is coherent or not.", "labels": [], "entities": []}, {"text": "In contrast to the methods mentioned above which learn the word relationship in or between the sentences separately, we propose a hierarchical recurrent neural network language model (HRNNLM) to capture the word sequence across the sentence boundaries at the document level.", "labels": [], "entities": []}, {"text": "HRNNLM is essentially a combination of a wordlevel language model and a sentence-level language model, both of which are recurrent neural networks.", "labels": [], "entities": [{"text": "HRNNLM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8111398816108704}]}, {"text": "The word-level recurrent neural network follows (.", "labels": [], "entities": []}, {"text": "The sentence-level language model is another recurrent neural network that takes sentence represen-tation as input, and predicts the words in the next sentence.", "labels": [], "entities": []}, {"text": "Similar to (, the hidden layer in the sentence-level recurrent neural network contains the sentence history information.", "labels": [], "entities": []}, {"text": "The hidden layer containing the history information of previous sentences is then linked as an input to the word-level recurrent neural network to predict the next word together with the word-level history vector.", "labels": [], "entities": []}, {"text": "This allows the language model to predict the next word probability distribution beyond the words in the current sentence.", "labels": [], "entities": []}, {"text": "We propose a two-step training approach to optimize the parameters of HRNNLM.", "labels": [], "entities": []}, {"text": "In the first step, we train the sentence-level language models independently . And then, we connect the hidden layer of the sentence-level language model to the input of word-level RNNLM and train the two models jointly until converged.", "labels": [], "entities": []}, {"text": "At sentence level, we evaluate our model with a sentence ordering task and the result shows our method can outperform a maximum entropy based and another stateof-the-art solution.", "labels": [], "entities": []}, {"text": "At word level, we compare our method with the conventional recurrent neural network based language model, finding the perplexity is reduced significantly.", "labels": [], "entities": []}, {"text": "We also apply our method to rank machine translation output and conduct experiments on a Chinese-English document translation task, yielding a better translation results compared with a state-of-the-art baseline system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.629792720079422}, {"text": "Chinese-English document translation task", "start_pos": 89, "end_pos": 130, "type": "TASK", "confidence": 0.708530381321907}]}, {"text": "The rest of this paper is organized as follows: Section 2 introduces work related to applying neural network to document modeling and SMT.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7494215667247772}, {"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9897953867912292}]}, {"text": "Section 3 introduces the general framework for document modeling.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7768293023109436}]}, {"text": "Our sentence-level language model and its training is described in Section 4, and the overall HRNNLM and its training is presented in Section 5.", "labels": [], "entities": [{"text": "HRNNLM", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.8431800603866577}]}, {"text": "Section 6 presents our experiments and their results.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the sentence-level performance of HRNNLM by the common coherence evaluation of sentence ordering task, its word-level performance by perplexity measure.", "labels": [], "entities": []}, {"text": "We also apply our HRNNLM to SMT reranking task in an open Chinese-English translation dataset.", "labels": [], "entities": [{"text": "SMT reranking", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9294020235538483}, {"text": "Chinese-English translation dataset", "start_pos": 58, "end_pos": 93, "type": "DATASET", "confidence": 0.6227334837118784}]}, {"text": "The translation performance index is the IBM version of BLEU-4 ().", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9619067311286926}, {"text": "BLEU-4", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.8749009370803833}]}], "tableCaptions": [{"text": " Table 3: BLEU scores of SMT systems. The I- WSLT is a public baseline which issued by the or- ganizer of IWSLT 2014, as described in (Cettolo  et al., 2012).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989590644836426}, {"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9890384674072266}, {"text": "I- WSLT", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.8100412885348002}, {"text": "IWSLT 2014", "start_pos": 106, "end_pos": 116, "type": "DATASET", "confidence": 0.8857836127281189}]}, {"text": " Table 4: Experimental results to test BLEU in- crease ratio after reranking", "labels": [], "entities": [{"text": "BLEU in- crease ratio", "start_pos": 39, "end_pos": 60, "type": "METRIC", "confidence": 0.9519695162773132}]}]}