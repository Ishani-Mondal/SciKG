{"title": [{"text": "Summarizing Student Responses to Reflection Prompts", "labels": [], "entities": [{"text": "Summarizing Student Responses to Reflection Prompts", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.9122116068998972}]}], "abstractContent": [{"text": "We propose to automatically summarize student responses to reflection prompts and introduce a novel summarization algorithm that differs from traditional methods in several ways.", "labels": [], "entities": [{"text": "summarize student responses to reflection prompts", "start_pos": 28, "end_pos": 77, "type": "TASK", "confidence": 0.8954695959885915}, {"text": "summarization", "start_pos": 100, "end_pos": 113, "type": "TASK", "confidence": 0.9711676239967346}]}, {"text": "First, since the linguistic units of student inputs range from single words to multiple sentences, our summaries are created from extracted phrases rather than from sentences.", "labels": [], "entities": []}, {"text": "Second, the phrase summarization algorithm ranks the phrases by the number of students who semantically mention a phrase in a summary.", "labels": [], "entities": [{"text": "phrase summarization", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.7049106955528259}]}, {"text": "Experimental results show that the proposed phrase summarization approach achieves significantly better sum-marization performance on an engineering course corpus in terms of ROUGE scores when compared to other summarization methods, including MEAD, LexRank and MMR.", "labels": [], "entities": [{"text": "phrase summarization", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.6640893816947937}, {"text": "ROUGE", "start_pos": 175, "end_pos": 180, "type": "METRIC", "confidence": 0.9958480596542358}, {"text": "MEAD", "start_pos": 244, "end_pos": 248, "type": "DATASET", "confidence": 0.8727394342422485}, {"text": "LexRank", "start_pos": 250, "end_pos": 257, "type": "DATASET", "confidence": 0.9042763113975525}]}], "introductionContent": [{"text": "Educational research has demonstrated the effectiveness of reflection prompts to enhance interaction between instructors and students (Van den Boom et al.,).", "labels": [], "entities": []}, {"text": "However, summarizing student responses to these prompts for large courses (e.g., introductory STEM, MOOCs) is an onerous task for humans and poses challenges for existing summarization methods.", "labels": [], "entities": [{"text": "summarizing student responses to", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.8831149488687515}]}, {"text": "First, the linguistic units of student inputs range from single words to multiple sentences.", "labels": [], "entities": []}, {"text": "Second, we assume that the concepts (represented as phrases) mentioned by more students should get more attention from the instructor.", "labels": [], "entities": []}, {"text": "Based on this assumption, we introduce the notion of student coverage, defined as the number of students who semantically mention a particular phrase.", "labels": [], "entities": []}, {"text": "The more student coverage a phrase has,", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the ROUGE evaluation metric) and report R-1 (unigrams), R-2 (bigrams), and R-SU4 (bigrams with skip distance up to 4 words), including the recall (R), precision (P) and FMeasure (F).", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 146, "end_pos": 156, "type": "METRIC", "confidence": 0.946784183382988}, {"text": "precision (P)", "start_pos": 158, "end_pos": 171, "type": "METRIC", "confidence": 0.9524451047182083}, {"text": "FMeasure (F)", "start_pos": 176, "end_pos": 188, "type": "METRIC", "confidence": 0.9513073265552521}]}, {"text": "These scores measure the overlap between human-generated summaries and a machine-generated summary.", "labels": [], "entities": []}, {"text": "We design and compare a number of other summarization methods to evaluate the proposed phrase summarization approach.", "labels": [], "entities": [{"text": "phrase summarization", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.652739942073822}]}, {"text": "Maui () is selected as the baseline, which is one of the state-of-the-art keyphrase extraction methods.", "labels": [], "entities": [{"text": "Maui", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9368881583213806}, {"text": "keyphrase extraction", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.87265345454216}]}, {"text": "Existing sentence summarization techniques can be used for phrase summarization by extracting candidate phrases and treating them as sentences.", "labels": [], "entities": [{"text": "sentence summarization", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.7338067591190338}, {"text": "phrase summarization", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.8024046719074249}]}, {"text": "Within this framework, we adapt MEAD ( ) and LexRank () to: Summarization performance.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9143763184547424}, {"text": "LexRank", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9089363217353821}, {"text": "Summarization", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.8293163776397705}]}, {"text": "The last row is our proposed approach.", "labels": [], "entities": []}, {"text": "The highest score for each column is shown in bold.", "labels": [], "entities": []}, {"text": "\u2020 indicates that the improvement over the MEAD+MMR baseline is statistically significant.", "labels": [], "entities": [{"text": "MEAD+MMR baseline", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.6947422996163368}]}, {"text": "* indicates that the improvement over LexRank+MMR is statistically significant.", "labels": [], "entities": [{"text": "LexRank+MMR", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.8093899885813395}]}, {"text": "We also include the original MEAD 5 for comparison (named as OriMEAD).", "labels": [], "entities": [{"text": "MEAD 5", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.6915358901023865}]}, {"text": "We applied the MMR (), a popular diversity-based summarization method as a post-processing step to the MEAD (MEAD+MMR) and LexRank (LexRank+MMR) baselines.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.8212810158729553}, {"text": "LexRank (LexRank+MMR) baselines", "start_pos": 123, "end_pos": 154, "type": "DATASET", "confidence": 0.8802128263882228}]}, {"text": "To show the performance using the clustering alone, this baseline selects the medoid phrase instead of using LexRank to rank the phrases in a cluster to form the summary.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9379491209983826}]}, {"text": "The performance on the test set is shown in with the length limit L as 4 phrases (the average phrase number in the TA's summary).", "labels": [], "entities": []}, {"text": "Similar results can be observed when the length limit is based on the number of words, but cannot be reported here due to page limit.", "labels": [], "entities": []}, {"text": "First, our proposed method (last row), which clusters the extracted phrases and uses LexRank to score them, can outperform all the baselines overall three ROUGE scores in terms of F-measure.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.908046305179596}, {"text": "ROUGE", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.9888501763343811}, {"text": "F-measure", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9847701787948608}]}, {"text": "In addition, the proposed model performs better than the clustering and LexRank alone.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9578436613082886}]}, {"text": "Through a paired t-test, our model outperforms LexRank statistically in terms of precision for all three ROUGE scores and significantly improves Clustering+Medoid on all R-2 scores (except the precision with 0.06 p-value).", "labels": [], "entities": [{"text": "LexRank", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.8597905039787292}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9994142055511475}, {"text": "ROUGE", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9244473576545715}, {"text": "Clustering", "start_pos": 145, "end_pos": 155, "type": "METRIC", "confidence": 0.9721453189849854}, {"text": "Medoid", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.5675392150878906}, {"text": "precision", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.9980432987213135}]}, {"text": "We believe that the semantic similarity based clustering complements LexRank in two ways: 1) LexRank depends on the cosine similarity of TF-IDF vectors to build the graph while the clustering takes semantic similarity into account.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9520811438560486}, {"text": "LexRank", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.9005568623542786}]}, {"text": "2) The clustering performed a global selection to form a summary by grouping similar phrases and ranking them by the number of covered students (similar to what the human did).", "labels": [], "entities": []}, {"text": "Compared to LexRank, our approach captures the student coverage explicitly.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.9512883424758911}]}, {"text": "While modifying LexRank by using semantic similarity is possible, estimating the student coverage is not straightforward.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.9311216473579407}]}, {"text": "Second, OriMEAD tends to select long sentences, resulting in a high recall but a low precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9993709921836853}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9982831478118896}]}, {"text": "The phrase version (MEAD) improves both the P and F scores by removing unnecessary parts in the original sentences.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9550606608390808}, {"text": "F scores", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.8725206255912781}]}, {"text": "Lastly, the proposed method outperforms the MMR based baselines on the precision and Fmeasure of all three ROUGE scores.", "labels": [], "entities": [{"text": "MMR", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.7664035558700562}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9996277093887329}, {"text": "Fmeasure", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9994803071022034}, {"text": "ROUGE", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.9793121814727783}]}, {"text": "We observed that the MMR baselines suffer from the issue of diverse expressions used the students (e.g., \"graphs\" and \"charts\").", "labels": [], "entities": [{"text": "MMR", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.96110600233078}]}], "tableCaptions": [{"text": " Table 2: Word Count (WC) in student responses  (Student-WC), WC per phrase in TA's summary  (TA-PWC), WC in TA's summary (TA-WC) and  phrase count in TA's summary (TA-PC)", "labels": [], "entities": [{"text": "Word Count (WC)", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.8420450568199158}, {"text": "WC", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.9871771335601807}]}, {"text": " Table 3: Summarization performance. The last row is our proposed approach. The highest score for each  column is shown in bold.  \u2020 indicates that the improvement over the MEAD+MMR baseline is statistically  significant.  *  indicates that the improvement over LexRank+MMR is statistically significant.", "labels": [], "entities": [{"text": "MEAD+MMR baseline", "start_pos": 172, "end_pos": 189, "type": "DATASET", "confidence": 0.6960623860359192}, {"text": "LexRank+MMR", "start_pos": 261, "end_pos": 272, "type": "DATASET", "confidence": 0.8661235769589742}]}]}