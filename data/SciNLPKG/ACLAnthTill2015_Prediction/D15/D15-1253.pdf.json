{"title": [{"text": "Efficient Hyper-parameter Optimization for NLP Applications", "labels": [], "entities": []}], "abstractContent": [{"text": "Hyper-parameter optimization is an important problem in natural language processing (NLP) and machine learning.", "labels": [], "entities": [{"text": "Hyper-parameter optimization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8373383581638336}, {"text": "natural language processing (NLP)", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.7862282991409302}]}, {"text": "Recently , a group of studies has focused on using sequential Bayesian Optimization to solve this problem, which aims to reduce the number of iterations and trials required during the optimization process.", "labels": [], "entities": []}, {"text": "In this paper, we explore this problem from a different angle, and propose a multi-stage hyper-parameter optimization that breaks the problem into multiple stages with increasingly amounts of data.", "labels": [], "entities": []}, {"text": "Early stage provides fast estimates of good candidates which are used to initialize later stages for better performance and speed.", "labels": [], "entities": []}, {"text": "We demonstrate the utility of this new algorithm by evaluating its speed and accuracy against state-of-the-art Bayesian Optimization algorithms on classification and prediction tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9992104768753052}, {"text": "classification and prediction tasks", "start_pos": 147, "end_pos": 182, "type": "TASK", "confidence": 0.8086151778697968}]}], "introductionContent": [{"text": "Hyper-parameter optimization has been receiving an increasingly amount of attention in the NLP and machine learning communities.", "labels": [], "entities": [{"text": "Hyper-parameter optimization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8430778682231903}]}, {"text": "The performance of learning algorithms depend on the correct instantiations of their hyperparameters, ranging from algorithms such as logistic regression and support vector machines, to more complex model families such as boosted regression trees and neural networks.", "labels": [], "entities": []}, {"text": "While hyper-parameter settings often make the difference between mediocre and state-of-the-art performance ( ), it is typically very time-consuming to find an optimal setting due to the complexity of model classes, and the amount of training data available for tuning.", "labels": [], "entities": []}, {"text": "The issue is particularly important in large-scale problems where the size of the data can be so large that even a quadratic running time is prohibitively large.", "labels": [], "entities": []}, {"text": "Recently several sequential Bayesian Optimization methods have been proposed for hyper-parameter search).", "labels": [], "entities": [{"text": "hyper-parameter search", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.8176681995391846}]}, {"text": "The common theme is to perform a set of iterative hyper-parameter optimizations, wherein each round, these methods fit a hyper-parameter response surface using a probabilistic regression function such as Gaussian Process) or tree-based models, where the response surface maps each hyperparameter setting to an approximated accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 323, "end_pos": 331, "type": "METRIC", "confidence": 0.9844160676002502}]}, {"text": "The learned regression model is then used as a cheap surrogate of the response surface to quickly explore the search space and identify promising hyper-parameter candidates to evaluate next in order to enhance validation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.9155300259590149}]}, {"text": "While these methods have enjoyed great success compared to conventional random search () and grid search algorithms by significantly reducing the number of iterations and trials required during the process, the focus and starting point of these work have largely been on dealing with many dimensions of hyper-parameters, rather than scaling to large amount of data as typical in many NLP tasks, where the efficiency bottleneck stems from the size of the training data in addition to hyper-parameter dimensions.", "labels": [], "entities": []}, {"text": "For example, as dataset size grows, even simple models (with few hyper-parameters) such as logistic regression can require more training time per iteration in these algorithms, leading to increased overall time complexity.", "labels": [], "entities": []}, {"text": "In this work, we introduce a multi-stage Bayesian Optimization framework for efficient hyper-parameter optimization, and empirically study the impact of the multi-stage algorithm on hyper-parameter tuning.", "labels": [], "entities": [{"text": "hyper-parameter optimization", "start_pos": 87, "end_pos": 115, "type": "TASK", "confidence": 0.7390043139457703}, {"text": "hyper-parameter tuning", "start_pos": 182, "end_pos": 204, "type": "TASK", "confidence": 0.7804214954376221}]}, {"text": "Unlike the previous approaches, the multi-stage approach considers hyper-parameter optimization in successive stages with increasingly amounts of training data.", "labels": [], "entities": [{"text": "hyper-parameter optimization", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.7426691353321075}]}, {"text": "The first stage uses a small subset of training data, applies sequential optimization to quickly identify an initial set of promising hyper-parameter settings, and these promising candidates are then used to initialize Bayesian Optimization on later stages with full training dataset to enable the expensive stages operate with better prior knowledge and converge to optimal solution faster.", "labels": [], "entities": []}, {"text": "The key intuition behind the proposed approach is that both dataset size and search space of hyperparameter can be large, and applying the Bayesian Optimization algorithm on the data can be both expensive and unnecessary, since many evaluated candidates may not even be within range of best final settings.", "labels": [], "entities": []}, {"text": "We note our approach is orthogonal and complementary to parallel Bayesian Optimization (Snoek et al., 2012) and multi-task learning (, because the improved efficiency per iteration, as achieved by our algorithm, is a basic building block of the other algorithms, thus can directly help the efficiency of multiple parallel runs, as well as runs across different datasets ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We empirically evaluate the algorithm on two tasks: classification and question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8332579135894775}]}, {"text": "For classification we use the Yelp dataset which is a customer review dataset.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9695652723312378}, {"text": "Yelp dataset", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9835321307182312}]}, {"text": "Each review contains a star/rating (1-5) fora business, and the task is to predict the rating based on the textual information in the review.", "labels": [], "entities": []}, {"text": "The training data contains half-million feature vectors, and unique unigrams are used as features (after standard stop-word re-moval and stemming).", "labels": [], "entities": []}, {"text": "For question answering (QA), the task is to identify correct answers fora given question.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.9212569952011108}]}, {"text": "We use a commercial QA dataset containing about 3800 unique training questions and a total of 900, 000 feature vectors.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.729328066110611}]}, {"text": "Each feature vector corresponds to an answer candidate fora given question, the vector consists of a binary label (1=correct, 0=incor-rect) and values from standard unigram/bigram, syntactic, and linguistic features used in typical QA applications).", "labels": [], "entities": []}, {"text": "Both QA and Yelp datasets contain independent training, validation, and test data, from which the machine learning models are built, accuracies are evaluated, and test results are reported, respectively.", "labels": [], "entities": [{"text": "Yelp datasets", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.875674307346344}]}, {"text": "We evaluate our multi-stage method against two methods: 1) state-of-the-art Bayesian Optimization for hyper-parameter learning), and 2) the same Bayesian Optimization but only applied on a small subset of data for speed.", "labels": [], "entities": []}, {"text": "For experiments, we consider learning hyper-parameters for two machine learning algorithms: SVM implementation for classification) and boosted regression trees for question answering) as shown in.", "labels": [], "entities": [{"text": "question answering", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.7725094556808472}]}], "tableCaptions": [{"text": " Table 3. In  terms of average accuracy, we see that the state- of-the-art Bayesian optimization on full training  data and the multi-stage algorithm achieve similar  test accuracy, and they both outperform the sub-", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9859916567802429}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9476904273033142}]}, {"text": " Table 2: Average test accuracy for QA (preci- sion@1) and Yelp dataset (classif. accuracy).", "labels": [], "entities": [{"text": "Average test accuracy", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.7703731854756674}, {"text": "QA", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.7569863796234131}, {"text": "Yelp dataset", "start_pos": 59, "end_pos": 71, "type": "DATASET", "confidence": 0.8303816616535187}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.8519086241722107}]}]}