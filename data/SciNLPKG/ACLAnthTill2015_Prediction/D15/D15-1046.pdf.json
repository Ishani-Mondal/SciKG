{"title": [{"text": "Hashtag Recommendation Using Dirichlet Process Mixture Models Incorporating Types of Hashtags", "labels": [], "entities": []}], "abstractContent": [{"text": "In recent years, the task of recommending hashtags for microblogs has been given increasing attention.", "labels": [], "entities": []}, {"text": "Various methods have been proposed to study the problem from different aspects.", "labels": [], "entities": []}, {"text": "However, most of the recent studies have not considered the differences in the types or uses of hashtags.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a novel nonparametric Bayesian method for this task.", "labels": [], "entities": []}, {"text": "Based on the Dirichlet Process Mixture Models (DPMM), we incorporate the type of hashtag as a hidden variable.", "labels": [], "entities": []}, {"text": "The results of experiments on the data collected from areal world microblogging service demonstrate that the proposed method outperforms state-of-the-art methods that do not consider these aspects.", "labels": [], "entities": []}, {"text": "By taking these aspects into consideration, the relative improvement of the proposed method over the state-of-the-art methods is around 12.2% in F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9983237385749817}]}], "introductionContent": [{"text": "Hashtags are used to mark keywords or topics in a microblog.", "labels": [], "entities": []}, {"text": "Over the past few years, social media services have become some of the most important communication channels for people.", "labels": [], "entities": []}, {"text": "According to the statistic reported by the Pew Research Centers Internet & American Life Project in, about 72% of adult internet users are also members of at least one social networking site.", "labels": [], "entities": [{"text": "Pew Research Centers Internet & American Life Project", "start_pos": 43, "end_pos": 96, "type": "DATASET", "confidence": 0.9196737855672836}]}, {"text": "Hence, microblogs have also been widely used as data sources for public opinion analyses), prediction), reputation management (, and many other applications (.", "labels": [], "entities": [{"text": "prediction", "start_pos": 91, "end_pos": 101, "type": "TASK", "confidence": 0.9662162065505981}, {"text": "reputation management", "start_pos": 104, "end_pos": 125, "type": "TASK", "confidence": 0.8901770114898682}]}, {"text": "In addition to the limited number of characters in the content, microblogs also contain a form of metadata tag (hashtag), which is a string of characters preceded by the symbol (#).", "labels": [], "entities": []}, {"text": "Hashtags are used to mark the keywords or topics of a microblog.", "labels": [], "entities": []}, {"text": "They can occur anywhere in a microblog, at the beginning, middle, or end.", "labels": [], "entities": []}, {"text": "Hashtags have been proven to be useful for many applications, including microblog retrieval, query expansion (A., and sentiment analysis ().", "labels": [], "entities": [{"text": "microblog retrieval", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.8076612651348114}, {"text": "query expansion", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.7051105201244354}, {"text": "sentiment analysis", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.9585792720317841}]}, {"text": "However, only a small percentages of microblogs contain hashtags provided by their authors.", "labels": [], "entities": []}, {"text": "Hence, the task of recommending hashtags for microblogs has become an important research topic and has received considerable attention in recent years.", "labels": [], "entities": []}, {"text": "Existing works have studied discriminative models (;) and generative models () based on the textual information of a single microblog.", "labels": [], "entities": [{"text": "generative models", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.9482075870037079}]}, {"text": "Since microblog users are free to develop and use their own hashtags, they may select hashtags for different purposes.", "labels": [], "entities": []}, {"text": "Based on an analysis of the hashtags crawled from areal online service, we observe that hashtags are used for events, conferences, conversation, disasters, memes, recall, quotes, and soon.", "labels": [], "entities": [{"text": "recall", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.9655047059059143}]}, {"text": "To illustrate it let us take the following examples: Example 1:#Apple iOS 9 includes music feature, new security and support for older iPhones.", "labels": [], "entities": []}, {"text": "Example 2:#BREAKING: Missing cyclist Natalie Donoghue has been found alive after she went missing in the Hunter Valley.", "labels": [], "entities": [{"text": "BREAKING", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9961509704589844}, {"text": "Hunter Valley", "start_pos": 105, "end_pos": 118, "type": "DATASET", "confidence": 0.9861287772655487}]}, {"text": "We can see that the hashtag #Apple iOS 9 used in the example summarize the main topics of the corresponding microblog.", "labels": [], "entities": [{"text": "Apple iOS 9", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.8262344797452291}]}, {"text": "While, the aim of hashtag #BREAKING in the example 2 is used as a label of the microblog.", "labels": [], "entities": [{"text": "BREAKING", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9840973019599915}]}, {"text": "The different uses greatly impact the strategy of hashtag recommendation.", "labels": [], "entities": [{"text": "hashtag recommendation", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.8755095899105072}]}, {"text": "However, there has been relatively few studies which take this issue into consideration.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel nonparametric Bayesian method to perform this problem.", "labels": [], "entities": []}, {"text": "Inspired by the methods proposed by, we assume that the hashtags and textual content in the corresponding microblog are parallel descriptions of the same thing in different languages.", "labels": [], "entities": []}, {"text": "We adapt a translation model with topic distribution to achieve this task.", "labels": [], "entities": []}, {"text": "Because of the ability of Dirichlet Process Mixture Models (DPMM) to handle an unbounded number of topics, the proposed method is extended from them.", "labels": [], "entities": [{"text": "Dirichlet Process Mixture Models (DPMM)", "start_pos": 26, "end_pos": 65, "type": "TASK", "confidence": 0.7520942645413535}]}, {"text": "Based on the different uses of hashtags, we incorporate the type of hashtag into the DPMM as a hidden variable.", "labels": [], "entities": []}, {"text": "The main contributions of this work can be summarized as follows: \u2022 Through analyzing the microblogs, we propose the problem of influences of types of hashtags.", "labels": [], "entities": []}, {"text": "\u2022 We adopt a nonparametric Bayesian method to perform the hashtag recommendation task, which also takes the types of hashtags into consideration.", "labels": [], "entities": []}, {"text": "\u2022 Experimental results on the dataset we construct from areal microblogging service show that the proposed method can achieve significantly better performance than the state-of-the-arts methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use precision (P ), recall (R), and F1-score (F 1 ) to evaluate the performance.", "labels": [], "entities": [{"text": "precision (P )", "start_pos": 7, "end_pos": 21, "type": "METRIC", "confidence": 0.9513928294181824}, {"text": "recall (R)", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9620208740234375}, {"text": "F1-score (F 1 )", "start_pos": 39, "end_pos": 54, "type": "METRIC", "confidence": 0.9599071621894837}]}, {"text": "Precision is calculated based on the percentage of \"hashtags truly assigned\" among \"hashtags assigned by system\".", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8865811824798584}]}, {"text": "Recall is calculated based on the \"hashtags truly 1 http://www.weibo.com assigned\" among \"hashtags manually assigned\".", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9853272438049316}]}, {"text": "F 1 is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F 1", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9849798679351807}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9994843006134033}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9976649284362793}]}, {"text": "We do 500 iterations of Gibbs sampling to train the model.", "labels": [], "entities": []}, {"text": "For optimizing the hyperparmeters of the proposed method and alternative methods, we use development data set to do it.", "labels": [], "entities": []}, {"text": "In this work, the scale parameter \u03b1 is set to Gamma(5, 0.5).", "labels": [], "entities": [{"text": "Gamma", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9502156972885132}]}, {"text": "The other settings of hyperparameters are as follows: \u03b2 w = 0.1, \u03b2 h = 0.1, \u03b7 = 0.01, and \u03c3 = 0.01.", "labels": [], "entities": []}, {"text": "The smoothing factor \u03b3 in Eq. is set to 0.8.", "labels": [], "entities": [{"text": "Eq.", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.8945556282997131}]}, {"text": "For estimating the translation probability without topical information, we use GIZA++ 1.07 to do it.", "labels": [], "entities": [{"text": "translation", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.953458845615387}, {"text": "GIZA++ 1.07", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.800232450167338}]}, {"text": "Since hashtag recommendation task can also be modeled as a classification problem, we compare the proposed model with the following alternative methods: \u2022 Naive Bayes (NB): We formulate hashtag recommendation as a binary classification task and apply NB to model the posterior probability of each hashtag given a microblog.", "labels": [], "entities": []}, {"text": "\u2022 Support Vector Machine (SVM): Similar to Naive Bayes, each hashtag can be regarded as one label and we use SVM to classify these microblogs.", "labels": [], "entities": []}, {"text": "\u2022 Translation model (IBM-1): IBM model 1 is directly applied to obtain the alignment probability between the word and the hashtag ( ).", "labels": [], "entities": []}, {"text": "\u2022 Topical translation model (TTM): proposed the TTM for hashtag extraction.", "labels": [], "entities": [{"text": "Topical translation model (TTM)", "start_pos": 2, "end_pos": 33, "type": "TASK", "confidence": 0.6684889445702235}, {"text": "hashtag extraction", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7902774810791016}]}, {"text": "We implemented and extended their method for evaluating on the corpus constructed in this work.", "labels": [], "entities": []}, {"text": "The number of topics in TTM is set to 20, and \u03b1 is set to 50/K.", "labels": [], "entities": [{"text": "TTM", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.7943417429924011}]}, {"text": "The hyperparameters used in TTM are also selected based on the development data set.", "labels": [], "entities": [{"text": "TTM", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.8308429718017578}]}, {"text": "shows the comparisons of the proposed method with the state-of-the-art methods on the constructed evaluation dataset.", "labels": [], "entities": []}, {"text": "\"CNHR\" denotes the method proposed in this paper.", "labels": [], "entities": []}, {"text": "\"NHR1\" is a degenerate variation of CNHR, in which we consider all the hashtags are generated from distribution \u03d5 1 . \"NHR2\" is a model in which we consider all the hashtags are generated from  From the results shown in, we also observe that the proposed method can achieve significantly better performance than existing methods.", "labels": [], "entities": []}, {"text": "The relative improvement of proposed CNHR over TTM is around 12.7% in F 1 . And we can see that the performances of TTM are similar as the results of NHR1.", "labels": [], "entities": [{"text": "F 1", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.942540168762207}, {"text": "NHR1", "start_pos": 150, "end_pos": 154, "type": "DATASET", "confidence": 0.9510568976402283}]}, {"text": "Since TTM and NHR1 are similar with each other except that TTM is based on LDA and NHR1 is adapted from DPMM.", "labels": [], "entities": [{"text": "NHR1", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.9226806163787842}, {"text": "TTM", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.8601232767105103}, {"text": "NHR1", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.8514630198478699}, {"text": "DPMM", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.9616075158119202}]}, {"text": "The results demonstrate the advantage of using DPMM over LDA.", "labels": [], "entities": []}, {"text": "It does not need prior knowledge about number of topics.", "labels": [], "entities": []}, {"text": "Comparing the results of the method CNHR with the methods NHR1 and NHR2 which do not take the types of hashtags into consideration, we can see that the proposed method benefits a lot from incorporating the types of hashtags.", "labels": [], "entities": []}, {"text": "shows the Precision, Recall, and F 1 curves of NB, IBM1, SVM, TTM, NHR1, NHR2 and CNHR on the test data.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.999169111251831}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9808964729309082}, {"text": "F 1", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9898678362369537}, {"text": "NB", "start_pos": 47, "end_pos": 49, "type": "DATASET", "confidence": 0.8302291631698608}, {"text": "IBM1", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.7095414996147156}, {"text": "NHR2", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.8133171200752258}]}, {"text": "Each point of a curve In curves, the curve that is the highest of the graph indicates the best performance.", "labels": [], "entities": []}, {"text": "Based on the results, we can observe that the performance of CNHR is the highest in all the curves.", "labels": [], "entities": []}, {"text": "This indicates that the proposed method was significantly better than the other methods.", "labels": [], "entities": []}, {"text": "In TTM, the number of topics K is also crucial factor.", "labels": [], "entities": [{"text": "TTM", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.8142394423484802}]}, {"text": "shows the impact of the number of topics.", "labels": [], "entities": []}, {"text": "From the table, we can observe that TTM obtains the best performance when K is set to 20.", "labels": [], "entities": [{"text": "TTM", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.6165415048599243}]}, {"text": "And performance decreases with more number of topics.", "labels": [], "entities": []}, {"text": "We think that data sparsity maybe one of the main reasons.", "labels": [], "entities": []}, {"text": "With much more topic number, the data sparsity problem will be more serious when estimating topic-specific translation probability.", "labels": [], "entities": [{"text": "estimating topic-specific translation", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.506570557753245}]}, {"text": "We compare our method with the best performance of TTM.", "labels": [], "entities": [{"text": "TTM", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.84372878074646}]}, {"text": "From the description of the proposed model, we can know that there is a smooth parameter \u03b3 in the proposed method CNHR.", "labels": [], "entities": [{"text": "CNHR", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.9394737482070923}]}, {"text": "To evaluate the impact of it, shows the influence of the translation probability smoothing parameter \u03b3.", "labels": [], "entities": []}, {"text": "When \u03b3 is set to 0.0, it means that the topical information is omitted.", "labels": [], "entities": []}, {"text": "Comparing the results of \u03b3 = 0.0 and other values, we can observe that the topical information can benefit this task.", "labels": [], "entities": []}, {"text": "When \u03b3 is set to 1.0, it represents the method without smoothing.", "labels": [], "entities": []}, {"text": "The results indicate that it is necessary to address the sparsity problem through smoothing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results of different methods on  the evaluation collection.", "labels": [], "entities": []}, {"text": " Table 2: The influence of the number of topics  K of TTM.  K Precision  Recall  F 1  10  0.441  0.389  0.413  20  0.445  0.393  0.417  30  0.432  0.381  0.405  40  0.413  0.364  0.387  50  0.391  0.345  0.367", "labels": [], "entities": [{"text": "TTM", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.6639012694358826}, {"text": "K", "start_pos": 60, "end_pos": 61, "type": "DATASET", "confidence": 0.6357898712158203}, {"text": "Precision  Recall  F 1  10  0.441  0.389  0.413", "start_pos": 62, "end_pos": 109, "type": "METRIC", "confidence": 0.6473464407026768}]}]}