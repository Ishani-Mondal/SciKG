{"title": [{"text": "Hierarchical Back-off Modeling of Hiero Grammar based on Non-parametric Bayesian Model", "labels": [], "entities": [{"text": "Hiero Grammar", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.7727662920951843}]}], "abstractContent": [{"text": "In hierarchical phrase-based machine translation, a rule table is automatically learned by heuristically extracting synchronous rules from a parallel corpus.", "labels": [], "entities": [{"text": "hierarchical phrase-based machine translation", "start_pos": 3, "end_pos": 48, "type": "TASK", "confidence": 0.6049470007419586}]}, {"text": "As a result, spuriously many rules are extracted which maybe composed of various incorrect rules.", "labels": [], "entities": []}, {"text": "The larger rule table incurs more run time for decoding and may result in lower translation quality.", "labels": [], "entities": []}, {"text": "To resolve the problems, we propose a hierarchical back-off model for Hiero grammar, an instance of asynchronous context free grammar (SCFG), on the basis of the hierarchical Pitman-Yor process.", "labels": [], "entities": [{"text": "Hiero grammar", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.6020627915859222}]}, {"text": "The model can extract a compact rule and phrase table without resorting to any heuristics by hierarchically backing off to smaller phrases under SCFG.", "labels": [], "entities": []}, {"text": "Inference is efficiently carried out using two-step synchronous parsing of Xiao et al., (2012) combined with slice sampling.", "labels": [], "entities": []}, {"text": "In our experiments, the proposed model achieved higher or at least comparable translation quality against a previous Bayesian model on various language pairs; German/French/Spanish/Japanese-English.", "labels": [], "entities": []}, {"text": "When compared against heuristic models, our model achieved comparable translation quality on a full size German-English language pair in Europarl v7 corpus with significantly smaller grammar size; less than 10% of that for heuristic model.", "labels": [], "entities": [{"text": "Europarl v7 corpus", "start_pos": 137, "end_pos": 155, "type": "DATASET", "confidence": 0.9445435603459676}]}], "introductionContent": [{"text": "Hierarchical phrase-based statistical machine translation (HPBSMT)) is a popular alternative to phrase-based SMT (PBSMT), in which synchronous context free grammar is used as the basis of the machine translation model.", "labels": [], "entities": [{"text": "Hierarchical phrase-based statistical machine translation (HPBSMT))", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.6318297386169434}, {"text": "phrase-based SMT (PBSMT)", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.7179485380649566}, {"text": "machine translation", "start_pos": 192, "end_pos": 211, "type": "TASK", "confidence": 0.7196954786777496}]}, {"text": "With HPBSMT, a restricted form of an SCFG, i.e., Hiero grammar, is usually used and is especially suited for linguistically divergent language pairs, such as Japanese and English.", "labels": [], "entities": [{"text": "HPBSMT", "start_pos": 5, "end_pos": 11, "type": "DATASET", "confidence": 0.93809974193573}]}, {"text": "However, a rule table, i.e., asynchronous grammar, maybe composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus.", "labels": [], "entities": []}, {"text": "As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality.", "labels": [], "entities": []}, {"text": "Pruning a rule table either on the basis of significance test) or entropy () used in PBSMT can be easily applied for HPBSMT.", "labels": [], "entities": [{"text": "HPBSMT", "start_pos": 117, "end_pos": 123, "type": "DATASET", "confidence": 0.9376301765441895}]}, {"text": "However, these methods still rely on a heuristically determined threshold parameter.", "labels": [], "entities": []}, {"text": "Bayesian SCFG methods) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7622398138046265}]}, {"text": "Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f | 3 |e| 3 ) when we use dynamic programming SCFG biparsing (.", "labels": [], "entities": [{"text": "O", "start_pos": 124, "end_pos": 125, "type": "METRIC", "confidence": 0.9597345590591431}]}, {"text": "Gibbs sampling without biparsing ( can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees.", "labels": [], "entities": []}, {"text": "Even though we may learn a statistically sound model on the basis of non-parametric Bayesian methods, current approaches for an SCFG still rely on exhaustive heuristic rule extraction from the wordalignment decided by derivation trees since the learned models cannot handle rules and phrases of various granularities.", "labels": [], "entities": []}, {"text": "We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) model) wherein phrases of various granularities are learned in a hierarchical back-off process.", "labels": [], "entities": [{"text": "non-parametric Inversion Transduction Grammar (ITG)", "start_pos": 60, "end_pos": 111, "type": "TASK", "confidence": 0.768424255507333}]}, {"text": "We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.6491069197654724}]}, {"text": "For efficient inference, we use a fast two-step bi-parsing approach () which basically runs in a time complexity of O(|f | 3 ).", "labels": [], "entities": []}, {"text": "Slice sampling for an SCFG) is used for efficiently sampling a derivation tree from a reduced space of possible derivations.", "labels": [], "entities": []}, {"text": "Our model achieved higher or at least comparable BLEU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the NewsCommentary corpus, and Japanese-English in the NTCIR10 corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9993195533752441}, {"text": "NewsCommentary corpus", "start_pos": 158, "end_pos": 179, "type": "DATASET", "confidence": 0.9780937433242798}, {"text": "NTCIR10 corpus", "start_pos": 209, "end_pos": 223, "type": "DATASET", "confidence": 0.9889714419841766}]}, {"text": "When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Europarl v7 corpus with significantly less grammar size.", "labels": [], "entities": [{"text": "GIZA++ pipeline", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.8515386581420898}, {"text": "Europarl v7 corpus", "start_pos": 165, "end_pos": 183, "type": "DATASET", "confidence": 0.8981824517250061}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results of translation evaluation in 100k corpus", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.940113365650177}]}, {"text": " Table 4: Results of translation evaluation in de-en  full size corpus.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.937094509601593}]}]}