{"title": [{"text": "FINET: Context-Aware Fine-Grained Named Entity Typing", "labels": [], "entities": [{"text": "FINET", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9120543599128723}, {"text": "Context-Aware Fine-Grained Named Entity Typing", "start_pos": 7, "end_pos": 53, "type": "TASK", "confidence": 0.5498840808868408}]}], "abstractContent": [{"text": "We propose FINET, a system for detecting the types of named entities in short inputs-such as sentences or tweets-with respect to WordNet's super fine-grained type system.", "labels": [], "entities": [{"text": "FINET", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9753089547157288}, {"text": "detecting the types of named entities in short inputs-such as sentences or tweets-with", "start_pos": 31, "end_pos": 117, "type": "TASK", "confidence": 0.8026723449046795}, {"text": "WordNet", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.9469798803329468}]}, {"text": "FINET generates candidate types using a sequence of multiple extrac-tors, ranging from explicitly mentioned types to implicit types, and subsequently selects the most appropriate using ideas from word-sense disambiguation.", "labels": [], "entities": []}, {"text": "FINET combats data scarcity and noise from existing systems: It does not rely on supervision in its extractors and generates training data for type selection from WordNet and other resources.", "labels": [], "entities": [{"text": "type selection", "start_pos": 143, "end_pos": 157, "type": "TASK", "confidence": 0.858752965927124}, {"text": "WordNet", "start_pos": 163, "end_pos": 170, "type": "DATASET", "confidence": 0.8746059536933899}]}, {"text": "FINET supports the most fine-grained type system so far, including types with no annotated training data.", "labels": [], "entities": [{"text": "FINET", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9322375059127808}]}, {"text": "Our experiments indicate that FINET out-performs state-of-the-art methods in terms of recall, precision, and granularity of extracted types.", "labels": [], "entities": [{"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9980864524841309}, {"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9989712238311768}]}], "introductionContent": [{"text": "Named entity typing (NET) is the task of detecting the type(s) of a named entity in context.", "labels": [], "entities": [{"text": "Named entity typing (NET)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7787966728210449}, {"text": "detecting the type(s) of a named entity in context", "start_pos": 41, "end_pos": 91, "type": "TASK", "confidence": 0.5619654109080633}]}, {"text": "For instance, given \"John plays guitar on the stage\", our goal is to infer that \"John\" is a guitarist or a musician and a person.", "labels": [], "entities": []}, {"text": "We propose FINET, a system for detecting the types of named entities in short inputs-such as sentences or tweets-with respect to WordNet's super fine-grained type system (16k types of organizations, persons and locations).", "labels": [], "entities": [{"text": "FINET", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9669437408447266}, {"text": "detecting the types of named entities in short inputs-such as sentences or tweets-with", "start_pos": 31, "end_pos": 117, "type": "TASK", "confidence": 0.7927506703596848}, {"text": "WordNet", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.9460644721984863}]}, {"text": "Named entity typing is a fundamental building block for many natural-language processing tasks.", "labels": [], "entities": [{"text": "Named entity typing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.613257517417272}]}, {"text": "NET is at the heart of information extraction methods for finding types for entities in a knowledge base 1 (KB) (.", "labels": [], "entities": [{"text": "NET", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.822207510471344}, {"text": "information extraction", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.7622979879379272}]}, {"text": "Likewise, NET aids named entity disambiguation by reducing the candidate space fora given entity mention.", "labels": [], "entities": [{"text": "NET", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.6969184279441833}, {"text": "named entity disambiguation", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.6322689255078634}]}, {"text": "Entity types are an important resource for entity-based retrieval or aggregation tasks, such as semantic search) or question answering ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.8307428956031799}]}, {"text": "Finally, type information helps to increase the semantic content of syntactic patterns ( or in open information extraction (.", "labels": [], "entities": [{"text": "open information extraction", "start_pos": 95, "end_pos": 122, "type": "TASK", "confidence": 0.6764691670735677}]}, {"text": "The extraction of explicit types has been studied in the literature, most prominently in the context of taxonomy induction ().", "labels": [], "entities": [{"text": "extraction of explicit types", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.8331079632043839}, {"text": "taxonomy induction", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.8621456921100616}]}, {"text": "Explicit types occur, for example, in phrases such as \"Steinmeier, the German Foreign Minister,\" or \"Foreign Minister Steinmeier.\"", "labels": [], "entities": [{"text": "German Foreign Minister", "start_pos": 71, "end_pos": 94, "type": "DATASET", "confidence": 0.7680714925130209}, {"text": "Foreign Minister Steinmeier", "start_pos": 101, "end_pos": 128, "type": "DATASET", "confidence": 0.8358375628789266}]}, {"text": "These explicit types are often extracted via patterns, such as the well-known Hearst patterns, and subsequently integrated into a taxonomy.", "labels": [], "entities": []}, {"text": "Patternbased methods often have high precision but low recall: Types are usually mentioned when a named entity is introduced or expected to be unknown to readers, but often are not explicitly stated.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9965671300888062}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9991400241851807}]}, {"text": "The NET problem differs from taxonomy induction in that (1) the type system is prespecified, (2) types are disambiguated, and (3) types are associated with each occurrence of named entity in context.", "labels": [], "entities": [{"text": "NET", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9478963613510132}, {"text": "taxonomy induction", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8331874907016754}]}, {"text": "Our FINET system makes use of explicit type extractions whenever possible.", "labels": [], "entities": [{"text": "FINET", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.6333850026130676}, {"text": "type extractions", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7047203183174133}]}, {"text": "But even when types are not explicitly mentioned, sentences may give clues to the correct type.", "labels": [], "entities": []}, {"text": "These clues range from almost explicit to highly implicit.", "labels": [], "entities": []}, {"text": "For example, in \"John plays soccer\", the type soccer player is almost explicit.", "labels": [], "entities": []}, {"text": "The sentence \"Pavano never even made it to the mound,\" however, only implicitly indicates that \"Pavano\" is a baseball player.", "labels": [], "entities": []}, {"text": "A key challenge in NET is to extract such implicit, context-aware types to improve recall.", "labels": [], "entities": [{"text": "NET", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.920590877532959}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9952923059463501}]}, {"text": "One way to extract implicit types is by training a supervised extractor on labeled data, in which each entity is annotated with appropriate types.", "labels": [], "entities": []}, {"text": "The key problem of this approach is that training data is scarce; this scarcity is amplified for finegrained type systems.", "labels": [], "entities": []}, {"text": "To address this problem, many existing systems generate training data by exploiting KBs as a resource (.", "labels": [], "entities": []}, {"text": "A popular approach is to train an extractor on a corpus of sentences (e.g., on Wikipedia), in which each named entity is associated with all its types in a KB.", "labels": [], "entities": []}, {"text": "The key problem with such an approach is that the so-obtained type information is oblivious to the context in which the entity was mentioned.", "labels": [], "entities": []}, {"text": "For example, in the sentences \"Klitschko is known for his powerful punches\" and \"Klitschko is the Mayor of Kiew,\" \"Klitschko\" will be associated with all its types, e.g., boxer, politician and mayor.", "labels": [], "entities": []}, {"text": "As a consequence, the labels in the training data can be misleading and negatively affect the extractors.", "labels": [], "entities": []}, {"text": "Moreover, such learned extractors are often biased towards prominent types but perform poorly on infrequent types, and they are generally problematic when types are correlated (e.g., most presidents are also graduates and authors).", "labels": [], "entities": []}, {"text": "FINET addresses the above problems by first generating a set of type candidates using multiple extractors and then selecting the most appropriate type(s).", "labels": [], "entities": []}, {"text": "To generate candidates, we make use of a sequence of extractors that range from explicit to highly implicit.", "labels": [], "entities": []}, {"text": "Implicit extractors are only used when more explicit extractors fail to produce a good type.", "labels": [], "entities": []}, {"text": "Our extractors are based on patterns, mention text, and verbal phrases.", "labels": [], "entities": []}, {"text": "To additionally extract highly implicit types, we use word vectors () trained on a large unlabeled corpus to determine the types of similar entities that appear in similar contexts.", "labels": [], "entities": []}, {"text": "This extractor is comparable to KB methods discussed above, but is unsupervised, and takes as candidates the types frequent within the related entities and contexts.", "labels": [], "entities": []}, {"text": "After type candidates have been generated, the final step of FINET selects the types that best fit the context.", "labels": [], "entities": []}, {"text": "In this step, we leverage previous work on word sense disambiguation (WSD) and resources such as WordNet glosses, and, if available, manually annotated training data.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.8308947483698527}]}, {"text": "FINET leverages ideas from existing systems and extends them by (1) handling short inputs (2) supporting a very fine-grained type hierarchy, and", "labels": [], "entities": [{"text": "FINET", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7947494387626648}]}], "datasetContent": [{"text": "We conducted an experimental study on multiple real-word datasets to compare FINET with two state-of-the-art approaches.", "labels": [], "entities": [{"text": "FINET", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.6474227905273438}]}, {"text": "FINET is used as-is; it does not require training or tuning for any specific dataset.", "labels": [], "entities": [{"text": "FINET", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.44190892577171326}]}, {"text": "All datasets, detected types, labels, and our source code are publicly available.", "labels": [], "entities": []}, {"text": "Hyena () is a representative supervised method that uses a hierarchical classifier.", "labels": [], "entities": []}, {"text": "Its features include the words in the named entity mention, in sentence and paragraph, and POS tags.", "labels": [], "entities": []}, {"text": "It performs basic co-reference resolution and marks entity mentions connected to a type in the KB using a binary feature.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.7373647391796112}]}, {"text": "Similar to, Hyena is trained on Wikipedia entities, each being annotated with its corresponding WordNet types from YAGO.", "labels": [], "entities": [{"text": "YAGO", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.5700735449790955}]}, {"text": "Hyena's type system is restricted to 505 WordNet types with top categories artifact-1, event-1, person-1, location-1, and organization-1.", "labels": [], "entities": [{"text": "Hyena", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8879952430725098}]}, {"text": "Hyena outperformed a number of previous systems.", "labels": [], "entities": [{"text": "Hyena", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.86454176902771}]}, {"text": "We used Hyena via its web service (.", "labels": [], "entities": [{"text": "Hyena", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.9574835300445557}]}, {"text": "Pearl () is a semisupervised system that leverages a repository of 300k relational patterns ().", "labels": [], "entities": []}, {"text": "Subjects and objects of each pattern carry type information.", "labels": [], "entities": []}, {"text": "Pearl types named entity mentions by the most likely type according to its pattern database.", "labels": [], "entities": []}, {"text": "Pearl's type system is based on around 200 \"interesting\" WordNet types.", "labels": [], "entities": []}, {"text": "We ran Pearl in its hard setting, which performed best.", "labels": [], "entities": []}, {"text": "We ran FINET in two configurations: (1) with KB lookup, (2) without the KB lookup.", "labels": [], "entities": [{"text": "FINET", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.7314121723175049}]}, {"text": "This allows us to estimate the extent to which referring to a KB helps.", "labels": [], "entities": []}, {"text": "Note that the corpus-based extractor makes use of the KB in both settings.", "labels": [], "entities": []}, {"text": "We used three different datasets representing real-world use cases.", "labels": [], "entities": []}, {"text": "We created two datasets, New York Times and Twitter, and sampled a subset of the CoNLL data, which provides gold annotations for CG types.", "labels": [], "entities": [{"text": "New York Times", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9123824238777161}, {"text": "CoNLL data", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.9444710612297058}]}, {"text": "We did not consider datasets such as FIGER ( or BBN () because they are not suitable for very fine-grained typing.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.7863607406616211}, {"text": "BBN", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.5370252132415771}]}, {"text": "New York Times consists of 500 random sentences from the New York Times corpus), year 2007; we selected only sentences that contained at least one named entity according to the Stanford CoreNLP 4.4.1 tool.", "labels": [], "entities": [{"text": "New York Times consists of 500 random sentences from the New York Times corpus", "start_pos": 0, "end_pos": 78, "type": "DATASET", "confidence": 0.7722814125674111}]}, {"text": "We sampled 500 sentences from CoNLL, a collection of newswires with manually annotated entities with CG types labels.", "labels": [], "entities": []}, {"text": "We directly used the annotations in our evaluation.", "labels": [], "entities": []}, {"text": "The sentences tend to be short and sometimes non-verbal (e.g., \"Jim Grabb ( U.S. ) vs. Sandon Stolle ( Australia )\").", "labels": [], "entities": []}, {"text": "Most entities are prominent and likely to be found in our KB (and the one of existing methods).", "labels": [], "entities": []}, {"text": "We collected the first 100 tweets with named entities retrieved by the Twitter API.", "labels": [], "entities": []}, {"text": "FINET's type system consists of more than 16k types with top categories persons, locations and organizations.", "labels": [], "entities": [{"text": "FINET", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9230305552482605}]}, {"text": "We used the mapping between these top categories and WordNet described in Sec.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9580546021461487}]}, {"text": "Hyena and Pearl use 505 and 200 WordNet types, resp., which is significantly smaller.", "labels": [], "entities": []}, {"text": "To compare the performance across different granularities, we classified each type as coarse-grained (CG), fine-grained (FG) or super fine-grained (SFG).", "labels": [], "entities": []}, {"text": "The CG types were artifact-1, event-1, person-1, location-1 and organization-1.", "labels": [], "entities": []}, {"text": "The FG types were those included in Pearl.", "labels": [], "entities": [{"text": "FG", "start_pos": 4, "end_pos": 6, "type": "DATASET", "confidence": 0.644399881362915}, {"text": "Pearl", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.9050881862640381}]}, {"text": "All remaining types were considered SFG.", "labels": [], "entities": [{"text": "SFG", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.8068839311599731}]}, {"text": "All extractions by all systems were independently evaluated by two labelers.", "labels": [], "entities": []}, {"text": "We adopted a pessimistic view, i.e., we treat an extraction as correct only if it was labeled correct by both labelers.", "labels": [], "entities": []}, {"text": "The Cohen's kappa measure ranged 0.54-0.86, indicating a substantial agreement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Summary of results", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7430906295776367}]}, {"text": " Table 4: Per-extractor performance NYT", "labels": [], "entities": [{"text": "NYT", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.5620822906494141}]}]}