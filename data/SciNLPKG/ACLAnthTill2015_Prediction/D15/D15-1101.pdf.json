{"title": [{"text": "C3EL: A Joint Model for Cross-Document Co-Reference Resolution and Entity Linking", "labels": [], "entities": [{"text": "Cross-Document Co-Reference Resolution", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.6807144582271576}, {"text": "Entity Linking", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.7301443219184875}]}], "abstractContent": [{"text": "Cross-document co-reference resolution (CCR) computes equivalence classes over textual mentions denoting the same entity in a document corpus.", "labels": [], "entities": [{"text": "Cross-document co-reference resolution (CCR", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7240717351436615}]}, {"text": "Named-entity linking (NEL) disambiguates mentions onto entities present in a knowledge base (KB) or maps them to null if not present in the KB.", "labels": [], "entities": [{"text": "Named-entity linking (NEL) disambiguates mentions onto entities present in a knowledge base (KB)", "start_pos": 0, "end_pos": 96, "type": "TASK", "confidence": 0.8550403573933769}]}, {"text": "Traditionally, CCR and NEL have been addressed separately.", "labels": [], "entities": []}, {"text": "However, such approaches miss out on the mutual syn-ergies if CCR and NEL were performed jointly.", "labels": [], "entities": []}, {"text": "This paper proposes C3EL, an unsuper-vised framework combining CCR and NEL for jointly tackling both problems.", "labels": [], "entities": []}, {"text": "C3EL incorporates results from the CCR stage into NEL, and vice versa: additional global context obtained from CCR improves the feature space and performance of NEL, while NEL in turn provides distant KB features for already disambiguated mentions to improve CCR.", "labels": [], "entities": []}, {"text": "The CCR and NEL steps are interleaved in an iterative algorithm that focuses on the highest-confidence still un-resolved mentions in each iteration.", "labels": [], "entities": []}, {"text": "Experimental results on two different corpora, news-centric and web-centric, demonstrate significant gains over state-of-the-art base-lines for both CCR and NEL.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the advent of large knowledge bases (KB) like DBpedia, YAGO, Freebase, and others, entities (people, places, organizations, etc.) along with their attributes and relationships form the basis of smart applications like search, analytics, recommendations, question answering, and more.", "labels": [], "entities": [{"text": "question answering", "start_pos": 259, "end_pos": 277, "type": "TASK", "confidence": 0.8847624361515045}]}, {"text": "The major task that arises in both the KB construction process and the entity-centric applications involves precise recognition, resolution, and linking of named entities distributed across web pages, news articles, and social media.", "labels": [], "entities": [{"text": "KB construction", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.8597496747970581}, {"text": "recognition, resolution, and linking of named entities distributed across web pages, news articles", "start_pos": 116, "end_pos": 214, "type": "TASK", "confidence": 0.7847282271832228}]}, {"text": "Named Entity Recognition (NER) deals with the identification of entity mentions in a text and their classification into coarse-grained semantic types (person, location, etc.)", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8074925194183985}, {"text": "identification of entity mentions in a text and their classification into coarse-grained semantic types (person, location, etc.)", "start_pos": 46, "end_pos": 174, "type": "Description", "confidence": 0.6585260006514463}]}, {"text": "(. This involves segmentation of token sequences to obtain mention boundaries, and mapping relevant token spans to pre-defined entity categories.", "labels": [], "entities": []}, {"text": "For example, NER on the text Einstein won the Nobel Prize identifies the mentions \"Einstein\" and \"Nobel Prize\" and marks them as person and misc type, respectively.", "labels": [], "entities": [{"text": "NER", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.6296260952949524}]}, {"text": "Named Entity Linking (NEL) 1 involves the disambiguation of textual mentions, based on context and semantic information, and their mapping to proper entities in a KB.", "labels": [], "entities": [{"text": "Named Entity Linking (NEL) 1", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7865232697554997}]}, {"text": "For example, in the above text, the mention \"Einstein\" is linked to the physicist Albert Einstein.", "labels": [], "entities": []}, {"text": "Entity Co-reference Resolution (CR)) is essentially a clustering task to identify mentions (and anaphoras) within a document referring to the same entity, thus computing equivalence classes or mention groups.", "labels": [], "entities": [{"text": "Entity Co-reference Resolution (CR))", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8046426673730215}]}, {"text": "For example, mentions Albert Einstein and Nobel laureate Einstein both refer to the same entity German physicist Albert Einstein, but are different from the mention Hans Albert Einstein.", "labels": [], "entities": []}, {"text": "When CR is extended to an entire text corpus, in order to generate equivalence classes of co-referring mentions across documents, the task is known as Cross-document Co-reference Resolution (CCR).", "labels": [], "entities": [{"text": "Cross-document Co-reference Resolution (CCR)", "start_pos": 151, "end_pos": 195, "type": "TASK", "confidence": 0.7583491504192352}]}, {"text": "Note that CCR is not the same as merely concatenating all documents in the corpus and utilizing existing CR methods.", "labels": [], "entities": []}, {"text": "The linguistic diversity across documents and high computational cost for huge numbers of mentions in the corpus would typically make such a CR-based simulation perform poorly.", "labels": [], "entities": []}, {"text": "Neither CR nor CCR links mention groups to corresponding KB entities.", "labels": [], "entities": [{"text": "CR", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.7372704148292542}]}, {"text": "Thus, they represent both in-KB entities and out-of-KB entities (e.g., long-tail or emerging entities that do not have a Wikipedia article) in the same way.", "labels": [], "entities": []}, {"text": "State-of-the-Art and its Limitations: Established CR methods rely on rule-based methods or supervised learning techniques on syntactic paths between mentions, semantic compatibility, and other linguistic features, with additional use of distant features from KBs (.", "labels": [], "entities": []}, {"text": "Modern cluster-ranking) and multi-sieve methods involve incremental expansion of mention groups by considering semantic types and Wikipedia categories.", "labels": [], "entities": []}, {"text": "CCR methods utilize transitivity-aware clustering techniques), by considering mention-mention similarities) along with features extracted from external KBs.", "labels": [], "entities": []}, {"text": "NEL methods often harness the semantic similarity between mentions and entities and also among candidate entities for different mentions (in Wikipedia or other KBs) for contextualization and coherence disambiguation).", "labels": [], "entities": []}, {"text": "However, in the absence of CR mention groups, NEL has limited context and is bound to miss out on certain kinds of difficult cases.", "labels": [], "entities": [{"text": "NEL", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9317283034324646}]}, {"text": "Although NER, CR, CCR and NEL involve closely related tasks and their tighter integration has been shown to be promising, they have mostly been explored in isolation.", "labels": [], "entities": []}, {"text": "Recently, several joint models have been proposed for CR-NER, CR-NEL (, and NER-CR-NEL.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, no method exists for jointly handling CCR and NEL on large text corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we empirically study the performance of C3EL against various state-of-the-art methods.", "labels": [], "entities": []}, {"text": "We analyze the individual gains in CCR and NEL due to the joint modeling.", "labels": [], "entities": [{"text": "NEL", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.7920947074890137}]}, {"text": "Datasets: We use the following 2 publicly available corpora: \u2022 EventCorefBank (ECB) corpus  Million mentions associated with 1.29 Million distinct entities to form our corpus.", "labels": [], "entities": [{"text": "EventCorefBank (ECB) corpus", "start_pos": 63, "end_pos": 90, "type": "DATASET", "confidence": 0.7551900565624237}]}, {"text": "For NEL ground-truth construction, we link the entities to their Wikipedia pages (using Freebase's \"on the web\" property).", "labels": [], "entities": [{"text": "NEL ground-truth construction", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.8948955734570821}]}, {"text": "Since no explicit annotations of inter-document entity co-references exists, we consider two mentions (in different documents) to co-refer if they are linked with the same Freebase entity.", "labels": [], "entities": []}, {"text": "Evaluation: To assess the output quality of C3EL we use the following established metrics: \u2022 B 3 F 1 score: measures the F1 score as the harmonic mean of average precision and recall computed overall mention groups in the final equivalence classes.", "labels": [], "entities": [{"text": "B 3 F 1 score", "start_pos": 93, "end_pos": 106, "type": "METRIC", "confidence": 0.9634608507156373}, {"text": "F1 score", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9870210587978363}, {"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.903278648853302}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9990978240966797}]}, {"text": "Precision (for a mention group) represents the ratio of the number of correctly reported coreferences (or linking) to the actual number; while recall computes the fraction of the goldstandard annotations correctly identified.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9901276230812073}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9972262978553772}]}, {"text": "\u2022 \u03c6 3 \u2212 CEAF score (: provides an alternate F1 score computed as in the B 3 measure; but calculates precision and recall of mention groups using the best 1-to-1 mapping (i.e., mapping with maximum mention overlap) between the resultant equivalence classes and those in the ground truth.", "labels": [], "entities": [{"text": "\u03c6 3 \u2212 CEAF score", "start_pos": 2, "end_pos": 18, "type": "METRIC", "confidence": 0.9063109755516052}, {"text": "F1 score", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9797230958938599}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9992671608924866}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9988651275634766}]}, {"text": "Normalization with the number of mentions for each of the resultant classes yields the \u03c6 4 -CEAF score.", "labels": [], "entities": [{"text": "\u03c6 4 -CEAF score", "start_pos": 87, "end_pos": 102, "type": "METRIC", "confidence": 0.9243879199028016}]}, {"text": "We consider only the 3 most notable mention types: person (PER), location (LOC), and organization (ORG) -accounting for 99.7% of entities present in the ECB corpus and 96.3% of our ClueWeb09 corpus.", "labels": [], "entities": [{"text": "organization (ORG)", "start_pos": 85, "end_pos": 103, "type": "METRIC", "confidence": 0.6850955784320831}, {"text": "ECB corpus", "start_pos": 153, "end_pos": 163, "type": "DATASET", "confidence": 0.9832810461521149}, {"text": "ClueWeb09 corpus", "start_pos": 181, "end_pos": 197, "type": "DATASET", "confidence": 0.9587445855140686}]}, {"text": "All experiments were conducted on a 4 core Intel i5 2.50 GHz processor with 8GB RAM running Ubuntu 12.04 LTS.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: CCR performance (%) comparison on ECB", "labels": [], "entities": [{"text": "ECB", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.9340861439704895}]}, {"text": " Table 3: CCR results on ECB", "labels": [], "entities": [{"text": "ECB", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.9049781560897827}]}, {"text": " Table 4: CCR results on ECB for mention types", "labels": [], "entities": [{"text": "ECB", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.914972186088562}]}, {"text": " Table 1: C3EL performance (a) in CCR with \u03b4s, (b) in out-of-KB NEL with \u03b4w, and (c) in CCR with \u03c4", "labels": [], "entities": []}, {"text": " Table 5: CCR results on ClueWeb09-FACC1", "labels": [], "entities": [{"text": "ClueWeb09-FACC1", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.7005223035812378}]}, {"text": " Table 6: NEL performance (%) comparison on ECB", "labels": [], "entities": [{"text": "NEL", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9549666047096252}, {"text": "ECB", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.937269926071167}]}, {"text": " Table 6.  C3EL attains comparable performance (\u223c 85%  precision) to that of AIDA for well-known entity- mentions present in KB; albeit with a few mentions  remaining unlinked due to our cautious link val- idation (using \u03c4 ) approach. However, the use of  \u03c4 reduces aggressive KB linking to provide a sig- nificant 15% improvement (over AIDA) in precise  detection of new/emerging entities absent in KB.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9982646107673645}, {"text": "precise  detection of new/emerging entities absent in KB", "start_pos": 346, "end_pos": 402, "type": "TASK", "confidence": 0.6948273926973343}]}, {"text": " Table 7: NEL results (%) on ClueWeb09-FACC1 (statistical", "labels": [], "entities": [{"text": "NEL", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6740931272506714}]}, {"text": " Table 8: Joint \"Simulated\" results on ECB subset for (a) CCR, and (b) NEL (statistical significance p < 0.05)", "labels": [], "entities": [{"text": "Simulated\"", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.8765231072902679}, {"text": "NEL", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.7855514883995056}]}, {"text": " Table 9: Joint \"Simulated\" results on ClueWeb09 subset for (a) CCR, and (b) NEL", "labels": [], "entities": [{"text": "Simulated\"", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.8891205489635468}, {"text": "NEL", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.5003559589385986}]}, {"text": " Table 10: CCR and NEL results (%) of C3EL for different baseline variations", "labels": [], "entities": [{"text": "NEL", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.7499115467071533}]}]}