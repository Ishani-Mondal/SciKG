{"title": [{"text": "An Empirical Comparison Between N-gram and Syntactic Language Models for Word Ordering", "labels": [], "entities": []}], "abstractContent": [{"text": "Syntactic language models and N-gram language models have both been used in word ordering.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.8149711787700653}]}, {"text": "In this paper, we give an empirical comparison between N-gram and syntactic language models on word order task.", "labels": [], "entities": []}, {"text": "Our results show that the quality of automatically-parsed training data has a relatively small impact on syntactic models.", "labels": [], "entities": []}, {"text": "Both of syntactic and N-gram models can benefit from large-scale raw text.", "labels": [], "entities": []}, {"text": "Compared with N-gram models, syntactic models give overall better performance, but they require much more training time.", "labels": [], "entities": []}, {"text": "In addition, the two models lead to different error distributions in word ordering.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.7345416843891144}]}, {"text": "A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark.", "labels": [], "entities": []}], "introductionContent": [{"text": "N-gram language models have been used in a wide range of the generation tasks, such as machine translation (), text summarization () and realization).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8134517073631287}, {"text": "text summarization", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7769652605056763}]}, {"text": "Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text.", "labels": [], "entities": []}, {"text": "More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation), syntactic analysis () and tree linearization ( ).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.769676148891449}, {"text": "syntactic analysis", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.8407880365848541}]}, {"text": "Compared with N-gram models, syntactic models capture rich structural information, and can be more effective in improving the fluency of large constituents, long-range dependencies and overall sentential grammaticality.", "labels": [], "entities": []}, {"text": "However, Syntactic models require annotated syntactic structures for training, which are expensive to obtain manually.", "labels": [], "entities": []}, {"text": "In addition, they can be slower compared to Ngram models.", "labels": [], "entities": []}, {"text": "In this paper, we make an empirical comparison between syntactic and N-gram language models on the task of word ordering (, which is to order a set of input words into a grammatical and fluent sentence.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 107, "end_pos": 120, "type": "TASK", "confidence": 0.7242973148822784}]}, {"text": "The task can be regarded as an abstract language modeling problem, although methods have been explored extending it for tree linearization, broader text generation ( ) and machine translation ( . We choose the model of as the syntactic language model.", "labels": [], "entities": [{"text": "text generation", "start_pos": 148, "end_pos": 163, "type": "TASK", "confidence": 0.719915971159935}, {"text": "machine translation", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.8230379223823547}]}, {"text": "There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on word orders.", "labels": [], "entities": []}, {"text": "As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks.", "labels": [], "entities": []}, {"text": "The second type models syntactic structures incrementally, thereby can be used to directly score surface orders (.", "labels": [], "entities": []}, {"text": "We choose the discriminative model of, which gives state-of-the-art results for word ordering.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.7730050981044769}]}, {"text": "We try to answer the following research questions by comparing the syntactic model and the Ngram model using the same search algorithm.", "labels": [], "entities": []}, {"text": "\u2022 What is the influence of automaticallyparsed training data on the performance of syntactic models.", "labels": [], "entities": []}, {"text": "Because manual syntactic annotations are relatively limited and highly expensive, it is necessary to use large-scale automatically-parsed sentences for training syntactic language models.", "labels": [], "entities": []}, {"text": "As a result, the syntactic structures that a word ordering system learns can be inaccurate.", "labels": [], "entities": []}, {"text": "However, this might not affect Initial State Induction Rules: Figure 1: Deduction system for transition-based linearization.", "labels": [], "entities": []}, {"text": "the quality of the synthesized output, which is a string only.", "labels": [], "entities": []}, {"text": "We quantitatively study the influence of parsing accuracy of syntactic training data on word ordering output.", "labels": [], "entities": [{"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9549974799156189}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.652504026889801}, {"text": "word ordering output", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.7891138990720113}]}, {"text": "\u2022 What is the influence of data scale on the performance.", "labels": [], "entities": []}, {"text": "N-gram language models can be trained efficiently overlarge numbers of raw sentences.", "labels": [], "entities": []}, {"text": "In contrast, syntactic language models can be much slower to train due to rich features.", "labels": [], "entities": []}, {"text": "We compare the output quality of the two models on different scales of training data, and also on different amounts of training time.", "labels": [], "entities": []}, {"text": "\u2022 What are the errors characteristics of each model.", "labels": [], "entities": []}, {"text": "Syntactic language models can potentially be better in capturing larger constituents and overall sentence structures.", "labels": [], "entities": []}, {"text": "However, compared with N-gram models, little work has been done to quantify the difference between the two models.", "labels": [], "entities": []}, {"text": "We characterise the outputs using a set of different measures, and show empirically the relative strength and weakness of each model.", "labels": [], "entities": []}, {"text": "\u2022 What is the effect of model combination.", "labels": [], "entities": []}, {"text": "Finally, because the two models make different types of errors, they can be combined to give better outputs.", "labels": [], "entities": []}, {"text": "We develop a combined model by discretizing probability from N-gram model, and using them as features in the syntactic model.", "labels": [], "entities": []}, {"text": "The combined model gives the best results in a standard benchmark.", "labels": [], "entities": []}], "datasetContent": [{"text": "We follow previous work and use the BLEU metric () for evaluation.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9782953560352325}]}, {"text": "Since BLEU only scores N-gram precisions, it can be in favour of N-gram language models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.99284428358078}, {"text": "precisions", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.8751094937324524}]}, {"text": "We additionally use METEOR 3) to evaluate the system performances.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9412186145782471}]}, {"text": "The BLEU metric measures the fluency of generated sentence without considering long range ordering.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9975003600120544}]}, {"text": "The ME-TEOR metric can potentially fix this problem using a set of mapping between generated sentences and references to evaluate distortion.", "labels": [], "entities": []}, {"text": "The following example illustrates the difference between BLEU and METEOR on long range reordering, where the reference is (1)  the METEOR gives a score of 61.34 out of 100.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9984810948371887}, {"text": "METEOR", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.7009981274604797}, {"text": "METEOR", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9356963038444519}]}, {"text": "This is because that METEOR is based on explicit word-to-word matches over the whole sentence.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.8749748468399048}]}, {"text": "For word ordering, word-to-word matches are unique, which facilitates METEOR evaluation between generated sentences and references.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.7739246189594269}, {"text": "METEOR evaluation", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7180672585964203}]}, {"text": "As can bee seen from the example, long range distortion can highly influence the METEOR scores making the METEOR metric more suitable for evaluating word ordering distortions.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.7759159207344055}, {"text": "word ordering distortions", "start_pos": 149, "end_pos": 174, "type": "TASK", "confidence": 0.7464210490385691}]}], "tableCaptions": [{"text": " Table 4: Parsing accuracy settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9308027029037476}]}, {"text": " Table 5: Influence result of parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.977784276008606}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9567370414733887}]}, {"text": " Table 6: NLM feature templates.", "labels": [], "entities": []}, {"text": " Table 7: Final results on various domains.", "labels": [], "entities": []}, {"text": " Table 8: Final results of all systems, where \"*\"  means that the system uses extra POS input.", "labels": [], "entities": []}]}