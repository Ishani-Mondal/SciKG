{"title": [{"text": "Learning Timeline Difference for Text Categorization", "labels": [], "entities": [{"text": "Learning Timeline Difference", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5756245950857798}, {"text": "Text Categorization", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.6782494634389877}]}], "abstractContent": [{"text": "This paper addresses text categorization problem that training data may derive from a different time period from the test data.", "labels": [], "entities": []}, {"text": "We present a learning framework which extends a boosting technique to learn accurate model for timeline adaptation.", "labels": [], "entities": [{"text": "timeline adaptation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.6805594712495804}]}, {"text": "The results showed that the method was comparable to the current state-of-the-art biased-SVM method, especially the method is effective when the creation time period of the test data differs greatly from the training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text categorization supports and improves several tasks such as creating digital libraries, information retrieval, and even helping users to interact with search engines (.", "labels": [], "entities": [{"text": "Text categorization", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7497708201408386}, {"text": "information retrieval", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.7888710498809814}]}, {"text": "A growing number of machine learning (ML) techniques have been applied to the text categorization task (.", "labels": [], "entities": [{"text": "text categorization task", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.8247528473536173}]}, {"text": "Each document is represented using a vector of features/terms.", "labels": [], "entities": []}, {"text": "Then, the documents with category label are used to train classifiers.", "labels": [], "entities": []}, {"text": "Once category models are trained, each test document is classified by using these models.", "labels": [], "entities": []}, {"text": "A basic assumption in the categorization task is that the distributions of terms between training and test documents are identical.", "labels": [], "entities": []}, {"text": "When the assumption does not hold, the classification accuracy is worse.", "labels": [], "entities": [{"text": "classification", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.8871073722839355}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9677212238311768}]}, {"text": "However, it is often the case that the term distribution in the training data is different from that of the test data when the training data may drive from a different time period from the test data.", "labels": [], "entities": []}, {"text": "Manual annotation of tagged new data is very expensive and timeconsuming.", "labels": [], "entities": [{"text": "Manual annotation of tagged new", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.587046331167221}]}, {"text": "The methodology for accurate classification of the new test data by making the maximum use of tagged old data is needed in learning techniques.", "labels": [], "entities": []}, {"text": "In this paper, we present a method for text categorization that minimizes the impact of temporal effects.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7737672924995422}]}, {"text": "Our approach extends a boosting technique to learn accurate model for timeline adaptation.", "labels": [], "entities": [{"text": "timeline adaptation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7416829764842987}]}, {"text": "We used two types of labeled training data: One is the same creation time period with the test data.", "labels": [], "entities": []}, {"text": "Another is different creation time period with the test data.", "labels": [], "entities": []}, {"text": "We call the former sameperiod training, and the latter diff-period training data.", "labels": [], "entities": []}, {"text": "For the same-period training data, the learner shows the same behavior as the boosting.", "labels": [], "entities": []}, {"text": "In contrast, for diff-period training data, once they are wrongly predicted by the learned model, these data would be useless to classify test data.", "labels": [], "entities": []}, {"text": "We decreased the weights of these data by applying Gaussian function in order to weaken their impacts.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our TABoost by using the Mainichi Japanese newspaper documents.", "labels": [], "entities": [{"text": "TABoost", "start_pos": 17, "end_pos": 24, "type": "DATASET", "confidence": 0.4561310112476349}, {"text": "Mainichi Japanese newspaper documents", "start_pos": 38, "end_pos": 75, "type": "DATASET", "confidence": 0.9599113315343857}]}, {"text": "We choose the Mainichi Japanese newspaper corpus from 1991 to 2012.", "labels": [], "entities": [{"text": "Mainichi Japanese newspaper corpus", "start_pos": 14, "end_pos": 48, "type": "DATASET", "confidence": 0.9678438752889633}]}, {"text": "The corpus consists of 2,883,623 documents organized into 16 categories.", "labels": [], "entities": []}, {"text": "We selected 8 categories, \"International(Int)\", \"Economy(Eco)\", \"Home\", \"Culture\", \"Reading\", \"Arts\", \"Sports\", and \"Local news(Local)\", each of which has sufficient number of documents.", "labels": [], "entities": []}, {"text": "All documents were tagged by using a morphological analyzer Chasen) and selected noun words.", "labels": [], "entities": []}, {"text": "The total number of documents assigned to these categories are 787,518.", "labels": [], "entities": []}, {"text": "For each category within each year, we divided documents into three folds: 2% of documents are used as the same-period training data, 50% of documents are the diff-period training data, and the remains are used to test our classification method.", "labels": [], "entities": []}, {"text": "2 We used LIBLINEAR) as a basic learner in the experiments.", "labels": [], "entities": [{"text": "LIBLINEAR", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9859419465065002}]}, {"text": "We compared our method, TABoost with four baselines: (1) TABoost with the same-period training data only (TAB s), (2) SVM, (3) TrAdaBoost (, and (4) biased-SVM ( by SVM-light.", "labels": [], "entities": [{"text": "TABoost", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.6911869049072266}, {"text": "TABoost", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.79107266664505}, {"text": "TrAdaBoost", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.9269523024559021}]}, {"text": "TAB sis the same behavior as boosting.", "labels": [], "entities": [{"text": "TAB", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.49118444323539734}]}, {"text": "TrAdaBoost (TrAdaB) is presented by.", "labels": [], "entities": [{"text": "TrAdaBoost (TrAdaB)", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.7207738906145096}]}, {"text": "Biased-SVM (b-SVM) is known as the state-of-the-art SVMs method, and often used for comparison.", "labels": [], "entities": []}, {"text": "Similar to SVM, for biased-SVM, we used the first two folds as a training data, and classified test documents directly, i.e. we used closed data.", "labels": [], "entities": []}, {"text": "We empirically selected values of two parameters, \"c\" (trade-off between training error and margin) and \"j\", i.e. cost (cost-factor, by which training errors on positive instances) that optimized result obtained by classification of test documents.", "labels": [], "entities": [{"text": "training error", "start_pos": 73, "end_pos": 87, "type": "METRIC", "confidence": 0.8845979869365692}, {"text": "margin", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.5427724123001099}]}, {"text": "Similar to (, \"c\" is searched in steps of 0.02 from 0.01 to 0.61.", "labels": [], "entities": []}, {"text": "\"j\" is searched in steps of 5 from 1 to 200.", "labels": [], "entities": []}, {"text": "As a result, we set c and j to 0.01 and 10, respectively.", "labels": [], "entities": []}, {"text": "To make comparisons fair, all five methods including our method are based on linear kernel.", "labels": [], "entities": []}, {"text": "Throughout the experiments, the number of iterations is set to 100.", "labels": [], "entities": []}, {"text": "We used error rate as an evaluation measure).", "labels": [], "entities": [{"text": "error rate", "start_pos": 8, "end_pos": 18, "type": "METRIC", "confidence": 0.974085658788681}]}], "tableCaptions": [{"text": " Table 1: The error rates across categories", "labels": [], "entities": [{"text": "error", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9900188446044922}]}]}