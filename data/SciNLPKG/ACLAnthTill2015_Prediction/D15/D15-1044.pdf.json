{"title": [{"text": "A Neural Attention Model for Sentence Summarization", "labels": [], "entities": [{"text": "Neural Attention", "start_pos": 2, "end_pos": 18, "type": "TASK", "confidence": 0.7099971026182175}, {"text": "Sentence Summarization", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.9470489323139191}]}], "abstractContent": [{"text": "Summarization based on text extraction is inherently limited, but generation-style ab-stractive methods have proven challenging to build.", "labels": [], "entities": [{"text": "text extraction", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.765177994966507}]}, {"text": "In this work, we propose a fully data-driven approach to abstrac-tive sentence summarization.", "labels": [], "entities": [{"text": "abstrac-tive sentence summarization", "start_pos": 57, "end_pos": 92, "type": "TASK", "confidence": 0.5799272656440735}]}, {"text": "Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence.", "labels": [], "entities": []}, {"text": "While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data.", "labels": [], "entities": []}, {"text": "The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.", "labels": [], "entities": [{"text": "DUC-2004 shared task", "start_pos": 53, "end_pos": 73, "type": "DATASET", "confidence": 0.8176241119702657}]}], "introductionContent": [{"text": "Summarization is an important challenge of natural language understanding.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9805354475975037}, {"text": "natural language understanding", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.6674810846646627}]}, {"text": "The aim is to produce a condensed representation of an input text that captures the core meaning of the original.", "labels": [], "entities": []}, {"text": "Most successful summarization systems utilize extractive approaches that crop out and stitch together portions of the text to produce a condensed version.", "labels": [], "entities": [{"text": "summarization", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9797795414924622}]}, {"text": "In contrast, abstractive summarization attempts to produce a bottom-up summary, aspects of which may not appear as part of the original.", "labels": [], "entities": []}, {"text": "We focus on the task of sentence-level summarization.", "labels": [], "entities": [{"text": "sentence-level summarization", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.4731304496526718}]}, {"text": "While much work on this task has looked at deletion-based sentence compression techniques (, among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.7143216282129288}]}, {"text": "Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints () or with syntactic transformations of the input text (Cohn and: Example output of the attention-based summarization (ABS) system.", "labels": [], "entities": [{"text": "summarization", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.664242148399353}, {"text": "attention-based summarization (ABS)", "start_pos": 198, "end_pos": 233, "type": "TASK", "confidence": 0.7406546711921692}]}, {"text": "The heatmap represents a soft alignment between the input (right) and the generated summary (top).", "labels": [], "entities": []}, {"text": "The columns represent the distribution over the input after generating each word.).", "labels": [], "entities": []}, {"text": "These approaches are described in more detail in Section 6.", "labels": [], "entities": []}, {"text": "We instead explore a fully data-driven approach for generating abstractive summaries.", "labels": [], "entities": []}, {"text": "Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.7024308840433756}]}, {"text": "Our encoder is modeled off of the attention-based encoder of in that it learns a latent soft alignment over the input text to help inform the summary (as shown in).", "labels": [], "entities": []}, {"text": "Crucially both the encoder and the generation model are trained jointly on the sentence summarization task.", "labels": [], "entities": [{"text": "sentence summarization task", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.7783715724945068}]}, {"text": "The model is described in detail in Section 3.", "labels": [], "entities": []}, {"text": "Our model also incorporates a beam-search decoder as well as additional features to model extractive elements; these aspects are discussed in Sections 4 and 5.", "labels": [], "entities": []}, {"text": "This approach to summarization, which we call Attention-Based Summarization (ABS), incorporates less linguistic structure than comparable abstractive summarization approaches, but can easily Input (x1, . .", "labels": [], "entities": [{"text": "summarization", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.9894031286239624}, {"text": "Attention-Based Summarization (ABS)", "start_pos": 46, "end_pos": 81, "type": "TASK", "confidence": 0.8036985516548156}, {"text": "Input", "start_pos": 191, "end_pos": 196, "type": "METRIC", "confidence": 0.9842782616615295}]}, {"text": "First sentence of article: russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism Output (y1, . .", "labels": [], "entities": []}, {"text": "Generated headline: russia calls for joint front against terrorism \u21d0 g(terrorism, x, for, joint, front, against) Figure 2: Example input sentence and the generated summary.", "labels": [], "entities": []}, {"text": "The score of generating yi+1 (terrorism) is based on the context yc (for . .", "labels": [], "entities": []}, {"text": "against) as well as the input x1 . .", "labels": [], "entities": []}, {"text": "Note that the summary generated is abstractive which makes it possible to generalize (russian defense minister to russia) and paraphrase (for combating to against), in addition to compressing (dropping the creation of), see Jing (2002) fora survey of these editing operations.", "labels": [], "entities": []}, {"text": "scale to train on a large amount of data.", "labels": [], "entities": []}, {"text": "Since our system makes no assumptions about the vocabulary of the generated summary it can be trained directly on any document-summary pair.", "labels": [], "entities": []}, {"text": "1 This allows us to train a summarization model for headline-generation on a corpus of article pairs from Gigaword () consisting of around 4 million articles.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.9379724860191345}]}, {"text": "An example of generation is given in, and we discuss the details of this task in Section 7.", "labels": [], "entities": []}, {"text": "To test the effectiveness of this approach we run extensive comparisons with multiple abstractive and extractive baselines, including traditional syntax-based systems, integer linear programconstrained systems, information-retrieval style approaches, as well as statistical phrase-based machine translation.", "labels": [], "entities": [{"text": "statistical phrase-based machine translation", "start_pos": 262, "end_pos": 306, "type": "TASK", "confidence": 0.5481573790311813}]}, {"text": "Section 8 describes the results of these experiments.", "labels": [], "entities": []}, {"text": "Our approach outperforms a machine translation system trained on the same large-scale dataset and yields a large improvement over the highest scoring system in the DUC-2004 competition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7399983704090118}, {"text": "DUC-2004 competition", "start_pos": 164, "end_pos": 184, "type": "DATASET", "confidence": 0.9151789546012878}]}], "datasetContent": [{"text": "We experiment with our attention-based sentence summarization model on the task of headline generation.", "labels": [], "entities": [{"text": "attention-based sentence summarization", "start_pos": 23, "end_pos": 61, "type": "TASK", "confidence": 0.5785515308380127}, {"text": "headline generation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.8594311773777008}]}, {"text": "In this section we describe the corpora used for this task, the baseline methods we compare with, and implementation details of our approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results on the main summary tasks on various ROUGE metrics . Baseline models are described in", "labels": [], "entities": []}, {"text": " Table 1. We  run experiments both using the DUC-2004 eval- uation data set (500 sentences, 4 references, 75  bytes) with all systems and a randomly held-out", "labels": [], "entities": [{"text": "DUC-2004 eval- uation data set", "start_pos": 45, "end_pos": 75, "type": "DATASET", "confidence": 0.8626469671726227}]}, {"text": " Table 2: Perplexity results on the Gigaword validation", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9654171466827393}, {"text": "Gigaword validation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.5987832546234131}]}, {"text": " Table 3: ROUGE scores on DUC-2003 development data", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9783526659011841}, {"text": "DUC-2003 development data", "start_pos": 26, "end_pos": 51, "type": "DATASET", "confidence": 0.9121857285499573}]}]}