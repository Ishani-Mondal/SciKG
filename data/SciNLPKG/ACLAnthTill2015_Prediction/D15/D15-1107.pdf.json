{"title": [{"text": "Auto-Sizing Neural Networks: With Applications to n-gram Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Neural networks have been shown to improve performance across a range of natural-language tasks.", "labels": [], "entities": []}, {"text": "However, designing and training them can be complicated.", "labels": [], "entities": []}, {"text": "Frequently, researchers resort to repeated experimentation to pick optimal settings.", "labels": [], "entities": []}, {"text": "In this paper, we address the issue of choosing the correct number of units in hidden layers.", "labels": [], "entities": []}, {"text": "We introduce a method for automatically adjusting network size by pruning out hidden units through \u221e,1 and 2,1 regularization.", "labels": [], "entities": []}, {"text": "We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7429859936237335}]}, {"text": "We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions .", "labels": [], "entities": [{"text": "machine translation decoder", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7665837109088898}]}], "introductionContent": [{"text": "Neural networks have proven to be highly effective at many tasks in natural language.", "labels": [], "entities": []}, {"text": "For example, neural language models and joint language/translation models improve machine translation quality significantly ().", "labels": [], "entities": [{"text": "joint language/translation", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.5879763886332512}, {"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7934499382972717}]}, {"text": "However, neural networks can be complicated to design and train well.", "labels": [], "entities": []}, {"text": "Many decisions need to be made, and performance can be highly dependent on making them correctly.", "labels": [], "entities": []}, {"text": "Yet the optimal settings are non-obvious and can be laborious to find, often requiring an extensive grid search involving numerous experiments.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the choice of the sizes of hidden layers.", "labels": [], "entities": []}, {"text": "We introduce a method for automatically pruning out hidden layer units, by adding a sparsity-inducing regularizer that encourages units to deactivate if not needed, so that they can be removed from the network.", "labels": [], "entities": []}, {"text": "Thus, after training with more units than necessary, a network is produced that has hidden layers correctly sized, saving both time and memory when actually putting the network to use.", "labels": [], "entities": []}, {"text": "Using a neural n-gram language model (, we are able to show that our novel auto-sizing method is able to learn models that are smaller than models trained without the method, while maintaining nearly the same perplexity.", "labels": [], "entities": []}, {"text": "The method has only a single hyperparameter to adjust (as opposed to adjusting the sizes of each of the hidden layers), and we find that the same setting works consistently well across different training data sizes, vocabulary sizes, and n-gram sizes.", "labels": [], "entities": []}, {"text": "In addition, we show that incorporating these models into a machine translation decoder still results in large BLEU point improvements.", "labels": [], "entities": [{"text": "machine translation decoder", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.7886763413747152}, {"text": "BLEU point", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9741296470165253}]}, {"text": "The result is that fewer experiments are needed to obtain models that perform well and are correctly sized.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model using the open-source NPLM toolkit released by, extending it to use the additional regularizers as described in this paper.", "labels": [], "entities": []}, {"text": "We use a vocabulary size of 100k and word embeddings with 50 dimensions.", "labels": [], "entities": []}, {"text": "We use two hidden layers of rectified linear units ().", "labels": [], "entities": []}, {"text": "We train neural language models (LMs) on two natural language corpora, Europarl v7 English and the AFP portion of English Gigaword 5.", "labels": [], "entities": [{"text": "Europarl v7 English", "start_pos": 71, "end_pos": 90, "type": "DATASET", "confidence": 0.9162053267161051}, {"text": "AFP portion of English Gigaword 5", "start_pos": 99, "end_pos": 132, "type": "DATASET", "confidence": 0.7100740671157837}]}, {"text": "After tokenization, Europarl has 56M tokens and Gigaword AFP has 870M tokens.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.9822414517402649}, {"text": "Gigaword AFP", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.8659238517284393}]}, {"text": "For both corpora, we holdout a validation set of 5,000 tokens.", "labels": [], "entities": []}, {"text": "We train each model for 10 iterations over the training data.", "labels": [], "entities": []}, {"text": "Our experiments breakdown into three parts.", "labels": [], "entities": []}, {"text": "First, we look at the impact of our pruning method on perplexity of a held-out validation set, across a variety of settings.", "labels": [], "entities": []}, {"text": "Second, we take a closer look at how the model evolves through the training process.", "labels": [], "entities": []}, {"text": "Finally, we explore the downstream impact of our method on a statistical phrase-based machine translation system.", "labels": [], "entities": [{"text": "statistical phrase-based machine translation", "start_pos": 61, "end_pos": 105, "type": "TASK", "confidence": 0.5526059865951538}]}], "tableCaptions": [{"text": " Table 1: Comparison of \u221e,1 regularization on 2-gram, 3-gram, and 5-gram neural language models. The  network initially started with 1,000 units in the first hidden layer and 50 in the second. A regularization  strength of \u03bb = 0.1 consistently is able to prune units while maintaining perplexity, even though the final  number of units varies considerably across models. The vocabulary size is 100k.", "labels": [], "entities": []}, {"text": " Table 2: Results from training a 5-gram neural LM  on the AFP portion of the Gigaword dataset. As  with the smaller Europarl corpus (Table 1), a reg- ularization strength of \u03bb = 0.1 is able to prune  units while maintaining perplexity.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.9433275461196899}, {"text": "Europarl corpus", "start_pos": 117, "end_pos": 132, "type": "DATASET", "confidence": 0.9916830062866211}]}, {"text": " Table 3: A regularization strength of \u03bb = 0.1 is  best across different vocabulary sizes.", "labels": [], "entities": []}, {"text": " Table 4: Results using 2,1 regularization.", "labels": [], "entities": []}]}