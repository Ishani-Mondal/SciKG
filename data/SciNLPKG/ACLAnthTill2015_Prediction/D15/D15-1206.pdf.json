{"title": [{"text": "Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths", "labels": [], "entities": []}], "abstractContent": [{"text": "Relation classification is an important research arena in the field of natural language processing (NLP).", "labels": [], "entities": [{"text": "Relation classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9787462949752808}, {"text": "natural language processing (NLP)", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.8134301205476125}]}, {"text": "In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence.", "labels": [], "entities": [{"text": "classify the relation of two entities in a sentence", "start_pos": 62, "end_pos": 113, "type": "TASK", "confidence": 0.6901421017116971}]}, {"text": "Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichan-nel recurrent neural networks, with long short term memory (LSTM) units, pickup heterogeneous information along the SDP.", "labels": [], "entities": []}, {"text": "Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 121, "end_pos": 144, "type": "TASK", "confidence": 0.7374336272478104}]}, {"text": "(2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths.", "labels": [], "entities": [{"text": "information integration", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.7145236879587173}]}, {"text": "(3) A customized dropout strategy regularizes the neural network to alleviate overfitting.", "labels": [], "entities": []}, {"text": "We test our model on the SemEval 2010 relation classification task, and achieve an F 1-score of 83.7%, higher than competing methods in the literature.", "labels": [], "entities": [{"text": "SemEval 2010 relation classification task", "start_pos": 25, "end_pos": 66, "type": "TASK", "confidence": 0.8556214451789856}, {"text": "F 1-score", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9934690594673157}]}], "introductionContent": [{"text": "Relation classification is an important NLP task.", "labels": [], "entities": [{"text": "Relation classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9783249795436859}]}, {"text": "It plays a key role in various scenarios, e.g., information extraction, question answering), medical informatics (), ontology learning (), etc.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.8274093866348267}, {"text": "question answering", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.8805866539478302}]}, {"text": "The aim of relation classification is to categorize into predefined classes the relations between pairs of marked entities in given texts.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.8488934934139252}]}, {"text": "For instance, in the sentence \"A trillion gallons of e 1 have been poured into an empty e 2 of outer * Corresponding authors.", "labels": [], "entities": []}, {"text": "space,\" the entities water and region are of relation Entity-Destination(e 1 , e 2 ).", "labels": [], "entities": []}, {"text": "Traditional relation classification approaches rely largely on feature representation, or kernel design ().", "labels": [], "entities": [{"text": "relation classification", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.8829234540462494}, {"text": "feature representation", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.6980198174715042}]}, {"text": "The former method usually incorporates a large set of features; it is difficult to improve the model performance if the feature set is not very well chosen.", "labels": [], "entities": []}, {"text": "The latter approach, on the other hand, depends largely on the designed kernel, which summarizes all data information.", "labels": [], "entities": []}, {"text": "Deep neural networks, emerging recently, provide away of highly automatic feature learning (, and have exhibited considerable potential (.", "labels": [], "entities": []}, {"text": "However, human engineering-that is, incorporating human knowledge to the network's architecture-is still important and beneficial.", "labels": [], "entities": []}, {"text": "This paper proposes anew neural network, SDP-LSTM, for relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.9554949998855591}]}, {"text": "Our model utilizes the shortest dependency path (SDP) between two entities in a sentence; we also design along short term memory (LSTM)-based recurrent neural network for information processing.", "labels": [], "entities": [{"text": "information processing", "start_pos": 171, "end_pos": 193, "type": "TASK", "confidence": 0.7689818441867828}]}, {"text": "The neural architecture is mainly inspired by the following observations.", "labels": [], "entities": []}, {"text": "\u2022 Shortest dependency paths are informative ().", "labels": [], "entities": []}, {"text": "To determine the two entities' relation, we find it mostly sufficient to use only the words along the SDP: they concentrate on most relevant information while diminishing less relevant noise.", "labels": [], "entities": []}, {"text": "depicts the dependency parse tree of the aforementioned sentence.", "labels": [], "entities": []}, {"text": "Words along the SDP form a trimmed phrase (gallons of water poured into region) of the original sentence, which conveys much information about the target relation.", "labels": [], "entities": []}, {"text": "Other words, such as a, trillion, outer space, are less informative and may bring noise if not dealt with properly.", "labels": [], "entities": []}, {"text": "Dependency trees area kind of directed graph.", "labels": [], "entities": []}, {"text": "The dependency relation between into and region is PREP; such relation hardly makes any sense if the directed edge is reversed.", "labels": [], "entities": [{"text": "PREP", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.8471795320510864}]}, {"text": "Moreover, the entities' relation distinguishes its directionality, that is, r(a, b) differs from r(b, a), fora same given relation rand two entities a, b.", "labels": [], "entities": []}, {"text": "Therefore, we think it necessary to let the neural model process information in a directionsensitive manner.", "labels": [], "entities": []}, {"text": "Out of this consideration, we separate an SDP into two sub-paths, each from an entity to the common ancestor node.", "labels": [], "entities": []}, {"text": "The extracted features along the two subpaths are concatenated to make final classification.", "labels": [], "entities": []}, {"text": "For example, with prior knowledge of hyponymy, we know \"water is a kind of substance.\"", "labels": [], "entities": []}, {"text": "This is a hint that the entities, water and region, are more of Entity-Destination relation than, say, Communication-Topic.", "labels": [], "entities": []}, {"text": "To gather heterogeneous information along SDP, we design a multichannel recurrent neural network.", "labels": [], "entities": []}, {"text": "It makes use of information from various sources, including words themselves, POS tags, WordNet hypernyms, and the grammatical relations between governing words and their children.", "labels": [], "entities": []}, {"text": "For effective information propagation and integration, our model leverages LSTM units during recurrent propagation.", "labels": [], "entities": [{"text": "information propagation", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.8045320510864258}]}, {"text": "We also customize anew dropout strategy for our SDP-LSTM network to alleviate the problem of overfitting.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to use LSTMbased recurrent neural networks for the relation classification task.", "labels": [], "entities": [{"text": "relation classification task", "start_pos": 98, "end_pos": 126, "type": "TASK", "confidence": 0.9497166275978088}]}, {"text": "We evaluate our proposed method on the SemEval 2010 relation classification task, and achieve an F 1 -score of 83.7%, higher than competing methods in the literature.", "labels": [], "entities": [{"text": "SemEval 2010 relation classification task", "start_pos": 39, "end_pos": 80, "type": "TASK", "confidence": 0.8662723064422607}, {"text": "F 1 -score", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9923591315746307}]}, {"text": "In the rest of this paper, we review related work in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we describe our SDP-LSTM model in detail.", "labels": [], "entities": []}, {"text": "Section 4 presents quantitative experimental results.", "labels": [], "entities": []}, {"text": "Finally, we have our conclusion in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present our experiments in detail.", "labels": [], "entities": []}, {"text": "Our implementation is built upon.", "labels": [], "entities": []}, {"text": "Section 4.1 introduces the dataset; Section 4.2 describes hyperparameter settings.", "labels": [], "entities": []}, {"text": "In Section 4.3, we compare SDP-LSTM's performance with other methods in the literature.", "labels": [], "entities": []}, {"text": "We also analyze the effect of different channels in Section 4.4.", "labels": [], "entities": []}, {"text": "The SemEval-2010 Task 8 dataset is a widely used benchmark for relation classification).", "labels": [], "entities": [{"text": "SemEval-2010 Task 8 dataset", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.6327072829008102}, {"text": "relation classification", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.9039801955223083}]}, {"text": "The dataset contains 8,000 sentences for training, and 2,717 for testing.", "labels": [], "entities": []}, {"text": "We split 1/10 samples out of the training set for validation.", "labels": [], "entities": [{"text": "validation", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.9650112986564636}]}, {"text": "The target contains 19 labels: 9 directed relations, and an undirected Other class.", "labels": [], "entities": []}, {"text": "The directed relations are list as below.", "labels": [], "entities": []}, {"text": "\u2022 Cause-Effect In the following are illustrated two sample sentences with directed relations.", "labels": [], "entities": []}, {"text": "[People] e 1 have been moving back into  The target labels are Entity-Destination (e 1 , e 2 ), and Cause-Effect(e 1 , e 2 ), respectively.", "labels": [], "entities": []}, {"text": "The dataset also contains an undirected Other class.", "labels": [], "entities": []}, {"text": "Hence, there are 19 target labels in total.", "labels": [], "entities": []}, {"text": "The undirected Other class takes in entities that do not fit into the above categories, illustrated by the following example.", "labels": [], "entities": []}, {"text": "We use the official macro-averaged F 1 -score to evaluate model performance.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9587445706129074}]}, {"text": "This official measurement excludes the Other relation.", "labels": [], "entities": []}, {"text": "Nonetheless, we have no special treatment of Other class in our experiments, which is typical in other studies.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of relation classification systems. The \" \u2020\" remark refers to special treatment for  the Other class.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.6912561655044556}]}, {"text": " Table 2: Effect of different channels.", "labels": [], "entities": []}]}