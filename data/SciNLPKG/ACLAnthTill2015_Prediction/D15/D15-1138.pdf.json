{"title": [{"text": "Alignment-Based Compositional Semantics for Instruction Following", "labels": [], "entities": [{"text": "Alignment-Based Compositional Semantics", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8104124466578165}]}], "abstractContent": [{"text": "This paper describes an alignment-based model for interpreting natural language instructions in context.", "labels": [], "entities": [{"text": "interpreting natural language instructions in context", "start_pos": 50, "end_pos": 103, "type": "TASK", "confidence": 0.8642183144887289}]}, {"text": "We approach instruction following as a search over plans, scoring sequences of actions conditioned on structured observations of text and the environment.", "labels": [], "entities": []}, {"text": "By explicitly modeling both the low-level compositional structure of individual actions and the high-level structure of full plans, we are able to learn both grounded representations of sentence meaning and pragmatic constraints on interpretation.", "labels": [], "entities": []}, {"text": "To demonstrate the model's flexibility, we apply it to a diverse set of benchmark tasks.", "labels": [], "entities": []}, {"text": "On every task, we outperform strong task-specific baselines, and achieve several new state-of-the-art results.", "labels": [], "entities": []}], "introductionContent": [{"text": "In instruction-following tasks, an agent executes a sequence of actions in areal or simulated environment, in response to a sequence of natural language commands.", "labels": [], "entities": []}, {"text": "Examples include giving navigational directions to robots and providing hints to automated game-playing agents.", "labels": [], "entities": [{"text": "giving navigational directions", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.7775454719861349}]}, {"text": "Plans specified with natural language exhibit compositionality both at the level of individual actions and at the overall sequence level.", "labels": [], "entities": []}, {"text": "This paper describes a framework for learning to follow instructions by leveraging structure at both levels.", "labels": [], "entities": []}, {"text": "Our primary contribution is anew, alignmentbased approach to grounded compositional semantics.", "labels": [], "entities": []}, {"text": "Building on related logical approaches (), we recast instruction following as a pair of nested, structured alignment problems.", "labels": [], "entities": []}, {"text": "Given instructions and a candidate plan, the model infers a sequenceto-sequence alignment between sentences and atomic actions.", "labels": [], "entities": []}, {"text": "Within each sentence-action pair, the model infers a structure-to-structure alignment between the syntax of the sentence and a graphbased representation of the action.", "labels": [], "entities": []}, {"text": "At a high level, our agent is a block-structured, graph-valued conditional random field, with alignment potentials to relate instructions to actions and transition potentials to encode the environment model ().", "labels": [], "entities": []}, {"text": "Explicitly modeling sequenceto-sequence alignments between text and actions allows flexible reasoning about action sequences, enabling the agent to determine which actions are specified (perhaps redundantly) by text, and which actions must be performed automatically (in order to satisfy pragmatic constraints on interpretation).", "labels": [], "entities": []}, {"text": "Treating instruction following as a sequence prediction problem, rather than a series of independent decisions, makes it possible to use general-purpose planning machinery, greatly increasing inferential power.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.699011817574501}]}, {"text": "The fragment of semantics necessary to complete most instruction-following tasks is essentially predicate-argument structure, with limited influence from quantification and scoping.", "labels": [], "entities": []}, {"text": "Thus the problem of sentence interpretation can reasonably be modeled as one of finding an alignment between language and the environment it describes.", "labels": [], "entities": [{"text": "sentence interpretation", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.7393970042467117}]}, {"text": "We allow this structure-to-structure alignmentan \"overlay\" of language onto the world-to be mediated by linguistic structure (in the form of dependency parses) and structured perception (in what we term grounding graphs).", "labels": [], "entities": []}, {"text": "Our model thereby reasons directly about the relationship between language and observations of the environment, without the need for an intermediate logical representation of sentence meaning.", "labels": [], "entities": []}, {"text": "This, in turn, makes it possible to incorporate flexible feature representations that have been difficult to integrate with previous work in semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 141, "end_pos": 157, "type": "TASK", "confidence": 0.7728879451751709}]}, {"text": "We apply our approach to three established . .", "labels": [], "entities": []}, {"text": "right round the whitewater but stay quite close 'cause you don't otherwise you're going to be in that stone creek . .", "labels": [], "entities": []}, {"text": "Go down the yellow hall.", "labels": [], "entities": []}, {"text": "Turn left at the intersection of the yellow and the gray.", "labels": [], "entities": []}, {"text": "instruction-following benchmarks: the map reading task of Vogel and Jurafsky (2010), the maze navigation task of, and the puzzle solving task of.", "labels": [], "entities": [{"text": "map reading", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.85450479388237}, {"text": "maze navigation task", "start_pos": 89, "end_pos": 109, "type": "TASK", "confidence": 0.7647403875986735}, {"text": "puzzle solving", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.7868263423442841}]}, {"text": "An example from each is shown in.", "labels": [], "entities": []}, {"text": "These benchmarks exhibit a range of qualitative properties-both in the length and complexity of their plans, and in the quantity and quality of accompanying language.", "labels": [], "entities": []}, {"text": "Each task has been studied in isolation, but we are unaware of any published approaches capable of robustly handling all three.", "labels": [], "entities": []}, {"text": "Our general model outperforms strong, task-specific baselines in each case, achieving relative error reductions of 15-20% over several state-of-the-art results.", "labels": [], "entities": []}, {"text": "Experiments demonstrate the importance of our contributions in both compositional semantics and search over plans.", "labels": [], "entities": []}, {"text": "We have released all code for this project at github.com/jacobandreas/instructions.", "labels": [], "entities": []}], "datasetContent": [{"text": "As one of the main advantages of this approach is its generality, we evaluate on several different benchmark tasks for instruction following.", "labels": [], "entities": [{"text": "instruction following", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.6874652355909348}]}, {"text": "These exhibit great diversity in both environment structure and language use.", "labels": [], "entities": []}, {"text": "We compare our full system to recent state-of-the-art approaches to each task.", "labels": [], "entities": []}, {"text": "In the introduction, we highlighted two core aspects of our approach to semantics: compositionality (by way of grounding graphs and structure-to-structure alignments) and planning (by way of inference with lookahead and sequence-to-sequence alignments).", "labels": [], "entities": []}, {"text": "To evaluate these, we additionally present a pair of ablation experiments: no grounding graphs (an agent with an unstructured representation of environment state), and no planning (a reflex agent with no lookahead).", "labels": [], "entities": []}, {"text": "Map reading Our first application is the map navigation task established by, based on data collected fora psychological experiment by.", "labels": [], "entities": [{"text": "Map reading", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8483127355575562}, {"text": "map navigation task", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.833444615205129}]}, {"text": "Each training datum consists of a map with a designated starting position, and a collection of landmarks, each labeled with a spatial coordinate and a string name.", "labels": [], "entities": []}, {"text": "Names are not always unique, and landmarks in the test set are never observed during training.", "labels": [], "entities": []}, {"text": "This map is accompanied by a set of instructions specifying a path from the starting position to some (unlabeled) destination point.", "labels": [], "entities": []}, {"text": "These instruction sets are informal and redundant, involving as many as a hundred utterances.", "labels": [], "entities": []}, {"text": "They are transcribed from spoken text, so grammatical errors, disfluencies, etc. are common.", "labels": [], "entities": []}, {"text": "This is a 0.51 0.60 0.55: Evaluation results for the map-reading task.", "labels": [], "entities": []}, {"text": "P is precision, R is recall and F1 is F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9995792508125305}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9988446235656738}, {"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9993447661399841}, {"text": "F-measure", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.990233302116394}]}, {"text": "Scores are calculated with respect to transitions between landmarks appearing in the reference path (for details see).", "labels": [], "entities": []}, {"text": "We use the same train / test split.", "labels": [], "entities": []}, {"text": "Some variant of our model achieves the best published results on all three metrics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results for the map-reading task. P is pre- cision, R is recall and F1 is F-measure. Scores are calculated  with respect to transitions between landmarks appearing in  the reference path (for details see", "labels": [], "entities": [{"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9973084926605225}, {"text": "F1", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9962245225906372}, {"text": "F-measure", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9579989910125732}]}, {"text": " Table 3: Evaluation results for the maze navigation task.  \"Success\" shows the percentage of actions resulting in a cor- rect position and orientation after observing a single instruc- tion. We use the leave-one-map-out evaluation employed by  previous work.", "labels": [], "entities": [{"text": "maze navigation task", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8394344449043274}]}, {"text": " Table 4: Results for the puzzle solving task. \"Match\" shows  the percentage of predicted action sequences that exactly  match the annotation. \"Success\" shows the percentage of  predicted action sequences that result in a winning game con- figuration, regardless of the action sequence performed. Fol- lowing Branavan et al. (2009), we average across five random  train / test folds. Our model achieves state-of-the-art results  on this task.", "labels": [], "entities": [{"text": "puzzle solving task", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8950566649436951}, {"text": "Match", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.9586175084114075}]}]}