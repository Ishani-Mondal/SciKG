{"title": [{"text": "Bilingual Correspondence Recursive Autoencoders for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.8475507497787476}]}], "abstractContent": [{"text": "Learning semantic representations and tree structures of bilingual phrases is beneficial for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 93, "end_pos": 124, "type": "TASK", "confidence": 0.7533327440420786}]}, {"text": "In this paper, we propose anew neu-ral network model called Bilingual Correspondence Recursive Autoencoder (BCor-rRAE) to model bilingual phrases in translation.", "labels": [], "entities": []}, {"text": "We incorporate word alignments into BCorrRAE to allow it freely access bilingual constraints at different levels.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.6892700642347336}, {"text": "BCorrRAE", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.8384560942649841}]}, {"text": "BCorrRAE minimizes a joint objective on the combination of a recursive au-toencoder reconstruction error, a structural alignment consistency error and a cross-lingual reconstruction error so as to not only generate alignment-consistent phrase structures, but also capture different levels of semantic relations within bilingual phrases.", "labels": [], "entities": [{"text": "BCorrRAE", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.5421354174613953}]}, {"text": "In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 221, "end_pos": 224, "type": "TASK", "confidence": 0.990484356880188}]}, {"text": "Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline.", "labels": [], "entities": [{"text": "NIST Chinese-English test sets", "start_pos": 15, "end_pos": 45, "type": "DATASET", "confidence": 0.9579322636127472}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9982531666755676}]}], "introductionContent": [{"text": "Recently a variety of \"deep architecture\" approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) ().", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 109, "end_pos": 146, "type": "TASK", "confidence": 0.8372739354769388}]}, {"text": "Typically, these approaches represent words as dense, low-dimensional and * Corresponding author.", "labels": [], "entities": []}, {"text": "real-valued vectors, i.e., word embeddings.", "labels": [], "entities": []}, {"text": "However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented byword embeddings.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7124354541301727}]}, {"text": "Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful \"deep\" SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.8886510133743286}]}, {"text": "To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings).", "labels": [], "entities": [{"text": "SMT", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.9847254157066345}]}, {"text": "The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models ( ), translation models (), or both language and translation models ( ).", "labels": [], "entities": [{"text": "SMT", "start_pos": 152, "end_pos": 155, "type": "TASK", "confidence": 0.9811382293701172}]}, {"text": "In spite of their success, these approaches center around capturing relations between entire source and target phrases.", "labels": [], "entities": []}, {"text": "They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases.", "labels": [], "entities": []}, {"text": "The neglect of these important clues maybe due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations.", "labels": [], "entities": [{"text": "bilingual phrase representations", "start_pos": 132, "end_pos": 164, "type": "TASK", "confidence": 0.611191044251124}]}, {"text": "However, we believe such internal structures and correspondences can help us learn better phrase representations since they provide multi-level syntactic and semantic constraints.", "labels": [], "entities": []}, {"text": "In this paper, we propose a Bilingual Correspondence Recursive Autoencoder (BCorrRAE) to learn bilingual phrase embeddings.", "labels": [], "entities": []}, {"text": "BCorrRAE substantially extends the Bilingually-constrained Recursive Auto-encoder (BRAE) ( ) to exploit both inner structures and corre-resolution 0 adopted  n \u00af e: BRAE vs BCorrRAE models for generating of a bilingual phrase (\"\u4638\u5755 \u56a4 \u57cf\u434c \u46fb\u61c6\", \"resolution adopted today\") with word alignments (\"0-2 2-1 3-0\").", "labels": [], "entities": [{"text": "BCorrRAE", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9272695779800415}, {"text": "BRAE", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.8855295181274414}]}, {"text": "The subscript number of each word indicates its position within phrase.", "labels": [], "entities": []}, {"text": "Solid lines depict the generation procedure of phrase structures, while dash lines illustrate the reconstruction procedure from one language to the other.", "labels": [], "entities": []}, {"text": "In this paper, the dimensionality of vector din all figures is set to 3 for better illustration.", "labels": [], "entities": []}, {"text": "The intuitions behind BCorrRAE are twofold: 1) bilingual phrase structure generation should satisfy word alignment constraints as much as possible; and 2) corresponding sub-phrases on the source and target side of bilingual phrases should be able to reconstruct each other as they are semantic equivalents.", "labels": [], "entities": [{"text": "bilingual phrase structure generation", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.6181083247065544}, {"text": "word alignment", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.6947427690029144}]}, {"text": "In order to model the first intuition, BCorrRAE punishes bilingual structures that violate word alignment constraints and rewards those in consistent with word alignments.", "labels": [], "entities": [{"text": "BCorrRAE", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.8994704484939575}]}, {"text": "This enables BCorrRAE to produce desirable bilingual phrase structures from the perspective of word alignments.", "labels": [], "entities": [{"text": "BCorrRAE", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.6324740052223206}, {"text": "word alignments", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.6951737105846405}]}, {"text": "With regard to the second intuition, BCorrRAE reconstructs structures of sub-phrases of one language according to aligned nodes in the other language and minimizes the gap between original and reconstructed structures.", "labels": [], "entities": [{"text": "BCorrRAE", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.7591589093208313}]}, {"text": "In doing so, BCorrRAE is capable of capturing semantic relations at different levels.", "labels": [], "entities": [{"text": "BCorrRAE", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.6305971145629883}]}, {"text": "To better illustrate our model, let us consider the example in.", "labels": [], "entities": []}, {"text": "Similar to the conventional recursive antoencoder (RAE), BRAE neglects bilingual correspondences of sub-phrases.", "labels": [], "entities": [{"text": "BRAE", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.862933337688446}]}, {"text": "Thus, it may combine \"adopted\" and \"today\" together to generate an undesirable target tree structure which violates word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 116, "end_pos": 131, "type": "TASK", "confidence": 0.6899847537279129}]}, {"text": "In contrast, BCorrRAE aligns source-side nodes (e.g. (\"\u57cf \u434c\", \"\u46fb \u61c6\")) to their corresponding target-side nodes (accordingly (\"resolution\", \"adopted\")) according to word alignments.", "labels": [], "entities": []}, {"text": "Furthermore, in BCorrRAE, each subtree on the target side can be reconstructed from the corresponding source node that aligns to the target-side node dominating the subtree and vice versa.", "labels": [], "entities": [{"text": "BCorrRAE", "start_pos": 16, "end_pos": 24, "type": "DATASET", "confidence": 0.659155011177063}]}, {"text": "These advantages allow us to obtain improved bilingual phrase embeddings with better inner correspondences of sub-phrases and word alignment consistency.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 126, "end_pos": 140, "type": "TASK", "confidence": 0.7363366484642029}]}, {"text": "We conduct experiments with a state-of-the-art SMT system on large-scale data to evaluate the effectiveness of BCorrRAE model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9930245876312256}, {"text": "BCorrRAE", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.5322054624557495}]}, {"text": "Results on the NIST 2006 and 2008 datasets show that our system achieves significant improvements over baseline methods.", "labels": [], "entities": [{"text": "NIST 2006 and 2008 datasets", "start_pos": 15, "end_pos": 42, "type": "DATASET", "confidence": 0.9571776390075684}]}, {"text": "The main contributions of our work lie in the following three aspects: \u2022 We learn both embeddings and tree structures for bilingual phrases using cross-lingual RAE reconstruction that minimizes semantic distances between original and reconstructed subtrees.", "labels": [], "entities": [{"text": "RAE reconstruction", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.9171860814094543}]}, {"text": "To the best of our knowledge, this has not been investigated before.", "labels": [], "entities": []}, {"text": "\u2022 We incorporate word alignment information to guide phrase structure generation and establish internal semantic associations of subphrases within bilingual phrases.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7352095991373062}, {"text": "phrase structure generation", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.7843073407808939}]}, {"text": "\u2022 We integrate two similarity features based on BCorrRAE to enhance translation candidate selection, and achieve an improvement of 1.55 BLEU points on Chinese-English translation.", "labels": [], "entities": [{"text": "BCorrRAE", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.5885883569717407}, {"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9982518553733826}, {"text": "Chinese-English translation", "start_pos": 151, "end_pos": 178, "type": "TASK", "confidence": 0.6455394476652145}]}], "datasetContent": [{"text": "We conducted experiments on NIST ChineseEnglish translation task to validate the effectiveness of BCorrRAE.", "labels": [], "entities": [{"text": "NIST ChineseEnglish translation", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.6769369542598724}, {"text": "BCorrRAE", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.5011302828788757}]}], "tableCaptions": [{"text": " Table 1: Hyper-parameters for BCorrRAE and BRAE model.", "labels": [], "entities": [{"text": "Hyper-parameters", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9476485848426819}, {"text": "BCorrRAE", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.7168365716934204}, {"text": "BRAE", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9824403524398804}]}, {"text": " Table 2: Experiment results for different dimensions (d).", "labels": [], "entities": []}, {"text": " Table  4. 6 We find that BCorrRAE significantly outper-Method  MT06  MT08  AVG  Baseline  29.66 \u21d3 21.52 \u21d3 25.59  BRAE  30.27 \u21d3 22.53 \u21d3 26.40  BCorrRAESM 30.58 \u2193 22.72 \u21d3 26.65  BCorrRAEST 30.94  23.33  27.14", "labels": [], "entities": [{"text": "MT06  MT08  AVG  Baseline", "start_pos": 64, "end_pos": 89, "type": "DATASET", "confidence": 0.5372797176241875}, {"text": "BRAE", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9819403290748596}]}, {"text": " Table 3: Experiment results on the test sets. AVG = average  BLEU scores for test sets. For both BRAE and BCorrRAE,  we set d=50. \u2193/\u21d3: significantly worse than the BCorrRAEST  with d=50 (p < 0.05/p < 0.01, respectively).", "labels": [], "entities": [{"text": "AVG", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9986907839775085}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9993802309036255}, {"text": "BRAE", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9970797896385193}, {"text": "BCorrRAE", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.5071110129356384}, {"text": "\u2193/\u21d3", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9792574048042297}]}, {"text": " Table 4: Aligned node ratio for source phrases of different  lengths.", "labels": [], "entities": [{"text": "Aligned node ratio", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.754997730255127}]}]}