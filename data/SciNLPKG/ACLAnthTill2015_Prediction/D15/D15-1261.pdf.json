{"title": [{"text": "Estimation of Discourse Segmentation Labels from Crowd Data", "labels": [], "entities": [{"text": "Estimation of Discourse Segmentation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6007519662380219}]}], "abstractContent": [{"text": "For annotation tasks involving independent judgments, probabilistic models have been used to infer ground truth labels from data where a crowd of many annotators labels the same items.", "labels": [], "entities": []}, {"text": "Such models have been shown to produce results superior to taking the majority vote, but have not been applied to sequential data.", "labels": [], "entities": []}, {"text": "We present two methods to infer ground truth labels from sequential annotations where we assume judgments are not independent, based on the observation that an annotator's segments all tend to be several utterances long.", "labels": [], "entities": []}, {"text": "The data consists of crowd labels for annotation of discourse segment boundaries.", "labels": [], "entities": []}, {"text": "The new methods extend Hidden Markov Models to relax the independence assumption.", "labels": [], "entities": []}, {"text": "The two methods are distinct, so positive labels proposed by both are taken to be ground truth.", "labels": [], "entities": []}, {"text": "In addition, results of the models are checked using metrics that test whether an annotator's accuracy relative to a given model remains consistent across different conversations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9930958151817322}]}], "introductionContent": [{"text": "A single, spontaneous, spoken interaction can consist of multiple activities, such as to plan a future event, to complain about a past situation, or to carryout a transaction that might consist of subtasks.", "labels": [], "entities": [{"text": "A single, spontaneous, spoken interaction can consist of multiple activities, such as to plan a future event, to complain about a past situation, or to carryout a transaction that might consist of subtasks", "start_pos": 0, "end_pos": 205, "type": "Description", "confidence": 0.8048850039118215}]}, {"text": "Speakers shift from one activity to the next with more or less awareness and explicit demarcation.", "labels": [], "entities": []}, {"text": "To treat such conversational activities as a sequence of discrete units is a convenient oversimplification that is often resorted to (.", "labels": [], "entities": []}, {"text": "Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization () or information retrieval (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 135, "end_pos": 148, "type": "TASK", "confidence": 0.9805927276611328}, {"text": "information retrieval", "start_pos": 155, "end_pos": 176, "type": "TASK", "confidence": 0.7694481611251831}]}, {"text": "Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (, multiparty meeting action items), decisions, or answers to questions).", "labels": [], "entities": []}, {"text": "To support such research, there is a need for annotation methods to segment conversational interaction into sequential, multi-utterance units.", "labels": [], "entities": []}, {"text": "We present and compare two methods to derive such data from crowdsourced annotations.", "labels": [], "entities": []}, {"text": "Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing.", "labels": [], "entities": []}, {"text": "Examples include word sense, named entities (, and several other tasks in (, including textual entailment.", "labels": [], "entities": [{"text": "word sense", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.7391103804111481}]}, {"text": "Three advantages to corpus annotation through application of a probabilistic model to crowdsourced labels, rather than reliance on interannotator agreement computed fora small number of trained annotators, are higher quality, lower cost, and a posterior probability for each ground truth label;).", "labels": [], "entities": [{"text": "corpus annotation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7048784494400024}]}, {"text": "The latter serves as a confidence measure, which contrasts with interannotator agreement measures and with majority-voted labels, neither of which provides quality information for the ground truth labels on individual items.", "labels": [], "entities": []}, {"text": "Previous work has demonstrated that model estimation of ground truth labels from crowd labels produces results superior to the crowd's majority vote, due to differences among annotators in the quality of their labels.", "labels": [], "entities": []}, {"text": "No previous work, however, provides modelbased estimation of labels for sequential annotation from crowd labels.", "labels": [], "entities": []}, {"text": "For the discourse segmentation data presented here, annotators were presented with audio files of conversations and corresponding transcriptions into utterances.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 8, "end_pos": 30, "type": "TASK", "confidence": 0.7196657657623291}]}, {"text": "The annotation task was to identify each utterance that completes a discourse segment spanning one or more utterances, based on the speakers' conversational activities or intentions, as in ().", "labels": [], "entities": []}, {"text": "The annotations from y annotators fora conversation with x utterances can be represented as a y \u00d7 x matrix, with cell values n ij \u2208 {0, 1} to represent the binary segment boundary label assigned by annotator y i at utterance x j . illustrates part of such a matrix.", "labels": [], "entities": []}, {"text": "The eight annotators for this conversation are on the y-axis and utterances 80 through 180 are on the x-axis.", "labels": [], "entities": []}, {"text": "Colored bars represent positive labels, and each color represents a distinct annotator.", "labels": [], "entities": []}, {"text": "The label distribution shown here is typical of our dataset: an annotator's positive labels are typically separated by several utterances, and annotators agree much more often on non-boundaries than on boundaries.", "labels": [], "entities": []}, {"text": "Full consensus on a positive label is rare, but does occur.", "labels": [], "entities": []}, {"text": "Here, all eight annotators assigned a positive label at utterance 120, six at utterance 178, and five at utterance 140.", "labels": [], "entities": []}, {"text": "Our work assumes that unobserved true labels condition the annotators' observed labels, and can be modeled as hidden states in a Markov-type process.", "labels": [], "entities": []}, {"text": "Because an annotator rarely assigns positive labels for adjacent utterances, we assume that neither the true labels nor the observations are conditionally independent, and therefore are not generated by a simple Markov process.", "labels": [], "entities": []}, {"text": "Our first model adapts the Double Chain Markov Model), designed to account for such cases.", "labels": [], "entities": []}, {"text": "We then propose a second model that assumes that each annotator's labels are drawn from a Bernoulli distribution, that annotator performance is a parameter of the model, and that the state transitions are conditioned by an empirical distribution of discourse segment lengths.", "labels": [], "entities": []}, {"text": "The two methods are quite distinct.", "labels": [], "entities": []}, {"text": "Each thus serves as an evaluation of the other.", "labels": [], "entities": []}, {"text": "The segment boundaries proposed by both models include all the majority vote cases, and in addition, cases voted on by a minority of relatively accurate annotators.", "labels": [], "entities": []}, {"text": "We take segment boundaries proposed by both methods as ground truth.", "labels": [], "entities": []}, {"text": "To further assess the results of the models, we assume that an annotator's accuracies should be consistent across the conversations she annotates.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Comparison of positive predictions from majority", "labels": [], "entities": []}, {"text": " Table 4: F-measure for annotators in conversation 945 for", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9862641096115112}]}]}