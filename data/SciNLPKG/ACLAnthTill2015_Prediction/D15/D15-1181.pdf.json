{"title": [{"text": "Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks", "labels": [], "entities": [{"text": "Sentence Similarity Modeling", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.7303919891516367}]}], "abstractContent": [{"text": "Modeling sentence similarity is complicated by the ambiguity and variability of linguistic expression.", "labels": [], "entities": [{"text": "Modeling sentence similarity", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.721817264954249}]}, {"text": "To cope with these challenges, we propose a model for comparing sentences that uses a multiplicity of perspectives.", "labels": [], "entities": []}, {"text": "We first model each sentence using a convolutional neural network that extracts features at multiple levels of gran-ularity and uses multiple types of pooling.", "labels": [], "entities": []}, {"text": "We then compare our sentence representations at several granularities using multiple similarity metrics.", "labels": [], "entities": []}, {"text": "We apply our model to three tasks, including the Microsoft Research paraphrase identification task and two SemEval semantic textual similarity tasks.", "labels": [], "entities": [{"text": "Microsoft Research paraphrase identification task", "start_pos": 49, "end_pos": 98, "type": "TASK", "confidence": 0.8260645866394043}, {"text": "SemEval semantic textual similarity", "start_pos": 107, "end_pos": 142, "type": "TASK", "confidence": 0.8060796856880188}]}, {"text": "We obtain strong performance on all tasks, rivaling or exceeding the state of the art without using external resources such as WordNet or parsers.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 127, "end_pos": 134, "type": "DATASET", "confidence": 0.9400429725646973}]}], "introductionContent": [{"text": "Measuring the semantic relatedness of two pieces of text is a fundamental problem in language processing tasks like plagiarism detection, query ranking, and question answering.", "labels": [], "entities": [{"text": "plagiarism detection", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.7990745306015015}, {"text": "query ranking", "start_pos": 138, "end_pos": 151, "type": "TASK", "confidence": 0.7453378736972809}, {"text": "question answering", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.8862946927547455}]}, {"text": "In this paper, we address the sentence similarity measurement problem: given a query sentence S 1 and a comparison sentence S 2 , the task is to compute their similarity in terms of a score sim(S 1 , S 2 ).", "labels": [], "entities": [{"text": "sentence similarity measurement", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.675913949807485}]}, {"text": "This similarity score can be used within a system that determines whether two sentences are paraphrases, e.g., by comparing it to a threshold.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 5, "end_pos": 21, "type": "METRIC", "confidence": 0.9629826545715332}]}, {"text": "Measuring sentence similarity is challenging because of the variability of linguistic expression and the limited amount of annotated training data.", "labels": [], "entities": []}, {"text": "This makes it difficult to use sparse, hand-crafted features as in conventional approaches in NLP.", "labels": [], "entities": []}, {"text": "Recent successes in sentence similarity have been obtained by using neural networks (.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7743793725967407}]}, {"text": "Our approach is also based on neural networks: we propose a modular functional architecture with two components, sentence modeling and similarity measurement.", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.7671502828598022}, {"text": "similarity measurement", "start_pos": 135, "end_pos": 157, "type": "TASK", "confidence": 0.6344497948884964}]}, {"text": "For sentence modeling, we use a convolutional neural network featuring convolution filters with multiple granularities and window sizes, followed by multiple types of pooling.", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8615868985652924}]}, {"text": "We experiment with two types of word embeddings as well as partof-speech tag embeddings (Sec. 4).", "labels": [], "entities": []}, {"text": "For similarity measurement, we compare pairs of local regions of the sentence representations, using multiple distance functions: cosine distance, Euclidean distance, and element-wise difference.", "labels": [], "entities": [{"text": "similarity measurement", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.6785624176263809}, {"text": "Euclidean distance", "start_pos": 147, "end_pos": 165, "type": "METRIC", "confidence": 0.8229787349700928}]}, {"text": "We demonstrate state-of-the-art performance on two SemEval semantic relatedness tasks, and highly competitive performance on the Microsoft Research paraphrase (MSRP) identification task ().", "labels": [], "entities": [{"text": "SemEval semantic relatedness tasks", "start_pos": 51, "end_pos": 85, "type": "TASK", "confidence": 0.8091320544481277}, {"text": "Microsoft Research paraphrase (MSRP) identification task", "start_pos": 129, "end_pos": 185, "type": "TASK", "confidence": 0.5981911085546017}]}, {"text": "On the SemEval-2014 task, we match the state-of-the-art dependency tree Long ShortTerm Memory (LSTM) neural networks of without using parsers or part-ofspeech taggers.", "labels": [], "entities": []}, {"text": "On the MSRP task, we outperform the recently-proposed convolutional neural network model of without any pretraining.", "labels": [], "entities": [{"text": "MSRP task", "start_pos": 7, "end_pos": 16, "type": "TASK", "confidence": 0.9091117084026337}]}, {"text": "In addition, we perform ablation experiments to show the contribution of our modeling decisions for all three datasets, demonstrating clear benefits from our use of multiple perspectives both in sentence modeling and structured similarity measurement.", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 195, "end_pos": 212, "type": "TASK", "confidence": 0.7177566736936569}, {"text": "structured similarity measurement", "start_pos": 217, "end_pos": 250, "type": "TASK", "confidence": 0.626291831334432}]}], "datasetContent": [{"text": "Everything necessary to replicate our experimental results can be found in our open-source code repository.", "labels": [], "entities": []}, {"text": "We consider three sentence pair similarity tasks:  We conduct experiments with ws values in the range as well as ws = \u221e (no convolution).", "labels": [], "entities": []}, {"text": "We use multiple kinds of embeddings to represent each sentence, both on words and part-ofspeech (POS) tags.", "labels": [], "entities": []}, {"text": "We use the Dim g = 300-dimensional GloVe word embeddings () trained on 840 billion tokens.", "labels": [], "entities": []}, {"text": "We use Dim k = 25-dimensional PARAGRAM vectors () only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Paraphrase Database ().", "labels": [], "entities": [{"text": "PARAGRAM", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9216351509094238}, {"text": "Paraphrase Database", "start_pos": 163, "end_pos": 182, "type": "DATASET", "confidence": 0.7061210721731186}]}, {"text": "For POS embeddings, we run the Stanford POS tagger ( ) on the English side of the Xinhua machine translation parallel corpus, which consists of Xinhua news articles with approximately 25 million words.", "labels": [], "entities": [{"text": "Xinhua machine translation parallel corpus", "start_pos": 82, "end_pos": 124, "type": "DATASET", "confidence": 0.5350774466991425}]}, {"text": "We then train Dim p = 200-dimensional POS embeddings using the word2vec toolkit (.", "labels": [], "entities": []}, {"text": "Adding POS embeddings is expected to retain syntactic information which is reported to be effective for paraphrase identification.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 104, "end_pos": 129, "type": "TASK", "confidence": 0.9474144577980042}]}, {"text": "We use POS embeddings only for the MSRP task.", "labels": [], "entities": []}, {"text": "Therefore for MSRP, we concatenate all word and POS embeddings and obtain Dim = Dim g + Dim p + Dim k = 525-dimension vectors for each input word; for SICK and MSRVID we only use Dim = 300-dimension GloVe embeddings.", "labels": [], "entities": []}, {"text": "We use 5-fold cross validation on the MSRP training data for tuning, then largely re-use the same hyperparameters for the other two datasets.", "labels": [], "entities": [{"text": "MSRP training data", "start_pos": 38, "end_pos": 56, "type": "DATASET", "confidence": 0.8389552036921183}]}, {"text": "However, there are two changes: 1) for the MSRP task we update word embeddings during training but not soon SICK and MSRVID tasks; 2) we set the fully connected layer to contain 250 hidden units for MSRP, and 150 for SICK and MSRVID.", "labels": [], "entities": []}, {"text": "These changes were done to speedup our experimental cycle on SICK and MSRVID; on SICK data they are the same experimental settings as used by, which makes fora cleaner empirical comparison.", "labels": [], "entities": [{"text": "MSRVID", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.8660504817962646}, {"text": "SICK data", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.8074597418308258}]}, {"text": "We set the number of holistic filters in block A to be the same as the input word embeddings, therefore numFilter A = 525 for MSRP and numFilter A = 300 for SICK and MSRVID.", "labels": [], "entities": [{"text": "MSRVID", "start_pos": 166, "end_pos": 172, "type": "DATASET", "confidence": 0.8676087856292725}]}, {"text": "We set the number of per-dimension filters in block B to be numFilter B = 20 per dimension for all three datasets, which corresponds to 20 * Dim filters in total.", "labels": [], "entities": []}, {"text": "We perform optimization using stochastic gradient descent.", "labels": [], "entities": []}, {"text": "The backpropagation algorithm is used to compute gradients for all parameters during training).", "labels": [], "entities": []}, {"text": "We fix the learning rate to 0.01 and regularization parameter \u03bb = 10 \u22124 .  Results on MSRP Data.", "labels": [], "entities": [{"text": "regularization parameter \u03bb", "start_pos": 37, "end_pos": 63, "type": "METRIC", "confidence": 0.9216850399971008}, {"text": "MSRP Data", "start_pos": 86, "end_pos": 95, "type": "DATASET", "confidence": 0.9281289875507355}]}, {"text": "We report F1 scores and accuracies from prior work in 73.0% 82.3% 74.1% 82.4% 75.0% 82.7% 76.1% 82.7% 75.6% 83.0% Socher et al.", "labels": [], "entities": [{"text": "F1 scores and accuracies", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.8887327760457993}]}, {"text": "76.8% 83.6% 77.4% 84.1% 80.41% 85.96%  When comparing to their model without pretraining, we outperform them by 6% absolute inaccuracy and 3% in F1.", "labels": [], "entities": [{"text": "absolute inaccuracy", "start_pos": 115, "end_pos": 134, "type": "METRIC", "confidence": 0.8482073247432709}, {"text": "F1", "start_pos": 145, "end_pos": 147, "type": "METRIC", "confidence": 0.999618649482727}]}, {"text": "Our model is also superior to other recent neural network models () without requiring sparse features or unlabeled data as in).", "labels": [], "entities": []}, {"text": "The best result on MSRP is from which uses unsupervised learning on the MSRP test set and rich sparse features.", "labels": [], "entities": [{"text": "MSRP test set", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.8877923885981241}]}, {"text": "Our results on the SICK task are summarized in, showing Pearson's r, Spearman's \u03c1, and mean squared error (MSE).", "labels": [], "entities": [{"text": "SICK task", "start_pos": 19, "end_pos": 28, "type": "TASK", "confidence": 0.930245965719223}, {"text": "Pearson's r", "start_pos": 56, "end_pos": 67, "type": "METRIC", "confidence": 0.9600119590759277}, {"text": "Spearman's \u03c1", "start_pos": 69, "end_pos": 81, "type": "METRIC", "confidence": 0.7905108531316122}, {"text": "mean squared error (MSE)", "start_pos": 87, "end_pos": 111, "type": "METRIC", "confidence": 0.9621270000934601}]}, {"text": "We include results from the literature as reported by, including prior work using recurrent neural networks (RNNs), the best submissions in the SemEval-2014 competition, and variants of LSTMs.", "labels": [], "entities": []}, {"text": "When measured by Pearson's r, the previous state-of-the-art approach uses a treestructured LSTM; note that their best results require a dependency parser.", "labels": [], "entities": []}, {"text": "On the contrary, our approach does not rely on parse trees, nor do we use POS/PARAGRAM embeddings for this task.", "labels": [], "entities": []}, {"text": "The word embeddings,, which includes the top 2 submissions in the Semantic Textual Similarity (STS) task from SemEval-2012.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 66, "end_pos": 104, "type": "TASK", "confidence": 0.8213669402258736}]}, {"text": "We find that we outperform the top system from the task by nearly 3 points in Pearson's r.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test set results on MSRP for paraphrase  identification. Rows in grey are neural network- based approaches.", "labels": [], "entities": [{"text": "paraphrase  identification", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.9257171750068665}]}, {"text": " Table 2: Test set results on SICK, as reported  by Tai et al. (2015), grouped as: (1) RNN vari- ants; (2) SemEval 2014 systems; (3) sequential  LSTM variants; (4) dependency and constituency  tree LSTMs (Tai et al., 2015). Evaluation metrics  are Pearson's r, Spearman's \u03c1, and mean squared  error (MSE).", "labels": [], "entities": [{"text": "SICK", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.9139518141746521}, {"text": "Pearson's r", "start_pos": 248, "end_pos": 259, "type": "METRIC", "confidence": 0.8391815821329752}, {"text": "mean squared  error (MSE)", "start_pos": 279, "end_pos": 304, "type": "METRIC", "confidence": 0.966099480787913}]}, {"text": " Table 3: Test set results on MSRVID data. The B\u00e4r  et al. (2012) and\u0160ari\u00b4cand\u02c7and\u0160ari\u00b4and\u0160ari\u00b4c et al. (2012) results were  the top two submissions in the Semantic Textual  Similarity task at the SemEval-2012 competition.", "labels": [], "entities": [{"text": "MSRVID data", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.8538062572479248}, {"text": "Semantic Textual  Similarity task", "start_pos": 156, "end_pos": 189, "type": "TASK", "confidence": 0.8633015751838684}, {"text": "SemEval-2012 competition", "start_pos": 197, "end_pos": 221, "type": "TASK", "confidence": 0.7672719955444336}]}, {"text": " Table 4: Ablation study over test sets of all three datasets. Nine components are divided into four groups.  We remove components one at a time and show differences.", "labels": [], "entities": []}]}