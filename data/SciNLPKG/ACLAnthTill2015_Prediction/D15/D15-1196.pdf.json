{"title": [{"text": "Online Learning of Interpretable Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings encode semantic meanings of words into low-dimension word vectors.", "labels": [], "entities": []}, {"text": "In most word embeddings, one cannot interpret the meanings of specific dimensions of those word vectors.", "labels": [], "entities": []}, {"text": "Non-negative matrix factorization (NMF) has been proposed to learn interpretable word embeddings via non-negative constraints.", "labels": [], "entities": []}, {"text": "However, NMF methods suffer from scale and memory issue because they have to maintain a global matrix for learning.", "labels": [], "entities": [{"text": "memory", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9839481711387634}]}, {"text": "To alleviate this challenge, we propose on-line learning of interpretable word embed-dings from streaming text data.", "labels": [], "entities": []}, {"text": "Experiments show that our model consistently outperforms the state-of-the-art word embedding methods in both representation ability and interpretability.", "labels": [], "entities": []}, {"text": "The source code of this paper can be obtained from http: //github.com/skTim/OIWE.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings () aim to encode semantic meanings of words into lowdimensional dense vectors.", "labels": [], "entities": []}, {"text": "As compared with traditional one-hot representation and distributional representation, word embeddings can better address the sparsity issue and have achieved success in many NLP applications recent years.", "labels": [], "entities": []}, {"text": "There are two typical approaches for word embeddings.", "labels": [], "entities": []}, {"text": "The neural-network (NN) approach () employs neural-based techniques to learn word embeddings.", "labels": [], "entities": []}, {"text": "The matrix factorization (MF) approach builds word embeddings by factorizing wordcontext co-occurrence matrices.", "labels": [], "entities": [{"text": "matrix factorization (MF)", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.6618459582328796}]}, {"text": "The MF approach requires a global statistical matrix, while the N-N approach can flexibly perform learning from * Corresponding author: Z.", "labels": [], "entities": []}, {"text": "Liu (liuzy@tsinghua.edu.cn) streaming text data, which is efficient in both computation and memory.", "labels": [], "entities": []}, {"text": "For example, two recent NN methods, Skip-Gram and Continuous Bagof-Word Model (CBOW) (), have achieved impressive impact due to their simplicity and efficiency.", "labels": [], "entities": []}, {"text": "For most word embedding methods, a critical issue is that, we are unaware of what each dimension represent in word embeddings.", "labels": [], "entities": []}, {"text": "Hence, the latent dimension for which a word has its largest value is difficult to interpret.", "labels": [], "entities": []}, {"text": "This makes word embeddings like a black-box, and prevents them from being human-readable and further manipulation.", "labels": [], "entities": []}, {"text": "People have proposed non-negative matrix factorization (NMF) for word representation, denoted as non-negative sparse embedding (NNSE).", "labels": [], "entities": [{"text": "word representation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7464558780193329}]}, {"text": "NNSE realizes interpretable word embeddings by applying non-negative constraints for word embeddings.", "labels": [], "entities": []}, {"text": "Although NNSE learns word embeddings with good interpretabilities, like other MF methods, it also requires a global matrix for learning, thus suffers from heavy memory usage and cannot well deal with streaming text data.", "labels": [], "entities": []}, {"text": "Inspired by the characteristics of NMF methods (), we note that, nonnegative constraints only allow additive combinations instead of subtractive combinations, and lead to a parts-based representation.", "labels": [], "entities": []}, {"text": "Hence, the non-negative constraints derive interpretabilities of word embeddings.", "labels": [], "entities": []}, {"text": "In this paper, we aim to design an online NN method to efficiently learn interpretable word embeddings.", "labels": [], "entities": []}, {"text": "In order to achieve the goal of interpretable embeddings, we design projected gradient descent for optimization so as to apply non-negative constraints on NN methods such as Skip-Gram.", "labels": [], "entities": []}, {"text": "We also employ adaptive gradient descent ( to speedup learning convergence.", "labels": [], "entities": []}, {"text": "We name the proposed models as online interpretable word embeddings (OIWE).", "labels": [], "entities": []}, {"text": "For experiments, we implement OIWE based on Skip-Gram.", "labels": [], "entities": [{"text": "OIWE", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.8463194966316223}]}, {"text": "We evaluate the representation performance of word embedding methods on the word similarity computation task.", "labels": [], "entities": [{"text": "word similarity computation task", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.7867446765303612}]}, {"text": "Experiment results show that, our OIWE models are significantly superior to other baselines including SkipGram, RNN and NNSE.", "labels": [], "entities": [{"text": "SkipGram", "start_pos": 102, "end_pos": 110, "type": "DATASET", "confidence": 0.9535182118415833}]}, {"text": "We also evaluate the interpretability performance on the word intrusion detection task.", "labels": [], "entities": [{"text": "word intrusion detection task", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.8681035339832306}]}, {"text": "The results demonstrate the effectiveness of OIWE as compared to NNSE.", "labels": [], "entities": [{"text": "OIWE", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.6609742045402527}]}], "datasetContent": [{"text": "In this section, we investigate the representation performance and interpretability of our OIWE models with other baselines including typical N-N and MF methods.", "labels": [], "entities": []}, {"text": "The representation performance is evaluated with the word similarity computation task, and the interpretability is evaluated with the word intrusion detection task.", "labels": [], "entities": [{"text": "word intrusion detection", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.7264737089474996}]}, {"text": "For the both tasks, we train our OIWE models using the text8 corpus obtained from word2vec website 1 , and the OIWE models achieve the best performance by setting the dimension number K = 300, \u03b2 = 0.6, \u03b4 = 1/60, and \u03b3 L = 2.5 \u00d7 10 \u22126 .", "labels": [], "entities": [{"text": "text8 corpus obtained from word2vec website 1", "start_pos": 55, "end_pos": 100, "type": "DATASET", "confidence": 0.8719434525285449}, {"text": "dimension number K", "start_pos": 167, "end_pos": 185, "type": "METRIC", "confidence": 0.9388250509897867}]}], "tableCaptions": [{"text": " Table 1: Spearman coefficient results (%) on word  similarity computation.", "labels": [], "entities": [{"text": "Spearman coefficient", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8108341097831726}, {"text": "word  similarity computation", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.7805138826370239}]}, {"text": " Table 2: Experiment results (%) on word intrusion  detection.", "labels": [], "entities": [{"text": "word intrusion  detection", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.8400938312212626}]}]}