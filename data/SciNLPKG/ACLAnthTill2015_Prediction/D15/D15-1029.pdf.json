{"title": [{"text": "Pre-Computable Multi-Layer Neural Network Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "In the last several years, neural network models have significantly improved accuracy in a number of NLP tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9989141225814819}]}, {"text": "However , one serious drawback that has impeded their adoption in production systems is the slow runtime speed of neu-ral network models compared to alternate models, such as maximum entropy classi-fiers.", "labels": [], "entities": []}, {"text": "(2014), the authors presented a simple technique for speeding up feed-forward embedding-based neural network models, where the dot product between each word embedding and part of the first hidden layer are pre-computed of-fline.", "labels": [], "entities": []}, {"text": "However, this technique cannot be used for hidden layers beyond the first.", "labels": [], "entities": []}, {"text": "In this paper, we explore a neural network architecture where the embedding layer feeds into multiple hidden layers that are placed \"next to\" one another so that each can be pre-computed independently.", "labels": [], "entities": []}, {"text": "On a large scale language modeling task, this architecture achieves a 10x speedup at run-time and a significant reduction in perplex-ity when compared to a standard multi-layer network.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural network models have become extremely popular in the last several years fora wide variety of NLP tasks, including language modeling, sentiment analysis (, translation modeling (, and many others).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 120, "end_pos": 137, "type": "TASK", "confidence": 0.7896959185600281}, {"text": "sentiment analysis", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.9233802556991577}, {"text": "translation modeling", "start_pos": 161, "end_pos": 181, "type": "TASK", "confidence": 0.9836630821228027}]}, {"text": "However, a serious drawback of neural network models is their slow speeds in training and test time (runtime) relative to alternative models such as maximum entropy or backoff models.", "labels": [], "entities": []}, {"text": "One popular application of neural network models in NLP is using neural network language models (NNLMs) as an additional feature in an existing machine translation (MT) or automatic speech recognition (ASR) engines.", "labels": [], "entities": [{"text": "machine translation (MT) or automatic speech recognition (ASR)", "start_pos": 144, "end_pos": 206, "type": "TASK", "confidence": 0.8136279284954071}]}, {"text": "NNLMs are particularly costly in this scenario, since decoding a single sentence typically requires tens of thousands or more n-gram lookups.", "labels": [], "entities": []}, {"text": "Although we will focus on this particular scenario in this paper, it is important to note that the techniques presented generalize to any feed-forward embedding-based neural network model.", "labels": [], "entities": []}, {"text": "One popular technique for improving the runtime speed of NNLMs involves training the network to be \"approximately normalized,\" so that the softmax normalizer does not have to be computed after training.", "labels": [], "entities": []}, {"text": "Two algorithms have been proposed to achieve this: (1) noise-contrastive estimation (NCE)) and (2) explicit self-normalization, which is used in this paper.", "labels": [], "entities": [{"text": "noise-contrastive estimation (NCE))", "start_pos": 55, "end_pos": 90, "type": "METRIC", "confidence": 0.7938484787940979}]}, {"text": "However, even with self-normalized networks, computing the output of an intermediate hidden layer still requires a costly matrix-vector multiplication.", "labels": [], "entities": []}, {"text": "To mitigate this, made the observation that for 1-layer NNLMs, the dot product between each embedding+position pair and the first hidden layer can be pre-computed after training is complete, which allows the matrixvector multiplication to be replaced by a handful of vector additions.", "labels": [], "entities": []}, {"text": "Using these two techniques in combination improves the runtime speed of NNLMs by several orders of magnitude with no degradation to accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9975070357322693}]}, {"text": "To understand pre-computation, first assume that we are training a NNLM that uses 250-dimensional word embeddings, a four word context window, and a 500-dimensional hidden layer.", "labels": [], "entities": []}, {"text": "The weight matrix for the first hidden layer is thus 1000 \u00d7 500.", "labels": [], "entities": []}, {"text": "For each word in the vocabulary and each of the four positions in the context vector, we  can pre-compute the dot product between the 250-dimensional word embedding and the 250 \u00d7 500 section of the hidden layer.", "labels": [], "entities": []}, {"text": "This results in four 500-dimensional vectors for each word that can be stored in a lookup table.", "labels": [], "entities": []}, {"text": "At test time, we can simply sum four vectors to obtain the output of the first hidden layer.", "labels": [], "entities": []}, {"text": "This is shown visually in.", "labels": [], "entities": []}, {"text": "Note that this is not an approximation, and the resulting output vector is identical to the original matrix-vector product.", "labels": [], "entities": []}, {"text": "However, the major limitation of the \"pre-computation trick\" is that it only works with 1-hidden layer architectures, even though more accurate models can nearly always be obtained by training multi-layer networks.", "labels": [], "entities": []}, {"text": "In this paper, we explore a network architecture where multiple hidden layers are placed \"next to\" one another instead of \"on top of\" one another, as is usually done.", "labels": [], "entities": []}, {"text": "The output of these lateral layers are combined using an inexpensive elementwise function and fed into the output layer.", "labels": [], "entities": []}, {"text": "Crucially, then, we can apply the pre-computation trick to each hidden layer independently, allowing for very powerful models that are orders of magnitude faster at runtime than a standard multi-layer network.", "labels": [], "entities": []}, {"text": "Mathematically, this can bethought of as a generalization of maxout networks (, where different element-wise combination functions are explored rather than just the max function.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Perplexity of 5-gram models on the New  York Times test set. k is the size of the hidden  layer(s).", "labels": [], "entities": [{"text": "New  York Times test set", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.846574330329895}]}, {"text": " Table 4: Results on English-German machine  translation test set.", "labels": [], "entities": [{"text": "machine  translation test set", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.7090433239936829}]}]}