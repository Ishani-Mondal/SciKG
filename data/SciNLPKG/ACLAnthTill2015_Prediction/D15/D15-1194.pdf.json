{"title": [{"text": "Adapting Phrase-based Machine Translation to Normalise Medical Terms in Social Media Messages", "labels": [], "entities": [{"text": "Adapting Phrase-based Machine Translation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8779859095811844}, {"text": "Normalise Medical Terms in Social Media Messages", "start_pos": 45, "end_pos": 93, "type": "TASK", "confidence": 0.8393233503614154}]}], "abstractContent": [{"text": "Previous studies have shown that health reports in social media, such as Dai-lyStrength and Twitter, have potential for monitoring health conditions (e.g. adverse drug reactions, infectious diseases) in particular communities.", "labels": [], "entities": [{"text": "Dai-lyStrength", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.9304409623146057}]}, {"text": "However, in order fora machine to understand and make inferences on these health conditions, the ability to recognise when laymen's terms refer to a particular medical concept (i.e. text normalisation) is required.", "labels": [], "entities": [{"text": "text normalisation)", "start_pos": 182, "end_pos": 201, "type": "TASK", "confidence": 0.738126277923584}]}, {"text": "To achieve this, we propose to adapt an existing phrase-based machine translation (MT) technique and a vector representation of words to map between asocial media phrase and a medical concept.", "labels": [], "entities": [{"text": "phrase-based machine translation (MT)", "start_pos": 49, "end_pos": 86, "type": "TASK", "confidence": 0.7899858852227529}]}, {"text": "We evaluate our proposed approach using a collection of phrases from tweets related to adverse drug reactions.", "labels": [], "entities": []}, {"text": "Our experimental results show that the combination of a phrase-based MT technique and the similarity between word vector representations outperforms the baselines that apply only either of them by up to 55%.", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.8495568633079529}]}], "introductionContent": [{"text": "Social media, such as DailyStrength 1 and Twitter 2 , is a fast growing and potentially rich source of voice of the patient data about experience in terms of benefits and side-effects of drugs and treatments).", "labels": [], "entities": []}, {"text": "However, natural language understanding from social media messages is a difficult task because of the lexical and grammatical variability of the language ().", "labels": [], "entities": [{"text": "natural language understanding from social media messages", "start_pos": 9, "end_pos": 66, "type": "TASK", "confidence": 0.7972412194524493}]}, {"text": "Indeed, language understanding by machines requires the ability to recognise when a phrase refers to a particular concept.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 8, "end_pos": 30, "type": "TASK", "confidence": 0.7195104956626892}]}, {"text": "Given a variable length phrase, an effective system should return a concept with the most similar meaning.", "labels": [], "entities": []}, {"text": "shows examples of mappings between Twitter phrases and medical concepts.", "labels": [], "entities": []}, {"text": "For example, a Twitter phrase 'No way I'm gettin any sleep 2nite' might be mapped to the medical concept, when using the SNOMED-CT ontology).", "labels": [], "entities": []}, {"text": "The success of the mapping between social media phrases and formal medical concepts would enable an automatic integration between patient experiences and biomedical databases (.", "labels": [], "entities": []}, {"text": "We refer to this mapping from social media phrases to medical concepts as medical term normalisation, which aims to determine the unique identifier of a medical concept that is mentioned in different forms in a free-text.", "labels": [], "entities": [{"text": "medical term normalisation", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.6896898945172628}]}, {"text": "Existing works, e.g. (, mostly focused on identifying medical concepts in medical documents.", "labels": [], "entities": [{"text": "identifying medical concepts in medical documents", "start_pos": 42, "end_pos": 91, "type": "TASK", "confidence": 0.7585298220316569}]}, {"text": "For example, proposed a na\u00a8\u0131vena\u00a8\u0131ve Bayesian-based technique to map phrases from clinical notes to medical concepts in the SNOMED-CT ontology.", "labels": [], "entities": []}, {"text": "identified medical concepts regarding adverse drug events in electronic medical records.", "labels": [], "entities": []}, {"text": "On the other hand, O' investigated the normalisation of medical terms in Twitter messages.", "labels": [], "entities": [{"text": "O", "start_pos": 19, "end_pos": 20, "type": "DATASET", "confidence": 0.6095351576805115}, {"text": "normalisation of medical terms in Twitter messages", "start_pos": 39, "end_pos": 89, "type": "TASK", "confidence": 0.8698537519999913}]}, {"text": "In particular, they proposed to use the Lucene retrieval engine 3 to retrieve medical concepts that could be potentially mapped to a given Twitter phrase, when mapping between Twitter phrases and medical concepts.", "labels": [], "entities": []}, {"text": "In contrast, we argue that the medical text normalisation task can be achieved by using wellestablished phrase-based MT techniques, where we translate a text written in asocial media language (e.g. 'No way I'm gettin any sleep 2nite') to a text written in a formal medical language (e.g. 'Insomnia') and then calculate the similarity between the translated phrase and the description of a medical concept.", "labels": [], "entities": [{"text": "medical text normalisation", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.6552032927672068}, {"text": "MT", "start_pos": 117, "end_pos": 119, "type": "TASK", "confidence": 0.856394350528717}]}, {"text": "Indeed, in this work we investigate an effective adaptation of phrase-based MT to map a Twitter phrase to a medical concept.", "labels": [], "entities": [{"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.7741194367408752}]}, {"text": "Moreover, we propose to combine the adapted phrasebased MT technique and the similarity between word vector representations to effectively map a Twitter phrase to a medical concept.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.6918999552726746}]}, {"text": "The main contributions of this paper are threefold: 1.", "labels": [], "entities": []}, {"text": "We investigate the adaptation of phrase-based MT to map a Twitter phrase to a SNOMED-CT concept.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.818579912185669}]}, {"text": "2. We propose to combine our adaptation of phrase-based MT and the similarity between word vector representations to map Twitter phrases to formal medical concepts.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.6621454358100891}]}, {"text": "3. We thoroughly evaluate the proposed approach using phrases from our collection of tweets related to the topic of adverse drug reactions (ADRs).", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments using 10-fold cross validation, where the Twitter phrases are randomly divided into 10 separated folds.", "labels": [], "entities": []}, {"text": "We address this task as a ranking task, where we aim to rank the medical concept with the highest similarity score, e.g. calculated using Equation (2), at the top rank.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 98, "end_pos": 114, "type": "METRIC", "confidence": 0.9691602289676666}, {"text": "Equation", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9342479705810547}]}, {"text": "Hence, we evaluate our approach using Mean Reciprocal Rank (MRR) measure, which is an information retrieval measure based on the user model where the user wants to see only one relevant concept.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR) measure", "start_pos": 38, "end_pos": 72, "type": "METRIC", "confidence": 0.9518514956746783}]}, {"text": "In particular, MRR is based on the the reciprocal of the rank at which the first relevant concept is viewed in the ranking (e.g. MRR = 0.5 if the first mapped concept is wrong but the second is correct).", "labels": [], "entities": [{"text": "MRR", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.7228512167930603}, {"text": "MRR", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.9646419882774353}]}, {"text": "We limit our evaluation at top 5 of the ranking (i.e. MRR-5).", "labels": [], "entities": [{"text": "MRR-5", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.8080382347106934}]}, {"text": "In addition, we compare the significant difference between the performance achieved by our proposed approach and the baselines using the paired t-test (p < 0.05).", "labels": [], "entities": []}, {"text": "We evaluate 6 different instantiations of the proposed approach discussed in Section 3, including: 1.", "labels": [], "entities": []}, {"text": "bestMT: set k = 1, when finding the translated phrase phr m fora Twitter phrase phr t (Equation), before ranking target medical concepts for the translated phrase phr musing Equation (2).", "labels": [], "entities": []}, {"text": "2. top5MT: similar to bestMT, but set k = 5. 3. top5MTr: similar to top5MT, but also consider the rank position of the translate phrases when ranking the target medical concepts by using Equation (3).", "labels": [], "entities": [{"text": "Equation", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.920953094959259}]}, {"text": "4. bestMT+vSim: incorporate with the ranking generated from bestMT, the cosine similarity between the vector representations of the Twitter phrase phr t and the description desc c of target medical concepts by using Equation (4).", "labels": [], "entities": []}, {"text": "5. top5MT+vSim: similar to bestMT+vSim, but use the ranking from top5MT.", "labels": [], "entities": []}, {"text": "6. top5MTr+vSim: similar to bestMT+vSim, but use the ranking from top5MTr.", "labels": [], "entities": []}, {"text": "Another baseline is vSim, where we consider only the cosine similarity between the vector representations of the Twitter phrase phr t and the description desc c of target medical concepts.", "labels": [], "entities": [{"text": "vSim", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.8513087034225464}]}, {"text": "compares the performance of these 6 instantiations and the vSim baseline in terms of MRR-5.", "labels": [], "entities": [{"text": "vSim baseline", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.8004507422447205}, {"text": "MRR-5", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.6168352365493774}]}, {"text": "We firstly observe that for the vSim baseline, excepting for word vector representation with vector size 50 learned using GloVe from the Twitter collection, word vector representations learned using either CBOW or GloVe are more effective than the one-hot representation.", "labels": [], "entities": [{"text": "vSim baseline", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.8260117471218109}, {"text": "word vector representation", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.6879733403523763}, {"text": "GloVe from the Twitter collection", "start_pos": 122, "end_pos": 155, "type": "DATASET", "confidence": 0.6247518062591553}]}, {"text": "However, the difference between the MRR-5 performance is not statistically significant (p > 0.05, paired t-test).", "labels": [], "entities": []}, {"text": "In addition, word vector representations learned either using CBOW or GloVe with vector size 200 is more effective than those with vector size 50.", "labels": [], "entities": []}, {"text": "Next, we find that our adaptation of phrasebased MT (i.e. bestMT, top5MT and top5MTr) significantly (p < 0.05) outperforms the vSim baseline.", "labels": [], "entities": []}, {"text": "For example, with the one-hot representation, top5MT (MRR-5 0.2491) and top5MTr (MRR-5 0.2458) perform significantly (p < 0.05) better than vSim (MRR-5 0.1675) by up to 49%.", "labels": [], "entities": []}, {"text": "Meanwhile, when using word vector representations with the vector size 200 learned using GloVe from the BMC collection, top5MT (MRR-5 0.2638) significantly (p < 0.05) outperforms vSim with either the GloVe vector representation (MRR-5 0.1869) or the one-hot representation (MRR-5 0.1675).", "labels": [], "entities": [{"text": "BMC collection", "start_pos": 104, "end_pos": 118, "type": "DATASET", "confidence": 0.9566739797592163}]}, {"text": "We observe the similar trends in performance: MRR-5 performance of the proposed approach and the baselines.", "labels": [], "entities": [{"text": "MRR-5", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.7332437634468079}]}, {"text": "Significant differences (p < 0.05) compared to the cosine similarity (vSim) baselines with the one-hot representation, and with the corresponding distributed word representation (e.g. CBOW or GloVe) are denoted and , respectively.", "labels": [], "entities": [{"text": "cosine similarity (vSim) baselines", "start_pos": 51, "end_pos": 85, "type": "METRIC", "confidence": 0.8210613032182058}]}], "tableCaptions": [{"text": " Table 2: MRR-5 performance of the proposed approach and the baselines. Significant differences (p <  0.05) compared to the cosine similarity (vSim) baselines with the one-hot representation, and with the  corresponding distributed word representation (e.g. CBOW or GloVe) are denoted and , respectively.", "labels": [], "entities": [{"text": "cosine similarity (vSim) baselines", "start_pos": 124, "end_pos": 158, "type": "METRIC", "confidence": 0.7935023407141367}]}]}