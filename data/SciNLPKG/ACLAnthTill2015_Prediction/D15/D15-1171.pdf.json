{"title": [{"text": "Solving Geometry Problems: Combining Text and Diagram Interpretation", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces GEOS, the first automated system to solve unaltered SAT geometry questions by combining text understanding and diagram interpretation.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.5581437349319458}, {"text": "SAT geometry questions", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.8100311160087585}, {"text": "text understanding", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.701964721083641}, {"text": "diagram interpretation", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.8439232707023621}]}, {"text": "We model the problem of understanding geometry questions as submodular optimization , and identify a formal problem description likely to be compatible with both the question text and diagram.", "labels": [], "entities": [{"text": "understanding geometry questions", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.8645097613334656}]}, {"text": "GEOS then feeds the description to a geometric solver that attempts to determine the correct answer.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9283501505851746}]}, {"text": "In our experiments, GEOS achieves a 49% score on official SAT questions , and a score of 61% on practice questions.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.4340934753417969}]}, {"text": "1 Finally, we show that by integrating textual and visual information, GEOS boosts the accuracy of dependency and semantic parsing of the question text.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.5248463749885559}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9991403818130493}, {"text": "semantic parsing of the question text", "start_pos": 114, "end_pos": 151, "type": "TASK", "confidence": 0.8194757699966431}]}], "introductionContent": [{"text": "This paper introduces the first fully-automated system for solving unaletered SAT-level geometric word problems, each of which consists of text and the corresponding diagram ().", "labels": [], "entities": []}, {"text": "The geometry domain has along history in AI, but previous work has focused on geometric theorem proving or geometric analogies.", "labels": [], "entities": [{"text": "geometric theorem proving", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.7616060376167297}]}, {"text": "Arithmetic and algebraic word problems have attracted several NLP researchers), but geometric word problems were first explored only last year by.", "labels": [], "entities": []}, {"text": "Still, this system merely aligned diagram elements with their textual mentions (e.g., \"Circle O\")-it did not attempt to fully represent geometry problems or solve them.", "labels": [], "entities": []}, {"text": "Answering geometry questions requires a method that interpert question text and diagrams in concert.", "labels": [], "entities": [{"text": "Answering geometry questions", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9332075516382853}]}, {"text": "In the diagram at the le., circle O has a radius of 5, and CE = 2.", "labels": [], "entities": [{"text": "CE", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.993493378162384}]}, {"text": "Diameter AC is perpendicular to chord BD.", "labels": [], "entities": [{"text": "BD", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.8363526463508606}]}, {"text": "What is the length of BD?", "labels": [], "entities": [{"text": "length", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.998180627822876}, {"text": "BD", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.4992330074310303}]}, {"text": "In isosceles triangle ABC at the le., lines AM and CM are the angle bisectors of angles BAC and BCA.", "labels": [], "entities": [{"text": "AM", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.8881465196609497}, {"text": "BAC", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9883684515953064}, {"text": "BCA", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.8843098878860474}]}, {"text": "What is the measure of angle AMC?", "labels": [], "entities": [{"text": "angle AMC", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9546882808208466}]}, {"text": "In the figure at le., The bisector of angle BAC is perpendicular to BC at point D.", "labels": [], "entities": [{"text": "BAC", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.6139732003211975}, {"text": "BC", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9921205043792725}]}, {"text": "If AB = 6 and BD = 3, what is the measure of angle BAC?", "labels": [], "entities": [{"text": "AB", "start_pos": 3, "end_pos": 5, "type": "METRIC", "confidence": 0.9986687898635864}, {"text": "BD", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9969605803489685}, {"text": "angle BAC", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.8247078061103821}]}, {"text": "The geometry genre has several distinctive characteristics.", "labels": [], "entities": []}, {"text": "First, diagrams provide essential information absent from question text.", "labels": [], "entities": []}, {"text": "In problem (a), for example, the unstated fact that lines BD and AC intersect at E is necessary to solve the problem.", "labels": [], "entities": []}, {"text": "Second, the text often includes difficult references to diagram elements.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"In the diagram, the longer line is tangent to the circle\", resolving the referent of the phrase \"longer line\" is challenging.", "labels": [], "entities": []}, {"text": "Third, the text often contains implicit relations.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"AB is 5\", the relations IsLine(AB) and length(AB)=5 are implicit.", "labels": [], "entities": [{"text": "IsLine(AB)", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9373136460781097}, {"text": "length(AB)=5", "start_pos": 69, "end_pos": 81, "type": "METRIC", "confidence": 0.9089149832725525}]}, {"text": "Fourth, geometric terms can be ambiguous as well.", "labels": [], "entities": []}, {"text": "For instance, radius can be a type identifier in \"the length of radius AO is 5\", or a predicate in \"AO is the radius of circle O\".", "labels": [], "entities": []}, {"text": "Fifth, identifying the correct arguments for each relation is challenging.", "labels": [], "entities": []}, {"text": "For example, in sentence \"Lines AB and CD are perpendicular to EF\", the parser has to determine what is perpendicular to EF-line AB?", "labels": [], "entities": []}, {"text": "Or both AB and CD?", "labels": [], "entities": [{"text": "AB", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.9917027354240417}]}, {"text": "Finally, it is hard to obtain large number of SAT-level geometry questions; Learning from a few examples makes this a particularly challenging NLP problem.", "labels": [], "entities": [{"text": "SAT-level geometry questions", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.6710437536239624}]}, {"text": "This paper introduces GEOS, a system that maps geometry word problems into a logical representation that is compatible with both the problem text and the accompanying diagram.", "labels": [], "entities": []}, {"text": "We cast the mapping problem as the problem of selecting the subset of relations that is most likely to correspond to each question.", "labels": [], "entities": []}], "datasetContent": [{"text": "Logical Language \u2126: \u2126 consists of 13 types of entities and 94 function and predicates observed in our development set of geometry questions.", "labels": [], "entities": []}, {"text": "Implementation details: Sentences in geometry questions often contain in-line mathematical expressions, such as \"If AB=x+5, what is x?\".", "labels": [], "entities": []}, {"text": "These mathematical expressions cause general purpose parsers to fail.", "labels": [], "entities": []}, {"text": "GEOS uses an equation analyzer and pre-processes question text by replacing \"=\" with \"equals\", and replacing mathematical terms (e.g., \"x+5\") with a dummy noun so that the dependency parser does not fail.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8701046109199524}]}, {"text": "GEOS uses Stanford dependency parser) to obtain syntactic information, which is used to compute features for relation identification.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9257940649986267}, {"text": "relation identification", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.8497180342674255}]}, {"text": "For diagram parsing, similar to, we assume that GEOS has access to ground truth optical character recognition for labels in the diagrams.", "labels": [], "entities": [{"text": "diagram parsing", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7990433275699615}, {"text": "GEOS", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.949016273021698}, {"text": "ground truth optical character recognition", "start_pos": 67, "end_pos": 109, "type": "TASK", "confidence": 0.6821380853652954}]}, {"text": "For optimization, we tune the parameters \u03bb to 0.5, based on the training examples.", "labels": [], "entities": []}, {"text": "4 Dataset: We built a dataset of SAT plane geometry questions where every question has a tex-: Data and annotation statistics tual description in English accompanied by a diagram and multiple choices.", "labels": [], "entities": []}, {"text": "Questions and answers are compiled from previous official SAT exams and practice exams offered by the College Board.", "labels": [], "entities": []}, {"text": "In addition, we use a portion of the publicly available high-school plane geometry questions () as our training set.", "labels": [], "entities": [{"text": "high-school plane geometry", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.6824359695116679}]}, {"text": "We annotate ground-truth logical forms for all questions in the dataset.", "labels": [], "entities": []}, {"text": "shows details of the data and annotation statistics.", "labels": [], "entities": []}, {"text": "For evaluating dependency parsing, we annotate 50 questions with the ground truth dependency tree structures of all sentences in the questions.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8161428868770599}]}, {"text": "5 Baselines: Rule-based text parsing + GEOS diagram solves geometry questions using literals extracted from a manually defined set of rules over the textual dependency parser, and scored by diagram.", "labels": [], "entities": [{"text": "Rule-based text parsing", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.6014422476291656}]}, {"text": "For this baseline, we manually designed 12 high-precision rules based on the development set.", "labels": [], "entities": []}, {"text": "Each rule compares the dependency tree of each sentence to pre-defined templates, and if a template pattern is matched, the rule outputs the relation or function structure corresponding to that template.", "labels": [], "entities": []}, {"text": "For example, a rule assigns a relation parent(child-1, child-2) fora triplet of (parent, child-1, child-2) where child-1 is the subject of parent and child-2 is the object of the parent.", "labels": [], "entities": []}, {"text": "GEOS without text parsing solves geometry questions using a simple heuristic.", "labels": [], "entities": [{"text": "text parsing", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.6903010010719299}]}, {"text": "With simple textual processing, this baseline extracts numerical relations from the question text and then computes the scale between the units in the question and the pixels in the diagram.", "labels": [], "entities": []}, {"text": "This baseline rounds the number to the closest choice available in the multiple choices.", "labels": [], "entities": []}, {"text": "GEOS without diagram parsing solves geometry questions only relying on the literals interpreted from the text.", "labels": [], "entities": [{"text": "diagram parsing", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.6903170794248581}]}, {"text": "It outputs all literals whose text scores are higher than a tuned threshold, 0.6 on the training set.", "labels": [], "entities": []}, {"text": "GEOS without relation completion solves ge- The source code, the dataset and the annotations are publicly available at geometry.allenai.org.", "labels": [], "entities": []}, {"text": "ometry questions when text parsing does not use the intermediate representation and does not include the relation completion step.", "labels": [], "entities": [{"text": "text parsing", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.7081485390663147}]}, {"text": "We evaluate our method on three tasks: solving geometry question, interpreting geometry questions, and dependency parsing.", "labels": [], "entities": [{"text": "solving geometry question", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.9032165209452311}, {"text": "interpreting geometry questions", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.9113229910532633}, {"text": "dependency parsing", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.816321462392807}]}, {"text": "Solving Geometry Questions: compares the score of GEOS in solving geometry questions in practice and official SAT questions with that of baselines.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.80141681432724}]}, {"text": "SAT's grading scheme penalizes a wrong answer with a negative score of 0.25.", "labels": [], "entities": [{"text": "SAT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7345578074455261}]}, {"text": "We report the SAT score as the percentage of correctly answered questions penalized by the wrong answers.", "labels": [], "entities": [{"text": "SAT score", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9041438400745392}]}, {"text": "For official questions, GEOS answers 27 questions correctly, 1 questions incorrectly, and leaves 27 un-answered, which gives it a score of 26.75 out of 55, or 49%.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.46003401279449463}]}, {"text": "Thus, GEOS's precision exceeds 96% on the 51% of questions that it chooses to answer.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9957127571105957}]}, {"text": "For practice SAT questions, GEOS scores 61%.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.8504480719566345}]}, {"text": "In order to understand the effect of individual components of GEOS, we compare the full method with a few ablations.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.6669607162475586}]}, {"text": "GEOS significantly outperforms the two baselines GEOS without text parsing and GEOS without diagram parsing, demonstrating that GEOS benefits from both text and diagram parsing.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9250091314315796}, {"text": "GEOS", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.8665502071380615}, {"text": "text parsing", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.6639739573001862}, {"text": "GEOS without diagram parsing", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.6092368736863136}, {"text": "GEOS", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.794165313243866}, {"text": "diagram parsing", "start_pos": 161, "end_pos": 176, "type": "TASK", "confidence": 0.7182814478874207}]}, {"text": "In order to understand the text parsing component, we compare GEOS with Rule-based text parsing + GEOS Diagram and GEOS without relation completion.", "labels": [], "entities": [{"text": "text parsing", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.7074310630559921}, {"text": "GEOS", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.7318880558013916}, {"text": "Rule-based text parsing", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.5599373579025269}, {"text": "GEOS", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.7884488701820374}]}, {"text": "The results show that our method of learning to interpret literals from the text is substantially better than the rule-based baseline.", "labels": [], "entities": []}, {"text": "In addition, the relation completion step, which relies on the intermediate representation, helps to improve text interpretation.", "labels": [], "entities": [{"text": "relation completion", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.9071077108383179}, {"text": "text interpretation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.7821414470672607}]}, {"text": "Error Analysis: In order to understand the errors made by GEOS, we use oracle text parsing and oracle diagram parsing).", "labels": [], "entities": [{"text": "Error Analysis", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6711152791976929}, {"text": "GEOS", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.8856213688850403}, {"text": "oracle text parsing", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.6914039254188538}, {"text": "oracle diagram parsing", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.7116504708925883}]}, {"text": "Roughly 38% of the errors are due to failures in text parsing, and about 46% of errors are due to failures in diagram parsing.", "labels": [], "entities": [{"text": "text parsing", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.7530893087387085}, {"text": "diagram parsing", "start_pos": 110, "end_pos": 125, "type": "TASK", "confidence": 0.9213069081306458}]}, {"text": "Among them, about 15% of errors were due to failures in both diagram and text parsing.", "labels": [], "entities": [{"text": "errors", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.985618531703949}, {"text": "diagram and text parsing", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.6235930100083351}]}, {"text": "For an example of text parsing failure, the literals in (a) are not scored accurately due to missing coreference relations ().", "labels": [], "entities": [{"text": "text parsing", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.7717800438404083}]}, {"text": "The rest of errors are due to problems that require more complex reasoning).", "labels": [], "entities": [{"text": "errors", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9781994223594666}]}, {"text": "Interpreting Question Texts: details the precision and recall of GEOS in deriving literals for geometry question texts for official SAT questions.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9975492358207703}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.996894359588623}, {"text": "GEOS", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.6532415747642517}]}, {"text": "The rule-based text parsing baseline achieves a high precision, but at the cost of lower recall.", "labels": [], "entities": [{"text": "text parsing", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.6762770116329193}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9994328618049622}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9989533424377441}]}, {"text": "On the other hand, the baseline GEOS without diagram achieves a high recall, but at the cost of lower precision.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.7101337909698486}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.999760091304779}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9980155229568481}]}, {"text": "Nevertheless, GEOS attains substantially higher F1 score compared to both baselines, which is the key factor in solving the questions.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.6551350355148315}, {"text": "F1 score", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9703851044178009}]}, {"text": "Direct application of a generic semantic parser) with full supervision does not perform well in the geometry domain, mainly due to lack of enough training data.", "labels": [], "entities": []}, {"text": "Our initial investigations show the performance of 33% F1 in the official set.", "labels": [], "entities": [{"text": "F1", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9992886781692505}]}, {"text": "Improving Dependency Parsing: shows the results of different methods in dependency parsing.", "labels": [], "entities": [{"text": "Improving Dependency Parsing", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8552737236022949}, {"text": "dependency parsing", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.8406809270381927}]}, {"text": "GEOS returns a dependency parse tree by selecting the dependency tree that maximizes the text score in the objective function from the top 50 trees produced by a generic dependency parser, Stanford parser).", "labels": [], "entities": [{"text": "dependency parse", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.7066963315010071}]}, {"text": "Note that Stanford parser cannot handle mathematical symbols and equations.", "labels": [], "entities": []}, {"text": "We report the results of a baseline that extends the Stanford dependency parser by adding a pre-processing step to separate the mathematical expressions from the plain sentences (Section 8).", "labels": [], "entities": []}, {"text": "We evaluate the performance of GEOS against the best tree returned by Stanford parser by reporting the fraction of the questions whose dependency parse structures match the ground truth annotations.", "labels": [], "entities": [{"text": "GEOS", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.5852909088134766}]}, {"text": "Our results show an improvement of 16% over the Stanford dependency parser when equipped with the equation analyzer.", "labels": [], "entities": []}, {"text": "For example, in \"AB is perpendicular to CD at E\", the StanAccuracy Stanford dep parse 0.05 Stanford dep parse + eq. analyzer 0.64 GEOS 0.78: Accuracy of dependency parsing.", "labels": [], "entities": [{"text": "StanAccuracy Stanford dep parse 0.05 Stanford dep parse + eq. analyzer 0.64 GEOS 0.78", "start_pos": 54, "end_pos": 139, "type": "METRIC", "confidence": 0.6733769689287458}, {"text": "Accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9892856478691101}, {"text": "dependency parsing", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.776915580034256}]}, {"text": "In the figure at the le-, the smaller circles each have radius 3.", "labels": [], "entities": []}, {"text": "They are tangent to the larger circle at points A and C, and are tangent to each other at point B, which is the center of the larger circle.", "labels": [], "entities": []}, {"text": "What is the perimeter of the shaded region?", "labels": [], "entities": []}, {"text": "In the figure at the le-, a shaded polygon which has equal angles is parCally covered with a sheet of blank paper.", "labels": [], "entities": []}, {"text": "If x+y=80, how many sides does the polygon have?", "labels": [], "entities": []}, {"text": "ford dependency parser predicts that \"E\" depends on \"CD\", while GEOS predicts the correct parse in which \"E\" depends on \"perpendicular\".", "labels": [], "entities": [{"text": "GEOS", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.5113762021064758}]}], "tableCaptions": [{"text": " Table 2: Data and annotation statistics", "labels": [], "entities": []}, {"text": " Table 3: SAT scores of solving geometry questions.", "labels": [], "entities": [{"text": "SAT", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9092830419540405}, {"text": "solving geometry questions", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.8550387223561605}]}, {"text": " Table 4: Precision and recall of text interpretation.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8653512001037598}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9440507292747498}, {"text": "text interpretation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.6443186402320862}]}]}