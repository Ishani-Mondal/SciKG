{"title": [], "abstractContent": [{"text": "We present a novel framework of system combination for multi-document summarization.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.5782534182071686}]}, {"text": "For each input set (input), we generate candidate summaries by combining whole sentences from the summaries generated by different systems.", "labels": [], "entities": []}, {"text": "We show that the oracle among these candidates is much better than the summaries that we have combined.", "labels": [], "entities": []}, {"text": "We then present a supervised model to select among the candidates.", "labels": [], "entities": []}, {"text": "The model relies on a rich set of features that capture content importance from different perspectives.", "labels": [], "entities": []}, {"text": "Our model performs better than the systems that we combined based on manual and automatic evaluations.", "labels": [], "entities": []}, {"text": "We also achieve very competitive performance on six DUC/TAC datasets, comparable to the state-of-the-art on most datasets.", "labels": [], "entities": [{"text": "DUC/TAC datasets", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.8102041184902191}]}], "introductionContent": [{"text": "Recent work shows that state-of-the-art summarization systems generate very different summaries, despite the fact that they have similar performance ).", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9475491642951965}]}, {"text": "This suggests that combining summaries from different systems might be helpful in improving content quality.", "labels": [], "entities": []}, {"text": "A handful of papers have studied system combination for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9871017932891846}]}, {"text": "Based on the ranks of the input sentences assigned by different systems (i.e., basic systems), methods have been proposed to re-rank these sentences ().", "labels": [], "entities": []}, {"text": "However, these methods require the basic systems to assign importance scores to all input sentences.", "labels": [], "entities": []}, {"text": "combine the summaries from different systems, based on a graph-based measure that computes summary-input or summary-summary similarity.", "labels": [], "entities": []}, {"text": "However, their method does not show an advantage over the basic systems.", "labels": [], "entities": []}, {"text": "In summary, few prior papers have successfully generating better summaries by combining the summaries from different systems (i.e., basic summaries).", "labels": [], "entities": [{"text": "summaries", "start_pos": 65, "end_pos": 74, "type": "TASK", "confidence": 0.9849936962127686}]}, {"text": "This paper focuses on practical system combination, where we combine the summaries generated by four portable unsupervised systems.", "labels": [], "entities": []}, {"text": "We choose these systems, because: First, these systems are either off-the-shelf or easy-to-implement.", "labels": [], "entities": []}, {"text": "Second, even though many systems have been proposed for multi-document summarization, the output of them are often available only on one dataset or even unavailable.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.6012587547302246}]}, {"text": "Third, compared to more sophisticated supervised methods (), simple unsupervised methods perform unexpectedly well.", "labels": [], "entities": []}, {"text": "Many of them achieved the state-of-the-art performance when they were proposed ( and still serve as competitive baselines . After the summarizers have been chosen, we present a two-step pipeline that combines the basic summaries.", "labels": [], "entities": []}, {"text": "In the first step, we generate combined candidate summaries (Section 4).", "labels": [], "entities": []}, {"text": "We investigate two methods to do this: one uses entire basic summaries directly, the other combines these summaries on the sentence level.", "labels": [], "entities": []}, {"text": "We show that the latter method has a much higher oracle performance.", "labels": [], "entities": []}, {"text": "The second step includes anew supervised model that selects among the candidate summaries (Section 5).", "labels": [], "entities": []}, {"text": "Our contributions are: \u2022 We show that by combining summaries on the sentence level, the best possible (oracle) performance is very high.", "labels": [], "entities": []}, {"text": "\u2022 In the second step of our pipeline, we propose a supervised model that includes a rich set of new features.", "labels": [], "entities": []}, {"text": "These features capture content importance from different perspectives, based on different sources.", "labels": [], "entities": []}, {"text": "We verify the effectiveness of these features.", "labels": [], "entities": []}, {"text": "\u2022 Our method outperforms the basic systems and several competitive baselines.", "labels": [], "entities": []}, {"text": "Our model achieves competitive performance on six DUC/TAC datasets, which is on par with the state-of-the-art on most of these datasets.", "labels": [], "entities": [{"text": "DUC/TAC datasets", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.8563134372234344}]}, {"text": "\u2022 Our method can be used to combine summaries generated by any systems.", "labels": [], "entities": [{"text": "summaries generated", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8914713263511658}]}], "datasetContent": [{"text": "We conduct a large scale experiment on six datasets from the Document Understanding Conference (DUC) and the Text Analysis Conference (TAC).", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC)", "start_pos": 61, "end_pos": 100, "type": "TASK", "confidence": 0.6828743467728297}, {"text": "Text Analysis Conference (TAC)", "start_pos": 109, "end_pos": 139, "type": "TASK", "confidence": 0.8202162484327952}]}, {"text": "The tasks include generic) and query-focused) multi-document summarization.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.630968302488327}]}, {"text": "We evaluate on the task of generating 100-word summaries.", "labels": [], "entities": []}, {"text": "We use ROUGE) for automatic evaluation, which compares the machine summaries to the human references.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.9934155941009521}]}, {"text": "We report ROUGE-1 (unigram recall) and ROUGE-2 (bigram recall), with stemming and stopwords included.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9943307638168335}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.6759299635887146}, {"text": "ROUGE-2", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9954476952552795}, {"text": "bigram recall", "start_pos": 48, "end_pos": 61, "type": "METRIC", "confidence": 0.543682873249054}]}, {"text": "1 Among automatic evaluation metrics, ROUGE-1 (R-1) can predict that one system performs significantly better than the other with the highest recall (.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9932016730308533}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9992460012435913}]}, {"text": "ROUGE-2 (R-2) provides the best agreement with manual evaluations ().", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8378911018371582}]}, {"text": "R-1 and R-2 are the most widely used metrics in summarization literature.", "labels": [], "entities": [{"text": "summarization", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.9869215488433838}]}, {"text": "We use the DUC 03, 04 datasets as training and development sets.", "labels": [], "entities": [{"text": "DUC 03, 04 datasets", "start_pos": 11, "end_pos": 30, "type": "DATASET", "confidence": 0.9729248404502868}]}, {"text": "The candidate summaries of these two sets are used as training instances.", "labels": [], "entities": []}, {"text": "There are 80 input sets; each input includes an average of 3336 candidate summaries.", "labels": [], "entities": []}, {"text": "During development, we perform four-fold cross-validation.", "labels": [], "entities": []}, {"text": "The DUC 01, 02 and TAC 08, 09 datasets are used as the held-out test sets.", "labels": [], "entities": [{"text": "DUC 01", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.9233821034431458}, {"text": "TAC 08, 09 datasets", "start_pos": 19, "end_pos": 38, "type": "DATASET", "confidence": 0.8887293696403503}]}, {"text": "We use two-sided Wilcoxon test to compare the performance between two systems.", "labels": [], "entities": []}, {"text": "We choose ROUGE-1 (R-1) as training labels, as it outperforms using ROUGE-2 (R-2) as labels (see).", "labels": [], "entities": []}, {"text": "We suspect that the advantage of R-1 is because it has higher sensitivity in capturing the differences in content between summaries.", "labels": [], "entities": []}, {"text": "In order to find a better learning method, we have experimented with support vector regression (SVR) ( and SVM-Rank (Joachims, 1999).", "labels": [], "entities": []}, {"text": "7 SVR has been used for estimating sentence) or document () importance in summarization.", "labels": [], "entities": [{"text": "estimating sentence) or document () importance", "start_pos": 24, "end_pos": 70, "type": "TASK", "confidence": 0.8252893515995571}, {"text": "summarization", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.9607316255569458}]}, {"text": "SVM-Rank has been used for ranking summaries according to their linguistic qualities (.", "labels": [], "entities": [{"text": "SVM-Rank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8806957006454468}]}, {"text": "In SVM-Rank, only the relative ranks between training instances of an input are considered while learning the model.", "labels": [], "entities": []}, {"text": "Our experiment shows that SVR outperforms SVM-Rank (see).", "labels": [], "entities": []}, {"text": "This means that it is useful to compare the summaries across different input sets and leverage the actual ROUGE scores.: Performance on the development set with different models and training labels.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9825892448425293}]}], "tableCaptions": [{"text": " Table 1: The performance of the basic systems and the performance of the oracle systems based on the  methods described in Section 4.2.1 and Section 4.2.2. The evaluation metric that each oracle optimizes  is shown in Bold.", "labels": [], "entities": []}, {"text": " Table 2: Average number of sentences (# sents),  unique sentences (# unique), candidate summaries  per input (# summaries) and the total number of  candidate summaries for each dataset (# total).", "labels": [], "entities": []}, {"text": " Table 3: Performance on the development set with  different models and training labels.", "labels": [], "entities": []}, {"text": " Table 4: Performance comparison on six DUC  and TAC datasets. Bold indicates statistical  significant compared to ICSISumm (p < 0.05).   \u2020 indicates the difference is close to significant  compared to ICSISumm (0.05 \u2264 p < 0.1).", "labels": [], "entities": [{"text": "DUC  and TAC datasets", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.7456562519073486}, {"text": "ICSISumm", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.7836481928825378}, {"text": "ICSISumm", "start_pos": 202, "end_pos": 210, "type": "DATASET", "confidence": 0.8587233424186707}]}, {"text": " Table 5: The Pyramid score on the TAC 08 data.", "labels": [], "entities": [{"text": "Pyramid score", "start_pos": 14, "end_pos": 27, "type": "METRIC", "confidence": 0.906717449426651}, {"text": "TAC 08 data", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.9376334547996521}]}, {"text": " Table 6: Comparison with other combination  methods on the DUC 04 dataset.", "labels": [], "entities": [{"text": "DUC 04 dataset", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.9651837150255839}]}, {"text": " Table 7: Performance after ablating features (row 2-7) or using a single class of features (row 8-12).", "labels": [], "entities": []}]}