{"title": [{"text": "Joint Prediction for Entity/Event-Level Sentiment Analysis using Probabilistic Soft Logic Models", "labels": [], "entities": [{"text": "Entity/Event-Level Sentiment Analysis", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.6577515721321106}]}], "abstractContent": [{"text": "In this work, we build an entity/event-level sentiment analysis system, which is able to recognize and infer both explicit and implicit sentiments toward entities and events in the text.", "labels": [], "entities": [{"text": "entity/event-level sentiment analysis", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.7005030512809753}]}, {"text": "We design Probabilistic Soft Logic models that integrate explicit sentiments , inference rules, and +/-effect event information (events that positively or negatively affect entities).", "labels": [], "entities": []}, {"text": "The experiments show that the method is able to greatly improve over baseline accuracies in recognizing entity/event-level sentiments.", "labels": [], "entities": [{"text": "recognizing entity/event-level sentiments", "start_pos": 92, "end_pos": 133, "type": "TASK", "confidence": 0.8330339550971985}]}], "introductionContent": [{"text": "There are increasing numbers of opinions expressed in various genres, including reviews, newswire, editorials, and forums.", "labels": [], "entities": []}, {"text": "While much early work was at the document or sentence level, to fully understand and utilize opinions, researchers are increasingly carrying out more finegrained sentiment analysis to extract components of opinion frames: the source (whose sentiment is it), the polarity, and the target (what is the sentiment toward).", "labels": [], "entities": []}, {"text": "Much fine-grained analysis is span or aspect based).", "labels": [], "entities": []}, {"text": "In contrast, this work contributes to entity/event-level sentiment analysis.", "labels": [], "entities": [{"text": "entity/event-level sentiment analysis", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.6582022249698639}]}, {"text": "A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as \"Who is negative/positive toward X?\"", "labels": [], "entities": [{"text": "Automatic Question Answering", "start_pos": 112, "end_pos": 140, "type": "TASK", "confidence": 0.6405012309551239}]}, {"text": "(), where X could be any entity or event.", "labels": [], "entities": []}, {"text": "Let us consider an example from the MPQA opinion annotated corpus (.", "labels": [], "entities": [{"text": "MPQA opinion annotated corpus", "start_pos": 36, "end_pos": 65, "type": "DATASET", "confidence": 0.9268348515033722}]}, {"text": "When the Imam ( may God be satisfied with him 1 ) issued the fatwa against 2 Salman Rushdie for insulting 3 the Prophet ( peace be upon him 4 ), the countries that are so-called 5 supporters of human rights protested against 6 the fatwa.", "labels": [], "entities": []}, {"text": "There are several sentiment expressions annotated in MPQA.", "labels": [], "entities": [{"text": "MPQA", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.9586965441703796}]}, {"text": "In the first clause, the writer is positive toward Imam and Prophet as expressed by may God be satisfied with him (1) and peace be upon him (4), respectively.", "labels": [], "entities": []}, {"text": "Imam is negative toward Salman Rushdie and the insulting event, as revealed by the expression issued the fatwa against (2).", "labels": [], "entities": []}, {"text": "And Salman Rushdie is negative toward Prophet, as revealed by the expression insulting (3).", "labels": [], "entities": []}, {"text": "In the second clause, the writer is negative toward the countries, as expressed by so-called (5).", "labels": [], "entities": []}, {"text": "And the countries are negative toward fatwa, as revealed by the expression protested against (6).", "labels": [], "entities": []}, {"text": "Using the source and the target, we summarize the positive opinions above in a set P , and the negative opinions above in another set N . Thus, P contains {(writer, Imam), (writer, Prophet)}, and N contains {(Imam, Rushdie), (Imam, insulting), (Rushdie, Prophet), (writer, countries), (countries, fatwa)}.", "labels": [], "entities": []}, {"text": "An (ideal) explicit sentiment analysis system is expected to extract the above sentiments expressed by (1)-(6).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7710291147232056}]}, {"text": "However, there are many more sentiments communicated by the writer but not expressed via explicit expressions.", "labels": [], "entities": []}, {"text": "First, Imam is positive toward the Prophet, because Rushdie insults the Prophet and Imam is angry that he does so.", "labels": [], "entities": []}, {"text": "Second, the writer is negative toward Rushdie, because the writer is positive toward the Prophet but Rushdie insults him!", "labels": [], "entities": []}, {"text": "Also, the writer is probably positive toward the fatwa since it is against Rushdie.", "labels": [], "entities": []}, {"text": "Third, the countries are probably negative toward Imam, because the countries are negative toward fatwa and it is Imam who issued the fatwa.", "labels": [], "entities": []}, {"text": "Thus, the set P should also contain {(Imam, Prophet), (writer, fatwa)}, and the set N should also contain {(writer, Rushdie), (countries, Imam)}.", "labels": [], "entities": []}, {"text": "These opinions are not directly expressed, but are inferred by a human reader.", "labels": [], "entities": []}, {"text": "The explicit and implicit sentiments are summarized in, where each green line represents a positive sentiment and each red line represents a negative sentiment.", "labels": [], "entities": []}, {"text": "The solid lines are explicit sentiments and the dashed lines are implicit sentiments.", "labels": [], "entities": []}, {"text": "In this work, we detect sentiments such as those in P and N , where the sources are entities (or the writer) and the targets are entities and events.", "labels": [], "entities": []}, {"text": "Previous work in sentiment analysis mainly focuses on detecting explicit opinions.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.9631834030151367}, {"text": "detecting explicit opinions", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.8880326350529989}]}, {"text": "Recently there is emerging focus on sentiment inference, which recognizes implicit sentiments by inferring them from explicit sentiments via inference rules.", "labels": [], "entities": [{"text": "sentiment inference", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8517444729804993}]}, {"text": "Current works in sentiment inference differ on how the sentiment inference rules are defined and how they are expressed.", "labels": [], "entities": [{"text": "sentiment inference", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.9434240162372589}]}, {"text": "For example, Zhang and Liu (2011) define linguistic templates to recognize phrases that express implicit sentiments, while previously we ) represent a few simple rules as (in)equality constraints in Integer Linear Programming.", "labels": [], "entities": []}, {"text": "In contrast to previous 2 Note that the inferences are conversational implicatures; they are defeasible and may not go through in context ).", "labels": [], "entities": []}, {"text": "work, we propose a more general set of inference rules and encode them in a probabilistic soft logic (PSL) framework (.", "labels": [], "entities": []}, {"text": "We chose PSL because it is designed to have efficient inference and, as similar methods in Statistical Relational Learning do, it allows probabilistic models to be specified in first-order logic, an expressive and natural way to represent if-then rules, and it supports joint prediction.", "labels": [], "entities": [{"text": "Statistical Relational Learning", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.8359716931978861}, {"text": "joint prediction", "start_pos": 270, "end_pos": 286, "type": "TASK", "confidence": 0.6825337409973145}]}, {"text": "Joint prediction is critical for our task because it involves multiple, mutually constraining ambiguities (the source, polarity, and target).", "labels": [], "entities": [{"text": "Joint prediction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8365624248981476}]}, {"text": "Thus, this work aims at detecting both implicit and explicit sentiments expressed by an entity toward another entity/event (i.e., an eTarget) within the sentence.", "labels": [], "entities": []}, {"text": "The contributions of this work are: (1) defining a method for entity/event-level sentiment analysis to provide a deeper understanding of the text; (2) exploiting first-order logic rules to infer such sentiments, where the source is not limited to the writer, and the target maybe any entity, event, or even another sentiment; and (3) developing a PSL model to jointly resolve explicit and implicit sentiment ambiguities by integrating inference rules.", "labels": [], "entities": [{"text": "entity/event-level sentiment analysis", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.7476005315780639}]}], "datasetContent": [{"text": "We carryout experiments on the MPQA 3.0 corpus.", "labels": [], "entities": [{"text": "MPQA 3.0 corpus", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.9408898750940958}]}, {"text": "Currently, there are 70 documents, 1,634 sentences, and 1,921 DS and ESEs in total.", "labels": [], "entities": [{"text": "DS and ESEs", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.5449606279532114}]}, {"text": "The total number of POSPAIR(s,t) and NEGPAIR(s,t) are 867 and 1,975, respectively.", "labels": [], "entities": [{"text": "NEGPAIR", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.7712830901145935}]}, {"text": "Though the PSL inference does not need supervision and the SVM classifier for agents and themes in Section 4.4 is trained on a separate corpus, we still have to train the eTarget SVM classifier to assign local scores as described in Section 4.2.", "labels": [], "entities": []}, {"text": "Thus, the experiments are carried out using 5-fold cross validation.", "labels": [], "entities": []}, {"text": "For each fold test set, the eTarget classifier is trained on the other folds.", "labels": [], "entities": []}, {"text": "The trained classifier is then run on the test set, and PSL inference is carried out on the test set.", "labels": [], "entities": [{"text": "PSL inference", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9003525674343109}]}, {"text": "In total, we have three methods for eTarget candidate selection (ET1, ET2, ET3) and three models for sentiment analysis (PSL1, PSL2, PSL3).", "labels": [], "entities": [{"text": "eTarget candidate selection", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.5608130196730295}, {"text": "sentiment analysis", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.927361786365509}]}, {"text": "Since each noun and verb maybe an eTarget, the first baseline (All NP/VP) regards all the nouns and verbs as eTargets.", "labels": [], "entities": []}, {"text": "The first baseline estimates the difficulty of this task.", "labels": [], "entities": []}, {"text": "The second baseline (SVM) uses the SVM local classification results from Section 4.2.", "labels": [], "entities": [{"text": "SVM local classification", "start_pos": 35, "end_pos": 59, "type": "DATASET", "confidence": 0.8475229938824972}]}, {"text": "The score of ETARGET(y,t) is assigned by the SVM classifier.", "labels": [], "entities": [{"text": "ETARGET", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.994738757610321}]}, {"text": "Then it is normalized as input into PSL.", "labels": [], "entities": []}, {"text": "Before normalization, if the score assigned by the SVM classifier is above 0, the SVM baseline considers it as a correct eTarget.", "labels": [], "entities": []}, {"text": "First, we examine the performance of the PSL models on correctly recognizing eTargets of a particular opinion.", "labels": [], "entities": []}, {"text": "This evaluation is carried out on a subset of the corpus: we only examine the opinions which are automatically extracted by the span-based systems (S1, S2 and S3).", "labels": [], "entities": []}, {"text": "If an opinion expression in the gold standard is not extracted by any span-based system, it is not input into PSL, so PSL cannot possibly find its eTargets.", "labels": [], "entities": [{"text": "PSL", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.7961134314537048}]}, {"text": "The second and third evaluations assess performance of the PSL models on correctly extracting positive and negative pairs.", "labels": [], "entities": []}, {"text": "Note that our sentiment analysis system has the capability, through inference, to recognize positive and negative pairs even if corresponding opinion expressions are not extracted.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.922272652387619}]}, {"text": "Thus, the second and third evaluations are carried out on the entire corpus.", "labels": [], "entities": []}, {"text": "The second evaluation uses ET3, and compares PSL1, PSL2 and PSL3.", "labels": [], "entities": []}, {"text": "The third evaluation uses PSL3 and compares performance using ET1, ET2 and ET3.", "labels": [], "entities": []}, {"text": "The results for the other combinations follow the same trends.", "labels": [], "entities": []}, {"text": "According to the gold standard in Section 3.1, each opinion has a set of eTargets.", "labels": [], "entities": []}, {"text": "But not all eTargets are equally important.", "labels": [], "entities": []}, {"text": "Thus, our first evaluation assesses the performance of extracting the most important eTarget.", "labels": [], "entities": []}, {"text": "As introduced in Section 3.1, a span-based target annotation of an opinion in MPQA 2.0 captures the most important target this opinion is expressed toward.", "labels": [], "entities": [{"text": "MPQA 2.0", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9091719686985016}]}, {"text": "Thus, the head of the target span can be considered to be the most important eTarget of an opinion.", "labels": [], "entities": []}, {"text": "We model this as a ranking problem to compare models.", "labels": [], "entities": []}, {"text": "For an opinion y automatically extracted by a span-based system, both the SVM baseline and PSL assign scores to ETARGET(y,t).", "labels": [], "entities": [{"text": "SVM baseline", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.8576281666755676}, {"text": "ETARGET", "start_pos": 112, "end_pos": 119, "type": "METRIC", "confidence": 0.9989026784896851}]}, {"text": "We rank the eTargets according to the scores.", "labels": [], "entities": []}, {"text": "Because the ALL NP/VP baseline does not assign scores to the nouns and verbs, we do not compare with that baseline in this ranking experiment.", "labels": [], "entities": []}, {"text": "We use the Precision@N evaluation metric.", "labels": [], "entities": []}, {"text": "If the top N eTargets of an opinion contain the head of target span, we consider it as a correct hit.", "labels": [], "entities": [{"text": "head of target span", "start_pos": 48, "end_pos": 67, "type": "METRIC", "confidence": 0.8524429500102997}]}, {"text": "The results are in shows that SVM is poor at ranking the most important eTarget.", "labels": [], "entities": [{"text": "SVM", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.5536095499992371}, {"text": "eTarget", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9659202694892883}]}, {"text": "The PSL models are much better, even PSL1, which does not include any inference rules.", "labels": [], "entities": []}, {"text": "This shows that SVM, which only uses local features, cannot distinguish the most important eTarget from the others.", "labels": [], "entities": []}, {"text": "But the PSL models consider all the opinions, and can recognize a true negative even if it ranks high in the local results.", "labels": [], "entities": []}, {"text": "The ability of PSL to rule out true negative candidates will be repeatedly shown in the later evaluations.", "labels": [], "entities": [{"text": "PSL", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9510197043418884}]}, {"text": "We not only evaluate the ability to recognize the most important eTarget of a particular opinion, we also evaluate the ability to extract all the eTargets of that opinion.", "labels": [], "entities": []}, {"text": "The F-measure of SVM is 0.2043, while the F-measures of PSL1, PSL2 and PSL3 are 0.3135, 0.3239, and 0.3275, respectively.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9953553676605225}, {"text": "F-measures", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9821600317955017}]}, {"text": "Correctly recognizing all the eTargets is difficult, but all the PSL models are better than the baseline.", "labels": [], "entities": []}, {"text": "Positive Pairs and Negative Pairs.", "labels": [], "entities": []}, {"text": "Now we evaluate the performance in a stricter way.", "labels": [], "entities": []}, {"text": "We compare automatically extracted sets of sentiment pairs: P auto = {POSPAIR(s, t) > 0} and N auto = {NEGPAIR(s, t) > 0}, against the gold standard sets P gold and N gold . shows the accuracies using ET3.", "labels": [], "entities": []}, {"text": "Note that higher accuracies can be achieved, as shown later.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9966208934783936}]}, {"text": "Here we use ET3 just to show the trend of results.", "labels": [], "entities": [{"text": "ET3", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.8485099077224731}]}, {"text": "As shown in, the low accuracy of baseline All NP/VP shows that entity/event-level sentiment analysis is a difficult task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.99937504529953}, {"text": "entity/event-level sentiment analysis", "start_pos": 63, "end_pos": 100, "type": "TASK", "confidence": 0.6592130064964294}]}, {"text": "Even the SVM baseline does not have good accuracy.", "labels": [], "entities": [{"text": "SVM baseline", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.74861079454422}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.998155415058136}]}, {"text": "Note that the SVM baseline in   ion spans, which are extracted by state-of-theart span-based sentiment analysis systems.", "labels": [], "entities": []}, {"text": "This shows the results from span-based sentiment analysis systems do not provide enough accurate information for the more fine-grained entity/eventlevel sentiment analysis task.", "labels": [], "entities": [{"text": "span-based sentiment analysis", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.6350102126598358}, {"text": "eventlevel sentiment analysis", "start_pos": 142, "end_pos": 171, "type": "TASK", "confidence": 0.7141435841719309}]}, {"text": "In contrast, PSL1 achieves much higher accuracy than the baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9991894364356995}]}, {"text": "PSL2 and PSL3, which add sentiment toward sentiment and +/-effect event inferences, give further improvements.", "labels": [], "entities": [{"text": "PSL2", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9059399366378784}]}, {"text": "A reason is that SVM uses a hard constraint to cutoff many eTarget candidates, while the PSL models take the scores as soft constraints.", "labels": [], "entities": []}, {"text": "A more critical reason is due to the definition of accuracy: (TruePositive+TrueNegative)/All.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9994418025016785}]}, {"text": "A significant benefit of using PSL is correctly recognizing true negative eTarget candidates and eliminating them from the set.", "labels": [], "entities": []}, {"text": "Interestingly, even though both PSL2 and PSL3 introduce more eTarget candidates, both are able to recognize more true negatives and improve the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9971774816513062}]}, {"text": "Note that F-measure does not count true negatives.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9473130702972412}]}, {"text": "Precision is T PT P +F P , and recall is T PT P +F N ; neither considers true negatives (TN).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9897051453590393}, {"text": "T PT P +F P", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.803284744421641}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9995654225349426}, {"text": "T PT P +F N", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.8947431047757467}]}, {"text": "As shown in, the increment of PSL model over baselines on F-measure is not as large as the increase inaccuracy.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.824199378490448}]}, {"text": "Comparing PSL2 and PSL3 to PSL1, the inference rules largely increase recall but lower precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9996135830879211}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.997879147529602}]}, {"text": "However, the accuracy in keeps growing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9996023774147034}]}, {"text": "Thus, the biggest advantage of PSL models is to correctly rule out true negative eTargets.", "labels": [], "entities": []}, {"text": "For the baselines, though the SVM baseline has higher precision, it eliminates so many eTarget candidates that the F-measure is not high.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9988652467727661}, {"text": "F-measure", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9973669648170471}]}, {"text": "To assess the methods for eTarget selection, we run PSL3 (the fullest PSL model) using each method in turn.", "labels": [], "entities": []}, {"text": "The Fmeasures and accuracies are listed in", "labels": [], "entities": [{"text": "accuracies", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9810256361961365}]}], "tableCaptions": [{"text": " Table 2: Precision@N of Most Important ETarget.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.961211621761322}]}, {"text": " Table 3: Accuracy comparing PSL models (ET3  used for all)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987258315086365}]}, {"text": " Table 4: F-measure comparing PSL models (ET3  used for all)", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9894700050354004}]}, {"text": " Table 5: Comparison of eTarget selection methods  (PSL3 used for all)", "labels": [], "entities": []}]}