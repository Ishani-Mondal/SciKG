{"title": [{"text": "ERSOM: A Structural Ontology Matching Approach Using Automatically Learned Entity Representation 1 Introductions", "labels": [], "entities": [{"text": "Structural Ontology Matching Approach", "start_pos": 9, "end_pos": 46, "type": "TASK", "confidence": 0.9005470871925354}]}], "abstractContent": [{"text": "As a key representation model of knowledge , ontology has been widely used in a lot of NLP related tasks, such as semantic parsing, information extraction and text mining etc.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.7164405137300491}, {"text": "information extraction", "start_pos": 132, "end_pos": 154, "type": "TASK", "confidence": 0.8093383014202118}, {"text": "text mining", "start_pos": 159, "end_pos": 170, "type": "TASK", "confidence": 0.7851555943489075}]}, {"text": "In this paper, we study the task of ontology matching, which concentrates on finding semantically related entities between different ontologies that describe the same domain, to solve the semantic heterogeneity problem.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.8589845597743988}]}, {"text": "Previous works exploit different kinds of descriptions of an entity in ontology directly and separately to find the correspondences without considering the higher level correlations between the descriptions.", "labels": [], "entities": []}, {"text": "Besides, the structural information of ontology haven't been utilized adequately for ontology matching.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.8850183486938477}]}, {"text": "We propose in this paper an ontology matching approach, named ERSOM, which mainly includes an unsupervised representation learning method based on the deep neural networks to learn the general representation of the entities and an iterative similarity propagation method that takes advantage of more abundant structure information of the ontology to discover more mappings.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8158818483352661}, {"text": "similarity propagation", "start_pos": 241, "end_pos": 263, "type": "TASK", "confidence": 0.7341138124465942}]}, {"text": "The experimental results on the datasets from Ontology Alignment Evaluation Initiative (OAEI 1) show that ER-SOM achieves a competitive performance compared to the state-of-the-art ontology matching systems.", "labels": [], "entities": []}, {"text": "1 The OAEI is an international initiative organizing annual campaigns for evaluating ontology matching systems.", "labels": [], "entities": []}, {"text": "All of the ontologies provided by OAEI are described in OWL-DL language, and like most of the other participates our ERSOM also manages the OWL ontology in its current version.", "labels": [], "entities": [{"text": "OAEI", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.8924651145935059}, {"text": "OWL-DL language", "start_pos": 56, "end_pos": 71, "type": "DATASET", "confidence": 0.8885374963283539}]}], "introductionContent": [], "datasetContent": [{"text": "The annual OAEI campaign is an authoritative contest in the area of ontology matching, we choose the data from OAEI in our experiments, because the evaluation metrics have been well defined and the comparision can be easily made.", "labels": [], "entities": [{"text": "ontology matching", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.8562347292900085}]}, {"text": "We observe strong structure similarities lies between OAEI ontologies and ontologies used in NLP tasks, such as WordNet and HowNet for WS-D (, and Freebase, YAGO, and knowledge graph for IE, text mining and QA (Yao and Van Durme, 2014; ), both describe entities and their relations with class, properties and instances.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9474890828132629}, {"text": "HowNet", "start_pos": 124, "end_pos": 130, "type": "DATASET", "confidence": 0.8797711133956909}, {"text": "text mining", "start_pos": 191, "end_pos": 202, "type": "TASK", "confidence": 0.7532227039337158}]}, {"text": "Development dataset: the Standard Benchmark 2012 dataset that OAEI provides for developers to test their system before participating in the competition is used as the development dataset in our experiments.", "labels": [], "entities": [{"text": "Standard Benchmark 2012 dataset", "start_pos": 25, "end_pos": 56, "type": "DATASET", "confidence": 0.9324545115232468}, {"text": "OAEI", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.818736732006073}]}, {"text": "This dataset contains one reference ontology and 109 target ontologies.", "labels": [], "entities": []}, {"text": "We use this dataset to test various values for the parameters in our ERSOM and apply the best ones to the experiments on the testing datasets.", "labels": [], "entities": []}, {"text": "Testing dataset: (1) the Benchmark-Biblio 2012 dataset which contains one reference ontology and 94 target ontologies; (2) the BenchmarkBiblioc 2013 dataset which has five sub-datasets and there are one reference ontology and 93 target ontologies in each sub-dataset.", "labels": [], "entities": [{"text": "Benchmark-Biblio 2012 dataset", "start_pos": 25, "end_pos": 54, "type": "DATASET", "confidence": 0.981346329053243}, {"text": "BenchmarkBiblioc 2013 dataset", "start_pos": 127, "end_pos": 156, "type": "DATASET", "confidence": 0.9611249963442484}]}, {"text": "We use these two datasets to evaluate the performance of our ER-SOM approach.", "labels": [], "entities": []}, {"text": "In the matching scenario, each target ontology should be mapped to the reference ontology.", "labels": [], "entities": []}, {"text": "We followed the standard evaluation criteria from the OAEI, calculating the precision, recall and fmeasure over each test.", "labels": [], "entities": [{"text": "OAEI", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.9010241627693176}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9997029900550842}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9993327260017395}, {"text": "fmeasure", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9744158983230591}]}, {"text": "The version computed here is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9996005892753601}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9975678324699402}]}, {"text": "We first use Jena 3 parsing the ontologies and extract descriptions for entities according to the description in section 2.1.1, then we create a vocabulary based on the dataset and denote each class and property as a binary term vector.", "labels": [], "entities": [{"text": "Jena 3 parsing", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7508437633514404}]}, {"text": "We apply the L-BFGS algorithm) to train the stacked auto-encoder described in section 2.1.3.", "labels": [], "entities": []}, {"text": "The size of the input layer is equals to the length of the vocabulary created from the dataset.", "labels": [], "entities": []}, {"text": "We fix the parameters \u03bb = 1e \u2212 4, \u03b2 = 3 and \u03c1 = 0.25 in Eq.2, and set the size of the first and second hidden layer of the stacked auto-encoder to 200 and 100, respectively, by experience.", "labels": [], "entities": []}, {"text": "The number of iterations of the L-BFGS algorithm is set to 500.", "labels": [], "entities": []}, {"text": "We use the learned representations to measure the similarities between classes and properties and apply the strategy presented in section 2.3 to extract final mappings.", "labels": [], "entities": []}, {"text": "The matching results of our Unsupervised Representation Learning (URL) method on the development dataset and testing datasets are shown in, respectively.: Representation learning on dev. dataset.", "labels": [], "entities": []}, {"text": "In, TV denotes the matcher in which https://jena.apache.org/ the similarities are calculated between binary term vectors of classes and properties by using cosine measure.", "labels": [], "entities": []}, {"text": "URL(i), where i \u2208 {1, 2}, represents that we use the representations learned by the ith hidden layer of the stacked auto-encoder to measure the similarities between classes and properties to find mappings.", "labels": [], "entities": [{"text": "URL", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8903212547302246}]}, {"text": "shows that on the development dataset, the F-measure of TV is 0.748 and it is improved 9.6% and 12.8% when we use the representations learned by the singlelayer and double-layer auto-encoder to find the mappings, respectively.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9993355870246887}]}, {"text": "It illustrates that we have learned some useful information from the term vectors, which can be explained as the interactions between descriptions of entities.", "labels": [], "entities": []}, {"text": "From the last two rows in, we can find that the F-measure improved by 2.9% when we use the representations learned by the second hidden layer (i.e., URL  From we can see that the F-measures are increased on both of the testing datasets when we use the learned representations to measure similarities between classes and properties compared with using term vectors, but the amount of improvements are less than that on the development dataset.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9894977807998657}]}, {"text": "This is because we estimate the parameters of the representation learning model on the development dataset and then apply them on the test tasks directly.", "labels": [], "entities": []}, {"text": "The precision is reduced when we use URL method, this maybe due to the learned representations of entities are too general.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9993485808372498}]}, {"text": "In addition, in the parameter adjustment process, we try to make the F value maximization, but not to care about mapping precision.", "labels": [], "entities": [{"text": "F value maximization", "start_pos": 69, "end_pos": 89, "type": "METRIC", "confidence": 0.9488299489021301}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9821575880050659}]}, {"text": "This is because we usually compare the performance of the systems based on their matching F values.", "labels": [], "entities": []}, {"text": "In this experiment, we compare our Similarity Propagation (SP) method to other structure based methods, that is, ADJACENTS and ASCOPATH in (); DSI and SSC in); Li's SP () and Ngo's SP (.", "labels": [], "entities": [{"text": "ASCOPATH", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9565705060958862}]}, {"text": "We first use entity's ID to compute the similarity between classes and properties to provide an unified initial similarity matrix as input (or initialization) for our SP and other structural methods.", "labels": [], "entities": []}, {"text": "Then, anew similarity matrix will be created and updated by considering the initial similarities and different structure information.", "labels": [], "entities": []}, {"text": "And finally, we extract the mappings from the newly created similarity matrix with the strategy described in section 2.3.", "labels": [], "entities": []}, {"text": "In ADJACENTS method, the parameter W k , where k \u2208 {1, 2, 3}, is set to 1/3.", "labels": [], "entities": []}, {"text": "The parameter MCP in the methods DSI and SSC is set to 0.75 as reported in their work.", "labels": [], "entities": [{"text": "MCP", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.8206754326820374}, {"text": "DSI", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.9455063343048096}]}, {"text": "The iterative times to SP algorithm are fixed to 50.", "labels": [], "entities": [{"text": "SP", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.7636874318122864}]}, {"text": "reports the matching F-measures of these structure based methods on the development dataset (Dev.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.908225417137146}]}, {"text": "for short) and testing datasets (Tes.1, Tes.2 for short).", "labels": [], "entities": []}, {"text": "From table 4 we can see that the local-structure based methods (i.e., ADJACENTS, ASCOPATH, DSI and SSC) provide low matching quality.", "labels": [], "entities": []}, {"text": "It means that these methods did not discover enough additional correct mappings or even find some incorrect mappings.", "labels": [], "entities": []}, {"text": "For example, the F-measure even reduced on the development dataset when use ASCOPATH method.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9983693957328796}]}, {"text": "This is because if two entities don't have any common entity in their ancestors, their similarity is equal to 0.", "labels": [], "entities": [{"text": "similarity", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.976815938949585}]}, {"text": "Whereas, Li's SP and Ngo's SP are global-structure based methods, and they seem to work well.", "labels": [], "entities": []}, {"text": "The F-measure has even improved by 21.9% when using the Ngo's SP compared with the initial matcher.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9990618824958801}, {"text": "Ngo's SP", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.6333331366380056}]}, {"text": "This is because in the SP method, the similarity score of a pair of entities depends on not only their current status but also on the status of the other pairs.", "labels": [], "entities": [{"text": "SP", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9579241871833801}, {"text": "similarity score", "start_pos": 38, "end_pos": 54, "type": "METRIC", "confidence": 0.965251088142395}]}, {"text": "That explains why SP outperforms all other local based structural methods.", "labels": [], "entities": [{"text": "SP", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9652671813964844}]}, {"text": "In our SP, all instances and their relations to other entities in ontology are exploited to help find the mappings between classes and properties, therefore the matching quality is distinctly improved.", "labels": [], "entities": []}, {"text": "The last two rows of shows that when we use the learned representations to create the initial similarity matrix to initialize our SP method, the matching quality is significantly improved.", "labels": [], "entities": []}, {"text": "For example, the F-measure is improved from 0.810 to 0.903 on the development dataset.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9991050362586975}, {"text": "development dataset", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.6725289076566696}]}, {"text": "This illustrates that the initialization step is very important to the SP method.", "labels": [], "entities": [{"text": "initialization", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.949194073677063}, {"text": "SP", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.968512237071991}]}], "tableCaptions": [{"text": " Table 1: Representation learning on dev. dataset.", "labels": [], "entities": [{"text": "Representation learning", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.9605214893817902}]}, {"text": " Table 2: Representation learning on test datasets.", "labels": [], "entities": [{"text": "Representation learning", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.9678944051265717}]}, {"text": " Table 3: Comparison with aggregation methods.", "labels": [], "entities": []}, {"text": " Table 4: Comparison with structural methods.", "labels": [], "entities": []}]}