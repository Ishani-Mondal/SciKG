{"title": [{"text": "Multi-and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception", "labels": [], "entities": [{"text": "Auditory Perception", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7089651226997375}]}], "abstractContent": [{"text": "Multi-modal semantics has relied on feature norms or raw image data for perceptual input.", "labels": [], "entities": []}, {"text": "In this paper we examine grounding semantic representations in raw auditory data, using standard evaluations for multi-modal semantics, including measuring conceptual similarity and related-ness.", "labels": [], "entities": []}, {"text": "We also evaluate cross-modal map-pings, through a zero-shot learning task mapping between linguistic and auditory modalities.", "labels": [], "entities": []}, {"text": "In addition, we evaluate multi-modal representations on an unsupervised musical instrument clustering task.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first work to combine linguistic and auditory information into multi-modal representations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Although distributional models have proved useful fora variety of NLP tasks, the fact that the meaning of a word is represented as a distribution over other words implies that they suffer from the grounding problem; i.e. they do not account for the fact that human semantic knowledge is grounded in the perceptual system.", "labels": [], "entities": []}, {"text": "Motivated by human concept acquisition, multi-modal semantics enhances linguistic representations with extra-linguistic perceptual input.", "labels": [], "entities": [{"text": "concept acquisition", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7349116206169128}]}, {"text": "These models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality).", "labels": [], "entities": [{"text": "predicting compositionality", "start_pos": 127, "end_pos": 154, "type": "TASK", "confidence": 0.94645956158638}]}, {"text": "Although feature norms have also been used, raw image data has become the de-facto perceptual modality in multi-modal models.", "labels": [], "entities": []}, {"text": "However, if the objective is to ground semantic representations in perceptual information, why stop at image data?", "labels": [], "entities": []}, {"text": "The meaning of violin is surely not only grounded in its visual properties, such as shape, color and texture, but also in its sound, pitch and timbre.", "labels": [], "entities": []}, {"text": "To understand how perceptual input leads to conceptual representation, we should use as many perceptual modalities as possible.", "labels": [], "entities": []}, {"text": "A recent preliminary study by found that it is possible to derive uni-modal semantic representations from sound data.", "labels": [], "entities": []}, {"text": "Here, we explore taking multi-modal semantics beyond its current reliance on image data and experiment with grounding semantic representations in the auditory perceptual modality.", "labels": [], "entities": []}, {"text": "Multi-modal models that rely on raw image data have typically used \"bag of visual words\" (BoVW) representations.", "labels": [], "entities": []}, {"text": "We follow a similar approach for the auditory modality and construct bag of audio words (BoAW) representations.", "labels": [], "entities": []}, {"text": "Following previous work in multi-modal semantics, we evaluate these models on measuring conceptual similarity and relatedness, and inducing cross-modal mappings between modalities to perform zeroshot learning.", "labels": [], "entities": []}, {"text": "In addition, we evaluate on an unsupervised musical instrument clustering task.", "labels": [], "entities": [{"text": "unsupervised musical instrument clustering task", "start_pos": 31, "end_pos": 78, "type": "TASK", "confidence": 0.7790146470069885}]}, {"text": "Our findings indicate that multi-modal representations enriched with auditory information perform well on relatedness and similarity tasks, particularly on words that have auditory assocations.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first work to combine linguistic and auditory representations in multimodal semantics.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following previous work in multi-modal semantics, we evaluate on two standard similarity and relatedness datasets: SimLex-999 ( ) and the MEN test collection ( ).", "labels": [], "entities": [{"text": "SimLex-999", "start_pos": 115, "end_pos": 125, "type": "DATASET", "confidence": 0.8724226355552673}, {"text": "MEN test collection", "start_pos": 138, "end_pos": 157, "type": "DATASET", "confidence": 0.8491166234016418}]}, {"text": "These datasets consist of concept pairs together with a human-annotated similarity or relatedness score, where the former dataset focuses on genuine similarity (e.g., teacher-instructor) and the latter focuses more on relatedness (e.g., riverwater).", "labels": [], "entities": []}, {"text": "In addition, following previous work in cross-modal semantics, we evaluate on the zeroshot learning task of inducing a cross-modal mapping to the correct label in the auditory modality from the linguistic one and vice-versa.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of pairs in the datasets where  auditory is relevant, with the similarity score.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 82, "end_pos": 98, "type": "METRIC", "confidence": 0.9781528115272522}]}, {"text": " Table 2: Number of concept pairs for which repre- sentations are available in each modality.", "labels": [], "entities": []}, {"text": " Table 3: Spearman \u03c1 s correlation comparison of  uni-modal and multi-modal representations. The  MMSG models perform early fusion, MM repre- sents middle and late fusion with \u03b1 = 0.5 (see  Section 4.3.2).", "labels": [], "entities": []}, {"text": " Table 4: Cross-modal zero-shot learning accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9933950304985046}]}, {"text": " Table 6: V-measure performance for clustering  musical instruments, together with instruments  closest to cluster centroid for linguistic and multi- modal.", "labels": [], "entities": []}]}