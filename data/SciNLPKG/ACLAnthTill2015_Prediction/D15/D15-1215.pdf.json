{"title": [{"text": "Transition-based Dependency Parsing Using Two Heterogeneous Gated Recursive Neural Networks", "labels": [], "entities": [{"text": "Transition-based Dependency Parsing", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5691791077454885}]}], "abstractContent": [{"text": "Recently, neural network based dependency parsing has attracted much interest, which can effectively alleviate the problems of data sparsity and feature engineering by using the dense features.", "labels": [], "entities": [{"text": "neural network based dependency parsing", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.6862420678138733}, {"text": "feature engineering", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7266525626182556}]}, {"text": "However , it is still a challenge problem to sufficiently model the complicated syntactic and semantic compositions of the dense features in neural network based methods.", "labels": [], "entities": []}, {"text": "In this paper, we propose two heterogeneous gated recursive neu-ral networks: tree structured gated re-cursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN).", "labels": [], "entities": []}, {"text": "Then we integrate them to automatically learn the compositions of the dense features for transition-based dependency parsing.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 89, "end_pos": 124, "type": "TASK", "confidence": 0.6075364450613657}]}, {"text": "Specifically, Tree-GRNN models the feature combinations for the trees in stack, which already have partial dependency structures.", "labels": [], "entities": []}, {"text": "DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet.", "labels": [], "entities": []}, {"text": "Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model.", "labels": [], "entities": [{"text": "PTB3", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.8925118446350098}, {"text": "CTB5", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.7703590393066406}]}], "introductionContent": [{"text": "Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community.", "labels": [], "entities": [{"text": "Transition-based dependency parsing", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.733823279539744}, {"text": "natural language processing", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.6452884475390116}]}, {"text": "The traditional discriminative dependency parsing methods have achieved great success (; Goldberg and Nivre, is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAG-GRNN for the nodes without given topological structure. 2013;).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.6810263246297836}]}, {"text": "However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.6936873942613602}]}, {"text": "Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (.", "labels": [], "entities": [{"text": "transition-based parsing", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.5651529878377914}]}, {"text": "However, most of the existing neural network based methods still need some efforts in feature engineering.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.8110452890396118}]}, {"text": "For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss some valuable information hidden in the unchosen nodes.", "labels": [], "entities": []}, {"text": "Besides, the features of the selected nodes are just simply concatenated and then fed into neural network.", "labels": [], "entities": []}, {"text": "Since the concatenation operation is relatively simple, it is difficult to model the com-plicated feature combinations which can be manually designed in the traditional discrete feature based methods.", "labels": [], "entities": []}, {"text": "To tackle these problems, we use two heterogeneous gated recursive neural networks, tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph gated structured recursive neural network (DAG-GRNN), to model each configuration during transition based dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 276, "end_pos": 294, "type": "TASK", "confidence": 0.6736795455217361}]}, {"text": "The two proposed GRNNs introduce the gate mechanism ( to improve the standard recursive neural network (RNN)), and can model the syntactic and semantic compositions of the nodes during parsing.", "labels": [], "entities": []}, {"text": "gives a rough sketch for the standard RNN, Tree-GRNN and DAG-GRNN.", "labels": [], "entities": [{"text": "RNN", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.8504414558410645}]}, {"text": "Tree-GRNN is applied to the partial-constructed trees in stack, which have already been constructed according to the previous transition actions.", "labels": [], "entities": []}, {"text": "DAG-GRNN is applied to model the feature composition of nodes in stack and buffer which have not been labeled their dependency relations yet.", "labels": [], "entities": []}, {"text": "Intuitively, Tree-GRNN selects and merges features recursively from children nodes into their parent according to their dependency structures, while DAG-GRNN further models the complicated combinations of extracted features and explicitly exploits features in different levels of granularity.", "labels": [], "entities": []}, {"text": "To evaluate our approach, we experiment on two prevalent benchmark datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets.", "labels": [], "entities": [{"text": "English Penn Treebank 3 (PTB3)", "start_pos": 77, "end_pos": 107, "type": "DATASET", "confidence": 0.8911546298435756}, {"text": "Chinese Penn Treebank 5 (CTB5) datasets", "start_pos": 112, "end_pos": 151, "type": "DATASET", "confidence": 0.8746285140514374}]}, {"text": "Experiment results show the effectiveness of our proposed method.", "labels": [], "entities": []}, {"text": "Compared to the parser of, we receive 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, while we receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set.", "labels": [], "entities": [{"text": "UAS", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9713004231452942}, {"text": "LAS", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9679030776023865}, {"text": "PTB3 test set", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9731484254201254}, {"text": "UAS", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9233654737472534}, {"text": "CTB5 test set", "start_pos": 152, "end_pos": 165, "type": "DATASET", "confidence": 0.9792704780896505}]}], "datasetContent": [{"text": "To evaluate our proposed model, we experiment on two prevalent datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets.", "labels": [], "entities": [{"text": "English Penn Treebank 3 (PTB3)", "start_pos": 73, "end_pos": 103, "type": "DATASET", "confidence": 0.9003047687666756}, {"text": "Chinese Penn Treebank 5 (CTB5) datasets", "start_pos": 108, "end_pos": 147, "type": "DATASET", "confidence": 0.8809749111533165}]}, {"text": "\u2022 English For English Penn Treebank 3 (PTB3) dataset, we use sections 2-21 for training, section 22 and section 23 as development set and test set respectively.", "labels": [], "entities": [{"text": "English Penn Treebank 3 (PTB3) dataset", "start_pos": 14, "end_pos": 52, "type": "DATASET", "confidence": 0.8751243576407433}]}, {"text": "We adopt CoNLL Syntactic Dependencies (CD)) using the LTH Constituent-to-Dependency Conversion Tool.", "labels": [], "entities": []}, {"text": "\u2022 Chinese For Chinese Penn Treebank 5 (CTB5) dataset, we follow the same split as described in.", "labels": [], "entities": [{"text": "Chinese Penn Treebank 5 (CTB5) dataset", "start_pos": 14, "end_pos": 52, "type": "DATASET", "confidence": 0.8875651732087135}]}, {"text": "Dependencies are converted by the Penn2Malt tool with the head-finding rules of).", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.9675233960151672}]}, {"text": "Embedding size de = 50 Dimensionality of child node vector dc = 50 Initial learning rate \u03b1 = 0.05 Regularization \u03bb = 10 \u22128 Dropout rate p = 20%: Hyper-parameter settings.", "labels": [], "entities": [{"text": "Initial learning rate \u03b1 = 0.05 Regularization \u03bb", "start_pos": 67, "end_pos": 114, "type": "METRIC", "confidence": 0.7710547298192978}]}, {"text": "For parameter initialization, we use random initialization within (-0.01, 0.01) for all parameters except the word embedding matrix E w . Specifically, we adopt pre-trained English word embeddings from).", "labels": [], "entities": [{"text": "parameter initialization", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6665823459625244}]}, {"text": "And we pretrain the Chinese word embeddings on a huge unlabeled data, the Chinese Wikipedia corpus, with word2vec toolkit (Mikolov et al., 2013a).", "labels": [], "entities": [{"text": "Chinese Wikipedia corpus", "start_pos": 74, "end_pos": 98, "type": "DATASET", "confidence": 0.7760756810506185}]}, {"text": "gives the details of hyper-parameter settings of our approach.", "labels": [], "entities": []}, {"text": "In addition, we set minibatch size to 20.", "labels": [], "entities": [{"text": "size", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.6302922368049622}]}, {"text": "In all experiments, we only take s 1 , s 2 , s 3 nodes in stack and b 1 , b 2 , b 3 nodes in buffer into account.", "labels": [], "entities": []}, {"text": "We also apply dropout strategy here, and only dropout at the nodes in stack and buffer with probability p = 20%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.", "labels": [], "entities": [{"text": "PTB3  dataset", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9728433787822723}, {"text": "UAS", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9317154884338379}, {"text": "unlabeled attachment score", "start_pos": 65, "end_pos": 91, "type": "METRIC", "confidence": 0.6335177620251974}, {"text": "LAS", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9721364378929138}, {"text": "labeled attachment score", "start_pos": 99, "end_pos": 123, "type": "METRIC", "confidence": 0.691597064336141}]}, {"text": " Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.", "labels": [], "entities": [{"text": "CTB5 dataset", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.974944680929184}, {"text": "UAS", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.8776960372924805}, {"text": "unlabeled attachment score", "start_pos": 65, "end_pos": 91, "type": "METRIC", "confidence": 0.635191281636556}, {"text": "LAS", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9663405418395996}]}]}