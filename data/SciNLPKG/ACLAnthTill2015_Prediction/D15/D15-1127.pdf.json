{"title": [], "abstractContent": [{"text": "This work focuses on the task of finding latent vector representations of the words in a corpus.", "labels": [], "entities": []}, {"text": "In particular, we address the issue of what to do when there are multiple languages in the corpus.", "labels": [], "entities": []}, {"text": "Prior work has, among other techniques, used canonical correlation analysis to project pre-trained vectors in two languages into a common space.", "labels": [], "entities": [{"text": "canonical correlation analysis", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.6980409026145935}]}, {"text": "We propose a simple and scal-able method that is inspired by the notion that the learned vector representations should be invariant to translation between languages.", "labels": [], "entities": []}, {"text": "We show empirically that our method outperforms prior work on multilingual tasks, matches the performance of prior work on monolingual tasks, and scales linearly with the size of the input data (and thus the number of languages being embedded).", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing words as vectors in some latent space has long been a central idea in natural language processing.", "labels": [], "entities": [{"text": "Representing words as vectors in some latent space", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8007042407989502}, {"text": "natural language processing", "start_pos": 83, "end_pos": 110, "type": "TASK", "confidence": 0.6389090518156687}]}, {"text": "The distributional hypothesis, perhaps best stated as \"You shall know a word by the company it keeps\", has had along and productive history, as well as a recent revival in neural-network-based models ().", "labels": [], "entities": []}, {"text": "These methods generally construct a word by context matrix, then either use the vectors directly (often weighted by term frequency and inverse document frequency), perform some factor- * These authors contributed equally.", "labels": [], "entities": []}, {"text": "ization of the matrix, or use it as input to a neural network which produces vectors for each word.", "labels": [], "entities": []}, {"text": "The resultant vectors can be used in a wide array of tasks, from information retrieval to part-of-speech tagging and parsing.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.8345401585102081}, {"text": "part-of-speech tagging", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.7212457358837128}]}, {"text": "There has also been some recent work addressing how to create these vectors when information from multiple languages is available.", "labels": [], "entities": []}, {"text": "Two recent attempts involve using canonical correlation analysis (CCA) to project pre-trained vectors from each of two languages into a common space and using an alignment matrix to heuristically project the vectors from one language onto the words in another language ().", "labels": [], "entities": [{"text": "canonical correlation analysis (CCA)", "start_pos": 34, "end_pos": 70, "type": "TASK", "confidence": 0.7876474459966024}]}, {"text": "These methods generally only work with two languages at a time, however.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a technique for constructing multilingual word embeddings that is inspired by the notion of translational invariance.", "labels": [], "entities": []}, {"text": "CCA and the heuristic projection mentioned above both attempt to construct vectors such that words that are translations of each other are close in the vector space, but the method we introduce formalizes this as part of the objective function of the original decomposition.", "labels": [], "entities": []}, {"text": "We further show how to optimize this objective function with a method that scales linearly in the size of the input data.", "labels": [], "entities": []}, {"text": "This results in a scalable, single-step method that is informed by both the monolingual corpus statistics and the multilingual alignment data.", "labels": [], "entities": []}, {"text": "We show experimentally that this results in vectors that outperform prior work on multilingual tasks and match the performance of prior work on monolingual tasks.", "labels": [], "entities": []}, {"text": "The contributions of this paper are the following: \u2022 Problem formulation: we formalize the notion of translation-invariance, regardless of the number of languages, as part of the objective function of a standard matrix decomposition; \u2022 Scalable algorithm: we introduce scalable means of optimizing this augmented objective functions; and \u2022 Effectiveness: we present state-of-the-art results on a multilingual task using the vectors obtained by these methods.", "labels": [], "entities": [{"text": "Problem formulation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.8026670813560486}]}, {"text": "The code and data used in this paper are publicly available at https://sites.google.", "labels": [], "entities": []}, {"text": "com/a/umn.edu/huang663/.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present three experiments to evaluate the method introduced in this paper.", "labels": [], "entities": []}, {"text": "The first experiment uses our word embeddings in a cross-lingual dependency parsing task; the second experiment looks at monolingual (English) performance on a series of word-similarity tasks; and the final experiment shows the scalability of our method by applying it to multiple languages.", "labels": [], "entities": [{"text": "cross-lingual dependency parsing task", "start_pos": 51, "end_pos": 88, "type": "TASK", "confidence": 0.736602708697319}]}, {"text": "recently introduced a method for using multilingual word embeddings to perform cross-lingual dependency parsing.", "labels": [], "entities": [{"text": "cross-lingual dependency parsing", "start_pos": 79, "end_pos": 111, "type": "TASK", "confidence": 0.7041433850924174}]}, {"text": "They train a neural-network-based dependency parsing model using word vectors from one language, and then test the model using data and word vectors from another language.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7419274151325226}]}, {"text": "They used the embeddings obtained by: Labeled and unlabeled attachment score (LAS/UAS) on a cross-lingual dependency task.", "labels": [], "entities": [{"text": "unlabeled attachment score (LAS/UAS)", "start_pos": 50, "end_pos": 86, "type": "METRIC", "confidence": 0.8215735405683517}]}, {"text": "TI-LSA outperforms prior work on this task.", "labels": [], "entities": [{"text": "TI-LSA", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7254571914672852}]}, {"text": "data to obtain our embeddings, our method is directly comparable to the CCA method of Faruqui and Dyer, and the projection method of Guo et al.", "labels": [], "entities": []}, {"text": "We used code and data graciously provided by Guo to run experiments, training a dependency parsing model on their English treebank, and testing it on the Spanish treebank.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.6876167505979538}, {"text": "English treebank", "start_pos": 114, "end_pos": 130, "type": "DATASET", "confidence": 0.9430074393749237}, {"text": "Spanish treebank", "start_pos": 154, "end_pos": 170, "type": "DATASET", "confidence": 0.9147652387619019}]}, {"text": "We report the results below for the methods used by Guo et al. and the method introduced in this paper.", "labels": [], "entities": []}, {"text": "We could not exactly reproduce Guo's result with the code we were provided, so we report all results from our use of the provided code, in case some parameter settings are different from those used in Guo's paper.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "As can be seen in the table, our first method for obtaining multilingual embeddings outperforms both the CCA method of Faruqui and Dyer, and the heuristic projection used by Guo et al.", "labels": [], "entities": []}, {"text": "While our focus is on generating embeddings that are invariant to translations (and thus most suited to multi-or cross-lingual tasks), we would hope that the addition of multiple languages would not hurt performance on monolingual tasks.", "labels": [], "entities": []}, {"text": "We used wordvectors.org to evaluate our learned vectors on a variety of English-language word similarity tasks.", "labels": [], "entities": []}, {"text": "The tasks are mostly all variations on performing word similarity judgments, finding the correlation between the system's output and human responses.", "labels": [], "entities": [{"text": "word similarity judgments", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6972625851631165}]}, {"text": "We used the same data as that used by Faruqui and Dyer (2014b) (English-Spanish only), and thus our method for obtaining multilingual embeddings is directly comparable to their technique for doing the same (CCA).", "labels": [], "entities": []}, {"text": "We used the first 11 tasks on wordvectors.org, and obtained Faruqui and Dyer's results from that website.", "labels": [], "entities": []}, {"text": "Due to space constraints, we only report the average performance across these 11 tasks for each of the methods we tested.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "To test statistical significance, we performed a paired permutation test, treating performance on each task as Method Average Correlation CCA 0.638 LSA 0.626 TI-LSA 0.628: Average correlation with human similarity judgments on 11 word-similarity tasks.", "labels": [], "entities": [{"text": "Method Average Correlation CCA 0.638 LSA 0.626 TI-LSA 0.628", "start_pos": 111, "end_pos": 170, "type": "METRIC", "confidence": 0.9228600263595581}]}, {"text": "The differences between these methods are not statistically significant, showing that the gains we see in cross-lingual tasks are not at the expense of monolingual tasks.", "labels": [], "entities": []}, {"text": "The important thing to note from the table is that the differences between the methods are all quite small, and none of them are statistically significant.", "labels": [], "entities": []}, {"text": "Note that LSA on just the English data performs on par with all of the other methods presented; we have not found away to improve performance on this monolingual task from using multilingual data.", "labels": [], "entities": []}, {"text": "1 However, it is also important to note that our multilingual methods do not hurt performance on these monolingual tasks, either-we get the benefits described in our other evaluations without losing performance on English-only tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1:  Labeled and unlabeled attachment score  (LAS/UAS) on a cross-lingual dependency task. TI-LSA out- performs prior work on this task.", "labels": [], "entities": [{"text": "unlabeled attachment score  (LAS/UAS)", "start_pos": 23, "end_pos": 60, "type": "METRIC", "confidence": 0.8189433850347996}]}, {"text": " Table 2: Average correlation with human similarity judg- ments on 11 word-similarity tasks. The differences between  these methods are not statistically significant, showing that  the gains we see in cross-lingual tasks are not at the expense  of monolingual tasks.", "labels": [], "entities": []}]}