{"title": [{"text": "Search-Aware Tuning for Hierarchical Phrase-based Decoding", "labels": [], "entities": [{"text": "Hierarchical Phrase-based Decoding", "start_pos": 24, "end_pos": 58, "type": "TASK", "confidence": 0.5735649764537811}]}], "abstractContent": [{"text": "Parameter tuning is a key problem for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Parameter tuning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7392570078372955}, {"text": "statistical machine translation (SMT)", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.8323843975861868}]}, {"text": "Most popular parameter tuning algorithms for SMT are agnostic of decoding, resulting in parameters vulnerable to search errors in decoding.", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9936635494232178}]}, {"text": "The recent research of \"search-aware tuning\" (Liu and Huang, 2014) addresses this problem by considering the partial derivations in every decoding step so that the promising ones are more likely to survive the inexact decoding beam.", "labels": [], "entities": [{"text": "search-aware tuning", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7050596475601196}]}, {"text": "We extend this approach from phrase-based translation to syntax-based translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in away inspired by inside-outside algorithm.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.7423765659332275}, {"text": "syntax-based translation", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.7384780645370483}]}, {"text": "Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin.", "labels": [], "entities": []}, {"text": "Extensive experiments on Chinese-to-English translation show significant BLEU improvements on MERT, MIRA and PRO.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.6319880187511444}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9993125200271606}, {"text": "MERT", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9792808890342712}, {"text": "MIRA", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9175083041191101}, {"text": "PRO", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.7084698677062988}]}], "introductionContent": [{"text": "Efforts in parameter tuning algorithms for SMT, such as MERT), MIRA (, and PRO () have improved the translation quality considerably in the past decade.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9945527911186218}, {"text": "MERT", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.802591860294342}, {"text": "MIRA", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9590199589729309}, {"text": "PRO", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.7892951369285583}]}, {"text": "These tuning algorithms share the same characteristic that they treat the decoder as a black box.", "labels": [], "entities": []}, {"text": "This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search decoding.", "labels": [], "entities": []}, {"text": "Recent researches reveal that the parameter tuning algorithms tailored for specific decoding algorithms can be beneficial in general structured prediction problems (, and in machine translation (;).", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.729159414768219}, {"text": "machine translation", "start_pos": 174, "end_pos": 193, "type": "TASK", "confidence": 0.7985706627368927}]}, {"text": "Particularly, show that by requiring the conventional parameter tuning algorithms to consider the final decoding results (full translations) as well as the intermediate decoding states (partial translations) at the same time, the inexact decoding can be significantly improved so that correct intermediate partial translations are more likely to survive the beam.", "labels": [], "entities": []}, {"text": "However, the underlying phrase-based decoding model suffers from limited distortion, and thus, may not be flexible enough for translating language pairs that are syntactically different, which require long distance reordering.", "labels": [], "entities": []}, {"text": "In order to better handle long distance reordering which beyonds the capability of phrase-based MT, we extend the search-aware tuning framework from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (HIERO).", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.8137978315353394}]}, {"text": "One key advantage of search-aware tuning for previous phrase-based MT is the minimal change to existing parameter tuning algorithms, which is achieved by defining BLEU-like metrics for the intermediate decoding states with sequencestructured derivations.", "labels": [], "entities": [{"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.7785980105400085}, {"text": "BLEU-like", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9933618307113647}]}, {"text": "To keep our approach simple, we generalize these BLEU-like metrics to handle intermediate decoding states with treestructured derivations in HIERO, which are calculated by dynamic programming algorithms inspired by the inside-outside algorithm.", "labels": [], "entities": [{"text": "BLEU-like", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9961859583854675}, {"text": "HIERO", "start_pos": 141, "end_pos": 146, "type": "DATASET", "confidence": 0.775692880153656}]}, {"text": "We make the following contributions: 1.", "labels": [], "entities": []}, {"text": "We extend the framework of search-aware tuning methods from phrase-based translation to syntax-based translation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.7433508038520813}, {"text": "syntax-based translation", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.7216037213802338}]}, {"text": "This extension is simple to be applied to most conventional parameter tuning methods, requiring minimal extra changes to existing algorithms.", "labels": [], "entities": []}, {"text": "2. We propose two BLEU metrics and their variants to evaluate partial derivations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9934301972389221}]}, {"text": "In order to efficiently compute the new BLEU metrics, we investigate dynamic programming based algorithms to recover the good partial derivations for search-aware tuning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.986348032951355}]}, {"text": "3. Our method obtains significant improvements on large-scale Chinese-to-English translation on top of MERT, MIRA and PRO baselines.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.5589848607778549}, {"text": "MERT", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9634734988212585}, {"text": "MIRA", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9607240557670593}]}], "datasetContent": [{"text": "To evaluate our method, we conduct experiments on Chinese-to-English translation.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.6547911614179611}]}, {"text": "The training data includes 1.8M bilingual sentence pairs, with about 40M Chinese words and 48M English words.", "labels": [], "entities": []}, {"text": "We generate symmetric word alignment using GIZA++ and the grow-diag-final-and strategy.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.684568777680397}]}, {"text": "We train a 4-gram language model on the Xinhua portion of English Gigaword corpus by SRILM toolkit).", "labels": [], "entities": [{"text": "Xinhua portion of English Gigaword corpus", "start_pos": 40, "end_pos": 81, "type": "DATASET", "confidence": 0.6875153283278147}, {"text": "SRILM toolkit", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.8733623325824738}]}, {"text": "We use BLEU 4 with \"average reference length\" to evaluate the translation performance for all experiments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9984651803970337}]}, {"text": "We use the NIST MT 2002 evaluation data (878 sentences) as the tuning set, and adopt NIST MT04 (1,789 sentences), MT05 (1,082 sentences), MT06 (616 sentences from new portion) and MT08 (691 sentences from new portion) data as the test set.", "labels": [], "entities": [{"text": "NIST MT 2002 evaluation data", "start_pos": 11, "end_pos": 39, "type": "DATASET", "confidence": 0.9286058068275451}, {"text": "NIST", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.9782145023345947}, {"text": "MT04", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.5439673662185669}, {"text": "MT05", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.8701223731040955}, {"text": "MT06", "start_pos": 138, "end_pos": 142, "type": "DATASET", "confidence": 0.8738572001457214}, {"text": "MT08", "start_pos": 180, "end_pos": 184, "type": "DATASET", "confidence": 0.9050329923629761}]}, {"text": "Our baseline system is an in-house hierarchical phrase-based system.", "labels": [], "entities": []}, {"text": "The translation rules are extracted with Moses toolkit () by default settings.", "labels": [], "entities": []}, {"text": "For the decoder, we set the beam size to 30, nbest list to 50, and 20 as the maximum length of spans for using non-glue rules.", "labels": [], "entities": []}, {"text": "The baseline tuning methods are batch tuning methods based on k-best translations, including MERT, MIRA (Cherry and Foster, 2012) and PRO (Hopkins and May, 2011) from Moses.", "labels": [], "entities": [{"text": "MERT", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9973835349082947}, {"text": "MIRA", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9970284104347229}, {"text": "PRO", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.9226111769676208}]}, {"text": "Another baseline tuning method is hypergraph-MERT from cdec toolkit (.", "labels": [], "entities": []}, {"text": "To guarantee the tuning efficiency, we constrain the minimum length of spans for searchaware tuning to restrict the number of training instances.", "labels": [], "entities": []}, {"text": "For the sentences with less than 20 words, we only use the spans longer than 0.75 times sentence length.", "labels": [], "entities": []}, {"text": "For the ones with more than 20 words, the minimum span length is set to 18. 7 All results are achieved by averaging three independent runs for fair comparison).", "labels": [], "entities": [{"text": "minimum span length", "start_pos": 42, "end_pos": 61, "type": "METRIC", "confidence": 0.6288033425807953}]}, {"text": "compares the main results of our MERTbased search-aware tuning with traditional tuning: MERT and hypergraph-MERT.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: MERT results: BLEU scores the test sets (nist03, nist04, nist05, nist06, and nist08). MERT-S is  an enhanced version of MERT, which uses a beam size 200 and nbest list 200 for tuning, and beam size  30 for testing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9982852339744568}]}, {"text": " Table 2: The oracle BLEU comparison between  baseline MERT and guided backtrace.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9984225034713745}, {"text": "MERT", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.7376288771629333}]}, {"text": " Table 2. In order to get the k-best oracle, we first  look for the best Bleu +1 translation in the k-best  list for each source sentence, and then use these  best translations to compute the BLEU score of the  entire set. To get the guided oracle, we use the  weights from baseline MERT to run Algorithm 2  on tuning set (nist02) and nist04 test set, and gen- erate the best oracle translation (section 3.2.3) for  each source sentence for evaluation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 192, "end_pos": 202, "type": "METRIC", "confidence": 0.978885293006897}]}, {"text": " Table 2. On both nist02 tuning set and nist04 test  set, guided backtrace method gains at lease 1.0", "labels": [], "entities": [{"text": "nist04 test  set", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.8149844308694204}]}, {"text": " Table 4: MIRA results: BLEU scores on test sets (nist03, nist04, nist05, nist06, and nist08).", "labels": [], "entities": [{"text": "MIRA", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.8197498917579651}, {"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9989314675331116}]}, {"text": " Table 5: PRO results: BLEU scores on test sets (nist03, nist04, nist05, nist06, and nist08). Similar to (Liu", "labels": [], "entities": [{"text": "PRO", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.47841575741767883}, {"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9989911913871765}]}, {"text": " Table 6: Comparison on training efficiency. The  time (in minutes) is measured at the last iteration  of tuning. Column \"Total\" refers to the time for an  entire iteration, while \"Optim.\" is the time of op- timization. We use a parallelized MERT for tuning  by 24 cores.", "labels": [], "entities": [{"text": "MERT", "start_pos": 242, "end_pos": 246, "type": "METRIC", "confidence": 0.7893917560577393}]}]}