{"title": [{"text": "A Neural Network Model for Low-Resource Universal Dependency Parsing", "labels": [], "entities": [{"text": "Low-Resource Universal Dependency Parsing", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.6654481738805771}]}], "abstractContent": [{"text": "Accurate dependency parsing requires large treebanks, which are only available fora few languages.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.6674033105373383}]}, {"text": "We propose a method that takes advantage of shared structure across languages to build a mature parser using less training data.", "labels": [], "entities": []}, {"text": "We propose a model for learning a shared \"univer-sal\" parser that operates over an inter-lingual continuous representation of language , along with language-specific mapping components.", "labels": [], "entities": []}, {"text": "Compared with supervised learning, our methods give a consistent 8-10% improvement across several treebanks in low-resource simulations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency parsing is an important task for Natural Language Processing (NLP) with application to text classification), relation extraction (), question answering (), statistical machine translation (, and sentiment analysis.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8796552419662476}, {"text": "text classification", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7290198802947998}, {"text": "relation extraction", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.8562550842761993}, {"text": "question answering", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.8621626794338226}, {"text": "statistical machine translation", "start_pos": 167, "end_pos": 198, "type": "TASK", "confidence": 0.757726768652598}, {"text": "sentiment analysis", "start_pos": 206, "end_pos": 224, "type": "TASK", "confidence": 0.9363456070423126}]}, {"text": "A mature parser normally requires a large treebank for training, yet such resources are rarely available and are costly to build.", "labels": [], "entities": []}, {"text": "Ideally, we would be able to construct a high quality parser with less training data, thereby enabling accurate parsing for lowresource languages.", "labels": [], "entities": []}, {"text": "In this paper we formalize the dependency parsing task fora low-resource language as a domain adaptation task, in which a target resource-poor language treebank is treated as in-domain, while a much larger treebank in a high-resource language forms the out-of-domain data.", "labels": [], "entities": [{"text": "dependency parsing task", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.8036373853683472}, {"text": "domain adaptation task", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.7677822808424631}]}, {"text": "In this way, we can apply well-understood domain adaptation techniques to the dependency parsing task.", "labels": [], "entities": [{"text": "dependency parsing task", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.8661293586095175}]}, {"text": "However, a crucial requirement for domain adaptation is that the in-domain and out-of-domain data have compatible representations.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.752039909362793}]}, {"text": "In applying our approach to data from several languages, we must learn such a cross-lingual representation.", "labels": [], "entities": []}, {"text": "Here we frame this representation learning as part of a neural network training.", "labels": [], "entities": []}, {"text": "The underlying hypothesis for the joint learning is that there are some shared-structures across languages that we can exploit.", "labels": [], "entities": []}, {"text": "This hypothesis is motivated by the excellent results of the cross-lingual application of unlexicalised parsing), whereby a delexicalized parser constructed on one language is applied directly to another language.", "labels": [], "entities": []}, {"text": "Our approach works by jointly training a neural network dependency parser to model the syntax in both a source and target language.", "labels": [], "entities": []}, {"text": "Many of the parameters of the source and target language parsers are shared, except fora small handful of language-specific parameters.", "labels": [], "entities": []}, {"text": "In this way, the information can flow back and forth between languages, allowing for the learning of a compatible cross-lingual syntactic representation, while also allowing the parsers to mutually correct one another's errors.", "labels": [], "entities": []}, {"text": "We include some language-specific components, in order to better model the lexicon of each language and allow learning of the syntactic idiosyncrasies of each language.", "labels": [], "entities": []}, {"text": "Our experiments show that this outperforms a purely supervised setting, on both small and large data conditions, with again as high as 10% for small training sets.", "labels": [], "entities": []}, {"text": "Our proposed joint training method also out-performs the conventional cascade approach where the parameters between source and target languages are related together through a regularization term (.", "labels": [], "entities": []}, {"text": "Our model is flexible, allowing easy incorporation of peripheral information.", "labels": [], "entities": []}, {"text": "For example, assuming the presence of a small bilingual dictionary is befitting of a low-resource setting, as this is prototypically one of the first artifacts generated by field linguists.", "labels": [], "entities": []}, {"text": "We incorporate a bilingual dictionary as a set of soft constraints on the model, such that it learns similar representations for each word and its translation(s).", "labels": [], "entities": []}, {"text": "For example, the representation of house in English should be close to haus in German.", "labels": [], "entities": []}, {"text": "We empirically show that adding a bilingual dictionary improves parser performance, particularly when target data is limited.", "labels": [], "entities": []}, {"text": "The final contribution of the paper concerns the learned word embeddings.", "labels": [], "entities": []}, {"text": "We demonstrate that these encode meaningful syntactic phenomena, both in terms of the observable clusters and through a verb classification task.", "labels": [], "entities": [{"text": "verb classification task", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.7661955753962199}]}, {"text": "The code for this paper is published as an open source project.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with the Universal Dependency Treebank (UDT) V1.0 (Nivre et al., 2015), simulating low resource settings.", "labels": [], "entities": [{"text": "Universal Dependency Treebank (UDT) V1.0", "start_pos": 23, "end_pos": 63, "type": "DATASET", "confidence": 0.7640613913536072}]}, {"text": "This treebank has many desirable properties for our model: the dependency types (arc labels set) and coarse POS tagset are the same across languages.", "labels": [], "entities": []}, {"text": "This removes the need for mapping the source and target language tagsets to a common tagset.", "labels": [], "entities": []}, {"text": "Moreover, the dependency types are also common across languages allowing evaluation of the labelled attachment score (LAS).", "labels": [], "entities": [{"text": "labelled attachment score (LAS)", "start_pos": 91, "end_pos": 122, "type": "METRIC", "confidence": 0.8260299563407898}]}, {"text": "The treebank covers 10 languages, with some languages very highly resourced-Czech, French and Spanish have 400k tokens-and only modest amounts of data for other languages-Hungarian and Irish have only around 25k tokens.", "labels": [], "entities": []}, {"text": "Cross-lingual models assume English as the source language, for which we have a large treebank, and only a small treebank of 3k tokens exists in each target language, simulated by subsampling the corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Labelled attachment score (LAS) for each model type trained on 3000 tokens for each target  language (columns). All bar the supervised model also use a large English treebank.", "labels": [], "entities": [{"text": "Labelled attachment score (LAS)", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.8547604580720266}]}, {"text": " Table 3: Compare the English side of our cross- lingual embeddings with various other embed- dings evaluated on Verb-143 dataset (Baker et al.,  2014). We directly use the pre-trained models  from corresponding papers.", "labels": [], "entities": [{"text": "Verb-143 dataset", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.9837643802165985}]}]}