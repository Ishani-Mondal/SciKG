{"title": [{"text": "Factorization of Latent Variables in Distributional Semantic Models", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper discusses the use of factoriza-tion techniques in distributional semantic models.", "labels": [], "entities": []}, {"text": "We focus on a method for redistributing the weight of latent variables, which has previously been shown to improve the performance of distributional semantic models.", "labels": [], "entities": []}, {"text": "However, this result has not been replicated and remains poorly understood.", "labels": [], "entities": []}, {"text": "We refine the method, and provide additional theoretical justification, as well as empirical results that demonstrate the viability of the proposed approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional Semantic Models (DSMs) have become standard paraphernalia in the natural language processing toolbox, and even though there is a wide variety of models available, the basic parameters of DSMs (context type and size, frequency weighting, and dimension reduction) are now well understood.", "labels": [], "entities": [{"text": "Distributional Semantic Models (DSMs)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6305882980426153}]}, {"text": "This is demonstrated by the recent convergence of state-of-the-art results.", "labels": [], "entities": []}, {"text": "However, there area few notable exceptions.", "labels": [], "entities": []}, {"text": "One is the performance improvements demonstrated in two different papers using a method for redistributing the weight of principal components (PCs) in factorized DSMs).", "labels": [], "entities": []}, {"text": "In the latter of these papers, the factorization of latent variables in DSMs is used to reach a perfect score of 100% correct answers on the TOEFL synonym test.", "labels": [], "entities": [{"text": "TOEFL synonym", "start_pos": 141, "end_pos": 154, "type": "TASK", "confidence": 0.44619907438755035}]}, {"text": "This result is somewhat surprising, since the factorization method is the inverse of what is normally used.", "labels": [], "entities": []}, {"text": "Neither the result nor the method has been replicated, and therefore remains poorly understood.", "labels": [], "entities": []}, {"text": "The goal of this paper is to replicate and explain the result.", "labels": [], "entities": []}, {"text": "In the following sections, we first provide a brief review of DSMs and factorization, and review the method for redistributing the weight of latent variables.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 62, "end_pos": 66, "type": "TASK", "confidence": 0.8950610756874084}]}, {"text": "We then replicate the 100% score on the TOEFL test and provide additional state-ofthe-art scores for the BLESS test.", "labels": [], "entities": [{"text": "TOEFL test", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8351693153381348}, {"text": "BLESS", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9957197308540344}]}, {"text": "We also provide a more principled reformulation of the factorization method that is better suited for practical applications.", "labels": [], "entities": []}], "datasetContent": [{"text": "We replicate the experiment setup of Bullinaria and Levy (2012) by removing punctuation and decapitalizing the ukWaC corpus ().", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.9855514466762543}]}, {"text": "The DSM includes the 50,000 most frequent words along with the remaining 23 TOEFL words and is populated using a \u00b12-sized context window.", "labels": [], "entities": [{"text": "DSM", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9207198619842529}]}, {"text": "Co-occurrence counts are weighted with PPMI, and SVD is applied to the resulting matrix, reducing the dimensionality to 5,000.", "labels": [], "entities": [{"text": "SVD", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9392081499099731}]}, {"text": "The results of removing the first PCs versus applying the Caron p-transform are shown in, which replicates the results from Bullinaria and.", "labels": [], "entities": []}, {"text": "In order to better understand what influence the transform has on the representations, we also provide results on the BLESS test (, which lists a number of related terms to 200 target terms.", "labels": [], "entities": [{"text": "BLESS test", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9455366432666779}]}, {"text": "The related terms represent 8 different kinds of semantic relations (co-hyponymy, hypernymy, meronymy, attribute, event, and three random classes corresponding to randomly selected nouns, adjectives and verbs), and it is thus possible to use the BLESS test to determine what type of semantic relation a model favors.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 246, "end_pos": 251, "type": "METRIC", "confidence": 0.9975162744522095}]}, {"text": "Since our primary interest here is in paradigmatic relations, we focus on the hypernymy and co-hyponymy relations, and require that the model scores one of the related terms from these classes higher than the related terms from the other classes.", "labels": [], "entities": []}, {"text": "The corpus was split into different sizes to test the statistical significance of the weight redistribution effect.", "labels": [], "entities": []}, {"text": "Furthermore, it shows that the optimal weight distribution depends on the size of the data.", "labels": [], "entities": []}, {"text": "shows the BLESS results for both the PC removal scheme and the Caron p-transform for different sizes of the corpus.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9983342289924622}, {"text": "PC removal", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.753623753786087}]}, {"text": "The best score is 92.96% for the PC removal, and 92.46% for the Caron p-transform, both using the full data set.", "labels": [], "entities": [{"text": "PC removal", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.7165712714195251}]}, {"text": "Similarly to the TOEFL results, we see better results fora larger number of removed PCs.", "labels": [], "entities": [{"text": "TOEFL", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.6788071393966675}]}, {"text": "Interestingly, there is clearly a larger improvement in performance of the Caron p-transform than for the PC removal scheme.", "labels": [], "entities": [{"text": "PC removal", "start_pos": 106, "end_pos": 116, "type": "TASK", "confidence": 0.7912940979003906}]}, {"text": "shows how the redistribution affects the different relations in the BLESS test.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9971984624862671}]}, {"text": "The violin plots are based on the maximum values of each relation, and the width of the violin represents the normalized probability density of cosine measures.", "labels": [], "entities": []}, {"text": "The cosine distributions, \u0398 i , are based on the best matches for each category i, and normalized by the total mean and variance amongst all categories\u02c6\u0398categories\u02c6 categories\u02c6\u0398 i = \u0398 i \u2212\u00b5 \u03c3 . Thus, the figure illustrates how well each category is separated from each other, the larger separation the better.", "labels": [], "entities": []}, {"text": "The results in indicate that the top 120 PCs contain a higher level of co-hyponymy rela-  tions than the lower; removing the top 120 PCs gives a violin shape that resembles the inverse of the plot for the top 120 PCs.", "labels": [], "entities": []}, {"text": "Although neither part of the PC span is significantly better in separating the categories, it is clear that removing the first 120 PCs increases the variance within the categories and especially amongst the coord-category.", "labels": [], "entities": []}, {"text": "This is an interesting result, since it seems to contradict the hypothesis that removing the first PCs improves the semantic quality of the representations -there is obviously valuable semantic information in the first PCs.", "labels": [], "entities": []}, {"text": "summarizes our top results on the TOEFL, BLESS, and also the SimLex-999 similarity test, and compares them to a baseline score from the Skipgram model (), trained on the same data using a window size of 2, negative samples, and 400-dimensional vectors.", "labels": [], "entities": [{"text": "TOEFL", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.7266753911972046}, {"text": "BLESS", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9971503615379333}, {"text": "SimLex-999", "start_pos": 61, "end_pos": 71, "type": "DATASET", "confidence": 0.57057124376297}, {"text": "similarity", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.6355883479118347}]}, {"text": "Unfortunately, the optimal redistribution of weight on the PCs for the respective top scores differ between the experiments.", "labels": [], "entities": []}, {"text": "For the PC removal the optimal number of removed PCs is 379 for TOEFL, 15 for BLESS and 128 for SimLex-999, while the optimal number for the Caron ptransform is -1.4 for TOEFL, 0.5 for BLESS and -0.40 for SimLex-999.", "labels": [], "entities": [{"text": "PC removal", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.9055272936820984}, {"text": "TOEFL", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.8195976614952087}, {"text": "BLESS", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9264600276947021}, {"text": "TOEFL", "start_pos": 170, "end_pos": 175, "type": "DATASET", "confidence": 0.8417889475822449}, {"text": "BLESS", "start_pos": 185, "end_pos": 190, "type": "METRIC", "confidence": 0.5862787961959839}]}, {"text": "Hence, there is likely no easy way to find a general expression of the optimal redistribution of weight on the PCs fora given application.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top results for the PC removal and Caron  p on each test compared to the Skipgram model.", "labels": [], "entities": [{"text": "PC removal", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.6811902523040771}, {"text": "Caron  p", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9849588572978973}, {"text": "Skipgram", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.8567150235176086}]}, {"text": " Table 2: Results for the PC removal and Caron p  using the 80/20 rule", "labels": [], "entities": [{"text": "PC removal", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.9014603197574615}]}]}