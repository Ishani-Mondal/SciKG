{"title": [{"text": "Not All Contexts Are Created Equal: Better Word Representations with Variable Attention", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce an extension to the bag-of-words model for learning words representations that take into account both syntactic and semantic properties within language.", "labels": [], "entities": []}, {"text": "This is done by employing an attention model that finds within the con-textual words, the words that are relevant for each prediction.", "labels": [], "entities": []}, {"text": "The general intuition of our model is that some words are only relevant for predicting local context (e.g. function words), while other words are more suited for determining global context , such as the topic of the document.", "labels": [], "entities": []}, {"text": "Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model.", "labels": [], "entities": []}, {"text": "Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging), dependency parsing () and machine translation ().", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.7123260051012039}, {"text": "dependency parsing", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.8751066625118256}, {"text": "machine translation", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.8223860859870911}]}, {"text": "These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words.", "labels": [], "entities": []}, {"text": "Once trained, these can be used as features, as initializations of other neural networks ().", "labels": [], "entities": []}, {"text": "The continuous bag-of-words () is one of the many models that learns word representations from raw textual data.", "labels": [], "entities": []}, {"text": "While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings ().", "labels": [], "entities": []}, {"text": "While models have been proposed to address this problem, the complexity of these models (\"Structured skip-n-gram\" and \"CWindow\") grows linearly assize of the window of words considered increases, as anew set of parameters is created for each relative position.", "labels": [], "entities": []}, {"text": "On the other hand, the continuous bag-of-words model requires no additional parameters as it builds the context representation by summing over the embeddings in the window and its performance is an order of magnitude higher than of other models.", "labels": [], "entities": []}, {"text": "In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word (distance to the left/right).", "labels": [], "entities": []}, {"text": "The main intuition behind our model is that the prediction of a word is only dependent on certain words within the context.", "labels": [], "entities": []}, {"text": "For instance, in the sentence We won the game!", "labels": [], "entities": []}, {"text": "Nicely played!, the prediction of the word played, depends on both the syntactic relation from nicely, which narrows down the list of candidates to verbs, and on the semantic relation from game, which further narrows down the list of candidates to verbs related to games.", "labels": [], "entities": []}, {"text": "On the other hand, the words we and the add very little to this particular prediction.", "labels": [], "entities": []}, {"text": "On the other hand, the word the is important for predicting the word game, since it is generally followed by nouns.", "labels": [], "entities": [{"text": "predicting", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.9722570776939392}]}, {"text": "Thus, we observe that the same word can be informative in some contexts and not in others.", "labels": [], "entities": []}, {"text": "In this case, distance is a key factor, as the word the is informative to predict the immediate neighboring words, but not distance ones.", "labels": [], "entities": [{"text": "distance", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9587281942367554}]}], "datasetContent": [{"text": "For syntax, we evaluate our embeddings in the domain of part-of-speech tagging in both supervised () and unsupervised tasks (.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7186848521232605}]}, {"text": "This later task is newly proposed, but we argue that success in it is a compelling demonstration of separation of words into syntactically coherent clusters.", "labels": [], "entities": [{"text": "separation of words into syntactically coherent clusters", "start_pos": 100, "end_pos": 156, "type": "TASK", "confidence": 0.8219887784549168}]}, {"text": "The work in () attempts to infer POS tags with a standard bigram hmm, which uses word embeddings to infer POS tags without supervision.", "labels": [], "entities": []}, {"text": "We use the same dataset, obtained from the ConLL 2007 shared task () Scoring is performed using the V-measure, which is used to predict syntactic classes at the word level.", "labels": [], "entities": [{"text": "ConLL 2007 shared task", "start_pos": 43, "end_pos": 65, "type": "DATASET", "confidence": 0.8841575235128403}, {"text": "Scoring", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9622741937637329}]}, {"text": "It has been shown in () that word embeddings learnt from structured skip-ngrams tend to work better at this task, mainly because it is less sensitive to larger window sizes.", "labels": [], "entities": []}, {"text": "These results are consistent with our observations found in, in rows \"Skip-ngram\" and \"SSkip-ngram\".", "labels": [], "entities": [{"text": "SSkip-ngram", "start_pos": 87, "end_pos": 98, "type": "DATASET", "confidence": 0.7938119769096375}]}, {"text": "We can observe that our attention based CBOW model (row \"CBOW Attention\") improves over these results for both tasks and also the original CBOW model (row \"CBOW\")..", "labels": [], "entities": [{"text": "Attention", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.7353619933128357}]}, {"text": "We can observe our model can obtain similar results compared to the structured skip-ngram model on this task, while training the model is significantly faster.", "labels": [], "entities": []}, {"text": "The gap between the usage of different embeddings is not as large as in POS induction, as this is a supervised task, where pre-training generally leads to smaller improvements.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.8892990946769714}]}, {"text": "To evaluate the quality of our vectors in terms of semantics, we use the sentiment analysis task (Senti), which is a binary classification task for movie reviews.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.8462300598621368}]}, {"text": "We simply use the mean of the word vectors of words in a sentence, and use them as features in an 2 -regularized logistic regression classifier.", "labels": [], "entities": []}, {"text": "We use the standard training/dev/test split and report accuracy on the test set in table 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9996040463447571}]}, {"text": "We can see that in this task, our models do not perform as well as the CBOW and Skipngram model, which hints that our model is learning embeddings that learn more towards syntax.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.8436589241027832}]}, {"text": "This is expected as it is generally uncommon for embeddings to outperform existing models on both syntactic and semantic tasks simultaneously, as embeddings tend to be either more semantically or syntactically oriented.", "labels": [], "entities": []}, {"text": "It is clear that the skipngram model learns embeddings that are more semantically oriented as it performs badly on all syntactic tasks.", "labels": [], "entities": []}, {"text": "The structured skip-ngram model on the other hand performs badly on the syntactic tasks, but we observe a large drop on this semantically oriented task.", "labels": [], "entities": []}, {"text": "Our attention-based model, on the other hand, out performs all other models on syntax-based tasks, while maintaining a competitive score on semantic tasks.", "labels": [], "entities": []}, {"text": "This is an encouraging result that shows that it is possible to learn representations that can perform well on both semantic and syntactic tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for unsupervised POS induction, supervised POS tagging and Sentiment Analysis (one  per column) using different types of embeddings (one per row).", "labels": [], "entities": [{"text": "POS induction", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.813528299331665}, {"text": "POS tagging", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.7179188430309296}, {"text": "Sentiment Analysis", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.8730053901672363}]}]}