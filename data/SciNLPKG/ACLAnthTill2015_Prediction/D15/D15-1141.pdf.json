{"title": [{"text": "Long Short-Term Memory Neural Networks for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.636220763127009}]}], "abstractContent": [{"text": "Currently most of state-of-the-art methods for Chinese word segmentation are based on supervised learning, whose features are mostly extracted from a local context.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.6125547289848328}]}, {"text": "These methods cannot utilize the long distance information which is also crucial for word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7806248962879181}]}, {"text": "In this paper, we propose a novel neural network model for Chinese word segmentation, which adopts the long short-term memory (LSTM) neu-ral network to keep the previous important information in memory cell and avoids the limit of window size of local context.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.5820016165574392}]}, {"text": "Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods.", "labels": [], "entities": [{"text": "PKU", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.9115092754364014}, {"text": "MSRA", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.7971232533454895}, {"text": "CTB6 benchmark datasets", "start_pos": 29, "end_pos": 52, "type": "DATASET", "confidence": 0.9069554607073466}]}], "introductionContent": [{"text": "Word segmentation is a fundamental task for Chinese language processing.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7050560116767883}, {"text": "Chinese language processing", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.7231979171435038}]}, {"text": "In recent years, Chinese word segmentation (CWS) has undergone great development.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.7672441701094309}]}, {"text": "The popular method is to regard word segmentation task as a sequence labeling problem).", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.8031861980756124}]}, {"text": "The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) and Conditional Random Fields (CRF) ().", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.674662247300148}]}, {"text": "However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus.", "labels": [], "entities": []}, {"text": "Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering et al.,.", "labels": [], "entities": []}, {"text": "developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English.", "labels": [], "entities": []}, {"text": "applied the architecture of to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speedup the training process with negligible loss in performance.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7017899751663208}, {"text": "POS tagging", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.7365500032901764}]}, {"text": "models tag-tag interactions, tagcharacter interactions and character-character interactions based on.  proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task.", "labels": [], "entities": [{"text": "Chinese word segmentation task", "start_pos": 211, "end_pos": 241, "type": "TASK", "confidence": 0.6478031873703003}]}, {"text": "Each neuron in GRNN can be regarded as a different combination of the input characters.", "labels": [], "entities": [{"text": "GRNN", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.8497105836868286}]}, {"text": "Thus, the whole GRNN has an ability to simulate the design of the sophisticated features in traditional methods.", "labels": [], "entities": [{"text": "GRNN", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.8343306183815002}]}, {"text": "Despite of their success, a limitation of them is that their performances are easily affected by the size of the context window.", "labels": [], "entities": []}, {"text": "Intuitively, many words are difficult to segment based on the local information only.", "labels": [], "entities": []}, {"text": "For example, the segmentation of the following sentence needs the information of the long distance collocation.", "labels": [], "entities": []}, {"text": "\u51ac \u5929 (winter)\uff0c \u80fd (can) \u7a7f (wear) \u591a \u5c11 (amount) \u7a7f (wear) \u591a \u5c11 (amount)\uff1b \u590f \u5929 (summer)\uff0c\u80fd (can) \u7a7f (wear) \u591a (more) \u5c11 (little) \u7a7f (wear) \u591a (more) \u5c11 (little)\u3002", "labels": [], "entities": []}, {"text": "Without the word \"\u590f\u5929 (summer)\" or \"\u51ac\u5929 (winter)\", it is difficult to segment the phrase \"\u80fd \u7a7f\u591a\u5c11\u7a7f\u591a\u5c11\".", "labels": [], "entities": []}, {"text": "Therefore, we usually need utilize the non-local information for more accurate word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.6996486335992813}]}, {"text": "However, it does notwork by simply increasing the context window size.", "labels": [], "entities": []}, {"text": "As reported in (, the performance drops smoothly when the window size is larger than 3.", "labels": [], "entities": []}, {"text": "The reason is that the number of its parameters is so large that the trained network has overfitted on training data.", "labels": [], "entities": []}, {"text": "Therefore, it is necessary to capture the potential long-distance dependencies without increasing the size of the context window.", "labels": [], "entities": []}, {"text": "In order to address this problem, we propose a neural model based on Long Short-Term Memory Neural Network (LSTM)) that explicitly model the previous information by exploiting input, output and forget gates to decide how to utilize and update the memory of pervious information.", "labels": [], "entities": []}, {"text": "Intuitively, if the LSTM unit detects an important feature from an input sequence at early stage, it easily carries this information (the existence of the feature) over along distance, hence, capturing the potential useful long-distance information.", "labels": [], "entities": []}, {"text": "We evaluate our model on three popular benchmark datasets (PKU, MSRA and CTB6), and the experimental results show that our model achieves the state-of-the-art performance with the smaller context window size (0,2).", "labels": [], "entities": [{"text": "PKU", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.9198259115219116}, {"text": "MSRA", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.7254323959350586}, {"text": "CTB6", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.7164778113365173}]}, {"text": "The contributions of this paper can be summarized as follows.", "labels": [], "entities": []}, {"text": "\u2022 We first introduce the LSTM neural network for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.5935830970605215}]}, {"text": "The LSTM can capture potential long-distance dependencies and keep the previous useful information in memory, which avoids the limit of the size of context window.", "labels": [], "entities": []}, {"text": "\u2022 Although there are relatively few researches of applying dropout method to the LSTM, we investigate several dropout strategies and find that dropout is also effective to avoid the overfitting of the LSTM.", "labels": [], "entities": []}, {"text": "\u2022 Despite Chinese word segmentation being a specific case, our model can be easily generalized and applied to the other sequence labeling tasks.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.591615895430247}, {"text": "sequence labeling tasks", "start_pos": 120, "end_pos": 143, "type": "TASK", "confidence": 0.711651623249054}]}], "datasetContent": [{"text": "We use three popular datasets, PKU, MSRA and CTB6, to evaluate our model.", "labels": [], "entities": [{"text": "PKU", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.780039370059967}, {"text": "MSRA", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.5491747856140137}, {"text": "CTB6", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.6141086220741272}]}, {"text": "The PKU  Figure 5: Performances of LSTM-1 (0,2) with 20% dropout on PKU development set. and MSRA data are provided by the second International Chinese Word Segmentation Bakeoff (, and CTB6 is from Chinese TreeBank 6.0 (LDC2007T36) (), which is a segmented, part-of-speech tagged and fully bracketed corpus in the constituency formalism.", "labels": [], "entities": [{"text": "PKU  Figure 5", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8927319447199503}, {"text": "PKU development set.", "start_pos": 68, "end_pos": 88, "type": "DATASET", "confidence": 0.8646615346272787}, {"text": "MSRA data", "start_pos": 93, "end_pos": 102, "type": "DATASET", "confidence": 0.7433606088161469}, {"text": "International Chinese Word Segmentation Bakeoff", "start_pos": 130, "end_pos": 177, "type": "DATASET", "confidence": 0.8622684597969055}, {"text": "Chinese TreeBank 6.0 (LDC2007T36)", "start_pos": 198, "end_pos": 231, "type": "DATASET", "confidence": 0.9312013586362203}]}, {"text": "These datasets are commonly used by previous state-of-the-art models and neural network models.", "labels": [], "entities": []}, {"text": "In addition, we use the first 90% sentences of the training data as training set and the rest 10% sentences as development set for PKU and MSRA datasets.", "labels": [], "entities": [{"text": "PKU", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.8706969022750854}, {"text": "MSRA datasets", "start_pos": 139, "end_pos": 152, "type": "DATASET", "confidence": 0.8392281830310822}]}, {"text": "For CTB6 dataset, we divide the training, development and test sets according to) All datasets are preprocessed by replacing the Chinese idioms and the continuous English characters and digits with a unique flag.", "labels": [], "entities": [{"text": "CTB6 dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9741008281707764}]}, {"text": "For evaluation, we use the standard bake-off scoring program to calculate precision, recall, F1-score and out-of-vocabulary (OOV) word recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9995830655097961}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9991558790206909}, {"text": "F1-score", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9991326928138733}]}, {"text": "In this section, we give comparisons of the LSTM-1 with pervious neural models and state-of-the-art methods on the PKU, MSRA and CTB6 datasets.", "labels": [], "entities": [{"text": "PKU", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.9675016403198242}, {"text": "MSRA", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.8108062744140625}, {"text": "CTB6 datasets", "start_pos": 129, "end_pos": 142, "type": "DATASET", "confidence": 0.8260961472988129}]}, {"text": "We first compare our model with two neural models () on Chinese word segmentation task with random initialized character embeddings.", "labels": [], "entities": [{"text": "Chinese word segmentation task", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.6275753155350685}]}, {"text": "As showing in Table 4, the performance is boosted significantly by utilizing LSTM unit.", "labels": [], "entities": []}, {"text": "And more notably, our window size of the context characters is set to (0,2), while the size of the other models is.", "labels": [], "entities": []}, {"text": "Previous works found that the performance can be improved by pre-training the character embeddings on large unlabeled data.", "labels": [], "entities": []}, {"text": "We use word2vec) toolkit to pre-train the character embeddings on the Chinese Wikipedia corpus.", "labels": [], "entities": [{"text": "Chinese Wikipedia corpus", "start_pos": 70, "end_pos": 94, "type": "DATASET", "confidence": 0.8611311713854471}]}, {"text": "The obtained embeddings are used to initialize the character lookup table instead of random initialization.", "labels": [], "entities": []}, {"text": "Inspired by), we also utilize bigram character embeddings which is simply initialized as the average of embeddings of two consecutive characters.    with previous neural models with pre-trained embedding and bigram embeddings.", "labels": [], "entities": []}, {"text": "lists the performances of our model as well as previous state-of-the-art systems.", "labels": [], "entities": []}, {"text": "() is a word-based segmentation algorithm, which exploit features of complete words, while the rest of the list are character-based word segmenters, whose features are mostly extracted from a window of characters.", "labels": [], "entities": [{"text": "word-based segmentation", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.7040101736783981}]}, {"text": "Moreover, some systems (such as Sun and Xu (2011) and) also exploit kinds of extra information such as unlabeled data or other knowledge.", "labels": [], "entities": []}, {"text": "Despite our model only uses simple bigram features, it outperforms previous state-of-the-art models which use more complex features.", "labels": [], "entities": []}, {"text": "Since that we do not focus on the speed of the algorithm in this paper, we do not optimize the speed a lot.", "labels": [], "entities": []}, {"text": "On PKU dataset, it takes about 3 days to train the model (last row of) using CPU (Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz) only.", "labels": [], "entities": [{"text": "PKU dataset", "start_pos": 3, "end_pos": 14, "type": "DATASET", "confidence": 0.9263476431369781}]}, {"text": "All implementation is based on Python.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Settings of the hyper-parameters.", "labels": [], "entities": []}, {"text": " Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.", "labels": [], "entities": [{"text": "PKU test set", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.9625253478686014}]}, {"text": " Table 3: Performance on our four proposed models  on PKU test set.", "labels": [], "entities": [{"text": "PKU test set", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.9783405462900797}]}, {"text": " Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.", "labels": [], "entities": []}, {"text": " Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.", "labels": [], "entities": []}, {"text": " Table 6: Comparison of our model with state-of- the-art methods on three test sets.", "labels": [], "entities": []}]}