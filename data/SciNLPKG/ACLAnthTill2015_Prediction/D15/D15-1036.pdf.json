{"title": [{"text": "Evaluation methods for unsupervised word embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a comprehensive study of evaluation methods for unsupervised embedding techniques that obtain meaningful representations of words from text.", "labels": [], "entities": []}, {"text": "Different evaluations result in different orderings of embedding methods, calling into question the common assumption that there is one single optimal vector representation.", "labels": [], "entities": []}, {"text": "We present new evaluation techniques that directly compare embeddings with respect to specific queries.", "labels": [], "entities": []}, {"text": "These methods reduce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural word embeddings represent meaning via geometry.", "labels": [], "entities": []}, {"text": "A good embedding provides vector representations of words such that the relationship between two vectors mirrors the linguistic relationship between the two words.", "labels": [], "entities": []}, {"text": "Despite the growing interest in vector representations of semantic information, there has been relatively little work on direct evaluations of these models.", "labels": [], "entities": []}, {"text": "In this work, we explore several approaches to measuring the quality of neural word embeddings.", "labels": [], "entities": []}, {"text": "In particular, we perform a comprehensive analysis of evaluation methods and introduce novel methods that can be implemented through crowdsourcing, providing better insights into the relative strengths of different embeddings.", "labels": [], "entities": []}, {"text": "Existing schemes fall into two major categories: extrinsic and intrinsic evaluation.", "labels": [], "entities": []}, {"text": "In extrinsic evaluation, we use word embeddings as input features to a downstream task and measure changes in performance metrics specific to that task.", "labels": [], "entities": []}, {"text": "Examples include part-of-speech tagging and named-entity recognition ().", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7592835128307343}, {"text": "named-entity recognition", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.7486056089401245}]}, {"text": "Extrinsic evaluation only provides one way to specify the goodness of an embedding, and it is not clear how it connects to other measures.", "labels": [], "entities": []}, {"text": "Intrinsic evaluations directly test for syntactic or semantic relationships between words ().", "labels": [], "entities": []}, {"text": "These tasks typically involve a pre-selected set of query terms and semantically related target words, which we refer to as a query inventory.", "labels": [], "entities": []}, {"text": "Methods are evaluated by compiling an aggregate score for each method such as a correlation coefficient, which then serves as an absolute measure of quality.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 80, "end_pos": 103, "type": "METRIC", "confidence": 0.9780473709106445}]}, {"text": "Query inventories have so far been collected opportunistically from prior work in psycholinguistics, information retrieval (), and image analysis ().", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.8154290914535522}, {"text": "image analysis", "start_pos": 131, "end_pos": 145, "type": "TASK", "confidence": 0.7685716450214386}]}, {"text": "Because these inventories were not constructed for word embedding evaluation, they are often idiosyncratic, dominated by specific types of queries, and poorly calibrated to corpus statistics.", "labels": [], "entities": [{"text": "word embedding evaluation", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.6643950442473093}]}, {"text": "To remedy these problems, this paper makes the following contributions.", "labels": [], "entities": []}, {"text": "First, this is the first paper to conduct a comprehensive study covering a wide range of evaluation criteria and popular embedding techniques.", "labels": [], "entities": []}, {"text": "In particular, we study how outcomes from three different evaluation criteria are connected: word relatedness, coherence, downstream performance.", "labels": [], "entities": []}, {"text": "We show that using different criteria results in different relative orderings of embeddings.", "labels": [], "entities": []}, {"text": "These results indicate that embedding methods should be compared in the context of a specific task, e.g., linguistic insight or good downstream performance.", "labels": [], "entities": []}, {"text": "Second, we study the connections between direct evaluation with real users and pre-collected offline data.", "labels": [], "entities": []}, {"text": "We propose anew approach to evaluation that focuses on direct comparison of embeddings with respect to individual queries rather than overall summary scores.", "labels": [], "entities": []}, {"text": "Because we phrase all tasks as choice problems rather than ordinal relevance tasks, we can ease the burden of the an-notators.", "labels": [], "entities": []}, {"text": "We show that these evaluations can be gathered efficiently from crowdsourcing.", "labels": [], "entities": []}, {"text": "Our results also indicate that there is in fact strong correlation between the results of automated similarity evaluation and direct human evaluation.", "labels": [], "entities": []}, {"text": "This result justifies the use of offline data, at least for the similarity task.", "labels": [], "entities": []}, {"text": "Third, we propose a model-and data-driven approach to constructing query inventories.", "labels": [], "entities": []}, {"text": "Rather than picking words in an ad hoc fashion, we select query words to be diverse with respect to their frequency, parts-of-speech and abstractness.", "labels": [], "entities": []}, {"text": "To facilitate systematic evaluation and comparison of new embedding models, we release anew frequency-calibrated query inventory along with all user judgments at http://www.cs. cornell.edu/ \u02dc schnabts/eval/.", "labels": [], "entities": []}, {"text": "Finally, we observe that word embeddings encode a surprising degree of information about word frequency.", "labels": [], "entities": []}, {"text": "We found this was true even in models that explicitly reserve parameters to compensate for frequency effects.", "labels": [], "entities": []}, {"text": "This finding may explain some of the variability across embeddings and across evaluation methods.", "labels": [], "entities": []}, {"text": "It also casts doubt on the common practice of using the vanilla cosine similarity as a similarity measure in the embedding space.", "labels": [], "entities": []}, {"text": "It is important to note that this work is a survey of evaluation methods not a survey of embedding methods.", "labels": [], "entities": []}, {"text": "The specific example embeddings presented here were chosen as representative samples only, and may not be optimal.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the absolute intrinsic evaluation, we used the same datasets and tasks as . While we present results on all tasks for completeness, we will mainly focus on relatedness in this section.", "labels": [], "entities": []}, {"text": "There are four broad categories: \u2022 Relatedness: These datasets contain relatedness scores for pairs of words; the cosine similarity of the embeddings for two words should have high correlation (Spearman or Pearson) with human relatedness scores.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 206, "end_pos": 213, "type": "METRIC", "confidence": 0.8695619106292725}]}, {"text": "\u2022 Analogy: This task was popularized by.", "labels": [], "entities": []}, {"text": "The goal is to find a term x fora given term y so that x : y best resembles a sample relationship a : b.", "labels": [], "entities": []}, {"text": "\u2022 Categorization: Here, the goal is to recover a clustering of words into different categories.", "labels": [], "entities": []}, {"text": "To do this, the corresponding word vectors of all words in a dataset are clustered and the purity of the returned clusters is computed with respect to the labeled dataset.", "labels": [], "entities": [{"text": "purity", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9759058952331543}]}, {"text": "\u2022 Selectional preference: The goal is to determine how typical a noun is fora verb either as a subject or as an object (e.g., people eat, but we rarely eat people).", "labels": [], "entities": []}, {"text": "We follow the procedure that is outlined in . Several important design questions come up when designing reusable datasets for evaluating relatedness.", "labels": [], "entities": []}, {"text": "While we focus mainly on challenges that arise in the relatedness evaluation task, many of the questions discussed also apply to other scenarios.", "labels": [], "entities": [{"text": "relatedness evaluation task", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.884268323580424}]}, {"text": "How we pick the word pairs to evaluate affects the results of the evaluation.", "labels": [], "entities": []}, {"text": "The commonly-used WordSim-353 dataset (), for example, only tries to have word pairs with a diverse set of similarity scores.", "labels": [], "entities": [{"text": "WordSim-353 dataset", "start_pos": 18, "end_pos": 37, "type": "DATASET", "confidence": 0.9756011366844177}]}, {"text": "The more recent MEN dataset () follows a similar strategy, but restricts queries to words that occur as annotations in an image dataset.", "labels": [], "entities": [{"text": "MEN dataset", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.8744631707668304}]}, {"text": "However, there are more important criteria that should be considered in order to create a diverse dataset: (i) the frequency of the words in the English language (ii) the parts of speech of the words and (iii) abstractness vs. concreteness of the terms.", "labels": [], "entities": []}, {"text": "Not only is frequency important because we want to test the quality of embeddings on rare words, but also because it is related with distance in the embedding space as we show later and should be explicitly considered.", "labels": [], "entities": [{"text": "frequency", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9633171558380127}]}, {"text": "The main conceptual shortcoming of using correlation-based metrics is that they aggregate scores of different pairseven though these scores can vary greatly in the embedding space.", "labels": [], "entities": []}, {"text": "We can view the relatedness task as the task of evaluating a set of rankings, similar to ranking evaluation in Information Retrieval.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.7015305608510971}]}, {"text": "More specifically, we have one query for each unique query word wand rank all remaining words v in the vocabulary accordingly.", "labels": [], "entities": []}, {"text": "The problem now is that we usually cannot directly compare scores from different rankings) as their scores are not guaranteed to have the same ranges.", "labels": [], "entities": []}, {"text": "An even worse case is the following scenario.", "labels": [], "entities": []}, {"text": "Assume we use rank correlation as our metric.", "labels": [], "entities": []}, {"text": "As a consequence, we need our gold ranking to define an order on all the word pairs.", "labels": [], "entities": []}, {"text": "However, this also means that we somehow need to order completely unrelated word pairs; for example, we have to decide whether (dog, cat) is more similar than (banana, apple).", "labels": [], "entities": []}, {"text": "presents the results on 14 different datasets for the six embedding models.", "labels": [], "entities": []}, {"text": "We excluded examples from datasets that contained words not in our vocabulary.", "labels": [], "entities": []}, {"text": "For the relatedness and selective preference tasks, the numbers in the table indicate the correlation coefficient of human scores and the cosine similarity times 100.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 90, "end_pos": 113, "type": "METRIC", "confidence": 0.9383055865764618}]}, {"text": "The numbers for the categorization tasks reflect the purities of the resulting clusters.", "labels": [], "entities": []}, {"text": "For the analogy task, we report accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9980947375297546}]}, {"text": "In comparative evaluation, users give direct feedback on the embeddings themselves, so we do not have to define a metric that compares scored word pairs.", "labels": [], "entities": []}, {"text": "Rather than defining both query and target words, we need only choose query words since the embeddings themselves will be used to define the comparable target words.", "labels": [], "entities": []}, {"text": "We compiled a diverse inventory of 100 query words that balance frequency, part of speech (POS), and concreteness.", "labels": [], "entities": []}, {"text": "First, we selected 10 out of 45 broad categories from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.9727832674980164}]}, {"text": "We then chose an equal number of categories that mostly contained abstract concepts and categories that referred to concrete concepts.", "labels": [], "entities": []}, {"text": "Among those categories, we had one for adjectives and adverbs each, and four for nouns and verbs each.", "labels": [], "entities": []}, {"text": "From each category, we drew ten random words with the restriction that there be exactly three rare words (i.e., occurring fewer than 2500 times in the training corpus) among the ten.", "labels": [], "entities": []}, {"text": "Our experiments were performed with users from Amazon Mechanical Turk (MTurk) that were native speakers of English with sufficient experience and positive feedback on the Amazon Mechanical Turk framework.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk)", "start_pos": 47, "end_pos": 77, "type": "DATASET", "confidence": 0.8503666718800863}, {"text": "Amazon Mechanical Turk framework", "start_pos": 171, "end_pos": 203, "type": "DATASET", "confidence": 0.9037396609783173}]}, {"text": "For each of the 100 query words in the dataset, the nearest neighbors at ranks k \u2208 {1, 5, 50} for the six embeddings were retrieved.", "labels": [], "entities": []}, {"text": "For each query word and k, we presented the six words along with the query word to the users.", "labels": [], "entities": []}, {"text": "Each Turker was requested to evaluate between 25 and 50 items per task, where an item corresponds to the query word and the set of 6 retrieved neighbor words from each of the 6 embeddings.", "labels": [], "entities": []}, {"text": "The payment was between $0.01 and $0.02 per item.", "labels": [], "entities": []}, {"text": "The users were then asked to pick the word that is most similar according to their perception (the instructions were almost identical to the WordSim-353 dataset instructions).", "labels": [], "entities": [{"text": "WordSim-353 dataset instructions", "start_pos": 141, "end_pos": 173, "type": "DATASET", "confidence": 0.9629871646563212}]}, {"text": "Duplicate words were consolidated, and a click was counted for all embeddings that returned that word.", "labels": [], "entities": [{"text": "click", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.7474608421325684}]}, {"text": "An option \"I don't know the meaning of one (or several) of the words\" was also provided as an alternative.", "labels": [], "entities": []}, {"text": "shows an example instance that was given to the Turkers.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9054533839225769}]}, {"text": "Query: skillfully (a) swiftly (b) expertly (c) cleverly (d) pointedly The combination of 100 query words and 3 ranks yielded 300 items on which we solicited judgements by a median of 7 Turkers (min=5, max=14).", "labels": [], "entities": [{"text": "cleverly", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9923824667930603}, {"text": "Turkers", "start_pos": 185, "end_pos": 192, "type": "METRIC", "confidence": 0.9181187748908997}]}, {"text": "We compare embeddings by average win ratio, where the win ratio was how many times raters chose embedding e divided by the number of total ratings for item i.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on absolute intrinsic evaluation. The best result for each dataset is highlighted in bold.  The second row contains the names of the corresponding datasets.", "labels": [], "entities": []}, {"text": " Table 4: F1 chunking results using different word  embeddings as features. The p-values are with re- spect to the best performing method.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9738666415214539}]}, {"text": " Table 5: F1 sentiment analysis results using differ- ent word embeddings as features. The p-values are  with respect to the best performing embedding.", "labels": [], "entities": [{"text": "F1 sentiment analysis", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8567923903465271}]}]}