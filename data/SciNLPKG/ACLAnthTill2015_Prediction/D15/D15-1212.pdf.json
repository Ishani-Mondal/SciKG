{"title": [{"text": "Multilingual discriminative lexicalized phrase structure parsing", "labels": [], "entities": [{"text": "Multilingual discriminative lexicalized phrase structure parsing", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.7102534423271815}]}], "abstractContent": [{"text": "We provide a generalization of discrimina-tive lexicalized shift reduce parsing techniques for phrase structure grammar to a wide range of morphologically rich languages.", "labels": [], "entities": [{"text": "phrase structure grammar", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.8353970249493917}]}, {"text": "The model is efficient and outper-forms recent strong baselines on almost all languages considered.", "labels": [], "entities": []}, {"text": "It takes advantage of a dependency based modelling of morphology and a shallow modelling of constituency boundaries.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexicalized phrase structure parsing techniques were first introduced by and as generative probabilistic models.", "labels": [], "entities": [{"text": "Lexicalized phrase structure parsing", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.776493027806282}]}, {"text": "Nowadays most statistical models used in natural language processing are discriminative: discriminative models provide more flexibility for modelling a large number of variables and conveniently expressing their interactions.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.6908349196116129}]}, {"text": "This trend is particularly striking if we consider the literature in dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8903744518756866}]}, {"text": "Most state of the art multilingual parsers are actually weighted by discriminative models (.", "labels": [], "entities": []}, {"text": "With respect to multilingual phrase structure parsing, the situation is quite different.", "labels": [], "entities": [{"text": "multilingual phrase structure parsing", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.6823093146085739}]}, {"text": "Most parsers focus on fixed word order languages like English or Chinese as exemplified by.", "labels": [], "entities": []}, {"text": "Despite a few exceptions (), multilingual state of the art results are generally derived from the generative model of.", "labels": [], "entities": []}, {"text": "Although more recently introduced a conditional random field parser that clearly improved the state of the art in the multilingual setting.", "labels": [], "entities": [{"text": "conditional random field parser", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.6370188146829605}]}, {"text": "Both and frame their parsing model to model in priority regular surfacic patterns and word order: crucially infers category refinements (called category 'splits') in order to specialize the grammar on recurrent informative patterns observed on input spans.", "labels": [], "entities": []}, {"text": "relies on a similar intuition : the model essentially aims to capture regularities on the spans of constituents and their immediate neighbourhood, following earlier intuitions of.", "labels": [], "entities": []}, {"text": "This modelling strategy has two main motivations.", "labels": [], "entities": []}, {"text": "First it reduces the burden of feature engineering, making it easier to generalize to multiple languages.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7806684672832489}]}, {"text": "Second it avoids modeling explicitly bilexical dependencies for which parameters are notoriously hard to estimate from small data sets such as existing treebanks.", "labels": [], "entities": []}, {"text": "On the other hand this strategy becomes less intuitive when it comes to modeling free word order languages where word order and constituency should in principle be less informative.", "labels": [], "entities": []}, {"text": "As such, the good results reported by are surprising.", "labels": [], "entities": []}, {"text": "It suggests that word order and constituency might be more relevant than often thought for modelling free word order languages.", "labels": [], "entities": [{"text": "word order", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.7451439201831818}]}, {"text": "Nevertheless, free word order languages also tend to be morphologically rich languages.", "labels": [], "entities": []}, {"text": "This paper shows that a parsing model that can effectively take morphology into account is key for parsing these languages.", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9648955464363098}, {"text": "parsing", "start_pos": 99, "end_pos": 106, "type": "TASK", "confidence": 0.9639459848403931}]}, {"text": "More specifically, we show that an efficient lexicalized phrase structure parser -modelling both dependencies and morphology -already significantly improves parsing accuracy.", "labels": [], "entities": [{"text": "phrase structure parser", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.659920354684194}, {"text": "parsing", "start_pos": 157, "end_pos": 164, "type": "TASK", "confidence": 0.9570446014404297}, {"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.8520093560218811}]}, {"text": "But we also show that an additional modelling of spans and constituency provides additional robustness that contributes to yield state of the art results on almost all languages considered, while remaining quite efficient.", "labels": [], "entities": []}, {"text": "Moreover, given the availability of existing multi-view treebanks (, our proposed solution only requires a lightweight infrastructure to achieve multilin-gual parsing without requiring costly languagedependent modifications such as feature engineering.", "labels": [], "entities": [{"text": "multilin-gual parsing", "start_pos": 145, "end_pos": 166, "type": "TASK", "confidence": 0.6477208286523819}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first review the properties of multiview treebanks (Section 2).", "labels": [], "entities": []}, {"text": "As these treebanks typically do not provide directly head annotation, an information required for lexicalized parsing, we provide an automated multilingual head annotation procedure (Section 3).", "labels": [], "entities": []}, {"text": "We then describe in section 4 a variant of lexicalized shift reduce parsing that we use for the multilingual setting.", "labels": [], "entities": [{"text": "lexicalized shift reduce parsing", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.6856821179389954}]}, {"text": "It provides away to integrate morphology in the model.", "labels": [], "entities": []}, {"text": "Section 5 finally describes a set of experiments designed to test our main hypothesis and to point out the improvements overstate of the art in multilingual parsing.", "labels": [], "entities": [{"text": "multilingual parsing", "start_pos": 144, "end_pos": 164, "type": "TASK", "confidence": 0.6616170704364777}]}], "datasetContent": [{"text": "The experiments aim to compare the contribution of span based features approximating some intuitions of for shift reduce parsing and morphological features for parsing free word order languages.", "labels": [], "entities": [{"text": "parsing free word order languages", "start_pos": 160, "end_pos": 193, "type": "TASK", "confidence": 0.8974642038345337}]}, {"text": "We start by describing the evaluation protocol and by defining the models used.", "labels": [], "entities": []}, {"text": "We use the standard SPMRL data set).", "labels": [], "entities": [{"text": "SPMRL data set", "start_pos": 20, "end_pos": 34, "type": "DATASET", "confidence": 0.8683018883069357}]}, {"text": "Part of speech tags are generated with Marmot), a CRF tagger specifically designed to provide tuple-structured tags.", "labels": [], "entities": []}, {"text": "The training and development sets are tagged by 10-fold jackknifing.", "labels": [], "entities": []}, {"text": "Head annotation is supplied by the Robust procedure described in Section 3.", "labels": [], "entities": [{"text": "Head annotation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8295117020606995}]}, {"text": "The parser is systematically trained for 25 epochs with a max violation update perceptron, abeam of size 8 and a minibatch size of 24.", "labels": [], "entities": []}, {"text": "To enable a comparison with other published results, the evaluation is performed with aversion of evalb provided by the SPMRL organizers) which takes punctuation into account.", "labels": [], "entities": [{"text": "SPMRL organizers", "start_pos": 120, "end_pos": 136, "type": "DATASET", "confidence": 0.7224506139755249}]}, {"text": "Baseline model (B) The baseline model uses a set of templates identical to those of for parsing English and Chinese except that we have no specific templates for unary reductions.", "labels": [], "entities": [{"text": "parsing English and Chinese", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.8550590425729752}]}, {"text": "Span-based model (B+S) This model extends the B model by modeling spans.", "labels": [], "entities": []}, {"text": "The span model approximates an intuition underlying: constituent boundaries contain very informative tokens (typically function words).", "labels": [], "entities": []}, {"text": "These tokens together with the pattern of their neighborhood provide key clues for detecting and (sub-)typing constituents.", "labels": [], "entities": []}, {"text": "Moreover, parameter estimation for frequent functional words should suffer less from data sparseness issues than the estimation of bilexical dependencies on lexical head words.", "labels": [], "entities": []}, {"text": "The model includes conjunctions of nonterminal symbols on the stack with their left and right corners (words or tags) and also their immediately adjacent tokens across constituents.", "labels": [], "entities": []}, {"text": "Using the notation given in  from which we derived additional backoff templates where only a single corner condition is expressed and/or words are replaced by tags.", "labels": [], "entities": []}, {"text": "Morphological model (B+M) This model extends the B model by adding morphological features.", "labels": [], "entities": []}, {"text": "This model aims to approximate the intuition that morphological features such as case are key for identifying the structure of free word order languages.", "labels": [], "entities": [{"text": "identifying the structure of free word order languages", "start_pos": 98, "end_pos": 152, "type": "TASK", "confidence": 0.6792643927037716}]}, {"text": "As feature engineering may become in principle quite complex once it comes to morphology, we targeted fairly crude models with the goal of providing a proof of concept.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.8372287452220917}]}, {"text": "Therefore the morphologically informed models use as input a rich set of morphological features specified in predicted by the CRF tagger) with the same jackkniffing as before.", "labels": [], "entities": []}, {"text": "The content of provides an explicit indication of the actual features defined in the original treebanks (see and references therein for details), while the columns are indicative normalized names.", "labels": [], "entities": []}, {"text": "For Basque most of the additional morphological features further encode case and verbal subcategorization.", "labels": [], "entities": []}, {"text": "For French the mwe field abbreviates IOB predicted tags derived from multi-word expression annotations found in the original dataset.", "labels": [], "entities": []}, {"text": "Now let M be the set of values enumerated fora language in, we systematically added the following templates to model B: Essentially the model expresses interactions between morphological features from the constituent heads on the top of the stack and the morphological features from the tokens at the beginning of the queue.", "labels": [], "entities": []}, {"text": "Mixed model (B+S+M) Our last model is the union of the span model (B+S) and the morphological model (B+M).", "labels": [], "entities": []}, {"text": "Results (development) We measured the impact of the model variations on the development set for c-parsing on the SPMRL data sets).", "labels": [], "entities": [{"text": "SPMRL data sets", "start_pos": 113, "end_pos": 128, "type": "DATASET", "confidence": 0.8810640970865885}]}, {"text": "We immediately observe that modelling spans tends to improve the results, in particular for languages where the head annotation is more problematic: Arabic 4 , Basque, German and Hungarian and also Swedish however.", "labels": [], "entities": []}, {"text": "So the span-based model seems to improve the parser's robustness in cases when dependencies lack precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9907387495040894}]}, {"text": "For this model, the average behaviour is similar to that of although the variance is high.", "labels": [], "entities": [{"text": "variance", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9380017518997192}]}, {"text": "On the other hand, the morphological model tends to be most important for languages where head annotation is easier: French, Korean, Polish and Swedish.", "labels": [], "entities": [{"text": "head annotation", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.7485037446022034}]}, {"text": "It is key for very richly inflected languages such as Basque and Hungarian even though our head annotation is more approximative   Results (test) We observe in that our joint B+S+M model yields a state of the art cparser on almost all languages considered 6 . It is quite clear that both our span and morphology enhanced models could be dramatically improved, but it shows that with reasonable feature engineering, these two sub-models are largely sufficient to improve the state of the art in c-parsing for these languages over strong baselines.", "labels": [], "entities": []}, {"text": "Although in principle the Berkeley parsers () are designed to be language-generic with an underlying design that is surprisingly accurate for free word order languages end up suffering from alack of sensitivity to morphological information.", "labels": [], "entities": []}, {"text": "Finally we also observe that our phrase structure parser clearly outperforms the TurboParser setup described by in which an elaborate output conversion procedure generates c-parses from d-parses.", "labels": [], "entities": [{"text": "phrase structure parser", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.6884519656499227}]}, {"text": "Comparison with related work We conclude with a few comparisons with related work.", "labels": [], "entities": []}, {"text": "This will enable us to show that our approach is not only accurate but also efficient.", "labels": [], "entities": []}, {"text": "A comparison with dependency parsers will also allow us to better identify the properties of our proposal.", "labels": [], "entities": []}, {"text": "In order to test efficiency, we compared our parser to c-parsers trained on Penn Treebank (PTB) for which we have running times reported by.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 76, "end_pos": 95, "type": "DATASET", "confidence": 0.9755910634994507}]}, {"text": "This required first assigning heads, for which we used the Stanford tool for converting PTB to Basic Dependencies, and then used our Robust conversion method.", "labels": [], "entities": [{"text": "PTB", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.7784490585327148}, {"text": "Robust conversion", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.7055346816778183}]}, {"text": "We performed a simple test using the PTB standard split with the same experimental setting as before, except that we use the standard evalb scorer (   Best ensemble is the best semi-supervised or ensemble system from either SPMRL 13 or SPMRL 14).", "labels": [], "entities": [{"text": "PTB standard split", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.9777032732963562}, {"text": "SPMRL 13 or SPMRL 14", "start_pos": 224, "end_pos": 244, "type": "DATASET", "confidence": 0.7469445824623108}]}, {"text": "simply reading them off from the lexicalized cstructure.", "labels": [], "entities": []}, {"text": "We report in the UAS evaluation of those dependencies and we compare them to the best results obtained by dependency parsers in both SPMRL13 and SPMRL14 shared tasks.", "labels": [], "entities": [{"text": "UAS", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.6991825699806213}]}, {"text": "For each language, the comparison is made with the best single dependency parsing system 8 . For English we compare against Standard TurboParser -which seems to be the most similar to our system-when parsing to Basic Stanford dependencies.", "labels": [], "entities": []}, {"text": "The comparison with semi-supervised and ensemble parsers still provides a reasonable upperline).", "labels": [], "entities": []}, {"text": "As can be seen in, our results partly generalize the observation summarized by and that phrase structure parsers tend to provide better dependencies than genuine dependency parsers for parsing to Stanford Dependencies.", "labels": [], "entities": [{"text": "phrase structure parsers", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.7039750317732493}]}, {"text": "For English, our UAS is similar to that of TurboParser, but in a broader multilingual framework, the left side of the table shows that the unlabeled dependencies are clearly better than those of genuine dependency parsers.", "labels": [], "entities": []}, {"text": "On the right side of the table are languages for which our dependencies are actually worse.", "labels": [], "entities": []}, {"text": "This is not a surprise, since these are also the languages for which head annotation was more problematic in the first place.", "labels": [], "entities": [{"text": "head annotation", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.8838963508605957}]}, {"text": "This last observation suggests that a lexicalized c-parser can also provide very accurate dependencies.", "labels": [], "entities": []}, {"text": "A way to further gen-eralize this observation to problematic languages would be either to design a less immediate postprocessing conversion scheme or to further normalize the data set to obtain the correct heads from the outset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Quantification of the conversion", "labels": [], "entities": [{"text": "conversion", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.787150502204895}]}, {"text": " Table 5: Penn treebank test (WSJ 23)", "labels": [], "entities": [{"text": "Penn treebank test", "start_pos": 10, "end_pos": 28, "type": "DATASET", "confidence": 0.9422983328501383}, {"text": "WSJ 23", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.8297938704490662}]}, {"text": " Table 3: Multilingual test (F-scores, phrase structure parsing)", "labels": [], "entities": [{"text": "phrase structure parsing", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.7575269540150961}]}, {"text": " Table 4: Multilingual test (UAS, dependency parsing)", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7350218147039413}]}]}