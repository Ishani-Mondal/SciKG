{"title": [{"text": "Comparing Word Representations for Implicit Discourse Relation Classification", "labels": [], "entities": [{"text": "Comparing Word Representations", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6667525072892507}, {"text": "Implicit Discourse Relation", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.7213353315989176}]}], "abstractContent": [{"text": "This paper presents a detailed comparative framework for assessing the usefulness of unsupervised word representations for identifying so-called implicit discourse relations.", "labels": [], "entities": []}, {"text": "Specifically, we compare standard one-hot word pair representations against low-dimensional ones based on Brown clusters and word embeddings.", "labels": [], "entities": []}, {"text": "We also consider various word vector combination schemes for deriving discourse segment representations from word vectors, and compare representations based either on all words or limited to head words.", "labels": [], "entities": []}, {"text": "Our main finding is that denser representations systematically outperform sparser ones and give state-of-the-art performance or above without the need for additional hand-crafted features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Identifying discourse relations is an important task, either to build a discourse parser or to help other NLP systems such as text summarization or question-answering.", "labels": [], "entities": [{"text": "Identifying discourse relations", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8735352357228597}, {"text": "text summarization", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7336311638355255}]}, {"text": "This task is relatively straightforward when a discourse connective, such as but or because, is used . The identification becomes much more challenging when such an overt marker is lacking, and the relation needs to be inferred through other means.", "labels": [], "entities": []}, {"text": "In (1), the presence of the pair of verbs (rose,tumbled) triggers a Contrast relation.", "labels": [], "entities": []}, {"text": "Such relations are extremely pervasive in real text corpora: they account for about 50% of all relations in the Penn Discourse Treebank ( Automatically classifying implicit relations is difficult in large part because it relies on numerous factors, ranging from syntax, and tense and aspect, to lexical semantics and even world knowledge.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 112, "end_pos": 135, "type": "DATASET", "confidence": 0.9808150132497152}, {"text": "Automatically classifying implicit relations", "start_pos": 138, "end_pos": 182, "type": "TASK", "confidence": 0.7748250812292099}]}, {"text": "Consequently, a lot of previous work on this problem have attempted to incorporate some of these information into their systems.", "labels": [], "entities": []}, {"text": "These assume the existence of syntactic parsers and lexical databases of various kinds, which are available but fora few languages, and they often involve heavy feature engineering (.", "labels": [], "entities": []}, {"text": "While acknowledging this knowledge bottleneck, this paper focuses on trying to predict implicit relations based on easily accessible lexical features, targeting in particular simple word-based features, such as pairs like (rose,tumbled) in (1).", "labels": [], "entities": []}, {"text": "Most previous studies on implicit relations, going back to (), incorporate word-based information in the form of word pair features defined across the pair of text segments to be related.", "labels": [], "entities": []}, {"text": "Such word pairs are often encoded in a one-hot representation, in which each possible word pair corresponds to a single component of a very highdimensional vector.", "labels": [], "entities": []}, {"text": "From a machine learning perspective, this type of sparse representation makes parameter estimation extremely difficult and prone to overfitting.", "labels": [], "entities": []}, {"text": "It also makes it difficult to achieve any interesting semantic generalization.", "labels": [], "entities": [{"text": "semantic generalization", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.6909267008304596}]}, {"text": "To see this, consider the distance (e.g., Euclidean or cosine) induced by such representation.", "labels": [], "entities": []}, {"text": "Assuming for simplicity that one characterizes each pair of discourse segments via their main verbs, the corresponding one-hot encoding for the pair (rose,tumbled) would beat equal distance from the synonymic pair (went up,lost) and the antonymic pair (went down,gained), as all three vectors are orthogonal to each others.", "labels": [], "entities": []}, {"text": "Various attempts have been made at reducing sparsity of lexical features.", "labels": [], "entities": []}, {"text": "Recently, Ruther-ford and proposed to use Brown clusters ( for this task, in effect replacing each token by its cluster binary code.", "labels": [], "entities": []}, {"text": "These authors conclude that these denser, clusterderived representations significantly improve the identification of implicit discourse relations and report the best performance to date using also additional features.", "labels": [], "entities": [{"text": "identification of implicit discourse relations", "start_pos": 99, "end_pos": 145, "type": "TASK", "confidence": 0.7943374872207641}]}, {"text": "Unfortunately, their claim is somewhat weakened by the fact that they fail to compare the use of their cluster word pairs against other types of word representations, including one-hot encodings of word pairs or other low-dimensional word representations.", "labels": [], "entities": []}, {"text": "This work also leaves other important questions open.", "labels": [], "entities": []}, {"text": "In particular, it is unclear whether all word pairs constructed over the two discourse segments are truly informative and should be included in the model.", "labels": [], "entities": []}, {"text": "Given that word embeddings capture latent syntactic and semantic information, yet another important question is to which extent the use of these representations dispenses us from using additional hand-crafted syntactic and semantic features.", "labels": [], "entities": []}, {"text": "This paper fills these gaps and significantly extends the work of by explicitly comparing various types of word representations and vector composition methods.", "labels": [], "entities": []}, {"text": "Specifically, we investigate three well-known word embeddings, namely Collobert and Weston, hierarchical log-bilinear model and Hellinger Principal Component Analysis (, in addition to Brown clusterbased and standard one-hot representations.", "labels": [], "entities": []}, {"text": "All these word representations are publicly available for English and can be easily acquired for other languages just using raw text data, thus alleviating the need for hand-crafted lexical databases.", "labels": [], "entities": []}, {"text": "This makes our approach easily extendable to resource-poor languages.", "labels": [], "entities": []}, {"text": "In addition, we also investigate the issue of which specific words need to be fed to the model, by comparing using just pairs of verbs against all pairs of words, and how word representations should be combined over discourse segments, comparing component-wise product against simple vector concatenation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Through the comparative framework described in section 3, our objective is to assess the usefulness of different vectorial representations for pairs of discourse segments.", "labels": [], "entities": []}, {"text": "Specifically, we want to establish whether dense representations are better than sparse ones, and whether certain word pairs are more relevant than others, which resource and which combination schemes are more adapted to the task, and, finally, whether standard features derived from external databases are still relevant in the presence of dense representations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word embeddings and Brown clusters  lexicon coverage.", "labels": [], "entities": []}, {"text": " Table 2: Number of examples in train, dev, test.", "labels": [], "entities": []}, {"text": " Table 3: F1 score for systems using all words and only heads for Temporal (Temp.), Contingency (Cont.),  Comparison (Compa.) and Expansion (Exp.).  *  p \u2264 0.1,  *  *  p \u2264 0.05 compared to One-hot \u2297 with t-test  and Wilcoxon ; for head words, all the improvements observed against One-hot \u2297 are significant.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.977069616317749}, {"text": "Expansion", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9549980163574219}]}, {"text": " Table 4: Systems using additional features (\"+ add.features\"), state-of-the art results either reported  or reproduced (\"repr.\") using Naive Bayes (\"NB\") or logistic regression (\"ME\") and best system from  previous table (\"only\").", "labels": [], "entities": []}]}