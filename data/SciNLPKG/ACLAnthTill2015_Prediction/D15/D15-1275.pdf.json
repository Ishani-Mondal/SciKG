{"title": [], "abstractContent": [{"text": "In this paper, we describe a method based on statistical machine translation (SMT) that is able to restore accents in Hungarian texts with high accuracy.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 45, "end_pos": 82, "type": "TASK", "confidence": 0.7510625521341959}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9902060627937317}]}, {"text": "Due to the ag-glutination in Hungarian, there are always plenty of word forms unknown to a system trained on a fixed vocabulary.", "labels": [], "entities": []}, {"text": "In order to be able to handle such words, we integrated a morphological analyzer into the system that can suggest accented word candidates for unknown words.", "labels": [], "entities": []}, {"text": "We evaluated the system in different setups, achieving an accuracy above 99% at the highest.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9996256828308105}]}], "introductionContent": [{"text": "Due to clumsy mobile device interfaces and reluctance of users to spend too much time entering their message, a great amount of text is generated in a format that lacks the diacritic marks normally used in the orthography of the language the text is written in.", "labels": [], "entities": []}, {"text": "Whatever the causes for the missing accents are, NLP applications should be able to restore or generate the accented version of such texts prior to any further syntactic or semantic processing to avoid upstream errors.", "labels": [], "entities": []}, {"text": "In this paper, we aim at solving the problem of restoring accents in Hungarian texts with the combined application of a statistical machine translation system and a morphological analyzer.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.6342551509539286}]}, {"text": "Our method can be applied to any other languages that have an accurate morphological analyzer.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, the Hungarian webcorpus) was used for training and testing purposes.", "labels": [], "entities": [{"text": "Hungarian webcorpus", "start_pos": 24, "end_pos": 43, "type": "DATASET", "confidence": 0.9333932101726532}]}, {"text": "A set of 100 000 sentences were separated from the corpus as the test set, and another 100 000 sentences were used as a development set.", "labels": [], "entities": []}, {"text": "The rest were used for training in different settings.", "labels": [], "entities": []}, {"text": "The size of each training set is shown in.", "labels": [], "entities": []}, {"text": "We evaluated the performance on all the 1 804 252 tokens of the test set (56.84% correct without accent) and on a subset of 1 472 200 words that included any vowels (47.09% correct without accent).", "labels": [], "entities": []}, {"text": "The experiments were then performed for the baseline system using the most frequent form (BL-FREQ), for the baseline SMT system (BL-SMT) and for the one augmented by the morphology with the first-ranked candidate (RANK).", "labels": [], "entities": [{"text": "BL-FREQ", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9589956998825073}, {"text": "SMT", "start_pos": 117, "end_pos": 120, "type": "TASK", "confidence": 0.9412772059440613}, {"text": "BL-SMT", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.8017865419387817}]}, {"text": "shows the detailed results for the smallest and largest training sets for all words (ALL) and for words that include vowels (VOWEL).", "labels": [], "entities": [{"text": "VOWEL", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.9100704193115234}]}, {"text": "It can be seen that the precision of the system is only  slightly improved when increasing the size of the training corpus, but the values of recall and accuracy do dramatically improve in the case of the baseline system.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9994170665740967}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9996753931045532}, {"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9980490207672119}]}, {"text": "However, the integration of the suggestions of the morphology can makeup for the lack of information due to the small training set improving recall a great deal while only slightly affecting precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9983887672424316}, {"text": "precision", "start_pos": 191, "end_pos": 200, "type": "METRIC", "confidence": 0.9975298047065735}]}, {"text": "Even for the biggest 437.6M-word training corpus, incorporating the morphological analyzer with ranking yielded a relative error rate reduction of 39.74%, reducing the word error rate from 1.56% to 0.94%.", "labels": [], "entities": [{"text": "relative error rate reduction", "start_pos": 114, "end_pos": 143, "type": "METRIC", "confidence": 0.7919290363788605}, {"text": "word error rate", "start_pos": 168, "end_pos": 183, "type": "METRIC", "confidence": 0.7337529957294464}]}, {"text": "For the smallest 1.74M-word training corpus tested, the relative error rate reduction was 85.85%.", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 65, "end_pos": 85, "type": "METRIC", "confidence": 0.9568511446317037}]}, {"text": "The system including the morphological analyzer performs better even with the smallest training corpus in terms of word accuracy than the baseline Moses system with the biggest corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9196374416351318}]}, {"text": "shows the learning curves for each system with accuracy as a function of training set size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.999421238899231}]}, {"text": "Comparing our results to those we obtained using Charlifter (89.75% with most frequent accented form baseline, 90.00% with the lexiconlookup+bigram contextual model and 93.31% with lookup+bigram context+character-n-grambased model), the results reveal that both the contextual model in the SMT system improves accuracy better than the bigram context model of Charlifter, and the performance boost we get by incorporating morphology vastly exceeds the accuracy improvement yielded by the incorporation of the character-n-gram-based model used in Charlifter.", "labels": [], "entities": [{"text": "SMT", "start_pos": 290, "end_pos": 293, "type": "TASK", "confidence": 0.9860092997550964}, {"text": "accuracy", "start_pos": 310, "end_pos": 318, "type": "METRIC", "confidence": 0.9977046847343445}, {"text": "accuracy", "start_pos": 451, "end_pos": 459, "type": "METRIC", "confidence": 0.9986333250999451}]}], "tableCaptions": [{"text": " Table 1: Possible accent variations in Hungarian", "labels": [], "entities": []}, {"text": " Table 2: Ratio of OOV after building a translation  model from a training set of a certain size", "labels": [], "entities": [{"text": "OOV", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.7916488647460938}]}, {"text": " Table 3: Performance results for each experimen- tal settings and training size", "labels": [], "entities": []}]}