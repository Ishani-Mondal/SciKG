{"title": [{"text": "Sentence Compression by Deletion with LSTMs", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8913813233375549}, {"text": "LSTMs", "start_pos": 38, "end_pos": 43, "type": "TASK", "confidence": 0.39716193079948425}]}], "abstractContent": [{"text": "We present an LSTM approach to deletion-based sentence compression where the task is to translate a sentence into a sequence of zeros and ones, corresponding to token deletion decisions.", "labels": [], "entities": [{"text": "deletion-based sentence compression", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.6435424784819285}]}, {"text": "We demonstrate that even the most basic version of the system, which is given no syntactic information (no PoS or NE tags, or dependencies) or desired compression length, performs surprisingly well: around 30% of the compressions from a large test set could be regenerated.", "labels": [], "entities": []}, {"text": "We compare the LSTM system with a competitive baseline which is trained on the same amount of data but is additionally provided with all kinds of linguistic features.", "labels": [], "entities": []}, {"text": "In an experiment with human raters the LSTM-based model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9258274734020233}]}, {"text": "Dozens of systems have been introduced in the past two decades and most of them are deletion-based: generated compressions are token subsequences of the input sentences, to name a few).", "labels": [], "entities": []}, {"text": "Existing compression systems heavily use syntactic information to minimize chances of introducing grammatical mistakes in the output.", "labels": [], "entities": []}, {"text": "A common approach is to use only some syntactic information among others) or use syntactic features as signals in a statistical model).", "labels": [], "entities": []}, {"text": "It is probably even more common to operate on syntactic trees directly (dependency or constituency) and generate compressions by pruning them.", "labels": [], "entities": []}, {"text": "Unfortunately, this makes such systems vulnerable to error propagation as there is noway to recover from an incorrect parse tree.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7051220238208771}]}, {"text": "With the state-of-the-art parsing systems achieving about 91 points in labeled attachment accuracy), the problem is not a negligible one.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9569923281669617}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.6001917719841003}]}, {"text": "To our knowledge, there is no competitive compression system so far which does not require any linguistic preprocessing but tokenization.", "labels": [], "entities": []}, {"text": "In this paper we research the following question: can a robust compression model be built which only uses tokens and has no access to syntactic or other linguistic information?", "labels": [], "entities": []}, {"text": "While phenomena like long-distance relations may seem to make generation of grammatically correct compressions impossible, we are going to present an evidence to the contrary.", "labels": [], "entities": [{"text": "generation of grammatically correct compressions", "start_pos": 62, "end_pos": 110, "type": "TASK", "confidence": 0.810182797908783}]}, {"text": "In particular, we will present a model which benefits from the very recent advances in deep learning and uses word embeddings and Long Short Term Memory models (LSTMs) to output surprisingly readable and informative compressions.", "labels": [], "entities": []}, {"text": "Trained on a corpus of less than two million automatically extracted parallel sentences and using a standard tool to obtain word embeddings, in its best and most simple configuration it achieves 4.5 points out of 5 in readability and 3.8 points in informativeness in an extensive evaluation with human judges.", "labels": [], "entities": []}, {"text": "We believe that this is an important result as it may suggest anew direction for sentence compression research which is less tied to modeling linguistic structures, especially syntactic ones, than the compression work so far.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.7980659902095795}]}, {"text": "The paper is organized as follows: Section 3 presents a competitive baseline which implements the system of McDonald (2006) for large training sets.", "labels": [], "entities": []}, {"text": "The LSTM model and its three configurations are introduced in Section 4.", "labels": [], "entities": []}, {"text": "The evaluation set-up and a discussion on wins and losses with examples are presented in Section 5 which is followed by the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the baseline and our systems on the 200-sentence test set in an experiment with human raters.", "labels": [], "entities": []}, {"text": "The raters were asked to rate readability and informativeness of compressions given the input which are the standard evaluation metrics for compression.", "labels": [], "entities": []}, {"text": "The former covers the grammatical correctness, comprehensibility and fluency of the output while the latter measures the amount of important content preserved in the compression.", "labels": [], "entities": []}, {"text": "Additionally, for experiments on the development set, we used two metrics for automatic evaluation: per-sentence accuracy (i.e., how many compressions could be fully reproduced) and word-based F1-score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.8473300337791443}, {"text": "F1-score", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.9129074811935425}]}, {"text": "The latter differs from the RASP-based relation F-score by in that we simply compute the recall and precision in terms of tokens kept in the golden and the generated compressions.", "labels": [], "entities": [{"text": "RASP-based relation F-score", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.6817223032315572}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9991718530654907}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9182787537574768}]}, {"text": "We report these results for completeness although it is the results of the human evaluation from which we draw our conclusions.", "labels": [], "entities": []}, {"text": "Compression ratio: The three versions of our system (LSTM*) and the baseline (MIRA) have comparable compression ratios (CR) which are defined as the length of the compression in characters divided over the sentence length.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.8712390661239624}, {"text": "compression ratios (CR)", "start_pos": 100, "end_pos": 123, "type": "METRIC", "confidence": 0.8491453766822815}]}, {"text": "Since the ratios are very close, a comparison of the systems' scores is justified).", "labels": [], "entities": []}, {"text": "Automatic evaluation: A total of 1,000 sentence pairs from the test set were used in the automatic evaluation.", "labels": [], "entities": []}, {"text": "The results are summarized in.: F1-score, per-sentence accuracy and compression ratio for the baseline and the systems There is a significant difference in performance of the MIRA baseline and the LSTM models, both in terms of F1-score and inaccuracy.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9990307092666626}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9626615643501282}, {"text": "compression ratio", "start_pos": 68, "end_pos": 85, "type": "METRIC", "confidence": 0.9319631457328796}, {"text": "MIRA baseline", "start_pos": 175, "end_pos": 188, "type": "DATASET", "confidence": 0.9267496168613434}, {"text": "F1-score", "start_pos": 227, "end_pos": 235, "type": "METRIC", "confidence": 0.9947521686553955}]}, {"text": "More than 30% of golden compressions could be fully regenerated by the LSTM systems which is in sharp contrast with the 20% of MIRA.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 127, "end_pos": 131, "type": "DATASET", "confidence": 0.7043768763542175}]}, {"text": "The differences in F-score between the three versions of LSTM are not significant, all scores are close to 0.81.", "labels": [], "entities": [{"text": "F-score", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9991244673728943}, {"text": "LSTM", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.6847847700119019}]}, {"text": "Evaluation with humans: The first 200 sentences from the set of 1,000 used in the automatic evaluation were compressed by each of the four systems.", "labels": [], "entities": []}, {"text": "Every sentence-compression pair was rated by three raters who were asked to select a rating on a five-point Likert scale, ranging from one to five.", "labels": [], "entities": []}, {"text": "In very few cases (around 1%) the ratings were inconclusive (i.e., 1, 3, 5 were given to the same pair) and had to be skipped.", "labels": [], "entities": []}, {"text": "the amount of parallel data we had at our disposal.", "labels": [], "entities": []}, {"text": "The simple LSTM model which only uses token embeddings to generate a sequence of deletion decisions significantly outperforms the baseline which was given not only embeddings but also syntactic and other features.", "labels": [], "entities": []}, {"text": "Discussion: What are the wins and losses of the LSTM systems?", "labels": [], "entities": []}, {"text": "presents some of the evaluated sentence-compression pairs.", "labels": [], "entities": []}, {"text": "In terms of readability, the basic LSTM system performed surprisingly well.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 35, "end_pos": 39, "type": "TASK", "confidence": 0.8848628997802734}]}, {"text": "Only in a few cases (out of 200) did it get an average score of two or three.", "labels": [], "entities": [{"text": "average score", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.9055978357791901}]}, {"text": "Sentences which pose difficulty to the model are the ones with quotes, intervening commas, or other uncommon punctuation patterns.", "labels": [], "entities": []}, {"text": "For example, in the second sentence in, if one removes from the input the age modifiers and the preceding commas, the words and Chris Martin are not dropped and the output compression is grammatical, preserving both conjoined elements.", "labels": [], "entities": []}, {"text": "With regard to informativeness, the difficult cases are those where there is very little to be removed and where the model still removed more than a half to achieve the compression ratio it observed in the training data.", "labels": [], "entities": []}, {"text": "For example, the only part that can be removed from the fourth sentence in is the modifier of police, everything else being important content.", "labels": [], "entities": []}, {"text": "Similarly, in the fifth sentence the context of the event must be retained in the compression for the event to be interpreted correctly.", "labels": [], "entities": []}, {"text": "Arguably, such cases would also be difficult for other systems.", "labels": [], "entities": []}, {"text": "In particular, recognizing when the context is crucial is a problem that can be solved only by including deep semantic and discourse features which has not been attempted yet.", "labels": [], "entities": []}, {"text": "And sentences with quotes (direct speech, a song or a book title, etc.) are challenging for parsers which in turn provide important signals for most compression systems.", "labels": [], "entities": []}, {"text": "The bottom of contains examples of good compressions.", "labels": [], "entities": []}, {"text": "Even though fora significant number of input sentences the compression was a continuous subsequence of tokens, there are many discontinuous compressions.", "labels": [], "entities": []}, {"text": "In particular, the LSTM model learned to drop appositions, no matter how long they are, temporal expressions, optional modifiers, introductory clauses, etc.", "labels": [], "entities": []}, {"text": "Our understanding of why the extended model (LSTM+PAR+PRES) performed worse in the human evlauation than the base model is that, in the absence of syntactic features, the basic LSTM learned a model of syntax useful for compression, while LSTM++, which was given syntactic information, learned to optimize for the particular way the \"golden\" set was created (tree pruning).", "labels": [], "entities": [{"text": "PAR+PRES", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.7192267775535583}]}, {"text": "While the automatic evaluation penalized all deviations from the single golden variant, inhuman evals there was no penalty for readable alternatives.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1-score, per-sentence accuracy and  compression ratio for the baseline and the systems", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999458372592926}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9751806259155273}, {"text": "compression ratio", "start_pos": 47, "end_pos": 64, "type": "METRIC", "confidence": 0.956062912940979}]}, {"text": " Table 2: Readability and informativeness for the  baseline and the systems:  \u2020 stands for significantly  better than MIRA with 0.95 confidence.", "labels": [], "entities": [{"text": "Readability", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9313150644302368}, {"text": "\u2020", "start_pos": 78, "end_pos": 79, "type": "METRIC", "confidence": 0.9789304733276367}, {"text": "MIRA", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9656515121459961}]}]}