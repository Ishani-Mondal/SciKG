{"title": [{"text": "Show Me Your Evidence -an Automatic Method for Context Dependent Evidence Detection", "labels": [], "entities": [{"text": "Context Dependent Evidence Detection", "start_pos": 47, "end_pos": 83, "type": "TASK", "confidence": 0.8769680708646774}]}], "abstractContent": [{"text": "Engaging in a debate with oneself or others to take decisions is an integral part of our day-today life.", "labels": [], "entities": []}, {"text": "A debate on a topic (say, use of performance enhancing drugs) typically proceeds by one party making an assertion/claim (say, PEDs are bad for health) and then providing an evidence to support the claim (say, a 2006 study shows that PEDs have psychiatric side effects).", "labels": [], "entities": []}, {"text": "In this work, we propose the task of automatically detecting such evidences from unstructured text that support a given claim.", "labels": [], "entities": []}, {"text": "This task has many practical applications in decision support and persuasion enhancement in a wide range of domains.", "labels": [], "entities": [{"text": "decision support", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.952804297208786}, {"text": "persuasion enhancement", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.9285851716995239}]}, {"text": "We first introduce an extensive benchmark data set tailored for this task, which allows training statistical models and assessing their performance.", "labels": [], "entities": []}, {"text": "Then, we suggest a system architecture based on supervised learning to address the evidence detection task.", "labels": [], "entities": [{"text": "evidence detection task", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.8728126883506775}]}, {"text": "Finally, promising experimental results are reported.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years there has been a growing interest in the area of argumentation mining (.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.9882461130619049}]}, {"text": "Part of this awakening is the The Debater TM project 1 whose goal is to develop technologies that will assist humans to debate and reason, e.g., by automatically suggesting arguments relevant to an examined topic.", "labels": [], "entities": []}, {"text": "The minimal definition of such an argument) is a set of statements, made up of three parts -a claim (aka conclusion, proposition), a set of evidence (aka premises), and an inference from the evidence to the claim.", "labels": [], "entities": []}, {"text": "Needless to say, evidence plays a critical role in a persuasive argument.", "labels": [], "entities": []}, {"text": "In most debate related skills, such as natural language understanding and generation, humans currently have an inherent advantage over a machine.", "labels": [], "entities": [{"text": "natural language understanding and generation", "start_pos": 39, "end_pos": 84, "type": "TASK", "confidence": 0.7141180992126465}]}, {"text": "However, in the ability to provide high quality and diverse evidence, machines have a very promising potential, being able to swiftly process large quantities of information.", "labels": [], "entities": []}, {"text": "Nonetheless, since most of the relevant information is represented by unstructured text, successfully exploiting these resources requires the ability to identify evidence in free text.", "labels": [], "entities": []}, {"text": "This is exactly the focus of our work.", "labels": [], "entities": []}, {"text": "Specifically, we formally define the task of evidence detection, introduce an architecture for attacking this problem, and demonstrate its performance over dedicated manually labeled data.", "labels": [], "entities": [{"text": "evidence detection", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.9057337939739227}]}, {"text": "Before defining the task formally, we introduce three concepts which will be used throughout this paper.", "labels": [], "entities": []}, {"text": "These concepts were earlier defined in ( ) and we use the same definitions here.", "labels": [], "entities": []}, {"text": "Topic: a short phrase that frames the discussion.", "labels": [], "entities": []}, {"text": "Claim: a general, concise statement that directly supports or contests the topic.", "labels": [], "entities": [{"text": "Claim", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9255547523498535}]}, {"text": "Context Dependent Evidence (CDE): a text segment that directly supports a claim in the context of the topic.", "labels": [], "entities": [{"text": "Context Dependent Evidence (CDE)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5866855929295222}]}, {"text": "The first three rows of show examples of a topic, a claim and CDE.", "labels": [], "entities": [{"text": "CDE", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.984915018081665}]}, {"text": "For the purpose of this work, we assume that we are given a concrete topic, a relevant claim, and potentially relevant documents, provided either manually or by automatic methods.", "labels": [], "entities": []}, {"text": "Our task, which we term Context Dependent Evidence Detection (CDED), is to automatically pinpoint CDE within these documents.", "labels": [], "entities": [{"text": "Context Dependent Evidence Detection (CDED)", "start_pos": 24, "end_pos": 67, "type": "TASK", "confidence": 0.727753221988678}]}, {"text": "We further require that a detected CDE is reasonably well phrased, and easily understandable in the given context, so that it can be instantly and naturally used to support the claim in a discussion.", "labels": [], "entities": []}, {"text": "gives examples of valid CDE (V) and non-valid CDE (X) according to the definition mentioned above.", "labels": [], "entities": []}, {"text": "It is well recognized that one can support a claim using different types of evidence (.", "labels": [], "entities": []}, {"text": "Furthermore, for different use cases, different evidence types could be more suitable.", "labels": [], "entities": []}, {"text": "Correspondingly, we develop a classification approach that is able to identify and distinguish between three common evidence types (): \u2022 Study Results of a quantitative analysis of data, given as numbers, or as conclusions.);: Examples for defined concepts.", "labels": [], "entities": []}, {"text": "The V/X indicates if the candidate is a CDE to the claim above it, according to our definition.", "labels": [], "entities": [{"text": "V/X", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9484834671020508}]}, {"text": "\u2022 Expert Testimony by a person / group / committee / organization with some known expertise / authority on the topic., S7); \u2022 Anecdotal A description of an episode(s), centered on individual(s) or clearly located in place and/or in time.); Examining the valid and non-valid CDEs in it should be clear that the distinction between them is often quite subtle.", "labels": [], "entities": []}, {"text": "For example, it is possible that apiece of text has the characteristics of a certain evidence type, but does not support the claim (see S4 in).", "labels": [], "entities": []}, {"text": "It is also possible that apiece of text supports the claim, but is irrelevant in the context of the topic (see S5 in).", "labels": [], "entities": []}, {"text": "It could also be the case that apiece of text entails the claim, but adds no new information to support it (see S6 in.", "labels": [], "entities": []}, {"text": "We present here a pipeline architecture, relying on supervised learning, to handle the different aspects of CDED which shows promising results over a variety of topics.", "labels": [], "entities": []}, {"text": "We demonstrate that the proposed solution and features can generalize well, namely that models learned over different topics can perform reasonably well on an entirely new topic.", "labels": [], "entities": []}, {"text": "On average, fora significant fraction of claims the proposed system succeeds to propose relevant CDE amongst its top 4 predictions, and properly determines the evidence type.", "labels": [], "entities": []}, {"text": "Furthermore we show that we are able to automatically pinpoint claims for which the performance of the system are of even greater quality, enabling the user to obtain higher precision for these claims.", "labels": [], "entities": [{"text": "precision", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9980148077011108}]}, {"text": "We believe that the ability to automatically provide evidence forgiven claims will have many practical uses, helping layman and professionals in different domains, to reach decisions and prepare for discussions, from a lawyer presenting a casein court, to a politician considering anew policy.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our approach using the Leave-One-Out schema: for every topic, we trained the classifiers using the claims and associated CDE in all other topics and then applied the resulting models to the left out topic.", "labels": [], "entities": []}, {"text": "In general, we consider a candidate as true-positive if it includes all sentences included in the CDE and no additional sentences.", "labels": [], "entities": []}, {"text": "However, for our analysis it is also interesting to separate between (i) errors in selecting the segment boundaries and (ii) errors of down the line components that are affected by these errors.", "labels": [], "entities": []}, {"text": "Thus, we also include the overlap measure where we consider a candidate as true-positive if at least one sentence within it overlaps a sentence in a labeled CDE.", "labels": [], "entities": [{"text": "overlap measure", "start_pos": 26, "end_pos": 41, "type": "METRIC", "confidence": 0.9039227962493896}]}, {"text": "Our final assessment measure is the mean reciprocal rank (MRR), that is the inverse of the rank of the first CDE detected fora particular claim, averaged overall claims selected by the claim selection component.", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 36, "end_pos": 62, "type": "METRIC", "confidence": 0.8920571406682333}]}, {"text": "This is motivated by the observation that inmost practical use cases, it is usually more important to be able to support many claims, than to provide all the CDE available fora single claim.", "labels": [], "entities": []}, {"text": "We define the MRR of a claim with no CDE (errors of the claim selection component) to be 0.", "labels": [], "entities": [{"text": "MRR", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.9827255010604858}, {"text": "CDE", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9577481746673584}]}, {"text": "Finally, we report the macro-averaged results over the different topics, that is all topics have the same weight regardless the amount of labeled claims and labeled CDE detected for them.", "labels": [], "entities": []}, {"text": "The rational behind this is that we wish to ensure that our system does reasonably well across all topics examined.", "labels": [], "entities": []}, {"text": "We note that micro-averaging gave overall similar results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: 'Topics' indicate the number of topics included for each CDE type. This determines the number of claims  considered for each type. The next columns indicate the number of articles in which at least one CDE was found;  the total number of CDE detected for each type; the average percent of claims for which at least one CDE was  found; and for these claims, the average number of CDE found. Note that the total number of CDE is not a simple  sum of the CDE per type, as CDE can be assigned with more than one type. Standard deviations of distribution  across topics are given in parenthesis where relevant.", "labels": [], "entities": []}, {"text": " Table 3: Macro-averaged MRR for each CDE type. Only claims with CDE in the labeled data were considered in  these results.", "labels": [], "entities": [{"text": "MRR", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.8214695453643799}]}]}