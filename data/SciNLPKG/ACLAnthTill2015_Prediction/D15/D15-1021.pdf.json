{"title": [{"text": "A Survey of Current Datasets for Vision and Language Research", "labels": [], "entities": [{"text": "Vision and Language Research", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.5960862562060356}]}], "abstractContent": [{"text": "Integrating vision and language has long been a dream in work on artificial intelligence (AI).", "labels": [], "entities": []}, {"text": "In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond.", "labels": [], "entities": []}, {"text": "The available corpora have played a crucial role in advancing this area of research.", "labels": [], "entities": []}, {"text": "In this paper, we propose a set of quality met-rics for evaluating and analyzing the vision & language datasets and categorize them accordingly.", "labels": [], "entities": []}, {"text": "Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bringing together language and vision in one intelligent system has long been an ambition in AI research, beginning with SHRDLU as one of the first vision-language integration systems and continuing with more recent attempts on conversational robots grounded in the visual world ().", "labels": [], "entities": [{"text": "SHRDLU", "start_pos": 121, "end_pos": 127, "type": "DATASET", "confidence": 0.7450642585754395}]}, {"text": "In the past few years, an influx of new, large vision & language corpora, alongside dramatic advances in vision research, has sparked renewed interest in connecting vision and language.", "labels": [], "entities": []}, {"text": "Vision & language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques.", "labels": [], "entities": []}, {"text": "Fueled in part by the newly emerging data, research that blends techniques in vision and in language has increased at an incredible rate.", "labels": [], "entities": []}, {"text": "In just * F.F. and N.M. contributed equally to this work.", "labels": [], "entities": [{"text": "F.F.", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.8556056618690491}]}, {"text": "the past year, recent work has proposed methods for image and video captioning), summarization, reference (, and question answering (, to name just a few.", "labels": [], "entities": [{"text": "image and video captioning", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.6456572562456131}, {"text": "summarization", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.9927143454551697}, {"text": "question answering", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.8816925585269928}]}, {"text": "The newly crafted large-scale vision & language datasets have played a crucial role in defining this research, serving as a foundation for training/testing and helping to set benchmarks for measuring system performance.", "labels": [], "entities": []}, {"text": "Crowdsourcing and large image collections such as those provided by Flickr 1 have made it possible for researchers to propose methods for vision and language tasks alongside an accompanying dataset.", "labels": [], "entities": [{"text": "Flickr 1", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.9142220616340637}]}, {"text": "However, as more and more datasets have emerged in this space, it has become unclear how different methods generalize beyond the datasets they are evaluated on, and what data maybe useful for moving the field beyond a single task, towards solving larger AI problems.", "labels": [], "entities": []}, {"text": "In this paper, we take a step back to document this moment in time, making a record of the major available corpora that are driving the field.", "labels": [], "entities": []}, {"text": "We provide a quantitative analysis of each of these corpora in order to understand the characteristics of each, and how they compare to one another.", "labels": [], "entities": []}, {"text": "The quality of a dataset must be measured and compared to related datasets, as low quality data may distort an entire subfield.", "labels": [], "entities": []}, {"text": "We propose a set of criteria for analyzing, evaluating and comparing the quality of vision & language datasets against each other.", "labels": [], "entities": []}, {"text": "Knowing the details of a dataset compared to similar datasets allows researchers to define more precisely what task(s) they are trying to solve, and select the dataset(s) best suited to their goals, while being aware of the implications and biases the datasets could impose on a task.", "labels": [], "entities": []}, {"text": "We categorize the available datasets into three major classes and evaluate them against these cri-teria.", "labels": [], "entities": []}, {"text": "The datasets we present here were chosen because they are all available to the community and cover the data that has been created to support the recent focus on image captioning work.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 161, "end_pos": 177, "type": "TASK", "confidence": 0.7471242249011993}]}, {"text": "More importantly, we provide an evolving website 2 containing pointers and references to many more vision-to-language datasets, which we believe will be valuable in unifying the quickly expanding research tasks in language and vision.", "labels": [], "entities": []}], "datasetContent": [{"text": "The quality of a dataset is highly dependent on the sampling and scraping techniques used early in the data collection process.", "labels": [], "entities": []}, {"text": "However, the content of datasets can play a major role in narrowing the focus of the field.", "labels": [], "entities": []}, {"text": "Datasets are affected by both reporting bias, where the frequency with which people write about actions, events, or states does not directly reflect real-world frequencies of those phenomena; they are also affected by photographer's bias (Torralba and Efros, 2011), where photographs are somewhat predictable within a given domain.", "labels": [], "entities": []}, {"text": "This suggests that new datasets maybe useful towards the larger AI goal if provided alongside a set of quantitative metrics that show how they compare against similar corpora, as well as more general \"background\" corpora.", "labels": [], "entities": []}, {"text": "Such metrics can be used as indicators of dataset bias and language richness.", "labels": [], "entities": []}, {"text": "At a higher level, we argue that clearly defined metrics are necessary to provide quantitative measurements of how anew dataset compares to previous work.", "labels": [], "entities": []}, {"text": "This helps clarify and benchmark how research is progressing towards a broader AI goal as more and more data comes into play.", "labels": [], "entities": []}, {"text": "In this section, we propose a set of such metrics that characterize vision & language datasets.", "labels": [], "entities": []}, {"text": "We focus on methods to measure language quality that can be used across several corpora.", "labels": [], "entities": []}, {"text": "We also briefly examine metrics for vision quality.", "labels": [], "entities": []}, {"text": "We evaluate several recent datasets based on all proposed metrics in Section 4, with results reported in, and Figure 1.", "labels": [], "entities": []}, {"text": "We group a representative set of available datasets based on their content.", "labels": [], "entities": []}, {"text": "For a complete list of datasets and their descriptions, please refer to the supplementary website.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of statistics and quality metrics of a sample set of major datasets. For Brown, we report Frazier and Yngve  scores on automatically acquired parses, but we also compute them for the 24K sentences with gold parses: in this setting, the  mean Frazier score is 15.26 while the mean Yngve score is 58.48.", "labels": [], "entities": []}, {"text": " Table 2: Perplexities across corpora, where rows represent test sets (20k sentences) and columns training sets (remaining  sentences). To make perplexities comparable, we used the same vocabulary frequency cutoff of 3. All models are 5-grams.", "labels": [], "entities": []}]}