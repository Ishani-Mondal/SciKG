{"title": [{"text": "Human Evaluation of Grammatical Error Correction Systems", "labels": [], "entities": [{"text": "Human Evaluation of Grammatical Error Correction", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7128876894712448}]}], "abstractContent": [{"text": "The paper presents the results of the first large-scale human evaluation of automatic grammatical error correction (GEC) systems.", "labels": [], "entities": [{"text": "automatic grammatical error correction (GEC)", "start_pos": 76, "end_pos": 120, "type": "TASK", "confidence": 0.7246776989528111}]}, {"text": "Twelve participating systems and the unchanged input of the CoNLL-2014 shared task have been reassessed in a WMT-inspired human evaluation procedure.", "labels": [], "entities": [{"text": "WMT-inspired human evaluation", "start_pos": 109, "end_pos": 138, "type": "TASK", "confidence": 0.5212191343307495}]}, {"text": "Methods introduced for the Workshop of Machine Translation evaluation campaigns have been adapted to GEC and extended where necessary.", "labels": [], "entities": [{"text": "Machine Translation evaluation campaigns", "start_pos": 39, "end_pos": 79, "type": "TASK", "confidence": 0.8634004145860672}, {"text": "GEC", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.9685665965080261}]}, {"text": "The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.6166390577952067}]}], "introductionContent": [{"text": "The field of automatic grammatical error correction (GEC) has seen a number of shared tasks of different scope and for different languages.", "labels": [], "entities": [{"text": "automatic grammatical error correction (GEC)", "start_pos": 13, "end_pos": 57, "type": "TASK", "confidence": 0.8005328348704747}]}, {"text": "The most impactful were the) shared tasks on Grammatical Error Correction for ESL (English as a second language) learners.", "labels": [], "entities": [{"text": "Grammatical Error Correction", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.6333356897036234}]}, {"text": "They were preceded by the HOO shared tasks (.", "labels": [], "entities": [{"text": "HOO shared tasks", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.5634825627009074}]}, {"text": "Shared tasks for other languages took place as well, including the QALB workshops for Arabic () and NLP-TEA competitions for Chinese.", "labels": [], "entities": []}, {"text": "These tasks use automatic metrics to determine the quality of the participating systems.", "labels": [], "entities": []}, {"text": "However, these efforts pale in comparison to competitions organized in other fields, e.g. during the annual Workshops for Machine Translation (WMT).", "labels": [], "entities": [{"text": "Machine Translation (WMT)", "start_pos": 122, "end_pos": 147, "type": "TASK", "confidence": 0.8682324767112732}]}, {"text": "It is a central idea of the WMTs that automatic measures of machine translation quality are an imperfect substitute for human assessments.", "labels": [], "entities": [{"text": "WMTs", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.838028073310852}, {"text": "machine translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7220571041107178}]}, {"text": "Therefore, manual evaluation of the system outputs are conducted and their results are reported as the final rankings of the workshops.", "labels": [], "entities": []}, {"text": "These human evaluation campaigns are an important driving factor for the advancement of MT and produce insightful \"by-products\", such as a huge number of human assessments of machine translation outputs that have been used to evaluate automatic metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9964025020599365}]}, {"text": "We believe that the unavailability of this kind of quality assessment may stall the development of GEC, as all the shared tasks and the entire field have to cope with an inherent uncertainty of their methods and metrics.", "labels": [], "entities": [{"text": "GEC", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.47776269912719727}]}, {"text": "We hope to make a step towards alleviating this lack of confidence by presenting the results of the first 1 large-scale human evaluation of automatic grammatical error correction systems submitted to the CoNLL-2014 shared task.", "labels": [], "entities": [{"text": "automatic grammatical error correction systems submitted to the CoNLL-2014 shared task", "start_pos": 140, "end_pos": 226, "type": "TASK", "confidence": 0.5836518406867981}]}, {"text": "Most of our inspiration is drawn from the recent WMT edition ( ) and its metrics task).", "labels": [], "entities": [{"text": "WMT edition", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.8622154891490936}]}, {"text": "We also provide an analysis of correlation between the standard metrics in GEC and human judgment and show that the commonly used parameters for standard metrics in the shared task may not be optimal.", "labels": [], "entities": []}, {"text": "The uncertainty about metrics quality leads to proposals of new metrics, with Felice and Briscoe (2015) being a recent example.", "labels": [], "entities": []}, {"text": "Based on human judgments we can show that this proposed metric maybe less useful than hoped.", "labels": [], "entities": []}], "datasetContent": [{"text": "(2011) addresses two problems of GEC evaluation: 1) alack of informative metrics and 2) an inability to directly compare the performance of systems developed by different researchers.", "labels": [], "entities": [{"text": "GEC evaluation", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.9481558203697205}]}, {"text": "Two evaluation methodologies are presented, both based on crowdsourcing which are used to grade types of errors rather than system performance as presented in this work.", "labels": [], "entities": []}, {"text": "draw attention to the many evalua-tion issues in error detection which make it hard to compare different approaches.", "labels": [], "entities": [{"text": "error detection", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.6752814203500748}]}, {"text": "The lack of consensus is due to the nature of the error detection task.", "labels": [], "entities": [{"text": "error detection task", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.7652242581049601}]}, {"text": "The authors argue that the choice of the metric should take into account factors such as the skew of the data and the application that the system is used for.", "labels": [], "entities": []}, {"text": "The most recent addition is who present a novel evaluation method for grammatical error correction that scores systems in terms of improvement on the original text.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.6229747235774994}]}, {"text": "The system outputs of the CoNLL-2014 shared task serve as evaluation data.", "labels": [], "entities": [{"text": "CoNLL-2014 shared task", "start_pos": 26, "end_pos": 48, "type": "DATASET", "confidence": 0.7782447934150696}]}, {"text": "The test set consists of 1312 sentences, there are twelve system outputs available.", "labels": [], "entities": []}, {"text": "The thirteenth participant NARA is missing from this set.", "labels": [], "entities": [{"text": "NARA", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.8626905679702759}]}, {"text": "However, in GEC evaluation there is also the input to consider.", "labels": [], "entities": [{"text": "GEC evaluation", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.8060489892959595}]}, {"text": "Often system outputs are equal to the unmodified input, as it is most desirable if there are in fact no errors.", "labels": [], "entities": []}, {"text": "We include INPUT as the thirteenth system.", "labels": [], "entities": [{"text": "INPUT", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.5851733684539795}]}, {"text": "Due to the small number of modifications that GEC systems apply to the input, there is not only a large overlap with the input, but also among all systems).", "labels": [], "entities": []}, {"text": "If we sample systems uniformly, we lose easily obtainable pairwise judgments for systems with the same output, and if we collapse before sampling we introduce a strong bias towards ties.", "labels": [], "entities": []}, {"text": "To counter that bias, we abandon uniform sampling of test set sentences and use instead a parametrized distribution that favors diverse sets of outputs.", "labels": [], "entities": []}, {"text": "The probability pi fora set of outputs O i is calculated as follows: N is the number of systems to be evaluated, M is the maximum number of sentences presented to the evaluator in a single ranking (we use M = 5).", "labels": [], "entities": []}, {"text": "The set of system outputs to be evaluated E = {O 1 , . .", "labels": [], "entities": []}, {"text": ", O n } \u2200 1\u2264i\u2264n |O i | = N , consists of n (= 1312) sets O i of N output sentences each.", "labels": [], "entities": []}, {"text": "Every sentence in O i can overlap with other sentences multiple times, so for each set O i we define the corresponding multiset of multiplicities U i , such that We define c i (j) as the number of possible ways to choose at most M different sentences that cover j systems for the i-th set of outputs: Then the expected number Ci of systems covered by choosing at most M sentences is . The pseudo-probability pi of sampling the i-th sentence is defined as where which is the ratio of pairwise comparisons of M versus Ci different systems.", "labels": [], "entities": []}, {"text": "By normalizing over  the entire set of output sets we obtain the probability pi of sampling the i-th set of outputs as", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for collected rankings (Ranks),  unexpanded and expanded pairwise judgments,  numbers for ties are given in parentheses.", "labels": [], "entities": []}, {"text": " Table 2: Inter-annotator and intra-annotator agree- ment (Cohen's \u03ba) on unexpanded pairwise judg- ments.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of official CoNLL-2014 ranking and human rankings. Ranges and clusters have  been calculated with bootstrap resampling at p \u2264 0.05.", "labels": [], "entities": [{"text": "CoNLL-2014 ranking", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.8189098238945007}]}, {"text": " Table 5: Correlation results for various metrics  and human ranking.", "labels": [], "entities": []}]}