{"title": [{"text": "Scientific Article Summarization Using Citation-Context and Article's Discourse Structure", "labels": [], "entities": [{"text": "Scientific Article Summarization", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5307925641536713}]}], "abstractContent": [{"text": "We propose a summarization approach for scientific articles which takes advantage of citation-context and the document discourse model.", "labels": [], "entities": [{"text": "summarization", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9672452807426453}]}, {"text": "While citations have been previously used in generating scientific summaries, they lack the related context from the referenced article and therefore do not accurately reflect the article's content.", "labels": [], "entities": [{"text": "generating scientific summaries", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6093589266141256}]}, {"text": "Our method overcomes the problem of inconsistency between the citation summary and the article's content by providing context for each citation.", "labels": [], "entities": []}, {"text": "We also leverage the inherent scientific article's discourse for producing better summaries.", "labels": [], "entities": []}, {"text": "We show that our proposed method effectively improves over existing summarization approaches (greater than 30% improvement over the best performing baseline) in terms of ROUGE scores on TAC2014 scientific summarization dataset.", "labels": [], "entities": [{"text": "summarization", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.9850902557373047}, {"text": "ROUGE", "start_pos": 170, "end_pos": 175, "type": "METRIC", "confidence": 0.989095151424408}, {"text": "TAC2014 scientific summarization dataset", "start_pos": 186, "end_pos": 226, "type": "DATASET", "confidence": 0.7813243567943573}]}, {"text": "While the dataset we use for evaluation is in the biomedical domain, most of our approaches are general and therefore adaptable to other domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Due to the expanding rate at which articles are being published in each scientific field, it has become difficult for researchers to keep up with the developments in their respective fields.", "labels": [], "entities": []}, {"text": "Scientific summarization aims to facilitate this problem by providing readers with concise and informative representation of contributions or findings of an article.", "labels": [], "entities": [{"text": "Scientific summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.541920617222786}]}, {"text": "Scientific summarization is different than general summarization in three main aspects).", "labels": [], "entities": [{"text": "Scientific summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5619430840015411}]}, {"text": "First, the length of scientific papers are usually much longer than general articles (e.g newswire).", "labels": [], "entities": []}, {"text": "Second, in scientific summarization, the goal is typically to provide a technical summary of the paper which includes important findings, contributions or impacts of a paper to the community.", "labels": [], "entities": [{"text": "scientific summarization", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.5589439868927002}]}, {"text": "Finally, scientific papers follow a natural discourse.", "labels": [], "entities": []}, {"text": "A common organization for scientific paper is the one in which the problem is first introduced and is followed by the description of hypotheses, methods, experiments, findings and finally results and implications.", "labels": [], "entities": []}, {"text": "Scientific summarization was recently further motivated by TAC2014 biomedical summarization track 1 in which they planned to investigate this problem in the domain of biomedical science.", "labels": [], "entities": [{"text": "Scientific summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.4974518418312073}, {"text": "TAC2014 biomedical summarization", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.7358448704083761}]}, {"text": "There are currently two types of approaches towards scientific summarization.", "labels": [], "entities": [{"text": "scientific summarization", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.5814063251018524}]}, {"text": "First is the articles' abstracts.", "labels": [], "entities": []}, {"text": "While abstracts provide a general overview of the paper, they cannot be considered as an accurate scientific summary by themselves.", "labels": [], "entities": []}, {"text": "That is due to the fact that not all the contributions and impacts of the paper are included in the abstract ().", "labels": [], "entities": []}, {"text": "In addition, the stated contributions are those that the authors deem important while they might be less important to the scientific community.", "labels": [], "entities": []}, {"text": "Moreover, contributions are stated in a general and less focused fashion.", "labels": [], "entities": []}, {"text": "These problems motivated the other form of scientific summaries, i.e., citation based summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 54, "end_pos": 63, "type": "TASK", "confidence": 0.756665050983429}, {"text": "citation based summaries", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.5797579685846964}]}, {"text": "Citation based summary is a summary which is formed by utilizing a set of citations to a referenced article.", "labels": [], "entities": []}, {"text": "This set of citations has been previously indicated as a good representation of important findings and contributions of the article.", "labels": [], "entities": []}, {"text": "Contributions stated in the citations are usually more focused than the abstract and contain additional information that is not in the abstract ().", "labels": [], "entities": []}, {"text": "However, citations may not accurately represent the content of the referenced article as they are biased towards the viewpoint of the citing authors.", "labels": [], "entities": []}, {"text": "Moreover, citations may address a contribution or a finding regarding the referenced article without referring to the assumptions and data under which it was obtained.", "labels": [], "entities": []}, {"text": "The problem of inconsistency between the degree of certainty of expressing findings between the citing article and referenced article has been also reported).", "labels": [], "entities": [{"text": "certainty", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9803245663642883}]}, {"text": "Therefore, citations by themselves lack the related \"context\" from the original article.", "labels": [], "entities": []}, {"text": "We call the textual spans in the reference articles that reflect the citation, the citation-context.", "labels": [], "entities": []}, {"text": "shows an example of the citation-context in the reference article (green color) fora citation in the citing article (blue color).", "labels": [], "entities": []}, {"text": "We propose an approach to overcome the aforementioned shortcomings of existing scientific summaries.", "labels": [], "entities": []}, {"text": "Specifically, we extract citation-context in the reference article for each citation.", "labels": [], "entities": []}, {"text": "Then, by using the discourse facets of the citations as well as community structure of the citation-contexts, we extract candidate sentences for the summary.", "labels": [], "entities": []}, {"text": "The final summary is formed by maximizing both novelty and informativeness of the sentences in the summary.", "labels": [], "entities": [{"text": "novelty", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9660205841064453}]}, {"text": "We evaluate and compare our methods against several well-known summarization methods.", "labels": [], "entities": [{"text": "summarization", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.9634090662002563}]}, {"text": "Evaluation results on the TAC2014 dataset show that our proposed methods can effectively improve over the well-known existing summarization approaches.", "labels": [], "entities": [{"text": "TAC2014 dataset", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.962760865688324}, {"text": "summarization", "start_pos": 126, "end_pos": 139, "type": "TASK", "confidence": 0.9625402688980103}]}, {"text": "That is, we obtained greater than 30% improvement over the highest performing baseline in terms of mean ROUGE scores.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.9577870965003967}]}], "datasetContent": [{"text": "We used the TAC2014 biomedical summarization dataset for evaluation of our proposed method.", "labels": [], "entities": [{"text": "TAC2014 biomedical summarization dataset", "start_pos": 12, "end_pos": 52, "type": "DATASET", "confidence": 0.8451772481203079}]}, {"text": "The TAC2014 benchmark contains 20 topics each of which consists of one reference article and several articles that have citations to each reference article (the statistics of the dataset is shown in).", "labels": [], "entities": [{"text": "TAC2014 benchmark", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8335484862327576}]}, {"text": "All articles are biomedical papers published by Elsevier.", "labels": [], "entities": []}, {"text": "For each topic, 4 experts in biomedical domain have written a scientific summary of length not exceeding 250 words for the reference article.", "labels": [], "entities": []}, {"text": "The data also contains annotated citation texts as well as the discourse facets.", "labels": [], "entities": []}, {"text": "The latter were used to build the supervised discourse model.", "labels": [], "entities": []}, {"text": "The distribution of discourse facets is shown in.", "labels": [], "entities": []}, {"text": "We use the ROUGE evaluation metrics which has shown consistent correlation with manually evaluated summarization scores).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.8552722930908203}, {"text": "summarization", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.9564750790596008}]}, {"text": "More specifically, we use ROUGE-L, ROUGE-1 and ROUGE-2 to evaluate and compare the quality of the summaries generated by our system.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9728052020072937}, {"text": "ROUGE-1", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9486916661262512}, {"text": "ROUGE-2", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9672175049781799}]}, {"text": "While ROUGE-N focuses on n-gram overlaps, ROUGE-L uses the longest common subsequence to measure the quality of the summary.", "labels": [], "entities": []}, {"text": "ROUGE-N where N is the n-gram order, is defined as follows: Where Wis the n-gram, f (.) is the count function, f match (.) is the maximum number of n-grams cooccurring in the generated summary and in a set of gold summaries.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9696780443191528}, {"text": "match", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.7960941195487976}]}, {"text": "For a candidate summary C with n words and a gold summary S with u sentences, ROUGE-L is defined as follows: Where LCS \u222a (., .) is the Longest common subsequence (LCS) score of the union of LCS between gold sentence r i and the candidate summary C.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.997122585773468}, {"text": "Longest common subsequence (LCS) score", "start_pos": 135, "end_pos": 173, "type": "METRIC", "confidence": 0.9524716820035662}]}, {"text": "ROUGE-L f score is the harmonic mean between precision and recall.", "labels": [], "entities": [{"text": "ROUGE-L f score", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9626683990160624}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9964504241943359}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9875182509422302}]}], "tableCaptions": [{"text": " Table 2: Distribution of annotated discourse facets", "labels": [], "entities": []}]}