{"title": [{"text": "Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections", "labels": [], "entities": [{"text": "Discovering Topic Correlation", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.7274313569068909}]}], "abstractContent": [{"text": "Weak topic correlation across document collections with different numbers of topics in individual collections presents challenges for existing cross-collection topic models.", "labels": [], "entities": []}, {"text": "This paper introduces two probabilistic topic models, Correlated LDA (C-LDA) and Correlated HDP (C-HDP).", "labels": [], "entities": []}, {"text": "These address problems that can arise when analyzing large, asymmetric, and potentially weakly-related collections.", "labels": [], "entities": []}, {"text": "Topic correlations in weakly-related collections typically lie in the tail of the topic distribution, where they would be overlooked by models unable to fit large numbers of topics.", "labels": [], "entities": []}, {"text": "To efficiently model this long tail for large-scale analysis, our models implement a parallel sampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et al., 2015).", "labels": [], "entities": []}, {"text": "The models are first evaluated on synthetic data, generated to simulate various collection-level asymmetries.", "labels": [], "entities": []}, {"text": "We then present a case study of modeling over 300k documents in collections of sciences and humanities research from JSTOR.", "labels": [], "entities": [{"text": "JSTOR", "start_pos": 117, "end_pos": 122, "type": "DATASET", "confidence": 0.9171442985534668}]}], "introductionContent": [{"text": "Comparing large text collections is a critical task for the curation and analysis of human cultural history.", "labels": [], "entities": []}, {"text": "Achievements of research and scholarship are most accessible through textual artifacts, which are increasingly available in digital archives.", "labels": [], "entities": []}, {"text": "Text-based research, often undertaken by humanists, historians, lexicographers, and corpus linguists, explores patterns of words in documents across time-periods and distinct collections of text.", "labels": [], "entities": []}, {"text": "Here, we introduce two new topic models designed to compare large collections, Correlated LDA (C-LDA) and Correlated HDP (C-HDP), which are sensitive to document-topic asymmetry (where collections have different topic distributions) and topic-word asymmetry (where a single topic has different word distributions in each collection).", "labels": [], "entities": []}, {"text": "These models seek to address terminological questions, such as how a topic on physics is articulated distinctively in scientific compared to humanistic research.", "labels": [], "entities": []}, {"text": "Accommodating potential collection-level asymmetries is particularly important when researchers seek to analyze collections with little prior knowledge about shared or collection-specific topic structure.", "labels": [], "entities": []}, {"text": "Our models extend existing cross-collection approaches to accommodate these asymmetries and implement an efficient parallel sampling algorithm enabling users to examine the long tail of topics in particularly large collections.", "labels": [], "entities": []}, {"text": "Using topic models for comparative text mining was introduced by, who developed the ccMix model which extended pLSA.", "labels": [], "entities": [{"text": "comparative text mining", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.6703162690003713}, {"text": "ccMix", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.9288648366928101}]}, {"text": "Later work by developed ccLDA, which adopted the hierarchical Bayes framework of Latent Dirichlet Allocation or LDA ().", "labels": [], "entities": [{"text": "ccLDA", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.9320423007011414}]}, {"text": "These models account for topic-word asymmetry by assuming variation in the vocabularies of topics is due to collection-level differences.", "labels": [], "entities": []}, {"text": "Nevertheless, they require the same topics to be present in each collection.", "labels": [], "entities": []}, {"text": "These models are useful for comparing collections under specific assumptions, but cannot accommodate collection-topic asymmetry (which arises in collections that do not share every topic or that have different numbers of topics).", "labels": [], "entities": []}, {"text": "In situations where collections do not share all topics, the results often include junk, mixed, or sparse topics, making them difficult to interpret.", "labels": [], "entities": []}, {"text": "Such asymmetries make it difficult to use models like ccLDA and ccMix when little is known about collections in advance.", "labels": [], "entities": [{"text": "ccLDA", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.9424459934234619}, {"text": "ccMix", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.9021830558776855}]}, {"text": "This motivates our efforts to model variation in the long tail of topic distributions, where correlations are more likely to appear when collections are weakly related.", "labels": [], "entities": []}, {"text": "C-LDA and C-HDP extend ccLDA) to accommodate collection-topic level asymmetries, particularly by allowing noncommon topics to appear in each collection.", "labels": [], "entities": []}, {"text": "This added flexibility allows our models to discover topic correlations across arbitrary collections with different numbers of topics, even when there are few (or unknown) numbers of common topics.", "labels": [], "entities": []}, {"text": "To demonstrate the effectiveness of our models, we evaluate them on synthetic data and show that they outperform related models such as ccLDA and differential topic models).", "labels": [], "entities": [{"text": "ccLDA", "start_pos": 136, "end_pos": 141, "type": "DATASET", "confidence": 0.8829295635223389}]}, {"text": "We then fit C-LDA to two large collections of humanities and sciences documents from JSTOR.", "labels": [], "entities": [{"text": "JSTOR", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.8217058777809143}]}, {"text": "Such historical analyses of text would be intractable without an efficient sampler.", "labels": [], "entities": []}, {"text": "An optimized sampler is required in such situations because common topics in weakly-correlated collections are usually found in the tail of the document-topic distribution of a sufficiently large set of topics.", "labels": [], "entities": []}, {"text": "To make this feasible on large datasets such as JSTOR, we employ a parallelized Metropolis-Hastings ( and alias-table sampling framework, adapted from LightLDA (.", "labels": [], "entities": [{"text": "LightLDA", "start_pos": 151, "end_pos": 159, "type": "DATASET", "confidence": 0.9340689778327942}]}, {"text": "These optimizations, which achieve O(1) amortized sampling time per token, allow our models to befit to large corpora with up to thousands of topics in a matter of hours -an order of magnitude speed-up from ccLDA.", "labels": [], "entities": [{"text": "O(1) amortized sampling time", "start_pos": 35, "end_pos": 63, "type": "METRIC", "confidence": 0.8434458460126605}, {"text": "ccLDA", "start_pos": 207, "end_pos": 212, "type": "DATASET", "confidence": 0.9685123562812805}]}, {"text": "After reviewing work related to topic modeling across collections, section 3 describes C-LDA and C-HDP, and then details their technical relationship to existing models.", "labels": [], "entities": []}, {"text": "Section 5 introduces the synthetic data and part of the JSTOR corpus used in our evaluations.", "labels": [], "entities": [{"text": "JSTOR corpus", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9051856398582458}]}, {"text": "We then compare our models' performances to other models in terms of heldout perplexity and a measure of distinguishability.", "labels": [], "entities": []}, {"text": "The final results section exemplifies the use of C-LDA in a qualitative analysis of humanities and sciences research.", "labels": [], "entities": []}, {"text": "We conclude with a brief discussion of the strengths of C-LDA and C-HDP, and outline directions for future work and applications.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}