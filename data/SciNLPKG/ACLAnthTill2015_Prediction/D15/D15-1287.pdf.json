{"title": [{"text": "Bilingual Structured Language Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.8403319120407104}]}], "abstractContent": [{"text": "This paper describes a novel target-side syntactic language model for phrase-based statistical machine translation, bilingual structured language model.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 70, "end_pos": 114, "type": "TASK", "confidence": 0.5770445540547371}]}, {"text": "Our approach represents anew way to adapt structured language models (Chelba and Jelinek, 2000) to statistical machine translation, and a first attempt to adapt them to phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 99, "end_pos": 130, "type": "TASK", "confidence": 0.6960070729255676}, {"text": "phrase-based statistical machine translation", "start_pos": 169, "end_pos": 213, "type": "TASK", "confidence": 0.633640430867672}]}, {"text": "We propose a number of variations of the bilingual structured language model and evaluate them in a series of rescoring experiments.", "labels": [], "entities": []}, {"text": "Rescoring of 1000-best translation lists produces statistically significant improvements of up to 0.7 BLEU over a strong baseline for Chinese-English, but does not yield improvements for Arabic-English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9992903470993042}]}], "introductionContent": [{"text": "Many model components of competitive statistical machine translation (SMT) systems are based on rather simplistic definitions with little linguistic grounding, which includes the definitions of phrase pairs, lexicalized reordering, and n-gram language models.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.8166816929976145}]}, {"text": "However, earlier work has also shown that statistical MT can benefit from additional linguistically motivated models.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.8266621828079224}]}, {"text": "Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels ().", "labels": [], "entities": [{"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.9277588129043579}]}, {"text": "On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008),,, Ge (2010),,,.", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.7543736100196838}]}, {"text": "This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 101, "end_pos": 112, "type": "TASK", "confidence": 0.9689611792564392}]}, {"text": "We work with the phrase-based SMT (PBSMT) ( framework as the baseline system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.6838560700416565}]}, {"text": "Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework.", "labels": [], "entities": []}, {"text": "It is typically quite straightforward to integrate an additional model into the system.", "labels": [], "entities": []}, {"text": "Also, PBSMT is the most widely used framework in the SMT research community, which ensures comparability of our results to other people's work on the topic.", "labels": [], "entities": [{"text": "SMT research", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.9208440482616425}]}, {"text": "There is a variety of ways syntax can be used in a PBSMT model.", "labels": [], "entities": []}, {"text": "Typically a syntactic representation of a source sentence is used to define constraints on the order in which the decoder translates it.", "labels": [], "entities": []}, {"text": "For example, Cherry (2008) defines soft constraints based on the notion of syntactic cohesion (Section 2).", "labels": [], "entities": []}, {"text": "Ge (2010) captures reordering patterns by defining soft constraints based on the currently translated word's POS tag and the words structurally related to it.", "labels": [], "entities": []}, {"text": "On the other hand, target syntax is more challenging to use in PBSMT, since a target-side syntactic model does not have access to the whole target sentence at decoding. is one of the few targetside syntactic approaches applicable to PBSMT, but it has been shown not to improve translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 277, "end_pos": 288, "type": "TASK", "confidence": 0.9518097043037415}]}, {"text": "Their approach uses a target side parser as a language model: one of the reasons why it fails is that a parser assumes its input to be grammatical and chooses the most likely parse for it.", "labels": [], "entities": []}, {"text": "What we are interested in during translation is how gram-matical the target sentence actually is.", "labels": [], "entities": [{"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9754816889762878}]}, {"text": "In addition to reordering constraints, source syntax can be used for target-side language modeling.", "labels": [], "entities": []}, {"text": "A target side string can be encoded with source-syntactic building blocks and then scored as to how well-formed it is.,, model target sequences as strings of tokens built from the target POS tag and the POS tags of the source words related to it through alignment and the source parse.", "labels": [], "entities": []}, {"text": "In this paper, we define a target-side syntactic language model that takes structural constraints from the source sentence, but uses the words from the target side (as 'building blocks').", "labels": [], "entities": []}, {"text": "We do it by adapting an existing monolingual model of, structured language models, to the bilingual setting.", "labels": [], "entities": []}, {"text": "Our contributions can be summarized as follows: \u2022 we propose a novel method to adapt monolingual structured language models) (Section 3) to a PBSMT system (Section 4), which does not require an external on-the-fly parser, but only uses the given source-side syntactic analysis to infer structural relations between target words; \u2022 building on the existing literature, we propose a set of deterministic rules that incrementally buildup a parse of a target translation hypothesis based on the source parse (Section 4); \u2022 we evaluate our models in a series of rescoring experiments and achieve statistically significant improvements of up to 0.7 BLEU for Chinese-English (Section 5).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 643, "end_pos": 647, "type": "METRIC", "confidence": 0.9986807703971863}]}, {"text": "Before describing the models, we motivate our method with a common assumption about crosslingual correspondence (Section 2).", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the effectiveness of BiSLMs for PB-SMT, we performed rescoring experiments for We extract dependency parses from its output based on Arabic-English and Chinese-English.", "labels": [], "entities": []}, {"text": "We compare the resulting 1-best translation lists with an output of the baseline system and the baseline augmented with soft cohesion constraints from: Arabic-English baseline and comparison model) results.", "labels": [], "entities": []}, {"text": "This section provides information about our baseline system.", "labels": [], "entities": []}, {"text": "Word-alignment is produced with GIZA++ (.", "labels": [], "entities": []}, {"text": "We use an inhouse implementation of a PBSMT system similar to Moses (.", "labels": [], "entities": []}, {"text": "Our baseline has all standard PBSMT features including language model, lexical weighting, and lexicalized reordering.", "labels": [], "entities": []}, {"text": "The distortion limit is set to 5.", "labels": [], "entities": [{"text": "distortion", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9964205026626587}]}, {"text": "A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 30, "end_pos": 53, "type": "DATASET", "confidence": 0.8743130763371786}, {"text": "SRILM", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.4937920868396759}]}, {"text": "Information about the training data for the Arabic-English and Chinese-English systems is in) is used to detect statistically significant differences.", "labels": [], "entities": []}, {"text": "Rescoring with BiSLMs is performed as follows: For the test runs of the baseline system we compute then = 1000 best translation hypotheses for each source sentence and extract their derivations (sequence of phrase pair applications).", "labels": [], "entities": []}, {"text": "Each phrase pair in our implementation is associated with a unique phrase-internal alignment and target POS-sequence.", "labels": [], "entities": []}, {"text": "We fully reconstruct wordalignment for each pair of a source sentence and its translation hypothesis.", "labels": [], "entities": []}, {"text": "We project a precomputed source parse onto the target side and compute representations of the target sentence to be computed by a BiSLM.", "labels": [], "entities": []}, {"text": "For each hypothesis, we take its BiSLM score and its score assigned by the baseline system and compute the final score as a weighted sum of the original baseline score and a length-normalized BiSLM score 10 , where the weight \u03bb is empirically set to 0.3: \u03bb \u00b7 score BiSLM length Hypothesis + (1 \u2212 \u03bb) \u00b7 score Baseline (3)", "labels": [], "entities": [{"text": "length-normalized BiSLM score 10", "start_pos": 174, "end_pos": 206, "type": "METRIC", "confidence": 0.7970607653260231}, {"text": "BiSLM length Hypothesis", "start_pos": 265, "end_pos": 288, "type": "METRIC", "confidence": 0.6770464877287546}]}], "tableCaptions": [{"text": " Table 1: Chinese-English baseline and compari- son model (Cherry, 2008; Bach et al., 2009) re- sults.", "labels": [], "entities": []}, {"text": " Table 2: Arabic-English baseline and comparison  model", "labels": [], "entities": []}, {"text": " Table 3: Training data for Arabic-English and  Chinese-English experiments.", "labels": [], "entities": []}, {"text": " Table 4: Rescoring experiments for Chinese  MT06+08 1000-best translation sets. Unrescored  BLEU is 29.56. The column labeling contains in- formation about the kind of labeling used on the  target side of a BiSLM: just target words, target", "labels": [], "entities": [{"text": "MT06+08 1000-best translation", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.5769302725791932}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9596115946769714}]}, {"text": " Table 6: LRscores (average inverse Kendall's  Tau distance and Hamming distance) for Chinese- English baseline and BiSLM with reduce-labeling,  weak completeness, unalign-adjoin-.", "labels": [], "entities": [{"text": "LRscores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9082821607589722}, {"text": "average inverse Kendall's  Tau distance", "start_pos": 20, "end_pos": 59, "type": "METRIC", "confidence": 0.5943194429079691}]}, {"text": " Table 7: LRscores for Arabic-English baseline and  BiSLM with plain-labeling, weak completeness,  unalign-adjoin+.", "labels": [], "entities": [{"text": "LRscores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8314215540885925}]}]}