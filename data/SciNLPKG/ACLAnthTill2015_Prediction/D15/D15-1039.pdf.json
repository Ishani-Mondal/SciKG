{"title": [{"text": "Density-Driven Cross-Lingual Transfer of Dependency Parsers", "labels": [], "entities": [{"text": "Density-Driven Cross-Lingual Transfer of Dependency Parsers", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7067240824302038}]}], "abstractContent": [{"text": "We present a novel method for the cross-lingual transfer of dependency parsers.", "labels": [], "entities": [{"text": "cross-lingual transfer of dependency parsers", "start_pos": 34, "end_pos": 78, "type": "TASK", "confidence": 0.8062390089035034}]}, {"text": "Our goal is to induce a dependency parser in a target language of interest without any direct supervision: instead we assume access to parallel translations between the target and one or more source languages, and to supervised parsers in the source language(s).", "labels": [], "entities": []}, {"text": "Our key contributions are to show the utility of dense projected structures when training the target language parser, and to introduce a novel learning algorithm that makes use of dense structures.", "labels": [], "entities": []}, {"text": "Results on several languages show an absolute improvement of 5.51% in average dependency accuracy over the state-of-the-art method of (Ma and Xia, 2014).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9278154969215393}]}, {"text": "Our average dependency accuracy of 82.18% compares favourably to the accuracy of fully supervised methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9754507541656494}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9982484579086304}]}], "introductionContent": [{"text": "In recent years there has been a great deal of interest in dependency parsing models for natural languages.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.8162747025489807}]}, {"text": "Supervised learning methods have been shown to produce highly accurate dependencyparsing models; unfortunately, these methods rely on human-annotated data, which is expensive to obtain, leading to a significant barrier to the development of dependency parsers for new languages.", "labels": [], "entities": []}, {"text": "Recent work has considered unsupervised methods (e.g. (), or methods that transfer linguistic structures across languages (e.g.; * Currently on leave at Google Inc.", "labels": [], "entities": []}, {"text": "New York.), in an effort to reduce or eliminate the need for annotated training examples.", "labels": [], "entities": []}, {"text": "Unfortunately the accuracy of these methods generally lags quite substantially behind the performance of fully supervised approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.999453604221344}]}, {"text": "This paper describes novel methods for the transfer of syntactic information between languages.", "labels": [], "entities": [{"text": "transfer of syntactic information between languages", "start_pos": 43, "end_pos": 94, "type": "TASK", "confidence": 0.8087058266003927}]}, {"text": "As in previous work, our goal is to induce a dependency parser in a target language of interest without any direct supervision (i.e., a treebank) in the target language: instead we assume access to parallel translations between the target and one or more source languages, and to supervised parsers in the source languages.", "labels": [], "entities": []}, {"text": "We can then use alignments induced using tools such as GIZA++), to transfer dependencies from the source language(s) to the target language (example projections are shown in).", "labels": [], "entities": []}, {"text": "A target language parser is then trained on the projected dependencies.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We demonstrate the utility of dense projected structures when training the target-language parser.", "labels": [], "entities": []}, {"text": "In the most extreme case, a \"dense\" structure is a sentence in the target language where the projected dependencies form a fully projective tree that includes all words in the sentence (we will refer to these structures as \"full\" trees).", "labels": [], "entities": []}, {"text": "In more relaxed definitions, we might include sentences whereat least some proportion (e.g., 80%) of the words participate as a modifier in some dependency, or where long sequences (e.g., 7 words or more) of words all participate as modifiers in some dependency.", "labels": [], "entities": []}, {"text": "We give empirical evidence that dense structures give particularly high accuracy for their projected dependencies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9985333681106567}]}, {"text": "The political priorities must beset by this House and the MEPs . ROOT Die politischen Priorit\u00e4ten m\u00fcssen von diesem Parlament und den Europaabgeordneten abgesteckt werden . ROOT: An example projection from English to German in the EuroParl data).", "labels": [], "entities": [{"text": "MEPs", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.9696844220161438}, {"text": "ROOT", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.6555868983268738}, {"text": "ROOT", "start_pos": 173, "end_pos": 177, "type": "METRIC", "confidence": 0.7664886116981506}, {"text": "EuroParl data", "start_pos": 231, "end_pos": 244, "type": "DATASET", "confidence": 0.9883065223693848}]}, {"text": "The English parse tree is the output from a supervised parser, while the German parse tree is projected from the English parse tree using translation alignments from GIZA++.", "labels": [], "entities": []}, {"text": "\u2022 We describe a training algorithm that builds on the definitions of dense structures.", "labels": [], "entities": []}, {"text": "The algorithm initially trains the model on full trees, then iteratively introduces increasingly relaxed definitions of density.", "labels": [], "entities": []}, {"text": "The algorithm makes use of a training method that can leverage partial (incomplete) dependency structures, and also makes use of confidence scores from a perceptron-trained model.", "labels": [], "entities": []}, {"text": "In spite of the simplicity of our approach, our experiments demonstrate significant improvements inaccuracy over previous work.", "labels": [], "entities": []}, {"text": "In experiments on transfer from a single source language (English) to a single target language (German, French, Spanish, Italian, Portuguese, and Swedish), our average dependency accuracy is 78.89%.", "labels": [], "entities": [{"text": "transfer from a single source language (English)", "start_pos": 18, "end_pos": 66, "type": "TASK", "confidence": 0.7616322636604309}, {"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9599554538726807}]}, {"text": "When using multiple source languages, average accuracy is improved to 82.18%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9987731575965881}]}, {"text": "This is a 5.51% absolute improvement over the previous best results reported on this data set, 76.67% for the approach of.", "labels": [], "entities": []}, {"text": "To give another perspective, our accuracy is close to that of the fully supervised approach of), which gives 84.29% accuracy on this data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9996505975723267}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.999220609664917}]}, {"text": "To the best of our knowledge these are the highest accuracy parsing results for an approach that makes no use of treebank data for the language of interest.", "labels": [], "entities": [{"text": "accuracy parsing", "start_pos": 51, "end_pos": 67, "type": "METRIC", "confidence": 0.7231835424900055}]}], "datasetContent": [{"text": "Throughout the experiments in this paper, we used German as the target language for development of our approach.", "labels": [], "entities": []}, {"text": "shows some preliminary results on transferring dependencies from English to German.", "labels": [], "entities": []}, {"text": "We can estimate the accuracy of dependency subsets such as P 100 , P 80 , P \u22657 and soon by comparing these dependencies to the dependencies from a supervised German parser on the same data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9992246627807617}]}, {"text": "That is, we use a supervised parser to provide gold standard annotations.", "labels": [], "entities": []}, {"text": "The full set of dependencies P give 74.0% accuracy under this measure; results for P 100 are considerably higher inaccuracy, ranging from 83.0% to 90.1% depending on how POS constraints are used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9993329644203186}]}, {"text": "As a second evaluation method, we can test the accuracy of a model trained on the P 100 data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9994633793830872}, {"text": "P 100 data", "start_pos": 82, "end_pos": 92, "type": "DATASET", "confidence": 0.9176261226336161}]}, {"text": "The benefit of the soft-matching POS definition is clear.", "labels": [], "entities": []}, {"text": "The hard match definition harms performance, presumably because it reduces the number of sentences used to train the model.", "labels": [], "entities": []}, {"text": "Throughout the rest of this paper, we use the soft POS constraints in all projection algorithms.", "labels": [], "entities": []}, {"text": "We now describe experiments using our approach.", "labels": [], "entities": []}, {"text": "We first describe data and tools used in the experiments, and then describe results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics showing the accuracy for various definitions of projected trees: see  \u00a73.2 for definitions  of P, P 100 etc. Columns labeled \"Acc.\"", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9994639754295349}]}, {"text": " Table 2: Parsing accuracies of different methods on the test data using the gold standard POS tags.  The models \u03b8 1 . . . \u03b8 4 are described in  \u00a73.4. \"en\u2192trgt\" is the single-source setting with English as the  source language. \"concat\u2192trgt\" and \"voting\u2192trgt\" are results with multiple source languages for the  concatenation and voting methods", "labels": [], "entities": [{"text": "POS tags", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.8019060790538788}]}, {"text": " Table 3: Parsing results with automatic part of speech tags on the test data. Sup (1st) is the supervised  first-order dependency parser (McDonald et al., 2005) and sup (ae) is the Yara arc-eager parser (Rasooli  and Tetreault, 2015).", "labels": [], "entities": []}]}