{"title": [{"text": "Broad-coverage CCG Semantic Parsing with AMR", "labels": [], "entities": [{"text": "Broad-coverage CCG Semantic Parsing", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5687799155712128}, {"text": "AMR", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.6829883456230164}]}], "abstractContent": [{"text": "We propose a grammar induction technique for AMR semantic parsing.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7022101581096649}, {"text": "AMR semantic parsing", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.9246606032053629}]}, {"text": "While previous grammar induction techniques were designed to re-learn anew parser for each target application, the recently annotated AMR Bank provides a unique opportunity to induce a single model for understanding broad-coverage newswire text and support a wide range of applications.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7097330689430237}, {"text": "AMR Bank", "start_pos": 134, "end_pos": 142, "type": "DATASET", "confidence": 0.9276861846446991}]}, {"text": "We present anew model that combines CCG parsing to recover compositional aspects of meaning and a factor graph to model non-compositional phenomena, such as anaphoric dependencies.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.734598308801651}]}, {"text": "Our approach achieves 66.2 Smatch F1 score on the AMR bank, significantly outperform-ing the previous state of the art.", "labels": [], "entities": [{"text": "Smatch", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9590198397636414}, {"text": "F1 score", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.8906038403511047}, {"text": "AMR bank", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.8776771724224091}]}], "introductionContent": [{"text": "Semantic parsers map sentences to formal representations of their meaning.", "labels": [], "entities": []}, {"text": "Existing learning algorithms have primarily focused on building actionable meaning representations which can, for example, directly query a database ( or instruct a robotic agent).", "labels": [], "entities": []}, {"text": "However, due to their end-to-end nature, such models must be relearned for each new target application and have only been used to parse restricted styles of text, such as questions and imperatives.", "labels": [], "entities": []}, {"text": "Recently, AMR () was proposed as a general-purpose meaning representation language for broad-coverage text, and work is ongoing to study its use for variety of applications such as machine translation) and summarization ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 181, "end_pos": 200, "type": "TASK", "confidence": 0.8420039415359497}, {"text": "summarization", "start_pos": 206, "end_pos": 219, "type": "TASK", "confidence": 0.9924260377883911}]}, {"text": "The * Work done at the University of Washington.", "labels": [], "entities": []}, {"text": "AMR meaning bank provides a large new corpus that, for the first time, enables us to study the problem of grammar induction for broad-coverage semantic parsing.", "labels": [], "entities": [{"text": "AMR meaning bank", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.8735667069753011}, {"text": "grammar induction", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7221924811601639}, {"text": "broad-coverage semantic parsing", "start_pos": 128, "end_pos": 159, "type": "TASK", "confidence": 0.7253020405769348}]}, {"text": "However, it also presents significant challenges for existing algorithms, including much longer sentences, more complex syntactic phenomena and increased use of noncompositional semantics, such as within-sentence coreference.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew, scalable Combinatory Categorial) induction approach that solves these challenges with a learned joint model of both compositional and non-compositional semantics, and achieves state-of-the-art performance on AMR Bank parsing.", "labels": [], "entities": [{"text": "AMR Bank parsing", "start_pos": 242, "end_pos": 258, "type": "DATASET", "confidence": 0.8584089477856954}]}, {"text": "We map sentences to AMR structures in a twostage process (Section 5).", "labels": [], "entities": []}, {"text": "First, we use CCG to construct lambda-calculus representations of the compositional aspects of AMR.", "labels": [], "entities": [{"text": "AMR", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.8418342471122742}]}, {"text": "CCG is designed to capture a wide range of linguistic phenomena, such as coordination and long-distance dependencies, and has been used extensively for semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 152, "end_pos": 168, "type": "TASK", "confidence": 0.7744086682796478}]}, {"text": "To use CCG for AMR parsing we define a simple encoding for AMRs in lambda calculus, for example, as seen with the logical form z and AMR a in for the sentence Pyongyang officials denied their involvement.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9801822900772095}]}, {"text": "However, using CCG to construct such logical forms requires anew mechanism for non-compositional reasoning, for example to model the long-range anaphoric dependency introduced by their in.", "labels": [], "entities": []}, {"text": "To represent such dependencies while maintaining a relatively compact grammar, we follow Steedman's (2011) use of generalized Skolem terms, a mechanism to allow global references in lambda calculus.", "labels": [], "entities": []}, {"text": "We then allow the CCG derivation to mark when non-compositional reasoning is required with underspecified placeholders.", "labels": [], "entities": []}, {"text": "For example, shows an underspecified logical form u that would be constructed by the grammar with the bolded placeholder ID indicating an un-resolved anaphoric reference.", "labels": [], "entities": []}, {"text": "These placeholders are resolved by a factor graph model that is defined over the output logical form and models which part of it they refer to, for example to find the referent fora pronoun.", "labels": [], "entities": []}, {"text": "Although primarily motivated by non-compositional reasoning, we also use this mechanism to underspecify certain relations during parsing, allowing for more effective search.", "labels": [], "entities": []}, {"text": "Following most work in semantic parsing, we consider two learning challenges: grammar induction, which assigns meaning representations to words and phrases, and parameter estimation, where we learn a model for combining these pieces to analyze full sentences.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7387858629226685}, {"text": "grammar induction", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.6554168164730072}]}, {"text": "We introduce anew CCG grammar induction algorithm which incorporates ideas from previous algorithms) in away that scales to the longer sentences and more varied syntactic constructions observed in newswire text.", "labels": [], "entities": [{"text": "CCG grammar induction", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.6145174006621043}]}, {"text": "During lexical generation (Section 6.1), the algorithm first attempts to use a set of templates to hypothesize new lexical entries.", "labels": [], "entities": [{"text": "lexical generation", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.8684207201004028}]}, {"text": "It then attempts to combine bottom-up parsing with top-down recursive splitting to select the best entries and learn new templates for complex syntactic and semantic phenomena, which are re-used in later sentences to hypothesize new entries.", "labels": [], "entities": []}, {"text": "Finally, while previous algorithms (e.g.,) have assumed the existence of a grammar that can parse nearly every sentence to update its parameters, this does not hold for AMR Bank.", "labels": [], "entities": [{"text": "AMR Bank", "start_pos": 169, "end_pos": 177, "type": "DATASET", "confidence": 0.8587307333946228}]}, {"text": "Due to sentence complexity and search errors, our model cannot produce fully correct logical forms fora significant portion of the training data.", "labels": [], "entities": []}, {"text": "To learn from as much of the data as possible and accelerate learning, we adopt an early update strategy to generate effective updates from partially correct analyses (Section 6.2).", "labels": [], "entities": []}, {"text": "We evaluate performance on the publicly available AMR Bank ( and demonstrate that our modeling and learning contributions are crucial for grammar induction at this scale and achieve new state-of-the-art results for AMR parsing).", "labels": [], "entities": [{"text": "AMR Bank", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.9421601593494415}, {"text": "grammar induction", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.8353136479854584}, {"text": "AMR parsing", "start_pos": 215, "end_pos": 226, "type": "TASK", "confidence": 0.9340259432792664}]}, {"text": "In addition, we also present, for the first time, results without surfaceform alignment heuristics, which demonstrates the need for future work, especially to generalize to other languages.", "labels": [], "entities": [{"text": "surfaceform alignment heuristics", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.7558843195438385}]}, {"text": "The source code and learned models are available online.", "labels": [], "entities": []}, {"text": "1 1 http://yoavartzi.com/amr x: Pyongyang officials denied their involvement.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data, Tools and Metric For evaluation, we use AMR Bank release 1.0 (LDC2014T12).", "labels": [], "entities": [{"text": "AMR Bank release 1.0 (LDC2014T12)", "start_pos": 46, "end_pos": 79, "type": "DATASET", "confidence": 0.8979794979095459}]}, {"text": "We use the proxy report portion, which includes newswire articles from the English Gigaword corpus, and follow the official split for training, development and evaluation (6603/826/823 sentences).", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 75, "end_pos": 98, "type": "DATASET", "confidence": 0.6958216726779938}]}, {"text": "We use EasyCCG () trained with the re-banked CCGBank) to generate CCGBank categories, the Illinois Named Entity Tagger for NER, Stanford CoreNLP () for tokenization and part-of-speech tagging and UW SPF) to develop our system.", "labels": [], "entities": [{"text": "Illinois Named Entity Tagger", "start_pos": 90, "end_pos": 118, "type": "TASK", "confidence": 0.7365748211741447}, {"text": "tokenization", "start_pos": 152, "end_pos": 164, "type": "TASK", "confidence": 0.9662142992019653}, {"text": "part-of-speech tagging", "start_pos": 169, "end_pos": 191, "type": "TASK", "confidence": 0.7072527557611465}, {"text": "UW SPF", "start_pos": 196, "end_pos": 202, "type": "DATASET", "confidence": 0.8623473346233368}]}, {"text": "We use SMATCH  to evaluate logical forms converted back to AMRs.", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.756534993648529}]}, {"text": "CCG We use three syntactic attributes: singular sg, mass nouns nb and plural pl.", "labels": [], "entities": [{"text": "CCG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9555907845497131}]}, {"text": "When factoring lexical entries, we avoid extracting binary relations and references, and leave them in the template.", "labels": [], "entities": []}, {"text": "We use backward and forward binary combinators for application, composition and crossing composition.", "labels": [], "entities": [{"text": "crossing composition", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.8945949673652649}]}, {"text": "We allow non-crossing composition up to the third order.", "labels": [], "entities": []}, {"text": "We also add rules to handle punctuation and unary rules for typeshifting non-adjectives in adjectival positions and verb phrases in adverbial positions.", "labels": [], "entities": []}, {"text": "We allow shifting of bare plurals, mass nouns and named entities to noun phrases.", "labels": [], "entities": []}, {"text": "To avoid spurious ambiguity during parsing, we use normal-form constraints.", "labels": [], "entities": []}, {"text": "We use five basic lambda calculus types: entity e, truth value t, identifier id, quoted text txt and integer i.", "labels": [], "entities": []}, {"text": "Features During CCG parsing, we use indicator features for unary type shifting, crossing composition, lexemes, templates and dynamically generated lexical entries.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.6696815192699432}, {"text": "unary type shifting", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.6345382233460745}]}, {"text": "We also use indicators for co-occurrence of part-of-speech tags and syntactic attributes, repetitions in logical conjunctions and attachments in the logical form.", "labels": [], "entities": []}, {"text": "In the factor graph, we use indicator features for control structures, parent-relation-child selectional preferences and for mapping a relation to its final form.", "labels": [], "entities": []}, {"text": "See the supplementary material fora detailed description.", "labels": [], "entities": []}, {"text": "Initialization and Parameters We created the seed lexicon from the training data by sampling and annotating 50 sentences with lexical entries, adding entries for pronouns and adding lexemes for all alignments generated by JAMR ().", "labels": [], "entities": [{"text": "Initialization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9434808492660522}, {"text": "JAMR", "start_pos": 222, "end_pos": 226, "type": "DATASET", "confidence": 0.9060994386672974}]}, {"text": "We initialize features weights as follows: 10 for all lexeme feature for seed entries and entries generated by named-entity matching (Section 6.1), IBM Model 1 scores for all other lexemes (), -3 for unary type shifting and crossing composition features, 3 for features that pair singular and plural part-ofspeech tags with singular and plural attributes and 0 for all other features.", "labels": [], "entities": [{"text": "unary type shifting", "start_pos": 200, "end_pos": 219, "type": "TASK", "confidence": 0.6438669959704081}]}, {"text": "We set the number of iterations T = 10 and select the best model based on development results.", "labels": [], "entities": []}, {"text": "We set the max number of tokens for lexical generation k gen = 2, learning rate \u00b5 = 0.1, CCG parsing beam K = 50, factor graph beam L = 100, mini batch size M = 40 and use abeam of 100 for GENMAX.", "labels": [], "entities": [{"text": "learning rate \u00b5", "start_pos": 66, "end_pos": 81, "type": "METRIC", "confidence": 0.9196980198224386}, {"text": "CCG parsing", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.6097726821899414}, {"text": "GENMAX", "start_pos": 189, "end_pos": 195, "type": "DATASET", "confidence": 0.8564812541007996}]}, {"text": "Two-pass Inference During testing, we perform two passes of inference for every sentence.", "labels": [], "entities": []}, {"text": "First, we run our inference procedure (Section 5.4).", "labels": [], "entities": []}, {"text": "If no derivations are generated, we run inference again, allowing the parser to skip words at a fixed cost and use the entries for related words if a word is unknown.", "labels": [], "entities": []}, {"text": "We find related words in the lexicon using case, plurality and inflection string transformations.", "labels": [], "entities": []}, {"text": "Finally, if necessary, we heuristically transform the logical forms at the root of the CCG parse trees to valid AMR logical forms.", "labels": [], "entities": []}, {"text": "We set the cost of logical form transformation and word skipping to 10 and the cost of using related entries to 5.", "labels": [], "entities": [{"text": "logical form transformation", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.6236077348391215}, {"text": "word skipping", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.6913272887468338}]}, {"text": "We compare our approach to the latest, fixed version of JAMR () available online, 8 the only system to report test results on the official LDC release.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.706872820854187}, {"text": "LDC release", "start_pos": 139, "end_pos": 150, "type": "DATASET", "confidence": 0.9179075956344604}]}, {"text": "Our approach outperforms JAMR by 3 SMATCH F1 points, with a significant gain in recall.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.4748023748397827}, {"text": "SMATCH F1", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.8254455924034119}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9993237257003784}]}, {"text": "Given consensus inter-annotator agreement of 83 SMATCH F1 (), this improvement reduces the gap between automated methods and human performance by 15%.", "labels": [], "entities": [{"text": "SMATCH F1", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.7608358263969421}]}, {"text": "Although not strictly comparable, also includes results on the pre-release AMR Bank corpus, including the published JAMR results, their fixed results and the results of. shows SMATCH scores for the developments set, with ablations.", "labels": [], "entities": [{"text": "AMR Bank corpus", "start_pos": 75, "end_pos": 90, "type": "DATASET", "confidence": 0.9628086487452189}, {"text": "JAMR results", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.8327770531177521}, {"text": "SMATCH", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9576568007469177}]}, {"text": "The supplementary material includes example output derivations and qualitative comparison to JAMR outputs.", "labels": [], "entities": [{"text": "JAMR outputs", "start_pos": 93, "end_pos": 105, "type": "DATASET", "confidence": 0.7272055745124817}]}, {"text": "We first remove underspecifying constants, which leaves the factor graph to resolve only references.", "labels": [], "entities": []}, {"text": "While the expressivity of the model remains the same, more decisions are considered during parsing, modestly impacting performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 91, "end_pos": 98, "type": "TASK", "confidence": 0.9636214375495911}]}], "tableCaptions": [{"text": " Table 1: Test SMATCH results.", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.36689868569374084}]}, {"text": " Table 2: Development SMATCH results.", "labels": [], "entities": [{"text": "Development SMATCH", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.5147043764591217}]}]}