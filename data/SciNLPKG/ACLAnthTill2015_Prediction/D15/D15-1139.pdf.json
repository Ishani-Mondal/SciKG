{"title": [{"text": "Do we need bigram alignment models? On the effect of alignment quality on transduction accuracy in G2P", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9672685265541077}]}], "abstractContent": [{"text": "We investigate the need for bigram alignment models and the benefit of supervised alignment techniques in grapheme-to-phoneme (G2P) conversion.", "labels": [], "entities": [{"text": "bigram alignment", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.615861251950264}]}, {"text": "Moreover, we quantitatively estimate the relationship between alignment quality and overall G2P system performance.", "labels": [], "entities": []}, {"text": "We find that, in English, bigram alignment models do perform better than unigram alignment models on the G2P task.", "labels": [], "entities": [{"text": "bigram alignment", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.6307956576347351}]}, {"text": "Moreover, we find that supervised alignment techniques may perform considerably better than their unsupervised brethren and that few manually aligned training pairs suffice for them to do so.", "labels": [], "entities": []}, {"text": "Finally, we estimate a highly significant impact of alignment quality on overall G2P transcription performance and that this relationship is linear in nature.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grapheme-to-phoneme (G2P) conversion is the problem of converting a string of letters into a string of phonetic symbols.", "labels": [], "entities": [{"text": "Grapheme-to-phoneme (G2P) conversion", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5552473872900009}]}, {"text": "Closely related to G2P are other string transduction problems in natural language processing (NLP) such as transliteration ), lemmatization, and spelling error correction).", "labels": [], "entities": [{"text": "spelling error correction", "start_pos": 145, "end_pos": 170, "type": "TASK", "confidence": 0.6066399315992991}]}, {"text": "The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data.", "labels": [], "entities": []}, {"text": "While there are exceptions (e.g.,), most state-of-the-art modelings (e.g.,) view string transduction as a two-stage process in which string pairs (x, y) in the training data are first aligned, and then a subsequent (e.g., sequence labeling) module is learned on the aligned data.", "labels": [], "entities": [{"text": "string transduction", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7386891841888428}]}, {"text": "ph oe n ix f in I ks: Sample monotone many-to-many alignment between x = phoenix and y = finIks.", "labels": [], "entities": []}, {"text": "State-of-the-art alignments in G2P are characterized by the following properties: (i) Alignments are monotone in that the ordering of characters in input and output sequences is preserved by the alignments.", "labels": [], "entities": []}, {"text": "Furthermore, they are many-to-many in the sense that several x sequence characters maybe matched up with several y sequence characters as illustrated in.", "labels": [], "entities": []}, {"text": "(ii) The alignment is a latent variable and learnt in an unsupervised manner from pairs of strings in the training data.", "labels": [], "entities": []}, {"text": "(iii) The unsupervised alignment models are unigram alignment models insofar as the overall score that the alignment model assigns an alignment is the same for all orderings of the matched-up subsequences (context independence).", "labels": [], "entities": []}, {"text": "To illustrate point (iii), consider, in the field of lemmatization, the case of aligning an inflected word form with the extended infinitive in German, such as absagt ('rejects') with abzusagen ('to reject').", "labels": [], "entities": []}, {"text": "Critically, the insertion -zu-appears in infixal position and a plausible alignment might be as in.", "labels": [], "entities": [{"text": "insertion", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9634036421775818}]}, {"text": "Then, correctly aligning certain ab s a gt ab zu s a g en: Alignment between absagen and abzusagen.", "labels": [], "entities": []}, {"text": "Empty string denoted by . analogous forms such as zusagt ('accepts') with their corresponding extended infinitive zuzusagen ('to accept') is beyond the scope of a unigram alignment model since this cannot distinguish the linguistically correct alignment from the following linguistically incorrect alignment z u s a gt zu z u s a g en precisely because it has no notion of context.", "labels": [], "entities": []}, {"text": "In this work, we firstly address bigram alignment models in G2P.", "labels": [], "entities": [{"text": "bigram alignment", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7579270601272583}]}, {"text": "We investigate whether there are phenomena in G2P that require bigram alignment models and, more generally, whether bigram alignment models produce better alignmentswith respect to a human gold standard -than unigram alignment models within the G2P setting.", "labels": [], "entities": []}, {"text": "We do so, secondly, in a supervised setting where the model learns from gold-standard alignments.", "labels": [], "entities": []}, {"text": "While this may seem an odd scenario at first sight, modern alignment toolkits in the related field of machine translation typically include the possibility to learn both in a supervised and unsupervised manner (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7485220432281494}]}, {"text": "The rationale behind supervised learning models maybe that they perform better than unsupervised models, and if alignment quality has a large impact upon subsequent string translation performance, then a supervised model maybe a suitable alternative.", "labels": [], "entities": [{"text": "string translation", "start_pos": 165, "end_pos": 183, "type": "TASK", "confidence": 0.7219700813293457}]}, {"text": "Thirdly, we investigate how alignment quality affects overall G2P performance.", "labels": [], "entities": []}, {"text": "This allows us to address whether it is worthwhile to work on better alignment models, which bigram and supervised alignment models promise to be.", "labels": [], "entities": []}, {"text": "To our knowledge, all three outlined aspects of alignments -bigram models, supervised learning, and systematically estimating the relationship between alignment quality and overall string transduction performance -are novel in the G2P setting and its related fields as outlined; however, see also the related work section.", "labels": [], "entities": []}, {"text": "This work is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents definitions and algorithms for uni-and bigram alignment models.", "labels": [], "entities": [{"text": "uni-and bigram alignment", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.5711317956447601}]}, {"text": "Section 3 surveys related work.", "labels": [], "entities": []}, {"text": "Section 4 presents our data and Section 5 our experiments.", "labels": [], "entities": []}, {"text": "We conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 5: Unigram model and its alignment accura- cies in % for various training sizes.", "labels": [], "entities": [{"text": "alignment accura- cies", "start_pos": 32, "end_pos": 54, "type": "METRIC", "confidence": 0.9361479729413986}]}, {"text": " Table 6: Bigram model and its alignment accura- cies in % for various training sizes.", "labels": [], "entities": [{"text": "alignment accura- cies", "start_pos": 31, "end_pos": 53, "type": "METRIC", "confidence": 0.9316893070936203}]}, {"text": " Table 7: Systems, alignment accuracies of corre- sponding produced alignments and transcription  accuracy of Phonetisaurus and DirecTL+ when  trained with the respective alignments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9305000901222229}]}, {"text": " Table 9: Overall G2P accuracy in % as a function  training size of aligned data and alignment system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9686017036437988}]}]}