{"title": [{"text": "Reinforcing the Topic of Embeddings with Theta Pure Dependence for Text Classification *", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.8166713416576385}]}], "abstractContent": [{"text": "For sentiment classification, it is often recognized that embedding based on dis-tributional hypothesis is weak in capturing sentiment contrast-contrasting words may have similar local context.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.9609374701976776}]}, {"text": "Based on broader context, we propose to incorporate Theta Pure Dependence (TPD) into the Paragraph Vector method to reinforce topical and sentimental information.", "labels": [], "entities": [{"text": "Theta Pure Dependence (TPD)", "start_pos": 52, "end_pos": 79, "type": "METRIC", "confidence": 0.8501390417416891}]}, {"text": "TPD has a theoretical guarantee that the word dependency is pure, i.e., the dependence pattern has the integral meaning whose underlying distribution cannot be conditionally factorized.", "labels": [], "entities": [{"text": "TPD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6309874653816223}]}, {"text": "Our method outperforms the state-of-the-art performance on text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.8487783273061117}]}], "introductionContent": [{"text": "Word embeddings can be learned by training a neural probabilistic language model or a unified neural network architecture for various NLP tasks ().", "labels": [], "entities": []}, {"text": "In global context-aware neural language model, the global context vector is a weighted average of all word embeddings of a single document/paragraph.", "labels": [], "entities": []}, {"text": "After trained with all word embeddings belonging to the current paragraph, a resulting Paragraph Vector can be obtained.", "labels": [], "entities": []}, {"text": "Actually, Le and Mikolov's Paragraph Vector () is trained based on the log-linear neural language model ().", "labels": [], "entities": []}, {"text": "For text classification, using a straightforward extension of language model (e.g. Le and Mikolov's Paragraph Vector) is considered not to be sensible.", "labels": [], "entities": [{"text": "text classification", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.843517392873764}]}, {"text": "Embeddings learned for text classification should be very different from that learned for language modeling.", "labels": [], "entities": [{"text": "text classification", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7741530537605286}, {"text": "language modeling", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7352779805660248}]}, {"text": "For example, language * Corresponding authors: Yuexian Hou and Peng Zhang.", "labels": [], "entities": []}, {"text": "models often calculate the probability of a sentence, therefore this is a good movie and this is a bad movie may not be discriminated from each other.", "labels": [], "entities": []}, {"text": "In sentiment analysis task, the semantic representation of words needs to tell word good from bad, even if the two words have the same local context.", "labels": [], "entities": [{"text": "sentiment analysis task", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.9439031481742859}]}, {"text": "For this reason, the local dependency is insufficient to model topical or sentiment information.", "labels": [], "entities": []}, {"text": "Fortunately, if we have the global context of good like interesting or amazing, the sentiment meaning of the embedding will be explicit.", "labels": [], "entities": []}, {"text": "However, the training of log-linear neural language model is based on local word dependencies (e.g., the co-occurrence of the words in a local window).", "labels": [], "entities": []}, {"text": "Thus, Paragraph Vector cannot explicitly model the word dependencies for those words that do not frequently appear in a local window but are actually closely dependent on each other.", "labels": [], "entities": []}, {"text": "In this paper, our aim is to extend the Paragraph Vector with global context which can capture topical or sentiment information effectively.", "labels": [], "entities": [{"text": "Paragraph Vector", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.7498699724674225}]}, {"text": "However, if one explicitly considers the dependency patterns that are beyond the local window level, there is a possibility that the noisy dependency patterns can be involved and modeled in the distributed representation methods.", "labels": [], "entities": []}, {"text": "Moreover, there should bean unique and explicit topical meaning in the patterns to guarantee no ambiguity in the global context.", "labels": [], "entities": []}, {"text": "Therefore, we need a dependency mining method that not only models the long range dependency patterns, but also provides a theoretical guarantee that the dependency patterns are pure.", "labels": [], "entities": [{"text": "dependency mining", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.8668865859508514}]}, {"text": "Here, the \"pure\" dependency pattern is an integral semantic meaning/concept that cannot be factorized into sub dependency patterns.", "labels": [], "entities": []}, {"text": "In the language of statistics, Conditional Pure Dependence (CPD) means that the underlying distribution of the dependency patterns cannot be factorized under certain conditions (e.g., priors, observed words, etc.).", "labels": [], "entities": []}, {"text": "It has been proved that CPD is the high-level pure dependence in ().", "labels": [], "entities": []}, {"text": "However, judging CPD is NP-hard).", "labels": [], "entities": [{"text": "judging CPD", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.6079474091529846}]}, {"text": "Fortunately, Theta Pure Dependence (TPD) is the sufficient criteria of CPD and can be identified in O(N) time, where N is the number of words (.", "labels": [], "entities": [{"text": "Theta Pure Dependence (TPD)", "start_pos": 13, "end_pos": 40, "type": "METRIC", "confidence": 0.9242640137672424}]}, {"text": "This finding motivates us to adopt TPD as the global context.", "labels": [], "entities": []}, {"text": "Moreover, compared with other conventional cooccurrence-based methods, such as the Apriori algorithm, TPD based on the Information Geometry (IG) framework has a solid theoretical interpretations in statistics to guarantee the dependence is pure.", "labels": [], "entities": []}], "datasetContent": [{"text": "Apriori (not a pure dependency method) is contrastively adopted to implement Glo-PV-DBOW.", "labels": [], "entities": []}, {"text": "Glo-PV-DBOW-TPD and Glo-PV-DBOW-Apri are all evaluated in two text classification tasks: sentiment analysis and topic discovery.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.9500932693481445}, {"text": "topic discovery", "start_pos": 112, "end_pos": 127, "type": "TASK", "confidence": 0.8865315616130829}]}, {"text": "The suffix (e.g., -2, -5) of our global method name denotes the order of dependency (the number of words in a dependence pattern).", "labels": [], "entities": []}, {"text": "The order of dependency is changed because we want to show the superiority of the high-order TPD.", "labels": [], "entities": []}, {"text": "The high-order TPD provides the more rich and explicit global context than the lower-order one since the high-order TPD cannot be reduced to the random coincidence of lower-order dependencies.", "labels": [], "entities": []}, {"text": "We cross-validate the hyperparameters and set the local context window size as 10, the dimension of embeddings as 100.", "labels": [], "entities": []}, {"text": "In sentiment analysis task, Apriori's minimum support and TPD's theta 0 is respectively set as 0.004 and 1.4.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.9699995815753937}, {"text": "TPD", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.7069200277328491}]}, {"text": "While in topic discovery task, Apriori's minimum support and TPD's theta 0 is around 0.020 and 2.0 respectively.", "labels": [], "entities": [{"text": "topic discovery task", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.8953860799471537}, {"text": "TPD", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.6571416854858398}, {"text": "theta 0", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.8612493872642517}]}, {"text": "Since the classification accuracy of the approaches compared is a single result, we do not include any results for test of significance in our method and only report the average accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.720493495464325}, {"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9879273772239685}]}], "tableCaptions": [{"text": " Table 1: The performance of our method com- pared with other approaches on the IMDB dataset.", "labels": [], "entities": [{"text": "IMDB dataset", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.9704374670982361}]}, {"text": " Table 2.  Compared with  Confidence-weighted (", "labels": [], "entities": []}, {"text": " Table 3: Nearest neighbors of words ranking list  based on cosine similarity.", "labels": [], "entities": []}]}