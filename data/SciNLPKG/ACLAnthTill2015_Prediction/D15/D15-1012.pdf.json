{"title": [], "abstractContent": [{"text": "The task of cross-language document summarization is to create a summary in a target language from documents in a different source language.", "labels": [], "entities": [{"text": "cross-language document summarization", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.6519186993439993}]}, {"text": "Previous methods only involve direct extraction of automatically translated sentences from the original documents.", "labels": [], "entities": []}, {"text": "Inspired by phrase-based machine translation, we propose a phrase-based model to simultaneously perform sentence scoring, extraction and compression.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.6508214771747589}, {"text": "sentence scoring", "start_pos": 104, "end_pos": 120, "type": "TASK", "confidence": 0.7245192378759384}]}, {"text": "We design a greedy algorithm to approximately optimize the score function.", "labels": [], "entities": []}, {"text": "Experimental results show that our methods outperform the state-of-the-art extractive systems while maintaining similar grammatical quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of cross-language summarization is to produce a summary in a target language from documents written in a different source language.", "labels": [], "entities": [{"text": "cross-language summarization", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6786596477031708}]}, {"text": "This task is particularly useful for readers to quickly get the main idea of documents written in a source language that they are not familiar with.", "labels": [], "entities": []}, {"text": "Following, we focus on English-toChinese summarization in this work.", "labels": [], "entities": [{"text": "English-toChinese summarization", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.4759248048067093}]}, {"text": "The simplest and the most straightforward way to perform cross-language summarization is pipelining general summarization and machine translation.", "labels": [], "entities": [{"text": "cross-language summarization", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.5968151688575745}, {"text": "pipelining general summarization", "start_pos": 89, "end_pos": 121, "type": "TASK", "confidence": 0.7204911311467489}, {"text": "machine translation", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.766146332025528}]}, {"text": "Such systems either translate all the documents before running generic summarization algorithms on the translated documents, or summarize from the original documents and then only translate the produced summary into the target language.", "labels": [], "entities": []}, {"text": "show that such pipelining approaches are inferior to methods that utilize information from both sides.", "labels": [], "entities": []}, {"text": "In that work, the author proposes graph-based models and achieves fair amount of improvement.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, no previous work of this task tries to focus on summarization beyond pure sentence extraction.", "labels": [], "entities": [{"text": "summarization", "start_pos": 87, "end_pos": 100, "type": "TASK", "confidence": 0.9886753559112549}, {"text": "sentence extraction", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.6868719458580017}]}, {"text": "On the other hand, cross-language summarization can be seen as a special kind of machine translation: translating the original documents into a brief summary in a different language.", "labels": [], "entities": [{"text": "cross-language summarization", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.6855562329292297}, {"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7519424259662628}]}, {"text": "Inspired by phrase-based machine translation models (, we propose a phrase-based scoring scheme for cross-language summarization in this work.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.6207533379395803}, {"text": "cross-language summarization", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.7406792342662811}]}, {"text": "Since our framework is based on phrases, we are not limited to produce extractive summaries.", "labels": [], "entities": []}, {"text": "We can use the scoring scheme to perform joint sentence selection and compression.", "labels": [], "entities": [{"text": "joint sentence selection", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.6252949833869934}]}, {"text": "Unlike typical sentence compression methods, our proposed algorithm does not require additional syntactic preprocessing such as part-of-speech tagging or syntactic parsing.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7209559082984924}, {"text": "part-of-speech tagging", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.7254821360111237}, {"text": "syntactic parsing", "start_pos": 154, "end_pos": 171, "type": "TASK", "confidence": 0.7750727832317352}]}, {"text": "We only utilize information from translated texts with phrase alignments.", "labels": [], "entities": [{"text": "phrase alignments", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7158945649862289}]}, {"text": "The scoring function consists of a submodular term of compressed sentences and a bounded distortion penalty term.", "labels": [], "entities": [{"text": "bounded distortion penalty term", "start_pos": 81, "end_pos": 112, "type": "METRIC", "confidence": 0.7405743598937988}]}, {"text": "We design a greedy procedure to efficiently get approximate solutions.", "labels": [], "entities": []}, {"text": "For experimental evaluation, we use the DUC2001 dataset with manually translated reference Chinese summaries.", "labels": [], "entities": [{"text": "DUC2001 dataset", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.9890658557415009}]}, {"text": "Results based on the ROUGE metrics show the effectiveness of our proposed methods.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.8008978366851807}]}, {"text": "We also conduct manual evaluation and the results suggest that the linguistic quality of produced summaries is not decreased by too much, compared with extractive counterparts.", "labels": [], "entities": []}, {"text": "In some cases, the grammatical smoothness can even be improved by compression.", "labels": [], "entities": []}, {"text": "The contributions of this paper include: \u2022 Utilizing the phrase alignment information, we design a scoring scheme for the crosslanguage document summarization task.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7798813581466675}, {"text": "crosslanguage document summarization task", "start_pos": 122, "end_pos": 163, "type": "TASK", "confidence": 0.80166856944561}]}, {"text": "\u2022 We design an efficient greedy algorithm to generate summaries.", "labels": [], "entities": []}, {"text": "The greedy algorithm is partially submodular and has a provable constant approximation factor to the optimal solution up to a small constant.", "labels": [], "entities": []}, {"text": "\u2022 We achieve state-of-the-art results using the extractive counterpart of our compressive summarization framework.", "labels": [], "entities": []}, {"text": "Performance in terms of ROUGE metrics can be significantly improved when simultaneously performing extraction and compression.", "labels": [], "entities": []}], "datasetContent": [{"text": "We will report the performance of our compressive solution, denoted as PBCS (for Phrase-Based Compressive Summarization), with comparisons of the following systems: \u2022 PBES: The acronym comes from PhraseBased Extractive Summarization.", "labels": [], "entities": [{"text": "Phrase-Based Compressive Summarization)", "start_pos": 81, "end_pos": 120, "type": "TASK", "confidence": 0.7495649605989456}, {"text": "PhraseBased Extractive Summarization", "start_pos": 196, "end_pos": 232, "type": "TASK", "confidence": 0.57329394419988}]}, {"text": "It is the extractive counterpart of our solution without calling Algorithm 2.", "labels": [], "entities": []}, {"text": "\u2022 Baseline (EN): This baseline relies on merely the English-side information for En-glish sentence ranking in the original documents.", "labels": [], "entities": [{"text": "Baseline (EN)", "start_pos": 2, "end_pos": 15, "type": "METRIC", "confidence": 0.6582333743572235}]}, {"text": "The scoring function is designed to be document frequencies of English bigrams, which is similar to the second term in our proposed sentence scoring function in Section 3.1 and is submodular.", "labels": [], "entities": []}, {"text": "The extracted English summary is finally automatically translated into the corresponding Chinese summary.", "labels": [], "entities": []}, {"text": "This is also known as the summary translation scheme.", "labels": [], "entities": [{"text": "summary translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.848937600851059}]}, {"text": "\u2022 Baseline (CN): This baseline relies on merely the Chinese-side information for Chinese sentence ranking.", "labels": [], "entities": [{"text": "Chinese sentence ranking", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.5896498163541158}]}, {"text": "The scoring function is similarly defined by document frequency of Chinese bigrams.", "labels": [], "entities": [{"text": "document frequency", "start_pos": 45, "end_pos": 63, "type": "METRIC", "confidence": 0.7938823401927948}]}, {"text": "The Chinese summary sentences are then directly extracted from the translated Chinese documents.", "labels": [], "entities": []}, {"text": "This is also known as the document translation scheme.", "labels": [], "entities": [{"text": "document translation", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7735950350761414}]}, {"text": "\u2022 CoRank: We reimplement the graph-based CoRank algorithm, which gives the state-ofthe-art performance on the same DUC 2001 dataset for comparison.", "labels": [], "entities": [{"text": "DUC 2001 dataset", "start_pos": 115, "end_pos": 131, "type": "DATASET", "confidence": 0.9886393745740255}]}, {"text": "\u2022 Baseline (ENcomp): This is a compressive baseline where the extracted English sentences in Baseline (EN) will be compressed before being translated to Chinese.", "labels": [], "entities": []}, {"text": "The compression process follows from an integer linear program as described by.", "labels": [], "entities": []}, {"text": "This baseline gives strong performance as we have found on English DUC 2001 dataset as well as other monolingual datasets.", "labels": [], "entities": [{"text": "English DUC 2001 dataset", "start_pos": 59, "end_pos": 83, "type": "DATASET", "confidence": 0.9161243140697479}]}, {"text": "We experiment with two kinds of summary budgets for comparative study.", "labels": [], "entities": []}, {"text": "The first one is limiting the summary length to be no more than five sentences.", "labels": [], "entities": []}, {"text": "The second one is limiting the total number of Chinese characters of each produced summary to be no more than 300.", "labels": [], "entities": []}, {"text": "They will be addressed as Sentence Budgeting and Character Budgeting in the experimental results respectively.", "labels": [], "entities": [{"text": "Sentence Budgeting", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.9238340258598328}]}, {"text": "Similar to traditional summarization tasks, we use the ROUGE metrics for automatic evaluation of all systems in comparison.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.9300926029682159}, {"text": "ROUGE", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9300822615623474}]}, {"text": "The ROUGE metrics measure summary quality by counting overlapping word units (e.g. n-grams) between the candidate summary and the reference summary.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.8179247379302979}]}, {"text": "Following previous work in the same task, we report the following ROUGE F-measure scores: ROUGE-1 (unigrams), ROUGE-2 (bigrams), ROUGE-W (weighted longest common subsequence; weight=1.2), ROUGE-L (longest common subsequences), and ROUGE-SU4 (skip bigrams with a maximum distance of 4).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.966945469379425}, {"text": "ROUGE-2", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9754335880279541}, {"text": "ROUGE-W", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.9654918313026428}, {"text": "ROUGE-SU4", "start_pos": 231, "end_pos": 240, "type": "METRIC", "confidence": 0.8822904825210571}]}, {"text": "Here we investigate two kinds of ROUGE metrics for Chinese: ROUGE metrics based on words (after Chinese word segmentation) and ROUGE metrics based on singleton Chinese characters.", "labels": [], "entities": []}, {"text": "The latter metrics will not suffer from the problem of word segmentation inconsistency.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7277301698923111}]}, {"text": "To compare our method with extractive baselines in terms of information loss and grammatical quality, we also ask three native Chinese students as annotators to carryout manual evaluation.", "labels": [], "entities": []}, {"text": "The aspects considered during evaluation include Grammaticality (GR), Non-Redundancy (NR), Referential Clarity (RC), Topical Focus (TF) and Structural Coherence (SC).", "labels": [], "entities": []}, {"text": "Each aspect is rated with scores from 1 (poor) to 5 (good) . This evaluation is performed on the same random sample of 10 document sets from the DUC 2001 dataset.", "labels": [], "entities": [{"text": "DUC 2001 dataset", "start_pos": 145, "end_pos": 161, "type": "DATASET", "confidence": 0.9888688127199808}]}, {"text": "One group of the gold-standard summaries is left out for evaluation of human-level performance.", "labels": [], "entities": []}, {"text": "The other two groups are shown to the annotators, giving them a sense of topics talked about in the document sets.", "labels": [], "entities": []}, {"text": "display the ROUGE results for our proposed methods and the baseline methods, including both word-based and character-based evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.988710343837738}]}, {"text": "We also conduct pairwise t-test and find that almost all the differences between PBCS and other systems are statistically significant with p 0.01 5 except for the ROUGE-W metric.", "labels": [], "entities": [{"text": "ROUGE-W", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.9758654832839966}]}], "tableCaptions": [{"text": " Table 1: Results of word-based ROUGE evaluation", "labels": [], "entities": []}, {"text": " Table 2: Results of character-based ROUGE evaluation", "labels": [], "entities": []}]}