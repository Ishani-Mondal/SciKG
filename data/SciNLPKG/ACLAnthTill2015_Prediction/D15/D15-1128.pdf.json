{"title": [], "abstractContent": [{"text": "This paper proposes a method for hierarchical phrase-based stream decoding.", "labels": [], "entities": [{"text": "hierarchical phrase-based stream decoding", "start_pos": 33, "end_pos": 74, "type": "TASK", "confidence": 0.5738614201545715}]}, {"text": "A stream decoder is able to take a continuous stream of tokens as input, and segments this stream into word sequences that are translated and output as a stream of target word sequences.", "labels": [], "entities": []}, {"text": "Phrase-based stream decoding techniques have been shown to be effective as a means of simultaneous interpretation.", "labels": [], "entities": [{"text": "simultaneous interpretation", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.5933449566364288}]}, {"text": "In this paper we transfer the essence of this idea into the framework of hierarchical machine translation.", "labels": [], "entities": [{"text": "hierarchical machine translation", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.6507376233736674}]}, {"text": "The hierarchical decoding framework organizes the decoding process into a chart; this structure is naturally suited to the process of stream decoding, leading to an efficient stream decoding algorithm that searches a restricted subspace containing only relevant hypotheses.", "labels": [], "entities": []}, {"text": "Furthermore, the de-coder allows more explicit access to the word reordering process that is of critical importance in decoding while interpreting.", "labels": [], "entities": []}, {"text": "The decoder was evaluated on TED talk data for English-Spanish and English-Chinese.", "labels": [], "entities": [{"text": "TED talk data", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.892912228902181}]}, {"text": "Our results show that like the phrase-based stream decoder, the hierarchical is capable of approaching the performance of the underlying hierarchical phrase-based machine translation de-coder, at useful levels of latency.", "labels": [], "entities": [{"text": "phrase-based machine translation de-coder", "start_pos": 150, "end_pos": 191, "type": "TASK", "confidence": 0.7189702540636063}]}, {"text": "In addition the hierarchical approach appeared to be robust to the difficulties presented by the more challenging English-Chinese task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation traditionally operates on sentence segmented input.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7372731367746989}]}, {"text": "This technology has advanced to the point where it is becoming capable enough to be useful for many applications.", "labels": [], "entities": []}, {"text": "However, this approach maybe unsuitable for simultaneous interpretation where the machine translation system is required to provide translations within a reasonably short space of time after words have been spoken.", "labels": [], "entities": [{"text": "simultaneous interpretation", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.6363387405872345}, {"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7350594699382782}]}, {"text": "Under this type of constraint, it may not be possible to wait for the end of the sentence before translating, and segmentation at the sub-sentential level maybe required as a consequence.", "labels": [], "entities": []}, {"text": "This segmentation process is difficult, even for skilled human interpreters, and presents a major challenge to a machine since in addition to the translation process, decisions need to be made about when to commit to outputting a partial translation.", "labels": [], "entities": []}, {"text": "Such decisions are critical since once such an output is made it can be difficult and highly undesirable to correct it later if it is in error.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our stream decoder was implemented within the framework of the AUGUSTUS decoder, a hierarchical statistical machine translation decoder) that operates in a similar manner to the moses-chart decoder provided in the Moses machine translation toolkit ().", "labels": [], "entities": [{"text": "AUGUSTUS decoder", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.9202980995178223}]}, {"text": "The training procedure was quite typical: 5-gram language models were used, trained with modified English input stream: ...", "labels": [], "entities": []}, {"text": "we want to encourage a world of creators of inventors of contributors because this world that we live in this interactive world is ours ...", "labels": [], "entities": []}, {"text": "Kneser-Ney smoothing; MERT was used to train the log-linear weights of the models; the decoding was performed with a distortion limit of 20 words.", "labels": [], "entities": [{"text": "Kneser-Ney smoothing", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5760568678379059}, {"text": "MERT", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9910629987716675}]}, {"text": "To allow the results to be directly comparable to those in (), the talk level BLEU score () was used to evaluate the machine translation quality in all experiments.", "labels": [], "entities": [{"text": "talk level BLEU score", "start_pos": 67, "end_pos": 88, "type": "METRIC", "confidence": 0.7901583313941956}, {"text": "machine translation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.718862920999527}]}], "tableCaptions": []}