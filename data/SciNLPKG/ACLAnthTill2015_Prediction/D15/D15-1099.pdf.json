{"title": [{"text": "Multi-label Text Categorization with Joint Learning Predictions-as-Features Method", "labels": [], "entities": []}], "abstractContent": [{"text": "Multi-label text categorization is a type of text categorization, where each document is assigned to one or more categories.", "labels": [], "entities": [{"text": "Multi-label text categorization", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6720727284749349}]}, {"text": "Recently , a series of methods have been developed , which train a classifier for each label, organize the classifiers in a partially ordered structure and take predictions produced by the former classifiers as the latter classifiers' features.", "labels": [], "entities": []}, {"text": "These predictions-as-features style methods model high order label dependencies and obtain high performance.", "labels": [], "entities": []}, {"text": "Nevertheless, the predictions-as-features methods suffer a drawback.", "labels": [], "entities": []}, {"text": "When training a classifier for one label, the predictions-as-features methods can model dependencies between former labels and the current label, but they can't model dependencies between the current label and the latter labels.", "labels": [], "entities": []}, {"text": "To address this problem, we propose a novel joint learning algorithm that allows the feedbacks to be propagated from the classifiers for latter labels to the classifier for the current label.", "labels": [], "entities": []}, {"text": "We conduct experiments using real-world tex-tual data sets, and these experiments illustrate the predictions-as-features models trained by our algorithm outperform the original models.", "labels": [], "entities": []}], "introductionContent": [{"text": "The multi-label text categorization is a type of text categorization, where each document is assigned to one or more categories simultaneously.", "labels": [], "entities": []}, {"text": "The multi-label setting is common and useful in the real world.", "labels": [], "entities": []}, {"text": "For example, in the news categorization task, a newspaper article concerning global warming can be classified into two categories simultaneously, namely environment and science.", "labels": [], "entities": []}, {"text": "For another example, in the task of classifying music lyrics into emotions, a song's lyrics can deliver happiness and excitement simultaneously.", "labels": [], "entities": [{"text": "classifying music lyrics into emotions", "start_pos": 36, "end_pos": 74, "type": "TASK", "confidence": 0.8542882800102234}]}, {"text": "The research about the multi-label text categorization attracts increasing attention.", "labels": [], "entities": [{"text": "multi-label text categorization", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.6806715130805969}]}, {"text": "Recently, a series of predictions-as-features style methods have been developed, which train a classifier for each label, organize the classifiers in a partially ordered structure and take predictions produced by the former classifiers as the latter classifiers' features.", "labels": [], "entities": []}, {"text": "These predictions-as-features style methods model high order label dependencies ( and obtain high performance.", "labels": [], "entities": []}, {"text": "Classifier chain (CC)) and multi-label Learning by Exploiting lAbel Dependency (Lead) ( are two famous predictions-as-features methods.", "labels": [], "entities": []}, {"text": "CC organizes classifiers along a chain and LEAD organizes classifiers in a Bayesian network.", "labels": [], "entities": []}, {"text": "Besides, there are other works on extending the predictions-as-features methods.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the predictionsas-features style methods.", "labels": [], "entities": []}, {"text": "The previous works of the predictions-asfeatures methods focus on learning the partially ordered structure.", "labels": [], "entities": []}, {"text": "When training a classifier for one label, predictions-as-features methods can model dependencies between former labels and the current label, but they can't model dependencies between the current label and the latter labels.", "labels": [], "entities": []}, {"text": "Consider the case of three labels.", "labels": [], "entities": []}, {"text": "We organize classifiers in a partially ordered structure shown in.", "labels": [], "entities": []}, {"text": "When training the classifier for the second label, the feature (the bold lines in consists of the origin feature and the prediction for the first label.", "labels": [], "entities": []}, {"text": "The information about the third label can't be incorporated.", "labels": [], "entities": []}, {"text": "It means that we only model the dependencies between the first label and the sec-: When training the classifier for the second label, the feature (the bold lines) consists of only the origin feature and the prediction for the first label.", "labels": [], "entities": []}, {"text": "In this time, it is impossible to model the dependencies between the second label and the third label.", "labels": [], "entities": []}, {"text": "ond label and that the dependencies between the second label and the third label is missing.", "labels": [], "entities": []}, {"text": "To address this problem, we propose a novel joint learning algorithm that allows the feedbacks to be propagated from the classifiers for latter labels to the classifier for the current label, so that the information about the latter labels can be incorporated.", "labels": [], "entities": []}, {"text": "It means that the proposed method can model, not only the dependencies between former labels and current label as the usual predictions-as-features methods, but also the dependencies between current label and latter labels.", "labels": [], "entities": []}, {"text": "Hence, the proposed method will improve the performance.", "labels": [], "entities": []}, {"text": "Our experiments illustrate the models trained by our algorithm outperform the original models.", "labels": [], "entities": []}, {"text": "You can find the code of this paper online 1 . The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the proposed method.", "labels": [], "entities": []}, {"text": "We conduct experiments to demonstrate the effectiveness of the proposed method in section 3.", "labels": [], "entities": []}, {"text": "Section 4 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments on four real world data sets: 1) the first data set is Slashdot).", "labels": [], "entities": []}, {"text": "The Slashdot data set is concerned about predicting multiple labels given science and technology news titles and partial blurbs mined from Slashdot.org.", "labels": [], "entities": [{"text": "Slashdot data set", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8900563518206278}, {"text": "predicting multiple labels given science and technology news titles", "start_pos": 41, "end_pos": 108, "type": "TASK", "confidence": 0.727960295147366}]}, {"text": "2) the second data set is Medical (.", "labels": [], "entities": [{"text": "Medical", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.7283401489257812}]}, {"text": "This data set involves the assignment of ICD-9-CM codes to radiology reports.", "labels": [], "entities": []}, {"text": "3) The third data set is Enron.", "labels": [], "entities": [{"text": "Enron", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.9582394361495972}]}, {"text": "The enron data set is a subset of the Enron Email Dataset, as labelled by the UC Berkeley Enron Email Analysis Project . It is concerned about classifying emails into some categories.", "labels": [], "entities": [{"text": "enron data set", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8072525560855865}, {"text": "Enron Email Dataset", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.9347957173983256}]}, {"text": "4) the fourth data set dataset n d: Multi-label data sets and associated statistics. is Tmc2007 (Srivastava and Zane-).", "labels": [], "entities": [{"text": "Tmc2007", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.5259905457496643}]}, {"text": "It is concerned about safety report categorization, which is to label aviation safety reports with respect to what types of problems they describe.", "labels": [], "entities": []}, {"text": "shows these multi-label data sets and associated statistics.", "labels": [], "entities": []}, {"text": "n denotes the size of the entire data set, d denotes the number of the bag-of-words features, m denotes the number of labels.", "labels": [], "entities": []}, {"text": "These data sets are available online 3 .  We use three common used evaluation metrics.", "labels": [], "entities": []}, {"text": "The Hamming loss is defined as the percentage of the wrong labels to the total number of labels.", "labels": [], "entities": []}, {"text": "where \u2206 denotes the symmetric difference of two sets, equivalent to XOR operator in Boolean logic.", "labels": [], "entities": [{"text": "XOR", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9369676113128662}]}, {"text": "The multi-label 0/1 loss is the exact match measure as it requires any predicted set of labels h(x x x) to match the true set of labels S exactly.", "labels": [], "entities": []}, {"text": "The 0/1 loss is defined as follows: Let p j and r j denote the precision and recall for the j-th label.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.999256432056427}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9987024068832397}]}, {"text": "The macro-averaged F score is a harmonic mean between precision and recall, defined as follows:", "labels": [], "entities": [{"text": "F score", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9316248595714569}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.999115526676178}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9965682029724121}]}], "tableCaptions": [{"text": " Table 2: Multi-label data sets and associated statis- tics.", "labels": [], "entities": []}, {"text": " Table 1: Performance (mean\u00b1std.) of each approach in terms of different evaluation metrics.", "labels": [], "entities": []}, {"text": " Table 3: The win/tie/loss results for the joint learn- ing algorithm against the original predictions-as- features methods in terms of different evaluation  metrics (pairwise t-test at 5% significance level).", "labels": [], "entities": []}, {"text": " Table 4: The average training time (in seconds) of  each approach", "labels": [], "entities": []}]}