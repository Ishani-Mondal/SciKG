{"title": [{"text": "Modeling Tweet Arrival Times using Log-Gaussian Cox Processes", "labels": [], "entities": [{"text": "Modeling Tweet Arrival", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8269421060880026}]}], "abstractContent": [{"text": "Research on modeling time series text corpora has typically focused on predicting what text will come next, but less well studied is predicting when the next text event will occur.", "labels": [], "entities": []}, {"text": "In this paper we address the latter case, framed as modeling continuous inter-arrival times under a log-Gaussian Cox process, a form of inhomo-geneous Poisson process which captures the varying rate at which the tweets arrive overtime.", "labels": [], "entities": []}, {"text": "In an application to rumour modeling of tweets surrounding the 2014 Ferguson riots, we show how inter-arrival times between tweets can be accurately predicted, and that incorporating textual features further improves predictions .", "labels": [], "entities": [{"text": "rumour modeling of tweets surrounding the 2014 Ferguson riots", "start_pos": 21, "end_pos": 82, "type": "TASK", "confidence": 0.8798125717375014}]}], "introductionContent": [{"text": "Twitter is a popular micro-blogging service which provides real-time information on events happening across the world.", "labels": [], "entities": []}, {"text": "Evolution of events overtime can be monitored therewith applications to disaster management, journalism etc.", "labels": [], "entities": [{"text": "disaster management", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7638014853000641}]}, {"text": "For example, Twitter has been used to detect the occurrence of earthquakes in Japan through user posts (.", "labels": [], "entities": []}, {"text": "Modeling the temporal dynamics of tweets provides useful information about the evolution of events.", "labels": [], "entities": []}, {"text": "Inter-arrival time prediction is a type of such modeling and has application in many settings featuring continuous time streaming text corpora, including journalism for event monitoring, real-time disaster monitoring and advertising on social media.", "labels": [], "entities": [{"text": "Inter-arrival time prediction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7317607800165812}, {"text": "real-time disaster monitoring", "start_pos": 187, "end_pos": 216, "type": "TASK", "confidence": 0.6020346283912659}]}, {"text": "For example, journalists track several rumours related to an event.", "labels": [], "entities": []}, {"text": "Predicted arrival times of tweets can be applied for ranking rumours according to their activity and narrow the interest to investigate a rumour with a short interarrival time over that of a longer one.", "labels": [], "entities": []}, {"text": "Modeling the inter-arrival time of tweets is a challenging task due to complex temporal patterns exhibited.", "labels": [], "entities": []}, {"text": "Tweets associated with an event stream arrive at different rates at different points in time.", "labels": [], "entities": []}, {"text": "For example, shows the arrival times (denoted by black crosses) of tweets associated with an example rumour around Ferguson riots in 2014.", "labels": [], "entities": [{"text": "Ferguson riots in 2014", "start_pos": 115, "end_pos": 137, "type": "DATASET", "confidence": 0.8846976608037949}]}, {"text": "Notice the existence of regions of both high and low density of arrival times over a one hour interval.", "labels": [], "entities": []}, {"text": "We propose to address inter-arrival time prediction problem with log-Gaussian Cox process (LGCP), an inhomogeneous Poisson process (IPP) which models tweets to be generated by an underlying intensity function which varies across time.", "labels": [], "entities": [{"text": "inter-arrival time prediction", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.6559114158153534}]}, {"text": "Moreover, it assumes a non-parametric form for the intensity function allowing the model complexity to depend on the data set.", "labels": [], "entities": []}, {"text": "We also provide an approach to consider textual content of tweets to model inter-arrival times.", "labels": [], "entities": []}, {"text": "We evaluate the models using Twitter rumours from the 2014 Ferguson unrest, and demonstrate that they provide good predictions for inter-arrival times, beating the baselines e.g. homogeneous Poisson Process, Gaussian Process regression and univariate Hawkes Process.", "labels": [], "entities": [{"text": "2014 Ferguson unrest", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.554371198018392}]}, {"text": "Even though the central application is rumours, one could apply the proposed approaches to model the arrival times of tweets corresponding to other types of memes, e.g. discussions about politics.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "Introduces log-Gaussian Cox process to predict tweet arrival times.", "labels": [], "entities": []}, {"text": "2. Demonstrates how incorporating text improves results of inter-arrival time prediction.", "labels": [], "entities": [{"text": "inter-arrival time prediction", "start_pos": 59, "end_pos": 88, "type": "TASK", "confidence": 0.6232715845108032}]}], "datasetContent": [{"text": "Data preprocessing In our experiments, we consider the first two hours of each rumour lifespan.", "labels": [], "entities": [{"text": "Data preprocessing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6298060715198517}]}, {"text": "The posts from the first hour of a target rumour is considered as observed (training data) and we predict the arrival times of tweets in the second hour.", "labels": [], "entities": []}, {"text": "We consider observations over equal sized time intervals of length six minutes in the rumour lifespan for learning the intensity function.", "labels": [], "entities": []}, {"text": "The text in the tweets is represented by using Brown cluster ids associated with the words.", "labels": [], "entities": []}, {"text": "This is obtained using 1000 clusters acquired on a large scale Twitter corpus ().", "labels": [], "entities": []}, {"text": "Evaluation metrics Let the arrival times predicted by a model be ( \u02c6 t 1 , . .", "labels": [], "entities": []}, {"text": ", \u02c6 t M ) and let the actual arrival times be (t 1 , . .", "labels": [], "entities": []}, {"text": "We introduce two metrics based on root mean squared error (RMSE) for evaluating predicted inter-arrival times.", "labels": [], "entities": [{"text": "root mean squared error (RMSE)", "start_pos": 34, "end_pos": 64, "type": "METRIC", "confidence": 0.8313871366637093}]}, {"text": "First is aligned root mean squared error (ARMSE), where we align the initial K = min(M, N ) arrival times and calculate the RMSE between such two subsequences.", "labels": [], "entities": [{"text": "aligned root mean squared error (ARMSE)", "start_pos": 9, "end_pos": 48, "type": "METRIC", "confidence": 0.9334375783801079}, {"text": "RMSE", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9149718880653381}]}, {"text": "The second is called penalized root mean squared error (PRMSE).", "labels": [], "entities": [{"text": "penalized root mean squared error (PRMSE)", "start_pos": 21, "end_pos": 62, "type": "METRIC", "confidence": 0.9118588045239449}]}, {"text": "In this metric we penalize approaches which predict a different number of inter-arrival times than the actual number.", "labels": [], "entities": []}, {"text": "The PRMSE metric is defined as the square root of the following expression.", "labels": [], "entities": []}, {"text": "The second and third term in  to 1000 (above the maximum count yielded by any rumour from our dataset), thus reducing the error from this method.", "labels": [], "entities": []}, {"text": "We also compare against Hawkes Process (HP)), a self exciting point process where an occurrence of a tweet increases the probability of tweets arriving soon afterwards.", "labels": [], "entities": [{"text": "Hawkes Process (HP))", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.7593525767326355}]}, {"text": "We consider a univariate Hawkes process where the intensity function is modeled as \u03bb i (t) = \u00b5 + ti j <t k time (t i j , t).", "labels": [], "entities": []}, {"text": "The kernel parameters and \u00b5 are learnt by maximizing the likelihood.", "labels": [], "entities": []}, {"text": "We apply the importance sampling algorithm discussed in Algorithm 1 for generating arrival times for Hawkess process.", "labels": [], "entities": []}, {"text": "We consider this baseline only in the single-task setting, where reference rumours are not considered.", "labels": [], "entities": []}, {"text": "LGCP settings In the case of LGCP, the model parameters of the intensity function associated with a rumour are learnt from the observed interarrival times from that rumour alone.", "labels": [], "entities": []}, {"text": "LGCP Pooled and LGCPTXT consider a different setting where this is learnt additionally using the interarrival times of all other rumours observed over the entire two hour life-span.", "labels": [], "entities": [{"text": "LGCP Pooled", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.972805917263031}, {"text": "LGCPTXT", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.935423731803894}]}, {"text": "Results reports the results of predicting arrival times of tweets in the second hour of the rumour lifecycle.", "labels": [], "entities": []}, {"text": "In terms of ARMSE, LGCP is the best method, performing better than LGCP-TXT (though not statistically significantly) and outperforming other approaches.", "labels": [], "entities": []}, {"text": "However, this metric does not penalize for the wrong number of predicted arrival times.", "labels": [], "entities": []}, {"text": "depicts an example rumour, where LGCP greatly overestimates the number of points in the interval of interest.", "labels": [], "entities": [{"text": "LGCP", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9467044472694397}]}, {"text": "Here, the three points from the ground truth (denoted by black crosses) and the initial three points predicted by the LGCP model (denoted by red pluses), happen to lie very close, yielding a low ARMSE error.", "labels": [], "entities": [{"text": "ARMSE error", "start_pos": 195, "end_pos": 206, "type": "METRIC", "confidence": 0.7847308218479156}]}, {"text": "However, LGCP predicts a large number of arrivals in this interval making it a bad model compared to LGCPTXT which predicts only four points (denoted by blue dots).", "labels": [], "entities": []}, {"text": "ARMSE fails to capture this and hence we use PRMSE.", "labels": [], "entities": [{"text": "ARMSE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9237039089202881}, {"text": "PRMSE", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.6525778770446777}]}, {"text": "Note that Hawkes Process is performing worse than the LGCP approach.", "labels": [], "entities": [{"text": "Hawkes Process", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.8575436770915985}]}, {"text": "According to PRMSE, LGCPTXT is the most successful method, significantly outperforming all other according to Wilcoxon signed rank test.", "labels": [], "entities": [{"text": "PRMSE", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.7986147403717041}]}, {"text": "depicts the behavior of LGCP and LGCP-TXT on rumour 39 with a larger number of points from the ground truth.", "labels": [], "entities": []}, {"text": "Here, LGCPTXT predicts relatively less number of arrivals than LGCP.", "labels": [], "entities": [{"text": "LGCP", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.8893131613731384}]}, {"text": "The performance of Hawkes Process is again worse than the LGCP approach.", "labels": [], "entities": [{"text": "Hawkes Process", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.8083149194717407}]}, {"text": "The self excitory nature of Hawkes process may not be appropriate for this dataset and setting, wherein the second hour the number of points tends to decrease as time passes.", "labels": [], "entities": []}, {"text": "We also note, that GPLIN performs very poorly according to PRMSE.", "labels": [], "entities": [{"text": "GPLIN", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.7743098139762878}, {"text": "PRMSE", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.8773418664932251}]}, {"text": "This is because the interarrival times predicted by GPLIN for several rumours become smaller as time grows resulting in a large number of arrival times.", "labels": [], "entities": [{"text": "GPLIN", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.9537032842636108}]}], "tableCaptions": [{"text": " Table 1: ARMSE and PRMSE between the true  event times and the predicted event times ex- pressed in minutes (lower is better) over the 114  Ferguson rumours, showing mean \u00b1 std. dev.  Key denotes significantly worse than LGCPTXT  method according to one-sided Wilcoxon signed  rank test (p < 0.05). In case of ARMSE, LGCP is  not significantly better than LGCP TXT according  to Wilcoxon test.", "labels": [], "entities": [{"text": "LGCP TXT", "start_pos": 357, "end_pos": 365, "type": "DATASET", "confidence": 0.8022077083587646}]}]}