{"title": [{"text": "Diversity in Spectral Learning for Natural Language Parsing", "labels": [], "entities": [{"text": "Natural Language Parsing", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6896238327026367}]}], "abstractContent": [{"text": "We describe an approach to create a diverse set of predictions with spectral learning of latent-variable PCFGs (L-PCFGs).", "labels": [], "entities": []}, {"text": "Our approach works by creating multiple spectral models where noise is added to the underlying features in the training set before the estimation of each model.", "labels": [], "entities": []}, {"text": "We describe three ways to decode with multiple models.", "labels": [], "entities": []}, {"text": "In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models.", "labels": [], "entities": []}, {"text": "Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6687889099121094}]}, {"text": "For English, we achieve the F 1 score of 90.18, and for German we achieve the F 1 score of 83.38.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9903313914934794}, {"text": "F 1 score", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9927949905395508}]}], "introductionContent": [{"text": "It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9985215067863464}]}, {"text": "Such problems include machine translation, syntactic parsing) and others).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8208055198192596}, {"text": "syntactic parsing", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7104059755802155}]}, {"text": "The main argument behind the use of such a diverse set of solutions (such as k-best list of parses fora natural language sentence) is the hope that each solution in the set is mostly correct.", "labels": [], "entities": []}, {"text": "Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions.", "labels": [], "entities": []}, {"text": "In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence.", "labels": [], "entities": []}, {"text": "More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.6334182322025299}]}, {"text": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 133, "end_pos": 140, "type": "TASK", "confidence": 0.9704557657241821}, {"text": "topic modeling", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.7791474759578705}]}, {"text": "showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm ().", "labels": [], "entities": []}, {"text": "Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine ().", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.6290056904157003}]}, {"text": "We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9987672567367554}, {"text": "natural language parsing", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6124541560808817}]}, {"text": "Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes.", "labels": [], "entities": []}, {"text": "Each such model, during test time, yields a single parse, and all parses are then used together in several ways to select a single best parse.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are twofold.", "labels": [], "entities": []}, {"text": "First, we present an algorithm for estimating L-PCFGs, akin to the spectral algorithm of, but simpler to understand and implement.", "labels": [], "entities": []}, {"text": "This algorithm has value for readers who are interested in learning more about spectral algorithms -it demonstrates some of the core ideas in spectral learning in a rather intuitive way.", "labels": [], "entities": [{"text": "spectral learning", "start_pos": 142, "end_pos": 159, "type": "TASK", "confidence": 0.7890940010547638}]}, {"text": "In addition, this algorithm leads to sparse grammar estimates and compact models.", "labels": [], "entities": []}, {"text": "Second, we describe how a diverse set of predictors can be used with spectral learning techniques.", "labels": [], "entities": []}, {"text": "Our approach relies on adding noise to the feature functions that help the spectral algorithm compute the latent states.", "labels": [], "entities": []}, {"text": "Our noise schemes are similar to those described by.", "labels": [], "entities": []}, {"text": "We add noise to the whole training data, then train a model using our algorithm (or other spectral algorithms;, and repeat this process multiple times.", "labels": [], "entities": []}, {"text": "We then use the set of parses we get from all models in a recombination step.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In \u00a72 we describe notation and background about L-PCFG parsing.", "labels": [], "entities": []}, {"text": "In \u00a73 we describe our new spectral algorithm for estimating L-PCFGs.", "labels": [], "entities": []}, {"text": "It is based on similar intuitions as older spectral algorithms for L-PCFGs.", "labels": [], "entities": []}, {"text": "In \u00a74 we describe the various noise schemes we use with our spectral algorithm and the spectral algorithm of.", "labels": [], "entities": []}, {"text": "In \u00a75 we describe how to decode with multiple models, each arising from a different noise setting.", "labels": [], "entities": []}, {"text": "In \u00a76 we describe our experiments with natural language parsing for English and German.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.7345333496729533}]}], "datasetContent": [{"text": "In this section, we describe parsing experiments with two languages: English and German.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9702296257019043}]}], "tableCaptions": [{"text": " Table 1: Results on section 22 (WSJ). MaxTre denotes decoding using maximal tree coverage, MaxMrg  denotes decoding using maximal marginal coverage, and MaxEnt denotes the use of a discriminative  reranker. Add, Mul and Dropout denote the use of additive Gaussian noise, multiplicative Gaussian  noise and dropout noise, respectively. The number of models used in the first three rows for the clustering  algorithm is 80: 20 for each \u03c3 \u2208 {0.05, 0.1, 0.15, 0.2}. For the spectral algorithm, it is 20, 5 for each \u03c3  (see footnotes). The number of latent states is m = 24. For All, we use all models combined from the  first three rows. The \"No noise\" baseline for the spectral algorithm is taken from", "labels": [], "entities": [{"text": "WSJ", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.9165794253349304}, {"text": "Mul", "start_pos": 213, "end_pos": 216, "type": "METRIC", "confidence": 0.9775062799453735}]}, {"text": " Table 2: Results on section 23 (English). The first  three results (Best) are taken with the best model  in each corresponding block in", "labels": [], "entities": []}, {"text": " Table 1. The last  three results (Hier) use a hierarchy of the above  tree combination methods in each block. It com- bines all MaxEnt results using the maximal tree  coverage (see text).", "labels": [], "entities": []}, {"text": " Table 4: Results on the test set for the German  data. The first three results (Best) are taken with  the best model in each corresponding block in Ta- ble 3. The last three results (Hier) use a hierarchy  of the above tree combination methods.", "labels": [], "entities": [{"text": "German  data", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.9119853973388672}]}]}