{"title": [{"text": "Answering Elementary Science Questions by Constructing Coherent Scenes using Background Knowledge", "labels": [], "entities": [{"text": "Answering Elementary Science Questions", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.9071698784828186}]}], "abstractContent": [{"text": "Much of what we understand from text is not explicitly stated.", "labels": [], "entities": []}, {"text": "Rather, the reader uses his/her knowledge to fill in gaps and create a coherent, mental picture or \"scene\" depicting what text appears to convey.", "labels": [], "entities": []}, {"text": "The scene constitutes an understanding of the text, and can be used to answer questions that go beyond the text.", "labels": [], "entities": []}, {"text": "Our goal is to answer elementary science questions, where this requirement is pervasive ; A question will often give a partial description of a scene and ask the student about implicit information.", "labels": [], "entities": []}, {"text": "We show that by using a simple \"knowledge graph\" representation of the question, we can leverage several large-scale linguistic resources to provide missing background knowledge , somewhat alleviating the knowledge bottleneck in previous approaches.", "labels": [], "entities": []}, {"text": "The coherence of the best resulting scene, built from a question/answer-candidate pair, reflects the confidence that the answer candidate is correct, and thus can be used to answer multiple choice questions.", "labels": [], "entities": []}, {"text": "Our experiments show that this approach outper-forms competitive algorithms on several datasets tested.", "labels": [], "entities": []}, {"text": "The significance of this work is thus to show that a simple \"knowl-edge graph\" representation allows aversion of \"interpretation as scene construc-tion\" to be made viable.", "labels": [], "entities": [{"text": "interpretation as scene construc-tion", "start_pos": 114, "end_pos": 151, "type": "TASK", "confidence": 0.669028989970684}]}], "introductionContent": [{"text": "Elementary grade science tests are challenging as they test a wide variety of commonsense knowledge that human beings largely take for granted, yet are very difficult for machines.", "labels": [], "entities": []}, {"text": "For example, consider a question from a NY Regents 4th Grade science test: * Work was done while the author was an intern at Allen Institute for Artificial Intelligence.", "labels": [], "entities": [{"text": "NY Regents 4th Grade science test", "start_pos": 40, "end_pos": 73, "type": "DATASET", "confidence": 0.8231633404890696}]}, {"text": "Question 1 \"When a baby shakes a rattle, it makes a noise.", "labels": [], "entities": []}, {"text": "Which form of energy was changed to sound energy?\"", "labels": [], "entities": []}, {"text": "[Answer: mechanical energy] Science questions are typically quite different from the entity-centric factoid questions extensively studied in the question answering (QA) community, e.g., \"In which year was Bill Clinton born?\").", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 145, "end_pos": 168, "type": "TASK", "confidence": 0.829879367351532}]}, {"text": "While factoid questions are usually answerable from text search or fact databases, science questions typically require deeper analysis.", "labels": [], "entities": []}, {"text": "A full understanding of the above question involves not just parsing and semantic interpretation; it involves adding implicit information to create an overall picture of the \"scene\" that the text is intended to convey, including facts such as: noise is a kind of sound, the baby is holding the rattle, shaking involves movement, the rattle is making the noise, movement involves mechanical energy, etc.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.7528996765613556}]}, {"text": "This mental ability to create a scene from partial information is at the heart of natural language understanding (NLU), which is essential for answering these kinds of question.", "labels": [], "entities": [{"text": "natural language understanding (NLU)", "start_pos": 82, "end_pos": 118, "type": "TASK", "confidence": 0.7807376384735107}]}, {"text": "It is also very difficult fora machine because it requires substantial world knowledge, and there are often many ways a scene can be elaborated.", "labels": [], "entities": []}, {"text": "We present a method for answering multiplechoice questions that implements a simple version of this.", "labels": [], "entities": []}, {"text": "A scene is represented as a \"knowledge graph\" of nodes (words) and relations, and the scene is elaborated with (node,relation,node) tuples drawn from three large-scale linguistic knowledge resources: WordNet, DART, and the FreeAssociation database ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 200, "end_pos": 207, "type": "DATASET", "confidence": 0.9619758129119873}]}, {"text": "These elaborations reflect the mental process of \"filling in the gaps\", and multiple choice questions can then be answered by finding which answer option creates the most coherent scene.", "labels": [], "entities": []}, {"text": "The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (, interpretation as ab-duction (), bridging anaphora), and paragraph understanding).", "labels": [], "entities": [{"text": "paragraph understanding", "start_pos": 199, "end_pos": 222, "type": "TASK", "confidence": 0.8701314330101013}]}, {"text": "These methods are inspiring, but have previously been limited by the lack of background knowledge to supply implicit information, and with the complexity of their representations.", "labels": [], "entities": []}, {"text": "To make progress, we have chosen to work with a simple \"knowledge graph\" representation of nodes (words) and edges (relations).", "labels": [], "entities": []}, {"text": "Although we lose some subtlety of expression, we gain the ability to leverage several vast resources of world knowledge to supply implicit information.", "labels": [], "entities": []}, {"text": "The significance of this work is thus to show that, by working with a simple \"knowledge graph\" representation, we can make a viable version of \"interpretation as scene construction\".", "labels": [], "entities": [{"text": "interpretation as scene construction", "start_pos": 144, "end_pos": 180, "type": "TASK", "confidence": 0.7424492985010147}]}, {"text": "Although the approach makes several simplifying assumptions, our experiments show that it outperforms competitive algorithms on several datasets of (real) elementary science questions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The system was developed using a dataset of natural (unedited) elementary science exam questions, and then tested on three similar, unseen (hidden) datasets.", "labels": [], "entities": []}, {"text": "Its performance was compared with two other state-of-the-art systems for this task.", "labels": [], "entities": []}, {"text": "As our system only fields questions where the answer options are all single words, we evaluate it, and the other systems, only on these subsets.", "labels": [], "entities": []}, {"text": "These subsets are in general easier than other questions, but this advantage is the same for all systems being compared so it is still a fair test.", "labels": [], "entities": []}, {"text": "The datasets used are the non-diagram, multiplechoice questions with single-word answer options drawn from the following exams: \u2022) Although these datasets are small (real exam questions of this type are in limited supply), the numbers are large enough to draw conclusions.", "labels": [], "entities": []}, {"text": "We compared our system (called SceneQA) with two other state-of-the-art systems for this task: \u2022 LSModel (Lexical semantics model): SVM combination of several language models (likelihood of answer given question) and information retrieval scores (score of top retrieved sentence matching question plus answer), trained on a set of questions plus answers.", "labels": [], "entities": []}, {"text": "(An expanded version of Section 4.3 of (Jansen et al., 2014)) \u2022 A*Rules: \"Prove\" the answer option from the question by applying lexical inference rules automatically extracted from science texts.", "labels": [], "entities": [{"text": "A", "start_pos": 64, "end_pos": 65, "type": "METRIC", "confidence": 0.9951116442680359}]}, {"text": "Select the option with the strongest \"proof\".: SceneQA outperforms two competitive systems on two of the three test sets.", "labels": [], "entities": []}, {"text": "The highlighted improvements are statistically significant.", "labels": [], "entities": []}, {"text": "The results (% scores, show SceneQA significantly outperforms the two other systems on two of the three test sets, including the largest, suggesting the approach has merit.", "labels": [], "entities": []}, {"text": "We also performed some case studies to identify what kinds of questions SceneQA does well on, relative to the baselines.", "labels": [], "entities": [{"text": "SceneQA", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.8565895557403564}]}, {"text": "In general, SceneQA performs well when the question words and the (correct) answer can be tightly related by background knowledge, including through intermediate nodes (words).", "labels": [], "entities": []}, {"text": "For example, in Question 2 below: Question 2 Which type of energy does a person use to pedal a bicycle?", "labels": [], "entities": []}, {"text": "(A) light (B) sound (C) mechanical (D) electrical the KB relates the correct answer \"mechanical\" to the question words \"energy\", \"pedal\", \"bicycle\", and the intermediate node \"power\" forming a tight graph.", "labels": [], "entities": []}, {"text": "In contrast, the other algorithms select the wrong answer \"light\" due to frequent mentions of \"bicycle lights\" in their supporting text corpora that confuses their algorithms.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: SceneQA outperforms two competitive  systems on two of the three test sets. The high- lighted improvements are statistically significant.", "labels": [], "entities": []}, {"text": " Table 3: SceneQA outperforms all the ablations  on two of the three test sets. The highlighted im- provements are statistically significant.", "labels": [], "entities": []}]}