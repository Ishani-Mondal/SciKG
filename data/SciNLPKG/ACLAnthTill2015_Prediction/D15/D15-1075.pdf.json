{"title": [{"text": "A large annotated corpus for learning natural language inference", "labels": [], "entities": [{"text": "learning natural language inference", "start_pos": 29, "end_pos": 64, "type": "TASK", "confidence": 0.5760395303368568}]}], "abstractContent": [{"text": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entail-ment and contradiction is a valuable testing ground for the development of semantic representations.", "labels": [], "entities": []}, {"text": "However, machine learning research in this area has been dramatically limited by the lack of large-scale resources.", "labels": [], "entities": []}, {"text": "To address this, we introduce the Stanford Natural Language Inference corpus, anew, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference corpus", "start_pos": 34, "end_pos": 76, "type": "DATASET", "confidence": 0.784413480758667}]}, {"text": "At 570K pairs, it is two orders of magnitude larger than all other resources of its type.", "labels": [], "entities": []}, {"text": "This increase in scale allows lexicalized classi-fiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.", "labels": [], "entities": []}], "introductionContent": [{"text": "The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning, from the lexicon to the content of entire texts.", "labels": [], "entities": []}, {"text": "Thus, natural language inference (NLI) -characterizing and using these relations in computational systems () -is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.", "labels": [], "entities": [{"text": "natural language inference (NLI)", "start_pos": 6, "end_pos": 38, "type": "TASK", "confidence": 0.7384376029173533}, {"text": "information retrieval to semantic parsing", "start_pos": 145, "end_pos": 186, "type": "TASK", "confidence": 0.6740329980850219}, {"text": "commonsense reasoning", "start_pos": 190, "end_pos": 211, "type": "TASK", "confidence": 0.8576042056083679}]}, {"text": "NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks.", "labels": [], "entities": []}, {"text": "In recent years, it has become an important testing ground for approaches employing distributed word and phrase representations.", "labels": [], "entities": [{"text": "distributed word and phrase representations", "start_pos": 84, "end_pos": 127, "type": "TASK", "confidence": 0.6057645380496979}]}, {"text": "Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g.,), but it is less clear that they can be trained to support the full range of logical and commonsense inferences required for NLI).", "labels": [], "entities": []}, {"text": "Ina SemEval 2014 task aimed at evaluating distributed representations for NLI, the best-performing systems relied heavily on additional features and reasoning capabilities.", "labels": [], "entities": []}, {"text": "Our ultimate objective is to provide an empirical evaluation of learning-centered approaches to NLI, advancing the case for NLI as a tool for the evaluation of domain-general approaches to semantic representation.", "labels": [], "entities": []}, {"text": "However, in our view, existing NLI corpora do not permit such an assessment.", "labels": [], "entities": []}, {"text": "They are generally too small for training modern data-intensive, wide-coverage models, many contain sentences that were algorithmically generated, and they are often beset with indeterminacies of event and entity coreference that significantly impact annotation quality.", "labels": [], "entities": []}, {"text": "To address this, this paper introduces the Stanford Natural Language Inference (SNLI) corpus, a collection of sentence pairs labeled for entailment, contradiction, and semantic independence.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) corpus", "start_pos": 43, "end_pos": 92, "type": "DATASET", "confidence": 0.6002661436796188}]}, {"text": "At 570,152 sentence pairs, SNLI is two orders of magnitude larger than all other resources of its type.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.7430470585823059}]}, {"text": "And, in contrast to many such resources, all of its sentences and labels were written by humans in a grounded, naturalistic context.", "labels": [], "entities": []}, {"text": "Ina separate validation phase, we collected four additional judgments for each label for 56,941 of the examples.", "labels": [], "entities": []}, {"text": "Of these, 98% of cases emerge with a threeannotator consensus, and 58% see a unanimous consensus from all five annotators.", "labels": [], "entities": []}, {"text": "In this paper, we use this corpus to evaluate A man inspects the uniform of a figure in some East Asian country.", "labels": [], "entities": []}, {"text": "contradiction The man is sleeping An older and younger man smiling.", "labels": [], "entities": []}, {"text": "neutral N NE N N Two men are smiling and laughing at the cats playing on the floor.", "labels": [], "entities": []}, {"text": "A black race car starts up in front of a crowd of people.", "labels": [], "entities": []}, {"text": "contradiction A man is driving down a lonely road.", "labels": [], "entities": []}, {"text": "A soccer game with multiple males playing.", "labels": [], "entities": []}, {"text": "entailment Some men are playing a sport.", "labels": [], "entities": []}, {"text": "A smiling costumed woman is holding an umbrella.", "labels": [], "entities": []}, {"text": "neutral N NE C N A happy woman in a fairy costume holds an umbrella.: Randomly chosen examples from the development section of our new corpus, shown with both the selected gold labels and the full set of labels (abbreviated) from the individual annotators, including (in the first position) the label used by the initial author of the pair.", "labels": [], "entities": []}, {"text": "a variety of models for natural language inference, including rule-based systems, simple linear classifiers, and neural network-based models.", "labels": [], "entities": []}, {"text": "We find that two models achieve comparable performance: a feature-rich classifier model and a neural network model centered around a Long Short-Term Memory network (LSTM; Hochreiter and Schmidhuber 1997).", "labels": [], "entities": []}, {"text": "We further evaluate the LSTM model by taking advantage of its ready support for transfer learning, and show that it can be adapted to an existing NLI challenge task, yielding the best reported performance by a neural network model and approaching the overall state of the art.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.9285274744033813}]}], "datasetContent": [{"text": "The most immediate application for our corpus is in developing models for the task of NLI.", "labels": [], "entities": []}, {"text": "In par-  ticular, since it is dramatically larger than any existing corpus of comparable quality, we expect it to be suitable for training parameter-rich models like neural networks, which have not previously been competitive at this task.", "labels": [], "entities": []}, {"text": "Our ability to evaluate standard classifier-base NLI models, however, was limited to those which were designed to scale to SNLI's size without modification, so a more complete comparison of approaches will have to wait for future work.", "labels": [], "entities": []}, {"text": "In this section, we explore the performance of three classes of models which could scale readily: (i) models from a well-known NLI system, the Excitement Open Platform; (ii) variants of a strong but simple feature-based classifier model, which makes use of both unlexicalized and lexicalized features, and (iii) distributed representation models, including a baseline model and neural network sequence models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Statistics for the validated pairs. The au- thor's label is the label used by the worker who  wrote the premise to create the sentence pair. A  gold label reflects a consensus of three votes from  among the author and the four annotators.", "labels": [], "entities": []}, {"text": " Table 4: 2-class test accuracy for two simple  baseline systems included in the Excitement Open  Platform, as well as SICK and RTE results for a  model making use of more sophisticated lexical  resources.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9898099899291992}, {"text": "SICK", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.6450544595718384}, {"text": "RTE", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.8424832820892334}]}, {"text": " Table 5: 3-class accuracy, training on either our  data or SICK, including models lacking cross- bigram features (Feature 6), and lacking all lexical  features (Features 4-6). We report results both on  the test set and the training set to judge overfitting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9958128333091736}]}, {"text": " Table 6: Accuracy in 3-class classification on our  training and test sets for each model.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9952644109725952}, {"text": "3-class classification", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.6135647594928741}]}, {"text": " Table 7: LSTM 3-class accuracy on the SICK  train and test sets under three training regimes.", "labels": [], "entities": [{"text": "LSTM 3-class", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.6200722754001617}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.8341614603996277}, {"text": "SICK  train and test sets", "start_pos": 39, "end_pos": 64, "type": "DATASET", "confidence": 0.7915434598922729}]}]}