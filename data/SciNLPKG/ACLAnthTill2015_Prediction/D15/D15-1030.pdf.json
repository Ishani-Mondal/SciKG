{"title": [{"text": "Birds of a Feather Linked Together: A Discriminative Topic Model using Link-based Priors", "labels": [], "entities": []}], "abstractContent": [{"text": "A wide range of applications, from social media to scientific literature analysis, involve graphs in which documents are connected by links.", "labels": [], "entities": [{"text": "scientific literature analysis", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.6717044313748678}]}, {"text": "We introduce a topic model for link prediction based on the intuition that linked documents will tend to have similar topic distributions, integrating a max-margin learning criterion and lexical term weights in the loss function.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.8195893168449402}]}, {"text": "We validate our approach on the tweets from 2,000 Sina Weibo users and evaluate our model's reconstruction of the social network.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many application areas for text analysis involve documents connected by links of one or more types-for example, analysis of scientific papers (citations, co-authorship), Web pages (hyperlinks), legislation (co-sponsorship, citations), and social media (followers, mentions, etc.).", "labels": [], "entities": [{"text": "text analysis involve documents connected by links of one or more types-for example, analysis of scientific papers (citations, co-authorship), Web pages (hyperlinks), legislation (co-sponsorship, citations), and social media (followers, mentions, etc.)", "start_pos": 27, "end_pos": 279, "type": "Description", "confidence": 0.7301654244991059}]}, {"text": "In this paper we work within the widely used framework of topic modeling ( to develop a model that is simple and intuitive, but which identifies high quality topics while also accurately predicting link structure.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.7646776437759399}]}, {"text": "Our work here is inspired by the phenomenon of homophily, the tendency of people to associate with others who are like themselves).", "labels": [], "entities": []}, {"text": "As manifested in social networks, the intuition is that people who are associated with one another are likely to discuss similar topics, and vice versa.", "labels": [], "entities": []}, {"text": "The new topic model we propose therefore takes association links into account so that a document's topic distribution is influenced by the topic distributions of its neighbors.", "labels": [], "entities": []}, {"text": "Specifically, we propose a joint model that uses link structure to define clusters (cliques) of documents and, following the intuition that documents in the same cluster are likely to have similar topic distributions, assigns each cluster its own separate Dirichlet prior over the cluster's topic distribution.", "labels": [], "entities": []}, {"text": "This use of priors is consistent with previous work that has shown document-topic priors to be useful in encoding various types of prior knowledge and improving topic modeling performance.", "labels": [], "entities": []}, {"text": "We then use distributed representations to \"seed\" the topic representations before getting down to modeling the documents.", "labels": [], "entities": []}, {"text": "Our joint objective function uses a discriminative, max-margin approach () to both model the contents of documents and produce good predictions of links; in addition, it improves prediction by including lexical terms in the decision function.", "labels": [], "entities": []}, {"text": "Our baseline for comparison is the Relational Topic Model (, which jointly captures topics and binary link indicators in a style similar to supervised LDA (, instead of modeling links alone, e.g., as in the Latent Multi-group Membership Graph model.", "labels": [], "entities": []}, {"text": "We also compare our approach, who uses document links to create a Markov random topic field (MRTF).", "labels": [], "entities": []}, {"text": "Daum\u00e9 does not, however, look at link prediction, as his upstream model) only generates documents conditioned on links.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.7678694725036621}]}, {"text": "In contrast, our downstream model allows the prediction of links, like RTM.", "labels": [], "entities": []}, {"text": "Our model's primary contribution is in its novel combination of a straightforward joint modeling approach, max-margin learning, and exploitation of lexical information in both topic seeding and regression, yielding a simple but effective model for topic-informed discriminative link prediction.", "labels": [], "entities": [{"text": "topic seeding", "start_pos": 176, "end_pos": 189, "type": "TASK", "confidence": 0.7592492699623108}, {"text": "topic-informed discriminative link prediction", "start_pos": 248, "end_pos": 293, "type": "TASK", "confidence": 0.6931377798318863}]}, {"text": "Like other topic models which treat binary values \"probabilistically\", our model can convert binary link indicators into non-zero weights, with potential application to improving models like Volkova: A graphical model of our model for two documents.", "labels": [], "entities": []}, {"text": "The contribution of our model is the use of document clusters (\u03c0), the use of words (w) in the prediction of document links (y), and a maxmargin objective.", "labels": [], "entities": [{"text": "prediction of document links", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.8220149725675583}]}, {"text": "et al., who use neighbor relationships to improve prediction of user-level attributes.", "labels": [], "entities": []}, {"text": "Our corpus is collected from Sina Weibo with three types of links between documents.", "labels": [], "entities": []}, {"text": "We first conduct a reality check of our model against LDA and MRTF and then perform link prediction tasks.", "labels": [], "entities": [{"text": "MRTF", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.5133615732192993}, {"text": "link prediction", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.7836871445178986}]}, {"text": "We demonstrate improvements in link prediction as measured by predictive link rank and provide both qualitative and quantitative perspectives on the improvements achieved by the model. is a two-document segment of our model, which has the following generative process:", "labels": [], "entities": [{"text": "link prediction", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.7379896193742752}]}], "datasetContent": [{"text": "We crawl data from Sina Weibo, the largest Chinese micro-blog platform.", "labels": [], "entities": [{"text": "Sina Weibo", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.7707162797451019}]}, {"text": "The dataset contains 2,000 randomly-selected verified users, each represented by a single document aggregating all the user's posts.", "labels": [], "entities": []}, {"text": "We also crawl links between pairs of users when both are in our dataset.", "labels": [], "entities": []}, {"text": "Links correspond to three types of interactions on Weibo: mentioning, retweeting and following.", "labels": [], "entities": [{"text": "Weibo", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.9468095898628235}]}], "tableCaptions": [{"text": " Table 1: Our simplified model I-LDA achieves  lower perplexities than both LDA and MRTF,  by incorporating different cliques extracted from  three types of user interactions.", "labels": [], "entities": [{"text": "MRTF", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.7351230978965759}]}, {"text": " Table 2: Data for Illustrative Example", "labels": [], "entities": []}, {"text": " Table 3: Values for Quantitative Analysis", "labels": [], "entities": [{"text": "Quantitative Analysis", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.9642234742641449}]}]}