{"title": [{"text": "On a Strictly Convex IBM Model 1", "labels": [], "entities": []}], "abstractContent": [{"text": "IBM Model 1 is a classical alignment model.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9475292166074117}]}, {"text": "Of the first generation word-based SMT models, it was the only such model with a concave objective function.", "labels": [], "entities": [{"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.8806627988815308}]}, {"text": "For concave optimization problems like IBM Model 1, we have guarantees on the convergence of optimization algorithms such as Expectation Maximization (EM).", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 125, "end_pos": 154, "type": "TASK", "confidence": 0.7792918741703033}]}, {"text": "However , as was pointed out recently, the objective of IBM Model 1 is not strictly concave and there is quite a bit of alignment quality variance within the optimal solution set.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.87800399462382}]}, {"text": "In this work we detail a strictly concave version of IBM Model 1 whose EM algorithm is a simple modification of the original EM algorithm of Model 1 and does not require the tuning of a learning rate or the insertion of an l 2 penalty.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.9243718783060709}]}, {"text": "Moreover , by addressing Model 1's shortcomings , we achieve AER and F-Measure improvements over the classical Model 1 by over 30%.", "labels": [], "entities": [{"text": "AER", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9995261430740356}, {"text": "F-Measure", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9959039092063904}]}], "introductionContent": [{"text": "The IBM translation models were introduced in and were the first-generation Statistical Machine Translation (SMT) systems.", "labels": [], "entities": [{"text": "IBM translation", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.5113437324762344}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 76, "end_pos": 113, "type": "TASK", "confidence": 0.8086910943190256}]}, {"text": "In the current pipeline, these word-based models are the seeds for more sophisticated models which need alignment tableaus to start their optimization procedure.", "labels": [], "entities": []}, {"text": "Among the original IBM Models, only IBM Model 1 can be formulated as a concave optimization problem.", "labels": [], "entities": []}, {"text": "Recently, there has been some research on IBM Model 2 which addresses either the model's non-concavity () * Currently on leave at Google Inc. or over parametrization ().", "labels": [], "entities": []}, {"text": "We make the following contributions in this paper: \u2022 We utilize and expand the mechanism introduced in) to construct strictly concave versions of IBM Model 1 1 . As was shown in (, IBM Model 1 is not a strictly concave optimization problem.", "labels": [], "entities": [{"text": "IBM Model 1 1", "start_pos": 146, "end_pos": 159, "type": "DATASET", "confidence": 0.9167980551719666}]}, {"text": "What this means in practice is that although we can initialize the model with random parameters and get to the same objective cost via the EM algorithm, there is quite a bit of alignment quality variance within the model's optimal solution set and ambiguity persists on which optimal solution truly is the best.", "labels": [], "entities": []}, {"text": "Typically, the easiest way to make a concave model strictly concave is to append an l 2 regularizer.", "labels": [], "entities": []}, {"text": "However, this method does not allow for seamless EM training: we have to either use a learning-rate dependent gradient based algorithm directly or use a gradient method within the M step of EM training.", "labels": [], "entities": [{"text": "EM training", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9223114252090454}]}, {"text": "In this paper we show how to get via a simple technique an infinite supply of models that still allows a straightforward application of the EM algorithm.", "labels": [], "entities": []}, {"text": "\u2022 As a concrete application of the above, we detail a very simple strictly concave version of IBM Model 1 and study the performance of different members within this class.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.9047522147496542}]}, {"text": "Our strictly concave models combine some of the elements of word association and positional dependance as in IBM Model 2 to yield a significant model improvement.", "labels": [], "entities": [{"text": "word association", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.7213890254497528}]}, {"text": "Furthermore, we now have guarantees that the solution we find is unique.", "labels": [], "entities": []}, {"text": "\u2022 We detail an EM algorithm fora subclass of strictly concave IBM Model 1 variants.", "labels": [], "entities": []}, {"text": "The EM algorithm is a small change to the original EM algorithm introduced in).", "labels": [], "entities": []}, {"text": "Throughout this paper, for any positive integer N , we use [N ] to denote {1 . .", "labels": [], "entities": []}, {"text": "N } and [N ] 0 to denote {0 . .", "labels": [], "entities": []}, {"text": "We denote by Rn + the set of nonnegative n dimensional vectors.", "labels": [], "entities": []}, {"text": "We denote by n the n\u2212dimensional unit cube.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on the English-French data for  various (\u03b1, \u03b2) settings as discussed in Section 5.  For the d parameters, we use \u03bb = 16 throughout.  The standard IBM Model 1 is column 1 and cor- responds to a setting of", "labels": [], "entities": []}]}