{"title": [], "abstractContent": [{"text": "ROUGE is a widely adopted, automatic evaluation measure for text summariza-tion.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9703890681266785}, {"text": "text summariza-tion", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.6742785274982452}]}, {"text": "While it has been shown to correlate well with human judgements, it is biased towards surface lexical similarities.", "labels": [], "entities": []}, {"text": "This makes it unsuitable for the evaluation of abstractive summarization, or summaries with substantial paraphrasing.", "labels": [], "entities": []}, {"text": "We study the effectiveness of word embed-dings to overcome this disadvantage of ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.9451267123222351}]}, {"text": "Specifically, instead of measuring lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead.", "labels": [], "entities": []}, {"text": "Our experimental results show that our proposal is able to achieve better correlations with human judgements when measured with the Spearman and Kendall rank coefficients .", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic text summarization is a rich field of research.", "labels": [], "entities": [{"text": "Automatic text summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7756538788477579}]}, {"text": "For example, shared task evaluation workshops for summarization were held for more than a decade in the Document Understanding Conference (DUC), and subsequently the Text Analysis Conference (TAC).", "labels": [], "entities": [{"text": "summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9851810932159424}, {"text": "Document Understanding Conference (DUC)", "start_pos": 104, "end_pos": 143, "type": "TASK", "confidence": 0.7146481871604919}, {"text": "Text Analysis Conference (TAC)", "start_pos": 166, "end_pos": 196, "type": "TASK", "confidence": 0.8530807197093964}]}, {"text": "An important element of these shared tasks is the evaluation of participating systems.", "labels": [], "entities": []}, {"text": "Initially, manual evaluation was carried out, where human judges were tasked to assess the quality of automatically generated summaries.", "labels": [], "entities": []}, {"text": "However in an effort to make evaluation more scaleable, the automatic ROUGE 1 measure) was introduced in DUC-2004.", "labels": [], "entities": [{"text": "ROUGE 1 measure", "start_pos": 70, "end_pos": 85, "type": "METRIC", "confidence": 0.9315544764200846}, {"text": "DUC-2004", "start_pos": 105, "end_pos": 113, "type": "DATASET", "confidence": 0.9665278196334839}]}, {"text": "ROUGE determines the quality of an automatic summary through comparing overlapping units such as ngrams, word sequences, and word pairs with human written summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9059316515922546}]}, {"text": "Recall-Oriented Understudy of Gisting Evaluation ROUGE is not perfect however.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.8072950839996338}]}, {"text": "Two problems with ROUGE are that 1) it favors lexical similarities between generated summaries and model summaries, which makes it unsuitable to evaluate abstractive summarization, or summaries with a significant amount of paraphrasing, and 2) it does not make any provision to cater for the readability or fluency of the generated summaries.", "labels": [], "entities": []}, {"text": "There has been on-going efforts to improve on automatic summarization evaluation measures, such as the Automatically Evaluating Summaries of Peers (AESOP) task in TAC (.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.9108952283859253}, {"text": "Automatically Evaluating Summaries of Peers (AESOP) task", "start_pos": 103, "end_pos": 159, "type": "TASK", "confidence": 0.6671278973420461}, {"text": "TAC", "start_pos": 163, "end_pos": 166, "type": "TASK", "confidence": 0.5239396095275879}]}, {"text": "However, ROUGE remains as one of the most popular metric of choice, as it has repeatedly been shown to correlate very well with human judgements.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.9975777268409729}]}, {"text": "In this work, we describe our efforts to tackle the first problem of ROUGE that we have identified above -its bias towards lexical similarities.", "labels": [], "entities": []}, {"text": "We propose to do this by making use of word embeddings (.", "labels": [], "entities": []}, {"text": "Word embeddings refer to the mapping of words into a multidimensional vector space.", "labels": [], "entities": []}, {"text": "We can construct the mapping, such that the distance between two word projections in the vector space corresponds to the semantic similarity between the two words.", "labels": [], "entities": []}, {"text": "By incorporating these word embeddings into ROUGE, we can overcome its bias towards lexical similarities and instead make comparisons based on the semantics of words sequences.", "labels": [], "entities": []}, {"text": "We believe that this will result in better correlations with human assessments, and avoid situations where two word sequences share similar meanings, but get unfairly penalized by ROUGE due to differences in lexicographic representations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 180, "end_pos": 185, "type": "METRIC", "confidence": 0.8163923621177673}]}, {"text": "As an example, consider these two phrases: 1) It is raining heavily, and 2) It is pouring.", "labels": [], "entities": []}, {"text": "If we are performing a lexical string match, as ROUGE does, there is nothing in common between the terms \"raining\", \"heavily\", and \"pouring\".", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.6842942237854004}]}, {"text": "However, these two phrases mean the same thing.", "labels": [], "entities": []}, {"text": "If one of the phrases was part of a human written summary, while the other was output by an automatic summarization system, we want to be able to reward the automatic system accordingly.", "labels": [], "entities": []}, {"text": "In our experiments, we show that word embeddings indeed give us better correlations with human judgements when measured with the Spearman and Kendall rank coefficient.", "labels": [], "entities": []}, {"text": "This is a significant and exciting result.", "labels": [], "entities": []}, {"text": "Beyond just improving the evaluation prowess of ROUGE, it has the potential to expand the applicability of ROUGE to abstractive summmarization as well.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we make use of the dataset used in AESOP (, and the corresponding correlation measures.", "labels": [], "entities": [{"text": "AESOP", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.7546736598014832}, {"text": "correlation", "start_pos": 87, "end_pos": 98, "type": "METRIC", "confidence": 0.9565778374671936}]}, {"text": "For clarity, let us first describe the dataset used in the main TAC summarization task.", "labels": [], "entities": [{"text": "TAC summarization task", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.9158244132995605}]}, {"text": "The main summarization dataset consists of 44 topics, each of which is associated with a set of 10 documents.", "labels": [], "entities": []}, {"text": "There are also four human-curated model summaries for each of these topics.", "labels": [], "entities": []}, {"text": "Each of the 51 participating systems generated a summary for each of these topics.", "labels": [], "entities": []}, {"text": "These automatically generated summaries, together with the human-curated model summaries, then form the basis of the dataset for AESOP.", "labels": [], "entities": [{"text": "AESOP", "start_pos": 129, "end_pos": 134, "type": "DATASET", "confidence": 0.8607670664787292}]}, {"text": "To assess how effective an automatic evaluation system is, the system is first tasked to assign a score for each of the summaries generated by all of the 51 participating systems.", "labels": [], "entities": []}, {"text": "Each of these summaries would also have been assessed by human judges using these three key metrics: Pyramid.", "labels": [], "entities": [{"text": "Pyramid", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.7184265851974487}]}, {"text": "As reviewed in Section 2, this is a semiautomated measure described in.", "labels": [], "entities": []}, {"text": "Human judges are tasked to evaluate how well a summary adheres to the information requested, as well as the linguistic quality of the generated summary.", "labels": [], "entities": []}, {"text": "Human judges give their judgement on how fluent and readable a summary is.", "labels": [], "entities": []}, {"text": "The evaluation system's scores are then tested to see how well they correlate with the human assessments.", "labels": [], "entities": []}, {"text": "The correlation is evaluated with a set of three metrics, including 1) Pearson correlation (P), 2) Spearman rank coefficient (S), and 3) Kendall rank coefficient (K).", "labels": [], "entities": [{"text": "Pearson correlation (P)", "start_pos": 71, "end_pos": 94, "type": "METRIC", "confidence": 0.9573397517204285}, {"text": "Spearman rank coefficient (S)", "start_pos": 99, "end_pos": 128, "type": "METRIC", "confidence": 0.8683299819628397}, {"text": "Kendall rank coefficient (K)", "start_pos": 137, "end_pos": 165, "type": "METRIC", "confidence": 0.8931795557339987}]}], "tableCaptions": [{"text": " Table 1: Correlation with pyramid scores, mea- sured with Pearson r (P), Spearman \u03c1 (S), and  Kendall \u03c4 (K) coefficients.", "labels": [], "entities": [{"text": "mea- sured", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.8252882162729899}, {"text": "Pearson r (P)", "start_pos": 59, "end_pos": 72, "type": "METRIC", "confidence": 0.9622598528862}, {"text": "Spearman \u03c1 (S)", "start_pos": 74, "end_pos": 88, "type": "METRIC", "confidence": 0.9478696703910827}, {"text": "Kendall \u03c4 (K) coefficients", "start_pos": 95, "end_pos": 121, "type": "METRIC", "confidence": 0.9533825218677521}]}, {"text": " Table 2: Correlation with responsiveness scores,  measured with Pearson r (P), Spearman \u03c1 (S), and  Kendall \u03c4 (K) coefficients.", "labels": [], "entities": [{"text": "Pearson r (P)", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.9715895056724548}, {"text": "Spearman \u03c1 (S)", "start_pos": 80, "end_pos": 94, "type": "METRIC", "confidence": 0.9497119426727295}, {"text": "Kendall \u03c4 (K) coefficients", "start_pos": 101, "end_pos": 127, "type": "METRIC", "confidence": 0.9562620023886362}]}, {"text": " Table 3: Correlation with readability scores, mea- sured with Pearson r (P), Spearman \u03c1 (S), and  Kendall \u03c4 (K) coefficients.", "labels": [], "entities": [{"text": "mea- sured", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.8318976759910583}, {"text": "Pearson r (P)", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.9625634789466858}, {"text": "Spearman \u03c1 (S)", "start_pos": 78, "end_pos": 92, "type": "METRIC", "confidence": 0.9383790612220764}, {"text": "Kendall \u03c4 (K) coefficients", "start_pos": 99, "end_pos": 125, "type": "METRIC", "confidence": 0.9566655953725179}]}, {"text": " Table 4: Correlation with pyramid scores of  top systems in AESOP 2011, measured with the  Spearman \u03c1 (S), and Kendall \u03c4 (K) coefficients.", "labels": [], "entities": [{"text": "AESOP 2011", "start_pos": 61, "end_pos": 71, "type": "DATASET", "confidence": 0.9308440983295441}, {"text": "Spearman \u03c1 (S)", "start_pos": 92, "end_pos": 106, "type": "METRIC", "confidence": 0.8635994791984558}, {"text": "Kendall \u03c4 (K)", "start_pos": 112, "end_pos": 125, "type": "METRIC", "confidence": 0.9429020047187805}]}]}