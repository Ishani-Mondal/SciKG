{"title": [{"text": "Sentence Modeling with Gated Recursive Neural Network", "labels": [], "entities": [{"text": "Sentence Modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9320548176765442}]}], "abstractContent": [{"text": "Recently, neural network based sentence modeling methods have achieved great progress.", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7103995829820633}]}, {"text": "Among these methods, the re-cursive neural networks (RecNNs) can effectively model the combination of the words in sentence.", "labels": [], "entities": []}, {"text": "However, RecNNs need a given external topological structure , like syntactic tree.", "labels": [], "entities": []}, {"text": "In this paper, we propose a gated recursive neural network (GRNN) to model sentences, which employs a full binary tree (FBT) structure to control the combinations in re-cursive structure.", "labels": [], "entities": []}, {"text": "By introducing two kinds of gates, our model can better model the complicated combinations of features.", "labels": [], "entities": []}, {"text": "Experiments on three text classification datasets show the effectiveness of our model.", "labels": [], "entities": [{"text": "text classification", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.704382061958313}]}], "introductionContent": [{"text": "Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (, Recursive Neural Network (RecNN) and Convolutional Neural Network (CNN) ().", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.741500049829483}, {"text": "feature engineering", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.7126621752977371}]}, {"text": "Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence.", "labels": [], "entities": []}, {"text": "However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application.", "labels": [], "entities": []}, {"text": "proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree * Corresponding author.", "labels": [], "entities": []}, {"text": "cannot agree with you I more agree with you I more cannot: Example of Gated Recursive Neural Networks (GRNNs).", "labels": [], "entities": []}, {"text": "Left is a GRNN using a directed acyclic graph (DAG) structure.", "labels": [], "entities": []}, {"text": "Right is a GRNN using a full binary tree (FBT) structure.", "labels": [], "entities": []}, {"text": "(The green nodes, gray nodes and white nodes illustrate the positive, negative and neutral sentiments respectively.) to model sentences.", "labels": [], "entities": []}, {"text": "However, DAG structure is relatively complicated.", "labels": [], "entities": [{"text": "DAG structure", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.9565533399581909}]}, {"text": "The number of the hidden neurons quadraticly increases with the length of sentences so that grConv cannot effectively deal with long sentences.", "labels": [], "entities": []}, {"text": "Inspired by grConv, we propose a gated recursive neural network (GRNN) for sentence modeling.", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.8426252901554108}]}, {"text": "Different with grConv, we use the full binary tree (FBT) as the topological structure to recursively model the word combinations, as shown in.", "labels": [], "entities": [{"text": "FBT", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.7426992058753967}]}, {"text": "The number of the hidden neurons linearly increases with the length of sentences.", "labels": [], "entities": []}, {"text": "Another difference is that we introduce two kinds of gates, reset and update gates (, to control the combinations in recursive structure.", "labels": [], "entities": []}, {"text": "With these two gating mechanisms, our model can better model the complicated combinations of features and capture the long dependency interactions.", "labels": [], "entities": []}, {"text": "In our previous works, we have investigated several different topological structures (tree and directed acyclic graph) to recursively model the semantic composition from the bottom layer to the top layer, and applied them on Chinese word segmentation () and dependency parsing) tasks.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 225, "end_pos": 250, "type": "TASK", "confidence": 0.590025375286738}, {"text": "dependency parsing", "start_pos": 258, "end_pos": 276, "type": "TASK", "confidence": 0.7532530426979065}]}, {"text": "However, these structures are not suitable for modeling sentences.", "labels": [], "entities": []}, {"text": "In this paper, we adopt the full binary tree as the topological structure to reduce the model complexity.", "labels": [], "entities": []}, {"text": "Experiments on the Stanford Sentiment Treebank dataset) and the TREC questions dataset ( show the effectiveness of our approach.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank dataset", "start_pos": 19, "end_pos": 54, "type": "DATASET", "confidence": 0.9242269992828369}, {"text": "TREC questions dataset", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.9247312148412069}]}], "datasetContent": [{"text": "To evaluate our approach, we test our model on three datasets: \u2022 SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank 1 (Socher et al., 2013b): negative, somewhat negative, neutral, somewhat positive, positive.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank 1", "start_pos": 114, "end_pos": 143, "type": "DATASET", "confidence": 0.8440058678388596}]}, {"text": "\u2022 SST-2 The movie reviews with binary classes in the Stanford Sentiment Treebank 1 (Socher et al., 2013b): negative, positive.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank 1", "start_pos": 53, "end_pos": 82, "type": "DATASET", "confidence": 0.8935006260871887}]}, {"text": "\u2022 QC The TREC questions dataset 2 () involves six different question types.", "labels": [], "entities": [{"text": "TREC questions dataset 2", "start_pos": 9, "end_pos": 33, "type": "DATASET", "confidence": 0.7424237728118896}]}, {"text": "lists the hyper-parameters of our model.", "labels": [], "entities": []}, {"text": "In this paper, we also exploit dropout strategy) to avoid overfitting.", "labels": [], "entities": []}, {"text": "In addition, we set the batch size to 20.", "labels": [], "entities": []}, {"text": "We set word embedding size d = 50 on the TREC dataset and d = 100 on the Stanford Sentiment Treebank dataset.", "labels": [], "entities": [{"text": "TREC dataset", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.9682502448558807}, {"text": "Stanford Sentiment Treebank dataset", "start_pos": 73, "end_pos": 108, "type": "DATASET", "confidence": 0.9518536478281021}]}, {"text": "shows the performance of our GRNN on three datasets.", "labels": [], "entities": []}, {"text": "42.4 80.5 88.2 PV ( 44.6 * 82.7 * 91.8 * CNN-non-static 48.0 87.2 93.6 CNN-multichannel 47.: Performances of the different models.", "labels": [], "entities": [{"text": "CNN-non-static 48.0 87.2 93.6 CNN-multichannel 47.", "start_pos": 41, "end_pos": 91, "type": "DATASET", "confidence": 0.8215083181858063}]}, {"text": "The result of PV is from our own implementation based on Gensim.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performances of the different models. The result of PV is from our own implementation based  on Gensim.", "labels": [], "entities": []}]}