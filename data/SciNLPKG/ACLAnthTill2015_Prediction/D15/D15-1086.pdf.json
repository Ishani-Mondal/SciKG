{"title": [{"text": "Extracting Relations between Non-Standard Entities using Distant Supervision and Imitation Learning", "labels": [], "entities": [{"text": "Extracting Relations between Non-Standard Entities", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8768701910972595}]}], "abstractContent": [{"text": "Distantly supervised approaches have become popular in recent years as they allow training relation extractors without text-bound annotation, using instead known relations from a knowledge base and a large textual corpus from an appropriate domain.", "labels": [], "entities": []}, {"text": "While state of the art distant supervision approaches use off-the-shelf named entity recognition and classification (NERC) systems to identify relation arguments, discrepancies in domain or genre between the data used for NERC training and the intended domain for the relation extractor can lead to low performance.", "labels": [], "entities": [{"text": "named entity recognition and classification (NERC)", "start_pos": 72, "end_pos": 122, "type": "TASK", "confidence": 0.846416637301445}]}, {"text": "This is particularly problematic for \"non-standard\" named entities such as album which would fall into the MISC category.", "labels": [], "entities": []}, {"text": "We propose to ameliorate this issue by jointly training the named entity classifier and the relation extractor using imitation learning which reduces struc-tured prediction learning to classification learning.", "labels": [], "entities": [{"text": "relation extractor", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7081598490476608}, {"text": "struc-tured prediction learning", "start_pos": 150, "end_pos": 181, "type": "TASK", "confidence": 0.7112540404001871}, {"text": "classification learning", "start_pos": 185, "end_pos": 208, "type": "TASK", "confidence": 0.8947544991970062}]}, {"text": "We further experiment with Web features different features and compare against using two off-the-shelf supervised NERC systems, Stanford NER and FIGER, for named entity classification.", "labels": [], "entities": [{"text": "Stanford NER", "start_pos": 128, "end_pos": 140, "type": "DATASET", "confidence": 0.8722223341464996}, {"text": "FIGER", "start_pos": 145, "end_pos": 150, "type": "METRIC", "confidence": 0.8665550947189331}, {"text": "named entity classification", "start_pos": 156, "end_pos": 183, "type": "TASK", "confidence": 0.6605871518452963}]}, {"text": "Our experiments show that imitation learning improves average precision by 4 points over an one-stage classification model, while removing Web features results in a 6 points reduction.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.866515576839447}]}, {"text": "Compared to using FIGER and Stanford NER, average precision is 10 points and 19 points higher with our imitation learning approach.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.7551366090774536}, {"text": "Stanford NER", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.6480964124202728}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9972023963928223}]}], "introductionContent": [{"text": "Factual answers to queries such as \"What albums did The Beatles release?\" are commonly stored in knowledge bases and can then be accessed by an information retrieval system, a commercial example for this being Google's knowledge vault ().", "labels": [], "entities": []}, {"text": "In order to keep knowledge bases up to date should new facts emerge, and to quickly adapt to new domains, there is a need for flexible and accurate information extraction (IE) approaches which do not require manual effort to be developed for new domains.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 148, "end_pos": 175, "type": "TASK", "confidence": 0.847415566444397}]}, {"text": "A popular approach for creating IE methods to extract such relations is distant supervision) which is a method for learning relation extractors using relations stored in a knowledge base combined with raw text to automatically generate training data.", "labels": [], "entities": [{"text": "learning relation extractors", "start_pos": 115, "end_pos": 143, "type": "TASK", "confidence": 0.6723865071932474}]}, {"text": "An important first step in distant supervision is to identify named entities (NEs) and their types to determine if a pair of NEs is a suitable candidate for the relation.", "labels": [], "entities": []}, {"text": "As an example, the album relation has a Musical Artist and an Album as arguments.", "labels": [], "entities": []}, {"text": "Existing works use supervised named entity recognisers and classifiers (NERC) with either a small set of types such as the Stanford NER system ( ), or fine-grained NE types ().", "labels": [], "entities": [{"text": "Stanford NER system", "start_pos": 123, "end_pos": 142, "type": "DATASET", "confidence": 0.8754871090253195}]}, {"text": "However, supervised NERCs typically focus on recognising persons, locations and organisations and perform poorly for other types of NEs, e.g. we find that Stanford NER only recognises 43% of all MISC NEs in our corpus.", "labels": [], "entities": [{"text": "Stanford NER", "start_pos": 155, "end_pos": 167, "type": "DATASET", "confidence": 0.8094114661216736}, {"text": "MISC NEs", "start_pos": 195, "end_pos": 203, "type": "TASK", "confidence": 0.5840635895729065}]}, {"text": "In addition, they do not always perform well if they are trained on a different type of text or fora different domain.", "labels": [], "entities": []}, {"text": "This issue becomes more important as focus is shifting from using curated text collections such as Wikipedia to texts collected from the Web via search queries (Web-based distant supervision) which can provide better coverage (.", "labels": [], "entities": []}, {"text": "In order to ameliorate this issue, we propose to recognise NEs with simple heuristics, then use the imitation learning algorithm DAGGER) to learn the NEC component jointly with relation extraction (RE), without requiring explicitly labeled data for NERC.", "labels": [], "entities": [{"text": "relation extraction (RE)", "start_pos": 177, "end_pos": 201, "type": "TASK", "confidence": 0.7823225378990173}]}, {"text": "Instead, training signal is obtained by assessing the predictions of the relation extraction component.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7817715406417847}]}, {"text": "In this paper we make the following contributions: 1.", "labels": [], "entities": []}, {"text": "We learn jointly training a named entity classifier and a relation extractor for Web-based distant supervision.", "labels": [], "entities": [{"text": "relation extractor", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7045538276433945}]}, {"text": "Our method does not rely on hand-labeled training data and is applicable to any domain, which is shown in our evaluation on 18 different relations.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for POS-based candidate identifi- cation strategies compared to Stanford NER", "labels": [], "entities": [{"text": "NER", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.7192342281341553}]}, {"text": " Table 3: Results for best model for each relation,  macro average over all relations.", "labels": [], "entities": []}, {"text": " Table 4: Results for best model for each relation, highest P-avg in bold", "labels": [], "entities": []}, {"text": " Table 5: Best feature combination for IL", "labels": [], "entities": []}, {"text": " Table 6: Imitation learning results for different NE and relation features, macro average over all relations.", "labels": [], "entities": [{"text": "Imitation learning", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8953583240509033}]}]}