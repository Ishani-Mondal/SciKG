{"title": [{"text": "Syntax-based Rewriting for Simultaneous Machine Translation", "labels": [], "entities": [{"text": "Simultaneous Machine Translation", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.8235055804252625}]}], "abstractContent": [{"text": "Divergent word order between languages causes delay in simultaneous machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7385785281658173}]}, {"text": "We present a sentence rewriting method that generates more mono-tonic translations to improve the speed-accuracy tradeoff.", "labels": [], "entities": [{"text": "sentence rewriting", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7517519593238831}]}, {"text": "We design grammati-cality and meaning-preserving syntactic transformation rules that operate on constituent parse trees.", "labels": [], "entities": []}, {"text": "We apply the rules to reference translations to make their word order closer to the source language word order.", "labels": [], "entities": []}, {"text": "On Japanese-English translation (two languages with substantially different structure), incorporating the rewritten , more monotonic reference translation into a phrase-based machine translation system enables better translations faster than a baseline system that only uses gold reference translations.", "labels": [], "entities": [{"text": "Japanese-English translation", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.6704067289829254}, {"text": "phrase-based machine translation", "start_pos": 162, "end_pos": 194, "type": "TASK", "confidence": 0.6468300620714823}]}], "introductionContent": [{"text": "Simultaneous interpretation is challenging because it demands both quality and speed.", "labels": [], "entities": [{"text": "Simultaneous interpretation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.848886251449585}, {"text": "speed", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9701646566390991}]}, {"text": "Conventional batch translation waits until the entire sentence is completed before starting to translate.", "labels": [], "entities": [{"text": "batch translation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7501588761806488}]}, {"text": "This merely optimizes translation quality and often introduces undesirable lag between the speaker and the audience.", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.968662440776825}]}, {"text": "Simultaneous interpretation instead requires a tradeoff between quality and speed.", "labels": [], "entities": [{"text": "speed", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.9621472954750061}]}, {"text": "A common strategy is to translate independently translatable segments as soon as possible.", "labels": [], "entities": []}, {"text": "Various segmentation methods () reduce translation delay; they are limited, however, by the unavoidable word reordering between languages with drastically different word orders.", "labels": [], "entities": [{"text": "translation delay", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.6638625115156174}]}, {"text": "We show an example of Japanese-English translation in.", "labels": [], "entities": [{"text": "Japanese-English translation", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.5772391557693481}]}, {"text": "Consider the batch translation: in English, the verb change comes immediately after the subject We, whereas in Japanese it comes at the end of the sentence; therefore, to produce an intelligible English sentence, we must translate the object after the final verb is observed, resulting in one large and painfully delayed segment.", "labels": [], "entities": [{"text": "batch translation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7307763993740082}]}, {"text": "To reduce structural discrepancy, we can apply syntactic transformations to make the word order of one language closer to the other.", "labels": [], "entities": []}, {"text": "Consider the monotone translation in.", "labels": [], "entities": []}, {"text": "By passivizing the English sentence, we can cache the subject and begin translating before observing the final verb.", "labels": [], "entities": []}, {"text": "Furthermore, by using the English possessive, we mimic the order of the Japanese genitive construction.", "labels": [], "entities": []}, {"text": "These transformations enable us to divide the input into shorter segments, thus reducing translation delay.", "labels": [], "entities": []}, {"text": "To produce such monotone translations, a straightforward approach is to incorporate interpretation data into the learning of a machine translation (MT) system, because human interpreters use a variety of strategies () to fine-tune the word order.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 127, "end_pos": 151, "type": "TASK", "confidence": 0.8362116813659668}]}, {"text": "shows that this approach improves the speed-accuracy tradeoff.", "labels": [], "entities": []}, {"text": "However, existing parallel simultaneous interpretation corpora () are often small, and collecting new data is expensive due to the inherent costs of recording and transcribing speeches.", "labels": [], "entities": []}, {"text": "In addition, due to the intense time pressure during interpretation, human interpretation has the disadvantage of simpler, less precise diction (Camayd-Freixas, 2011; Al-) compared to human translations done at the translator's leisure, allowing for more introspection and precise word choice.", "labels": [], "entities": []}, {"text": "We aim to address the data scarcity problem and combine translators' lexical precision and interpreters' syntactic flexibility.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.8715904355049133}]}, {"text": "We propose to rewrite the reference translation in away that uses the original lexicon, obeys standard grammar rules of 55 Monotone: Figure 1: Divergent word order between language pairs can cause long delays in simultaneous translation: Segments (||) mark the portions of the sentence that can be translated together.", "labels": [], "entities": []}, {"text": "(Case markers: topic (TOP), genitive (GEN), accusative (ACC), copula (COP).) the target language, preserves the original semantics, and yields more monotonic translations.", "labels": [], "entities": []}, {"text": "We then train the MT system with the rewritten references so that it learns how to produce low-latency translations from the data.", "labels": [], "entities": [{"text": "MT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9275550842285156}]}, {"text": "A data-driven approach to learning these rewriting rules is hampered by the dearth of parallel data: we have few examples of text that have been both interpreted and translated.", "labels": [], "entities": []}, {"text": "Therefore, we design syntactic transformation rules based on linguistic analysis of the source and the target languages.", "labels": [], "entities": []}, {"text": "We apply these rules to parsed text and decide whether to accept the rewritten sentence based on the amount of delay reduction.", "labels": [], "entities": []}, {"text": "In this work, we focus on Japanese to English translation, because (i) Japanese and English have significantly different word orders (SOV vs. SVO); and consequently, (ii) the syntactic constituents required earlier by an English sentence often come late in the corresponding Japanese sentence.", "labels": [], "entities": [{"text": "Japanese to English translation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.619521476328373}]}, {"text": "We evaluate our approach using standard machine translation data (the Reuters newsfeed Japanese-English corpus) in a simultaneous translation setting.", "labels": [], "entities": [{"text": "Reuters newsfeed Japanese-English corpus", "start_pos": 70, "end_pos": 110, "type": "DATASET", "confidence": 0.9563655257225037}]}, {"text": "Our experimental results show that including the rewritten references into the learning of a phrase-based MT system results in a better speed-accuracy tradeoff against both the original and the rewritten reference translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.8450422286987305}]}], "datasetContent": [{"text": "We evaluate our method on the Reuters JapaneseEnglish corpus of news articles).", "labels": [], "entities": [{"text": "Reuters JapaneseEnglish corpus of news articles", "start_pos": 30, "end_pos": 77, "type": "DATASET", "confidence": 0.9097440242767334}]}, {"text": "For training the MT system, we also include the EIJIRO dictionary entries and the accompanying example sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9822967648506165}, {"text": "EIJIRO dictionary entries", "start_pos": 48, "end_pos": 73, "type": "DATASET", "confidence": 0.8933663368225098}]}, {"text": "Statistics of the dataset are shown in.", "labels": [], "entities": []}, {"text": "The rewritten translation is generally slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents.", "labels": [], "entities": []}, {"text": "We use the TreebankWordTokenizer from NLTK () to tokenize English sentences and Kuromoji Japanese morphological analyzer 8 to tokenize Japanese sentences.", "labels": [], "entities": [{"text": "tokenize English sentences", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.847153902053833}]}, {"text": "Our phrase-based MT system is trained by Moses () with standard parameters settings.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.8209636807441711}]}, {"text": "We use GIZA++ (Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.8576162457466125}, {"text": "word alignment", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.818320095539093}, {"text": "MIRA", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.797635555267334}]}, {"text": "The translation quality is evaluated by) and RIBES (.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9446081519126892}, {"text": "RIBES", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.5533679127693176}]}, {"text": "To obtain the parse trees for English sentences, we use the Stanford Parser ( and the included English model.", "labels": [], "entities": [{"text": "Stanford Parser", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.8140937387943268}]}], "tableCaptions": [{"text": " Table 2: Percentage of sentences that each rule  category can be applied to (Applicable) and the  percentage of sentences for which the rule results  in a more monotonic sentence (Accepted).", "labels": [], "entities": [{"text": "Applicable", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9929914474487305}, {"text": "Accepted", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9887140393257141}]}]}