{"title": [{"text": "A Binarized Neural Network Joint Model for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7972129881381989}]}], "abstractContent": [{"text": "The neural network joint model (NNJM), which augments the neural network language model (NNLM) with an m-word source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.7282779216766357}, {"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.8651909232139587}]}, {"text": "Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost.", "labels": [], "entities": [{"text": "NNJM", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.9287972450256348}, {"text": "maximum likelihood estimation (MLE", "start_pos": 79, "end_pos": 113, "type": "METRIC", "confidence": 0.7424463033676147}]}, {"text": "In this paper, we propose an alternative to NCE, the bina-rized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE.", "labels": [], "entities": []}, {"text": "We compare the BNNJM and NNJM trained by NCE on various translation tasks.", "labels": [], "entities": [{"text": "BNNJM", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.7264792323112488}, {"text": "NNJM", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.8867361545562744}, {"text": "NCE", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.9512084126472473}, {"text": "translation tasks", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.9181946814060211}]}], "introductionContent": [{"text": "Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (.", "labels": [], "entities": [{"text": "Neural network translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6282632350921631}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9135919809341431}]}, {"text": "Notably, proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in.", "labels": [], "entities": []}, {"text": "While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary.", "labels": [], "entities": [{"text": "SMT task", "start_pos": 86, "end_pos": 94, "type": "TASK", "confidence": 0.9360252022743225}]}, {"text": "To solve this problem, presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding.", "labels": [], "entities": [{"text": "NNJM", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.8783167004585266}]}, {"text": "However, they also note that this self-normalization technique sacrifices neural network accuracy, and the training process for the self-normalized neural network is very slow, as with standard maximum likelihood estimation (MLE).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9915665984153748}]}, {"text": "To remedy the problem of long training times in the context of NNLMs, used a method called noise contrastive estimation (NCE).", "labels": [], "entities": [{"text": "noise contrastive estimation (NCE)", "start_pos": 91, "end_pos": 125, "type": "TASK", "confidence": 0.6505824029445648}]}, {"text": "Compared with MLE, NCE does not require repeated summations over the whole vocabulary and performs nonlinear logistic regression to discriminate between the observed data and artificially generated noise.", "labels": [], "entities": []}, {"text": "This paper proposes an alternative framework of binarized NNJMs (BNNJM), which are similar to the NNJM, but use the current target word not as the output, but as the input of the neural network, estimating whether the target word under examination is corrector not, as shown in.", "labels": [], "entities": []}, {"text": "Because the BNNJM uses the current target word as input, the information about the current target word can be combined with the context word information and processed in the hidden layers.", "labels": [], "entities": [{"text": "BNNJM", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.904341459274292}]}, {"text": "The BNNJM learns a simple binary classifier, given the context and target words, therefore it can be trained by MLE very efficiently.", "labels": [], "entities": [{"text": "BNNJM", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.6500880122184753}]}, {"text": "\"Incorrect\" target words for the BNNJM can be generated in the same way as NCE generates noise for the NNJM.", "labels": [], "entities": [{"text": "BNNJM", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.8744045495986938}]}, {"text": "We present a novel noise distribution based on translation probabilities to train the NNJM and the BNNJM efficiently.", "labels": [], "entities": [{"text": "BNNJM", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.8819164633750916}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Epochs (E) and time (T) in minutes per  epoch for each task.", "labels": [], "entities": [{"text": "Epochs (E)", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9134746640920639}, {"text": "time (T)", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8942468017339706}]}, {"text": " Table 2: Translation results. The symbol + and *  represent significant differences at the p < 0.01  level against Base and NNJM+UPD, respectively.  Significance tests were conducted using bootstrap  resampling", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9670681357383728}, {"text": "NNJM+UPD", "start_pos": 125, "end_pos": 133, "type": "DATASET", "confidence": 0.8227149446805319}]}, {"text": " Table 5: 1-gram precisions and improvements.", "labels": [], "entities": [{"text": "precisions", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9887604713439941}]}]}