{"title": [{"text": "Trans-gram, Fast Cross-lingual Word-embeddings rey es \u2212 Mann", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce Trans-gram, a simple and computationally-efficient method to simultaneously learn and align word-embeddings fora variety of languages, using only monolingual data and a smaller set of sentence-aligned data.", "labels": [], "entities": []}, {"text": "We use our new method to compute aligned word-embeddings for twenty-one languages using English as a pivot language.", "labels": [], "entities": []}, {"text": "We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language.", "labels": [], "entities": []}, {"text": "We also achieve state of the art results on standard cross-lingual text classification and word translation tasks.", "labels": [], "entities": [{"text": "cross-lingual text classification", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.6241719524065653}, {"text": "word translation", "start_pos": 91, "end_pos": 107, "type": "TASK", "confidence": 0.7910295128822327}]}], "introductionContent": [{"text": "Word-embeddings area representation of words with fixed-sized vectors.", "labels": [], "entities": []}, {"text": "It is a distributed representation in the sense that there is not necessarily a one-to-one correspondence between vector dimensions and linguistic properties.", "labels": [], "entities": []}, {"text": "The linguistic properties are distributed along the dimensions of the space.", "labels": [], "entities": []}, {"text": "A popular method to compute wordembeddings is the Skip-gram model).", "labels": [], "entities": []}, {"text": "This algorithm learns high-quality word vectors with a computation cost much lower than previous methods.", "labels": [], "entities": []}, {"text": "This allows the processing of very important amounts of data.", "labels": [], "entities": []}, {"text": "For instance, a 1.6 billion words dataset can be processed in less than one day.", "labels": [], "entities": []}, {"text": "Several authors came up with different methods to align word-embeddings across two languages (.", "labels": [], "entities": []}, {"text": "* These authors contributed equally.", "labels": [], "entities": []}, {"text": "In this article, we introduce anew method called Trans-gram, which learns word embeddings aligned across many languages, in a simple and efficient fashion, using only sentence alignments rather than word alignments.", "labels": [], "entities": []}, {"text": "We compare our method with previous approaches on a crosslingual document classification task and on a word translation task and obtain state of the art results on these tasks.", "labels": [], "entities": [{"text": "crosslingual document classification task", "start_pos": 52, "end_pos": 93, "type": "TASK", "confidence": 0.6868675947189331}, {"text": "word translation task", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.815405031045278}]}, {"text": "Additionally, word-embeddings for twenty-one languages are learned simultaneously -to our knowledge -for the first time, in less than two and a half hours.", "labels": [], "entities": []}, {"text": "Furthermore, we illustrate some interesting properties that are captured such as cross-lingual analogies, e.g rey es \u2212 Mann de + femme fr \u2248 regina it which can be used for disambiguation.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of Trans-gram with various methods for Reuters English/German classification", "labels": [], "entities": [{"text": "Reuters English/German classification", "start_pos": 60, "end_pos": 97, "type": "TASK", "confidence": 0.7272905230522155}]}, {"text": " Table 2: Results on the translation task", "labels": [], "entities": [{"text": "translation", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.979716956615448}]}]}