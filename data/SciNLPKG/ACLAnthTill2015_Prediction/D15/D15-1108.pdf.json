{"title": [{"text": "Dual Decomposition Inference for Graphical Models over Strings *", "labels": [], "entities": [{"text": "Dual Decomposition Inference", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7380986015001932}]}], "abstractContent": [{"text": "We investigate dual decomposition for joint MAP inference of many strings.", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9227160811424255}]}, {"text": "Given an arbitrary graphical model, we decompose it into small acyclic sub-models, whose MAP configurations can be found by finite-state composition and dynamic programming.", "labels": [], "entities": []}, {"text": "We force the solutions of these subproblems to agree on overlapping variables, by tuning Lagrange multi-pliers for an adaptively expanding set of variable-length n-gram count features.", "labels": [], "entities": []}, {"text": "This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simplification, or abound on string length.", "labels": [], "entities": []}, {"text": "Provided that the inference method terminates, it gives a certificate of global optimality (though MAP inference in our setting is undecid-able in general).", "labels": [], "entities": []}, {"text": "On our global phonolog-ical inference problems, it always terminates , and achieves more accurate results than max-product and sum-product loopy belief propagation.", "labels": [], "entities": [{"text": "sum-product loopy belief propagation", "start_pos": 127, "end_pos": 163, "type": "TASK", "confidence": 0.6111863031983376}]}], "introductionContent": [{"text": "Graphical models allow expert modeling of complex relations and interactions between random variables.", "labels": [], "entities": []}, {"text": "Since a graphical model with given parameters defines a probability distribution, it can be used to reconstruct values for unobserved variables.", "labels": [], "entities": []}, {"text": "The marginal inference problem is to compute the posterior marginal distributions of these variables.", "labels": [], "entities": []}, {"text": "The MAP inference (or MPE) problem is to compute the single highest-probability joint assignment to all the unobserved variables.", "labels": [], "entities": [{"text": "MAP inference (or MPE)", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7694537738958994}]}, {"text": "Inference in general graphical models is NPhard even when the variables' values are finite discrete values such as categories, tags or domains.", "labels": [], "entities": []}, {"text": "In this paper, we address the more challenging setting * This material is based upon work supported by the National Science Foundation under Grant where the variables in the graphical models range over strings.", "labels": [], "entities": []}, {"text": "Thus, the domain of the variables is an infinite space of discrete structures.", "labels": [], "entities": []}, {"text": "In NLP, such graphical models can deal with large, incompletely observed lexicons.", "labels": [], "entities": []}, {"text": "They could be used to model diverse relationships among strings that represent spellings or pronunciations; morphemes, words, phrases (such as named entities and URLs), or utterances; standard or variant forms; clean or noisy forms; contemporary or historical forms; underlying or surface forms; source or target language forms.", "labels": [], "entities": [{"text": "model diverse relationships among strings that represent spellings or pronunciations; morphemes, words, phrases (such as named entities and URLs), or utterances; standard or variant forms; clean or noisy forms; contemporary or historical forms; underlying or surface forms; source or target language", "start_pos": 22, "end_pos": 321, "type": "Description", "confidence": 0.8795733921802961}]}, {"text": "Such relationships arise in domains such as morphology, phonology, historical linguistics, translation between related languages, and social media text analysis.", "labels": [], "entities": [{"text": "translation between related languages", "start_pos": 91, "end_pos": 128, "type": "TASK", "confidence": 0.8866926580667496}, {"text": "social media text analysis", "start_pos": 134, "end_pos": 160, "type": "TASK", "confidence": 0.7103515937924385}]}, {"text": "In this paper, we assume a given graphical model, whose factors evaluate the relationships among observed and unobserved strings.", "labels": [], "entities": []}, {"text": "We present a dual decomposition algorithm for MAP inference, which returns a certifiably optimal solution when it converges.", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.9695029556751251}]}, {"text": "We demonstrate our method on a graphical model for phonology proposed by . We show that the method generally converges and that it achieves better results than alternatives.", "labels": [], "entities": []}, {"text": "The rest of the paper is arranged as follows: We will review graphical models over strings in section 2, and briefly introduce our sample problem in section 3.", "labels": [], "entities": []}, {"text": "Section 4 develops dual decomposition inference for graphical models over strings.", "labels": [], "entities": [{"text": "dual decomposition inference", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.8507939775784811}]}, {"text": "Then our experimental setup and results are presented in sections 5 and 6, with some discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare DD to belief propagation, using the graphical model for generative phonology discussed in section 3.", "labels": [], "entities": [{"text": "DD", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9671931266784668}, {"text": "belief propagation", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7747990489006042}, {"text": "generative phonology", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.9055884480476379}]}, {"text": "Inference in this model aims to reconstruct underlying morphemes.", "labels": [], "entities": []}, {"text": "Since our focus is inference, we will evaluate these reconstructions directly (whereas  evaluated their ability to predict novel surface forms using the reconstructions).", "labels": [], "entities": []}, {"text": "Our factor graphs have a similar topology to the pedagogical fragment shown in.", "labels": [], "entities": []}, {"text": "How-ever, they are actually derived from datasets constructed by , which are available with full descriptions at http://hubal.cs. jhu.edu/tacl2015/.", "labels": [], "entities": []}, {"text": "Briefly: EXERCISE Small datasets of Catalan, English, Maori, and Tangale, drawn from phonology textbooks.", "labels": [], "entities": []}, {"text": "Each dataset contains 55 to 106 surface words, formed from a collection of 16 to 55 morphemes.", "labels": [], "entities": []}, {"text": "CELEX Larger datasets of German, English, and Dutch, drawn from the CELEX database (.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9660407304763794}, {"text": "CELEX database", "start_pos": 68, "end_pos": 82, "type": "DATASET", "confidence": 0.9795607328414917}]}, {"text": "Each dataset contains 1000 surface words, formed from 341 to 381 underlying morphemes.", "labels": [], "entities": []}, {"text": "We compared three types of inference: DD Use DD to perform exact MAP inference.", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.8731575012207031}]}, {"text": "SP Perform approximate marginal inference by sum-product loopy BP with pruning.", "labels": [], "entities": [{"text": "BP", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.607755720615387}]}, {"text": "MP Perform approximate MAP inference by max-product loopy BP with pruning.", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.8991772830486298}, {"text": "BP", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.48789602518081665}]}, {"text": "DD and SP improve this baseline in different ways.", "labels": [], "entities": [{"text": "SP", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.9300957322120667}]}, {"text": "DD predicts a string value for each variable.", "labels": [], "entities": []}, {"text": "For SP and MP, we deem the prediction at a variable to be the string that is scored most highly by the belief at that variable.", "labels": [], "entities": []}, {"text": "We report the fraction of predicted morpheme URs that exactly match the gold-standard URs proposed by a human ( ).", "labels": [], "entities": []}, {"text": "We also compare these predicted URs to one another, to see how well the methods agree.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Pairwise agreement (on morpheme URs) of DD, SP,  MP and the gold standard, for each group of inference prob- lems. Boldface is highest accuracy (agreement with gold).", "labels": [], "entities": [{"text": "Pairwise", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9268724322319031}, {"text": "agreement", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.5271846652030945}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9992144107818604}]}]}