{"title": [{"text": "Do Multi-Sense Embeddings Improve Natural Language Understanding?", "labels": [], "entities": [{"text": "Multi-Sense Embeddings Improve Natural Language Understanding", "start_pos": 3, "end_pos": 64, "type": "TASK", "confidence": 0.7933545708656311}]}], "abstractContent": [{"text": "Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations.", "labels": [], "entities": []}, {"text": "Yet while 'multi-sense' methods have been proposed and tested on artificial word-similarity tasks, we don't know if they improve real natural language understanding tasks.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 134, "end_pos": 170, "type": "TASK", "confidence": 0.8310230523347855}]}, {"text": "In this paper we introduce a multi-sense embedding model based on Chinese Restaurant Processes that achieves state of the art performance on matching human word similarity judgments, and propose a pipelined architecture for incorporating multi-sense embeddings into language understanding.", "labels": [], "entities": []}, {"text": "We then test the performance of our model on part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relat-edness, controlling for embedding dimen-sionality.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.6847184300422668}, {"text": "named entity recognition", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.6327467262744904}, {"text": "sentiment analysis", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.9440245032310486}, {"text": "semantic relation identification", "start_pos": 115, "end_pos": 147, "type": "TASK", "confidence": 0.6819549004236857}]}, {"text": "We find that multi-sense embed-dings do improve performance on some tasks (part-of-speech tagging, semantic relation identification, semantic relatedness) but not on others (named entity recognition , various forms of sentiment analysis).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.7183578908443451}, {"text": "semantic relation identification", "start_pos": 99, "end_pos": 131, "type": "TASK", "confidence": 0.6598063309987386}, {"text": "named entity recognition", "start_pos": 174, "end_pos": 198, "type": "TASK", "confidence": 0.6486098965009054}, {"text": "sentiment analysis", "start_pos": 218, "end_pos": 236, "type": "TASK", "confidence": 0.8735288977622986}]}, {"text": "We discuss how these differences maybe caused by the different role of word sense information in each of the tasks.", "labels": [], "entities": []}, {"text": "The results highlight the importance of testing embedding models in real applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Enriching vector models of word meaning so they can represent multiple word senses per word type seems to offer the potential to improve many language understanding tasks.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 142, "end_pos": 164, "type": "TASK", "confidence": 0.7014802098274231}]}, {"text": "Most traditional embedding models associate each word type with a single embedding (e.g.,).", "labels": [], "entities": []}, {"text": "Thus the embedding for homonymous words like bank (with senses including 'sloping land' and 'financial institution') is forced to represent some uneasy central tendency between the various meanings.", "labels": [], "entities": []}, {"text": "More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.6996322125196457}]}, {"text": "Early research pointed out that embeddings could model aspects of word sense) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding.", "labels": [], "entities": []}, {"text": "Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments-WS353 () or MC30 (.", "labels": [], "entities": [{"text": "matching human word similarity judgments-WS353", "start_pos": 95, "end_pos": 141, "type": "TASK", "confidence": 0.6998604714870453}]}, {"text": "Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: 1.", "labels": [], "entities": []}, {"text": "Sense-specific representation learning: learn word sense specific embeddings from a large corpus, either unsupervised or aided by external resources like WordNet.", "labels": [], "entities": [{"text": "Sense-specific representation learning", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7243398229281107}, {"text": "WordNet", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.9626928567886353}]}, {"text": "2. Sense induction: given a text unit (a phrase, sentence, document, etc.), infer word senses for its tokens and associate them with corresponding sense-specific embeddings.", "labels": [], "entities": [{"text": "Sense induction", "start_pos": 3, "end_pos": 18, "type": "TASK", "confidence": 0.7504782676696777}]}, {"text": "3. Representation acquisition for phrases or sentences: learn representations for text units given sense-specific embeddings and pass them to machine learning classifiers.", "labels": [], "entities": [{"text": "Representation acquisition for phrases or sentences", "start_pos": 3, "end_pos": 54, "type": "TASK", "confidence": 0.9021788438161215}]}, {"text": "Most existing work on multi-sense embeddings emphasizes the first step by learning sense spe-cific embeddings, but does not explore the next two steps.", "labels": [], "entities": []}, {"text": "These are important steps, however, since it isn't clear how existing multi-sense embeddings can be incorporated into and benefit realworld NLU tasks.", "labels": [], "entities": []}, {"text": "We propose a pipelined architecture to address all three steps and apply it to a variety of NLP tasks: part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.7376717627048492}, {"text": "named entity recognition", "start_pos": 127, "end_pos": 151, "type": "TASK", "confidence": 0.6336463590463003}, {"text": "sentiment analysis", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.9374426305294037}, {"text": "semantic relation identification", "start_pos": 173, "end_pos": 205, "type": "TASK", "confidence": 0.7219197750091553}]}, {"text": "We find: \u2022 Multi-sense embeddings give improved performance in some tasks (e.g., semantic similarity for words and sentences, semantic relation identification part-of-speech tagging), but not others (e.g., sentiment analysis, named entity extraction).", "labels": [], "entities": [{"text": "semantic relation identification part-of-speech tagging", "start_pos": 126, "end_pos": 181, "type": "TASK", "confidence": 0.7591852068901062}, {"text": "sentiment analysis", "start_pos": 206, "end_pos": 224, "type": "TASK", "confidence": 0.9405251741409302}, {"text": "named entity extraction", "start_pos": 226, "end_pos": 249, "type": "TASK", "confidence": 0.6039334336916605}]}, {"text": "In our analysis we offer some suggested explanations for these differences.", "labels": [], "entities": []}, {"text": "\u2022 Some of the improvements for multi-sense embeddings are no longer visible when using more sophisticated neural models like LSTMs which have more flexibility in filtering away the informational chaff from the wheat.", "labels": [], "entities": []}, {"text": "\u2022 It is important to carefully compare against embeddings of the same dimensionality.", "labels": [], "entities": []}, {"text": "\u2022 When doing so, the most straightforward way to yield better performance on these tasks is just to increase embedding dimensionality.", "labels": [], "entities": []}, {"text": "After describing related work, we introduce the new unsupervised sense-learning model in section 3, give our sense-induction algorithm in section 4, and then in following sections evaluate its performance for word similarity, and then various NLP tasks.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 209, "end_pos": 224, "type": "TASK", "confidence": 0.7569199800491333}]}], "datasetContent": [{"text": "We evaluate our embeddings by comparing with other multi-sense embeddings on the standard artificial task for matching human word similarity judgments.", "labels": [], "entities": [{"text": "matching human word similarity judgments", "start_pos": 110, "end_pos": 150, "type": "TASK", "confidence": 0.7745003700256348}]}, {"text": "Early work used similarity datasets like WS353 () or RG (, whose context-free nature makes them a poor evaluation.", "labels": [], "entities": [{"text": "WS353", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.9240733981132507}, {"text": "RG", "start_pos": 53, "end_pos": 55, "type": "DATASET", "confidence": 0.4796295166015625}]}, {"text": "We therefore adopt Stanford's Contextual Word Similarities (SCWS) (, in which human judgments are associated with pairs of words in context.", "labels": [], "entities": [{"text": "Contextual Word Similarities (SCWS)", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.7368229826291403}]}, {"text": "Thus for example \"bank\" in the context of \"river bank\" would have low relatedness with \"deficit\" in the context \"financial deficit\".", "labels": [], "entities": []}, {"text": "We first use the Greedy or Expectation strategies to obtain word vectors for tokens given their context.", "labels": [], "entities": [{"text": "Greedy", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9440097212791443}]}, {"text": "These vectors are then used as input to get the value of cosine similarity between two words.", "labels": [], "entities": []}, {"text": "Consistent with earlier work (e.g..,), we find that multi-sense embeddings result in better performance in the context-dependent SCWS task (SG+Greedy and SG+Expect are better than SG).", "labels": [], "entities": []}, {"text": "As expected, performance is not as high when global level information is ignored when choosing word senses (SG+Greedy) as when it is included (SG+Expect), as neighboring words don't provide sufficient information for word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 217, "end_pos": 242, "type": "TASK", "confidence": 0.7133851150671641}]}, {"text": "To note, the proposed CRF models work a little better than earlier baselines, which gives some evidence that it is sufficiently strong to stand in for this class of multi-sense models and serves as a promise for being extended to NLU tasks.", "labels": [], "entities": []}, {"text": "Visualization shows examples of semantically related words given the local context.", "labels": [], "entities": []}, {"text": "Word embeddings for tokens are obtained by using the inferred sense labels from the Greedy model and are then used to search for nearest neighbors in the vector space based on cosine similarity.", "labels": [], "entities": []}, {"text": "Like earlier models (e.g.,)., the model can disambiguate different word senses (in examples like bank, rock and apple) based on their local context; although of course the model is also capable of dealing with polysemy-senses that are less distinct.", "labels": [], "entities": []}, {"text": "Having shown that multi-sense embeddings improve word similarity tasks, we turn to ask whether they improve real-world NLU tasks: POS tagging, NER tagging, sentiment analysis at the phrase and sentence level, semantic relationship identification and sentence-level semantic relatedness.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.7794180611769358}, {"text": "POS tagging", "start_pos": 130, "end_pos": 141, "type": "TASK", "confidence": 0.8118850290775299}, {"text": "NER tagging", "start_pos": 143, "end_pos": 154, "type": "TASK", "confidence": 0.9125387370586395}, {"text": "sentiment analysis", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.803884893655777}, {"text": "semantic relationship identification", "start_pos": 209, "end_pos": 245, "type": "TASK", "confidence": 0.7341503302256266}]}, {"text": "For each task, we experimented on the following sets of embeddings, which are trained using the word2vec package on the same corpus: \u2022 Standard one-word-one-vector embeddings from skip-gram (50d).", "labels": [], "entities": []}, {"text": "\u2022 Sense disambiguated embeddings from Section 3 and 4 using Greedy Search and Expectation (50d) \u2022 The concatenation of global word embeddings and sense-specific embeddings (100d).", "labels": [], "entities": []}, {"text": "\u2022 Standard one-word-one-vector skip-gram embeddings with dimensionality doubled (100d) (100d is the correct corresponding baseline since the concatenation above doubles the dimensionality of word vectors) \u2022 Embeddings with very high dimensionality (300d).", "labels": [], "entities": []}, {"text": "As far as possible we try to perform an appleto-apple comparison on these tasks, and our goal is an analytic one-to investigate how well semantic information can be encoded in multi-sense embeddings and how they can improve NLU performances-rather than an attempt to create state-of-the-art results.", "labels": [], "entities": []}, {"text": "Thus for example, in tagging tasks (e.g., NER, POS), we follow the protocols in) using the concatenation of neighboring embeddings as input features rather than treating embeddings as auxiliary features which are fed into a CRF model along with other manually developed features as in.", "labels": [], "entities": [{"text": "tagging tasks", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9080179631710052}]}, {"text": "Or for experiments on sentiment and other tasks where sentence level embeddings are required we only employ standard recurrent or recursive models for sentence embedding rather than models with sophisticated state-of-theart methods (e.g.,).", "labels": [], "entities": []}, {"text": "Significance testing for comparing models is done via the bootstrap test.", "labels": [], "entities": []}, {"text": "Unless otherwise noted, significant testing is performed on one-word-one-vector embedding (50d) versus multi-sense embedding using Expectation inference (50d) and one-vector embedding (100d) versus Expectation (100d).", "labels": [], "entities": [{"text": "Expectation", "start_pos": 198, "end_pos": 209, "type": "DATASET", "confidence": 0.5100030899047852}]}, {"text": "we obtained embeddings for tree nodes by using a recursive neural network model, where the embedding for parent node is obtained in a bottom-up fashion based on its children.", "labels": [], "entities": []}, {"text": "The embeddings for each parse tree constituent are output to a softmax layer; see.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performances for different set of multi- sense embeddings (300d) evaluated on SCWS  by measuring the Spearman correlation between  each model's similarity and the human judgments.  Baselines performances are reprinted from Nee- lakantan et al. (2014) and Chen et al. (2014);  we report the best performance across all settings  mentioned in their paper.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 111, "end_pos": 131, "type": "METRIC", "confidence": 0.9032546877861023}]}, {"text": " Table 2: Nearest neighbors of words given context. The embeddings from context words are first in- ferred with the Greedy strategy; nearest neighbors are computed by cosine similarity between word  embeddings. Similar phenomena have been observed in earlier work (Neelakantan et al., 2014)", "labels": [], "entities": []}, {"text": " Table 3: Accuracy for Different Models on  Name Entity Recognition.  Global+E stands  for Global+Expectation inference and Global+G  stands for Global+Greedy inference. p-value  0.223 for Standard(50) verse Expectation (50) and  0.310 for Standard(100) verse Expectation (100).", "labels": [], "entities": [{"text": "Name Entity Recognition", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.8460326790809631}]}, {"text": " Table 4: Accuracy for Different Models on Part of  Speech Tagging. P-value 0.033 for 50d and 0.031  for 100d.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9963311553001404}, {"text": "Part of  Speech Tagging", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.6320741027593613}, {"text": "P-value", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.9924077987670898}]}, {"text": " Table 8: Pearson's r for Different Models on Se- mantic Relatedness for Standard Models. P-value  0.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.6969963113466898}, {"text": "Se- mantic Relatedness", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.6874703764915466}, {"text": "P-value", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9622993469238281}]}, {"text": " Table 9: Pearson's r for Different Models on Se- mantic Relatedness for LSTM Models. P-value  0.145 for 50d and 0.170 for 100d.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.7068150440851847}, {"text": "Se- mantic Relatedness", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7011790126562119}, {"text": "P-value", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9893286824226379}]}]}