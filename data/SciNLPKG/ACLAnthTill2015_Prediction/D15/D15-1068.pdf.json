{"title": [{"text": "Global Thread-Level Inference for Comment Classification in Community Question Answering", "labels": [], "entities": [{"text": "Global Thread-Level Inference", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.588426411151886}, {"text": "Comment Classification in Community Question Answering", "start_pos": 34, "end_pos": 88, "type": "TASK", "confidence": 0.706092526515325}]}], "abstractContent": [{"text": "Community question answering, a recent evolution of question answering in the Web context, allows a user to quickly consult the opinion of a number of people on a particular topic, thus taking advantage of the wisdom of the crowd.", "labels": [], "entities": [{"text": "Community question answering", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6302221616109213}, {"text": "question answering", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7581297755241394}]}, {"text": "Here we try to help the user by deciding automatically which answers are good and which are bad fora given question.", "labels": [], "entities": []}, {"text": "In particular, we focus on exploiting the output structure at the thread level in order to make more consistent global decisions.", "labels": [], "entities": []}, {"text": "More specifically, we exploit the relations between pairs of comments at any distance in the thread, which we incorporate in a graph-cut and in an ILP frameworks.", "labels": [], "entities": []}, {"text": "We evaluated our approach on the benchmark dataset of SemEval-2015 Task 3.", "labels": [], "entities": [{"text": "benchmark dataset of SemEval-2015 Task 3", "start_pos": 33, "end_pos": 73, "type": "DATASET", "confidence": 0.7915273904800415}]}, {"text": "Results improved over the state of the art, confirming the importance of using thread level information .", "labels": [], "entities": []}], "introductionContent": [{"text": "Community question answering (CQA) is a recent evolution of question answering, in the Web context, where users pose questions and then receive answers from other users.", "labels": [], "entities": [{"text": "Community question answering (CQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.761844610174497}, {"text": "question answering", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7966664135456085}]}, {"text": "This setup is very attractive, as the anonymity on the Web allows users to ask just about anything and then hope to get some honest answers from a number of people.", "labels": [], "entities": []}, {"text": "On the negative side, there is no guarantee about the quality of the answers as people of very different background, knowledge, and with different motivation contribute answers to a given question.", "labels": [], "entities": []}, {"text": "Unlike traditional question answering (QA), in CQA answering takes the form of commenting in a forum.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.8595575034618378}, {"text": "CQA answering", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.7301915735006332}]}, {"text": "Thus, many comments are only loosely connected to the original question, and some are not answers at all, but are rather interactions between users.", "labels": [], "entities": []}, {"text": "As question-comment threads can get quite long, finding good answers in a thread can be timeconsuming.", "labels": [], "entities": []}, {"text": "This has triggered research in trying to automatically determine which answers might be good and which ones are likely to be bad or irrelevant.", "labels": [], "entities": []}, {"text": "One early work going in this direction is that of, who tried to determine whether a question is \"solved\" or not, given its associated thread of comments.", "labels": [], "entities": []}, {"text": "As a first step in the process, they performed a comment-level classification, considering four classes: problem, solution, good feedback, and bad feedback.", "labels": [], "entities": []}, {"text": "More recently, the shared task at SemEval 2015 on Answer Selection in CQA ( , whose benchmark datasets we will use below, tackled the task of identifying good, potentially useful, and bad comments within a thread.", "labels": [], "entities": [{"text": "Answer Selection in CQA", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.8000314682722092}]}, {"text": "In that task, the top participating systems used threadlevel features, in addition to the usual local features that only look at the question-answer pair.", "labels": [], "entities": []}, {"text": "For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (.", "labels": [], "entities": [{"text": "HITSZ-ICRC", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.7463758587837219}]}, {"text": "Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (.", "labels": [], "entities": []}, {"text": "Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (.", "labels": [], "entities": [{"text": "ICRC-HIT", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.9091194868087769}, {"text": "answer selection task", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.889463742574056}]}, {"text": "Ina follow-up work, included a long-short term memory in their convolution neural network to learn the classification sequence for the thread.", "labels": [], "entities": []}, {"text": "In parallel, in our recent work , we tried to exploit the dependencies between the thread comments to tackle the same task.", "labels": [], "entities": []}, {"text": "We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields ().", "labels": [], "entities": []}, {"text": "Our goal in this paper goes in the same direction: we are interested in exploiting the output structure at the thread level to make more consistent global assignments.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there is no work in QA that identifies good answers based on the selection of the other answers retrieved fora question.", "labels": [], "entities": []}, {"text": "This is mainly due to the loose dependencies between the different answer passages in standard QA.", "labels": [], "entities": []}, {"text": "In contrast, we postulate that in a CQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9917629957199097}]}, {"text": "In particular, we focus on the relations between two comments at any distance in the thread.", "labels": [], "entities": []}, {"text": "This is more general than previous approaches, which were either limited to sequential interactions or considered conversational interactions only at the level of features.", "labels": [], "entities": []}, {"text": "We propose a model based on the idea that similar comments should have similar labels.", "labels": [], "entities": []}, {"text": "Below, we apply graph-cut and we compare it to an integer linear programming (ILP) formulation for decoding under global constraints; we also provide results with a linear-chain CRF.", "labels": [], "entities": []}, {"text": "We show that the CRF is ineffective due to long-distance relations, e.g., a conversation in a thread can branch and then comeback later.", "labels": [], "entities": [{"text": "CRF", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.8968949913978577}]}, {"text": "On the contrary, the global inference models (either graph-cut or ILP) using the similarity between pairs of comments manage to significantly improve a strong baseline performing local comment-based classifications.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed standard pre-processing, and we further filtered user's signatures.", "labels": [], "entities": []}, {"text": "All parameters (e.g., Gaussian prior for MaxEnt and the mixing \u03bb for the graph-cut and ILP) were tuned on the development set.", "labels": [], "entities": [{"text": "Gaussian prior", "start_pos": 22, "end_pos": 36, "type": "METRIC", "confidence": 0.9484284818172455}]}, {"text": "We also trained a second-order linear-chain CRF to check the contribution of the sequential relations between comments.", "labels": [], "entities": []}, {"text": "We report results on the official SemEval test set for all methods.", "labels": [], "entities": [{"text": "SemEval test set", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.8910457889238993}]}, {"text": "For the Same-vs-Different problem, we explored a variant of training with three classes, by splitting the Same class into Same-Good and Same-Bad.", "labels": [], "entities": []}, {"text": "At test time, the probabilities of these two subclasses are added to get the probability of Same and all the algorithms are run unchanged.", "labels": [], "entities": []}, {"text": "Subtracting vectors is standard in preference learning.", "labels": [], "entities": [{"text": "preference learning", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7373419404029846}]}, {"text": "The absolute value is necessary to emphasize comment differences instead of preferences.: Same-vs-Different classification.", "labels": [], "entities": []}, {"text": "P, R, and F 1 are calculated with respect to Same.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9930163621902466}]}, {"text": "shows the results for the Same-vsDifferent classification.", "labels": [], "entities": []}, {"text": "We can see that the twoclass MaxEnt-2C classifier works better than the three-class MaxEnt-3C.", "labels": [], "entities": []}, {"text": "MaxEnt-3C has more balanced P and R, but loses in both F 1 and accuracy.", "labels": [], "entities": [{"text": "MaxEnt-3C", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9186581969261169}, {"text": "P", "start_pos": 28, "end_pos": 29, "type": "METRIC", "confidence": 0.979251503944397}, {"text": "R", "start_pos": 34, "end_pos": 35, "type": "METRIC", "confidence": 0.8975133895874023}, {"text": "F 1", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.988052248954773}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9986253976821899}]}, {"text": "MaxEnt-2C is very skewed towards the majority class, but performs better due to the class imbalance.", "labels": [], "entities": [{"text": "MaxEnt-2C", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9000017642974854}]}, {"text": "Overall, it seems very difficult to learn with the current features, and both methods only outperform the majority-class baseline by a small margin.", "labels": [], "entities": []}, {"text": "Yet, while the overall accuracy is low, note that the graph-cut/ILP inference allows us to recover from some errors, because if nearby utterances are clustered correctly, the wrong decisions should be outvoted by correct ones.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9995893836021423}]}, {"text": "The results for Good-vs-Bad are shown in Table 3.", "labels": [], "entities": []}, {"text": "On the top are the best systems at SemEval-2015 Task 3.", "labels": [], "entities": [{"text": "SemEval-2015 Task 3", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7176895538965861}]}, {"text": "We can see that our MaxEnt classifier is competitive: it shows higher accuracy than two of them, and the highest F 1 overall.: Good-vs-Bad classification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9992535710334778}, {"text": "F 1", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.98966845870018}]}, {"text": "\u2021 and \u2020 mark statistically significant differences inaccuracy compared to the baseline MaxEnt classifier with confidence levels of 99% and 95%, respectively (randomized test).", "labels": [], "entities": []}, {"text": "The CRF model is worse than MaxEnt on all measures, which suggests that the sequential information does not help.", "labels": [], "entities": []}, {"text": "This can be because many interactions between comments are long-distance and there are gaps in the threads due to the annotation procedure at SemEval ( . However, global inference with graph-cut and ILP improves both F 1 and accuracy, mostly due to better recall.", "labels": [], "entities": [{"text": "F 1", "start_pos": 217, "end_pos": 220, "type": "METRIC", "confidence": 0.9928634762763977}, {"text": "accuracy", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.998487114906311}, {"text": "recall", "start_pos": 256, "end_pos": 262, "type": "METRIC", "confidence": 0.9961323738098145}]}, {"text": "Graph-cut works better than ILP as it has higher precision, which helps F 1 and accuracy.", "labels": [], "entities": [{"text": "Graph-cut", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.927155077457428}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9993487000465393}, {"text": "F 1", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9664374589920044}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9989683628082275}]}, {"text": "Both yield statistically significant improvements over the MaxEnt classifier; they also improve over the state-of-the-art JAIST system.", "labels": [], "entities": []}, {"text": "Note that the devtest-tuned values of \u03bb for graph-cut and ILP put much lower weight to the Same-vsDifferent component (values are 0.95 and 0.91, respectively).", "labels": [], "entities": []}, {"text": "Finally, as expected, using the predictions of MaxEnt-2C in the global classifiers is better than using those from MaxEnt-3C.", "labels": [], "entities": []}, {"text": "Q: I have a female friend who is leaving fora teaching job in Qatar in January.", "labels": [], "entities": []}, {"text": "What would be a useful portable gift to give her to take with her?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics about the CQA-QL dataset:  after merging Bad and Potential into Bad.", "labels": [], "entities": [{"text": "CQA-QL dataset", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.9781284630298615}]}, {"text": " Table 2: Same-vs-Different classification. P, R,  and F 1 are calculated with respect to Same.", "labels": [], "entities": [{"text": "F 1", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.977748692035675}]}, {"text": " Table 3: Good-vs-Bad classification.  \u2021 and  \u2020  mark statistically significant differences in accu- racy compared to the baseline MaxEnt classifier  with confidence levels of 99% and 95%, respec- tively (randomized test).", "labels": [], "entities": [{"text": "accu- racy", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9476466178894043}]}]}