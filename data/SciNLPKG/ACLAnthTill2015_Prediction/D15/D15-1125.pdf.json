{"title": [{"text": "Investigating Continuous Space Language Models for Machine Translation Quality Estimation", "labels": [], "entities": [{"text": "Machine Translation Quality Estimation", "start_pos": 51, "end_pos": 89, "type": "TASK", "confidence": 0.8598299622535706}]}], "abstractContent": [{"text": "We present novel features designed with a deep neural network for Machine Translation (MT) Quality Estimation (QE).", "labels": [], "entities": [{"text": "Machine Translation (MT) Quality Estimation (QE)", "start_pos": 66, "end_pos": 114, "type": "TASK", "confidence": 0.7043680995702744}]}, {"text": "The features are learned with a Continuous Space Language Model to estimate the probabilities of the source and target segments.", "labels": [], "entities": []}, {"text": "These new features, along with standard MT system-independent features, are benchmarked on a series of datasets with various quality labels, including post-editing effort, human translation edit rate, post-editing time and METEOR.", "labels": [], "entities": [{"text": "MT system-independent", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.8418637812137604}, {"text": "METEOR", "start_pos": 223, "end_pos": 229, "type": "METRIC", "confidence": 0.9645124673843384}]}, {"text": "Results show significant improvements in prediction over the baseline, as well as over systems trained on state of the art feature sets for all datasets.", "labels": [], "entities": []}, {"text": "More notably, the addition of the newly proposed features improves over the best QE systems in WMT12 and WMT14 by a significant margin.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.9702121019363403}, {"text": "WMT14", "start_pos": 105, "end_pos": 110, "type": "DATASET", "confidence": 0.9267054200172424}]}], "introductionContent": [{"text": "Quality Estimation (QE) is concerned with predicting the quality of Machine Translation (MT) output without reference translations.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7421390354633332}, {"text": "predicting the quality of Machine Translation (MT) output", "start_pos": 42, "end_pos": 99, "type": "TASK", "confidence": 0.7243962585926056}]}, {"text": "QE is addressed with various features indicating fluency, adequacy and complexity of the translation pair.", "labels": [], "entities": []}, {"text": "These features are used by a machine learning algorithm along with quality labels given by humans to learn models to predict the quality of unseen translations.", "labels": [], "entities": []}, {"text": "A variety of features play a key role in QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9442849159240723}]}, {"text": "A wide range of features from source segments and their translated segments, extracted with the help of external resources and tools, have been proposed.", "labels": [], "entities": []}, {"text": "These go from simple, languageindependent features, to advanced, linguistically motivated features.", "labels": [], "entities": []}, {"text": "They include features that summarise how the MT systems generate translations, as well as features that are oblivious to the systems.", "labels": [], "entities": [{"text": "MT systems generate translations", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.6691165119409561}]}, {"text": "The majority of the features in the literature are extracted from each sentence pair in isolation, ignoring the context of the text.", "labels": [], "entities": []}, {"text": "QE performance usually differs depending on the language pair, the specific quality score being optimised (e.g., post-editing time vs translation adequacy) and the feature set.", "labels": [], "entities": []}, {"text": "Features based on n-gram language models, despite their simplicity, are among those with the best performance inmost QE tasks ().", "labels": [], "entities": []}, {"text": "However, they may not generalise well due to the underlying discrete nature of words in n-gram modelling.", "labels": [], "entities": []}, {"text": "Continuous Space Language Models (CSLM), on the other hand, have shown their potential to capture long distance dependencies among words).", "labels": [], "entities": []}, {"text": "The assumption of these models is that semantically or grammatically related words are mapped to similar geometric locations in a high-dimensional continuous space.", "labels": [], "entities": []}, {"text": "The probability distribution is thus much smoother and therefore the model has a better generalisation power on unseen events.", "labels": [], "entities": []}, {"text": "The representations are learned in a continuous space to estimate the probabilities using neural networks with single (called shallow networks) or multiple (called deep networks) hidden layers.", "labels": [], "entities": []}, {"text": "Deep neural networks have been shown to perform better than shallow ones due to their capability to learn higher-level, abstract representations of the input (.", "labels": [], "entities": []}, {"text": "In this paper, we explore the potential of these models in context of QE for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9881949424743652}]}, {"text": "We obtain more robust features with CSLM and improve the overall prediction power for translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.9602535367012024}]}, {"text": "The paper is organised as follows: In Section 2 we briefly present the related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the CSLM model training and its various settings.", "labels": [], "entities": []}, {"text": "In Section 4 we propose the use of CSLM features for QE.", "labels": [], "entities": []}, {"text": "In Section 5 we present our experiments along with their results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We focus on experiments with sentence level QE tasks.", "labels": [], "entities": [{"text": "sentence level QE tasks", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.6159471422433853}]}, {"text": "Our English-Spanish experiments are based on the WMT QE shared task data from 2012 to 2015.", "labels": [], "entities": [{"text": "WMT QE shared task data from 2012", "start_pos": 49, "end_pos": 82, "type": "DATASET", "confidence": 0.8600270748138428}]}, {"text": "1 These tasks are diverse in nature, with different sizes and labels such as post-editing effort (PEE), post-editing time (PET) and human translation error rate (HTER).", "labels": [], "entities": [{"text": "post-editing effort (PEE)", "start_pos": 77, "end_pos": 102, "type": "METRIC", "confidence": 0.76309734582901}, {"text": "post-editing time (PET)", "start_pos": 104, "end_pos": 127, "type": "METRIC", "confidence": 0.8369751930236816}, {"text": "human translation error rate (HTER)", "start_pos": 132, "end_pos": 167, "type": "METRIC", "confidence": 0.7120942047664097}]}, {"text": "The results reported in Section 5.5 are directly comparable with the official systems submitted for each of the respective tasks.", "labels": [], "entities": []}, {"text": "We also performed experiments on the IWSLT 2014 English-French SLT task 2 to study the applicability of our models on n-best ASR (MT inputs) comparison.", "labels": [], "entities": [{"text": "IWSLT 2014 English-French SLT task 2", "start_pos": 37, "end_pos": 73, "type": "DATASET", "confidence": 0.8026713530222574}]}, {"text": "In we summarise the data and tasks for our experiments.", "labels": [], "entities": []}, {"text": "We refer readers to the WMT and IWSLT websites for detailed descriptions of these datasets.", "labels": [], "entities": [{"text": "WMT", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.8791530728340149}, {"text": "IWSLT websites", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.7686165869235992}]}, {"text": "All datasets are publicly available.", "labels": [], "entities": []}, {"text": "WMT12: English-Spanish news sentence translations produced by a Moses \"baseline\" statistical MT (SMT) system, and judged for perceived post-editing effort in 1-5 (highest-lowest), taking a weighted average of three annotators).", "labels": [], "entities": [{"text": "WMT12", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9637454152107239}, {"text": "English-Spanish news sentence translations produced", "start_pos": 7, "end_pos": 58, "type": "TASK", "confidence": 0.6851086318492889}]}, {"text": "WMT13 (Task-1): English-Spanish sentence translations of news texts produced by a Moses \"baseline\" SMT system.", "labels": [], "entities": [{"text": "WMT13", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9612533450126648}, {"text": "English-Spanish sentence translations of news texts produced", "start_pos": 16, "end_pos": 76, "type": "TASK", "confidence": 0.8255481464522225}, {"text": "SMT", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9247101545333862}]}, {"text": "These were then postedited by a professional translator and labelled using HTER.", "labels": [], "entities": [{"text": "HTER", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.8761992454528809}]}, {"text": "This is a superset of the WMT12 dataset, with 500 additional sentences for test, and a different quality label ().", "labels": [], "entities": [{"text": "WMT12 dataset", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.9591752290725708}]}, {"text": "WMT14 (Task-1.1): English-Spanish news sentence translations.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9644951820373535}, {"text": "English-Spanish news sentence translations", "start_pos": 18, "end_pos": 60, "type": "TASK", "confidence": 0.6043568700551987}]}, {"text": "The dataset contains source sentences and their human translations, as well as three versions of machine translations: by an SMT system, a rule-based system system and a hybrid system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9598872065544128}]}, {"text": "Each translation was labelled by professional translators with 1-3 (lowest-highest) scores for perceived post-editing effort.", "labels": [], "entities": []}, {"text": "IWSLT14: English-French dataset containing source language data from the 10-best (sentences) ASR system output.", "labels": [], "entities": [{"text": "IWSLT14", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.920292317867279}]}, {"text": "On the target side, the 1-best MT translation is used.", "labels": [], "entities": [{"text": "MT translation", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.8272348940372467}]}, {"text": "The ASR system leads to different source segments, which in turn lead to different translations.", "labels": [], "entities": [{"text": "ASR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9319413304328918}]}, {"text": "METEOR () is used to label these alternative translations against a reference (human) translation.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7081096768379211}]}, {"text": "Both ASR and MT outputs come from a system submission in IWSLT 2014 ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.782163679599762}, {"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9625642895698547}, {"text": "IWSLT 2014", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.938382625579834}]}, {"text": "The ASR system is a multi-pass deep neural network tandem system with feature and model adaptation and rescoring.", "labels": [], "entities": [{"text": "ASR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9093546867370605}]}, {"text": "The MT system is a phrasebased SMT system produced using Moses.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8736217021942139}, {"text": "SMT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.8684488534927368}]}, {"text": "Lang: QE datasets: # sentences and labels.", "labels": [], "entities": [{"text": "QE datasets", "start_pos": 6, "end_pos": 17, "type": "DATASET", "confidence": 0.8698940873146057}]}, {"text": "The dataset used for CSLM training consists of Europarl, News-commentary and News-crawl corpus.", "labels": [], "entities": [{"text": "CSLM training", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9133485853672028}, {"text": "Europarl", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.9738674759864807}, {"text": "News-commentary", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.9292820692062378}, {"text": "News-crawl corpus", "start_pos": 77, "end_pos": 94, "type": "DATASET", "confidence": 0.9357633292675018}]}, {"text": "We used a data selection method to select the most relevant training data with respect to a development set.", "labels": [], "entities": []}, {"text": "For English-Spanish, the development data is the concatenation of newstest2012 and newstest2013 of the WMT translation track.", "labels": [], "entities": [{"text": "WMT translation track", "start_pos": 103, "end_pos": 124, "type": "DATASET", "confidence": 0.7244249979654948}]}, {"text": "For English-French, the development set is the concatenation of the IWSLT dev2010 and eval2010.", "labels": [], "entities": [{"text": "IWSLT dev2010", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.9497134387493134}]}, {"text": "In we show statistics on the selected monolingual data used to train back-off LM and CSLM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: QE datasets: # sentences and labels.", "labels": [], "entities": [{"text": "QE datasets", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.8430423736572266}]}, {"text": " Table 2: Training data size (number of tokens) and  language models perplexity (ppl). The values in  parentheses in last column shows percentage de- crease in perplexity.", "labels": [], "entities": []}, {"text": " Table 3: Results for datasets with various feature  sets. Figures with  *  beat the official best systems,  and with  *  *  are second best. Results with CSLM  features are significantly better than BL and AF on  all tasks (paired t-test with p \u2264 0.05).", "labels": [], "entities": [{"text": "BL", "start_pos": 200, "end_pos": 202, "type": "METRIC", "confidence": 0.9961930513381958}, {"text": "AF", "start_pos": 207, "end_pos": 209, "type": "METRIC", "confidence": 0.917868435382843}]}, {"text": " Table 4: Impact of different combinations of  CSLM features on the WMT12 task. Figures with   *  beat the official best system. Results with CSLM  features are significantly better than BL and AF on  all tasks (paired t-test with p \u2264 0.05).", "labels": [], "entities": [{"text": "WMT12 task", "start_pos": 68, "end_pos": 78, "type": "TASK", "confidence": 0.5533665418624878}, {"text": "BL", "start_pos": 187, "end_pos": 189, "type": "METRIC", "confidence": 0.9973364472389221}, {"text": "AF", "start_pos": 194, "end_pos": 196, "type": "METRIC", "confidence": 0.9132835268974304}]}, {"text": " Table 5: MAE score on official WMT12 feature  sets using SVR with and without CSLM features.", "labels": [], "entities": [{"text": "MAE score", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9683997333049774}, {"text": "WMT12 feature  sets", "start_pos": 32, "end_pos": 51, "type": "DATASET", "confidence": 0.9288258751233419}]}]}