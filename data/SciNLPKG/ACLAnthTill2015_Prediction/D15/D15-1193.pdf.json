{"title": [], "abstractContent": [{"text": "A prerequisite relation describes a basic relation among concepts in cognition, education and other areas.", "labels": [], "entities": []}, {"text": "However, as a semantic relation, it has not been well studied in computational linguistics.", "labels": [], "entities": []}, {"text": "We investigate the problem of measuring prerequisite relations among concepts and propose a simple link-based metric, namely reference distance (RefD), that effectively models the relation by measuring how differently two concepts refer to each other.", "labels": [], "entities": [{"text": "reference distance (RefD)", "start_pos": 125, "end_pos": 150, "type": "METRIC", "confidence": 0.8843799352645874}]}, {"text": "Evaluations on two datasets that include seven domains show that our single metric based method outperforms existing supervised learning based methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "What should one know/learn before starting to learn anew area such as \"deep learning\"?", "labels": [], "entities": []}, {"text": "A key for answering this question is to understand what a prerequisite is.", "labels": [], "entities": []}, {"text": "A prerequisite is usually a concept or requirement before one can proceed to a following one.", "labels": [], "entities": [{"text": "prerequisite", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.9611061811447144}]}, {"text": "And the prerequisite relation exists as a natural dependency among concepts in cognitive processes when people learn, organize, apply, and generate knowledge).", "labels": [], "entities": []}, {"text": "While there has been serious effort in understanding prerequisite relations in learning and education), it has not been well studied as a semantic relation in computational linguistics, where researchers focus more on lexical relations among lexical items and fine-grained entity relations in knowledge bases (.", "labels": [], "entities": []}, {"text": "Instead of treating it as a relation extraction or link prediction problem using traditional machine learning approaches, we seek to better understand prerequisite relations from a perspective of cognitive semantics).", "labels": [], "entities": [{"text": "relation extraction or link prediction", "start_pos": 28, "end_pos": 66, "type": "TASK", "confidence": 0.781791603565216}]}, {"text": "Partially motivated by the theory of frame semantics), or, to understand a concept, one needs to understand all the related concepts in its \"frame\", we propose a metric that measures prerequisite relations based on a simple observation of human learning.", "labels": [], "entities": []}, {"text": "When learning concept A, if one needs to refer to concept B fora lot of A's related concepts but not vice versa, B would more likely be a prerequisite of A than A of B.", "labels": [], "entities": []}, {"text": "Specifically, we model a concept in a vector space using its related concepts and measure the prerequisite relation between two concepts by computing how differently the two's related concepts refer to each other, or reference distance (RefD).", "labels": [], "entities": [{"text": "reference distance (RefD)", "start_pos": 217, "end_pos": 242, "type": "METRIC", "confidence": 0.8534126877784729}]}, {"text": "Our simple metric RefD successfully reflects some properties of the prerequisite relation such as asymmetry and irreflexivity; and can be properly implemented for various applications using different concept models.", "labels": [], "entities": []}, {"text": "We present an implementation of the metric using Wikipedia by leveraging the links as reference relations among concepts; and present a scalable prerequisite dataset construction method by crawling public available university course prerequisite websites and mapping them to Wikipedia concepts.", "labels": [], "entities": []}, {"text": "Experimental results on two datasets that include seven domains demonstrate its effectiveness and robustness on measuring prerequisites.", "labels": [], "entities": []}, {"text": "Surprisingly, our single metric based approach significantly outperforms baselines which use more sophisticated supervised learning.", "labels": [], "entities": []}, {"text": "All the datasets are publicly available upon request.", "labels": [], "entities": []}, {"text": "Our main contributions include: \u2022 A novel metric to measure the prerequisite relation among concepts that outperforms existing supervised learning baselines.", "labels": [], "entities": []}, {"text": "\u2022 A new dataset containing 1336 concept pairs in Computer Science and Math.", "labels": [], "entities": []}, {"text": "Figure 1: An example of the reference structure for two concepts (\"Data mining\" and \"Algorithm\") with a prerequisite relation.", "labels": [], "entities": [{"text": "Data mining", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.655062809586525}]}], "datasetContent": [{"text": "In order to evaluate the proposed metric, we apply it to predicting prerequisite relations in Wikipedia, i.e., whether one article in Wikipedia is a prerequisite of another article.", "labels": [], "entities": []}, {"text": "Given a pair of concepts (A,B), we predict whether B is a prerequisite of  B and pairs where no prerequisite relation exists will be viewed as negative examples.", "labels": [], "entities": []}, {"text": "RefD is tested on two datasets: CrowdComp dataset () and a Course prerequisite dataset collected by us.", "labels": [], "entities": [{"text": "RefD", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.5230411291122437}, {"text": "CrowdComp dataset", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.8105774521827698}, {"text": "Course prerequisite dataset collected", "start_pos": 59, "end_pos": 96, "type": "DATASET", "confidence": 0.672215074300766}]}, {"text": "We compare RefD with a Maximum Entropy (MaxEnt) classifier which exploits graph-based features such as PageRank scores and content-based features such as the category information, whether a title of concept is mentioned in the first sentence of the other concept, the number of times a concept is linked from the other, etc.", "labels": [], "entities": []}, {"text": "(. All experiments use a Wikipedia dump of Dec 8, 2014.", "labels": [], "entities": [{"text": "Wikipedia dump of Dec 8", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.9107876181602478}]}, {"text": "The CrowdComp dataset was collected using Amazon Mechanical Turk by.", "labels": [], "entities": [{"text": "CrowdComp dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8820067942142487}, {"text": "Amazon Mechanical Turk", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.9525567094484965}]}, {"text": "It contains binary-labeled concept pairs from five different domains, including meiosis, public-key cryptography, the parallel postulate, Newton's laws of motion, and global warming.", "labels": [], "entities": []}, {"text": "The label of the prerequisite relation for each pair is assigned using majority vote.", "labels": [], "entities": []}, {"text": "Details of the dataset are shown in lists the accuracies of different methods.", "labels": [], "entities": []}, {"text": "In terms of average performance, RefD achieves comparable average accuracy as MaxEnt.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.980767548084259}, {"text": "MaxEnt", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.9001009464263916}]}, {"text": "When TFIDF is used to calculate w, RefD performs better than MaxEnt.", "labels": [], "entities": []}, {"text": "Also we notice that our implementation of MaxEnt classifier achieves higher accuracy than reported in the original paper, which maybe due to the difference between Wiki dumps used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9989142417907715}]}, {"text": "In addition, we can see that there are large differences in performance across different domains, which is mainly due to two reasons.", "labels": [], "entities": []}, {"text": "First, the coverage of Wikipedia for different domains may vary a lot.", "labels": [], "entities": []}, {"text": "Some domains are more popular and thus edited more frequently, leading to a better quality of articles and a more complete link structure.", "labels": [], "entities": []}, {"text": "Second, since the ground-truth labels are collected by crowdsourcing and there is no guarantee for workers' knowledge about a certain domain, the quality of labels for different domains varies.", "labels": [], "entities": []}, {"text": "We also built a Course dataset with the help of information available on a university's course website containing prerequisite relations between courses.", "labels": [], "entities": [{"text": "Course dataset", "start_pos": 16, "end_pos": 30, "type": "DATASET", "confidence": 0.8464415967464447}]}, {"text": "For example, \"CS 331 Data Structures and Algorithms\" is a prerequisite for \"CS 422 Data mining\".", "labels": [], "entities": [{"text": "CS 422 Data mining", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.5962428376078606}]}, {"text": "We get the prerequisite pairs by crawling the website and linking the course to Wikipedia using simple rules such as title matching and content similarity.", "labels": [], "entities": [{"text": "title matching", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.6993100643157959}]}, {"text": "In order to get negative samples, we randomly sample 600 pairs using concepts appearing in the prerequisite pairs.", "labels": [], "entities": []}, {"text": "All pairs are then checked by two domain experts by removing pairs with incorrect labels.", "labels": [], "entities": []}, {"text": "validation and classes are balanced by oversampling the minority class.", "labels": [], "entities": []}, {"text": "lists the performance comparison of different methods on accuracy, precision, recall and F1 score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.999687671661377}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9995961785316467}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9997636675834656}, {"text": "F1 score", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9837238788604736}]}, {"text": "We can see that RefD outperforms MaxEnt in terms of accuracy, recall, and F1 score on both CS and MATH domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9996732473373413}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9997128844261169}, {"text": "F1 score", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9842094779014587}]}, {"text": "Because MaxEnt relies on many features but there are only limited distinct positive samples in the dataset, it is more likely to overfit the training data, which leads to high precision but low recall on test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.9977906942367554}, {"text": "recall", "start_pos": 194, "end_pos": 200, "type": "METRIC", "confidence": 0.9986076951026917}]}, {"text": "In order to better compare precision and recall, we plot the Precision-Recall curves of different methods, as shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9994251728057861}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9991293549537659}, {"text": "Precision-Recall", "start_pos": 61, "end_pos": 77, "type": "METRIC", "confidence": 0.9947686195373535}]}, {"text": "RefD shows a clear improvement in the area under the Precision-Recall curve.", "labels": [], "entities": [{"text": "RefD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7924783229827881}, {"text": "Precision-Recall", "start_pos": 53, "end_pos": 69, "type": "METRIC", "confidence": 0.9114423990249634}]}, {"text": "Comparing two weighting methods, we find that TFIDF performs slightly better than EQUAL on CS while EQUAL has higher scores than TFIDF on MATH.", "labels": [], "entities": [{"text": "MATH", "start_pos": 138, "end_pos": 142, "type": "DATASET", "confidence": 0.7742012739181519}]}, {"text": "Since how to compute win RefD is a crucial problem, our ongoing work is to explore more sophisticated semantic representations to measure prerequisite relations.", "labels": [], "entities": []}, {"text": "A natural extension to the two simple methods here is to represent a concept using WordNet, Explicit Semantic Analysis (, or Word2vec embeddings ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9618510603904724}]}, {"text": "Incorporating these representations may improve the performance of RefD.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of CrowdComp and Course  Datasets", "labels": [], "entities": [{"text": "Course  Datasets", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.7832944095134735}]}, {"text": " Table 2: Comparison of out-of-domain training  accuracies of a MaxEnt classifier and RefD using  EQUAL and TFIDF weighting. MaxEnt  \u2020 is the  number reported by Talukdar et al. (2012). Max- Ent shows the performance of our implementation.  * indicates the difference between RefD and Max- Ent is statistically significant (p < 0.01).", "labels": [], "entities": []}, {"text": " Table 3: Comparison of in-domain training accu- racies, precision, recall, and F1 measure of Max- Ent and RefD using EQUAL and TFIDF weight- ing. * indicates the improvement over MaxEnt is  statistically significant (p < 0.01).", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.999599039554596}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9990918636322021}, {"text": "F1", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.9998360872268677}, {"text": "Max- Ent", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.8289390802383423}, {"text": "RefD", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.8876423239707947}, {"text": "TFIDF weight- ing", "start_pos": 128, "end_pos": 145, "type": "METRIC", "confidence": 0.9110201001167297}]}, {"text": " Table 4: RefD scores between \"deep learning\" and the concepts linked from it. All scores are calculated  by RefD('deep learning', concept).", "labels": [], "entities": [{"text": "RefD", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.8651281595230103}, {"text": "RefD", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.8869721293449402}]}]}