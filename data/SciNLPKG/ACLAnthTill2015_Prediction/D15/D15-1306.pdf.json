{"title": [{"text": "That's So Annoying!!!: A Lexical and Frame-Semantic Embedding Based Data Augmentation Approach to Automatic Categorization of Annoying Behaviors using #petpeeve Tweets *", "labels": [], "entities": [{"text": "Frame-Semantic Embedding Based Data Augmentation Approach", "start_pos": 37, "end_pos": 94, "type": "TASK", "confidence": 0.74562007188797}]}], "abstractContent": [{"text": "We propose a novel data augmentation approach to enhance computational behav-ioral analysis using social media text.", "labels": [], "entities": [{"text": "computational behav-ioral analysis", "start_pos": 57, "end_pos": 91, "type": "TASK", "confidence": 0.6120935281117758}]}, {"text": "In particular, we collect a Twitter corpus of the descriptions of annoying behaviors using the #petpeeve hashtags.", "labels": [], "entities": []}, {"text": "In the qualitative analysis, we study the language use in these tweets, with a special focus on the fine-grained categories and the geographic variation of the language.", "labels": [], "entities": []}, {"text": "In quantitative analysis, we show that lexical and syntactic features are useful for automatic categorization of annoying behaviors , and frame-semantic features further boost the performance; that leveraging large lexical embeddings to create additional training instances significantly improves the lexical model; and incorporating frame-semantic embedding achieves the best overall performance.", "labels": [], "entities": [{"text": "automatic categorization of annoying behaviors", "start_pos": 85, "end_pos": 131, "type": "TASK", "confidence": 0.6583666265010834}]}], "introductionContent": [{"text": "In the ever-expanding era of social media, many scientific disciplines, such as health and healthcare, biology, and learning sciences, have adopted computational approaches to exploit patterns and behaviors in large datasets (.", "labels": [], "entities": []}, {"text": "In contrast, the primary methods for behavioral sciences still rely on lab experiments with limited amount of subjects, which are time consuming and financially expensive.", "labels": [], "entities": []}, {"text": "In addition to this, it is also difficult to obtain a set of samples with geograph- * We understand that many people find long titles annoying, so we intentionally use a very long one to help people understand what \"pet peeve\" means.", "labels": [], "entities": []}, {"text": "ical variations in traditional lab-based behavioral experiments.", "labels": [], "entities": []}, {"text": "While the social media data are abundantly available, computational approaches to behavioral sciences using Twitter are not well-studied.", "labels": [], "entities": []}, {"text": "Even when statistical techniques are applied to these tasks, their concentration has been on simple statistical significance tests and descriptive statistics.", "labels": [], "entities": []}, {"text": "Therefore, we believe that statistical natural language processing techniques are needed for insightful analysis and interpretation in behavioral studies.", "labels": [], "entities": [{"text": "statistical natural language processing", "start_pos": 27, "end_pos": 66, "type": "TASK", "confidence": 0.6293115839362144}, {"text": "insightful analysis and interpretation", "start_pos": 93, "end_pos": 131, "type": "TASK", "confidence": 0.6342851594090462}]}, {"text": "In this paper, we use Twitter as a corpus for computational behavioral science.", "labels": [], "entities": []}, {"text": "More specifically, we focus on a case study of analyzing annoying behaviors.", "labels": [], "entities": []}, {"text": "To do this, we exploit a corpus of 9 million tweets (, and extract the tweets that describe these behaviors using the #petpeeve hashtags.", "labels": [], "entities": []}, {"text": "#petpeeve is a popular Twitter hashtag, which describes behaviors that might be annoying to others.", "labels": [], "entities": []}, {"text": "An example of #petpeeve tweets is shown in.", "labels": [], "entities": []}, {"text": "To facilitate the analysis, we manually annotate 3,375 tweets with 60 fine-grained categories, which will be described in Section 3.", "labels": [], "entities": []}, {"text": "We use a sparse mixedeffects topic model to analyze the salient words in each category, as well as the geographic variations.", "labels": [], "entities": []}, {"text": "We show that lexical, syntactic, and semantic features enhance the automatic categorization of annoying behaviors; and that the performance is further improved with a novel lexical and framesemantic embedding based data augmentation ap-proach.", "labels": [], "entities": []}, {"text": "Our main contributions are three-fold: \u2022 We provide a Twitter corpus with finegrained annotations for computational behavior studies; \u2022 We qualitatively analyze the Twitter language concerning annoying behaviors, with a focus on the topics and geographical variations; \u2022 We propose various linguistic features and a novel data augmentation approach for automatic categorization of annoying behaviors.", "labels": [], "entities": []}, {"text": "We outline related work in the next section.", "labels": [], "entities": []}, {"text": "The dataset is described in Section 3.", "labels": [], "entities": []}, {"text": "We introduce the approach for analyzing #petpeeve Tweets in Section 4.", "labels": [], "entities": []}, {"text": "Experimental results are shown in Section 5.", "labels": [], "entities": []}, {"text": "We discuss possible applications in Section 6, and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Twitter corpus with 9 million sampled messages collected in prior work (, which includes a total of 121K users.", "labels": [], "entities": [{"text": "Twitter corpus", "start_pos": 11, "end_pos": 25, "type": "DATASET", "confidence": 0.7505349218845367}]}, {"text": "The dataset includes latitude and longitude information.", "labels": [], "entities": []}, {"text": "We extract 3,375 tweets 1 with #petpeeve hashtags.", "labels": [], "entities": []}, {"text": "We follow past work to annotate the tweets (): we apply the LDA clustering + human-identification approach to label the categories of the described annoying behaviors in these tweets.", "labels": [], "entities": []}, {"text": "The human annotation process includes two stages: first, the annotators identify the 50 categories from the clustering process, and use these topics as a candi-date label set to annotate the data; in the second stage, the categories are refined (to 60 classes) from the first pass, and the data is re-annotated with the refined human-specified category labels.", "labels": [], "entities": []}, {"text": "Due to the complexity of this fine-grained annotation task, the inter-annotator agreement rate between two annotators is moderate (0.445).", "labels": [], "entities": [{"text": "inter-annotator agreement rate", "start_pos": 64, "end_pos": 94, "type": "METRIC", "confidence": 0.7383429209391276}]}, {"text": "The annotated categories and label distribution 2 of the dataset are shown in.", "labels": [], "entities": []}, {"text": "In our random samples, the states that post the most #petpeeve tweets are NY, MD, CA, NJ, FL, GA, VA, TX, NC, PA, and DC.", "labels": [], "entities": []}, {"text": "In our predictive experiments, we randomly select 60% of tweets for training, and 40% for testing.", "labels": [], "entities": []}, {"text": "Experimental Setup We use the logistic regression model from LibShortText (: The effectiveness of leveraging continuous embeddings to create additional training instances.", "labels": [], "entities": [{"text": "LibShortText", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.9565284252166748}]}, {"text": "Imp.: relative improvement to the baseline without data augmentation.", "labels": [], "entities": []}, {"text": "The best results for each section are highlighted in bold.* indicates that the result is significantly better than the baseline without data augmentation (p < .0001).", "labels": [], "entities": []}, {"text": "as the classifier in our 60-way multi-class classification experiments.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.7090657651424408}]}, {"text": "Grid search is used to select the best hyper-parameter using the training data only.", "labels": [], "entities": []}, {"text": "A final classifier is then trained using the best hyper-parameters and test set results are reported.", "labels": [], "entities": []}, {"text": "We set k = 5 for knn in our data augmentation experiments: the training data is expanded to 5 times of the original size.", "labels": [], "entities": []}, {"text": "We use a paired two-tailed student's t test to assess the statistical significance.", "labels": [], "entities": [{"text": "t test", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9183655381202698}]}, {"text": "Word2Vec is used to train various lexical and semantic embedding models.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.925832986831665}]}, {"text": "We consider three lexical embeddings and one frame-semantic embeddings for data augmentation: 1) GoogleNews Lexical Embeddings trained with 100 billion words (Mikolov et al., 2013b); 2) Twitter Lexical Embeddings trained with 51 million of words; 3) Urban Dictionary lexical embeddings trained with 53 million of words from slang definitions and examples; 4) Twitter Semantic Frame Embeddings trained with 27 million frames.", "labels": [], "entities": []}, {"text": "Varying Feature Sets We compare various features in.", "labels": [], "entities": []}, {"text": "We see that adding shallow partof-speech features does not have a strong effect on the performance, but adding the dependency triples significantly outperforms the lexical baseline.", "labels": [], "entities": []}, {"text": "We see that the semantic frames are particular useful, showing a 7% relative improvement over the baseline.", "labels": [], "entities": []}, {"text": "The Effectiveness of Data Augmentation shows the results of data augmentation.", "labels": [], "entities": [{"text": "Data Augmentation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7693351805210114}]}, {"text": "We see that using the Google News lexical embeddings to augment the training data brings a 6.1% relative F1 improvement over the lexical baseline.", "labels": [], "entities": [{"text": "Google News lexical embeddings", "start_pos": 22, "end_pos": 52, "type": "DATASET", "confidence": 0.9133486449718475}, {"text": "F1", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9996457099914551}]}, {"text": "When considering the additional frame-semantic embeddings from Twitter, our system obtains the best F1 of 0.380, bringing a 3.8% improvement over the no data augmentation baseline with all linguistic features.", "labels": [], "entities": [{"text": "F1", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.9986923336982727}]}], "tableCaptions": [{"text": " Table 2: The salient words for categories of annoying behaviors learned by the sparse additive generative model of text.", "labels": [], "entities": []}, {"text": " Table 5: The effectiveness of leveraging continuous embeddings to create additional training instances. Imp.: relative improve- ment to the baseline without data augmentation. The best results for each section are highlighted in bold.* indicates that the  result is significantly better than the baseline without data augmentation (p < .0001).", "labels": [], "entities": [{"text": "Imp.", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9876511096954346}]}]}