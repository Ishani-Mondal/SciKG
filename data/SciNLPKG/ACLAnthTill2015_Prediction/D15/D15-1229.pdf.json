{"title": [{"text": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset", "labels": [], "entities": [{"text": "Large Scale Chinese Short Text Summarization", "start_pos": 9, "end_pos": 53, "type": "TASK", "confidence": 0.5781658639510473}]}], "abstractContent": [{"text": "Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6438387334346771}, {"text": "text summarization", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.657019317150116}]}, {"text": "Due to the great challenge of constructing the large scale summaries for full text, in this paper , we introduce a large corpus of Chi-nese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public 1.", "labels": [], "entities": [{"text": "Chinese microblogging website Sina Weibo", "start_pos": 194, "end_pos": 234, "type": "DATASET", "confidence": 0.6399550020694733}]}, {"text": "This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text.", "labels": [], "entities": []}, {"text": "We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts.", "labels": [], "entities": []}, {"text": "Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.", "labels": [], "entities": [{"text": "summary generation", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7710161805152893}, {"text": "short text summarization", "start_pos": 180, "end_pos": 204, "type": "TASK", "confidence": 0.5853025118509928}]}], "introductionContent": [{"text": "Nowadays, individuals or organizations can easily share or post information to the public on the social network.", "labels": [], "entities": []}, {"text": "Take the popular Chinese microblogging website (Sina Weibo) as an example, the People's Daily, one of the media in China, posts more than tens of weibos (analogous to tweets) each day.", "labels": [], "entities": [{"text": "Chinese microblogging website (Sina Weibo)", "start_pos": 17, "end_pos": 59, "type": "DATASET", "confidence": 0.6880250360284533}, {"text": "People's Daily", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.8703974882761637}]}, {"text": "Most of these weibos are wellwritten and highly informative because of the text length limitation (less than140 Chinese characters).", "labels": [], "entities": []}, {"text": "Such data is regarded as naturally annotated web resources).", "labels": [], "entities": []}, {"text": "If we can mine these high-quality data from these naturally annotated web resources, it will be beneficial to the research that has been hampered by the lack of data.", "labels": [], "entities": []}, {"text": "In the Natural Language Processing (NLP) community, automatic text summarization is a hot and difficult task.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.5840851962566376}]}, {"text": "A good summarization system should understand the whole text and re-organize the information to generate coherent, informative, and significantly short summaries which convey important information of the original text,.", "labels": [], "entities": [{"text": "summarization", "start_pos": 7, "end_pos": 20, "type": "TASK", "confidence": 0.9861682057380676}]}, {"text": "Most of traditional abstractive summarization methods divide the process into two phrases.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.5206487476825714}]}, {"text": "First, key textual elements are extracted from the original text by using unsupervised methods or linguistic knowledge.", "labels": [], "entities": []}, {"text": "And then, unclear extracted components are rewritten or paraphrased to produce a concise summary of the original text by using linguistic rules or language generation techniques.", "labels": [], "entities": []}, {"text": "Although extensive researches have been done, the linguistic quality of abstractive summary is still far from satisfactory.", "labels": [], "entities": []}, {"text": "Recently, deep learning methods have shown potential abilities to learn representation () and generate language () from large scale data by utilizing GPUs.", "labels": [], "entities": []}, {"text": "Many researchers realize that we are closer to generate abstractive summarizations by using the deep learning methods.", "labels": [], "entities": []}, {"text": "However, the publicly available and high-quality large scale summarization data set is still very rare and not easy to be constructed manually.", "labels": [], "entities": [{"text": "summarization data set", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.8520822326342264}]}, {"text": "For example, the popular document summarization dataset DUC 2 , TAC   per, we take one step back and focus on constructing LCSTS, the Large-scale Chinese Short Text Summarization dataset by utilizing the naturally annotated web resources on Sina Weibo.", "labels": [], "entities": [{"text": "document summarization dataset DUC 2", "start_pos": 25, "end_pos": 61, "type": "DATASET", "confidence": 0.6661564886569977}, {"text": "TAC", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9249247908592224}, {"text": "Large-scale Chinese Short Text Summarization", "start_pos": 134, "end_pos": 178, "type": "TASK", "confidence": 0.5042164027690887}]}, {"text": "shows one weibo posted by the People's Daily.", "labels": [], "entities": [{"text": "the People's Daily", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.8803415894508362}]}, {"text": "In order to convey the import information to the public quickly, it also writes a very informative and short summary (in the blue circle) of the news.", "labels": [], "entities": []}, {"text": "Our goal is to mine a large scale, high-quality short text summarization dataset from these texts.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions: (1) We introduce a large scale Chinese short text summarization dataset.", "labels": [], "entities": [{"text": "Chinese short text summarization", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.4672735035419464}]}, {"text": "To our knowledge, it is the largest one to date; (2) We provide standard splits for the dataset into large scale training set and human labeled test set which will be easier for benchmarking the related methods; (3) We explore the properties of the dataset and sample 10,666 instances for manually checking and scoring the quality of the dataset; (4) We perform recurrent neural network based encoder-decoder method on the dataset to generate summary and get promising results, which can be used as one baseline of the task.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: The experiment result: \"Word\" and  \"Char\" denote the word-based and character- based input respectively.", "labels": [], "entities": []}]}