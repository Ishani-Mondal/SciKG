{"title": [{"text": "Can Symbol Grounding Improve Low-Level NLP? Word Segmentation as a Case Study", "labels": [], "entities": [{"text": "Symbol Grounding Improve Low-Level NLP", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.92940753698349}, {"text": "Word Segmentation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7452335357666016}]}], "abstractContent": [{"text": "We propose a novel framework for improving a word segmenter using information acquired from symbol grounding.", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.7295208871364594}]}, {"text": "We generate a term dictionary in three steps: generating a pseudo-stochastically segmented corpus, building a symbol grounding model to enumerate word candidates, and filtering them according to the grounding scores.", "labels": [], "entities": []}, {"text": "We applied our method to game records of Japanese chess with commentaries.", "labels": [], "entities": []}, {"text": "The experimental results show that the accuracy of a word segmenter can be improved by incorporating the generated dictionary.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.998928964138031}, {"text": "word segmenter", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.7208040952682495}]}], "introductionContent": [{"text": "Today we can easily obtain a large amount of text associated with multi-modal information, and there is a growing interest in the use of nontextual information in the natural language processing (NLP) community.", "labels": [], "entities": []}, {"text": "Many of these studies aim to output natural language sentences from a nonlinguistic modality, such as image.", "labels": [], "entities": []}, {"text": "showed that multi-modal information improves the performance of a language model.", "labels": [], "entities": []}, {"text": "Inspired by these studies, we explore a method for improving the performance of a low-level NLP task using multi-modal information.", "labels": [], "entities": []}, {"text": "In this work, we focus on the task of word segmentation (WS) in Japanese.", "labels": [], "entities": [{"text": "word segmentation (WS)", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.8584034621715546}]}, {"text": "WS is often performed as the first processing step for languages without clear word boundaries, and it is as important as part-of-speech (POS) tagging in English.", "labels": [], "entities": [{"text": "WS", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9750592112541199}, {"text": "part-of-speech (POS) tagging", "start_pos": 122, "end_pos": 150, "type": "TASK", "confidence": 0.6403926968574524}]}, {"text": "We assume that a large set of pairs of non-textual data and sentences describing them is available as the information source.", "labels": [], "entities": []}, {"text": "In our experiments, the pairs consist of game states in Shogi (Japanese chess) and textual comments on them, which were made by Shogi experts.", "labels": [], "entities": []}, {"text": "We enumerate substrings (character sequences) in the sentences and match them with Shogi states by a neural network model.", "labels": [], "entities": []}, {"text": "The rationale here is that substrings which match with non-language data well tend to be real words.", "labels": [], "entities": []}, {"text": "Our method consists of three steps (see).", "labels": [], "entities": []}, {"text": "First, we segment commentary sentences fora game state in various ways to produce word candidates.", "labels": [], "entities": []}, {"text": "Then, we match them with game states of a Shogi playing program.", "labels": [], "entities": []}, {"text": "Finally, we compile the symbol grounding results at all states and incorporate them to an automatic WS.", "labels": [], "entities": [{"text": "WS", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.8306563496589661}]}, {"text": "To the best of our knowledge, this is the first result reporting a performance improvement in an NLP task by symbol grounding.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted word segmentation experiments in the following settings.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7754524648189545}]}], "tableCaptions": [{"text": " Table 1: Corpus specifications.  #sent.  #words  #char.", "labels": [], "entities": []}, {"text": " Table 2: WS accuracy on BCCWJ.  Recall Prec. F-meas.  Baseline  98.99 99.06  99.03  + Sym.Gro. 99.03 99.01  99.02", "labels": [], "entities": [{"text": "WS", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.5946522951126099}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9390243887901306}, {"text": "BCCWJ", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.6721309423446655}, {"text": "Recall Prec. F-meas.  Baseline  98.99", "start_pos": 33, "end_pos": 70, "type": "METRIC", "confidence": 0.843032888003758}]}, {"text": " Table 3: WS accuracy on Shogi commentaries.  Recall Prec. F-meas.  Baseline  90.12 91.43  90.77  + Sym.Gro. 90.60 91.66  91.13", "labels": [], "entities": [{"text": "WS", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9368905425071716}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9803274273872375}, {"text": "Recall Prec. F-meas.  Baseline  90.12 91.43  90.77  + Sym.Gro. 90.60 91.66  91.13", "start_pos": 46, "end_pos": 127, "type": "DATASET", "confidence": 0.8719354788462321}]}]}