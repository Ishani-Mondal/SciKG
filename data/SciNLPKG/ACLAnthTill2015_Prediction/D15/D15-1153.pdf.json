{"title": [{"text": "Combining Discrete and Continuous Features for Deterministic Transition-based Dependency Parsing", "labels": [], "entities": [{"text": "Deterministic Transition-based Dependency Parsing", "start_pos": 47, "end_pos": 96, "type": "TASK", "confidence": 0.5420854464173317}]}], "abstractContent": [{"text": "We investigate a combination of a traditional linear sparse feature model and a multi-layer neural network model for deterministic transition-based dependency parsing, by integrating the sparse features into the neural model.", "labels": [], "entities": [{"text": "deterministic transition-based dependency parsing", "start_pos": 117, "end_pos": 166, "type": "TASK", "confidence": 0.5933148637413979}]}, {"text": "Correlations are drawn between the hybrid model and previous work on integrating word embedding features into a discrete linear model.", "labels": [], "entities": []}, {"text": "By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods.", "labels": [], "entities": [{"text": "web-domain parsing", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.622331291437149}]}], "introductionContent": [{"text": "Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions.", "labels": [], "entities": [{"text": "Transition-based parsing", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6182410418987274}]}, {"text": "They are attractive in computational efficiency, allowing linear time decoding with deterministic or beam-search ( algorithms.", "labels": [], "entities": []}, {"text": "Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.8144669830799103}]}, {"text": "Deterministic transition-based parsers works by making a sequence of greedy local decisions ().", "labels": [], "entities": []}, {"text": "They are attractive by very fast speeds.", "labels": [], "entities": []}, {"text": "Traditionally, a linear model has been used for the local action classifier.", "labels": [], "entities": []}, {"text": "Recently, use a neural network (NN) to replace linear models, and report improved accuracies.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9862887859344482}]}, {"text": "A contrast between a neural network model and a linear model is shown in   A neural network model takes continuous vector representations of words as inputs, which can be pre-trained using large amounts of unlabeled data, thus containing more information.", "labels": [], "entities": []}, {"text": "In addition, using an extra hidden layer, a neural network is capable of learning non-linear relations between automatic features, achieving feature combinations automatically.", "labels": [], "entities": []}, {"text": "Discrete manual features and continuous features complement each other.", "labels": [], "entities": []}, {"text": "A natural question that arises from the contrast is whether traditional discrete features and continuous neural features can be integrated for better accuracies.", "labels": [], "entities": []}, {"text": "We study this problem by constructing the neural network shown in (e), which incorporates the discrete input layer of the linear model) into the NN model) by conjoining it with the hidden layer.", "labels": [], "entities": []}, {"text": "This architecture is connected with previous work on incorporating word embeddings into a linear model.", "labels": [], "entities": []}, {"text": "In particular, incorporate word embeddings as real-valued features into a CRF model.", "labels": [], "entities": []}, {"text": "The architecture is shown in(c), which can be regarded as(e) without the hidden layer.", "labels": [], "entities": []}, {"text": "find that the accuracies of Turian et al can be enhanced by discretizing the embedding features before combining them with the traditional features.", "labels": [], "entities": []}, {"text": "They use simple binarization and clustering to this end, finding that the latter works better.", "labels": [], "entities": []}, {"text": "The architecture is shown in.", "labels": [], "entities": []}, {"text": "In contrast,(e) directly combines discrete and continuous features, replacing the hard-coded transformation function of with a hidden layer, which can be tuned by supervised training.", "labels": [], "entities": []}, {"text": "We correlate and compare all the five systems in empirically, using the SANCL 2012 data) and the standard Penn Treebank data.", "labels": [], "entities": [{"text": "SANCL 2012 data", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.965783417224884}, {"text": "Penn Treebank data", "start_pos": 106, "end_pos": 124, "type": "DATASET", "confidence": 0.9936488469441732}]}, {"text": "Results show that the method of this paper gives higher accuracies than the other methods.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.995829164981842}]}, {"text": "In addition, the method of gives slightly better accuracies compared to the method of for parsing task, consistent with Guo et al's observation on named entity recognition (NER).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9904087781906128}, {"text": "parsing task", "start_pos": 90, "end_pos": 102, "type": "TASK", "confidence": 0.9165364503860474}, {"text": "named entity recognition (NER)", "start_pos": 147, "end_pos": 177, "type": "TASK", "confidence": 0.7758317738771439}]}, {"text": "We make our C++ code publicly available under GPL at https://github.com/ SUTDNLP/NNTransitionParser.", "labels": [], "entities": []}], "datasetContent": [{"text": "For comparison with related work, we conduct experiments on Penn Treebank corpus also.", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 60, "end_pos": 80, "type": "DATASET", "confidence": 0.9955383539199829}]}, {"text": "We use the WSJ sections 2-21 for training, section 22 for development and section 23 for testing.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.832831084728241}]}, {"text": "WSJ constituent trees are converted to dependency trees using Penn2Malt . We use auto POS tags consistent with previous work.", "labels": [], "entities": [{"text": "WSJ constituent trees", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.8881605466206869}, {"text": "Penn2Malt", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.9851096868515015}]}, {"text": "The ZPar POS-tagger is used to assign POS tags.", "labels": [], "entities": []}, {"text": "Ten-fold jackknifing is performed on the training data to assign POS automatically.", "labels": [], "entities": [{"text": "POS", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.8507452011108398}]}, {"text": "For this set of experiments, the parser hyper-parameters are taken directly from the best settings in the Web Domain experiments.", "labels": [], "entities": []}, {"text": "The results are shown in, together with some state-of-the-art deterministic parsers.", "labels": [], "entities": []}, {"text": "Comparing the L, NN and This models, the observations are consistent with the web domain.: Main results on WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.9577699303627014}]}, {"text": "Our combined parser gives accuracies competitive to state-of-the-art deterministic parsers in the literature.", "labels": [], "entities": []}, {"text": "In particular, the method of is the same as our NN baseline.", "labels": [], "entities": []}, {"text": "Note that reports a UAS of 91.47% by this parser, which is higher than the results we obtained.", "labels": [], "entities": [{"text": "UAS", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9993667006492615}]}, {"text": "The main results include the use of different batch size during, while used a batch size of 100,000, we used a batch size of 10,000 in all experiments.", "labels": [], "entities": []}, {"text": "applies dynamic oracle to the deterministic transition-based parsing, giving a UAS of 91.30%. is similar to ZPar local, except that they use the arc-standard transitions, while ZPar-local is based on arc-eager transitions.", "labels": [], "entities": [{"text": "UAS", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9990631937980652}]}, {"text": "uses a special method to process punctuations, leading to about 1% UAS improvements over the vanilla system.", "labels": [], "entities": [{"text": "UAS", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9750094413757324}]}, {"text": "Recently, proposed a deterministic transition-based parser using LSTM, which gives a UAS of 93.1% on Stanford conversion of the Penn Treebank.", "labels": [], "entities": [{"text": "UAS", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9992632269859314}, {"text": "Penn Treebank", "start_pos": 128, "end_pos": 141, "type": "DATASET", "confidence": 0.90837562084198}]}, {"text": "Their work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9988836646080017}]}, {"text": "Their work is orthogonal to ours.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.", "labels": [], "entities": [{"text": "TA", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9982422590255737}, {"text": "POS tagging", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.6860925853252411}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9536308646202087}]}, {"text": " Table 2: Main results on SANCL. All systems are deterministic.", "labels": [], "entities": [{"text": "SANCL", "start_pos": 26, "end_pos": 31, "type": "TASK", "confidence": 0.5926532745361328}]}, {"text": " Table 3: Main results on WSJ. All systems are de- terministic.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.7896779179573059}]}]}