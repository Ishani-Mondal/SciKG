{"title": [{"text": "Learning Better Embeddings for Rare Words Using Distributional Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "There are two main types of word representations: low-dimensional embeddings and high-dimensional distributional vectors , in which each dimension corresponds to a context word.", "labels": [], "entities": []}, {"text": "In this paper, we initialize an embedding-learning model with distributional vectors.", "labels": [], "entities": []}, {"text": "Evaluation on word similarity shows that this initialization significantly increases the quality of embed-dings for rare words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Standard neural network (NN) architectures for inducing embeddings have an input layer that represents each word as a one-hot vector (e.g.,,,).", "labels": [], "entities": []}, {"text": "There is no usable information available in this input-layer representation except for the identity of the word.", "labels": [], "entities": []}, {"text": "We call this standard initialization method one-hot initialization.", "labels": [], "entities": []}, {"text": "Distributional representations (e.g.,,,,,) represent a word as a highdimensional vector in which each dimension corresponds to a context word.", "labels": [], "entities": []}, {"text": "They have been successfully used fora wide variety of tasks in natural language processing such as phrase similarity and sentiment analysis (.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.660452683766683}, {"text": "phrase similarity", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.8197838068008423}, {"text": "sentiment analysis", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.9271036684513092}]}, {"text": "In this paper, we investigate distributional initialization: the use of distributional vectors as representations of words at the input layer of NN architectures for embedding learning to improve the embeddings of rare words.", "labels": [], "entities": [{"text": "distributional initialization", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.5658251643180847}]}, {"text": "It is difficult for onehot initialization to learn good embeddings from only a few examples.", "labels": [], "entities": []}, {"text": "In contrast, distributional initialization provides an additional source of information -the global distribution of the word in the corpus -that improves embeddings learned for rare words.", "labels": [], "entities": [{"text": "distributional initialization", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.6600843071937561}]}, {"text": "We will demonstrate this type of improvement in the experiments reported below.", "labels": [], "entities": []}, {"text": "In summary, we introduce the idea of distributional initialization for embedding learning, an alternative to one-hot initialization that combines distributed representations (or embeddings) with distributional representations (or highdimensional vectors).", "labels": [], "entities": []}, {"text": "We show that distributional initialization significantly improves the quality of embeddings learned for rare words.", "labels": [], "entities": []}, {"text": "We will first describe our methods in Section 2 and the experimental setup in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 presents and discusses experimental results.", "labels": [], "entities": []}, {"text": "We summarize related work in Section 5 and finish with conclusion in Section 6 and discussion of future work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use ukWaC+WaCkypedia (), a corpus of 2.4 billion tokens and 6 million word types.", "labels": [], "entities": [{"text": "ukWaC+WaCkypedia", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.7730375727017721}]}, {"text": "Based on (, we preprocess the corpus by removing sentences that are less than 90% lowercase; lowercasing; replacing URLs, email addresses and digits by special tokens; tokenization); replacing words with frequency 1 with <unk>; and adding end-of-sentence tokens.", "labels": [], "entities": []}, {"text": "After preprocessing, the size n of the context word vocabulary is 2.7 million.", "labels": [], "entities": []}, {"text": "We evaluate on six word similarity judgment data sets (number of pairs in parentheses): RG Our goal in this paper is to investigate the effect of using distributional initialization vs. onehot initialization on the quality of embeddings of rare words.", "labels": [], "entities": [{"text": "RG", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9372776746749878}]}, {"text": "However, except for RW, the six data sets contain only a single word with frequency \u2264100, all other words are more frequent.", "labels": [], "entities": []}, {"text": "To address this issue, we artificially make all words in the six data sets rare.", "labels": [], "entities": []}, {"text": "We do this by keeping only \u03b8 randomly chosen occurrences in the corpus (for words with frequency >\u03b8) and replacing all other occurrences with a different token (e.g., \"fire\" is replaced with \"*fire*\").", "labels": [], "entities": []}, {"text": "This procedure -corpus downsampling -ensures that all words in the six data sets are rare in the corpus and that our setup directly evaluates the impact of distributional initialization on rare words.", "labels": [], "entities": []}, {"text": "Note that we use \u03b8 for two different purposes: (i) \u03b8 is the frequency threshold that determines which words are classified as rare and which as frequent in -changing \u03b8 corresponds to moving the horizontal dashed line in separate and mixed initialization up and down; (ii) \u03b8 is the parameter that determines how many occurrences of a word are left in the corpus when we remove oc-: Spearman correlation coefficients \u00d7100 between human and embedding-based similarity judgments, averaged over 5 runs.", "labels": [], "entities": []}, {"text": "Distributional initialization correlations that are higher (resp.", "labels": [], "entities": []}, {"text": "significantly higher) than corresponding one-hot correlations are set in bold (resp. marked *).", "labels": [], "entities": []}, {"text": "currences to ensure that words from the evaluation data sets are rare in the corpus.", "labels": [], "entities": []}, {"text": "We covary these two parameters in the experiments below; e.g., we apply distributional initialization with \u03b8 = 20 to a corpus constructed to have \u03b8 = 20 occurrences of words from similarity data sets.", "labels": [], "entities": []}, {"text": "We do this to ensure that all evaluation words are rare words for the purpose of distributional initialization and so we can exploit all pairs in the evaluation data sets for evaluating the efficacy of our method for rare words.", "labels": [], "entities": [{"text": "distributional initialization", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.7154032289981842}]}, {"text": "We modified word2vec) to accommodate distributional initialization; to support distributional vectors at the input layer, we changed the implementation of activation functions and backpropagation.", "labels": [], "entities": []}, {"text": "We use the skipgram model, hierarchical softmax, set the size of the context window to 10 (10 words to the left and 10 to the right), min-count to 1 (train on all tokens), embedding size to 100, sampling rate to 10 \u22123 and train models for one epoch.", "labels": [], "entities": []}, {"text": "For four values of the frequency threshold, \u03b8 \u2208 {10, 20, 50, 100}, we train word2vec models code.google.com/p/word2vec 6 A reviewer asks whether the value of \u03b8 should depend on the size of the training corpus.", "labels": [], "entities": []}, {"text": "Our intuition is that it is independent of corpus size.", "labels": [], "entities": []}, {"text": "If a certain amount of information -corresponding to a certain number of contexts -is required to learn a meaningful representation of a word, then it should not matter whether that given number of contexts occurs in a small corpus or in a large corpus.", "labels": [], "entities": []}, {"text": "However, if the contexts themselves contain many rare words (which is more likely in a small corpus), then corpus size could bean important variwith one-hot initialization and with the four combinations of weighting (BINARY, PPMI) and distributional initialization (mixed, separate), a total of 4 \u00d7 (1 + 2 \u00d7 2) = 20 models.", "labels": [], "entities": [{"text": "BINARY", "start_pos": 217, "end_pos": 223, "type": "METRIC", "confidence": 0.9878113865852356}]}, {"text": "For each training run, we perform corpus downsampling and initialize the parameters of the models randomly.", "labels": [], "entities": []}, {"text": "To get a reliable assessment of performance, we train 5 instances of each model and report averages of the 5 runs.", "labels": [], "entities": []}, {"text": "One model takes \u223c3 hours to train on 23 CPU cores, 2.30GHz.", "labels": [], "entities": []}, {"text": "shows experimental results, averaged over 5 runs.", "labels": [], "entities": []}, {"text": "The evaluation measure is Spearman correlation \u00d7100 between human and machinegenerated pair similarity judgments.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 26, "end_pos": 46, "type": "METRIC", "confidence": 0.9015759229660034}]}, {"text": "The main result is that for \u03b8 \u2208 {10, 20} distributional initialization is better than one-hot initialization (see bold numbers): compare lines 1&5 with line 9; and lines 2&6 with line 10.", "labels": [], "entities": []}, {"text": "This is true for both mixed and separate initialization, with the exception of WS, for which mixed (column G) is better in only 1 (line 5) of 4 cases.", "labels": [], "entities": []}, {"text": "Looking only at results for \u03b8 \u2208 {10, 20}, 18 of 24 improvements are significant 7 for mixed initialization and 16 of 24 improvements are significant for separate initialization (lines 1&5 vs 9 and lines able to take into account.2&6 vs 10).", "labels": [], "entities": []}, {"text": "For \u03b8 \u2208 {50, 100}, mixed initialization does well for RG, MC and SL, but the gap between mixed and one-hot initializations is generally smaller for these larger values of \u03b8; e.g., the difference is larger than 9 for \u03b8 = 10 (A1&A5 vs A/B9, C1&C5 vs C/D9, K1&K5 vs K/L9) and less than 9 for \u03b8 = 100 (A4&A8 vs A/B12, C4&C8 vs C/D12, K4&K8 vs K/L12) for these three data sets.", "labels": [], "entities": []}, {"text": "Recall that each value of \u03b8 effectively results in a different training corpus -a training corpus in which the number of occurrences of the words in the evaluation data sets has been reduced to \u2264 \u03b8 (cf. Section 3).", "labels": [], "entities": []}, {"text": "Our results indicate that distributional initialization is beneficial for very rare words -those that occur no more than 20 times in the corpus.", "labels": [], "entities": [{"text": "distributional initialization", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.6714769899845123}]}, {"text": "Our results for medium rare words -those that occur between 50 and 100 times -are less clear: either there are no improvements or improvements are small.", "labels": [], "entities": []}, {"text": "Thus, our recommendation is to use \u03b8 = 20.", "labels": [], "entities": []}, {"text": "The time complexity of the basic version of word2vec is O(ECW D log V ) () where E is the number of epochs, C is the corpus size, Wis the context window size, Dis the number of dimensions of the embedding space, and V is the vocabulary size.", "labels": [], "entities": [{"text": "O", "start_pos": 56, "end_pos": 57, "type": "METRIC", "confidence": 0.9862011075019836}, {"text": "ECW D log V )", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.7518364429473877}]}, {"text": "Distributional initialization adds a term I, the average number of entries in the distributional vectors, so that time complexity increases to O(IECW D log V ).", "labels": [], "entities": [{"text": "Distributional initialization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8223294913768768}, {"text": "IECW D log V", "start_pos": 145, "end_pos": 157, "type": "METRIC", "confidence": 0.8222995847463608}]}, {"text": "For rare words, I is small, so that there is no big difference in efficiency between one-hot initialization and distributional initialization of word2vec.", "labels": [], "entities": []}, {"text": "However, for frequent words I would be large, so that distributional initialization may not be scalable in that case.", "labels": [], "entities": []}, {"text": "So even if our experiments had shown that distributional initialization helps for both rare and frequent words, scalability would bean argument for only using it for rare words.", "labels": [], "entities": []}, {"text": "PPMI weighting is almost always better than BINARY, with three exceptions (I8, L7, L8) where the difference between the two is small and not significant.", "labels": [], "entities": [{"text": "BINARY", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9793327450752258}]}, {"text": "The probable explanation is that the PPMI weights in [0, 1] convey detailed, graded information about the strength of association between two words, taking into account their base frequencies.", "labels": [], "entities": []}, {"text": "In contrast, the BINARY weights in {0, 1} only indicate if there was any instance of cooccurrence at all -without considering frequency of cooccurrence and without normalizing for base frequencies.", "labels": [], "entities": [{"text": "BINARY", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9913833141326904}]}, {"text": "Mixed initialization is less variable and more predictable than separate initialization: performance for mixed initialization always goes up as \u03b8 increases, e.g., 56.54 \u2192 59.08 \u2192 63.20 \u2192 68.33 (column A, lines 1-4).", "labels": [], "entities": []}, {"text": "In contrast, separate initialization performance often decreases, e.g., from 47.06 to 45.31 (column B, lines 1-2) when \u03b8 is increased.", "labels": [], "entities": []}, {"text": "Since more information (more occurrences of the words that similarity judgments are computed for) should generally not have a negative effect on performance, the only explanation is that separate is more variable than mixed and that this variability sometimes results in decreased performance.", "labels": [], "entities": []}, {"text": "explains this difference between the two initializations: in mixed initialization (right panel), rare words are tied to frequent words, so their representations are smoothed by representations learned for frequent words.", "labels": [], "entities": []}, {"text": "In separate initialization (left panel), no such links to frequent words exist, resulting in higher variability.", "labels": [], "entities": []}, {"text": "Because of its lower variability, our experiments suggest that mixed initialiation is a better choice than separate initialization.", "labels": [], "entities": []}, {"text": "Our experiments show that distributional representation is helpful for rare words.", "labels": [], "entities": [{"text": "distributional representation", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.7855200171470642}]}, {"text": "It is difficult for one-hot initialization to learn good embeddings for such words, based on only a small number of contexts in the corpus.", "labels": [], "entities": []}, {"text": "In such cases, distributional initialization makes the learning task easier since in addition to the contexts of the rare word, the learner now also has access to the global distribution of the rare word and can take advantage of weight sharing with other words that have similar distributional representations to smooth embeddings systematically.", "labels": [], "entities": [{"text": "distributional initialization", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.617406889796257}]}, {"text": "Thus, distributional initialization is a form of smoothing: the embedding of a rare word is tied to the embeddings of other words via the links shown in: the 1s in the lower \"rare words\" part of the illustrations for separate and mixed initialization.", "labels": [], "entities": []}, {"text": "As is true for smoothing in general, parameter estimates for frequent events benefit less from smoothing or can even deteriorate.", "labels": [], "entities": []}, {"text": "In contrast, smoothing is essential for rare events.", "labels": [], "entities": []}, {"text": "Where the boundary lies between rare and frequent events depends on the specifics of the problem and the smoothing method used and is usually an empirical question.", "labels": [], "entities": []}, {"text": "Our results indicate that that boundary lies somewhere between 20 and 50 in our setting.", "labels": [], "entities": []}, {"text": "shows averages of five runs.", "labels": [], "entities": []}, {"text": "The variance of results was quite high for low-performing models.", "labels": [], "entities": [{"text": "variance", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9949742555618286}]}, {"text": "For higher performing models -those with values \u2265 40 -the ratio of standard deviation divided by mean ranged from .005 to .29.", "labels": [], "entities": [{"text": "standard", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9745755195617676}]}, {"text": "The median was .044.", "labels": [], "entities": []}, {"text": "While the variance from run to run is quite high for lowperforming models and fora few high-performing models, the significance test takes this into account, so that the relatively high variability does not undermine our results.", "labels": [], "entities": []}, {"text": "In summary, we have shown that distributional initialization improves the quality of word embeddings for rare words.", "labels": [], "entities": [{"text": "distributional initialization", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.6659431159496307}]}, {"text": "Our recommendation is to use mixed initialization with PPMI weighting and the value \u03b8 = 20 of the frequency threshold.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman correlation coefficients \u00d7100 between human and embedding-based similarity judgments, averaged over 5  runs. Distributional initialization correlations that are higher (resp. significantly higher) than corresponding one-hot correlations  are set in bold (resp. marked *).", "labels": [], "entities": []}]}