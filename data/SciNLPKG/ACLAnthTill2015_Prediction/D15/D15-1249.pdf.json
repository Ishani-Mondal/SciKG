{"title": [{"text": "Variable-Length Word Encodings for Neural Translation Models", "labels": [], "entities": [{"text": "Variable-Length Word Encodings", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6231738924980164}, {"text": "Neural Translation", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7615286707878113}]}], "abstractContent": [{"text": "Recent work in neural machine translation has shown promising performance, but the most effective architectures do not scale naturally to large vocabulary sizes.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.678563247124354}]}, {"text": "We propose and compare three variable-length encoding schemes that represent a large vocabulary corpus using a much smaller vocabulary with no loss in information.", "labels": [], "entities": []}, {"text": "Common words are unaffected by our encoding , but rare words are encoded using a sequence of two pseudo-words.", "labels": [], "entities": []}, {"text": "Our method is simple and effective: it requires no complete dictionaries, learning procedures , increased training time, changes to the model, or new parameters.", "labels": [], "entities": []}, {"text": "Compared to a baseline that replaces all rare words with an unknown word symbol, our best variable-length encoding strategy improves WMT English-French translation performance by up to 1.7 BLEU.", "labels": [], "entities": [{"text": "WMT English-French translation", "start_pos": 133, "end_pos": 163, "type": "TASK", "confidence": 0.8443969885508219}, {"text": "BLEU", "start_pos": 189, "end_pos": 193, "type": "METRIC", "confidence": 0.9984360337257385}]}], "introductionContent": [{"text": "propose a neural translation model that learns vector representations for individual words as well as word sequences.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8015535473823547}]}, {"text": "Their approach jointly predicts a translation and a latent word-level alignment fora sequence of source words.", "labels": [], "entities": []}, {"text": "However, the architecture of the network does not scale naturally to large vocabularies (.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach to circumvent the large-vocabulary challenge by preprocessing the source and target word sequences, encoding them as a longer token sequence drawn from a small vocabulary that does not discard any information.", "labels": [], "entities": []}, {"text": "Common words are unaffected, but rare words are encoded as a sequence of two pseudo-words.", "labels": [], "entities": []}, {"text": "The exact same learning and inference machinery applied to these transformed data yields improved translations.", "labels": [], "entities": []}, {"text": "We evaluate a family of 3 different encoding schemes based on Huffman codes.", "labels": [], "entities": []}, {"text": "All of them eliminate the need to replace rare words with the unknown word symbol.", "labels": [], "entities": []}, {"text": "Our approach is simpler than other methods recently proposed to address the same issue.", "labels": [], "entities": []}, {"text": "It does not introduce new parameters into the model, change the model structure, affect inference, require access to a complete dictionary, or require any additional learning procedures.", "labels": [], "entities": []}, {"text": "Nonetheless, compared to a baseline system that replaces all rare words with an unknown word symbol, our encoding approach improves EnglishFrench news translation by up to 1.7 BLEU.", "labels": [], "entities": [{"text": "EnglishFrench news translation", "start_pos": 132, "end_pos": 162, "type": "TASK", "confidence": 0.8356658220291138}, {"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9985126852989197}]}], "datasetContent": [{"text": "We trained a public implementation 1 of the system described in on the English-French parallel corpus from ACL WMT 2014, which contains 348M tokens.", "labels": [], "entities": [{"text": "English-French parallel corpus from ACL WMT 2014", "start_pos": 71, "end_pos": 119, "type": "DATASET", "confidence": 0.8345783352851868}]}, {"text": "We evaluated on news-test-2014, also from WMT 2014, which contains 3003 sentences.", "labels": [], "entities": [{"text": "WMT 2014", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9563946723937988}]}, {"text": "All experiments used the same learning parameters and vocabulary size of 30,000.", "labels": [], "entities": []}, {"text": "We constructed each encoding by the following method.", "labels": [], "entities": []}, {"text": "First, we used the formulas derived in the previous section to calculate the optimal number of common words C for each encoding scheme, using V to be the true vocabulary size of the training corpus and W = 30, 000.", "labels": [], "entities": []}, {"text": "We then found the C most common words in the text and encoded them as themselves.", "labels": [], "entities": []}, {"text": "For the remaining rare words, we encoded them using a distinct symbol whose form matched the one prescribed for each encoding scheme.", "labels": [], "entities": []}, {"text": "The encoding was then applied separately: BLEU scores (%) on detokenized test set for each encoding scheme after training for 5 days.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9996078610420227}, {"text": "detokenized test set", "start_pos": 61, "end_pos": 81, "type": "DATASET", "confidence": 0.7074076731999716}]}, {"text": "to both the source text and the target text.", "labels": [], "entities": []}, {"text": "Our encoding schemes all increased the total number of tokens in the training corpus by approximately 4%.", "labels": [], "entities": []}, {"text": "To construct the mapping from rare words to their 2-word encodings, we binned rare words by frequency into branches.", "labels": [], "entities": []}, {"text": "Thus, rare words of similar frequency in the training corpus tended to have encodings with the same first symbol.", "labels": [], "entities": []}, {"text": "Similarly, the standard Huffman construction algorithm groups together rare words with similar frequencies within subtrees.", "labels": [], "entities": [{"text": "Huffman construction", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.6277895420789719}]}, {"text": "More intelligent heuristics for constructing trees, such as using translation statistics instead of training corpus frequency, would bean interesting area of future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores (%) on detokenized test set  for each encoding scheme after training for 5 days.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994903802871704}, {"text": "detokenized test set", "start_pos": 29, "end_pos": 49, "type": "DATASET", "confidence": 0.7023595174153646}]}, {"text": " Table 2: Test set precision (%) on common words  and rare words for each encoding strategy. 1st Sym- bol denotes the precision of the first pseudo-word  symbol in an encoded rare word.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9798640012741089}, {"text": "Sym- bol", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.8902554909388224}, {"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9984422326087952}]}]}