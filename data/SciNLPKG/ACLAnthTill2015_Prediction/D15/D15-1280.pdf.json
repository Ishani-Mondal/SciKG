{"title": [{"text": "Multi-Timescale Long Short-Term Memory Neural Network for Modelling Sentences and Documents", "labels": [], "entities": []}], "abstractContent": [{"text": "Neural network based methods have obtained great progress on a variety of natural language processing tasks.", "labels": [], "entities": []}, {"text": "However, it is still a challenge task to model long texts, such as sentences and documents.", "labels": [], "entities": []}, {"text": "In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) neu-ral network to model long texts.", "labels": [], "entities": []}, {"text": "MT-LSTM partitions the hidden states of the standard LSTM into several groups.", "labels": [], "entities": [{"text": "MT-LSTM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.650667130947113}]}, {"text": "Each group is activated at different time periods.", "labels": [], "entities": []}, {"text": "Thus, MT-LSTM can model very long documents as well as short sentences.", "labels": [], "entities": []}, {"text": "Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.8828839262326559}]}], "introductionContent": [{"text": "Distributed representations of words have been widely used in many natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents).", "labels": [], "entities": []}, {"text": "The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector.", "labels": [], "entities": []}, {"text": "A good representation of the variable-length text should fully capture the semantics of natural language.", "labels": [], "entities": []}, {"text": "Recently, the long short-term memory neural network (LSTM)) has been applied successfully in many NLP tasks, such as spoken language understanding (), sequence labeling (Chen et al., * Corresponding author 2015) and machine translation.", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 117, "end_pos": 146, "type": "TASK", "confidence": 0.6318876246611277}, {"text": "sequence labeling", "start_pos": 151, "end_pos": 168, "type": "TASK", "confidence": 0.713189959526062}, {"text": "machine translation", "start_pos": 216, "end_pos": 235, "type": "TASK", "confidence": 0.8481537401676178}]}, {"text": "LSTM is an extension of the recurrent neural network (RNN), which can capture the long-term and short-term dependencies and is very suitable to model the variable-length texts.", "labels": [], "entities": []}, {"text": "Besides, LSTM is also sensitive to word order and does not rely on the external syntactic structure as recursive neural network.", "labels": [], "entities": []}, {"text": "However, when modeling long texts, such as documents, LSTM need to keep the useful features fora quite long period of time.", "labels": [], "entities": []}, {"text": "The longterm dependencies need to be transmitted one-byone along the sequence.", "labels": [], "entities": []}, {"text": "Some important features could be lost in transmission process.", "labels": [], "entities": []}, {"text": "Besides, the error signal is also back-propagated one-byone through multiple time steps in the training phase with back-propagation through time (BPTT) algorithm.", "labels": [], "entities": []}, {"text": "The learning efficiency could also be decreased for the long texts.", "labels": [], "entities": []}, {"text": "For example, if a valuable feature occurs at the begin of along document, we need to back-propagate the error through the whole document.", "labels": [], "entities": []}, {"text": "In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) to capture the valuable information with different timescales.", "labels": [], "entities": []}, {"text": "Inspired by the works of and), we partition the hidden states of the standard LSTM into several groups.", "labels": [], "entities": []}, {"text": "Each group is activated and updated at different time periods.", "labels": [], "entities": []}, {"text": "The fast-speed groups keep the short-term memories, while the slow-speed groups keep the long-term memories.", "labels": [], "entities": []}, {"text": "We evaluate our model on four benchmark datasets of text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7686960101127625}]}, {"text": "Experimental results show that our model cannot only handle short texts, but can model long texts.", "labels": [], "entities": []}, {"text": "Our contributions can be summarized as follows.", "labels": [], "entities": []}, {"text": "\u2022 With the multiple different timescale memories, MT-LSTM easily carries the crucial information over along distance.", "labels": [], "entities": []}, {"text": "MT-LSTM can well model both short and long texts.", "labels": [], "entities": [{"text": "MT-LSTM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8088043332099915}]}, {"text": "\u2022 MT-LSTM has faster convergence speed than the standard LSTM since the error signal can be back-propagated through multiple timescales in the training phase.", "labels": [], "entities": [{"text": "MT-LSTM", "start_pos": 2, "end_pos": 9, "type": "DATASET", "confidence": 0.562420129776001}]}], "datasetContent": [{"text": "In this section, we investigate the empirical performances of our proposed MT-LSTM model on four benchmark datasets for sentence and document classification and then compare it to other competitor models.", "labels": [], "entities": [{"text": "MT-LSTM", "start_pos": 75, "end_pos": 82, "type": "TASK", "confidence": 0.8901472687721252}, {"text": "sentence and document classification", "start_pos": 120, "end_pos": 156, "type": "TASK", "confidence": 0.6025280356407166}]}, {"text": "We evaluate our model on four different datasets.", "labels": [], "entities": []}, {"text": "The first three datasets are sentence-level, and the last dataset is document-level.", "labels": [], "entities": []}, {"text": "The detailed statistics about the four datasets are listed in.", "labels": [], "entities": []}, {"text": "Each dataset is briefly described as follows.", "labels": [], "entities": []}, {"text": "\u2022 SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank).", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 119, "end_pos": 146, "type": "DATASET", "confidence": 0.9159814516703287}]}, {"text": "\u2022 IMDB The IMDB dataset consists of 100,000 movie reviews with binary classes).", "labels": [], "entities": [{"text": "IMDB dataset", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.8621396720409393}]}, {"text": "One key aspect of this dataset is that each movie review has several sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the four datasets used in this paper.", "labels": [], "entities": []}, {"text": " Table 2: Hyper-parameter settings for the LSTM  and MT-LSTM.", "labels": [], "entities": [{"text": "MT-LSTM", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.8334749937057495}]}, {"text": " Table 3: Results of our MT-LSTM model against state-of-the-art neural models. All the results without  marks are reported in the corresponding paper.", "labels": [], "entities": [{"text": "MT-LSTM", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.8965346217155457}]}]}