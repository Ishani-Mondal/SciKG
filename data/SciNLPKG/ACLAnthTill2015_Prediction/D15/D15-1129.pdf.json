{"title": [{"text": "Rule Selection with Soft Syntactic Features for String-to-Tree Statistical Machine Translation", "labels": [], "entities": [{"text": "Rule Selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.783877819776535}, {"text": "Statistical Machine Translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.6545980970064799}]}], "abstractContent": [{"text": "In syntax-based machine translation, rule selection is the task of choosing the correct target side of a translation rule among rules with the same source side.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.6884007602930069}, {"text": "rule selection", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7161744982004166}]}, {"text": "We define a discriminative rule selection model for systems that have syntactic annotation on the target language side (string-to-tree).", "labels": [], "entities": []}, {"text": "This is anew and clean way to integrate soft source syntactic constraints into string-to-tree systems as features of the rule selection model.", "labels": [], "entities": []}, {"text": "We release our implementation as part of Moses.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntax-based machine translation is well known for its ability to handle non-local reordering.", "labels": [], "entities": [{"text": "Syntax-based machine translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6955230434735616}]}, {"text": "Syntax-based models either use linguistic annotation on the source language side;), target language side () or are syntactic in a structural sense only).", "labels": [], "entities": []}, {"text": "Recent shared tasks have shown that systems integrating information on the target language side, also called string-to-tree systems, achieve the best performance on several language pairs).", "labels": [], "entities": []}, {"text": "At the same time, soft syntactic features significantly improve the translation quality of hierarchical systems (Hiero) as shown in.", "labels": [], "entities": []}, {"text": "Improving the performance of stringto-tree systems through the integration of soft syntactic constraints on the source language side is therefore an interesting task.", "labels": [], "entities": []}, {"text": "So far, all approaches on this topic include soft syntactic constraints into the rules of string-to-tree () or stringto-dependency () systems and define heuristics to determine to what extent these constituents match the syntactic structure of the source sentence.", "labels": [], "entities": []}, {"text": "We propose a novel way to integrate soft syntactic constraints into a string-totree system.", "labels": [], "entities": []}, {"text": "We define a discriminative rule selection model for string-to-tree machine translation.", "labels": [], "entities": [{"text": "string-to-tree machine translation", "start_pos": 52, "end_pos": 86, "type": "TASK", "confidence": 0.6611413260300955}]}, {"text": "We consider rule selection as a multi-class classification problem where the task is to select the correct target side of a rule given its source side as well as contextual information about the source sentence and the considered rule.", "labels": [], "entities": [{"text": "rule selection", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7837373316287994}, {"text": "multi-class classification", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.7481778562068939}]}, {"text": "So far, such models have been applied to systems without syntactic annotation on the target language side.", "labels": [], "entities": []}, {"text": ", and apply such rule selection models to hierarchical machine translation,  to tree-to-string systems and to systems based on predicate argument structures.", "labels": [], "entities": [{"text": "hierarchical machine translation", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.6871662139892578}]}, {"text": "When target side syntactic annotations are taken into account, the task of rule selection has to be reformulated (see Section 2) while the same type of model can be used in approaches without target annotations.", "labels": [], "entities": [{"text": "rule selection", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.7714460492134094}]}, {"text": "This work is the first attempt to define a rule selection model fora string-to-tree system.", "labels": [], "entities": []}, {"text": "We make our implementation publicly available as part of Moses.", "labels": [], "entities": []}, {"text": "We show in Section 2 that string-to-tree rule selection is different from the hierarchical case addressed by previous work and define our rule selection model.", "labels": [], "entities": [{"text": "rule selection", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.6663325130939484}]}, {"text": "In Section 3 we present the training procedure before providing a proof-of-concept evaluation in Section 4. 2 Rule selection for string-to-tree SMT 2.1 String-to-tree machine translation We present string-to-tree machine translation as implemented in Moses (which is the framework that we use).", "labels": [], "entities": [{"text": "SMT 2.1 String-to-tree machine translation", "start_pos": 144, "end_pos": 186, "type": "TASK", "confidence": 0.8093697547912597}, {"text": "string-to-tree machine translation", "start_pos": 198, "end_pos": 232, "type": "TASK", "confidence": 0.7415870825449625}]}, {"text": "String-to-tree rules have the form X/A \u2192 \u03b1, \u03b3, \u223c\u223c.", "labels": [], "entities": []}, {"text": "On the source language side, all non-terminals have the unique label X while on the target language side non-terminals are annotated with syntactic labels n t \u2208 N t . The lefthand side X/A consists of source and target nonterminals.", "labels": [], "entities": []}, {"text": "In the right hand side (rhs), \u03b1 is a string of source terminal symbols and the non-terminal X.", "labels": [], "entities": []}, {"text": "The string \u03b3 consists of target terminals and non-terminals n t \u2208 N t . The alignment \u223c is a oneto-one correspondence between source and target non-terminal symbols.", "labels": [], "entities": []}, {"text": "String-to-tree rules are extracted from pairs of strings and trees as exemplified in.", "labels": [], "entities": []}, {"text": "Rules r 1 and r 2 are example rules extracted from this data.", "labels": [], "entities": []}, {"text": "During decoding, CYK+ chart parsing (Chappelier et al., 1998) with cube pruning and language model scoring is performed on an input sentence such as F below.", "labels": [], "entities": [{"text": "CYK+ chart parsing", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.51603864133358}]}, {"text": "Each time a rule is applied to the input sentence, candidate target trees are built.", "labels": [], "entities": []}, {"text": "shows the partial translations built after the segments Diverses and importantes have been decoded.", "labels": [], "entities": []}, {"text": "Given these partial translations, ruler 1 can be applied in a further decoding step.", "labels": [], "entities": []}, {"text": "F (Diverses)X1 caract\u00e9ristiques (importantes)X2 n'ont pas\u00e9t\u00e9pas\u00b4pas\u00e9t\u00e9 prises en compte.", "labels": [], "entities": []}, {"text": "(Various)X 1 characteristics (important)X 2 were not considered.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our baseline system is a syntax-based system with linguistic annotation on the target language side (string-to-tree).", "labels": [], "entities": []}, {"text": "We use the version implemented in the Moses open source toolkit () with standard parameters.", "labels": [], "entities": [{"text": "Moses open source toolkit", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.8754009008407593}]}, {"text": "Rule extraction is performed as in () with rule composition (.", "labels": [], "entities": [{"text": "Rule extraction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8568397462368011}]}, {"text": "Non-lexical unary rules are removed (Chung et al., 2011) and scope-3 pruning () is performed.", "labels": [], "entities": []}, {"text": "Rule scoring is done using relative frequencies normalized over the source rhs and aligned non-terminals in the target rhs.", "labels": [], "entities": [{"text": "Rule scoring", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9152659773826599}]}, {"text": "The contrastive system is the same string-to-tree system but augmented with our rule selection model as a feature of the log-linear model.: String-to-tree system evaluation results.", "labels": [], "entities": []}, {"text": "We evaluate the baseline and our global model on three domains: (1) news, (2) medical, and (3) science.", "labels": [], "entities": []}, {"text": "The training data for news is taken from Europarl-v4.", "labels": [], "entities": [{"text": "Europarl-v4", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9937216639518738}]}, {"text": "Development and test sets are from the news translation task of WMT 2009).", "labels": [], "entities": [{"text": "news translation task", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7528848250706991}, {"text": "WMT 2009", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.871774435043335}]}, {"text": "For medical we use the biomedical data from EMEA (.", "labels": [], "entities": [{"text": "EMEA", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8654972314834595}]}, {"text": "Since this is a parallel corpus only, we first removed duplicate sentences and then constructed development and test sets by randomly selecting sentence pairs.", "labels": [], "entities": []}, {"text": "As training data for science we use the scientific abstracts data provided by. gives an overview of the corpora sizes.", "labels": [], "entities": []}, {"text": "Berkeley parser () is used to parse the English side of each parallel corpus (for string-to-tree rule extraction) as well as for parsing the French source side (for feature extraction).", "labels": [], "entities": [{"text": "string-to-tree rule extraction", "start_pos": 82, "end_pos": 112, "type": "TASK", "confidence": 0.6998172203699747}, {"text": "feature extraction", "start_pos": 165, "end_pos": 183, "type": "TASK", "confidence": 0.717629611492157}]}, {"text": "We trained a 5-gram language model on the English side of each training corpus using the SRI Language Modeling Toolkit.", "labels": [], "entities": [{"text": "SRI Language Modeling Toolkit", "start_pos": 89, "end_pos": 118, "type": "DATASET", "confidence": 0.7879743576049805}]}, {"text": "We train the model in the standard way and generate word alignments using GIZA++.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.6950212270021439}]}, {"text": "After training, we reduced the number of translation rules by only keeping the 30-best rules with the same source side according to the direct rule translation rule probability.", "labels": [], "entities": [{"text": "translation", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.9584720730781555}]}, {"text": "Our rule selection model was trained with VW.", "labels": [], "entities": [{"text": "rule selection", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9549894034862518}, {"text": "VW", "start_pos": 42, "end_pos": 44, "type": "DATASET", "confidence": 0.9335141777992249}]}, {"text": "All systems were tuned using batch MIRA.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.984901487827301}]}, {"text": "We measured the overall translation quality with 4-gram BLEU (), which was computed on tokenized and lowercased data for all systems.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9517154097557068}, {"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9967522621154785}]}, {"text": "Statistical significance is computed with the pairwise bootstrap resampling technique of Koehn (2004).", "labels": [], "entities": []}, {"text": "displays the BLEU scores for our experiments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9990788698196411}]}, {"text": "On science and news, small improvements are achieved while for medical a small decrease is observed.", "labels": [], "entities": []}, {"text": "None of these differences is statistically significant.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: String-to-tree system evaluation results.", "labels": [], "entities": []}, {"text": " Table 3: Hierarchical system evaluation results.  The results in bold are statistically significant im- provements over the Baseline (at confidence p <  0.05).", "labels": [], "entities": [{"text": "statistically significant im- provements", "start_pos": 75, "end_pos": 115, "type": "METRIC", "confidence": 0.6996693968772888}]}, {"text": " Table 4: String-to-tree system evaluation results  with concatenated training data.", "labels": [], "entities": []}]}