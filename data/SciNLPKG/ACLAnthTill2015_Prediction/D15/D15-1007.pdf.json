{"title": [{"text": "Identifying Political Sentiment between Nation States with Social Media", "labels": [], "entities": [{"text": "Identifying Political Sentiment", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9328111807505289}]}], "abstractContent": [{"text": "This paper describes an approach to large-scale modeling of sentiment analysis for the social sciences.", "labels": [], "entities": [{"text": "large-scale modeling of sentiment analysis", "start_pos": 36, "end_pos": 78, "type": "TASK", "confidence": 0.6111116766929626}]}, {"text": "The goal is to model relations between nation states through social media.", "labels": [], "entities": []}, {"text": "Many cross-disciplinary applications of NLP involve making predictions (such as predicting political elections), but this paper instead focuses on a model that is applicable to broader analysis.", "labels": [], "entities": [{"text": "predicting political elections)", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.882082387804985}]}, {"text": "Do citizens express opinions inline with their home country's formal relations?", "labels": [], "entities": []}, {"text": "When opinions diverge overtime, what is the cause and can social media serve to detect these changes?", "labels": [], "entities": []}, {"text": "We describe several learning algorithms to study how the populace of a country discusses foreign nations on Twitter, ranging from state-of-the-art contextual sentiment analysis to some required practical learners that filter irrelevant tweets.", "labels": [], "entities": [{"text": "contextual sentiment analysis", "start_pos": 147, "end_pos": 176, "type": "TASK", "confidence": 0.7089786728223165}]}, {"text": "We evaluate on standard sentiment evaluations, but we also show strong correlations with two public opinion polls and current international alliance relationships.", "labels": [], "entities": []}, {"text": "We conclude with some political science use cases.", "labels": [], "entities": []}], "introductionContent": [{"text": "The volume of text available on social media provides anew opportunity for public policy and political science.", "labels": [], "entities": []}, {"text": "Specifically in the area of international relations, advances in natural language understanding and sentiment analysis may offer new insights into the sentiment of one nation toward another.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.643417646487554}, {"text": "sentiment analysis", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.8709105551242828}]}, {"text": "This paper processes 17 months of Twitter data to identify discussions about sovereign states, and it aggregates opinions toward these states from foreign nations.", "labels": [], "entities": []}, {"text": "We present a novel application of contextual sentiment with this task, and identify several semi-supervised learning algorithms that are needed to address the reference resolution challenge inherent to country names.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 159, "end_pos": 179, "type": "TASK", "confidence": 0.7998555302619934}]}, {"text": "We present intrinsic evaluations of our learners on labeled datasets as well as four extrinsic political science evaluations that show strong alignment with our large-scale sentiment extraction.", "labels": [], "entities": [{"text": "sentiment extraction", "start_pos": 173, "end_pos": 193, "type": "TASK", "confidence": 0.7347966879606247}]}, {"text": "An open question for international policy makers is the extent to which public opinion drives decision making.", "labels": [], "entities": []}, {"text": "How do military conflicts affect a neutral nation's relationship?", "labels": [], "entities": []}, {"text": "Does public opinion shift toward a country after a formal alliance is created, or must popular opinion shift first?", "labels": [], "entities": []}, {"text": "These questions are difficult to address due to the lack of measurable data.", "labels": [], "entities": []}, {"text": "While polling data can be collected, collection beyond a handful of countries is cost prohibitive.", "labels": [], "entities": []}, {"text": "This paper hypothesizes that sentiment analysis can be used as a proxy to track international relations between nation states.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9450526833534241}]}, {"text": "We describe the largest attempt (over 2 billion tweets) to measure nation state sentiment across hundreds of country pairs.", "labels": [], "entities": [{"text": "measure nation state sentiment", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.678247332572937}]}, {"text": "The core challenge to measuring public opinion between countries is an accurate algorithm to judge the sentiment of a text toward another nation.", "labels": [], "entities": []}, {"text": "Unlike traditional sentiment analysis, the general sentiment of the text is not adequate.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.8630686402320862}]}, {"text": "Let the following serve as an example.", "labels": [], "entities": []}, {"text": "I am in full sad mode right about now.", "labels": [], "entities": []}, {"text": "@RachOrange This tweet is a positive example from the USA toward Pakistan.", "labels": [], "entities": []}, {"text": "However, atypical sentiment classifier misclassifies this as negative because miss and sad express sadness.", "labels": [], "entities": []}, {"text": "A contextual sentiment classification is needed to identify that the predicate miss is positive toward its argument.", "labels": [], "entities": [{"text": "contextual sentiment classification", "start_pos": 2, "end_pos": 37, "type": "TASK", "confidence": 0.739699105421702}]}, {"text": "Several recent competitions included contextual classification tasks, and this paper builds on the best of those algorithms fora unique nation-nation sentiment classifier.", "labels": [], "entities": [{"text": "contextual classification", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.7806859314441681}, {"text": "nation-nation sentiment classifier", "start_pos": 136, "end_pos": 170, "type": "TASK", "confidence": 0.6284076968828837}]}, {"text": "We describe a multi-classifier model that aggregates tweets into counts of positive and negative sentiment from one country toward another.", "labels": [], "entities": []}, {"text": "Several unique filters are required to resolve textual references toward country names.", "labels": [], "entities": []}, {"text": "We first present standard NLP sentiment experiments that show the classifiers achieve good performance on individual tweets.", "labels": [], "entities": [{"text": "NLP sentiment", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.7720186710357666}]}, {"text": "To evaluate the complete nation-nation system, we present four novel evaluations, including two public opinion polls.", "labels": [], "entities": []}, {"text": "Correlation with the polls is high at \u03c1 = .8, and our nation-nation sentiment is 84% accurate with NATO and EU relations.", "labels": [], "entities": [{"text": "\u03c1", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.9928268790245056}]}, {"text": "We then discuss the implications for both NLP as a technical science and political science as asocial science.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main dataset for this study is 17 months of tweets obtained through the keyword Twitter API that mention one of 187 unique countries.", "labels": [], "entities": []}, {"text": "The dataset spans from Sep. 3, 2013 to Jan 10, 2015 with 3-5 million tweets per day.", "labels": [], "entities": []}, {"text": "Each tweet includes the profile location and geolocation data (if available) of the user who posted the tweet.", "labels": [], "entities": []}, {"text": "Collection was not limited to a specific location in order to retrieve samples from across the world.", "labels": [], "entities": []}, {"text": "This dataset is used in all political science experiments (Sections 6.2 and 6.3).", "labels": [], "entities": []}, {"text": "A smaller labeled dataset is used for supervised classification.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.7368011176586151}]}, {"text": "We randomly sampled the data to create a dataset of 4250 tweets.", "labels": [], "entities": []}, {"text": "The authors initially labeled each tweet with one of four sentiment labels: positive, negative, objective, or irrelevant.", "labels": [], "entities": []}, {"text": "Text was only labeled as positive if it is positive toward the nation's mention.", "labels": [], "entities": []}, {"text": "Text that contains a nation's mention, but does not contain sentiment toward the mention is labeled objective.", "labels": [], "entities": []}, {"text": "Text with a mention that is not referent to a physical country is labeled irrelevant despite presence of sentiment.", "labels": [], "entities": []}, {"text": "This irrelevant distinction is a departure from sentiment competitions.", "labels": [], "entities": []}, {"text": "A second labeling added a fifth label to the irrelevant tweets to split off dining topics.", "labels": [], "entities": []}, {"text": "Usernames (e.g., @user) and URLs are replaced with placeholder tokens.", "labels": [], "entities": []}, {"text": "Multiple whitespace characters are condensed and the text is split on it.", "labels": [], "entities": []}, {"text": "Punctuation attached to tokens is removed (but saved) and used in later punctuation features.", "labels": [], "entities": []}, {"text": "Punctuation is not treated as separate tokens in the n-gram features.", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9177405834197998}]}, {"text": "We prepend occurrences of \"not\" to their subsequent tokens, merging the two into anew token (e.g., \"not happy\" becomes \"nothappy\").", "labels": [], "entities": []}, {"text": "Once the raw text of the tweet is tokenized as above, non-English tweets are filtered out.", "labels": [], "entities": []}, {"text": "English filtering is performed by LingPipe 1 . We manually evaluated this filter and found it 86.2% accurate over 400 tweets.", "labels": [], "entities": [{"text": "English filtering", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6625188589096069}]}, {"text": "Accuracy is lost due to slang and the short nature of the text.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9886309504508972}]}, {"text": "Our goal is to first prove the accuracy of our sentiment classifiers, then show the broader pipeline's correlation with known nation-nation politics.We thus conducted three types of experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9984447360038757}, {"text": "sentiment classifiers", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.6488147526979446}]}, {"text": "The first is an intrinsic evaluation of the classifiers with common frameworks from the NLP community.", "labels": [], "entities": []}, {"text": "The second is an extrinsic evaluation from multiple political science datasets.", "labels": [], "entities": []}, {"text": "The third is a set of use case proposals for application of this analysis.: Classifier performance.", "labels": [], "entities": []}, {"text": "Precision/Recall is calculated for each label separately.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9913627505302429}, {"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.5176333785057068}]}, {"text": "Accuracy is overall labels: # correct/total.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.995782732963562}]}, {"text": "The dining, irrelevant, and sentiment classifiers are supervised systems trained on a labeled dataset of 4,250 tweets.", "labels": [], "entities": [{"text": "sentiment classifiers", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.7964905798435211}]}, {"text": "We split the dataset into training, dev, and test sets.", "labels": [], "entities": []}, {"text": "The dev set contains 200 tweets, the test set has 750 tweets, and the training set size varied based on the available labels.", "labels": [], "entities": []}, {"text": "The features in this paper were developed solely on the training and dev datasets.", "labels": [], "entities": []}, {"text": "Reported results are on the unseen test set of 750 tweets.", "labels": [], "entities": [{"text": "unseen test set of 750 tweets", "start_pos": 28, "end_pos": 57, "type": "DATASET", "confidence": 0.8082251946131388}]}, {"text": "The bootstrapped classifiers for sports and concerts were learned without labeled data, so we ran the sports and concerts classifiers on an unseen portion of our data, and manually evaluated the first 200 tweets that were labeled by each classifier.", "labels": [], "entities": []}, {"text": "Precision and recall are calculated individually for each class label: P = #correct/#guessed and R = #correct/#gold.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9957256317138672}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9991089701652527}]}, {"text": "Where #guessed is how many times the classifier predicted the target label, and #gold is how many times the target label appears in the dataset.", "labels": [], "entities": []}, {"text": "Accuracy is also shown, calculated as a single score overall labels together: Accuracy = #correct/N . The first table in shows the dining classifier's performance.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9974919557571411}, {"text": "Accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9994244575500488}]}, {"text": "The majority class baseline is high at 93% because only 7% of the data is about dining.", "labels": [], "entities": []}, {"text": "The classifier achieves a 29% decrease inaccuracy error (2% absolute increase).", "labels": [], "entities": [{"text": "inaccuracy error", "start_pos": 39, "end_pos": 55, "type": "METRIC", "confidence": 0.8449917733669281}, {"text": "absolute increase", "start_pos": 60, "end_pos": 77, "type": "METRIC", "confidence": 0.9289056658744812}]}, {"text": "The second table shows the more general irrelevant classifier.", "labels": [], "entities": []}, {"text": "The majority class baseline is much lower than dining at 58.7%.", "labels": [], "entities": []}, {"text": "Many tweets that contain a country name are not relevant nor references to the geolocated country itself.", "labels": [], "entities": []}, {"text": "Our trained classifier does well on this task achieving 84% accuracy, a 26% absolute increase over baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9942001104354858}]}, {"text": "It is 84% precise with 90% recall on detecting irrelevant tweets.", "labels": [], "entities": [{"text": "precise", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9933562278747559}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9996896982192993}]}, {"text": "The third table in shows sentiment classifier results.", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.9202125370502472}]}, {"text": "Accuracy is almost 10% absolute above the majority class.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9957555532455444}]}, {"text": "Finally, the bootstrapped classifiers perform at 98% accuracy for sports and 90% for concerts.: Positive/Negative ratios for the US toward its top 10 frequently mentioned nations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9987280964851379}, {"text": "Negative", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.8333380818367004}]}, {"text": "Nation opinions are represented as directed edges: each edge (X,Y) represents the opinion of nation X toward nation Y.", "labels": [], "entities": []}, {"text": "The weight of an edge is the ratio of positive to negative counts: where C(X,Y,L) is the number of tweets by nation X users about nation Y with sentiment L.", "labels": [], "entities": []}, {"text": "Only tweets that make it through the Nation to Nation Pipeline of Section 5 receive sentiment labels.", "labels": [], "entities": [{"text": "Nation to Nation Pipeline of Section 5", "start_pos": 37, "end_pos": 75, "type": "DATASET", "confidence": 0.9173597012247358}]}, {"text": "If a nation pair (X,Y) was observed less than 1000 times, it is not included in the evaluations.", "labels": [], "entities": []}, {"text": "We provide experiments later to evaluate this cutoff's affect.", "labels": [], "entities": []}, {"text": "The dataset (Section 3) spans 17 months from 2013-2015.", "labels": [], "entities": []}, {"text": "All tweets are classified or filtered out, and R(X,Y) is computed for all pairs.", "labels": [], "entities": [{"text": "R(X,Y)", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.89908067882061}]}, {"text": "shows the top 10 nation pair ratios (with over 500k tweets between them) for the U.S. We present four formal evaluations to answer the central question of this paper: can sentiment from social media be used to help approximate international opinions?", "labels": [], "entities": []}, {"text": "The first two experiments use public opinion polls of national sentiment toward other nations.", "labels": [], "entities": []}, {"text": "The third uses military conflicts as a proxy for negative relations, and the fourth uses current formal alliances as a proxy for positive relations.", "labels": [], "entities": []}, {"text": "None of these can provide a complete picture of the connection between popular sentiment and international relations, but the four together provide a strong case that sentiment contains a useful signal.: Polling Data: ranking of a nation's \"positive contribution\" to the world, compared to automatically identified nation-nation sentiment.", "labels": [], "entities": []}, {"text": "Each year, GlobeScan/PIPA releases polling data of 16 nations in a ranked ordering based on how 26,000 people view their \"positive contribution\" to the world 3 . This poll helps to determine whether or not this paper's sentiment pipeline matches human polling.", "labels": [], "entities": [{"text": "GlobeScan/PIPA releases polling data", "start_pos": 11, "end_pos": 47, "type": "DATASET", "confidence": 0.8896619280179342}]}, {"text": "We created our own ranking by assigning a world score to each nation n: the average sentiment ratio of all other nations toward n.", "labels": [], "entities": []}, {"text": "Since the polling data also ranks the EU, we average the EU member nation world scores for an EU world score.", "labels": [], "entities": []}, {"text": "shows the PIPA poll (Human Poll) and our world ranking (Sentiment).", "labels": [], "entities": [{"text": "PIPA poll (Human Poll)", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.7962771554787954}]}, {"text": "Using Spearman's rank correlation coefficient to measure agreement, our ranking is strongly correlated at \u03c1 = 0.8 (perfect is 1.0).", "labels": [], "entities": [{"text": "rank correlation coefficient", "start_pos": 17, "end_pos": 45, "type": "METRIC", "confidence": 0.7487330834070841}, {"text": "agreement", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.918796956539154}]}, {"text": "The main mistake in our ranking is Germany.", "labels": [], "entities": [{"text": "Germany", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.8799449801445007}]}, {"text": "We also compare against a Frequency Baseline to eliminate the possibility that it's simply a matter of topic popularity.", "labels": [], "entities": []}, {"text": "Poll rankings could simply be correlated with who people choose to discuss, or vice versa.", "labels": [], "entities": []}, {"text": "The frequency baseline is the average number of twitter mentions per nation (i.e., the most discussed).", "labels": [], "entities": [{"text": "frequency baseline", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.943510115146637}]}, {"text": "This baseline shows no correlation at \u03c1 = \u2212.06.", "labels": [], "entities": [{"text": "\u03c1", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.9839073419570923}]}, {"text": "We then evaluated against a US-centric polling agency, Gallup.", "labels": [], "entities": [{"text": "Gallup", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.9558767676353455}]}, {"text": "They asked Americans to rate other nations as 'favorable' or 'unfavorable' in a 2014 poll . The result is a ranking of favorability.", "labels": [], "entities": []}, {"text": "In contrast to the PIPA poll which evalu-ates many nations looking in, Gallup evaluates a single nation looking out.", "labels": [], "entities": [{"text": "PIPA poll", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.8358060419559479}, {"text": "Gallup", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.9179377555847168}]}, {"text": "Space constraints prevent us from visually showing the US ranking, but again the sentiment ratios have a strong correlation at \u03c1 = .81.", "labels": [], "entities": [{"text": "US ranking", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.8642253279685974}]}, {"text": "The frequency baseline is \u03c1 = .23.", "labels": [], "entities": [{"text": "\u03c1", "start_pos": 26, "end_pos": 27, "type": "METRIC", "confidence": 0.9793867468833923}]}, {"text": "The nation-nation sentiment extraction strongly correlates with both world views (PIPA) and US-specific views (Gallup).", "labels": [], "entities": []}, {"text": "The third evaluation uses apolitical science dataset from the Correlates of War Project, the Militarized Interstate Disputes v4.01 (MID) ().", "labels": [], "entities": [{"text": "Militarized Interstate Disputes v4.01 (MID)", "start_pos": 93, "end_pos": 136, "type": "TASK", "confidence": 0.7127801861081805}]}, {"text": "This dataset is used in the field of international relations, listing conflicts since 1816.", "labels": [], "entities": [{"text": "international relations", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.8107494413852692}]}, {"text": "We limit the evaluation to conflicts after 1990 to keep relations current.", "labels": [], "entities": []}, {"text": "The dataset ends at 2001, so while not a completely current evaluation, it stands as a proxy for negative relations.", "labels": [], "entities": []}, {"text": "Each conflict in MID is labeled with a conflict severity.", "labels": [], "entities": [{"text": "MID", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9788824915885925}]}, {"text": "We convert severity labels between nations to a pair score MID(X,Y): where Disputes(X,Y) is the set of conflicts between the two nations X and Y, and score(d) is a severity score for the type of disputed.", "labels": [], "entities": [{"text": "MID", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.5996322631835938}, {"text": "score(d)", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.8699603974819183}, {"text": "severity score", "start_pos": 164, "end_pos": 178, "type": "METRIC", "confidence": 0.9774519205093384}]}, {"text": "War is -5, use of force is -4, displays of force is -3, threatening use of force is -2, and no militarized action is -1.", "labels": [], "entities": []}, {"text": "We take the sum of severity scores and save all nation pairs (X,Y) such that M ID(X, Y ) < \u221210.", "labels": [], "entities": [{"text": "severity scores", "start_pos": 19, "end_pos": 34, "type": "METRIC", "confidence": 0.9616365730762482}, {"text": "M ID", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9462708234786987}]}, {"text": "This score indicates multiple conflicts and are thus considered as nations with true negative relations.", "labels": [], "entities": []}, {"text": "We then compare our sentiment ratios R(X,Y) against these gold negative pairs.", "labels": [], "entities": []}, {"text": "Each continuous R(X,Y) is discretized into sentiment categories for ease of comparison.", "labels": [], "entities": []}, {"text": "Since the mean across all R(X,Y) is 1.25, we consider an interval around 1.25 as neutral and create positive and negative labels above and below that neutral center: The bottom table in shows the number of nation pairs that align with the negative labels of the MID dataset.", "labels": [], "entities": [{"text": "MID dataset", "start_pos": 262, "end_pos": 273, "type": "DATASET", "confidence": 0.7432649433612823}]}, {"text": "Only pairs that have at least 1000 tweets are evaluated.", "labels": [], "entities": []}, {"text": "Of the resulting 90 pairs, 61 are correctly identified by our system as negative or slight negative for an accuracy of 68%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9995612502098083}]}, {"text": "labels are between nations with a smaller Twitter presence, so performance likely suffers due to lack of data.", "labels": [], "entities": []}, {"text": "For robustness testing, we shifted the thresholds that discretize the nation ratios and MID scores into postive and negative categories.", "labels": [], "entities": [{"text": "MID", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.5954243540763855}]}, {"text": "The accuracy result shows little change.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999570906162262}]}, {"text": "We also reran the experiment with a higher cutoff of 10,000 instead of 1,000.", "labels": [], "entities": []}, {"text": "The negative disputes accuracy increases from 68% to 81%, but the recall obviously drops as less countries are included.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9991580247879028}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9993026256561279}]}, {"text": "This suggests the sentiment ratios might be used on a sliding confidence scale based on frequency of mention.", "labels": [], "entities": []}, {"text": "We now briefly discuss how these positive results for nation-nation sentiment relates to political science analysis.", "labels": [], "entities": [{"text": "political science analysis", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.730276882648468}]}, {"text": "One core area of study is how national sentiment shifts overtime, and why.", "labels": [], "entities": [{"text": "national sentiment", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7801032960414886}]}, {"text": "Computing R(X,Y) on a bi-weekly basis, graphs the sentiment ratio from the USA toward India and Israel.", "labels": [], "entities": []}, {"text": "The timeline shows significant favorability toward India during their extended election sea-  son, but afterward the opinion is similar to before the election.", "labels": [], "entities": []}, {"text": "In contrast, the 2014 Israel-Gaza conflict shows a very different effect.", "labels": [], "entities": []}, {"text": "US opinion of Israel is initially steady (slightly positive) until the conflict causes a large dip.", "labels": [], "entities": []}, {"text": "Unlike India's spike, US opinion stays depressed even after the conflict concludes.", "labels": [], "entities": []}, {"text": "It appears to have only risen to 'normal' levels months later.", "labels": [], "entities": []}, {"text": "We do note that the water is slightly muddied because our algorithm may not distinguish well between sentiment toward the war, Israel itself, or even sympathy toward casualties.", "labels": [], "entities": []}, {"text": "However, it's clear that nation-nation sentiment is captured, and future work is needed to identify finer grained sentiment as needed.", "labels": [], "entities": []}, {"text": "Another application is inter-alliance relations.", "labels": [], "entities": []}, {"text": "For instance, shows how NATO member nations view other alliances.", "labels": [], "entities": []}, {"text": "The table shows the average of all R(X,Y) edges for each nation within an alliance to a nation in the other.", "labels": [], "entities": [{"text": "R(X,Y) edges", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.8374411225318908}]}, {"text": "According to our ratios, NATO countries have stark differences between how they view themselves versus how they view African Union/Arab League nations.", "labels": [], "entities": []}, {"text": "Further, our pipeline enables analysis of outside nations looking in.", "labels": [], "entities": []}, {"text": "For instance, the nations with the most positive view of the EU are Uruguay, Lithuania (EU member), Belarus, Moldova, and Slovakia (EU member).", "labels": [], "entities": []}, {"text": "Almost all (not Uruguay) are eastern european nations.", "labels": [], "entities": []}, {"text": "Moldova is currently seeking EU membership and Belarus had closer ties until recently.", "labels": [], "entities": []}, {"text": "Our results might point to potential future alliances.", "labels": [], "entities": []}, {"text": "Future work is needed to explore this implied connection.", "labels": [], "entities": []}, {"text": "Finally, the R(X,Y) ratios can also represent a nation's opinion profile.", "labels": [], "entities": [{"text": "R(X,Y) ratios", "start_pos": 13, "end_pos": 26, "type": "METRIC", "confidence": 0.9127299308776855}]}, {"text": "Represent each nation X by a vector of its R(X,Y) ratios.", "labels": [], "entities": []}, {"text": "This represents its entire international view based on social me-   dia sentiment.", "labels": [], "entities": []}, {"text": "Space prohibits more detail, but we clustered opinion profiles with k-means (k=12) and cosine similarity.", "labels": [], "entities": []}, {"text": "Typical alliances, such as European and African clusters, are learned.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Classifier performance. Precision/Recall is calculated for each label separately. Accuracy is  over all labels: # correct/total.", "labels": [], "entities": [{"text": "Precision/Recall", "start_pos": 34, "end_pos": 50, "type": "METRIC", "confidence": 0.8295690615971884}, {"text": "Accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9990319013595581}]}, {"text": " Table 3: Positive/Negative ratios for the US to- ward its top 10 frequently mentioned nations.", "labels": [], "entities": []}, {"text": " Table 5: Top: The number of NATO/EU nation  pairs with automatic sentiment labels. Bottom:  The number of pairs with military disputes (MID  dataset) and automatic sentiment labels.", "labels": [], "entities": [{"text": "MID  dataset)", "start_pos": 137, "end_pos": 150, "type": "DATASET", "confidence": 0.7932272255420685}]}, {"text": " Table 6: Average pos/neg ratio of NATO nations  toward the nations in other formal alliances.", "labels": [], "entities": [{"text": "Average pos/neg ratio", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.775061571598053}]}, {"text": " Table 7: Filtering effects on the MID results.", "labels": [], "entities": [{"text": "Filtering", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8741527199745178}, {"text": "MID", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9240436553955078}]}]}