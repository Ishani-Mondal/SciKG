{"title": [{"text": "Chinese Semantic Role Labeling with Bidirectional Recurrent Neural Networks", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 8, "end_pos": 30, "type": "TASK", "confidence": 0.6095612049102783}]}], "abstractContent": [{"text": "Traditional approaches to Chinese Semantic Role Labeling (SRL) almost heavily rely on feature engineering.", "labels": [], "entities": [{"text": "Chinese Semantic Role Labeling (SRL)", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.6931336862700326}]}, {"text": "Even worse, the long-range dependencies in a sentence can hardly be modeled by these methods.", "labels": [], "entities": []}, {"text": "In this paper, we introduce bidirection-al recurrent neural network (RNN) with long-short-term memory (LSTM) to capture bidirectional and long-range dependencies in a sentence with minimal feature engineering.", "labels": [], "entities": []}, {"text": "Experimental results on Chinese Proposition Bank (CPB) show a significant improvement over the state-of-the-art methods.", "labels": [], "entities": [{"text": "Chinese Proposition Bank (CPB)", "start_pos": 24, "end_pos": 54, "type": "DATASET", "confidence": 0.9404893716176351}]}, {"text": "Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Role Labeling (SRL) is defined as the task to recognize arguments fora given predicate and assign semantic role labels to them.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8331366380055746}]}, {"text": "Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages ().", "labels": [], "entities": [{"text": "SRL", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9882949590682983}]}, {"text": "shows an example in Chinese Proposition Bank (CPB), which is a Chinese corpus annotated with semantic role labels.", "labels": [], "entities": [{"text": "Chinese Proposition Bank (CPB)", "start_pos": 20, "end_pos": 50, "type": "DATASET", "confidence": 0.9004819790522257}]}, {"text": "Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.6899981498718262}]}, {"text": "However, these methods suffer from three major problems.", "labels": [], "entities": []}, {"text": "Firstly, their performances are heavily dependent on feature engi- neering, which needs domain knowledge and laborious work of feature extraction and selection.", "labels": [], "entities": [{"text": "feature extraction and selection", "start_pos": 127, "end_pos": 159, "type": "TASK", "confidence": 0.7093241438269615}]}, {"text": "Secondly, although sophisticated features are designed, the long-range dependencies in a sentence can hardly be modeled.", "labels": [], "entities": []}, {"text": "Thirdly, a specific annotated dataset is often limited in its scalability, but the existence of heterogenous resource, which has very different semantic role labels and annotation schema but related latent semantic meaning, can alleviate this problem.", "labels": [], "entities": []}, {"text": "However, traditional methods cannot relate distinct annotation schemas and introduce heterogeneous resource with ease.", "labels": [], "entities": []}, {"text": "Concerning these problems, in this paper, we propose bidirectional recurrent neural network (RNN) with long-short-term memory (LSTM) to solve the problem of Chinese SRL.", "labels": [], "entities": [{"text": "Chinese SRL", "start_pos": 157, "end_pos": 168, "type": "TASK", "confidence": 0.5035911351442337}]}, {"text": "Our approach makes the following contributions: \u2022 We formulate Chinese SRL with bidirectional LSTM RNN model.", "labels": [], "entities": [{"text": "SRL", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.6742994785308838}]}, {"text": "With bidirectional RNN, the dependencies in a sentence from both directions can be captured, and with L-STM architecture, long-range dependencies can be well modeled.", "labels": [], "entities": []}, {"text": "The test results on the bechmark dataset CPB show a significant improvement over the state-of-the-art methods.", "labels": [], "entities": [{"text": "bechmark dataset CPB", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.8422778050104777}]}, {"text": "\u2022 Compared with previous work that relied on a huge number of handcrafted features, our model can achieve much better performance only with minimal feature engineering.", "labels": [], "entities": []}, {"text": "\u2022 The framework of our model makes the introduction of heterogeneous resource efficient and convenient, and this can further improve our experimental performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments to compare our model with previous landmark methods on the benchmark dataset CPB for Chinese SRL.", "labels": [], "entities": [{"text": "benchmark dataset CPB", "start_pos": 82, "end_pos": 103, "type": "DATASET", "confidence": 0.8192796309789022}, {"text": "Chinese SRL", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.7258122265338898}]}, {"text": "The result  To facilitate comparison with previous work, we conduct experiments on the standard benchmark dataset CPB 1.0. 1 We follow the same data setting as previous work, which divided the dataset into three parts: 648 files (from chtb 081.fid to chtb 899.fid) are used as the training set.", "labels": [], "entities": [{"text": "benchmark dataset CPB 1.0. 1", "start_pos": 96, "end_pos": 124, "type": "DATASET", "confidence": 0.8080042839050293}]}, {"text": "The development set includes 40 files, from chtb 041.fid to chtb 080.fid.", "labels": [], "entities": []}, {"text": "The test set includes 72 files, which are chtb 001.fid to chtb 040.fid, and chtb 900.fid to chtb 931.fid.", "labels": [], "entities": []}, {"text": "We use another annotated corpus 2 with distinct semantic role labels and annotation schema, which is designed by ourselves for other projects, as heterogeneous resource.", "labels": [], "entities": []}, {"text": "This labeled dataset has 17,308 annotated sentences, and the semantic roles concerned are like \"agent\" and \"patient\", resulting in 21 kinds of types, which are all distinct from the semantic roles defined in CPB.", "labels": [], "entities": []}, {"text": "We use the development set of CPB for model selection, and the hyper parameter setting of our model is reported in.", "labels": [], "entities": [{"text": "model selection", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7054638564586639}]}, {"text": "As indicated by this table, our approach significantly outperforms previous stateof-the-art methods even with all parameters randomly initialized, that is without introducing other resources.", "labels": [], "entities": []}, {"text": "This result can prove the ability of our model to capture useful dependencies for Chinese SRL with minimal feature engineering.", "labels": [], "entities": [{"text": "SRL", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.7118436098098755}]}, {"text": "Further, we conduct experiments with the introduction of heterogenous resource.", "labels": [], "entities": []}, {"text": "Previous work found that the performance can be improved by pre-training the word embeddings on large unlabeled data and using the obtained embeddings to make initialization.", "labels": [], "entities": []}, {"text": "With the result in, it is true that these pre-trained word embeddings have a good effect on our performance (we use word2vec 3 on Chinese Gigaword Corpus for word pre-training).", "labels": [], "entities": [{"text": "Chinese Gigaword Corpus", "start_pos": 130, "end_pos": 153, "type": "DATASET", "confidence": 0.8563937147458395}]}, {"text": "However, as shown in, compared to standard pre-training, the influence of heterogenous data is more evident.", "labels": [], "entities": []}, {"text": "We can explain this difference via the distinction between these two kinds of methods for performance improvement.", "labels": [], "entities": []}, {"text": "The information provided by standard pretraining with unlabeled data is more general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyper parameters of our model.", "labels": [], "entities": []}]}