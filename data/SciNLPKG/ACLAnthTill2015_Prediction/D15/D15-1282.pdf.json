{"title": [{"text": "Social Media Text Classification under Negative Covariate Shift", "labels": [], "entities": [{"text": "Social Media Text Classification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5631602630019188}, {"text": "Negative Covariate Shift", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.5796138346195221}]}], "abstractContent": [{"text": "Ina typical social media content analysis task, the user is interested in analyzing posts of a particular topic.", "labels": [], "entities": [{"text": "social media content analysis task", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.8361402750015259}]}, {"text": "Identifying such posts is often formulated as a classification problem.", "labels": [], "entities": [{"text": "Identifying such posts", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8955073356628418}]}, {"text": "However, this problem is challenging.", "labels": [], "entities": []}, {"text": "One key issue is covariate shift.", "labels": [], "entities": [{"text": "covariate shift", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.6469827741384506}]}, {"text": "That is, the training data is not fully representative of the test data.", "labels": [], "entities": []}, {"text": "We observed that the covariate shift mainly occurs in the negative data because topics discussed in social media are highly diverse and numerous, but the user-labeled negative training data may cover only a small number of topics.", "labels": [], "entities": [{"text": "covariate", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9518874287605286}]}, {"text": "This paper proposes a novel technique to solve the problem.", "labels": [], "entities": []}, {"text": "The key novelty of the technique is the transformation of document representation from the traditional n-gram feature space to a center-based similarity (CBS) space.", "labels": [], "entities": []}, {"text": "In the CBS space, the covariate shift problem is significantly mitigated, which enables us to build much better classifiers.", "labels": [], "entities": []}, {"text": "Experiment results show that the proposed approach markedly improves classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.9687731862068176}]}], "introductionContent": [{"text": "Applications using social media data, such as reviews, discussion posts, and (micro) blogs are becoming increasingly popular.", "labels": [], "entities": []}, {"text": "We observed from our collaborations with social science and health science researchers that in atypical application, the researcher first need to obtain a set of posts of a particular topic that he/she wants to study, e.g., apolitical issue.", "labels": [], "entities": []}, {"text": "Keyword search is often used as the first step.", "labels": [], "entities": [{"text": "Keyword search", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7530959844589233}]}, {"text": "However, that is not sufficient due to low precision and low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9992954730987549}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9989522695541382}]}, {"text": "A post containing the keyword \"politics\" may not be apolitical post while a post that does not contain the keyword maybe apolitical post.", "labels": [], "entities": []}, {"text": "Thus, text classification is needed to make more sophisticated decisions to improve accuracy.", "labels": [], "entities": [{"text": "text classification", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.8555161356925964}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9926954507827759}]}, {"text": "For classification, the user first manually labels a set of relevant posts (positive data) about the political issue and irrelevant posts (negative data) not about the political issue and then builds a classifier by running a learning algorithm, e.g. SVM or na\u00efve Bayes.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9611055254936218}]}, {"text": "However, the resulting classifier may not be satisfactory.", "labels": [], "entities": []}, {"text": "One key reason we observed is that the labeled negative training data is not fully representative of the negative test data.", "labels": [], "entities": []}, {"text": "Let the user-interested topic be P (positive), and the set of all other irrelevant topics discussed in asocial media source be T = {T 1 , T 2 , \u2026, T n }, which forms the negative data.", "labels": [], "entities": []}, {"text": "However, due to the labor-intensive effort of manual labeling, the user can label only a certain number of training posts.", "labels": [], "entities": []}, {"text": "Then the labeled negative training posts may cover only a small number of irrelevant topics S of T (S \u2286 T) as negative.", "labels": [], "entities": []}, {"text": "Further, due to the highly dynamic nature of social media, it is probably impossible to label all possible negative topics.", "labels": [], "entities": []}, {"text": "In testing, when posts of other negative topics in T\u2212S show up, their classification can be unpredictable.", "labels": [], "entities": []}, {"text": "For example, in an application, the training data has no negative examples about sports.", "labels": [], "entities": []}, {"text": "However, in testing, some sports posts show up.", "labels": [], "entities": []}, {"text": "These unexpected sports posts maybe classified arbitrarily, which results in low classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9762732982635498}]}, {"text": "In this paper, we aim to solve this problem.", "labels": [], "entities": []}, {"text": "In machine learning, this problem is called covariate shift, a type of sample selection bias.", "labels": [], "entities": []}, {"text": "In classic machine learning, it is assumed that the training and testing data are drawn from the same distribution.", "labels": [], "entities": []}, {"text": "However, this assumption may not hold in practice such as in our case above, i.e., the training and the test distributions are different).", "labels": [], "entities": []}, {"text": "In general, the sample selection bias problem is not solvable because the two distributions can be arbitrarily far apart from each other.", "labels": [], "entities": [{"text": "sample selection bias", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.7419353326161703}]}, {"text": "Various assumptions were made to solve special cases of the problem.", "labels": [], "entities": []}, {"text": "One main assumption was that the conditional distribution of the class given a data instance is the same in the training and test data sets).", "labels": [], "entities": []}, {"text": "This gives the covariate shift problem.", "labels": [], "entities": []}, {"text": "In this paper, we focus on a special case of the covariate shift problem.", "labels": [], "entities": []}, {"text": "We assume that the covariate shift problem occurs mainly in the negative training and test data, and no or minimum covariate shift exists in the positive training and test data.", "labels": [], "entities": []}, {"text": "This assumption is reasonable because the user knows the type of posts/documents that s/he is looking for and can label many of them.", "labels": [], "entities": []}, {"text": "Following the notations in (), our special case of the covariate shift problem can be stated formally as follows: let the set of training examples be {(x 1 , y 1 ), (x 2 , y 2 ), \u2026, (x k , y k )}, where xi is the data/feature vector and y i is the class label of xi . Let the set of test cases be {x k+1 , x k+2 , \u2026, x n }, which have no class labels.", "labels": [], "entities": []}, {"text": "Since we are interested in binary classification, y i is either 1 (positive class) or -1 (negative class).", "labels": [], "entities": [{"text": "binary classification", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.7284051179885864}]}, {"text": "The labeled training data and the unseen test data have the same target conditional distribution p(y|x) and the marginal distributions of the positive data in both the training and testing are also the same.", "labels": [], "entities": []}, {"text": "But the marginal distributions of the negative data in the training and testing are different, i.e., \u00ed \u00b5\u00ed\u00b1\u009d ! (\u00ed \u00b5\u00ed\u00b0\u00b1 ! ) \u2260 \u00ed \u00b5\u00ed\u00b1\u009d ! (\u00ed \u00b5\u00ed\u00b0\u00b1 ! ), where L, T, andrepresent the labeled training data, test data, and the negative class respectively.", "labels": [], "entities": []}, {"text": "Existing methods for addressing the covariate shift problem basically work as follows (see the Related Work section).", "labels": [], "entities": []}, {"text": "First, they estimate the bias of the training data based on the given test data using some statistical techniques.", "labels": [], "entities": []}, {"text": "Then, a classifier is trained on a weighted version of the original training set based on the estimated bias.", "labels": [], "entities": []}, {"text": "Requiring the test data to be available in training is, however, a major weakness.", "labels": [], "entities": []}, {"text": "In the social media post classification setting, the system needs to constantly classify the incoming data.", "labels": [], "entities": []}, {"text": "It is infeasible to perform training constantly.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel learning technique that does not need the test data to be available during training due to the specific nature of our problem, i.e., the positive training data does not have the covariate shift issue.", "labels": [], "entities": []}, {"text": "One obvious solution to this problem is oneclass classification (, i.e., one-class SVM.", "labels": [], "entities": [{"text": "oneclass classification", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.6885914206504822}]}, {"text": "We simply discard the negative training posts/documents completely because they have the covariate shift problem.", "labels": [], "entities": []}, {"text": "Although this is a valid solution, as we will see in the evaluation section, the models built based on one-class SVM perform poorly.", "labels": [], "entities": []}, {"text": "Although it is conceivable to use an unsupervised method such clustering, SVD () or LDA (, supervised learning usually give much higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9962228536605835}]}, {"text": "In our proposed method, instead of performing supervised learning in the original document space based on n-grams, we perform learning in a similarity space.", "labels": [], "entities": []}, {"text": "Thus, the key novelty of the method is the transformation from the original document space (DS) to a center-based similarity space (CBS).", "labels": [], "entities": []}, {"text": "In the new space, the covariate shift problem is significantly mitigated, which enables us to build more accurate classifiers.", "labels": [], "entities": []}, {"text": "The reason for this is that in CBS based learning the vectors in the similarity space enable SVM (which is the learning algorithm that we use) to find a good boundary of the positive class data based on similarity and to separate it from all possible negative class data, including those negative data that is not represented in training.", "labels": [], "entities": []}, {"text": "We will explain this in greater detail in Section 3.5 after we present the proposed algorithm, which we call CBS-L (for CBS Learning).", "labels": [], "entities": []}, {"text": "This paper makes three contributions: First, it formulates a special case of the covariate shift problem.", "labels": [], "entities": []}, {"text": "This case occurs frequently in social media data classification as we discussed above.", "labels": [], "entities": []}, {"text": "Second, it proposes a novel CBS space based learning method, CBS-L, which avoids the covariate shift problem to a large extent because it is able to find a good similarity boundary of the positive data.", "labels": [], "entities": []}, {"text": "Third, it experimentally demonstrates the effectiveness of the proposed method.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the proposed learning in the center-based similarity space (CBS-L) and compare it with baselines.", "labels": [], "entities": []}, {"text": "As stated at the beginning of the paper, this work was motivated by the real-life problem of identifying the right social media posts or documents for specific applications.", "labels": [], "entities": []}, {"text": "For an effective evaluation, we need a large number of classes in the data to reflect the topic richness and diversity of the social media.", "labels": [], "entities": []}, {"text": "The whole data also has to be labeled for evaluation.", "labels": [], "entities": []}, {"text": "Using online reviews of a large number of products is a natural choice because there are many types of products and services and there is no need to do manual labeling, which is very labor intensive, time consuming, and error prone.", "labels": [], "entities": []}, {"text": "We obtained the Amazon review database from the authors of (Jindal and Liu 2008), and constructed a dataset with reviews of 50 types of products, which we also call 50 topics.", "labels": [], "entities": [{"text": "Amazon review database", "start_pos": 16, "end_pos": 38, "type": "DATASET", "confidence": 0.88914954662323}]}, {"text": "Each topic (a type of products) have 1000 reviews.", "labels": [], "entities": []}, {"text": "For each topic, we randomly sampled 700 reviews/documents for training and the remaining 300 reviews for testing.", "labels": [], "entities": []}, {"text": "Note that although we use this product review collection, we do not perform sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.9424248337745667}]}, {"text": "Instead, we still perform the traditional topic based classification.", "labels": [], "entities": [{"text": "topic based classification", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.6222370763619741}]}, {"text": "That is, given a review, the system decides what type of product the review is about.", "labels": [], "entities": []}, {"text": "In our experiments, we use every topic as the positive class.", "labels": [], "entities": []}, {"text": "This gives us 50 classification results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Summary results of the 50 topics", "labels": [], "entities": []}, {"text": " Table 3: F1-score for each positive topic or class  in the combined case", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995589852333069}]}]}