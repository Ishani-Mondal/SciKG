{"title": [{"text": "A Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution", "labels": [], "entities": []}], "abstractContent": [{"text": "Most existing word embedding methods can be categorized into Neural Embedding Models and Matrix Factorization (MF)-based methods.", "labels": [], "entities": []}, {"text": "However some models are opaque to probabilistic interpretation , and MF-based methods, typically solved using Singular Value Decomposition (SVD), may incur loss of corpus information.", "labels": [], "entities": []}, {"text": "In addition, it is desirable to incorporate global latent factors, such as topics, sentiments or writing styles, into the word embedding model.", "labels": [], "entities": []}, {"text": "Since gen-erative models provide a principled way to incorporate latent factors, we propose a generative word embedding model, which is easy to interpret, and can serve as a basis of more sophisticated latent factor models.", "labels": [], "entities": []}, {"text": "The model inference reduces to a low rank weighted positive semidefinite approximation problem.", "labels": [], "entities": []}, {"text": "Its optimization is approached by eigendecomposition on a submatrix, followed by online blockwise regression, which is scalable and avoids the information loss in SVD.", "labels": [], "entities": []}, {"text": "In experiments on 7 common benchmark datasets, our vectors are competitive to word2vec, and better than other MF-based methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of word embedding is to model the distribution of a word and its context words using their corresponding vectors in a Euclidean space.", "labels": [], "entities": []}, {"text": "Then by doing regression on the relevant statistics derived from a corpus, a set of vectors are recovered which best fit these statistics.", "labels": [], "entities": []}, {"text": "These vectors, commonly referred to as the embeddings, capture semantic/syntactic regularities between the words.", "labels": [], "entities": []}, {"text": "The core of a word embedding method is the link function that connects the input -the embeddings, with the output -certain corpus statistics.", "labels": [], "entities": []}, {"text": "Based on the link function, the objective function is developed.", "labels": [], "entities": []}, {"text": "The reasonableness of the link function impacts the quality of the obtained embeddings, and different link functions are amenable to different optimization algorithms, with different scalability.", "labels": [], "entities": []}, {"text": "Based on the forms of the link function and the optimization techniques, most methods can be divided into two classes: the traditional neural embedding models, and more recent low rank matrix factorization methods.", "labels": [], "entities": []}, {"text": "The neural embedding models use the softmax link function to model the conditional distribution of a word given its context (or vice versa) as a function of the embeddings.", "labels": [], "entities": []}, {"text": "The normalizer in the softmax function brings intricacy to the optimization, which is usually tackled by gradient-based methods.", "labels": [], "entities": []}, {"text": "The pioneering work was (.", "labels": [], "entities": []}, {"text": "Later propose three different link functions.", "labels": [], "entities": []}, {"text": "However there are interaction matrices between the embeddings in all these models, which complicate and slowdown the training, hindering them from being trained on huge corpora. and greatly simplify the conditional distribution, where the two embeddings interact directly.", "labels": [], "entities": []}, {"text": "They implemented the well-known \"word2vec\", which can be trained efficiently on huge corpora.", "labels": [], "entities": []}, {"text": "The obtained embeddings show excellent performance on various tasks.", "labels": [], "entities": []}, {"text": "Low-Rank Matrix Factorization (MF in short) methods include various link functions and optimization methods.", "labels": [], "entities": []}, {"text": "The link functions are usually not softmax functions.", "labels": [], "entities": []}, {"text": "MF methods aim to reconstruct certain corpus statistics matrix by the product of two low rank factor matrices.", "labels": [], "entities": [{"text": "MF", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.943348228931427}]}, {"text": "The objective is usually to minimize the reconstruction error, optionally with other constraints.", "labels": [], "entities": []}, {"text": "In this line of research, find that \"word2vec\" is essentially doing stochastic weighted factorization of the word-context pointwise mutual information (PMI) matrix.", "labels": [], "entities": []}, {"text": "They then factorize this matrix directly as anew method.", "labels": [], "entities": []}, {"text": "propose a bilinear regression function of the conditional distribution, from which a weighted MF problem on the bigram logfrequency matrix is formulated.", "labels": [], "entities": []}, {"text": "Gradient Descent is used to find the embeddings.", "labels": [], "entities": []}, {"text": "Recently, based on the intuition that words can be organized in semantic hierarchies,  add hierarchical sparse regularizers to the matrix reconstruction error.", "labels": [], "entities": []}, {"text": "With similar techniques,  reconstruct a set of pretrained embeddings using sparse vectors of greater dimensionality.", "labels": [], "entities": []}, {"text": "apply Canonical Correlation Analysis (CCA) to the word matrix and the context matrix, and use the canonical correlation vectors between the two matrices as word embeddings. and assume a Brown language model, and prove that doing CCA on the bigram occurrences is equivalent to finding a transformed solution of the language model.", "labels": [], "entities": []}, {"text": "assume there is a hidden discourse vector on a random walk, which determines the distribution of the current word.", "labels": [], "entities": []}, {"text": "The slowly evolving discourse vector puts a constraint on the embeddings in a small text window.", "labels": [], "entities": []}, {"text": "The maximum likelihood estimate of the embeddings within this text window approximately reduces to a squared norm objective.", "labels": [], "entities": []}, {"text": "There are two limitations in current word embedding methods.", "labels": [], "entities": []}, {"text": "The first limitation is, all MFbased methods map words and their context words to two different sets of embeddings, and then employ Singular Value Decomposition (SVD) to obtain a low rank approximation of the word-context matrix M . As SVD factorizes MM , some information in M is lost, and the learned embeddings may not capture the most significant regularities in M . Appendix A gives a toy example on which SVD does notwork properly.", "labels": [], "entities": []}, {"text": "The second limitation is, a generative model for documents parametered by embeddings is absent in recent development.", "labels": [], "entities": []}, {"text": "Although) are based on generative processes, the generative processes are only for deriving the local relationship between embeddings within a small text window, leaving the likelihood of a document undefined.", "labels": [], "entities": []}, {"text": "In addition, the learning objectives of some models, e.g. (, Eq.1), even have no clear probabilistic interpretation.", "labels": [], "entities": []}, {"text": "A generative word embedding model for documents is not only easier to interpret and analyze, but more importantly, provides a basis upon which documentlevel global latent factors, such as document topics), sentiments, writing styles (), can be incorporated in a principled manner, to better model the text distribution and extract relevant information.", "labels": [], "entities": []}, {"text": "Based on the above considerations, we propose to unify the embeddings of words and context words.", "labels": [], "entities": []}, {"text": "Our link function factorizes into three parts: the interaction of two embeddings capturing linear correlations of two words, a residual capturing nonlinear or noisy correlations, and the unigram priors.", "labels": [], "entities": []}, {"text": "To reduce overfitting, we put Gaussian priors on embeddings and residuals, and apply Jelinek-Mercer Smoothing to bigrams.", "labels": [], "entities": []}, {"text": "Furthermore, to model the probability of a sequence of words, we assume that the contributions of more than one context word approximately add up.", "labels": [], "entities": []}, {"text": "Thereby a generative model of documents is constructed, parameterized by embeddings and residuals.", "labels": [], "entities": []}, {"text": "The learning objective is to maximize the corpus likelihood, which reduces to a weighted low-rank positive semidefinite (PSD) approximation problem of the PMI matrix.", "labels": [], "entities": []}, {"text": "A Block Coordinate Descent algorithm is adopted to find an approximate solution.", "labels": [], "entities": [{"text": "Block Coordinate Descent", "start_pos": 2, "end_pos": 26, "type": "TASK", "confidence": 0.6392133831977844}]}, {"text": "This algorithm is based on Eigendecomposition, which avoids information loss in SVD, but brings challenges to scalability.", "labels": [], "entities": []}, {"text": "We then exploit the sparsity of the weight matrix and implement an efficient online blockwise regression algorithm.", "labels": [], "entities": []}, {"text": "On seven benchmark datasets covering similarity and analogy tasks, our method achieves competitive and stable performance.", "labels": [], "entities": [{"text": "similarity and analogy tasks", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.649569496512413}]}, {"text": "The source code of this method is provided at https://github.com/askerlee/topicvec.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance of each method across different tasks.", "labels": [], "entities": []}]}