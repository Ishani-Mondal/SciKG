{"title": [{"text": "Spelling Correction of User Search Queries through Statistical Machine Translation", "labels": [], "entities": [{"text": "Spelling Correction of User Search Queries", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7796801030635834}, {"text": "Statistical Machine Translation", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.6006198823451996}]}], "abstractContent": [{"text": "We use character-based statistical machine translation in order to correct user search queries in the e-commerce domain.", "labels": [], "entities": [{"text": "character-based statistical machine translation", "start_pos": 7, "end_pos": 54, "type": "TASK", "confidence": 0.6005225479602814}]}, {"text": "The training data is automatically extracted from event logs where users reissue their search queries with potentially corrected spelling within the same session.", "labels": [], "entities": []}, {"text": "We show results on a test set which was annotated by humans and compare against online autocorrection capabilities of three additional web sites.", "labels": [], "entities": []}, {"text": "Overall, the methods presented in this paper outperform fully productized spellchecking and autocorrec-tion services in terms of accuracy and F1 score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9995853304862976}, {"text": "F1 score", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.975904643535614}]}, {"text": "We also propose novel evaluation steps based on retrieved search results of the corrected queries in terms of quantity and relevance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spelling correction is an important feature for any interactive service that takes input generated by users, e.g. an e-commerce website that allows searching for goods and products.", "labels": [], "entities": [{"text": "Spelling correction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9715632796287537}]}, {"text": "Misspellings are very common with user-generated input and the reason why many web sites offer spelling correction in the form of \"Did you mean?\" suggestions or automatic corrections.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.8290183544158936}]}, {"text": "Autocorrection increases user satisfaction by correcting obvious errors, whereas suggestions make it convenient for users to accept a proposed correction without retyping or correcting the query manually.", "labels": [], "entities": []}, {"text": "Spelling correction is not a trivial task, as search * The author is now affiliated with Lilt Inc., Stanford, CA, USA.", "labels": [], "entities": [{"text": "Spelling correction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8251872658729553}]}, {"text": "\u2020 The author is now affiliated with Stylight GmbH, Munich, Germany.", "labels": [], "entities": [{"text": "Stylight GmbH", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.8653291463851929}]}, {"text": "queries are often short and lack context.", "labels": [], "entities": []}, {"text": "Misspelled queries might be considered correct by a statistical spelling correction system as there is evidence in the data through frequent occurrences.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.6165475100278854}]}, {"text": "While common successful methods (cf. Section 1.1) rely on either human-annotated data or the entire web, we wanted to use easily accessible in-domain data and on top of that technology that is already available.", "labels": [], "entities": []}, {"text": "In this work, we use user event logs from an e-commerce website to fetch similar search query pairs within an active session.", "labels": [], "entities": []}, {"text": "The main idea is that users issue a search query but alter it into something similar within a given time window which might be the correction of a potential typo.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, the idea of collecting query corrections using user session and time information is novel.", "labels": [], "entities": [{"text": "collecting query corrections", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.6232974231243134}]}, {"text": "Previous work suggested collecting queries using information that a user clicked on a proposed correction.", "labels": [], "entities": []}, {"text": "Our proposed method for collecting training data has several advantages.", "labels": [], "entities": []}, {"text": "First, we do not rely on a previous spelling correction system, but on user formulations.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7911095023155212}]}, {"text": "Second, for many search queries, especially from the tail where search recall is generally low, these misspellings yield few results, and thus, users looking for certain products are inclined to correct the query themselves in order to find what they are looking for.", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9642344117164612}]}, {"text": "We use DamerauLevenshtein distance on character level as similarity criterion, i.e. queries within a specific edit distance are considered to be related.", "labels": [], "entities": []}, {"text": "The steps proposed in this work are: 1.", "labels": [], "entities": []}, {"text": "Extraction of similar user queries from search logs for bootstrapping training data (Section 2), 2.", "labels": [], "entities": []}, {"text": "classification and filtering of data to remove noisy entries (Section 3), and 3.", "labels": [], "entities": [{"text": "classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9520010948181152}]}, {"text": "spelling correction cast into a statistical machine translation framework based on character bigram sequences (Section 4).", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8640504479408264}, {"text": "statistical machine translation", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.6210809946060181}]}, {"text": "We also evaluate the work thoroughly in Section 5 where we compare our method to three other online sites, two of them from the e-commerce domain, and present a novel approach that determines quality based on retrieved search results.", "labels": [], "entities": []}, {"text": "We show examples indicating that our method can handle both corrections of misspelled queries and queries with segmentation issues (i.e. missing whitespace delimiters).", "labels": [], "entities": []}, {"text": "A summary can be found in Section 6.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.9181085824966431}]}, {"text": "To our knowledge, this is the first work that uses character-based machine translation technology on user-generated data for spelling correction.", "labels": [], "entities": [{"text": "character-based machine translation", "start_pos": 51, "end_pos": 86, "type": "TASK", "confidence": 0.6580602824687958}, {"text": "spelling correction", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.94761723279953}]}, {"text": "Moreover, it is the first to evaluate the performance in an e-commerce setting with there relevant measures.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we report on a series of hillclimbing experiments used to optimize performance.", "labels": [], "entities": []}, {"text": "In the following, we use standard information retrieval criteria for evaluation, namely accuracy, precision, recall, and F1 score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9996094107627869}, {"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9989125728607178}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9991459846496582}, {"text": "F1 score", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9834056496620178}]}, {"text": "depicts a natural way of how the scoring is performed.", "labels": [], "entities": []}, {"text": "TP denotes true positives, FN false negatives, FP false positives, and TN true negatives, respectively.", "labels": [], "entities": [{"text": "TP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.7389663457870483}, {"text": "FN false negatives", "start_pos": 27, "end_pos": 45, "type": "METRIC", "confidence": 0.7728312611579895}, {"text": "FP false positives", "start_pos": 47, "end_pos": 65, "type": "METRIC", "confidence": 0.8881305058797201}, {"text": "TN true negatives", "start_pos": 71, "end_pos": 88, "type": "METRIC", "confidence": 0.7112311323483785}]}, {"text": "Note that for the case where the gold reference demands a correction, and the speller produces a wrong correction different from the source query, we have to increase both false positives FP and false negatives FN . With this, we can do stan-  dard calculation of accuracy as (TP + TN )/(P + N ) with P = TP + FN and N = TN + FP , precision as TP /(TP + FP ), recall as TP /P , and F1 score as 2TP /(2TP + FP + FN ).", "labels": [], "entities": [{"text": "FP", "start_pos": 188, "end_pos": 190, "type": "METRIC", "confidence": 0.858300507068634}, {"text": "accuracy", "start_pos": 264, "end_pos": 272, "type": "METRIC", "confidence": 0.9992444515228271}, {"text": "precision", "start_pos": 331, "end_pos": 340, "type": "METRIC", "confidence": 0.9995606541633606}, {"text": "recall", "start_pos": 360, "end_pos": 366, "type": "METRIC", "confidence": 0.9995939135551453}, {"text": "F1 score", "start_pos": 382, "end_pos": 390, "type": "METRIC", "confidence": 0.9864838719367981}, {"text": "2TP", "start_pos": 394, "end_pos": 397, "type": "METRIC", "confidence": 0.8650904297828674}]}, {"text": "An interesting way to evaluate the performance of the autocorrection system is to look at it from a search perspective.", "labels": [], "entities": []}, {"text": "If the goal is to improve user experience in search and retrieve more relevant results, we need to compare the search result sets in terms of quantity and quality.", "labels": [], "entities": []}, {"text": "For quantity we are mainly interested in how many queries lead to 0 search results before and after autocorrection.", "labels": [], "entities": []}, {"text": "the source set we improve the query behavior for 63.8% queries after autocorrection while we only decrease from non-null to null for 1.6%.", "labels": [], "entities": []}, {"text": "A manual inspection of the 1.6% shows that those queries already led to very low search results (<10) even for the source.", "labels": [], "entities": []}, {"text": "For quality, we compare the search result sets for the autocorrected queries with those of the reference and get an estimate of how similar each autocorrected query behaves to the expected behavior given by its gold correction.", "labels": [], "entities": []}, {"text": "In particular, we extract the categories from all items which are returned as part of the search API calls (see).", "labels": [], "entities": []}, {"text": "We use Kullback-Leibler divergence to compute the difference between the category distributions.", "labels": [], "entities": []}, {"text": "The KL divergence defines the information lost when using one distribution to approximate another: In our case, we use the category distribution of the results of the reference query as the observed data P which we want to model with the category distribution Q from the results of the autocorrected query.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.500052735209465}]}, {"text": "This gives us a more realistic evaluation of the method, as it is more relevant to the retrieval problem that autocorrection is trying to address.", "labels": [], "entities": []}, {"text": "The queries perfumes for women and perfume for women, e.g., retrieve similar amounts and types of items although their strings are different (note the plural sin the first one) which is penalized when computing accuracy on string level.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9963985681533813}]}, {"text": "In order to deal with zero-probability events, we smooth both distributions such that they contain exactly the same categories.", "labels": [], "entities": []}, {"text": "For this we add categories that are present in one distribution but not Misspelled query otterbox iphone 5 womens realhairsaltandpeper womens real hair salt and pepper: Examples of misspelled user search queries that the presented system is able to correct.", "labels": [], "entities": [{"text": "otterbox iphone 5 womens realhairsaltandpeper womens real hair salt", "start_pos": 89, "end_pos": 156, "type": "DATASET", "confidence": 0.8332850204573737}]}, {"text": "the other with a very small probability which we subtract from the highest probability in the distribution so that all probabilities still sum up to 1.", "labels": [], "entities": []}, {"text": "In the case of getting 0 search results for either the autocorrected or reference query we cannot compute the KL divergence and we simply mark those cases with a high number.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.5424356758594513}]}, {"text": "If both queries return 0 search results we return a distance of 0.", "labels": [], "entities": []}, {"text": "Note that with this we do not penalize queries that are misspelled and should have been corrected even if they still retrieve 0 items in search after correction.", "labels": [], "entities": []}, {"text": "However, in this test we are only interested in the search results we get and not in the correction itself.", "labels": [], "entities": []}, {"text": "shows that the presented method is closest in terms of KL divergence to category distributions of search results based on the gold references.", "labels": [], "entities": []}, {"text": "Even though this is a nice and easy way to indicate quality of search results, we do not claim this method to bean in-depth analysis of relevance of search results and leave this for future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Extracted query pairs found in user event logs. Labels in column 4 indicate whether user- initiated spelling correction (Yes) has taken place vs. a search reformulation that entails the source query  not being misspelled (No).", "labels": [], "entities": [{"text": "spelling correction (Yes)", "start_pos": 110, "end_pos": 135, "type": "METRIC", "confidence": 0.6980064034461975}]}, {"text": " Table 3: Preprocessing of input data as single character or character bigram sequences. \"S\" denotes  whitespace. After translation, postprocessing transforms back to word-level surface forms.", "labels": [], "entities": []}, {"text": " Table 4: Statistics on dev and test portions of  the post-edited evaluation data. CER is the char- acter error rate of the misspelled queries against  the gold reference, SER is the sentence (i.e. here  query-level) error rate of the sets.", "labels": [], "entities": [{"text": "CER", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9925421476364136}, {"text": "char- acter error rate", "start_pos": 94, "end_pos": 116, "type": "METRIC", "confidence": 0.6807945251464844}, {"text": "SER", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.9989970326423645}]}, {"text": " Table 5: Comparison of accuracy, precision, recall  and F1 score against other online sites on the test  set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9989237189292908}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9994810223579407}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9995607733726501}, {"text": "F1 score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9811483025550842}]}, {"text": " Table 6: Hillclimbing on the dev set.", "labels": [], "entities": []}, {"text": " Table 8: Search results evaluation on the test set.  For null results, lower rates are better. For KL div.  evaluation, higher rates are better, as they indicate  a closer match with the category distribution from  the gold corrections.", "labels": [], "entities": [{"text": "KL div.  evaluation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.5861020088195801}]}]}