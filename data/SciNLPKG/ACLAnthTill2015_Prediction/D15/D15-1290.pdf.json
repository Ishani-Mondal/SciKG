{"title": [{"text": "A Single Word is not Enough: Ranking Multiword Expressions Using Distributional Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "We present anew unsupervised mechanism , which ranks word n-grams according to their multiwordness.", "labels": [], "entities": []}, {"text": "It heavily relies on anew uniqueness measure that computes, based on a distributional thesaurus , how often an n-gram could be replaced in context by a single-worded term.", "labels": [], "entities": []}, {"text": "In addition with a downweighting mechanism for incomplete terms this forms anew measure called DRUID.", "labels": [], "entities": [{"text": "DRUID", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.6819149851799011}]}, {"text": "Results show large improvements on two small test sets over competitive baselines.", "labels": [], "entities": []}, {"text": "We demonstrate the scalability of the method to large corpora, and the independence of the measure of shallow syntactic filtering.", "labels": [], "entities": []}], "introductionContent": [{"text": "While it seems intuitive to treat certain sequences of tokens as single terms, there is still considerable controversy about the definition of what exactly such a multiword expression (MWE) constitutes.", "labels": [], "entities": []}, {"text": "pinpoint the need of treating MWEs correctly and classify a range of syntactic formations that could form MWEs and define MWEs as being non-compositional with respect to the meaning of their parts.", "labels": [], "entities": []}, {"text": "While the exact requirements on MWEs is bound to specific tasks (such as parsing, keyword extraction, etc.), we operationalize the notion of non-compositionality by using distributional semantics and introduce anew measure that works well fora range of task-based MWE definitions.", "labels": [], "entities": [{"text": "parsing, keyword extraction", "start_pos": 73, "end_pos": 100, "type": "TASK", "confidence": 0.6398772820830345}]}, {"text": "Most previous MWE ranking approaches use the following mechanisms to determine multiwordness: part-of-speech (POS) tags, word/multiword frequency and significance of co-occurrence of the parts.", "labels": [], "entities": [{"text": "MWE ranking", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9415532052516937}]}, {"text": "In this paper we do not want to introduce \"yet another ranking function\" but rather an additional mechanism, which performs ranking based on distributional semantics.", "labels": [], "entities": []}, {"text": "Distributional semantics has already been used for MWE identification, but mainly to discriminate between compositional and noncompositional MWEs ().", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9900662899017334}]}, {"text": "Here we introduce anew concept to describe the multiwordness of a term by its uniqueness.", "labels": [], "entities": []}, {"text": "Using the uniqueness score we measure how likely a term in context can be replaced by a single word.", "labels": [], "entities": []}, {"text": "This measure is motivated by the semiotic consideration that due to parsimony concepts are often expressed as single words.", "labels": [], "entities": []}, {"text": "Furthermore, we implement a context-aware punishment, called incompleteness, which degrades the score of terms that seem incomplete regarding their contexts.", "labels": [], "entities": []}, {"text": "Both concepts are combined into a single score we call DRUID, which is calculated based on a distributional thesaurus.", "labels": [], "entities": [{"text": "DRUID", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9088587164878845}]}, {"text": "In the following, we show the impact of that new method for French and English and also examine the effect of corpus size on MWE extraction.", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 125, "end_pos": 139, "type": "TASK", "confidence": 0.9876992702484131}]}, {"text": "Additionally, we report on results without using any linguistic preprocessing except tokenization.", "labels": [], "entities": []}], "datasetContent": [{"text": "We examine two experimental settings: First, we compute all measures on a small corpus that has been annotated for MWEs, which serves as the gold standard.", "labels": [], "entities": []}, {"text": "In the second setting we compute the measures on a larger in-domain corpus.", "labels": [], "entities": []}, {"text": "The evaluation is again performed for the same candidate terms as given by the gold standard.", "labels": [], "entities": []}, {"text": "Results for the top k ranked entries are reported using the precision at k (P @k = 1 k k i=1 xi with xi equals 1 if the i-th ranked candidate is annotated as MWE and 0 otherwise).", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9985560774803162}, {"text": "MWE", "start_pos": 158, "end_pos": 161, "type": "DATASET", "confidence": 0.7491960525512695}]}, {"text": "For an overall performance we use the average precision (AP) as defined in: AP = 1 |Tmwe| |T | k=1 x k P @k, with T mwe beeing the set of positive MWE.", "labels": [], "entities": [{"text": "average precision (AP)", "start_pos": 38, "end_pos": 60, "type": "METRIC", "confidence": 0.8518367648124695}, {"text": "AP", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9951311349868774}]}, {"text": "When facing tied scores we mix false and true candidates randomly cf..", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Number of candidates after filtering for  the expected POS-tag and their distribution over  n-grams with n \u2208 {1, 2, 3, 4}.", "labels": [], "entities": []}, {"text": " Table 4: Results for the GENIA dataset.", "labels": [], "entities": [{"text": "GENIA dataset", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.9698636531829834}]}, {"text": " Table 5: Results for the French SPMRL dataset", "labels": [], "entities": [{"text": "French SPMRL dataset", "start_pos": 26, "end_pos": 46, "type": "DATASET", "confidence": 0.8944555719693502}]}, {"text": " Table 6: Results computed on the Medline corpus.", "labels": [], "entities": [{"text": "Medline corpus", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.9928033649921417}]}, {"text": " Table 7: Results computed based on the ERC.", "labels": [], "entities": [{"text": "ERC", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.570257842540741}]}, {"text": " Table 8: Results without linguistic pre-processing.", "labels": [], "entities": []}, {"text": " Table 9: Top ranked candidates from the GENIA  dataset using our method (left) and the competitive  method (right). Each term is marked, whether the  term is an MWE (1) or not (0).", "labels": [], "entities": [{"text": "GENIA  dataset", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.979659229516983}]}, {"text": " Table 10: Top ranked candidates from the SPMRL  dataset for the best DRUID method (left) and the  best competitive method (right). Each term is  marked, if it is an MWE (1) or not (0).", "labels": [], "entities": [{"text": "SPMRL  dataset", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.8362507522106171}]}]}