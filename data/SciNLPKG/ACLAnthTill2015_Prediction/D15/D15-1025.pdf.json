{"title": [{"text": "Non-lexical neural architecture for fine-grained POS Tagging", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.8433321118354797}]}], "abstractContent": [{"text": "In this paper we explore a POS tagging application of neural architectures that can infer word representations from the raw character stream.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.856540858745575}]}, {"text": "It relies on two modelling stages that are jointly learnt: a convolutional network that infers a word representation directly from the character stream, followed by a prediction stage.", "labels": [], "entities": []}, {"text": "Models are evaluated on a POS and morphological tagging task for German.", "labels": [], "entities": [{"text": "POS", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.6237170100212097}]}, {"text": "Experimental results show that the convolu-tional network can infer meaningful word representations, while for the prediction stage, a well designed and structured strategy allows the model to outperform state-of-the-art results, without any feature engineering .", "labels": [], "entities": []}], "introductionContent": [{"text": "Most modern statistical models for natural language processing (NLP) applications are strongly or fully lexicalized, for instance part-of-speech (POS) and named entity taggers, as well as language models, and parsers.", "labels": [], "entities": [{"text": "part-of-speech (POS) and named entity taggers", "start_pos": 130, "end_pos": 175, "type": "TASK", "confidence": 0.6270868740975857}]}, {"text": "In these models, the observed word form is considered as the elementary unit, while its morphological properties remain neglected.", "labels": [], "entities": []}, {"text": "As a result, the vocabulary observed on training data heavily restricts the generalization power of lexicalized models.", "labels": [], "entities": []}, {"text": "Designing subword-level systems is appealing for several reasons.", "labels": [], "entities": []}, {"text": "First, words sharing morphological properties often share grammatical function and meaning, and leveraging that information can yield improved word representations.", "labels": [], "entities": []}, {"text": "Second, a subword-level analysis can address the outof-vocabulary issue i.e the fact that word-level models fail to meaningfully process unseen word forms.", "labels": [], "entities": []}, {"text": "This allows a better processing of morphologically rich languages in which there is a combinatorial explosion of word forms, most of which are not observed during training.", "labels": [], "entities": []}, {"text": "Finally, using subword units could allow processing of noisy text such as user-generated content on the Web, where abbreviations, slang usage and spelling mistakes cause the number of word types to explode.", "labels": [], "entities": []}, {"text": "This work investigates models that do not rely on a fixed vocabulary to make a linguistic prediction.", "labels": [], "entities": []}, {"text": "Our main focus in this paper is POS tagging, yet the proposed approach could be applied to a wide variety of language processing tasks.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.8721285760402679}]}, {"text": "Our main contribution is to show that neural networks can successfully learn unlexicalized models that infer a useful word representation from the character stream.", "labels": [], "entities": []}, {"text": "This approach achieves state of-the-art performance on a German POS tagging task.", "labels": [], "entities": [{"text": "POS tagging task", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.5866725246111552}]}, {"text": "This task is difficult because German is a morphologically rich language 1 , as reflected by the large number of morphological tags (255) in our study, yielding a grand total of more than 600 POS+MORPH tags.", "labels": [], "entities": []}, {"text": "An aggravating factor is that these morphological categories are overtly marked by a handful of highly ambiguous inflection marks (suffixes).", "labels": [], "entities": []}, {"text": "We therefore believe that this case study is well suited to assess both the representation and prediction power of our models.", "labels": [], "entities": []}, {"text": "The architecture we explore in section 2 differs from previous work that only consider the character level.", "labels": [], "entities": []}, {"text": "Following, it consists in two stages that are jointly learnt.", "labels": [], "entities": []}, {"text": "The lower stage is a convolutional network that infers a word embedding from a character string of arbitrary size, while the higher network infers the POS tags based on this word embedding sequence.", "labels": [], "entities": []}, {"text": "For the latter, we investigate different architectures of increasing complexities: from a feedforward and context-free inference to a bi-recurrent network that predicts the global sequence.", "labels": [], "entities": []}, {"text": "Experimental results (section 4) show that the proposed approach can achieve state of the art performance and that the choice of architecture for the prediction part of the model has a significant impact.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments are carried out on the Part-of-Speech and Morphological tagging tasks using the German corpus TIGER Treebank ().", "labels": [], "entities": [{"text": "Part-of-Speech and Morphological tagging tasks", "start_pos": 35, "end_pos": 81, "type": "TASK", "confidence": 0.6574059069156647}, {"text": "German corpus TIGER Treebank", "start_pos": 92, "end_pos": 120, "type": "DATASET", "confidence": 0.9315720945596695}]}, {"text": "To the best of our knowledge, the best results on this task were published in (), who applied a high-order CRF that includes an intensive feature engineering to five different languages.", "labels": [], "entities": []}, {"text": "German was highlighted as having 'the most ambiguous morphology'.", "labels": [], "entities": []}, {"text": "The corpus,: Comparison of the feedforward and bidirectional recurrent architectures for predictions, with different settings.", "labels": [], "entities": []}, {"text": "The non-lexical encoding is convolutional.", "labels": [], "entities": []}, {"text": "CRF refers to state-of-the-art system of ().", "labels": [], "entities": []}, {"text": "respectively denote the position-by-position and structured prediction.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.6822376102209091}]}, {"text": "* indicates our best configuration.", "labels": [], "entities": []}, {"text": "scribed in details in (, contains a training set of 40472 sentences, a development and a test set of both 5000 sentences.", "labels": [], "entities": []}, {"text": "We consider the two tagging tasks, with first a coarse tagset (54 tags), and then a morpho-syntactical rich tagset (619 items observed on the the training set).", "labels": [], "entities": []}, {"text": "All the models are implemented 4 with the Theano library (.", "labels": [], "entities": [{"text": "Theano library", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9609134495258331}]}, {"text": "For optimization, we use Adagrad (), with a learning rate of 0.1.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 44, "end_pos": 57, "type": "METRIC", "confidence": 0.9608715772628784}]}, {"text": "The other hyperparameters are: the window sizes, dc and d w , respectively set to 5 and 9, the dimension of character embeddings, word embeddings and of the hidden layer, n c , n f and n h , that are respectively of 100, 200 and 200 5 . The models were trained on 7 epochs.", "labels": [], "entities": []}, {"text": "Parameter initialization and corpus ordering are random, and the results presented are the average and standard deviation of the POS Tagging error rate over 5 runs.", "labels": [], "entities": [{"text": "Parameter initialization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5766774863004684}, {"text": "corpus ordering", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7576701939105988}, {"text": "POS Tagging error rate", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.605266772210598}]}, {"text": "Implementation is available at https://github.", "labels": [], "entities": []}, {"text": "com/MatthieuLabeau/NonlexNN 5 For both the learning rate and the embedding sizes, results does not differ in a significant way in a large range of hyperparameters, and their impact resides more in convergence speed and computation time", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of the feedforward and bidirectional recurrent architectures for predictions, with  different settings. The non-lexical encoding is convolutional. CRF refers to state-of-the-art system of  (", "labels": [], "entities": []}, {"text": " Table 2: Error counts for known/unknown words  in the test set, with a structured feedforward pre- diction model for the tagging task.", "labels": [], "entities": [{"text": "Error counts", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9569589197635651}, {"text": "tagging task", "start_pos": 122, "end_pos": 134, "type": "TASK", "confidence": 0.8901724219322205}]}, {"text": " Table 1. Surprisingly,  this architecture performs poorly with simple in- ference, but clearly improves when predicting a  structured output using the Viterbi algorithm, both  for training and testing. Moreover, a non-lexical  model trained to infer a tag sequence with the  Viterbi algorithm achieves results that are close to  the state-of-the-art, thus validating our approach.  We consider that this improvement comes from the  synergy between using a global training objective  with a global hidden representation, complexify- ing the model but allowing a more efficient solu- tion. Finally, the model that uses the combination  of both the character and word-level embeddings  yields the best results. It is interesting to notice  that the predictive architecture has no influence on  the results of the simple task when the prediction is structured, but improves them on the difficult task.  This also shows that the contribution of word em- beddings to our model corresponds to a difference  of 1.5 to 2 points in performance.", "labels": [], "entities": []}]}