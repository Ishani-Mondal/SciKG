{"title": [{"text": "A Multi-lingual Annotated Dataset for Aspect-Oriented Opinion Mining", "labels": [], "entities": []}], "abstractContent": [{"text": "We present the Trip-MAML dataset, a MultiLingual dataset of hotel reviews that have been manually annotated at the sentence-level with Multi-Aspect sentiment labels.", "labels": [], "entities": [{"text": "Trip-MAML dataset", "start_pos": 15, "end_pos": 32, "type": "DATASET", "confidence": 0.9311547577381134}]}, {"text": "This dataset has been built as an extension of an existent English-only dataset, adding documents written in Ital-ian and Spanish.", "labels": [], "entities": []}, {"text": "We detail the dataset construction process, covering the data gathering, selection, and annotation.", "labels": [], "entities": [{"text": "data gathering", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.7966406047344208}]}, {"text": "We present inter-annotator agreement figures and baseline experimental results, comparing the three languages.", "labels": [], "entities": []}, {"text": "Trip-MAML is a multilingual dataset for aspect-oriented opinion mining that enables researchers (i) to face the problem on languages other than English and (ii) to the experiment the application of cross-lingual learning methods to the task.", "labels": [], "entities": [{"text": "aspect-oriented opinion mining", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6714828709761301}]}], "introductionContent": [{"text": "Reviews of products and services that are spontaneously produced by customers represent a source of unquestionable value not only for marketing strategies of private companies and organizations, but also for other users since their purchasing decisions are likely influenced by other customers' opinions ().", "labels": [], "entities": []}, {"text": "Overall ratings (e.g., in terms of a five stars rating scale), and also aspect-specific ratings (e.g., the Cleanliness or Location of a hotel), are the typical additional information expressed by customers in their reviews.", "labels": [], "entities": []}, {"text": "Those ratings help to derive a number of global scores to facilitate a first screening of the product or service at hand.", "labels": [], "entities": []}, {"text": "Notwithstanding, users who pay more attention to a particular aspect (e.g., the Rooms of a hotel) remain constrained to manually inspect the entire text of reviews in order to find out the reasons other users argued in that respect.", "labels": [], "entities": []}, {"text": "Methods for automatic analysis of the aspect-oriented sentiment expressed in reviews would enable highlighting aspect-relevant parts of the document, so as to allow users to perform a faster and focused inspection of them.", "labels": [], "entities": []}, {"text": "Previous work on opinion mining has already faced the overall sentiment prediction (), multiple aspectoriented analysis (), and finegrained phrase-level analysis ().", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.8431271612644196}, {"text": "sentiment prediction", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.8572080731391907}, {"text": "multiple aspectoriented analysis", "start_pos": 87, "end_pos": 119, "type": "TASK", "confidence": 0.6217566728591919}, {"text": "phrase-level analysis", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.6768401712179184}]}, {"text": "Most of the available opinion mining datasets contain only documents written in English, as this language is the most used on the Internet and the one for which more NLP tools and resources are available.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7693381607532501}]}, {"text": "() worked on the summarization of reviews by means of weakly supervised feature mining.) used a finer-grained dataset in which global polarity annotation is applied also to each sentence composing the document.", "labels": [], "entities": [{"text": "summarization of reviews", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8934388558069865}]}, {"text": "Similarly did) with the Stanford Sentiment Treebank, which annotates each syntactically plausible phrase in thousands of sentences using annotators from Amazon's Mechanical Turk, annotating the polarity of phrases on a five-level scale.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 24, "end_pos": 51, "type": "DATASET", "confidence": 0.8743809858957926}]}, {"text": "() performed a single-label polarity annotation of elementary discourse units of TripAdvisor reviews, adopting ten aspect labels.", "labels": [], "entities": []}, {"text": "() did a similar annotation work, using sentences as the annotation elements and adopting a multi-label polarity annotation, i.e., each sentence can be assigned to zero, one, or more than one aspect.", "labels": [], "entities": []}, {"text": "Cross-lingual sentiment classification) explores the scenario in which training data are available fora language that is different from the language of the test documents.", "labels": [], "entities": [{"text": "Cross-lingual sentiment classification", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7692359685897827}]}, {"text": "Cross-lingual learning methods have important practical applications, since they allow to build classifiers for many languages reusing the training data produced fora single language (typically English), probably giving up a bit of accuracy, but compensating it with a large save in terms of human annotation costs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 232, "end_pos": 240, "type": "METRIC", "confidence": 0.9938392043113708}]}, {"text": "Multi-lingual datasets are beneficial to the research community both as a benchmark to explore cross-lingual learning and also as resources on which to develop and test new NLP tools for languages other than English.", "labels": [], "entities": []}, {"text": "used a multi-lingual dataset focused on full-document classification at the global polarity level.", "labels": [], "entities": [{"text": "full-document classification", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6904231905937195}]}, {"text": "used a dataset of 200 Amazon reviews in German to test cross-lingual document polarity classification using an English training set.", "labels": [], "entities": [{"text": "cross-lingual document polarity classification", "start_pos": 55, "end_pos": 101, "type": "TASK", "confidence": 0.7093737572431564}]}, {"text": "produced a bi-lingual dataset (English and German), named USAGE, in which aspect expressions and subjective expressions are annotated in Amazon product reviews.", "labels": [], "entities": [{"text": "USAGE", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.9652382731437683}]}, {"text": "In) aspect expressions can be any piece of text that mentions a relevant property of the reviewed entity (e.g., washer, hose, looks) and are not categorical label, as in our dataset.", "labels": [], "entities": []}, {"text": "The USAGE dataset is thus more oriented at information extraction rather than at text classification applications.", "labels": [], "entities": [{"text": "USAGE dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9937698841094971}, {"text": "information extraction", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.807550698518753}, {"text": "text classification", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7373040020465851}]}, {"text": "used machine translation to create a multilingual version of the information-extraction oriented MPQA dataset () on six languages (English, Arabic, French, German, Romanian and Spanish).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7440574765205383}, {"text": "MPQA dataset", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.9060109257698059}]}, {"text": "In this paper we present Trip-MAML, which extends the Trip-MA 1 dataset of with Italian and Spanish annotated reviews.", "labels": [], "entities": [{"text": "Trip-MA 1 dataset", "start_pos": 54, "end_pos": 71, "type": "DATASET", "confidence": 0.8774267633756002}]}, {"text": "We describe Trip-MAML and report experiments aimed at defining a first baseline.", "labels": [], "entities": []}, {"text": "Both the dataset and the software used in experiments are publicly available at http://hlt.isti.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments we present here are aimed at defining a shared baseline for future experiments.", "labels": [], "entities": []}, {"text": "For this reason we chose a relatively simple setup that uses a simple learning model and minimal linguistic resources.", "labels": [], "entities": []}, {"text": "We used a sentence-level Linear Chain (LC) Conditional Random Field () as described by.", "labels": [], "entities": []}, {"text": "With respect to the features extracted from text, we used three simple features types: word unigrams, bigrams, and SentiWordNet-based features, which consist of a Positive and a Negative feature extracted every time the review contains a word that is marked as such in SentiWordNet (.", "labels": [], "entities": []}, {"text": "To use SentiWordNet on ES and IT, we used Multilingual Central Repository (Gonzalez-Agirre et al., 2012) and MultiWordNet () to map sentiment labels to Spanish and to Italian, respectively.", "labels": [], "entities": []}, {"text": "Experiments were run separately on the EN, ES, and IT parts, leaving cross-lingual experiments to future work.", "labels": [], "entities": [{"text": "EN", "start_pos": 39, "end_pos": 41, "type": "DATASET", "confidence": 0.9078975915908813}]}, {"text": "On each part we built five 70%/30% train/test splits, randomly generated by sampling the reviews annotated by single reviewers (we left out reviews annotated by all the reviewers, as we consider that part of the dataset more useful as a validation set for the optimization of methods tested in future experiments).", "labels": [], "entities": []}, {"text": "We then run the five experiments and averaged their results.", "labels": [], "entities": []}, {"text": "As for the agreement evaluation (Section 3), we split the evaluation of experiments into two parts, aspect detection and sentiment labeling.", "labels": [], "entities": [{"text": "agreement evaluation", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.6883389502763748}, {"text": "aspect detection", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.6941272169351578}, {"text": "sentiment labeling", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.8932315111160278}]}, {"text": "For the sentiment labeling part we used simple Macroaveraged Mean Absolute Error (MAE M , not the symmetric version) as the true dataset labels are the reference ones in this case, while in the annotator agreement case the two sets of labels have equal importance.", "labels": [], "entities": [{"text": "sentiment labeling", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.9001559913158417}, {"text": "Macroaveraged Mean Absolute Error (MAE M", "start_pos": 47, "end_pos": 87, "type": "METRIC", "confidence": 0.8326103006090436}]}], "tableCaptions": [{"text": " Table 1: Number of reviews, sentences, and sen- tences with at least one opinion annotation.", "labels": [], "entities": []}, {"text": " Table 2: Number of opinion expressions at the sentence level of the datasets.", "labels": [], "entities": []}, {"text": " Table 3: Inter-annotator agreement. F 1 on sentence-level aspect identification (higher is better). sMAE M  on sentence-level sentiment agreement (only on matching aspects, lower is better).", "labels": [], "entities": [{"text": "F", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.9917868971824646}, {"text": "sentence-level aspect identification", "start_pos": 44, "end_pos": 80, "type": "TASK", "confidence": 0.6305983563264211}]}, {"text": " Table 4: Linear Chain CRFs experiments. F 1 on sentence-level aspect identification (higher is better).  MAE M on sentence-level sentiment assignment (only on correctly identified aspects, lower is better).", "labels": [], "entities": [{"text": "F", "start_pos": 41, "end_pos": 42, "type": "METRIC", "confidence": 0.9846823215484619}, {"text": "sentence-level aspect identification", "start_pos": 48, "end_pos": 84, "type": "TASK", "confidence": 0.6299494206905365}, {"text": "MAE M", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.947423130273819}, {"text": "sentence-level sentiment assignment", "start_pos": 115, "end_pos": 150, "type": "TASK", "confidence": 0.6434875925381979}]}]}