{"title": [{"text": "Generalized Agreement for Bidirectional Word Alignment", "labels": [], "entities": [{"text": "Generalized Agreement", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8117312490940094}, {"text": "Bidirectional Word Alignment", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.7629509965578715}]}], "abstractContent": [{"text": "While agreement-based joint training has proven to deliver state-of-the-art alignment accuracy, the produced word alignments are usually restricted to one-to-one mappings because of the hard constraint on agreement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9680201411247253}]}, {"text": "We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments.", "labels": [], "entities": []}, {"text": "The loss functions cannot only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations.", "labels": [], "entities": [{"text": "phrase segmentations", "start_pos": 136, "end_pos": 156, "type": "TASK", "confidence": 0.7157321870326996}]}, {"text": "We use a Viterbi EM algorithm to train the joint model since the inference is intractable.", "labels": [], "entities": []}, {"text": "Experiments on Chinese-English translation show that joint training with generalized agreement achieves significant improvements over two state-of-the-art alignment methods.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.6387721300125122}]}], "introductionContent": [{"text": "Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7282674908638}]}, {"text": "It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (;).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.8232397983471552}, {"text": "translation rule extraction", "start_pos": 124, "end_pos": 151, "type": "TASK", "confidence": 0.7306255102157593}]}, {"text": "Although state-of-the-art generative alignment models ( have been widely used in practical SMT systems, they fail to model the symmetry of word alignment.", "labels": [], "entities": [{"text": "generative alignment", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.9290088415145874}, {"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9932635426521301}, {"text": "word alignment", "start_pos": 139, "end_pos": 153, "type": "TASK", "confidence": 0.7275554835796356}]}, {"text": "While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly * Corresponding author: Yang Liu.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.7019210904836655}]}, {"text": "To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been proposed to combine two asymmetric alignments (source-to-target and target-to-source) to generate symmetric bidirectional alignments).", "labels": [], "entities": []}, {"text": "Instead of using heuristic symmetrization, introduce a principled approach that encourages the agreement between asymmetric alignments in two directions.", "labels": [], "entities": []}, {"text": "The basic idea is to favor links on which both unidirectional models agree.", "labels": [], "entities": []}, {"text": "They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly.", "labels": [], "entities": [{"text": "alignment", "start_pos": 109, "end_pos": 118, "type": "TASK", "confidence": 0.9205315709114075}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9112634062767029}]}, {"text": "However, enforcing agreement in joint training faces a major problem: the two models are restricted to one-to-one alignments ().", "labels": [], "entities": []}, {"text": "This significantly limits the translation accuracy, especially for distantly-related language pairs such as Chinese-English (see Section 5).", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9603421688079834}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9641438722610474}]}, {"text": "Although posterior decoding can potentially address this problem, find that many-to-many alignments occur infrequently because posteriors are sharply peaked around the Viterbi alignments.", "labels": [], "entities": []}, {"text": "We believe that this happens because their model imposes a hard constraint on agreement: the two models must share the same alignment when estimating the parameters by calculating the products of alignment posteriors (see Section 2).", "labels": [], "entities": []}, {"text": "In this work, we propose a general framework for imposing agreement constraints in joint training of unidirectional models.", "labels": [], "entities": []}, {"text": "The central idea is to use the expectation of a loss function, which measures the disagreement between two models, to replace the original probability of agreement.", "labels": [], "entities": []}, {"text": "This allows for many possible ways to quantify agreement.", "labels": [], "entities": []}, {"text": "Experiments on Chinese-English translation show that our approach outperforms two state-ofthe-art baselines significantly.: Comparison of (a) independent training without agreement, (b) joint training with agreement, and (c) joint training with generalized agreement.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.5833191275596619}]}, {"text": "Bold squares are gold-standard links and solid squares are model predictions.", "labels": [], "entities": []}, {"text": "The Chinese and English sentences are segmented into phrases in (c).", "labels": [], "entities": []}, {"text": "Joint training with agreement achieves a high precision but generally only produces one-to-one alignments.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9992145299911499}]}, {"text": "We propose generalized agreement to account for not only the consensus between asymmetric alignments, but also the conformity of alignments to other latent structures such as phrase segmentations.", "labels": [], "entities": [{"text": "phrase segmentations", "start_pos": 175, "end_pos": 195, "type": "TASK", "confidence": 0.7080391198396683}]}], "datasetContent": [{"text": "where A C\u2192E is the set of Chinese-to-English alignments on the training data and A E\u2192C is the set of English-to-Chinese alignments.", "labels": [], "entities": []}, {"text": "It is clear that independent training leads to low agreement and joint training results in high agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9727391600608826}]}, {"text": "BERKELEY achieves the highest value of agreement because of the hard constraint.", "labels": [], "entities": [{"text": "BERKELEY", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9844368696212769}]}], "tableCaptions": [{"text": " Table 1: Comparison with GIZA++ and BERKELEY. \"word-word\" denotes the agreement between  Chinese-to-English and English-to-Chinese word alignments. \"word-phrase\" denotes the agreement be- tween word alignments and phrase segmentations. \"HM\" denotes the hard matching loss function, \"SM\"  denotes soft matching, and \"SV\" denotes segmentation violation. \"GDF\" denotes grow-diag-final. \"PD\"  denotes posterior decoding.", "labels": [], "entities": [{"text": "BERKELEY", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9976160526275635}, {"text": "phrase segmentations", "start_pos": 215, "end_pos": 235, "type": "TASK", "confidence": 0.7768324613571167}]}, {"text": " Table 2: Results on (hierarchical) phrase-based translation. The evaluation metric is case-insensitive  BLEU. \"HM\" denotes the hard matching loss function, \"SM\" denotes soft matching, and \"SV\" denotes  segmentation violation. \"*\": significantly better than GIZA++ (p < 0.05). \"**\": significantly better than  GIZA++ (p < 0.01). \"+\": significantly better than BERKELEY (p < 0.05). \"++\": significantly better  than BERKELEY (p < 0.01).", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6800393462181091}, {"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9880175590515137}, {"text": "BERKELEY", "start_pos": 360, "end_pos": 368, "type": "METRIC", "confidence": 0.9956041574478149}, {"text": "BERKELEY", "start_pos": 414, "end_pos": 422, "type": "METRIC", "confidence": 0.9975516200065613}]}]}