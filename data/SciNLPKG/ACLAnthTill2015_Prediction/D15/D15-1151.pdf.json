{"title": [{"text": "An Improved Tag Dictionary for Faster Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Part-of-Speech Tagging", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7161022424697876}]}], "abstractContent": [{"text": "Ratnaparkhi (1996) introduced a method of inferring a tag dictionary from annotated data to speedup part-of-speech tagging by limiting the set of possible tags for each word.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.7118244171142578}]}, {"text": "While Ratnaparkhi's tag dictionary makes tagging faster but less accurate , an alternative tag dictionary that we recently proposed (Moore, 2014) makes tagging as fast as with Ratnaparkhi's tag dictionary, but with no decrease inaccuracy.", "labels": [], "entities": [{"text": "Ratnaparkhi's tag dictionary", "start_pos": 6, "end_pos": 34, "type": "DATASET", "confidence": 0.7925312668085098}]}, {"text": "In this paper, we show that a very simple semi-supervised variant of Ratna-parkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging-more than 100,000 tokens per second in Perl.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9989739656448364}]}, {"text": "1 Overview In this paper, we present anew method of constructing tag dictionaries for part-of-speech (POS) tagging.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.6141362667083741}]}, {"text": "A tag dictionary is simply a list of words 1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed.", "labels": [], "entities": []}, {"text": "Tag dictionaries are commonly used to speedup POS-tag inference by restricting the tags considered fora particular word to those specified by the dictionary.", "labels": [], "entities": [{"text": "POS-tag inference", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.8907684683799744}]}, {"text": "Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9171454012393951}]}, {"text": "Merialdo (1994) relied only on a tag dictionary extracted from annotated data, but he used the annotated 1 According to the conventions of the field, POS tags are assigned to all tokens in a tokenized text, including punctuation marks and other non-word tokens.", "labels": [], "entities": []}, {"text": "In this paper, all of these will be covered by the term word.", "labels": [], "entities": []}, {"text": "tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair.", "labels": [], "entities": []}, {"text": "2 Ratnaparkhi (1996) seems to have been the first to use a tag dictionary automatically extracted only from training data.", "labels": [], "entities": []}, {"text": "Ratnaparkhi's method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model.", "labels": [], "entities": [{"text": "tagging", "start_pos": 78, "end_pos": 85, "type": "TASK", "confidence": 0.9637753367424011}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9988031387329102}]}, {"text": "We recently presented (Moore, 2014) anew method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi's, but with no decrease in tagging accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.976876437664032}]}, {"text": "In this paper , we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging-more than 100,000 tokens per second even in a Perl implementation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.998694121837616}]}], "introductionContent": [], "datasetContent": [{"text": "In our experiments, we use the WSJ corpus from Penn Treebank-3, split into the standard training (sections 0-18), development (sections 19-21), and test (sections 22-24) sets for POS tagging.", "labels": [], "entities": [{"text": "WSJ corpus from Penn Treebank-3", "start_pos": 31, "end_pos": 62, "type": "DATASET", "confidence": 0.9298249840736389}, {"text": "POS tagging", "start_pos": 179, "end_pos": 190, "type": "TASK", "confidence": 0.9294995963573456}]}, {"text": "The tagging model we use has the property that all digits are treated as indistinguishable for all features.", "labels": [], "entities": []}, {"text": "We therefore also make all digits indistinguishable in constructing tag dictionaries (by internally replacing all digits by \"9\"), since it does not seem sensible to give two different dictionary entries based on containing different digits, when the tagging model assigns them the same features.", "labels": [], "entities": []}, {"text": "Tagging the WSJ development set with an unpruned semi-supervised tag dictionary obtained from the automatic tagging of the English Gigaword corpus produced the same tagging accuracy as allowing all tags for all tokens or using the pruned KN-smoothed tag dictionary used in tagging the Gigaword corpus.", "labels": [], "entities": [{"text": "WSJ development set", "start_pos": 12, "end_pos": 31, "type": "DATASET", "confidence": 0.9536681373914083}, {"text": "English Gigaword corpus", "start_pos": 123, "end_pos": 146, "type": "DATASET", "confidence": 0.8827017545700073}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9876185655593872}, {"text": "Gigaword corpus", "start_pos": 285, "end_pos": 300, "type": "DATASET", "confidence": 0.8917735517024994}]}, {"text": "Additional experiments showed that we could prune this dictionary with a threshold on p(t|w) as high as T = 0.0024 without decreasing development set accuracy.", "labels": [], "entities": [{"text": "T", "start_pos": 104, "end_pos": 105, "type": "METRIC", "confidence": 0.9921584129333496}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9058282375335693}]}, {"text": "In addition to applying this threshold to the tag probabilities for all listed words, we also applied it to the tag probabilities for unknown words, leaving 13 possible tags 5 for those.", "labels": [], "entities": []}, {"text": "Tagging the WSJ development set with these two dictionaries is compared in to tagging with our previous pruned KN-smoothed dictionary.", "labels": [], "entities": [{"text": "WSJ development set", "start_pos": 12, "end_pos": 31, "type": "DATASET", "confidence": 0.9246328274408976}]}, {"text": "The second column shows the accuracy per tag, which is 97.31% for all three dictionaries.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9997121691703796}]}, {"text": "The third column shows the mean number of tags per token allowed by each dictionary.", "labels": [], "entities": []}, {"text": "The fourth column shows the percentage of tokens with only one tag allowed, which is significant since the tagger need not apply the model for such tokens-it can simply output the single possible tag.", "labels": [], "entities": []}, {"text": "The last column shows the tagging speed in tokens per second for each of the three tag dictionaries, using the fast tagging method we previously described, in a singlethreaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors.", "labels": [], "entities": []}, {"text": "Speed is rounded to the nearest 1,000 tokens per second, because we measured times to a precision of only about one part in one hundred.", "labels": [], "entities": [{"text": "Speed", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9872877597808838}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9828394055366516}]}, {"text": "For the pruned KN-smoothed dictionary, we previously reported a speed of 49,000 tokens per second under similar conditions.", "labels": [], "entities": [{"text": "KN-smoothed dictionary", "start_pos": 15, "end_pos": 37, "type": "DATASET", "confidence": 0.7795324623584747}]}, {"text": "Our current faster speed of 69,000 tokens per second is due to an improved low-level implementation for computing the model scores for permitted tags, and a slightly faster version of Perl (v5.18.2).", "labels": [], "entities": []}, {"text": "The most restrictive tag dictionary, the pruned semi-supervised dictionary, allows only 1.51 tags per token, and our implementation runs at 103,000 tokens per second on the WSJ development set.", "labels": [], "entities": [{"text": "WSJ development set", "start_pos": 173, "end_pos": 192, "type": "DATASET", "confidence": 0.9598170320192972}]}, {"text": "For our final experiments, we tested our tagger with this dictionary on the standard Penn Treebank WSJ test set and on the Penn Treebank-3 parsed Brown corpus subset, as an out-of-domain evaluation.", "labels": [], "entities": [{"text": "Penn Treebank WSJ test set", "start_pos": 85, "end_pos": 111, "type": "DATASET", "confidence": 0.9852090954780579}, {"text": "Penn Treebank-3 parsed Brown corpus subset", "start_pos": 123, "end_pos": 165, "type": "DATASET", "confidence": 0.9792657295862833}]}, {"text": "For comparison, we tested our previous tagger and the fast version (english-left3words-distsim) of the Stanford tagger () recommended for practical use on the Stanford tagger website, which we found to be by far the fastest of the six publicly available taggers tested in our previous work.", "labels": [], "entities": [{"text": "Stanford tagger website", "start_pos": 159, "end_pos": 182, "type": "DATASET", "confidence": 0.7907435894012451}]}, {"text": "The results of these tests are shown in: WSJ test set and Brown corpus tagging speeds and token accuracies For our previous tagger, we give three speeds: the speed we reported earlier, a speed fora duplicate of the earlier experiment using the faster version of Perl that we use here, and a third measurement including both the faster version of Perl and our improved low-level tagger implementation.", "labels": [], "entities": [{"text": "WSJ test set", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.9460501869519552}, {"text": "Brown corpus tagging", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.7946908473968506}]}, {"text": "With the pruned semi-supervised dictionary, our new tagger has slightly higher all-token accuracy than our previous tagger on both the WSJ test set and Brown corpus set, and it is much more accurate than the fast Stanford tagger.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9447544813156128}, {"text": "WSJ test set", "start_pos": 135, "end_pos": 147, "type": "DATASET", "confidence": 0.9850310881932577}, {"text": "Brown corpus set", "start_pos": 152, "end_pos": 168, "type": "DATASET", "confidence": 0.9264350533485413}, {"text": "accurate", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9839428663253784}]}, {"text": "The accuracy on the standard WSJ test set is 97.36%, one of the highest ever reported.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9998239874839783}, {"text": "WSJ test set", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.9196784694989523}]}, {"text": "The new tagger is also much faster than either of the other taggers, achieving a speed of more than 100,000 tokens per second on the WSJ test set, and almost 100,000 tokens per second on the out-of-domain Brown corpus data.", "labels": [], "entities": [{"text": "WSJ test set", "start_pos": 133, "end_pos": 145, "type": "DATASET", "confidence": 0.9819689591725668}, {"text": "Brown corpus data", "start_pos": 205, "end_pos": 222, "type": "DATASET", "confidence": 0.9678221344947815}]}], "tableCaptions": [{"text": " Table 1: WSJ development set token accuracy and tagging speed for different tag dictionaries", "labels": [], "entities": [{"text": "WSJ", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7305837869644165}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9892388582229614}]}, {"text": " Table 2: WSJ test set and Brown corpus tagging speeds and token accuracies", "labels": [], "entities": [{"text": "WSJ test set", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.8375495870908102}, {"text": "Brown corpus tagging", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.5812445282936096}]}]}