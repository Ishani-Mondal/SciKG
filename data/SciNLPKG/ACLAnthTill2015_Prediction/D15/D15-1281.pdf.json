{"title": [{"text": "Verbal and Nonverbal Clues for Real-life Deception Detection", "labels": [], "entities": [{"text": "Real-life Deception Detection", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.6487562358379364}]}], "abstractContent": [{"text": "Deception detection has been receiving an increasing amount of attention from the computational linguistics, speech, and multimodal processing communities.", "labels": [], "entities": [{"text": "Deception detection", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9306621849536896}]}, {"text": "One of the major challenges encountered in this task is the availability of data, and most of the research work to date has been conducted on acted or artificially collected data.", "labels": [], "entities": []}, {"text": "The generated deception models are thus lacking real-world evidence.", "labels": [], "entities": []}, {"text": "In this paper, we explore the use of multi-modal real-life data for the task of deception detection.", "labels": [], "entities": [{"text": "deception detection", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.9609057605266571}]}, {"text": "We develop anew deception dataset consisting of videos from real-life scenarios, and build deception tools relying on verbal and nonverbal features.", "labels": [], "entities": []}, {"text": "We achieve classification accuracies in the range of 77-82% when using a model that extracts and fuses features from the linguistic and visual modalities.", "labels": [], "entities": []}, {"text": "We show that these results outperform the human capability of identifying deceit.", "labels": [], "entities": []}], "introductionContent": [{"text": "As deceptive behavior occurs on a daily basis in different areas of life), the need arises for automated methodologies to detect deception in an efficient, yet reliable manner.", "labels": [], "entities": []}, {"text": "There are many applications that can benefit from automatic deception identification, such as airport security screening, crime investigation and interrogation, interviews, advertisement, and others.", "labels": [], "entities": [{"text": "automatic deception identification", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.6688203910986582}, {"text": "airport security screening", "start_pos": 94, "end_pos": 120, "type": "TASK", "confidence": 0.6029651363690695}, {"text": "crime investigation and interrogation", "start_pos": 122, "end_pos": 159, "type": "TASK", "confidence": 0.8020166605710983}]}, {"text": "In many of these settings, the polygraph test has been used as the main method to identify deceptive behavior.", "labels": [], "entities": []}, {"text": "However, this method requires the use of skin-contact devices and human expertise, making it infeasible for large-scale applications.", "labels": [], "entities": []}, {"text": "Moreover, polygraph tests were shown to be misleading in multiple cases, as human judgment is often biased.", "labels": [], "entities": []}, {"text": "Given the difficulties associated with the use of polygraph-like methods, learning-based approaches have been proposed to address the deception detection task using a number of modalities, including text) and speech (.", "labels": [], "entities": [{"text": "deception detection task", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.882919192314148}]}, {"text": "Unlike the polygraph methods, learning-based methods for deception detection rely mainly on data collected from deceivers and truth-tellers.", "labels": [], "entities": [{"text": "deception detection", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.941262811422348}]}, {"text": "The data is usually elicited from human contributors, in a lab setting or via crowdsourcing.", "labels": [], "entities": []}, {"text": "An important problem identified in this data-driven research is the lack of real data.", "labels": [], "entities": []}, {"text": "Because of the artificial setting, the subjects may not be emotionally aroused, as they may not take the experiments seriously given the lack of motivation and/or penalty.", "labels": [], "entities": []}, {"text": "In this paper, we describe what we believe is a first attempt at building a multimodal system that detects deception in real-life settings.", "labels": [], "entities": []}, {"text": "We collect a dataset consisting of 118 deceptive and truthful video clips, from real trials and live street interviews aired in television shows.", "labels": [], "entities": []}, {"text": "We use the transcription of these videos to extract several linguistic features, and we manually annotate the videos for the presence of several gestures that are used to extract nonverbal features.", "labels": [], "entities": []}, {"text": "We then build a system that jointly uses the verbal and nonverbal modalities to automatically detect the presence of deception.", "labels": [], "entities": []}, {"text": "Our experiments show that the multimodal system can identify deception with an accuracy in the range of 77-82%, significantly improving over the baseline.", "labels": [], "entities": [{"text": "identify deception", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7721602320671082}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.99956876039505}]}, {"text": "In addition, we present a study on the human ability to detect deception in single or multimodal data streams, and show that our system outperforms humans on this task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our goal is to build a multimodal collection of occurrences of real deception, which will allow us to analyze both verbal and nonverbal behaviors in relation to deception.", "labels": [], "entities": []}, {"text": "Starting at the top left-hand corner: deceptive interview with up gaze (Up), deceptive interview with side gaze (Side), deceptive trial with both hands (Both-H), truthful trial with forward head (Forward), truthful interview with side turn (Side-Turn), and truthful interview with single hand (Single-H).", "labels": [], "entities": []}, {"text": "We start our experiments with an analysis of the nonverbal behaviors occurring in deceptive and truthful videos.", "labels": [], "entities": []}, {"text": "We compare the percentage of each behavior as observed in each class.", "labels": [], "entities": []}, {"text": "For instance, there is a total of 41 videos in the dataset  that include the Smile feature (as shown in Table 3), out of which 12 are part of the deceptive set of 59 videos, and 29 are part of the truthful set (again, of 59 videos).", "labels": [], "entities": []}, {"text": "Hence, the percentages for this feature are 20.33% in the deceptive class, and 49.13% in the truthful class.", "labels": [], "entities": []}, {"text": "shows the percentages of all the nonverbal features for which we observe noticeable differences for the deceptive and truthful groups.", "labels": [], "entities": []}, {"text": "As the figure suggests, facial displays seem to help differentiate between the deceptive and truthful conditions.", "labels": [], "entities": []}, {"text": "For instance, we can observe that truth-tellers smile (Smile) and blink more (Close-R).", "labels": [], "entities": []}, {"text": "Interestingly deceivers seem to make more eye contact (Interlocutor gaze) and nod (Side-Turn-R) more frequently than truth-tellers.", "labels": [], "entities": [{"text": "Side-Turn-R)", "start_pos": 83, "end_pos": 95, "type": "METRIC", "confidence": 0.8780601322650909}]}, {"text": "This agrees with the findings in () that liars who are more motivated to getaway with their lies (i.e., trials) are likely to increase their eye-contact behavior.", "labels": [], "entities": []}, {"text": "Motivated by these results, we proceed to conduct further experiments to evaluate the performance of the extracted features using a machine learning approach.: Feature ablation study.", "labels": [], "entities": []}, {"text": "We run our learning experiments on the realdeception dataset introduced earlier.", "labels": [], "entities": []}, {"text": "Given the even distribution between deceptive and truthful clips, the baseline on this dataset is 50%.", "labels": [], "entities": []}, {"text": "For each video clip, we create feature vectors formed by combinations of the verbal and nonverbal features described in the previous section.", "labels": [], "entities": []}, {"text": "We build deception classifiers using three classification algorithms: Support Vector Machines (SVM), Decision Trees (DT), and Random Forest (RF).", "labels": [], "entities": []}, {"text": "We run several comparative experiments using leaveone-out cross-validation.", "labels": [], "entities": []}, {"text": "shows the accuracy figures obtained by the three classifiers on the major feature groups described in Section 3.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999666690826416}]}, {"text": "As shown in this table, the facial displays classifier achieves the highest accuracy among the individual classifiers, followed by the unigrams classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9990547299385071}]}, {"text": "We also evaluate classifiers that rely on combined sets of features.", "labels": [], "entities": []}, {"text": "The nonverbal features clearly outperform the verbal features, and the classifier that includes all the features improves over the classifiers that rely on all the verbal features or all the nonverbal features.", "labels": [], "entities": []}, {"text": "Importantly, several of the classifiers improve significantly over the baseline.", "labels": [], "entities": []}, {"text": "We perform three sets of experiments to determine the role played by the domain.", "labels": [], "entities": []}, {"text": "The first set of experiments uses only the Interviews video clips (62 in total), and the results are shown in the left column of.", "labels": [], "entities": []}, {"text": "The second set uses only the Trials instances (56 in total), with results shown in the right column of: Cross-domain classification results using a SVM classifier trained on all the features nificantly when there is no overlap in domain between the training and the test instances.", "labels": [], "entities": [{"text": "Cross-domain classification", "start_pos": 104, "end_pos": 131, "type": "TASK", "confidence": 0.8217740654945374}]}, {"text": "Overall, in all our machine learning experiments, the combined classifier that makes use of all the verbal and nonverbal features achieves the best trade-off between performance and robustness, as it always leads to the best or second best performance across all the experiments using individual or combined feature sets.", "labels": [], "entities": []}, {"text": "While a classifier based on an individual feature set can sometime lead to a better performance (e.g., the Facial Displays classifier has better performance when all the video clips are used), that same classifier may not perform well in another setting (e.g., the Facial Displays classifier is significantly below the All Features classifier in the domain experiments).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Frequency counts for nine facial displays and hand gestures", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9085755348205566}]}, {"text": " Table 4: Deception classifiers using individual and  combined sets of verbal and nonverbal features.", "labels": [], "entities": [{"text": "Deception classifiers", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8856288492679596}]}, {"text": " Table 5: Feature ablation study.", "labels": [], "entities": []}, {"text": " Table 6: LIWC word classes most strongly associ- ated with deception and truth.", "labels": [], "entities": []}, {"text": " Table 7. The second set uses only the Tri- als instances (56 in total), with results shown in the  right column of", "labels": [], "entities": []}, {"text": " Table 7. Finally, we also perform  cross-domain experiments, with the training data  drawn from one domain and the test data from the  other. The results of these experiments are shown  in Table 8. Given the uneven distribution of the  truthful and deceptive video clips in two domains,  the baselines are 54.83% for the Interviews do- main (34 truthful, 28 deceptive), and 55.35% for  the Trials domain (25 truthful, 31 deceptive).  What we learn from these experiments is that  the domain does matter. Despite the smaller  dataset, the experiments run on one domain at a  time lead to results that are higher than the ones  obtained with more data but with a mix of do- mains. The cross-domain experiments also sup- port this argument, as the performance drops sig-", "labels": [], "entities": []}, {"text": " Table 9: Agreement among three human annota- tors on text, audio, silent video, and full video  modalities.", "labels": [], "entities": []}, {"text": " Table 10: Performance of three annotators and  the developed automatic system (Sys) on the real- deception dataset over four modalities.", "labels": [], "entities": []}]}