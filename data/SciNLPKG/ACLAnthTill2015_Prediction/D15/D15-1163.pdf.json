{"title": [{"text": "Improving Statistical Machine Translation with a Multilingual Paraphrase Database", "labels": [], "entities": [{"text": "Improving Statistical Machine Translation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8861963450908661}]}], "abstractContent": [{"text": "The multilingual Paraphrase Database (PPDB) is a freely available automatically created resource of paraphrases in multiple languages.", "labels": [], "entities": []}, {"text": "In statistical machine translation, paraphrases can be used to provide translation for out-of-vocabulary (OOV) phrases.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.659028430779775}]}, {"text": "In this paper, we show that a graph propagation approach that uses PPDB paraphrases can be used to improve overall translation quality.", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7678223252296448}]}, {"text": "We provide an extensive comparison with previous work and show that our PPDB-based method improves the BLEU score by up to 1.79 percent points.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.9822359383106232}]}, {"text": "We show that our approach improves on the state of the art in three different settings: when faced with limited amount of parallel training data; a domain shift between training and test data; and handling a morphologically complex source language.", "labels": [], "entities": []}, {"text": "Our PPDB-based method outperforms the use of distributional profiles from monolin-gual source data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Translation coverage is a major concern in statistical machine translation (SMT) which relies on large amounts of parallel, sentence-aligned text.", "labels": [], "entities": [{"text": "Translation coverage", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9736789464950562}, {"text": "statistical machine translation (SMT)", "start_pos": 43, "end_pos": 80, "type": "TASK", "confidence": 0.7914394537607828}]}, {"text": "In), even with a training data size of 10 million word tokens, source vocabulary coverage in unseen data does not go above 90%.", "labels": [], "entities": []}, {"text": "The problem is worse with multi-word OOV phrases.", "labels": [], "entities": []}, {"text": "Copying OOVs to the output is the most common solution.", "labels": [], "entities": []}, {"text": "However, even noisy translations of OOVs can improve reordering and language model scores (.", "labels": [], "entities": []}, {"text": "Transliteration is useful but not a panacea for the OOV problem).", "labels": [], "entities": [{"text": "OOV problem", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.7772765755653381}]}, {"text": "We find and remove the named entities, dates, etc.", "labels": [], "entities": []}, {"text": "in the source and focus on the use of paraphrases to help translate the remaining OOVs.", "labels": [], "entities": []}, {"text": "5.2 we show that handling such OOVs correctly does improve translation scores.", "labels": [], "entities": [{"text": "translation", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.8867826461791992}]}, {"text": "In this paper, we build on the following research: Bilingual lexicon induction is the task of learning translations of words from monolingual data in source and target languages ().", "labels": [], "entities": [{"text": "Bilingual lexicon induction", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.9332093000411987}, {"text": "learning translations of words from monolingual data in source and target languages", "start_pos": 94, "end_pos": 177, "type": "TASK", "confidence": 0.8096136351426443}]}, {"text": "The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates).", "labels": [], "entities": []}, {"text": "DPs have been used in SMT to assign translation candidates to).", "labels": [], "entities": [{"text": "SMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9953456521034241}]}, {"text": "Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (.", "labels": [], "entities": []}, {"text": "extend paraphrases for SMT from the words to phrases, which we also do in this work.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9912247061729431}]}, {"text": "Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction.", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.7942778468132019}]}, {"text": "published a large-scale multilingual Paraphrase Database (PPDB) http://paraphrase.", "labels": [], "entities": []}, {"text": "org which includes lexical, phrasal, and syntactic paraphrases (available for 22 languages with up to 170 million paraphrases each).", "labels": [], "entities": []}, {"text": "To our knowledge, this paper is the first comprehensive study of the use of PPDB for statistical machine translation model training.", "labels": [], "entities": [{"text": "statistical machine translation model training", "start_pos": 85, "end_pos": 131, "type": "TASK", "confidence": 0.8014158546924591}]}, {"text": "Our framework has three stages: 1) a novel graph construction approach for PPDB paraphrases linked with phrases from parallel training data.", "labels": [], "entities": [{"text": "PPDB paraphrases linked with phrases from parallel training", "start_pos": 75, "end_pos": 134, "type": "TASK", "confidence": 0.8817261829972267}]}, {"text": "2) Graph propagation that uses PPDB paraphrases.", "labels": [], "entities": [{"text": "Graph propagation", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.9471886456012726}]}, {"text": "3) An SMT model that incorporates new translation candidates.", "labels": [], "entities": [{"text": "SMT", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.994690477848053}]}, {"text": "3 explains these three stages in detail.", "labels": [], "entities": []}, {"text": "Using PPDB has several advantages: 1) Resources such as PPDB can be built and used for many different tasks including but not limited to SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.9834439754486084}]}, {"text": "2) PPDB contains many features that are useful to rank the strength of a paraphrase connection and with more information than distributional profiles.", "labels": [], "entities": []}, {"text": "3) Paraphrases in PPDB are often better than paraphrases extracted from monolingual or comparable corpora because a large-scale multilingual paraphrase database such as PPDB can pivot through a large amount of data in many different languages.", "labels": [], "entities": []}, {"text": "It is not limited to using the source language data for finding paraphrases which distinguishes it from previous uses of paraphrases for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.978770911693573}]}, {"text": "PPDB is a natural resource for paraphrases.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9011114835739136}]}, {"text": "However, PPDB was not built with the specific application to SMT in mind.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.7462103962898254}, {"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9826571345329285}]}, {"text": "Other applications such as text-to-text generation have used PPDB (Ganitkevitch et al., 2011) but SMT brings along a specific set of concerns when using paraphrases: translation candidates should be transferred suitably across paraphrases.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.7257612198591232}, {"text": "SMT", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9867858290672302}]}, {"text": "There are many cases, e.g. when faced with different word senses where transfer of a translation is not appropriate.", "labels": [], "entities": []}, {"text": "Our proposed methods of using PPDB use graph propagation to transfer translation candidates in away that is sensitive to SMT concerns.", "labels": [], "entities": [{"text": "SMT", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.9844813346862793}]}, {"text": "In our experiments (Sec. 5) we compare our approach with the state-of-the-art in three different settings in SMT: 1) when faced with limited amount of parallel training data; 2) a domain shift between training and test data; and 3) handling a morphologically complex source language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.9855248332023621}]}, {"text": "In each case, we show that our PPDB-based approach outperforms the distributional profile approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first show the effect of OOVs on translation quality, then evaluate our approach in three different SMT settings: low resource SMT, domain shift, and morphologically complex languages.", "labels": [], "entities": [{"text": "SMT", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9817100167274475}, {"text": "SMT", "start_pos": 130, "end_pos": 133, "type": "TASK", "confidence": 0.8298290967941284}]}, {"text": "In each case, we compare results of using paraphrases extracted by Distributional Profile (DP) and PPDB in an end-to-end SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 121, "end_pos": 124, "type": "TASK", "confidence": 0.9679033160209656}]}, {"text": "Important: no subset of the test data sentences are used in the bilingual corpora for paraphrase extraction process.", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.8620190918445587}]}, {"text": "We use CDEC 1 as an endto-end SMT pipeline with its standard features 2 . fast align) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA.", "labels": [], "entities": [{"text": "SMT pipeline", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.8816688060760498}, {"text": "word alignment", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.792207658290863}, {"text": "BLEU", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.998099148273468}, {"text": "MIRA", "start_pos": 181, "end_pos": 185, "type": "METRIC", "confidence": 0.8255955576896667}]}, {"text": "This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7063886672258377}]}, {"text": "5.5: Statistics of settings in Sec.", "labels": [], "entities": []}, {"text": "5. Last column shows how many rules added in the phrase table integration step.", "labels": [], "entities": [{"text": "phrase table integration", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.6686440308888754}]}, {"text": "KenLM) is used to train a 5-gram language model on English Gigaword (V5: LDC2011T07).", "labels": [], "entities": []}, {"text": "For scalable graph propagation we use the Junto framework 3 . We use maximum phrase length 10.", "labels": [], "entities": [{"text": "scalable graph propagation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.7709273298581442}]}, {"text": "For our experiments we use the Hadoop distributed computing framework executed on a cluster with 12 nodes (each node has 8 cores and 16GB of RAM).", "labels": [], "entities": []}, {"text": "Each graph propagation iteration takes about 3 minutes.", "labels": [], "entities": [{"text": "graph propagation iteration", "start_pos": 5, "end_pos": 32, "type": "TASK", "confidence": 0.7976483702659607}]}, {"text": "For French, we apply a simple heuristic to detect named entities: words that are capitalized in the original dev/test set that do not appear at the beginning of a sentence are named entities.", "labels": [], "entities": []}, {"text": "Based on eyeballing the results, this works very well in our data.", "labels": [], "entities": []}, {"text": "For Arabic, AQMAR is used to exclude named-entities ().", "labels": [], "entities": [{"text": "AQMAR", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.9691157937049866}]}, {"text": "For each of the experimental settings below we show the OOV statistics in.", "labels": [], "entities": [{"text": "OOV", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9906623959541321}]}, {"text": "This oracle experiment shows that translation of OOVs beyond named entities, dates, etc. is potentially very useful in improving output translation.", "labels": [], "entities": []}, {"text": "We trained a SMT system on 10K French-English sentences from the Europarl corpus(v7).", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9910888671875}, {"text": "Europarl corpus(v7)", "start_pos": 65, "end_pos": 84, "type": "DATASET", "confidence": 0.9573036074638367}]}, {"text": "WMT 2011 and WMT 2012 are used as dev and test data respectively.", "labels": [], "entities": [{"text": "WMT 2011", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9643986821174622}, {"text": "WMT 2012", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.9044308364391327}]}, {"text": "shows the results in terms of BLEU on dev and test.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9991195797920227}]}, {"text": "The first row is baseline which simply copies OOVs to output.", "labels": [], "entities": []}, {"text": "The second and third rows show the result of augmenting phrase-table by adding translations for single-word OOVs and phrases containing OOVs.", "labels": [], "entities": []}, {"text": "The last row shows the oracle result where dev and test sentences exist inside the training data and all the OOVs are known (Fully observers cannot avoid model and search errors).", "labels": [], "entities": [{"text": "OOVs", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9801986813545227}]}], "tableCaptions": [{"text": " Table 1: Statistics of the graph constructed using  the English lexical PPDB. We have built similar  graphs for French and Arabic.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of settings in Sec. 5. Last col- umn shows how many rules added in the phrase  table integration step.", "labels": [], "entities": [{"text": "phrase  table integration", "start_pos": 92, "end_pos": 117, "type": "TASK", "confidence": 0.6033399800459543}]}, {"text": " Table 4: The impact of translating OOVs.", "labels": [], "entities": [{"text": "translating OOVs", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.8235762417316437}]}, {"text": " Table 3: Examples comparing DP versus PPDB outputs on the test sets. NNs refer to nearest neighbours  in the graph for OOV phrase. Each row respectively corresponds to experimental settings (cases 1 to 3).", "labels": [], "entities": []}, {"text": " Table 5: Results of PPDB and DP techniques.", "labels": [], "entities": []}, {"text": " Table 6: BLEU scores for domain adaptation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9978154897689819}, {"text": "domain adaptation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.8046010434627533}]}, {"text": " Table 7: BLEU score results for Arabic-English.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9652315080165863}]}]}