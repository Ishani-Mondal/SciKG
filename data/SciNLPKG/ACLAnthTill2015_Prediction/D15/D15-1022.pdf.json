{"title": [{"text": "Combining Geometric, Textual and Visual Features for Predicting Prepositions in Image Descriptions", "labels": [], "entities": [{"text": "Predicting Prepositions in Image Descriptions", "start_pos": 53, "end_pos": 98, "type": "TASK", "confidence": 0.807491683959961}]}], "abstractContent": [{"text": "We investigate the role that geometric, tex-tual and visual features play in the task of predicting a preposition that links two visual entities depicted in an image.", "labels": [], "entities": [{"text": "predicting a preposition", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.8471202452977499}]}, {"text": "The task is an important part of the subsequent process of generating image descriptions.", "labels": [], "entities": [{"text": "generating image descriptions", "start_pos": 59, "end_pos": 88, "type": "TASK", "confidence": 0.6775122682253519}]}, {"text": "We explore the prediction of prepositions fora pair of entities, both in the case when the labels of such entities are known and unknown.", "labels": [], "entities": []}, {"text": "In all situations we found clear evidence that all three features contribute to the prediction task.", "labels": [], "entities": [{"text": "prediction task", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.901544839143753}]}], "introductionContent": [{"text": "In recent years, there has been an increased interest in the task of automatic generation of natural language image descriptions at sentence level, compared to earlier work that annotates images with a laundry list of terms ().", "labels": [], "entities": [{"text": "automatic generation of natural language image descriptions", "start_pos": 69, "end_pos": 128, "type": "TASK", "confidence": 0.8252971427781242}]}, {"text": "The task is important in that such detailed annotations are more informative and discriminative compared to isolated textual labels, and are essential for improved text and image retrieval.", "labels": [], "entities": [{"text": "text and image retrieval", "start_pos": 164, "end_pos": 188, "type": "TASK", "confidence": 0.6110478937625885}]}, {"text": "The most standard approach to generating such descriptions involves first detecting instances of pre-defined concepts in the image, and then reasoning about these concepts to generate image descriptions e.g. ().", "labels": [], "entities": []}, {"text": "Our work is also based on this paradigm.", "labels": [], "entities": []}, {"text": "However, we assume that object instances have already been pre-detected by visual recognisers, and concentrate on a specific subtask of description generation.", "labels": [], "entities": [{"text": "description generation", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.7127252072095871}]}, {"text": "More specifically, given two visual entity instances where one could potentially act as a modifier to the other, we address the problem of identifying the appropriate preposition to connect these two entities).", "labels": [], "entities": []}, {"text": "The inferred prepositional relations will subsequently act as an *A.", "labels": [], "entities": []}, {"text": "Ramisa and J. Wang contributed equally to this work.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is therefore to learn to predict the most suitable preposition given its context, and to learn this jointly from images and their descriptions.", "labels": [], "entities": []}, {"text": "In particular, we concentrate on learning from (i) geometric relations between two visual entities from image annotations; (ii) textual features from textual descriptions; (iii) visual features from images.", "labels": [], "entities": []}, {"text": "Previous work exists) that uses text corpora to 'guess' the prepositions given the context without considering the appropriate spatial relations between the entities in the image, signifying a gap between visual content and its corresponding description.", "labels": [], "entities": []}, {"text": "For example, although person on horse might commonly occur in text corpora, a particular image might actually depict a person standing beside a horse.", "labels": [], "entities": []}, {"text": "On the other hand, work that does consider the image content for generating prepositions map geometric relations to a limited set of prepositions using manually defined rules, not as humans would naturally use them with a richer vocabulary.", "labels": [], "entities": []}, {"text": "We would like to have the best of both worlds, by considering image content as well as textual information to select the preposition best used to express the relation between two entities.", "labels": [], "entities": []}, {"text": "Our hypothesis is that the combination of geometric, textual and visual features can help with the task of predicting the most appropriate preposition, since incorporating geometric and visual information should help generate a relation that is consistent with the image content, whilst incorporating textual information should help generate a description that is consistent with natural language.", "labels": [], "entities": []}], "datasetContent": [{"text": "We base the preposition prediction task on two large-scale image datasets with human authored descriptions, namely MSCOCO () and Flickr30k (.", "labels": [], "entities": [{"text": "preposition prediction", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.9401606917381287}, {"text": "Flickr30k", "start_pos": 129, "end_pos": 138, "type": "DATASET", "confidence": 0.7355177998542786}]}, {"text": "To extract instances of triples (trajector, preposition, landmark) from image descriptions, we used the Neural Network, transition-based dependency parser of as implemented in Stanford CoreNLP ( ).", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 176, "end_pos": 192, "type": "DATASET", "confidence": 0.8269496262073517}]}, {"text": "Dependencies signifying prepositional Bounding Box feature (number of dimensions) \u2022 Vector (x, y) from centroid of trajector to centroid of landmark, normalised by the size of the bounding box enclosing both objects (2) \u2022 Area of trajector bounding box relative to landmark (1) \u2022 Aspect ratio of each bounding box (2) \u2022 Area of each bounding box w.r.t. enclosing box \u2022 Intersection over union of the bounding boxes \u2022 Euclidean distance between the trajector and landmark bounding boxes, normalised by the image size (1) \u2022 Area of each bounding box w.r.t. the whole image (2) We consider two variants of trajector and landmark terms in our experiments: (i) using the provided high level categories as terms (80 for MSCOCO and 8 for Flickr30k); (ii) using the terms occurring in the sentence directly, which constitute a bigger and more realistic challenge.", "labels": [], "entities": [{"text": "MSCOCO", "start_pos": 714, "end_pos": 720, "type": "DATASET", "confidence": 0.9152196645736694}, {"text": "Flickr30k", "start_pos": 731, "end_pos": 740, "type": "DATASET", "confidence": 0.8991950750350952}]}, {"text": "For Flickr30k, the descriptive phrases may cause data sparseness (the furry, black and white dog).", "labels": [], "entities": [{"text": "Flickr30k", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.9477041363716125}]}, {"text": "Thus, we extracted the lemmatised headword of each phrase, using a 'semantic head' variant of the head finding rules of in Stanford CoreNLP.", "labels": [], "entities": [{"text": "head finding", "start_pos": 98, "end_pos": 110, "type": "TASK", "confidence": 0.7531838417053223}, {"text": "Stanford CoreNLP", "start_pos": 123, "end_pos": 139, "type": "DATASET", "confidence": 0.8884162902832031}]}, {"text": "Entities from the same coreference chain are denoted with a common head noun chosen by majority vote among the group, with ties broken by the most frequent head noun in the corpus, and further ties broken at random.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Top: Mean rank of the correct preposition (lower is better). Bottom: Accuracy with different  feature configurations. All results are with the original trajector/landmark terms from descriptions. IND  stands for Indicator Vectors, W2V for Word2Vec, and GF for Geometric Features. As baseline we rank  the prepositions by their relative frequencies in the training dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9888018369674683}, {"text": "IND", "start_pos": 206, "end_pos": 209, "type": "METRIC", "confidence": 0.8905924558639526}]}, {"text": " Table 3: Accuracy (acc) and mean rank (rank, with max rank in parenthesis) for each variable of the  CRF model, trained using the high-level concept labels. Columns under Prep (known labels) refer to  the results of predicting prepositions with the trajector and landmark labels fixed to the correct values.", "labels": [], "entities": [{"text": "Accuracy (acc)", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8702008277177811}, {"text": "Prep", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.8799148797988892}]}]}