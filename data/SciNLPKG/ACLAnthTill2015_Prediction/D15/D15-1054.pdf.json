{"title": [{"text": "Joint Embedding of Query and Ad by Leveraging Implicit Feedback", "labels": [], "entities": []}], "abstractContent": [{"text": "Sponsored search is at the center of a multibil-lion dollar market established by search technology.", "labels": [], "entities": []}, {"text": "Accurate ad click prediction is a key component for this market to function since the pricing mechanism heavily relies on the estimation of click probabilities.", "labels": [], "entities": [{"text": "ad click prediction", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.63331338763237}]}, {"text": "Lexical features derived from the text of both the query and ads play a significant role, complementing features based on historical click information.", "labels": [], "entities": []}, {"text": "The purpose of this paper is to explore the use of word embedding techniques to generate effective text features that can capture not only lexical similarity between query and ads but also the latent user intents.", "labels": [], "entities": []}, {"text": "We identify several potential weaknesses of the plain application of conventional word embedding methodolo-gies for ad click prediction.", "labels": [], "entities": [{"text": "ad click prediction", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.6731600066026052}]}, {"text": "These observations motivated us to propose a set of novel joint word embedding methods by leveraging implicit click feedback.", "labels": [], "entities": []}, {"text": "We verify the effectiveness of these new word embedding models by adding features derived from the new models to the click prediction system of a commercial search engine.", "labels": [], "entities": [{"text": "click prediction", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.6446924954652786}]}, {"text": "Our evaluation results clearly demonstrate the effectiveness of the proposed methods.", "labels": [], "entities": []}, {"text": "To the best of our knowledge this work is the first successful application of word embedding techniques for the task of click prediction in sponsored search.", "labels": [], "entities": [{"text": "click prediction in sponsored search", "start_pos": 120, "end_pos": 156, "type": "TASK", "confidence": 0.6540313124656677}]}], "introductionContent": [{"text": "Sponsored search is a multibillion dollar market) that makes most search engine revenue and is one of the most successful ways for advertisers to reach their intended audiences.", "labels": [], "entities": [{"text": "Sponsored search", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7998223900794983}]}, {"text": "When search engines deliver results to a user, sponsored advertisement impressions (ads) are shown alongside the organic search results).", "labels": [], "entities": []}, {"text": "Typically the advertiser pays the search engine based on the pay-per-click model.", "labels": [], "entities": []}, {"text": "In this model the advertiser pays only if the impression that accompanies the search results is clicked.", "labels": [], "entities": []}, {"text": "The price is usually set by a generalized second-price (GSP) auction) that encourages advertisers to bid truthfully.", "labels": [], "entities": []}, {"text": "An advertiser wins if the expected revenue for this advertiser, which is the bid: Sponsored ads when \"pizza\" was searched at Yahoo!", "labels": [], "entities": []}, {"text": "price times the expected click probability (also know as click through rate, or CTR), is ranked the highest.", "labels": [], "entities": [{"text": "click through rate, or CTR)", "start_pos": 57, "end_pos": 84, "type": "METRIC", "confidence": 0.8062584400177002}]}, {"text": "The price the advertiser pays, known as cost-per-click (CPC), is the bid price for the second ranked advertiser times the ratio of the expected CTR between the second and first ranked advertisers.", "labels": [], "entities": [{"text": "cost-per-click (CPC)", "start_pos": 40, "end_pos": 60, "type": "METRIC", "confidence": 0.6852241307497025}, {"text": "CTR", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.9091553092002869}]}, {"text": "From this discussion it should be clear that CTR plays a key role in deciding both the ranking and the pricing of the ads.", "labels": [], "entities": []}, {"text": "Therefore it is very important to predict CTR accurately.", "labels": [], "entities": [{"text": "CTR", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.6997928619384766}]}, {"text": "The state of the art search engine typically uses a machine learning model to predict CTR by exploiting various features that have been found useful in practice.", "labels": [], "entities": [{"text": "predict CTR", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.5091859251260757}]}, {"text": "These include historical click performance features such as historical click probability for the query, the ad, the user, and a combination of these; contextual features such as temporal and geographical information; and text-based features such as query keywords or ad title and description.", "labels": [], "entities": [{"text": "historical click probability", "start_pos": 60, "end_pos": 88, "type": "METRIC", "confidence": 0.7550768653551737}]}, {"text": "Among these, historical click performance features often have the most predictive power for queries, ads and users that have registered many impressions.", "labels": [], "entities": []}, {"text": "For queries, ads and users that have not registered many impressions, however, historical CTR may have too high a variance to be useful.", "labels": [], "entities": []}, {"text": "observed that the number of impressions and clicks recorded on query-ad pairs have a very long tail: only 61% of queries has greater than three clicks.", "labels": [], "entities": []}, {"text": "They also reported a drastic drop in the accuracy of the click prediction model when fewer historical observations are available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9995691180229187}, {"text": "click prediction", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7723914682865143}]}, {"text": "Furthermore, fine-grained historical CTR information takes a huge amount of space, which makes it costly to maintain.", "labels": [], "entities": []}, {"text": "On the other hand, text features are always readily available, and thus are particularly useful for those cases for which there is insufficient historical information.", "labels": [], "entities": []}, {"text": "Multiple researchers, for example, reported the usage of text features including simple lexical similarity scores between the query and ads, word or phrase overlaps and the number of overlapping words and characters.", "labels": [], "entities": []}, {"text": "Such features rely on the assumption that query-ad overlap is correlated with perceived relevance.", "labels": [], "entities": []}, {"text": "While this is true to a certain extent, the use of simple lexical similarity cannot capture semantic information such as synonyms, entities of the same type and strong relationships between entities (e.g. CEO-company, brandmodel, part-of).", "labels": [], "entities": []}, {"text": "Recently a host of studies on word embedding have been conducted; all map words into a vector space such that semantically relevant words are placed near each other in the space ().", "labels": [], "entities": []}, {"text": "The use of continuous word vectors has been shown to be helpful fora wide range of NLP tasks by better capturing both syntactic and semantic information than simple lexical features.", "labels": [], "entities": []}, {"text": "No previous research on sponsored search has successfully used word embeddings to generate text features.", "labels": [], "entities": []}, {"text": "In this paper, we explore the use of word embeddings for click prediction.", "labels": [], "entities": [{"text": "click prediction", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7140447050333023}]}, {"text": "However, it is clear that conventional word embeddings (which solely rely on word co-occurrence in a context window) can only offer limited discriminative power because queries and ad text are typically very short.", "labels": [], "entities": []}, {"text": "In addition, conventional word embeddings cannot capture user intents, preferences and desires.", "labels": [], "entities": []}, {"text": "showed that specific frequently occurring lexical patterns, e.g., x% off, guaranteed return in x days and official site, are effective in triggering users desires, and thus lead to significant differences in CTR.", "labels": [], "entities": []}, {"text": "Conventional word embeddings cannot capture these phenomena since they do not incorporate the implicit feedback users provide through clicks and non-clicks.", "labels": [], "entities": []}, {"text": "These observations naturally lead us to leverage click feedback to infuse users' intentions and desires into the vector space.", "labels": [], "entities": []}, {"text": "The simplest way to harness click feedback is to train conventional word embedding models on a corpus that only includes clicked impressions, where each \"sentence\" is constructed by mixing the query and ad text.", "labels": [], "entities": []}, {"text": "Having trained a word embedding model, we simply take the average of word vectors of the query and ads respectively to obtain sentence (or paragraph) vectors, which in turn are used to compute the similarity scores between the query and ads.", "labels": [], "entities": []}, {"text": "Our experiments show that this method does improve click prediction performance.", "labels": [], "entities": [{"text": "click prediction", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.767918050289154}]}, {"text": "However, this method has several potential weaknesses.", "labels": [], "entities": []}, {"text": "First, the use of only clicked impressions ignores the large amount of negative signals contained in the non-clicked ad impressions.", "labels": [], "entities": []}, {"text": "Second, the use of indirect signals (word co-occurrences) can be noisy or even harmful to our ultimate goal (accurate click prediction) when it is combined with direct signals (impressions with click feedback).", "labels": [], "entities": []}, {"text": "Third, without explicit consideration about the averaging step in the training process of word embedding models, a simple averaging scheme across word vectors maybe a suboptimal.", "labels": [], "entities": []}, {"text": "We therefore propose several joint word embedding models; all of these aim to put query vectors close to relevant ad vectors by explicitly utilizing both positive and negative click feedback.", "labels": [], "entities": []}, {"text": "We evaluate all these models against a large sponsored search data set from a commercial search engine, and demonstrate that our proposed models significantly improve click prediction performance.", "labels": [], "entities": [{"text": "click prediction", "start_pos": 167, "end_pos": 183, "type": "TASK", "confidence": 0.676043301820755}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we present a brief summary of related work.", "labels": [], "entities": []}, {"text": "In Section 3 we give some background information on ad click prediction in sponsored search.", "labels": [], "entities": [{"text": "ad click prediction", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.6159964799880981}]}, {"text": "In Section 4 we describe our methods.", "labels": [], "entities": []}, {"text": "In Section 5 we discuss our experiments.", "labels": [], "entities": []}, {"text": "We finish with some conclusions and future directions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data The data used in our experiments were collected from a random bucket of the Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.9171368479728699}]}, {"text": "sponsored search traffic logs fora period of 4 weeks in October 2014.", "labels": [], "entities": []}, {"text": "In the data there are approximately 65 million unique users, 150 million unique queries and 12 million unique ads.", "labels": [], "entities": []}, {"text": "There are approximately 985 million ad impression events in total.", "labels": [], "entities": []}, {"text": "We split the data into 3 partitions with respect to time: the first 14 days' data are used for training word embedding models, the next 7 days' data for training click prediction models, and the last 7 days' data for testing.", "labels": [], "entities": [{"text": "training click prediction", "start_pos": 153, "end_pos": 178, "type": "TASK", "confidence": 0.600529670715332}]}, {"text": "presents more detailed statistics for the data.", "labels": [], "entities": []}, {"text": "Models We are interested in evaluating the usefulness of different word embedding models as features for click prediction, and already have a very good baseline system for this task.", "labels": [], "entities": [{"text": "click prediction", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.7432704567909241}]}, {"text": "Consequently, in all experiments below, we used the same personalized historical CTRs, contextual and lexical features as the baseline system described in Section 3.", "labels": [], "entities": []}, {"text": "We tested models with the following additional features derived from word embeddings: 1.", "labels": [], "entities": []}, {"text": "cosine similarity between the mean vectors of the query and ad title 2.", "labels": [], "entities": []}, {"text": "cosine similarity between the mean vectors of the query and ad description 3.", "labels": [], "entities": []}, {"text": "sum of 1 and 2 4.", "labels": [], "entities": []}, {"text": "sigmoid function value for the dot product of the mean vectors of the query and ad title 5.", "labels": [], "entities": []}, {"text": "sigmoid function value for the dot product of the query and ad description 6.", "labels": [], "entities": []}, {"text": "sum of 4 and 5 All these continuous features are quantized into 50 bins.", "labels": [], "entities": []}, {"text": "We compared five different word embedding algorithms: 1.", "labels": [], "entities": []}, {"text": "Skip-gram trained on Wikipedia (SK-WIKI) We used the skip-gram mode of word2vec 2 with a window size of 5 and negative sampling to train the SK-WIKI model.", "labels": [], "entities": []}, {"text": "For all other models we used in-house implementation which employs AdagradRDA (Duchi et al., 2011) to minimize the loss functions introduced in Section 4.", "labels": [], "entities": []}, {"text": "The dimension of a word vector is set to 100 for all algorithms . We removed a set of prefixed stop-words and all words occurring fewer than 100 times; the resulting vocabulary comprised 126K unique words.", "labels": [], "entities": []}, {"text": "To process our web-scale data, we implemented a multithread program where each thread randomly traverses over a partition of the data D to compute gradients and update the matrix W stored in a shared memory.", "labels": [], "entities": []}, {"text": "For computational efficiency, the hogwild lock-free approach) is used.", "labels": [], "entities": []}, {"text": "We set \u03b7 in Eq.", "labels": [], "entities": []}, {"text": "9 to 0.2 through grid search based on two-fold cross-validation.", "labels": [], "entities": []}, {"text": "This small value indeed verifies the idea of weak negative feedback for unclicked impressions.", "labels": [], "entities": []}, {"text": "In order to circumvent the severe influence of ad position on the click prediction model, only the impressions that are placed at the top position were used for training the click prediction model.", "labels": [], "entities": [{"text": "click prediction", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.78232342004776}, {"text": "click prediction", "start_pos": 174, "end_pos": 190, "type": "TASK", "confidence": 0.7858316600322723}]}, {"text": "Note that CTR at other positions can be derived from that of the ad at the top position through scaling.", "labels": [], "entities": [{"text": "CTR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8683465719223022}]}, {"text": "showed that CTR can be decomposed as a product of the probability of an ad getting clicked given its being seen and the probability of an ad being seen at a particular position.", "labels": [], "entities": []}, {"text": "Results We ordered query-ad pairs by the predicted score to compute AucLoss (i.e. 1 -AUC where AUC is the area under the Receiver Operating Characteristic (ROC) curve).", "labels": [], "entities": [{"text": "AucLoss", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.848469614982605}, {"text": "AUC", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.6848762631416321}]}, {"text": "The ROC AUC is known to have a correlation with the quality of ranking by the predicted score; thus is one of the most important 2 Available at https://code.google.com/p/word2vec/ 3 Higher vector dimensions such as 200 were also tried but did not give a significant improvement.", "labels": [], "entities": [{"text": "ROC AUC", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.7777499258518219}]}, {"text": "show that the use of features derived from our proposed word embedding models significantly reduce AucLoss by up to 4.1%.", "labels": [], "entities": [{"text": "AucLoss", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.8598954677581787}]}, {"text": "For commercial search engines which have a very strong baseline AucLoss, a reduction of 1% can be considered large.", "labels": [], "entities": [{"text": "AucLoss", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.8566451668739319}]}, {"text": "Moreover, as expected, the more issues (as identified in Section 4) an algorithm addresses, the better performance it achieves.", "labels": [], "entities": []}, {"text": "From the comparison between the SK-WIKI model and the rest, we can recognize the importance of word embedding models specialized to domain text and supervision signals.", "labels": [], "entities": []}, {"text": "Also the difference between the CB-CI (SK-CI) model and the JIWV model indicates the noisiness of indirect signals such as word co-occurrence compared to direct signals like click feedback.", "labels": [], "entities": []}, {"text": "Finally the gap between the JIWV and JWV models highlights the significance of considering compositionality in the word embedding training process.", "labels": [], "entities": [{"text": "JIWV", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.8982080221176147}, {"text": "JWV", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.688190758228302}]}, {"text": "Eyeballing the most similar words to several queries in the vector space is often helpful forgetting some sense about how different methods influence the resulting vector spaces.", "labels": [], "entities": []}, {"text": "lists the top 20 most similar words to the query \"metal watch.\"", "labels": [], "entities": []}, {"text": "The top words for the SK-WIKI model are not semantically interesting in terms of capturing the intent or desires.", "labels": [], "entities": []}, {"text": "This clearly shows the limitation of word embedding methods that only rely on indirect signals (i.e. word co-occurrences) from a generic text corpus (e.g. Wikipedia) for sponsored search click prediction.", "labels": [], "entities": [{"text": "sponsored search click prediction", "start_pos": 170, "end_pos": 203, "type": "TASK", "confidence": 0.5834001898765564}]}, {"text": "Noticeably the methods: Top 20 most similar words to \"princess costumes\" that incorporate click feedback find more words related to products, services or websites instead of just conceptuatlly related words.", "labels": [], "entities": []}, {"text": "Given that real products or services can be regarded as the best possible surrogates to user intents and desires, this demonstrates the effectiveness of our methods.", "labels": [], "entities": []}, {"text": "This tendency gets stronger as a method takes into account both positive and negative click feedback.", "labels": [], "entities": []}, {"text": "Another very interesting observation comes from the fact that none of the methods except JMV successfully captures the composite meaning of \"metal watch\"; they tend to either find related words separately for each query word (e.g. \"watch\" is strongly associated to the sense of watching something like movie or other types of video) or find totally unrelated words (particularly SK-WIKI).", "labels": [], "entities": []}, {"text": "This demonstrates that it is crucial to address compositionality in the very process of learning word vectors.", "labels": [], "entities": []}, {"text": "shows the top 20 most similar words to the query \"princess costumes.\"", "labels": [], "entities": []}, {"text": "In this example we can spot another surprising result.", "labels": [], "entities": []}, {"text": "The JMV model pushes a lot of price related expressions to the top 4 . This may imply that many parents search for lower cost costumes, clearly showing a clear psychological desire in the financial dimension.", "labels": [], "entities": []}, {"text": "This observation confirms the findings in () about the significant role of certain ad expressions in triggering users' psychological desires.", "labels": [], "entities": []}, {"text": "We also note that the CB-CI model returns a lot of misspells for \"costume(s)\", which would not be possible with simple lexical features of the baseline system.", "labels": [], "entities": []}, {"text": "A close look at this example generally confirms the observations we made for the previous ex-: Top 20 most similar words to \"game for kids\" ample.", "labels": [], "entities": []}, {"text": "Finally shows top the 20 most similar words for the query \"game for kids.\"", "labels": [], "entities": []}, {"text": "Once again we found the same analysis holds for this case.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparative evaluation results in AucLoss re- duction from the baseline system", "labels": [], "entities": [{"text": "AucLoss re- duction", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.6166249737143517}]}]}