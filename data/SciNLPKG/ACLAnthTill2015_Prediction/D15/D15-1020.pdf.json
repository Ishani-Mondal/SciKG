{"title": [{"text": "Cross-document Event Coreference Resolution based on Cross-media Features", "labels": [], "entities": [{"text": "Cross-document Event Coreference Resolution", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.6992136538028717}]}], "abstractContent": [{"text": "In this paper we focus on anew problem of event coreference resolution across television news videos.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.855649729569753}]}, {"text": "Based on the observation that the contents from multiple data modalities are complementary, we develop a novel approach to jointly encode effective features from both closed captions and video key frames.", "labels": [], "entities": []}, {"text": "Experiment results demonstrate that visual features provided 7.2% absolute F-score gain on state-of-the-art text based event extraction and coreference resolution.", "labels": [], "entities": [{"text": "F-score", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9763327240943909}, {"text": "event extraction", "start_pos": 119, "end_pos": 135, "type": "TASK", "confidence": 0.7107444703578949}, {"text": "coreference resolution", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.9787547588348389}]}], "introductionContent": [{"text": "TV news is the medium that broadcasts events, stories and other information via television.", "labels": [], "entities": []}, {"text": "The broadcast is conducted in programs with the name of \"Newscast\".", "labels": [], "entities": [{"text": "Newscast", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9483434557914734}]}, {"text": "Typically, newscasts require one or several anchors who are introducing stories and coordinating transition among topics, reporters or journalists who are presenting events in the fields and scenes that are captured by cameramen.", "labels": [], "entities": []}, {"text": "Similar to newspapers, the same stories are often reported by multiple newscast agents.", "labels": [], "entities": []}, {"text": "Moreover, in order to increase the impact on audience, the same stories and events are reported for mutliple times.", "labels": [], "entities": []}, {"text": "TV audience passively receives redundant information, and often has difficulty in obtaining clear and useful digest of ongoing events.", "labels": [], "entities": []}, {"text": "These properties lead to needs for automatic methods to cluster information and remove redundancy.", "labels": [], "entities": []}, {"text": "We propose anew research problem of event coreference resolution across multiple news videos.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.8649687568346659}]}, {"text": "To tackle this problem, a good starting point is processing the Closed Captions (CC) which is accompanying videos in newcasts.", "labels": [], "entities": [{"text": "processing the Closed Captions (CC)", "start_pos": 49, "end_pos": 84, "type": "TASK", "confidence": 0.7373601240771157}]}, {"text": "The CC is either generated by automatic speech recognition (ASR) systems or transcribed by a human stenotype operator who inputs phonetics which are instantly and automatically translated into texts, where events can be extracted.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.8123728136221567}]}, {"text": "There exist some previous event coreference resolution work such as.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.8136779268582662}]}, {"text": "However, they only focused on formally written newswire articles and utilized textual features.", "labels": [], "entities": []}, {"text": "Such approaches do not perform well on CC due to (1).", "labels": [], "entities": []}, {"text": "the propagated errors from upper stream components (e.g., automatic speech/stenotype recognition and event extraction); (2).", "labels": [], "entities": [{"text": "speech/stenotype recognition", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.6038581505417824}, {"text": "event extraction", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.7415000796318054}]}, {"text": "Different from written news, newscasts are often limited in time due to fixed TV program schedules, thus, anchors and journalists are trained and expected to organize reports which are comprehensively informative with complementary visual and CC descriptions within a short time.", "labels": [], "entities": []}, {"text": "These two sides have minimal overlapped information while they are inter-dependent.", "labels": [], "entities": []}, {"text": "For example, anchors and reporters introduce the background story which are not presented in the videos, and thus the events extracted from CC often lack information about participants.", "labels": [], "entities": []}, {"text": "For example, as shown in, these two Conflict.Attack event mentions are coreferential.", "labels": [], "entities": []}, {"text": "However, in the first event mention, a mistake in Closed Caption (\"he was killed\" \u2192 \"it was killed\") makes event extraction and text based coreference systems unable to detect and link \"it\" to the entity of \"Jordanian pilot\".", "labels": [], "entities": [{"text": "Closed Caption", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.6364668011665344}, {"text": "event extraction", "start_pos": 107, "end_pos": 123, "type": "TASK", "confidence": 0.7782367765903473}]}, {"text": "Fortunately, videos often illustrate brief descriptions by vivid visual contents.", "labels": [], "entities": []}, {"text": "Moreover, diverse anchors, reporters and TV channels tend to use similar or identical video contents to describe the same story, even though they usually use different words and phrases.", "labels": [], "entities": []}, {"text": "Therefore, the challenges in coreference resolution methods based on text information can be addressed by incorporating visual similarity.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.9429940283298492}]}, {"text": "In this example, the visual similarity between the corresponding video frames is high because both of them show the scene of the Jordanian pilot.", "labels": [], "entities": []}, {"text": "Similar work such as (), (Ramanathan et al., 2014), (Motwani and Mooney, 2012) and () have explored methods of linking visual materials with texts.", "labels": [], "entities": []}, {"text": "However, these methods mainly focus on connecting image concepts with entities in text mentions; and some of them do not clearly distinguish entity and event in the documents since the definition of visual concepts often require both of them.", "labels": [], "entities": []}, {"text": "Moreover, the aforementioned work mainly focuses on improving visual contents recognition by introducing text features while our work will take the opposite route, which takes advantage of visual information to improve event coreference resolution.", "labels": [], "entities": [{"text": "visual contents recognition", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.7005204558372498}, {"text": "event coreference resolution", "start_pos": 219, "end_pos": 247, "type": "TASK", "confidence": 0.7971091667811075}]}, {"text": "In this paper, we propose to jointly incorporate features from both speech (textual) and video (visual) channels for the first time.", "labels": [], "entities": []}, {"text": "We also build a newscast crawling system that can automatically accumulate video records and transcribe closed captions.", "labels": [], "entities": [{"text": "newscast crawling", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.6687123626470566}]}, {"text": "With the crawler, we created a benchmark dataset which is fully annotated with crossdocument coreferential events 1 .", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}