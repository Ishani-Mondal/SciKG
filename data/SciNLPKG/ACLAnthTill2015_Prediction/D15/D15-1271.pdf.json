{"title": [{"text": "Adapting Coreference Resolution for Narrative Processing", "labels": [], "entities": [{"text": "Adapting Coreference Resolution", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8825857837994894}]}], "abstractContent": [{"text": "Domain adaptation is a challenge for supervised NLP systems because of expensive and time-consuming manual annotated resources.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8121110796928406}]}, {"text": "We present a novel method to adapt a supervised coreference resolution system trained on newswire to short narrative stories without retraining the system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.7866877913475037}]}, {"text": "The idea is to perform inference via an Integer Linear Programming (ILP) formulation with the features of narratives adopted as soft constraints.", "labels": [], "entities": []}, {"text": "When testing on the UMIREC 1 and N2 2 corpora with the-state-of-the-art Berkeley coreference resolution system trained on OntoNotes 3 , our inference substantially outperforms the original inference on the CoNLL 2011 metric.", "labels": [], "entities": [{"text": "CoNLL 2011 metric", "start_pos": 206, "end_pos": 223, "type": "DATASET", "confidence": 0.937072773774465}]}], "introductionContent": [{"text": "Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or 'chains') corresponding to those referents.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9214663505554199}, {"text": "partitioning the set of mentions of discourse referents in a text", "start_pos": 38, "end_pos": 103, "type": "TASK", "confidence": 0.6872308308427985}]}, {"text": "To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based ( or rule-based () coreference systems.", "labels": [], "entities": []}, {"text": "These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions.", "labels": [], "entities": []}, {"text": "However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify.", "labels": [], "entities": []}, {"text": "propose a learningbased, mention-synchronous coreference system to  tackle the various aspects of coreference by using the simplest possible set of features.", "labels": [], "entities": []}, {"text": "Its advantage is that the system can both implicitly model important linguistic effects and capture other patterns in the data that are not easily teased out by hand.", "labels": [], "entities": []}, {"text": "With a simple set of features including head/first/last words, preceding/following words, length, exact string match, head match, sentence/mention distance, gender, number etc. and an efficient training using conditional log-likelihood augmented with a parameterized loss function optimization they report state-of-the-art results on CoNLL 2011 data.", "labels": [], "entities": [{"text": "exact string match", "start_pos": 98, "end_pos": 116, "type": "METRIC", "confidence": 0.8876685897509257}, {"text": "CoNLL 2011 data", "start_pos": 334, "end_pos": 349, "type": "DATASET", "confidence": 0.9836873809496561}]}, {"text": "But while CoNLL 2011 training data (OntoNotes) includes a few different source domains (newswire, weblogs, etc.), we witness significant drops in performance when systems trained on CoNLL 2011 are applied to new target domains such as narratives.", "labels": [], "entities": [{"text": "CoNLL 2011 training data", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.8158271461725235}]}, {"text": "Some linguistic effects and patterns that are very important for the target domain were never seen in the source domain on which the model was trained.", "labels": [], "entities": []}, {"text": "In such cases, when adapting a coreference system to anew domain, it is necessary to incorporate these more complex linguistic features and patterns into the model.", "labels": [], "entities": []}, {"text": "We propose a novel method to adopt the target domain's features to a supervised coreference system without retraining the model.", "labels": [], "entities": []}, {"text": "We present a case of transferring the system of, which is trained on OntoNotes, to short narrative stories.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 69, "end_pos": 78, "type": "DATASET", "confidence": 0.939930260181427}]}, {"text": "The idea is to perform inference via a linear programming formulation with the features of narratives adopted as soft constraints.", "labels": [], "entities": []}, {"text": "Since the new features are incorporated only into the linear program, there is no need to retrain the original model.", "labels": [], "entities": []}, {"text": "Our formulation models three phenomena that are important for short narrative stories: local discourse coherence, which we model via centering theory constraints, speaker-listener relations, which we model via direct speech act constraints, and character-naming, which we model via definite noun phrase and exact match constraints.", "labels": [], "entities": []}, {"text": "We also suggest a method to compute back pointers (as defined in) globally.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test our model on 30 English folktales from the UCM/MIT Indications, Referring Expressions, and Coreference (UMIREC) Corpus v1.1, and 64 text stories from the Hadith section of the Narrative Networks (N2) Corpus ().", "labels": [], "entities": [{"text": "UCM/MIT Indications, Referring Expressions, and Coreference (UMIREC) Corpus v1.1", "start_pos": 51, "end_pos": 131, "type": "DATASET", "confidence": 0.6580709338188171}, {"text": "Narrative Networks (N2) Corpus", "start_pos": 184, "end_pos": 214, "type": "DATASET", "confidence": 0.6050141602754593}]}, {"text": "The texts are preprocessed using the Stanford sentence splitter (Manning et al., 2014) 8 and the Berkeley coreference system's preprocessor.", "labels": [], "entities": [{"text": "Stanford sentence splitter", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.717461109161377}]}, {"text": "The Berkeley coreference system is trained on OntoNotes (newswire, broad-cast news/conversation, and web texts).", "labels": [], "entities": []}, {"text": "We use Gurobi 9 to solve our ILP problem, and the Lund semantic role labeler) to detect semantic frames.", "labels": [], "entities": []}, {"text": "Note that in our implementation, \"subject\" and \"object\" used in Section 4 and Section 5 refer to \"subject role\" and \"object role\" of the semantic frames respectively.", "labels": [], "entities": []}, {"text": "We use a separate section of the N2 corpus, the Inspire story texts, as the held-out validation set used for parameter tuning, resulting in We compare our ILP inference (ILPI) to the standard Berkeley coreference system (BER) with both gold and predicted mentions.", "labels": [], "entities": [{"text": "N2 corpus", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.8794124126434326}, {"text": "Inspire story texts", "start_pos": 48, "end_pos": 67, "type": "DATASET", "confidence": 0.8097945551077524}, {"text": "parameter tuning", "start_pos": 109, "end_pos": 125, "type": "TASK", "confidence": 0.6967315971851349}]}, {"text": "shows that our inference improves the MUC, BCUB and CEAFE scores on both datasets, especially when using gold mentions . The average ILP running times are 42.37s per UMIREC document and 22.7s per N2 document on a Core I7 2.3 GHz quad-core computer.", "labels": [], "entities": [{"text": "MUC", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9596140384674072}, {"text": "BCUB", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9902704954147339}, {"text": "CEAFE", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9877126812934875}]}, {"text": "shows the effects of each constraint type when used alone.", "labels": [], "entities": []}, {"text": "Surprisingly, the simplest constraint type (definite & exact match constraints) gives us the best improvement especially in terms of CEAFE score.", "labels": [], "entities": [{"text": "CEAFE score", "start_pos": 133, "end_pos": 144, "type": "METRIC", "confidence": 0.8386123180389404}]}, {"text": "This maybe because definite & exact match constraint links mentions in the whole document, while the centering theory and direct speech act constraints are more local.", "labels": [], "entities": []}, {"text": "And since short narrative stories often have a small set of characters (usually represented by definite noun phrases or proper nouns), when these characters are linked correctly, the coreference resolution result is improved considerably.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ILPI and BER inference results on UMIREC (Tales) and N2 (Hadith) data.", "labels": [], "entities": [{"text": "BER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9976454377174377}, {"text": "UMIREC (Tales) and N2 (Hadith) data", "start_pos": 44, "end_pos": 79, "type": "DATASET", "confidence": 0.8578644275665284}]}, {"text": " Table 2: Effects of different constraints on ILP  inference on UMIREC (Tales) with gold mentions.", "labels": [], "entities": [{"text": "UMIREC (Tales)", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.7581766992807388}]}]}