{"title": [], "abstractContent": [{"text": "Model combination techniques have consistently shown state-of-the-art performance across multiple tasks, including syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.6908386647701263}]}, {"text": "However, they dramatically increase runtime and can be difficult to employ in practice.", "labels": [], "entities": []}, {"text": "We demonstrate that applying constituency model combination techniques to n-best lists instead of n different parsers results in significant parsing accuracy improvements.", "labels": [], "entities": [{"text": "parsing", "start_pos": 141, "end_pos": 148, "type": "TASK", "confidence": 0.9573546051979065}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.8972374200820923}]}, {"text": "Parses are weighted by their probabilities and combined using an adapted version of Sagae and Lavie (2006).", "labels": [], "entities": [{"text": "Parses", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8170843720436096}]}, {"text": "These accuracy gains come with marginal computational costs and are obtained on top of existing parsing techniques such as discrim-inative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9987444877624512}, {"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9988771080970764}, {"text": "WSJ section 23", "start_pos": 217, "end_pos": 231, "type": "DATASET", "confidence": 0.9582052628199259}]}, {"text": "On out-of-domain corpora , accuracy is improved by 0.4% on average.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9996312856674194}]}, {"text": "We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse.", "labels": [], "entities": []}, {"text": "These new parses are substantially better than the originals: combine outputs from multiple n-best parsers and achieve an F 1 of 92.6% on the WSJ test set, a 0.5% improvement over their best n-best parser.", "labels": [], "entities": [{"text": "F 1", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9969369471073151}, {"text": "WSJ test set", "start_pos": 142, "end_pos": 154, "type": "DATASET", "confidence": 0.9669760664304098}]}, {"text": "Model combination approaches tend to fall into the following categories: hybridization, where multiple parses are combined into a single parse; switching, which picks a single parse according to some criteria (usually a form of voting); grammar merging where grammars are combined before or during parsing; and stacking, where one parser sends its prediction to another at runtime.", "labels": [], "entities": [{"text": "grammar merging", "start_pos": 237, "end_pos": 252, "type": "TASK", "confidence": 0.6580428630113602}, {"text": "stacking", "start_pos": 311, "end_pos": 319, "type": "TASK", "confidence": 0.9673973321914673}]}, {"text": "All of these have at least one of the caveats that (1) overall computation is increased and runtime is determined by the slowest parser and (2) using multiple parsers increases the system complexity, making it more difficult to deploy in practice.", "labels": [], "entities": []}, {"text": "In this paper, we describe a simple hybridization extension (\"fusion\") which obtains much of hybridization's benefits while using only a single n-best parser and minimal extra computation.", "labels": [], "entities": []}, {"text": "Our method treats each parse in a single parser's n-best list as a parse from n separate parsers.", "labels": [], "entities": []}, {"text": "We then adapt parse combination methods by, to fuse the constituents from then parses into a singletree.", "labels": [], "entities": []}, {"text": "We empirically show that six n-best parsers benefit from parse fusion across six domains, obtaining stateof-the-art results.", "labels": [], "entities": [{"text": "parse fusion", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.770533412694931}]}, {"text": "These improvements are complementary to other techniques such as reranking and self-training.", "labels": [], "entities": []}, {"text": "Our best system obtains an F 1 of 92.6% on WSJ section 23, a score previously obtained only by combining the outputs from multiple parsers.", "labels": [], "entities": [{"text": "F 1", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9956108033657074}, {"text": "WSJ section 23", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.9323466221491495}]}, {"text": "A reference implementation is available as part of BLLIP Parser at http: //github.com/BLLIP/bllip-parser/", "labels": [], "entities": [{"text": "BLLIP Parser", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.8071006536483765}]}], "datasetContent": [{"text": "Corpora: Parse fusion is evaluated on British National Corpus (BNC), Brown, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) Dependencies (basic dependencies, version 3.3.0) and provide dependency metrics (UAS, LAS) as well.", "labels": [], "entities": [{"text": "Parse fusion", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.9253198206424713}, {"text": "British National Corpus (BNC)", "start_pos": 38, "end_pos": 67, "type": "DATASET", "confidence": 0.9670221706231436}, {"text": "Brown", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.8870161175727844}, {"text": "GENIA", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.5267062783241272}, {"text": "Question Bank (QB)", "start_pos": 83, "end_pos": 101, "type": "DATASET", "confidence": 0.8239895462989807}, {"text": "Wall Street Journal (WSJ) Dependencies", "start_pos": 124, "end_pos": 162, "type": "DATASET", "confidence": 0.9100720882415771}, {"text": "LAS", "start_pos": 236, "end_pos": 239, "type": "METRIC", "confidence": 0.751161515712738}]}, {"text": "Supervised parsers are trained on the WSJ training set (sections 2-21) and use section 22 or 24 for development.", "labels": [], "entities": [{"text": "WSJ training set", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.9621183673540751}]}, {"text": "Self-trained BLLIP is selftrained using two million sentences from Gigaword and Stanford RNN uses word embeddings trained from larger corpora.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.606694221496582}, {"text": "Stanford RNN", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.9080393016338348}]}, {"text": "Parameter tuning: There are three parameters for our fusion process: the size of the n-best list (2 < n \u2264 50), the smoothing exponent from Section 2.1 (\u03b2 \u2208 [0.5, 1.5] with 0.1 increments), and the minimum threshold for constituents (t \u2208 [0.2, 0.7] with 0.01 increments).", "labels": [], "entities": []}, {"text": "We use grid search to tune these parameters for two separate scenarios.", "labels": [], "entities": []}, {"text": "When parsing WSJ (in-domain), we tune parameters on WSJ section 24.", "labels": [], "entities": [{"text": "parsing", "start_pos": 5, "end_pos": 12, "type": "TASK", "confidence": 0.9597004055976868}, {"text": "WSJ", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.678317666053772}, {"text": "WSJ section", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.9275813698768616}]}, {"text": "For the remaining corpora (out-ofdomain), we use the tuning section from Brown.", "labels": [], "entities": []}, {"text": "Each parser is tuned separately, resulting in 12 different tuning scenarios.", "labels": [], "entities": []}, {"text": "In practice, though, in-domain and out-of-domain tuning regimes tend to pick similar settings within a parser.", "labels": [], "entities": []}, {"text": "Across parsers, settings are also fairly similar (n is usually 30 or 40, t is usually between 0.45 and 0.5).", "labels": [], "entities": []}, {"text": "While the smoothing exponent varies from 0.5 to 1.3, setting \u03b2 = 1 does not significantly hurt accuracy for most parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9992896318435669}]}, {"text": "To study the effects of these parameters, shows three slices of the tuning surface for BLLIP parser on WSJ section 24 around the optimal settings (n = 30, \u03b2 = 1.1, t = 0.47).", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.6179773211479187}, {"text": "WSJ section 24", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.9658164978027344}]}, {"text": "In each graph, one of the parameters is varied while the other is held constant.", "labels": [], "entities": []}, {"text": "Increasing n-best size improves accuracy until about n = 30 where there seems to be sufficient diversity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9989748001098633}]}, {"text": "For BLLIP, the  smoothing exponent (\u03b2) is best set around 1.0, with accuracy falling off if the value deviates too much.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.6889459490776062}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9995322227478027}]}, {"text": "Finally, the threshold parameter is empirically optimized a little below t = 0.5 (the value suggested by).", "labels": [], "entities": []}, {"text": "Since score values are normalized, this means that constituents need roughly half the \"score mass\" in order to be included in the chart.", "labels": [], "entities": []}, {"text": "Varying the threshold changes the precision/recall balance since a high threshold adds only the most confident constituents to the chart ().", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9993840456008911}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9453476071357727}]}, {"text": "Baselines: gives the accuracy of fusion and baselines for BLLIP on the development corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9997218251228333}, {"text": "fusion", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9400702714920044}, {"text": "BLLIP", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.5822725892066956}]}, {"text": "Majority voting sets n = 50, \u03b2 = 0, t = 0.5 giving all parses equal weight and results in constituent-level majority voting.", "labels": [], "entities": []}, {"text": "We explore a rank-based weighting which ignores parse probabilities and weight parses only using the rank: W rank (k) = 1/(2 k ).", "labels": [], "entities": []}, {"text": "These show that accurate parse-level scores are critical for good performance.", "labels": [], "entities": [{"text": "accurate parse-level scores", "start_pos": 16, "end_pos": 43, "type": "METRIC", "confidence": 0.8390576243400574}]}, {"text": "Final evaluation: gives our final results for all parsers across all domains.", "labels": [], "entities": []}, {"text": "Results in blue are significant at p < 0.01 using a randomized permutation test.", "labels": [], "entities": []}, {"text": "Fusion generally improves F 1 for in-domain and out-of-domain parsing by a significant margin.", "labels": [], "entities": [{"text": "F 1", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.990441083908081}]}, {"text": "For the self-trained BLLIP parser, in-domain F 1 increases by 0.4% and out-of-domain F 1 increases by 0.4% on average.", "labels": [], "entities": [{"text": "BLLIP parser", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.5113073885440826}, {"text": "F 1", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.8955103754997253}]}, {"text": "Berkeley parser obtains the smallest gains from fusion since Berkeley's n-best lists are ordered by factors other than probabilities.", "labels": [], "entities": []}, {"text": "As a result, the probabilities from Berkeley can mislead the fusion process.", "labels": [], "entities": []}, {"text": "We also compare against model combination using our reimplementation of.", "labels": [], "entities": []}, {"text": "For these results, all six parsers were given equal weight.", "labels": [], "entities": []}, {"text": "The threshold was set to 0.42 to optimize model combination F 1 on development data (similar to Setting 2 for constituency parsing in).", "labels": [], "entities": [{"text": "F 1", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.8301831781864166}, {"text": "constituency parsing", "start_pos": 110, "end_pos": 130, "type": "TASK", "confidence": 0.7795236706733704}]}, {"text": "Model combination: Evaluation of the constituency fusion method on six parsers across six domains.", "labels": [], "entities": [{"text": "constituency fusion", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8086463212966919}]}, {"text": "x/y indicates the F 1 from the baseline parser (x) and the baseline parser with fusion (y) respectively.", "labels": [], "entities": [{"text": "F 1", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.990499347448349}]}, {"text": "Blue indicates a statistically significant difference between fusion and its baseline parser (p < 0.01).", "labels": [], "entities": []}, {"text": "performs better than fusion on BNC and GENIA, but surprisingly fusion outperforms model combination on three of the six domains (not usually not by a significant margin).", "labels": [], "entities": [{"text": "GENIA", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.8837429881095886}]}, {"text": "With further tuning (e.g., specific weights for each constituent-parser pair), the benefits from model combination should increase.", "labels": [], "entities": []}, {"text": "Multilingual evaluation: We evaluate fusion with the Berkeley parser on Arabic (,, and German () from the SPMRL 2014 shared task (Seddah et al., 2014) but did not observe any improvement.", "labels": [], "entities": [{"text": "SPMRL 2014 shared task", "start_pos": 106, "end_pos": 128, "type": "DATASET", "confidence": 0.6831889897584915}]}, {"text": "We suspect this has to do with the same ranking issues seen in the Berkeley Parser's English results.", "labels": [], "entities": [{"text": "Berkeley Parser's English results", "start_pos": 67, "end_pos": 100, "type": "DATASET", "confidence": 0.9510287284851074}]}, {"text": "On the other hand, fusion helps the parser of Narayan and Cohen (2015) on the German NEGRA treebank () to improve from 80.9% to 82.4%.", "labels": [], "entities": [{"text": "German NEGRA treebank", "start_pos": 78, "end_pos": 99, "type": "DATASET", "confidence": 0.8174887100855509}]}, {"text": "Runtime: As discussed in Section 2, fusion's runtime overhead is minimal.", "labels": [], "entities": []}, {"text": "Reranking parsers (e.g., BLLIP and Stanford RNN) already need to perform n-best decoding as input for the reranker.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.871813952922821}]}, {"text": "Using a somewhat optimized implementation fusion in C++, the overhead over BLLIP parser is less than 1%.", "labels": [], "entities": []}, {"text": "Discussion: Why does fusion help?", "labels": [], "entities": []}, {"text": "It is possible that a parser's n-list and its scores act as a weak approximation to the full parse forest.", "labels": [], "entities": []}, {"text": "As a result, fusion seems to provide part of the benefits seen in forest reranking.", "labels": [], "entities": []}, {"text": "Results from imply that fusion and model combination might not be complementary.", "labels": [], "entities": []}, {"text": "Both n-best lists and additional parsers provide syntactic diversity.", "labels": [], "entities": []}, {"text": "While additional parsers provide greater diversity, n-best lists from common parsers are varied enough to provide improvements for parse hybridization.", "labels": [], "entities": [{"text": "parse hybridization", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.8782860934734344}]}, {"text": "We analyzed how often fusion produces completely novel trees.", "labels": [], "entities": []}, {"text": "For BLLIP on WSJ section 24, this only happens about 11% of the time.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.8594515323638916}, {"text": "WSJ section 24", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.9184635480244955}]}, {"text": "Fusion picks the 1-best tree 72% of the time.", "labels": [], "entities": []}, {"text": "This means that for the remaining 17%, fusion picks an existing parse from the rest of the nlist, acting similar to a reranker.", "labels": [], "entities": []}, {"text": "When fusion creates unique trees, they are significantly better than the original 1-best trees (for the 11% subset of WSJ 24, F 1 scores are 85.5% with fusion and 84.1% without, p < 0.003).", "labels": [], "entities": [{"text": "WSJ 24", "start_pos": 118, "end_pos": 124, "type": "DATASET", "confidence": 0.9217238426208496}, {"text": "F 1", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.9777868688106537}]}, {"text": "This contrasts with where novel predic-tions from model combination (stacking) were worse than baseline performance.", "labels": [], "entities": []}, {"text": "The difference is that novel predictions with fusion better incorporate model confidence whereas when stacking, a novel prediction is less trusted than those produced by one or both of the base parsers.", "labels": [], "entities": [{"text": "stacking", "start_pos": 102, "end_pos": 110, "type": "TASK", "confidence": 0.9679319262504578}]}, {"text": "Preliminary extensions: Here, we summarize two extensions to fusion which have yet to show benefits.", "labels": [], "entities": []}, {"text": "The first extension explores applying fusion to dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8511563539505005}]}, {"text": "We explored two ways to apply fusion when starting from constituency parses: (1) fuse constituents and then convert them to dependencies and (2) convert to dependencies then fuse the dependencies as in.", "labels": [], "entities": []}, {"text": "Approach (1) does not provide any benefit (LAS drops between 0.5% and 2.4%).", "labels": [], "entities": [{"text": "Approach", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9927034378051758}, {"text": "LAS", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9944041967391968}]}, {"text": "This may result from fusion's artifacts including unusual unary chains or nodes with a large number of children -it is possible that adjusting unary handling and the precision/recall tradeoff may reduce these issues.", "labels": [], "entities": [{"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.998887836933136}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.911720871925354}]}, {"text": "Approach (2) provided only modest benefits compared to those from constituency parsing fusion.", "labels": [], "entities": [{"text": "Approach", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9629146456718445}, {"text": "constituency parsing fusion", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.8527464866638184}]}, {"text": "The largest LAS increase for (2) is 0.6% for the Stanford Parser, though for Berkeley and Self-trained BLLIP, dependency fusion results in small losses (-0.1% LAS).", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.412908673286438}, {"text": "LAS", "start_pos": 159, "end_pos": 162, "type": "METRIC", "confidence": 0.965928852558136}]}, {"text": "Two possible reasons are that the dependency baseline is higher than its constituency counterpart and some dependency graphs from the n-best list are duplicates which lowers diversity and may need special handling, but this remains an open question.", "labels": [], "entities": []}, {"text": "While fusion helps on top of a self-trained parser, we also explored whether a fused parser can self-train ().", "labels": [], "entities": []}, {"text": "To test this, we (1) parsed two million sentences with BLLIP (trained on WSJ), (2) fused those parses, (3) added the fused parses to the gold training set, and (4) retrained the parser on the expanded training.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.991909921169281}, {"text": "WSJ", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.9096850752830505}]}, {"text": "The resulting model did not perform better than a selftrained parsing model that didn't use fusion.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Six parsers along with their 1-best F 1  scores, unlabeled attachment scores (UAS) and la- beled attachment scores (LAS) on WSJ section 23.", "labels": [], "entities": [{"text": "1-best F 1  scores", "start_pos": 39, "end_pos": 57, "type": "METRIC", "confidence": 0.825180858373642}, {"text": "unlabeled attachment scores (UAS)", "start_pos": 59, "end_pos": 92, "type": "METRIC", "confidence": 0.8201284358898798}, {"text": "la- beled attachment scores (LAS)", "start_pos": 97, "end_pos": 130, "type": "METRIC", "confidence": 0.7722398787736893}, {"text": "WSJ section 23", "start_pos": 134, "end_pos": 148, "type": "DATASET", "confidence": 0.90533979733785}]}, {"text": " Table 2: F 1 of a baseline parser, fusion, and base- lines on development sections of corpora (WSJ  section 24 and Brown tune).", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9546415209770203}, {"text": "WSJ  section 24", "start_pos": 96, "end_pos": 111, "type": "DATASET", "confidence": 0.8893584410349528}]}, {"text": " Table 3: Evaluation of the constituency fusion method on six parsers across six domains. x/y indicates  the F 1 from the baseline parser (x) and the baseline parser with fusion (y) respectively. Blue indicates a  statistically significant difference between fusion and its baseline parser (p < 0.01).", "labels": [], "entities": [{"text": "constituency fusion", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.6950419843196869}, {"text": "F 1", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9875430464744568}]}]}