{"title": [{"text": "Learn to Solve Algebra Word Problems Using Quadratic Programming", "labels": [], "entities": [{"text": "Learn to Solve Algebra Word Problems", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7873887320359548}]}], "abstractContent": [{"text": "This paper presents anew algorithm to automatically solve algebra word problems.", "labels": [], "entities": []}, {"text": "Our algorithm solves a word problem via analyzing a hypothesis space containing all possible equation systems generated by assigning the numbers in the word problem into a set of equation system templates extracted from the training data.", "labels": [], "entities": []}, {"text": "To obtain a robust decision surface, we train a log-linear model to make the margin between the correct assignments and the false ones as large as possible.", "labels": [], "entities": []}, {"text": "This results in a quadratic programming (QP) problem which can be efficiently solved.", "labels": [], "entities": []}, {"text": "Experimental results show that our algorithm achieves 79.7% accuracy, about 10% higher than the state-of-the-art base-line (Kushman et al., 2014).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9978289008140564}]}], "introductionContent": [{"text": "An algebra word problem describes a mathematical problem which can be typically modeled by an equation system, as demonstrated in.", "labels": [], "entities": []}, {"text": "Seeking to automatically solve word problems is a classical AI problem.", "labels": [], "entities": []}, {"text": "The word problem solver is traditionally created by the rulebased approach (.", "labels": [], "entities": [{"text": "problem solver", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.707963302731514}]}, {"text": "Recently, using machine learning techniques to construct the solver has become anew trend (.", "labels": [], "entities": [{"text": "solver", "start_pos": 61, "end_pos": 67, "type": "TASK", "confidence": 0.9769802093505859}]}, {"text": "This is based on the fact that word problems derived from the same mathematical problem share some common semantic and syntactic features due to the same underlying logic.", "labels": [], "entities": []}, {"text": "Our method follows this trend.", "labels": [], "entities": []}, {"text": "To solve a word problem, our algorithm analyzes all the possible ways to assign the numbers", "labels": [], "entities": []}], "datasetContent": [{"text": "Dataset: The dataset used in our experiment is provided by ).", "labels": [], "entities": []}, {"text": "Equivalent equation systme templates are automatically merged.", "labels": [], "entities": []}, {"text": "The word problems are parsed by).", "labels": [], "entities": []}, {"text": "The version of the parser is the same as ( ).", "labels": [], "entities": []}, {"text": "The performance of our algorithm is evaluated by comparing each number of the correct answer with the calculated one, regardless of the ordering.", "labels": [], "entities": []}, {"text": "We report the average accuracy of 5-fold cross-validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9997244477272034}]}, {"text": "Learning: We use liblinear to solve the QP problem.", "labels": [], "entities": []}, {"text": "The parameter C in is set to 0.01 in all the following experiments.", "labels": [], "entities": []}, {"text": "We randomly select 300 false derivations of each word problem to form the initial training set.", "labels": [], "entities": []}, {"text": "We add at most 300 false derivations for each word problem during the constraint generation step, and use 5-fold cross-validation to avoid overfitting.", "labels": [], "entities": []}, {"text": "We stop iterating when the cross-validation error becomes worse or the training error converges or none new constraints are generated.", "labels": [], "entities": []}, {"text": "Supervision Level: We consider the learning with two different levels of supervision.", "labels": [], "entities": []}, {"text": "In the first case, the learning is conducted by providing the equation and the correct answer of every training sample.", "labels": [], "entities": []}, {"text": "In the second case, the correct answer is available for every training sample but without the equation.", "labels": [], "entities": []}, {"text": "Instead, all the templates are given, but the correspondence between the template and the training sample is not available.", "labels": [], "entities": []}, {"text": "During learning, the algorithm should evaluate every derivation of each template to find the true one.", "labels": [], "entities": []}, {"text": "Results: lists the learning statistics for our algorithm and ( ).", "labels": [], "entities": []}, {"text": "We can observe that the number of possible alignments per word problem of our algorithm is much smaller than ( ).", "labels": [], "entities": []}, {"text": "However, the number of all the false alignments is still 80K.", "labels": [], "entities": []}, {"text": "Using the constraint generation algorithm, only 9K false alignments are used in the quadratic programming.", "labels": [], "entities": [{"text": "constraint generation", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7429459989070892}]}, {"text": "We trained our model on a Intel i5-3210M CUP and 4G RAM laptop.", "labels": [], "entities": [{"text": "RAM", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.7129273414611816}]}, {"text": "Kushman's algorithm (2014) needs much more memory than our algorithm and cannot run on a general laptop.", "labels": [], "entities": []}, {"text": "Therefore, we tested their algorithm on a workstation with Intel E5-2620 CPU and 128G memory.", "labels": [], "entities": []}, {"text": "As shown in, their algorithm takes more time than our algorithm.: Ablation study for fully supervised data.", "labels": [], "entities": []}, {"text": "The result of the weakly supervised data is worse than the fully supervised one.", "labels": [], "entities": []}, {"text": "But this result is still higher than Kushman's fully supervised result.", "labels": [], "entities": []}, {"text": "gives the results of our algorithm with different feature ablations.", "labels": [], "entities": []}, {"text": "We can find that all the features are helpful to get the correct solution and none of them dramatically surpasses the others.", "labels": [], "entities": []}, {"text": "Discuss: Although our algorithm gives a better result than ( ), there still exist two main problems that need to be further investigated, as demonstrated in.", "labels": [], "entities": []}, {"text": "The first problem is caused by our feature for semantic representation.", "labels": [], "entities": [{"text": "semantic representation", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.7528796792030334}]}, {"text": "Our current lexicalized feature cannot generalize well for the unseen words.", "labels": [], "entities": []}, {"text": "For example, it is hard for our algorithm to relate the word \"forfeits\" to \"minus\", if it does not appear in the training corpus.", "labels": [], "entities": []}, {"text": "The second problem is caused by the fact that our algorithm only considers the single noun as the entity of a word problem.", "labels": [], "entities": []}, {"text": "Thus when the entity is a complicated noun phrase, our algorithm may fail.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Ablation study for fully supervised data.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9904622435569763}]}]}