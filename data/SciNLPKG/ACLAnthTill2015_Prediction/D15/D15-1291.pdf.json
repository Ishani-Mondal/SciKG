{"title": [{"text": "Syntactic Dependencies and Distributed Word Representations for Chinese Analogy Detection and Mining", "labels": [], "entities": [{"text": "Syntactic Dependencies and Distributed Word Representations", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.680972546339035}, {"text": "Chinese Analogy Detection and Mining", "start_pos": 64, "end_pos": 100, "type": "TASK", "confidence": 0.6760409474372864}]}], "abstractContent": [{"text": "Distributed word representations capture relational similarities by means of vector arithmetics, giving high accuracies on analogy detection.", "labels": [], "entities": [{"text": "analogy detection", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.8622973263263702}]}, {"text": "We empirically investigate the use of syntactic dependencies on improving Chinese analogy detection based on distributed word representations , showing that a dependency-based em-beddings does not perform better than an ngram-based embeddings, but dependency structures can be used to improve analogy detection by filtering candidates.", "labels": [], "entities": [{"text": "Chinese analogy detection", "start_pos": 74, "end_pos": 99, "type": "TASK", "confidence": 0.7238528529802958}, {"text": "analogy detection", "start_pos": 293, "end_pos": 310, "type": "TASK", "confidence": 0.7893961071968079}]}, {"text": "In addition, we show that distributed representations of dependency structure can be used for measuring relational similarities, thereby help analogy mining.", "labels": [], "entities": [{"text": "analogy mining", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.8020719885826111}]}], "introductionContent": [{"text": "Relational similarity measures the correspondence between word-word relations (.", "labels": [], "entities": [{"text": "Relational similarity", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.5493208467960358}]}, {"text": "It is relevant to many tasks in NLP), such as word sense disambiguation, information extraction, question answering, information retrieval, semantic role identification and metaphor detection.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.7087251742680868}, {"text": "information extraction", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.8276655077934265}, {"text": "question answering", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.8789433836936951}, {"text": "information retrieval", "start_pos": 117, "end_pos": 138, "type": "TASK", "confidence": 0.8068271577358246}, {"text": "semantic role identification", "start_pos": 140, "end_pos": 168, "type": "TASK", "confidence": 0.6960822542508444}, {"text": "metaphor detection", "start_pos": 173, "end_pos": 191, "type": "TASK", "confidence": 0.9095690548419952}]}, {"text": "Typical tasks on relational similarity include analogy detection, which measures the degree of relational similarities, and analogy mining, which extracts analogous word pairs from unstructured text.", "labels": [], "entities": [{"text": "analogy detection", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.9170098304748535}, {"text": "analogy mining", "start_pos": 124, "end_pos": 138, "type": "TASK", "confidence": 0.8681893348693848}]}, {"text": "Recently, distributed word representations (i.e. embeddings) () have been used for unsupervised analogy detection.", "labels": [], "entities": [{"text": "analogy detection", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.8254902362823486}]}, {"text": "attributional similarities between words in a relation to compute relational similarities, and show that the method outperforms the best system in the SemEval 2012 shared task on analogy detection.", "labels": [], "entities": [{"text": "SemEval 2012 shared task", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.7199930548667908}, {"text": "analogy detection", "start_pos": 179, "end_pos": 196, "type": "TASK", "confidence": 0.8316969275474548}]}, {"text": "further improve Mikolov's relational similarity measure method using novel arithmetic combinations of attributional similarities.", "labels": [], "entities": []}, {"text": "For simplicity, we call the method of Mikolov et al. embeddingbased analogy detection, without stressing the difference between distributed and distributional (i.e. counting-based) word representations.", "labels": [], "entities": [{"text": "embeddingbased analogy detection", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.7738482356071472}]}, {"text": "Most work on embedding-based analogy detection uses relational similarities as a measure of the quality of embeddings.", "labels": [], "entities": [{"text": "embedding-based analogy detection", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.8258799910545349}]}, {"text": "However, relatively little has been done in the opposite direction, exploring how to leverage embeddings for improving relational similarity algorithms.", "labels": [], "entities": [{"text": "relational similarity algorithms", "start_pos": 119, "end_pos": 151, "type": "TASK", "confidence": 0.8468315005302429}]}, {"text": "We empirically study the use of word embeddings for Chinese analogy detection and mining, leveraging syntactic dependencies, which has been shown to be closely associated with semantic relations).", "labels": [], "entities": [{"text": "Chinese analogy detection and mining", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.7390097618103028}]}, {"text": "Compared with many other languages, this association is particularly strong for Chinese, which is fully configurational and lacks morphology.", "labels": [], "entities": []}, {"text": "To our knowledge, relatively little work has been reported on Chinese relational similarities, compared to other tasks in Chinese NLP, including syntactic parsing, information extraction and machine translation.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.7029147893190384}, {"text": "information extraction", "start_pos": 164, "end_pos": 186, "type": "TASK", "confidence": 0.8247244656085968}, {"text": "machine translation", "start_pos": 191, "end_pos": 210, "type": "TASK", "confidence": 0.7943305671215057}]}, {"text": "We work on three specific problems.", "labels": [], "entities": []}, {"text": "First, we study the effect of dependency-based word embeddings for analogy detection.", "labels": [], "entities": [{"text": "analogy detection", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.8724392354488373}]}, {"text": "There are two variations of Mikolov et al's skip-gram embedding model, one training the distributed word representation of a word using its context words in local ngram window (, and the other training the distributed representation of a word using words in a syntactic dependency context ().", "labels": [], "entities": []}, {"text": "The latter has attracted much recent atten-tion due to its potential in capturing more syntactic regularities.", "labels": [], "entities": []}, {"text": "It has been shown to outperform the former in a variety of NLP tasks, and can potentially also improve relation similarity.", "labels": [], "entities": []}, {"text": "Our experiments on both English and Chinese show that the dependency-context embeddings consistently under-perform ngram-context embeddings.", "labels": [], "entities": []}, {"text": "We give some theoretical justifications to the findings.", "labels": [], "entities": []}, {"text": "Second, we propose to use syntactic dependencies as a context for improving embeddingbased analogy detection, pruning the search space and filtering noise using syntactic dependencies.", "labels": [], "entities": [{"text": "embeddingbased analogy detection", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.7415846387545267}]}, {"text": "While highly useful for measuring relational similarities, attributional similarities between words are not the only source of information for analogy detection.", "labels": [], "entities": [{"text": "analogy detection", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.891187459230423}]}, {"text": "Traditional methods, such as Turney and, Turney (2006), and\u00b4Oand\u00b4 and\u00b4O, also leverage context between word pairs in a corpus for better accuracies, which the current embedding-based methods ignore.", "labels": [], "entities": []}, {"text": "Results show that our proposed method achieves significant improvements for this task.", "labels": [], "entities": []}, {"text": "Third, we show that a novel distributed representation of syntactic dependencies between word pairs can be used to mine analogous dependencies from a large Chinese corpus.", "labels": [], "entities": []}, {"text": "Inspired by the fact that distributed word representations can be used to measure word similarities, we use our distributed dependency representations to measure relation similarities.", "labels": [], "entities": []}, {"text": "We propose a bootstrapping algorithm for analogy mining using dependency embeddings, and experiments on a large Chinese corpus show that the method can achieve a precision of 95.2% at a recall of 56.8%.", "labels": [], "entities": [{"text": "analogy mining", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.868218183517456}, {"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9988076686859131}, {"text": "recall", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.998954176902771}]}, {"text": "Our automatically-parsed corpus, trained embeddings and evaluation datasets are released publicly at http://people.sutd.edu.sg/ \u02dc yue_zhang/publication.html.", "labels": [], "entities": []}, {"text": "To our knowledge, we are the first to present results on Chinese analogy detection and to release largescale Chinese word embeddings.", "labels": [], "entities": [{"text": "Chinese analogy detection", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.7698661883672079}]}], "datasetContent": [{"text": "Three datasets are used for evaluating Chinese embeddings.", "labels": [], "entities": []}, {"text": "First, we construct a set of semantic analogy questions.", "labels": [], "entities": []}, {"text": "This set contains five types of semantic analogy questions, including capital-country (136 word pairs, and 18354 analogy questions), provincial capital-province, city-province (637, 386262), family member (male-female) and currency-country.", "labels": [], "entities": []}, {"text": "We collect the five types of word pairs and then produce analogy questions automatically by concatenating two word pairs.", "labels": [], "entities": []}, {"text": "The resulting analogy dataset contains 400K analogy questions.", "labels": [], "entities": []}, {"text": "We refer to this dataset as the Chinese Analogy Question Set (CAQS).", "labels": [], "entities": [{"text": "Chinese Analogy Question Set (CAQS)", "start_pos": 32, "end_pos": 67, "type": "DATASET", "confidence": 0.9270528554916382}]}, {"text": "This dataset consists of 297 word pairs.", "labels": [], "entities": []}, {"text": "The second one is the Chinese thesaurus Tongyicicilin (Cilin) (, which groups 74,000 Chinese words into five-layer hierarchies and has been used for evaluating the accuracy of word similarity by traditional sparse vector space models ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9929966330528259}, {"text": "word similarity", "start_pos": 176, "end_pos": 191, "type": "TASK", "confidence": 0.6033229976892471}]}, {"text": "The third level of Cilin, which contains 1428 classes, is used to evaluate whether two words are semantically similar.", "labels": [], "entities": []}, {"text": "For comparison between Chinese and English, we also use an English analogy question dataset, the Google dataset 7 (, to evaluate the English word embeddings of on analogy detection.", "labels": [], "entities": [{"text": "Google dataset 7", "start_pos": 97, "end_pos": 113, "type": "DATASET", "confidence": 0.9023100336392721}, {"text": "analogy detection", "start_pos": 163, "end_pos": 180, "type": "TASK", "confidence": 0.7193880826234818}]}, {"text": "On both the CAQS and the Google datasets, the 3COSMUL method () is used to to answer analogy questions based on given embeddings.", "labels": [], "entities": [{"text": "CAQS", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.9220041632652283}, {"text": "Google datasets", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.9343697428703308}]}, {"text": "The results on the CWS dataset are evaluated using the two standard metrics for the task, namely Spearman's \u03c1 and Kendall's \u03c4 rank correlation coefficients.", "labels": [], "entities": [{"text": "CWS dataset", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.775835394859314}, {"text": "Kendall's \u03c4 rank correlation", "start_pos": 114, "end_pos": 142, "type": "METRIC", "confidence": 0.6405233263969421}]}, {"text": "The results on Cilin are evaluated using Precision@K: the percentage of words from the top-K candidates that belong to the Cilin category of the target word.", "labels": [], "entities": [{"text": "Precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.951852023601532}]}, {"text": "If one of the top-K candidates belongs to the same third-level category in Cilin as the target word, the candidate word is taken as correct.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on Cilin and CWS.", "labels": [], "entities": [{"text": "CWS", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.676034152507782}]}, {"text": " Table 2: Results on CAQS. MUL and IMP indi- cate 3COSMUL and our improved method, re- spectively.", "labels": [], "entities": [{"text": "CAQS. MUL", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.6198000907897949}]}, {"text": " Table 3: English results on the Google set.", "labels": [], "entities": [{"text": "Google set", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.9346525371074677}]}, {"text": " Table 5: Main results of Analogy Mining.", "labels": [], "entities": [{"text": "Analogy Mining", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.9384729266166687}]}]}