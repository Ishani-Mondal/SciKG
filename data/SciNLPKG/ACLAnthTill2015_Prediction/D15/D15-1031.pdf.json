{"title": [{"text": "Aligning Knowledge and Text Embeddings by Entity Descriptions", "labels": [], "entities": [{"text": "Aligning Knowledge and Text Embeddings by Entity Descriptions", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.7453173696994781}]}], "abstractContent": [{"text": "We study the problem of jointly embedding a knowledge base and a text corpus.", "labels": [], "entities": []}, {"text": "The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space.", "labels": [], "entities": []}, {"text": "(2014a) rely on Wikipedia anchors , making the applicable scope quite limited.", "labels": [], "entities": []}, {"text": "In this paper we propose anew alignment model based on text descriptions of entities, without dependency on anchors.", "labels": [], "entities": []}, {"text": "We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description.", "labels": [], "entities": []}, {"text": "Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of Wang et al.", "labels": [], "entities": []}, {"text": "(2014a), which is encouraging as we do not use any anchor information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Knowledge base embedding has attracted surging interest recently.", "labels": [], "entities": []}, {"text": "The aim is to learn continuous vector representations (embeddings) for entities and relations of a structured knowledge base (KB) such as Freebase.", "labels": [], "entities": []}, {"text": "Typically it optimizes a global objective function overall the facts in the KB and hence the embedding vector of an entity / relation is expected to encode global information in the KB.", "labels": [], "entities": []}, {"text": "It is capable of reasoning missing facts in a KB and helping facts extraction.", "labels": [], "entities": [{"text": "reasoning missing facts in a KB", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.7463612457116445}, {"text": "facts extraction", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7951143085956573}]}, {"text": "Although seeming encouraging, the approaches in the aforementioned literature suffer from two common issues: (1) Embeddings are exclusive to entities/relations within KBs.", "labels": [], "entities": []}, {"text": "Computation between KBs and text cannot be handled, which are prevalent in practice.", "labels": [], "entities": []}, {"text": "For example, in fact extraction, a candidate value maybe just a phrase in text.", "labels": [], "entities": [{"text": "fact extraction", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7435164749622345}]}, {"text": "(2) KB sparsity.", "labels": [], "entities": [{"text": "KB", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.7061302661895752}, {"text": "sparsity", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.5700063705444336}]}, {"text": "The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts.", "labels": [], "entities": []}, {"text": "An important milestone, the approach of solves issue (1) by jointly embedding entities, relations, and words into the same vector space and hence is able to deal with words/phrases beyond entities in KBs.", "labels": [], "entities": []}, {"text": "The key component is the so-called alignment model, which makes sure the embeddings of entities, relations, and words are in the same space.", "labels": [], "entities": []}, {"text": "Two alignment models are introduced there: one uses entity names and another uses Wikipedia anchors.", "labels": [], "entities": []}, {"text": "However, both of them have drawbacks.", "labels": [], "entities": []}, {"text": "As reported in the paper, using entity names severely pollutes the embeddings of words.", "labels": [], "entities": []}, {"text": "Thus it is not recommended in practice.", "labels": [], "entities": []}, {"text": "Using Wikipedia anchors completely relies on the special data source and hence the approach cannot be applied to other customer data.", "labels": [], "entities": []}, {"text": "To fully address the two issues, this paper proposes anew alignment method, aligning by entity descriptions.", "labels": [], "entities": []}, {"text": "We only assume some entities in KBs have text descriptions, which almost always holds in practice.", "labels": [], "entities": []}, {"text": "We require the embedding of an entity not only fits the structured constraints in KBs but also equals the vector computed from the text description.", "labels": [], "entities": []}, {"text": "Meanwhile, if an entity has few facts, the description will provide information for embedding, thus the issue of KB sparsity is also well handled.", "labels": [], "entities": []}, {"text": "We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach (.", "labels": [], "entities": [{"text": "triplet classification", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.8676488995552063}, {"text": "link prediction", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.8664425611495972}, {"text": "relational fact extraction", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.8048970103263855}, {"text": "analogical reasoning", "start_pos": 122, "end_pos": 142, "type": "TASK", "confidence": 0.8877240717411041}]}, {"text": "Results show that our approach consistently achieves better or comparable performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on the following tasks:   to study whether the proposed alignment model, without using any anchor information, is able to achieve comparable or better performance than alignment by anchors.", "labels": [], "entities": []}, {"text": "As to the methods, \"Separately\" denotes the method of separately embedding knowledge bases and text.", "labels": [], "entities": []}, {"text": "\"Jointly(anchor)\" and \"Jointly(name)\" denote the jointly embedding methods based on Alignment by Wikipedia Anchors and Alignment by Entity Names in (Wang et al., 2014a) respectively.", "labels": [], "entities": []}, {"text": "\"Jointly(desp)\" is the joint embedding method based on alignment by entity descriptions.", "labels": [], "entities": []}, {"text": "Data For link prediction, FB15K from ( ) is used as the knowledge base.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.8684185147285461}, {"text": "FB15K", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.8209066390991211}]}, {"text": "For triplet classification, a large dataset provided by () is used as the knowledge base.", "labels": [], "entities": [{"text": "triplet classification", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8967801332473755}]}, {"text": "Both sets are subsets of Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.9816993474960327}]}, {"text": "For all tasks, Wikipedia articles are used as the text corpus.", "labels": [], "entities": []}, {"text": "As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 144, "end_pos": 152, "type": "DATASET", "confidence": 0.9547016024589539}]}, {"text": "Following the settings in (), we apply the same preprocessing steps, including sentence segmentation, tokenization, and named entity recognition.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.7540202438831329}, {"text": "tokenization", "start_pos": 102, "end_pos": 114, "type": "TASK", "confidence": 0.9671252965927124}, {"text": "named entity recognition", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.6080836157004038}]}, {"text": "We combine the consecutive tokens covered by an anchor or identically tagged as \"Location/Person/Organization\" and regard them as phrases.", "labels": [], "entities": []}, {"text": "Link Prediction This task aims to complete a fact (h, r, t) in absence of h or t, simply based on h + r \u2212 t.", "labels": [], "entities": [{"text": "Link Prediction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6932896822690964}]}, {"text": "We follow the same protocol in).", "labels": [], "entities": []}, {"text": "We directly copy the results of the baseline (TransE) from  and implement \"Jointly(anchor)\".", "labels": [], "entities": []}, {"text": "The results are in 10 predictions containing the true entity.", "labels": [], "entities": []}, {"text": "Lower \"MEAN\" and higher \"HITS@10\" is better.", "labels": [], "entities": [{"text": "MEAN", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9663175940513611}, {"text": "HITS@10\"", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9415169507265091}]}, {"text": "\"Raw\" and \"Filtered\" are two settings on processing candidates ( ).", "labels": [], "entities": [{"text": "Filtered", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9351188540458679}]}, {"text": "We train \"Jointly(anchor)\" and \"Jointly(desp)\" with the embedding dimension k among {50, 100, 150}, the learning rate \u03b1 in {0.01, 0.025}, the number of negative examples per positive example c in {5, 10}, the max skiprange sin {5, 10} and traverse the text corpus with only 1 epoch.", "labels": [], "entities": [{"text": "learning rate \u03b1", "start_pos": 104, "end_pos": 119, "type": "METRIC", "confidence": 0.9067902763684591}]}, {"text": "The best configurations of \"Jointly(anchor)\" and \"Jointly(desp)\" are exactly the same: k = 100, \u03b1 = 0.025, c = 10, s = 5.", "labels": [], "entities": []}, {"text": "From the results, we observe that: (1) Both jointly embedding methods are much better than the baseline TransE, which demonstrates that external textual resources make entity embeddings become more discriminative.", "labels": [], "entities": []}, {"text": "Intuitively, \"Jointly(anchor)\" indicates \"how to use an entity in text\", while \"Jointly(desp)\" shows \"what is the definition/meaning of an entity\".", "labels": [], "entities": []}, {"text": "Both are helpful to distinguish an entity from others.", "labels": [], "entities": []}, {"text": "(2) Under the setting of \"Raw\", \"Jointly(desp)\" and \"Jointly(anchor)\" are comparable.", "labels": [], "entities": []}, {"text": "In other settings \"Jointly(desp)\" wins.", "labels": [], "entities": []}, {"text": "Triplet Classification This is a binary classification task, predicting whether a candidate triplet (h, r, t) is a correct factor not.", "labels": [], "entities": [{"text": "Triplet Classification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7473477423191071}]}, {"text": "It is used in).", "labels": [], "entities": []}, {"text": "We follow the same protocol in ().", "labels": [], "entities": []}, {"text": "We train their models via our own implementation on our dataset.", "labels": [], "entities": []}, {"text": "\"e-e\" means both sides of a triplet (h, r, t) are entities in KB, \"e-w\" means the tail side is a word out of KB entity vocabulary, similarly for \"w-e\" and \"w-w\".", "labels": [], "entities": []}, {"text": "The best configurations of the models are: k = 150, \u03b1 = 0.025, c = 10, s = 5 and traversing the text corpus with 6 epochs.", "labels": [], "entities": []}, {"text": "The results reveal that: (1) Jointly embedding is indeed effective.", "labels": [], "entities": []}, {"text": "Both jointly embedding methods can well handle the cases of \"e-w\", \"w-e\" and \"ww\", which means the vector computation between entities/relations and words are really meaningful.", "labels": [], "entities": []}, {"text": "Meanwhile, even the case of \"e-e\" is also improved.", "labels": [], "entities": []}, {"text": "(2) Our method, \"Jointly(desp)\", outperforms \"Jointly(anchor)\" on all types of triplets.", "labels": [], "entities": []}, {"text": "We believe that the good performance of \"Jointly(desp)\" is due to the appropriate design of the alignment mechanism.", "labels": [], "entities": []}, {"text": "Using entity's description information is a more straightforward and effective way to align entity embeddings and word embeddings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Link prediction results.", "labels": [], "entities": [{"text": "Link prediction", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7728310227394104}]}, {"text": " Table 2: Triplet classification results.", "labels": [], "entities": [{"text": "Triplet classification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.9822931289672852}]}, {"text": " Table 2.  \"e-e\" means both sides of a triplet (h, r, t) are en- tities in KB, \"e-w\" means the tail side is a word  out of KB entity vocabulary, similarly for \"w-e\"  and \"w-w\". The best configurations of the mod- els are: k = 150, \u03b1 = 0.025, c = 10, s = 5 and  traversing the text corpus with 6 epochs.  The results reveal that: (1) Jointly embedding is  indeed effective. Both jointly embedding methods  can well handle the cases of \"e-w\", \"w-e\" and \"w- w\", which means the vector computation between  entities/relations and words are really meaning- ful. Meanwhile, even the case of \"e-e\" is also  improved. (2) Our method, \"Jointly(desp)\", out- performs \"Jointly(anchor)\" on all types of triplets.  We believe that the good performance of \"Joint- ly(desp)\" is due to the appropriate design of the  alignment mechanism. Using entity's description  information is a more straightforward and effec- tive way to align entity embeddings and word em- beddings.", "labels": [], "entities": []}, {"text": " Table 3: Analogical reasoning results", "labels": [], "entities": [{"text": "Analogical reasoning", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9097248017787933}]}, {"text": " Table 3. \"Acc\" is the  accuracy of the predicted word. \"HITS@10\" is the  accuracy of the top 10 candidates containing the  ground truth. The evaluation analogical pairs are  organized into two groups, \"Words\" and \"Phras- es\", by whether an analogical pair contains phras- es (i.e., multiple words). From the table we ob- serve that: (1) Both \"Jointly(anchor)\" and \"Joint- ly(desp)\" outperform \"Skip-gram\". (2) \"Joint-", "labels": [], "entities": [{"text": "Acc", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9971551895141602}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9976442456245422}, {"text": "HITS", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9901490211486816}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9952201247215271}]}]}