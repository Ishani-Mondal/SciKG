{"title": [{"text": "More Features Are Not Always Better: Evaluating Generalizing Models in Incident Type Classification of Tweets", "labels": [], "entities": [{"text": "Incident Type Classification of Tweets", "start_pos": 71, "end_pos": 109, "type": "TASK", "confidence": 0.7502807199954986}]}], "abstractContent": [{"text": "Social media represents a rich source of up-to-date information about events such as incidents.", "labels": [], "entities": []}, {"text": "The sheer amount of available information makes machine learning approaches a necessity for further processing.", "labels": [], "entities": []}, {"text": "This learning problem is often concerned with regionally restricted datasets such as data from only one city.", "labels": [], "entities": []}, {"text": "Because social media data such as tweets varies considerably across different cities, the training of efficient models requires labeling data from each city of interest , which is costly and time consuming.", "labels": [], "entities": []}, {"text": "In this study, we investigate which features are most suitable for training generalizable models, i.e., models that show good performance across different datasets.", "labels": [], "entities": []}, {"text": "We re-implemented the most popular features from the state of the art in addition to other novel approaches, and evaluated them on data from ten different cities.", "labels": [], "entities": []}, {"text": "We show that many sophisticated features are not necessarily valuable for training a generalized model and are outperformed by classic features such as plain word-n-grams and character-n-grams.", "labels": [], "entities": []}], "introductionContent": [{"text": "Incident information contained in social media has proven to frequently include information not captured by standard emergency channels (e.g. 911 calls, bystander reports).", "labels": [], "entities": []}, {"text": "Therefore, stakeholders like emergency management and city administration can highly benefit from social media.", "labels": [], "entities": [{"text": "emergency management", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.7495487630367279}]}, {"text": "Due to its unstructured and unfocused nature, automatic filtering of social media content is a necessity for further analysis.", "labels": [], "entities": [{"text": "automatic filtering of social media content", "start_pos": 46, "end_pos": 89, "type": "TASK", "confidence": 0.7327114989360174}]}, {"text": "A standard approach for this filtering is automatic classification using a trained machine learning model (.", "labels": [], "entities": [{"text": "automatic classification", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.550147294998169}]}, {"text": "A problem for the classification approach is that language, style and named entities used in social media highly vary across different regions.", "labels": [], "entities": []}, {"text": "Consider the following two tweets as examples: \"RT: @People 0noe friday afternoon in heavy traffic, car crash on I-90, right lane closed\" and \"Road blocked due to traffic collision on I-495\".", "labels": [], "entities": [{"text": "RT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.5395301580429077}]}, {"text": "Both tweets comprise entities that might refer to the same thing with different wording, either on a semantically low (\"accident\" and \"car collision\") or more abstract level (\"I90\" and \"I-495\").", "labels": [], "entities": []}, {"text": "With simple syntactical text similarity approaches using standard bag of words features, it is not easily possible to make use of this semantic similarity, even though it is highly valuable for classification.", "labels": [], "entities": []}, {"text": "These limitations impose constraints on the dataset, because tokens are likely to be related to the location where the text was created or contain location-or incident-sensitive topics.", "labels": [], "entities": []}, {"text": "Models trained using spatially and temporally restricted data from one region are bound by the specific aspects of language and style expressed in the training data, thus, model reuse is not easily possible.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the creation of generalized models.", "labels": [], "entities": []}, {"text": "Such models avoid the use of features that -overfitting like -are only useful fora specific region.", "labels": [], "entities": []}, {"text": "Generalized models are intended to work in different regions, even if training data originates only from one ore few regions.", "labels": [], "entities": []}, {"text": "This can ensure high classification rates even in areas where only few training samples are available.", "labels": [], "entities": [{"text": "classification", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.9471466541290283}]}, {"text": "Finally, in times of increasing growth of cities and the merging with surrounding towns to large metropolitan areas, they allow to cope with the latent transitions in token use.", "labels": [], "entities": []}, {"text": "To create generalized models for incident type classification (and social media classification in general) the most important step is an appropriate feature generation.", "labels": [], "entities": [{"text": "incident type classification", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.7605259815851847}, {"text": "social media classification", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.7313184340794882}, {"text": "feature generation", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.7678754925727844}]}, {"text": "Therefore, in this paper we investigate the suitability of standard and novel features and different machine learning algorithms for the creation of generalized classification models for incident type classification.", "labels": [], "entities": [{"text": "incident type classification", "start_pos": 187, "end_pos": 215, "type": "TASK", "confidence": 0.6577815810839335}]}, {"text": "We conduct intensive feature engineering and evaluation.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7180620431900024}]}, {"text": "For this purpose, we have collected and labeled 10 datasets with high regional variation.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first investigation of the challenges of heterogeneous datasets in this domain, and of the suitability of state of the art classification and feature extraction techniques.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.7153744250535965}]}, {"text": "In summary, our contributions are: 1) Investigation of features and feature groups for generalized social media/incident type classification models.", "labels": [], "entities": [{"text": "social media/incident type classification", "start_pos": 99, "end_pos": 140, "type": "TASK", "confidence": 0.6964847892522812}]}, {"text": "2) Identification of the best feature combinations and classifiers fora generalized model.", "labels": [], "entities": []}, {"text": "For an evaluation (qualitative and inferential statistics) often tweet datasets with high regional variation we get an overall F-measure of > 83%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9991775155067444}]}, {"text": "3) The evaluation shows that features extending a plain ngram-based approach are not necessarily valuable for training a generalized model as these provide little improvement.", "labels": [], "entities": []}, {"text": "Following this introduction, we give an overview of related work in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we provide a description of our datasets followed by a comprehensive evaluation in Section 4.", "labels": [], "entities": []}, {"text": "We close with our conclusion and future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our evaluation is to determine which features were most useful for creating a generalizing model.", "labels": [], "entities": []}, {"text": "We first describe our method, including the feature sets, the classification algorithms used, and our sampling procedure.", "labels": [], "entities": []}, {"text": "This is followed by a results section in which we report differences in performance by means of qualitative and inferential statistics.", "labels": [], "entities": []}, {"text": "We first evaluated which of our 20 baseline feature sets, as described in Section 3, lead to the best classification performance over different datasets.", "labels": [], "entities": []}, {"text": "Notably, the ten best-performing approaches were all combinations of word-n-grams.", "labels": [], "entities": []}, {"text": "contains the average F-Measures for these approaches.", "labels": [], "entities": [{"text": "F-Measures", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9914712309837341}]}, {"text": "The Friedman test indicated strong significant differences between the performances of these groups (\u03c7 2 r (9) = 112.467, p < 0.001, \u03b1 = 0.01).", "labels": [], "entities": []}, {"text": "The subsequent Nemenyi test indicated strong significant pairwise differences between the performances of the models (\u03b1 = 0.01), with pvalues listed in in the supplementary.", "labels": [], "entities": []}, {"text": "illustrates the differences by means of a CD diagram: the approaches of using simple unigrams of the most frequent 5000 and all words provide the best results, i.e. they have the lowest rank.", "labels": [], "entities": []}, {"text": "They are not significantly different from the 1000 most frequent word-uni and bigrams.", "labels": [], "entities": []}, {"text": "Nevertheless, they are significantly better than all other baseline approaches.", "labels": [], "entities": []}, {"text": "This also applies to the char-n-gram approaches, that were not considered in this statisti- cal comparison due to their inferior performance.", "labels": [], "entities": []}, {"text": "It is important to note that the differences between the worst word-n-gram approaches and the best char-n-gram approaches could still be statistically non-significant.", "labels": [], "entities": []}, {"text": "The best performing baseline approach for LibLinear is using unigrams of the top 5000 words, i.e. words(5000,1,1), with F 1 = 82.87.", "labels": [], "entities": [{"text": "F 1", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9831748008728027}]}, {"text": "We therefore picked this baseline feature group for the second part of our evaluation.", "labels": [], "entities": []}, {"text": "We added each non-baseline feature individually to the selected baseline approach and compared the performances of these combinations and the non-extended baseline group.", "labels": [], "entities": []}, {"text": "When comparing the ten best-performing groups, the Friedman test showed strong significant differences between the performances of the models (\u03c7 2 r (9) = 87.274, p < 0.001, \u03b1 = 0.01).", "labels": [], "entities": []}, {"text": "The Nemenyi test partly showed strong significant differences between the performances of the models (for the corresponding p-values see in the supplementary).", "labels": [], "entities": []}, {"text": "They are illustrated in the CD diagram in.", "labels": [], "entities": []}, {"text": "The tests indicate that adding NER and NR AT MT to the baseline approach provides the best performances with F 1 = 83.32 and F 1 = 83.03 respectively.", "labels": [], "entities": [{"text": "NER", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.5403515696525574}, {"text": "NR AT", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.574262946844101}, {"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.4534507095813751}, {"text": "F 1 = 83.32", "start_pos": 109, "end_pos": 120, "type": "METRIC", "confidence": 0.9482425302267075}, {"text": "F 1", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.976525068283081}]}, {"text": "Finally, we evaluated the power set of these feature groups, i.e. we compared the individual groups and their combination.", "labels": [], "entities": []}, {"text": "contains the corresponding averaged F-Measures.", "labels": [], "entities": [{"text": "F-Measures", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.8385547995567322}]}, {"text": "For the resulting performance samples, the Friedman test showed strong significant differences between the models (\u03c7 in the supplementary and illustrated in.", "labels": [], "entities": []}, {"text": "The diagram shows that the combination of NER and NR AT MN with the words(5000,1,1) baseline outperforms all other models with respect to F1 (F 1 = 83.48), but does not differ significantly from the plain NER approach (F 1 = 83.32).", "labels": [], "entities": [{"text": "NER", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.7189052104949951}, {"text": "F1", "start_pos": 138, "end_pos": 140, "type": "METRIC", "confidence": 0.9961386322975159}, {"text": "F 1", "start_pos": 142, "end_pos": 145, "type": "METRIC", "confidence": 0.9534039199352264}, {"text": "F 1", "start_pos": 219, "end_pos": 222, "type": "METRIC", "confidence": 0.9583689868450165}]}, {"text": "This combination gives us the final and best feature set for training a generalizing model over our datasets.", "labels": [], "entities": []}, {"text": "As can be seen, the plain n-gram approach (F 1 = 82.87) can be improved further by 0.5%.", "labels": [], "entities": [{"text": "F 1", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.982930064201355}]}, {"text": "In this section, we repeat the previous steps for the NB classifier.", "labels": [], "entities": []}, {"text": "As baseline feature sets, we first evaluated the word-n-gram and char-n-gram approaches.", "labels": [], "entities": []}, {"text": "The averaged F-Measures can be found in.", "labels": [], "entities": [{"text": "F-Measures", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9627388119697571}]}, {"text": "The Friedman test showed strong significant differences between the performances of the models (\u03c7 2 r (9) = 110.293, p < 0.001, \u03b1 = 0.01).", "labels": [], "entities": []}, {"text": "The Nemenyi test partly showed strong significant differences between the performances of the models (for the corresponding p-values see Table 1 in the supplementary).", "labels": [], "entities": []}, {"text": "In contrast to the LibLinear classifier, using the 5000 most frequent combinations of two to five subsequent characters, i.e. chars(5000,2,5) provide the best F1 score (F 1 = 80.48).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.985742062330246}, {"text": "F 1", "start_pos": 169, "end_pos": 172, "type": "METRIC", "confidence": 0.9328554272651672}]}, {"text": "Thus, char-n-grams outperform the word-n-gram approaches with respect to F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9937304258346558}]}, {"text": "The CD diagram in shows that using either the 5000 most frequent char-n-grams with a length of two to five and two to four respectively, the 1000 most frequent word-n-grams with a length of one and one to two respectively, and the 1000 most frequent char-n-grams with a length of two to four do not differ significantly.", "labels": [], "entities": []}, {"text": "However, using either the 5000 most frequent char-n-grams with a length of two to five and two to four respectively significantly outperform all other baseline approaches.", "labels": [], "entities": []}, {"text": "As a subsequent step, we added each single feature to chars(5000,2,5) as the best baseline approach to find if these provide better performance for the NB classifier.", "labels": [], "entities": []}, {"text": "contains the corresponding averaged F-Measures.", "labels": [], "entities": [{"text": "F-Measures", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.8385547995567322}]}, {"text": "Though the Friedman test indicated strong significant differences between the performances of the models (\u03c7 2 r (9) = 22.209, p = 0.008, \u03b1 = 0.01), the subsequent Nemenyi test did not indicate significant pairwise differences.", "labels": [], "entities": []}, {"text": "We can therefore not repeat the third step of our evaluation, but infer that fora NB classifier, the plain char-n-gram-based approach is sufficient for training a generalizing model for our dataset.", "labels": [], "entities": []}, {"text": "The results indicate that LibLinear provides a better avg.", "labels": [], "entities": [{"text": "avg", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9659043550491333}]}, {"text": "performance (F 1 = 83.32) when training a generalizing model, compared to the NB classifier (F 1 = 80.48).", "labels": [], "entities": [{"text": "F 1", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9828748404979706}, {"text": "F 1", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9520112872123718}]}], "tableCaptions": [{"text": " Table 1: Overview of related approaches for incident type classification. (NEs = Named Entities)", "labels": [], "entities": [{"text": "incident type classification", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.8311851223309835}]}, {"text": " Table 2: Class distributions for all datasets.", "labels": [], "entities": []}, {"text": " Table 4: Average F1-Measure F 1 for the ten best performing baseline feature groups", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9716441035270691}, {"text": "F1-Measure F 1", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.950256367524465}]}, {"text": " Table 5: Average F1-Measure F 1 for the power set  of best performing feature groups and LibLinear.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.950776219367981}, {"text": "F1-Measure F 1", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.9070139328638712}, {"text": "LibLinear", "start_pos": 90, "end_pos": 99, "type": "DATASET", "confidence": 0.8294023871421814}]}, {"text": " Table 6: Average F1-Measure F 1 for the ten best performing combinations of the best baseline and an  additional feature", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9728350639343262}, {"text": "F1-Measure F 1", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.9443268974622091}]}]}