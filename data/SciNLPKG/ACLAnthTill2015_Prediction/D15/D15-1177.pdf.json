{"title": [{"text": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning", "labels": [], "entities": [{"text": "Deep Compositional Models of Meaning", "start_pos": 45, "end_pos": 81, "type": "TASK", "confidence": 0.6826582193374634}]}], "abstractContent": [{"text": "Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research.", "labels": [], "entities": []}, {"text": "We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence.", "labels": [], "entities": []}, {"text": "Embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context.", "labels": [], "entities": []}, {"text": "Furthermore, each word is associated with a number of senses, the most plausible of which is selected dynamically during the composition process.", "labels": [], "entities": []}, {"text": "We evaluate the produced vectors qualitatively and quantitatively with positive results.", "labels": [], "entities": []}, {"text": "At the sentence level, the effectiveness of the framework is demonstrated on the MSRPar task, for which we report results within the state-of-the-art range.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing the meaning of words by using their distributional behaviour in a large text corpus is a well-established technique in NLP research that has been proved useful in numerous tasks.", "labels": [], "entities": [{"text": "Representing the meaning of words", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9141220569610595}]}, {"text": "Ina distributional model of meaning, the semantic representation of a word is given as a vector in some high dimensional vector space, obtained either by explicitly collecting co-occurrence statistics of the target word with words belonging to a representative subset of the vocabulary, or by directly optimizing the word vectors against an objective function in some neural-network based architecture.", "labels": [], "entities": []}, {"text": "Regardless their method of construction, distributional models of meaning do not scale up to larger text constituents such as phrases or sentences, since the uniqueness of multi-word expressions would inevitably lead to data sparsity problems, thus to unreliable vectorial representations.", "labels": [], "entities": []}, {"text": "The problem is usually addressed by the provision of a compositional function, the purpose of which is to prepare a vectorial representation fora phrase or sentence by combining the vectors of the words therein.", "labels": [], "entities": []}, {"text": "While the nature and complexity of these compositional models may vary, approaches based on deep-learning architectures have been shown to be especially successful in modelling the meaning of sentences fora variety of tasks ().", "labels": [], "entities": []}, {"text": "The mutual interaction of distributional word vectors by a means of a compositional model provides many opportunities for interesting research, the majority of which still remains to be explored.", "labels": [], "entities": []}, {"text": "One such direction is to investigate in what way lexical ambiguity affects the compositional process.", "labels": [], "entities": []}, {"text": "In fact, recent work has shown that shallow multi-linear compositional models that explicitly handle extreme cases of lexical ambiguity in a step prior to composition present consistently better performance than their \"ambiguous\" counterparts ( . A first attempt to test these observations in a deep compositional setting has been presented by with promising results.", "labels": [], "entities": []}, {"text": "Furthermore, a second important question relates to the very nature of the word embeddings used in the context of a compositional model.", "labels": [], "entities": []}, {"text": "Ina setting of this form, word vectors are not anymore just a means for discriminating words based on their underlying semantic relationships; the main goal of a word vector is to contribute to a bigger whole-a task in which syntax, along with semantics, also plays a very important role.", "labels": [], "entities": []}, {"text": "It is a central point of this paper, therefore, that in a compositional distributional model of meaning word vectors should be injected with information that reflects their syntactical roles in the training corpus.", "labels": [], "entities": []}, {"text": "The purpose of this work is to improve the current practice in deep compositional models of meaning in relation to both the compositional process itself and the quality of the word embeddings used therein.", "labels": [], "entities": []}, {"text": "We propose an architecture for jointly training a compositional model and a set of word embeddings, in away that imposes dynamic word sense induction for each word during the learning process.", "labels": [], "entities": []}, {"text": "Note that this is in contrast with recent work in multi-sense neural word embeddings (, in which the word senses are learned without any compositional considerations in mind.", "labels": [], "entities": []}, {"text": "Furthermore, we make the word embeddings syntax-aware by introducing a variation of the hinge loss objective function of, in which the goal is not only to predict the occurrence of a target word in a context, but to also predict the position of the word within that context.", "labels": [], "entities": [{"text": "hinge loss objective function", "start_pos": 88, "end_pos": 117, "type": "METRIC", "confidence": 0.8387807309627533}]}, {"text": "A qualitative analysis shows that our vectors reflect both semantic and syntactic features in a concise way.", "labels": [], "entities": []}, {"text": "In all current deep compositional distributional settings, the word embeddings are internal parameters of the model with no use for any other purpose than the task for which they were specifically trained.", "labels": [], "entities": []}, {"text": "In this work, one of our main considerations is that the joint training step should be generic enough to not be tied in any particular task.", "labels": [], "entities": []}, {"text": "In this way the word embeddings and the derived compositional model can be learned on data much more diverse than any task-specific dataset, reflecting a wider range of linguistic features.", "labels": [], "entities": []}, {"text": "Indeed, experimental evaluation shows that the produced word embeddings can serve as a high quality general-purpose semantic word space, presenting performance on the Stanford Contextual Word Similarity (SCWS) dataset of competitive to and even better of the performance of well-established neural word embeddings sets.", "labels": [], "entities": [{"text": "Stanford Contextual Word Similarity (SCWS) dataset", "start_pos": 167, "end_pos": 217, "type": "DATASET", "confidence": 0.6483075171709061}]}, {"text": "Finally, we propose a dynamic disambiguation framework fora number of existing deep compositional models of meaning, in which the multisense word embeddings and the compositional model of the original training step are further refined according to the purposes of a specific task at hand.", "labels": [], "entities": []}, {"text": "In the context of paraphrase detection, we achieve a result very close to the current state-ofthe-art on the Microsoft Research Paraphrase Corpus).", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.9847721755504608}, {"text": "Microsoft Research Paraphrase Corpus", "start_pos": 109, "end_pos": 145, "type": "DATASET", "confidence": 0.8961722552776337}]}, {"text": "An interesting aspect at the sideline of the paraphrase detection experiment is that, in contrast to mainstream approaches that mainly rely on simple forms of classifiers, we approach the problem by following a siamese architecture).", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.9548558592796326}]}], "datasetContent": [{"text": "We evaluate the quality of the compositional word vectors and the proposed deep compositional framework in the tasks of word similarity and paraphrase detection, respectively.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 120, "end_pos": 135, "type": "TASK", "confidence": 0.7230880856513977}, {"text": "paraphrase detection", "start_pos": 140, "end_pos": 160, "type": "TASK", "confidence": 0.8359696865081787}]}, {"text": "As a first step, we qualitatively evaluate the trained word embeddings by examining the nearest neighbours lists of a few selected words.", "labels": [], "entities": []}, {"text": "We compare the results with those produced by the skipgram model (SG) of and the language model (CW) of.", "labels": [], "entities": []}, {"text": "We refer to our model as SAMS (SyntaxAware Multi-Sense).", "labels": [], "entities": []}, {"text": "The results in show clearly that our model tends to group words that are both semantically and syntactically related; for example, and in contrast with the compared models which group words only at the semantic level, our model is able to retain tenses, numbers (singulars and plurals), and gerunds.", "labels": [], "entities": []}, {"text": "The observed behaviour is comparable to that of embedding models with objective functions conditioned on grammatical relations between words;, for example, present a similar table for their dependency-based extension of the skip-gram model.", "labels": [], "entities": []}, {"text": "The advantage of our approach against such models is twofold: firstly, the word embeddings are accompanied by a generic compositional model that can be used for creating sentence representations independently of any specific task; and secondly, the training is quite forgiving to data sparsity problems that in general a dependency-based approach would intensify (since context words are paired with the grammatical relations they occur with the target word).", "labels": [], "entities": []}, {"text": "As a result, a small corpus such as the BNC is sufficient for producing high quality syntax-aware word embeddings.: Nearest neighbours fora number of words with various embedding models.", "labels": [], "entities": [{"text": "BNC", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.9205262064933777}]}], "tableCaptions": [{"text": " Table 2: Results for the word similarity task  (Spearman's \u03c1 \u00d7 100).", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7721956074237823}]}, {"text": " Table 3: Results with different error functions for  the paraphrase detection task (accuracy \u00d7 100).", "labels": [], "entities": [{"text": "paraphrase detection task", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.9358142614364624}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9988396763801575}]}, {"text": " Table 4: Different disambiguation choices for the  paraphrase detection task (accuracy \u00d7 100).", "labels": [], "entities": [{"text": "paraphrase detection task", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.9117293159166971}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9983479976654053}]}, {"text": " Table 5: Results with average pooling for the para- phrase detection task (accuracy \u00d7 100).", "labels": [], "entities": [{"text": "para- phrase detection task", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6611555755138397}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.998765230178833}]}, {"text": " Table 6: Cross-model comparison in the para- phrase detection task.", "labels": [], "entities": [{"text": "para- phrase detection task", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.6498963296413421}]}]}