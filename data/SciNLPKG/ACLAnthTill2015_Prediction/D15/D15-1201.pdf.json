{"title": [{"text": "Learning Semantic Composition to Detect Non-compositionality of Multiword Expressions", "labels": [], "entities": []}], "abstractContent": [{"text": "Non-compositionality of multiword expressions is an intriguing problem that can be the source of error in a variety of NLP tasks such as language generation , machine translation and word sense disambiguation.", "labels": [], "entities": [{"text": "language generation", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.7376415729522705}, {"text": "machine translation", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.8126612603664398}, {"text": "word sense disambiguation", "start_pos": 183, "end_pos": 208, "type": "TASK", "confidence": 0.7070327599843343}]}, {"text": "We present methods of non-compositionality detection for En-glish noun compounds using the unsu-pervised learning of a semantic composition function.", "labels": [], "entities": [{"text": "non-compositionality detection", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.7183222472667694}]}, {"text": "Compounds which are not well modeled by the learned semantic composition function are considered non-compositional.", "labels": [], "entities": []}, {"text": "We explore a range of dis-tributional vector-space models for semantic composition, empirically evaluate these models, and propose additional methods which improve results further.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.7756280303001404}]}, {"text": "We show that a complex function such as polynomial projection can learn semantic composition and identify non-compositionality in an unsupervised way, beating all other baselines ranging from simple to complex.", "labels": [], "entities": []}, {"text": "We show that enforcing sparsity is a useful regularizer in learning complex composition functions.", "labels": [], "entities": []}, {"text": "We show further improvements by training a decomposition function in addition to the composition function.", "labels": [], "entities": []}, {"text": "Finally, we propose an EM algorithm over latent compositionality annotations that also improves the performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multiword Expressions (MWEs) are sequences of words that exhibit some kind of idiosyncrasy.", "labels": [], "entities": [{"text": "Multiword Expressions (MWEs) are sequences of words", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.780715372827318}]}, {"text": "This idiosyncrasy can be semantic, statistical, or syntactic . Ivory tower, speed limit, and at large are examples of semantically, statistically and syntactically idiosyncratic MWEs respectively.", "labels": [], "entities": []}, {"text": "Note that an MWE can be idiosyncratic at several levels.", "labels": [], "entities": []}, {"text": "In general, semantically idiosyncratic MWEs are commonly referred to as non-compositional ( and statistically idiosyncratic MWEs are commonly referred to as collocations ().", "labels": [], "entities": []}, {"text": "Non-compositional MWEs are those whose meaning cannot be readily inferred from the meaning of their constituents and collocations are those MWEs whose constituents co-occur more than expected by chance.", "labels": [], "entities": []}, {"text": "Collocations constitute the largest subset of all kinds of MWEs, however, non-compositional ones cause more problems in various NLP tasks, for example word sense disambiguation () and machine translation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 151, "end_pos": 176, "type": "TASK", "confidence": 0.6705416639645895}, {"text": "machine translation", "start_pos": 184, "end_pos": 203, "type": "TASK", "confidence": 0.837667852640152}]}, {"text": "It may also be more challenging to model noncompositionality than collocational weight as the former has to do with modelling the semantics and the latter canto some extent be modeled by conventional statistical measures such as mutual information.", "labels": [], "entities": []}, {"text": "Detecting non-compositionality in an automatic fashion has been the aim of much previous research.", "labels": [], "entities": []}, {"text": "In this paper, we capture non-compositionality of English Noun Compounds (NCs) 2 based on the assumption that the majority of the compounds are compositional, for which a composition function can be learned.", "labels": [], "entities": [{"text": "English Noun Compounds (NCs) 2", "start_pos": 50, "end_pos": 80, "type": "DATASET", "confidence": 0.8789546660014561}]}, {"text": "This implies that the compounds for which a composition function cannot be learned with a relatively low error are noncompositional.", "labels": [], "entities": []}, {"text": "In previous work on vector-space models of distributional semantics, semantic composition has been commonly assumed to be a trivial predetermined function such as addition, multiplication, and their weighted variations.", "labels": [], "entities": []}, {"text": "Nevertheless there is some work that regards composition as a more complex function.", "labels": [], "entities": []}, {"text": "For instance who propose (but doesn't empirically test) the use of Tensor and Convolution products for modelling non-compositionality, who regard adjectives in adjectival-noun compositions as matrices that can be learned by linear regression, and  who present a model that learns phrase composition by means of a recursive neural network.", "labels": [], "entities": [{"text": "phrase composition", "start_pos": 280, "end_pos": 298, "type": "TASK", "confidence": 0.7173603177070618}]}, {"text": "The two latter works show that complex composition models significantly outperform additive and multiplicative functions.", "labels": [], "entities": []}, {"text": "In this work, we too assume that composition is arguably a complex function.", "labels": [], "entities": []}, {"text": "We believe simplified composition functions, such as additive and multiplicative functions and their weighted variations, while having advantages such as being impervious to overfitting, cannot completely capture semantic composition.", "labels": [], "entities": []}, {"text": "Nevertheless modelling composition by means of a powerful function can be equally inadequate for our purposes.", "labels": [], "entities": []}, {"text": "An overly powerful composition function memorizes all compositional and non-compositional compounds, resulting in overfitting and low learning error that hinders discrimination between compositional and non-compositional compounds.", "labels": [], "entities": []}, {"text": "We examine various classes of composition functions, ranging from the least to the most powerful (in terms of learning capacity).", "labels": [], "entities": []}, {"text": "We show that complex functions clearly do a better job in modelling semantic composition and in detecting noncompositionality compared to commonly used additive and multiplicative functions.", "labels": [], "entities": []}, {"text": "Compositional compounds are also decomposable; intuitively, their semantics is the union of the semantics of their components.", "labels": [], "entities": []}, {"text": "More formally, conditioned on the vector of the compound, vectors of the component words should be independently predictable.", "labels": [], "entities": []}, {"text": "This principle, together with the assumption that most of the compounds are compositional, leads to the conclusion that a model of composition should be able to be auto-reconstructive: the composition function that maps component-words' vectors to their compound vector should have an associated decomposition function that independently predicts each of the component-words' vectors from this compound vector.", "labels": [], "entities": []}, {"text": "An auto-reconstructive model enables us to exploit more data in order to learn semantic composition and predict compositionality.", "labels": [], "entities": []}, {"text": "We show that auto-reconstruction can improve the accuracy of composition functions and improve detecting non-compositionality.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9984440207481384}]}, {"text": "To further improve non-compositionality detection, we propose an EM-like detection algorithm based on hidden compositionality annotations.", "labels": [], "entities": [{"text": "non-compositionality detection", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.7223983108997345}, {"text": "EM-like detection", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.8027194142341614}]}, {"text": "The best composition is the one that is the best fit on all the data points except the noncompositional ones.", "labels": [], "entities": []}, {"text": "Since we don't use annotated data at training time, we assume annotations to be hidden variables and iteratively alternate between optimizing the composition function and optimizing the hidden compositionality annotations.", "labels": [], "entities": []}, {"text": "We show that this iterative algorithm increases the accuracy of non-compositionality detection compared to the case when training is done on all examples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9994170665740967}, {"text": "non-compositionality detection", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.7582532465457916}]}, {"text": "We run our experiments on the data set of who provide a set of English NCs which are annotated with noncompositionality judgments.", "labels": [], "entities": []}, {"text": "We show that quadratic regression significantly outperforms additive and multiplicative baselines and all other models in modelling semantic composition and identifying the non-compositional NCs.", "labels": [], "entities": []}, {"text": "In short, the contributions of our work are: to empirically evaluate various composition functions ranging from simple to overly complex in order to find the most accurate function; to propose, to the best of our knowledge for the first time, a method of identifying non-compositional phrases as phrases for which a composition function cannot be readily learned; to propose learning decomposability as another criterion to detect non-compositionality; and to examine possible ways of improving the accuracy of the models by means of EM on hidden compositionality annotations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 499, "end_pos": 507, "type": "METRIC", "confidence": 0.9954385161399841}]}], "datasetContent": [{"text": "We evaluated the above models on the data set of.", "labels": [], "entities": []}, {"text": "They provide a set of 1042 English NCs with four non-compositionality judgments.", "labels": [], "entities": [{"text": "English NCs", "start_pos": 27, "end_pos": 38, "type": "DATASET", "confidence": 0.8412758708000183}]}, {"text": "The judgments are binary decisions taken by four experts about whether or not a compound is non-compositional.", "labels": [], "entities": []}, {"text": "We calculate a votebased non-compositionality score for each of the data set compounds by summing over its noncompositionality judgments.", "labels": [], "entities": []}, {"text": "The neural network models are trained using stochastic gradient descent.", "labels": [], "entities": []}, {"text": "We use the additive and multiplicative models of modelling composition and detecting noncompositionality presented by and) as state of the art baselines.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The second column shows the correlation between different models' predictions and the annotated data in terms of Spearman \u03c1.", "labels": [], "entities": [{"text": "Spearman \u03c1", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.9845840930938721}]}, {"text": "The last three columns show the performance of different models in terms of Normalized Discounted Cumulative Gain (NDCG), F 1 score and and Precision at 100 (P @100).", "labels": [], "entities": [{"text": "Normalized Discounted Cumulative Gain (NDCG)", "start_pos": 76, "end_pos": 120, "type": "METRIC", "confidence": 0.7121352553367615}, {"text": "F 1 score", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9886426329612732}, {"text": "Precision at 100 (P @100)", "start_pos": 140, "end_pos": 165, "type": "METRIC", "confidence": 0.9124265387654305}]}, {"text": "For these three scores we consider the problem of predicting noncompositional NCs a problem with a binary solution where we assume compounds (of the evaluation set) with at least two non-compositionality votes are non-compositional.", "labels": [], "entities": [{"text": "predicting noncompositional NCs", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.8602526982625326}]}, {"text": "NDCG assigns a higher score to a ranked list of compounds if the non-compositional ones are ranked higher in the list.", "labels": [], "entities": [{"text": "NDCG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9056283235549927}]}, {"text": "F 1 column represents the maximum F 1 score on the top-n elements of the ranked list returned by the corresponding model for all n in: Results for each model's ability to predict non-compositionality.", "labels": [], "entities": [{"text": "F 1 column", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9207919438680013}, {"text": "F 1 score", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9641123016675314}]}, {"text": "[1 \u2212 size-of-ranked-list].", "labels": [], "entities": []}, {"text": "P @100 shows the precision at the first 100 compounds ranked as noncompositional.", "labels": [], "entities": [{"text": "P", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9439039826393127}, {"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9993671774864197}]}, {"text": "The models are listed in the order of complexity of the composition function.", "labels": [], "entities": []}, {"text": "The addition-based baseline which was explored in a variety of previous work does not seem to be as powerful as the other models.", "labels": [], "entities": []}, {"text": "It is outperformed by almost all learned models.", "labels": [], "entities": []}, {"text": "In general, we can see that more complex functions tend to learn compositionality in a more effective way.", "labels": [], "entities": []}, {"text": "As mentioned earlier, overly powerful learners overfit and do not produce meaningful errors for the detection task.", "labels": [], "entities": []}, {"text": "Sparsity seems to address this issue by reducing the number of non-zero parameters while the function can still keep the complex terms if needed.", "labels": [], "entities": []}, {"text": "In general sparse models show improvement over their non-sparse counterparts, specifically for more powerful models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for each model's ability to predict  non-compositionality.", "labels": [], "entities": []}, {"text": " Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.", "labels": [], "entities": []}, {"text": " Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.", "labels": [], "entities": []}]}