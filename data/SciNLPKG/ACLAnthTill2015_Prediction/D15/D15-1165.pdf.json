{"title": [{"text": "A Comparison between Count and Neural Network Models Based on Joint Translation and Reordering Sequences", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a conversion of bilingual sentence pairs and the corresponding word alignments into novel linear sequences.", "labels": [], "entities": []}, {"text": "These are joint translation and reordering (JTR) uniquely defined sequences, combining interdepending lexical and alignment dependencies on the word level into a single framework.", "labels": [], "entities": [{"text": "joint translation and reordering (JTR) uniquely defined sequences", "start_pos": 10, "end_pos": 75, "type": "TASK", "confidence": 0.8619308412075043}]}, {"text": "They are constructed in a simple manner while capturing multiple alignments and empty words.", "labels": [], "entities": []}, {"text": "JTR sequences can be used to train a variety of models.", "labels": [], "entities": [{"text": "JTR sequences", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.7566947937011719}]}, {"text": "We investigate the performances of n-gram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b).", "labels": [], "entities": []}, {"text": "Evaluations on the IWSLT German\u2192English, WMT German\u2192English and BOLT Chinese\u2192English tasks show that JTR models improve state-of-the-art phrase-based systems by up to 2.2 BLEU.", "labels": [], "entities": [{"text": "IWSLT German\u2192English", "start_pos": 19, "end_pos": 39, "type": "DATASET", "confidence": 0.7740620672702789}, {"text": "WMT German\u2192English", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.7413702607154846}, {"text": "BOLT", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9751160144805908}, {"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9983364939689636}]}], "introductionContent": [{"text": "Standard phrase-based machine translation ( uses relative frequencies of phrase pairs to estimate a translation model.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.6196717321872711}]}, {"text": "The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA ++ (.", "labels": [], "entities": []}, {"text": "Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each other, they fail to model dependencies that extend beyond phrase boundaries.", "labels": [], "entities": []}, {"text": "Phrase-based decoding involves concatenating target phrases.", "labels": [], "entities": []}, {"text": "The burden of ensuring that the result is linguistically consistent falls on the language model (LM).", "labels": [], "entities": []}, {"text": "This work proposes word-based translation models that are potentially capable of capturing long-range dependencies.", "labels": [], "entities": [{"text": "word-based translation", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7165639698505402}]}, {"text": "We do this in two steps: First, given bilingual sentence pairs and the associated word alignments, we convert the information into uniquely defined linear sequences.", "labels": [], "entities": []}, {"text": "These sequenecs encode both word reordering and translation information.", "labels": [], "entities": []}, {"text": "Thus, they are referred to as joint translation and reordering (JTR) sequences.", "labels": [], "entities": [{"text": "joint translation and reordering (JTR)", "start_pos": 30, "end_pos": 68, "type": "TASK", "confidence": 0.8057624101638794}]}, {"text": "Second, we train an n-gram model with modified Kneser-Ney smoothing) on the resulting JTR sequences.", "labels": [], "entities": [{"text": "JTR sequences", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.7985762655735016}]}, {"text": "This yields a model that fuses interdepending reordering and translation dependencies into a single framework.", "labels": [], "entities": []}, {"text": "Although JTR n-gram models are closely related to the operation sequence model (OSM) (, there are three main differences.", "labels": [], "entities": []}, {"text": "To begin with, the OSM employs minimal translation units (MTUs), which are essentially atomic phrases.", "labels": [], "entities": [{"text": "OSM", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9505646824836731}]}, {"text": "As the MTUs are extracted sentence-wise, a word can potentially appear in multiple MTUs.", "labels": [], "entities": []}, {"text": "In order to avoid overlapping translation units, we define the JTR sequences on the level of words.", "labels": [], "entities": []}, {"text": "Consequently, JTR sequences have smaller vocabulary sizes than OSM sequences and lead to models with less sparsity.", "labels": [], "entities": []}, {"text": "Moreover, we argue that JTR sequences offer a simpler reordering approach than operation sequences, as they handle reorderings without the need to predict gaps.", "labels": [], "entities": []}, {"text": "Finally, when used as an additional model in the log-linear framework of phrase-based decoding, an n-gram model trained on JTR sequences introduces only one single feature to be tuned, whereas the OSM additionally uses 4 supportive features ().", "labels": [], "entities": []}, {"text": "Experimental results confirm that this simplification does not make JTR models less expressive, as their performance is on par with the OSM.", "labels": [], "entities": []}, {"text": "Due to data sparsity, increasing the n-gram order of count-based models beyond a certain point becomes useless.", "labels": [], "entities": []}, {"text": "To address this, we resort to neu-ral networks (NNs), as they have been successfully applied to machine translation recently).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7453332245349884}]}, {"text": "They are able to score any word combination without requiring additional smoothing techniques.", "labels": [], "entities": []}, {"text": "We experiment with feed-forward and recurrent translation networks, benefiting from their smoothing capabilities.", "labels": [], "entities": []}, {"text": "To this end, we split the linear sequence into two sequences for the neural translation models to operate on.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7561533451080322}]}, {"text": "This is possible due to the simplicity of the JTR sequence.", "labels": [], "entities": [{"text": "JTR sequence", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.6867974698543549}]}, {"text": "We show that the count and NN models perform well on their own, and that combining them yields even better results.", "labels": [], "entities": [{"text": "count", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.9369280338287354}]}, {"text": "In this work, we apply n-gram models with modified Kneser-Ney smoothing during phrasebased decoding and neural JTR models in rescoring.", "labels": [], "entities": []}, {"text": "However, using a phrase-based system is not required by the model, but only the initial step to demonstrate the strength of JTR models, which can be applied independently of the underlying decoding framework.", "labels": [], "entities": []}, {"text": "While the focus of this work is on the development and comparison of the models, the long-term goal is to decode using JTR models without the limitations introduced by phrases, in order to exploit the full potential of JTR models.", "labels": [], "entities": []}, {"text": "The JTR models are estimated on word alignments, which we obtain using GIZA ++ in this paper.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.6923515349626541}]}, {"text": "The future aim is to also generate improved word alignments by a joint optimization of both the alignments and the models, similar to the training of IBM models ().", "labels": [], "entities": [{"text": "word alignments", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.7636733651161194}]}, {"text": "In the long run, we intend to achieve a consistency between decoding and training using the introduced JTR models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments on the largescale   (.", "labels": [], "entities": []}, {"text": "We use a standard phrasebased translation system (.", "labels": [], "entities": [{"text": "phrasebased translation", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.7116796374320984}]}, {"text": "All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (.", "labels": [], "entities": []}, {"text": "The OSM and the count-based JTR model are implemented in the phrasal decoder.", "labels": [], "entities": []}, {"text": "NNs are used only in rescoring.", "labels": [], "entities": []}, {"text": "The 9-gram FFNNs are trained with two hidden layers.", "labels": [], "entities": [{"text": "FFNNs", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.48280757665634155}]}, {"text": "The short lists contain the 10k most frequent words, and all remaining words are clusterd into 1000 word classes.", "labels": [], "entities": []}, {"text": "The projecton layer has 17 \u00d7 100 nodes, the first hidden layer 1000 and the second 500.", "labels": [], "entities": []}, {"text": "The RNNs have LSTM architectures.", "labels": [], "entities": []}, {"text": "The URNN has 2 hidden layers while the BRNN has one forward, one backward and one additional hidden layer.", "labels": [], "entities": [{"text": "URNN", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8290876746177673}, {"text": "BRNN", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.7818657755851746}]}, {"text": "All layers have 200 nodes, while the output layer is class-factored using 2000 classes.", "labels": [], "entities": []}, {"text": "For the count-based JTR model and OSM we tuned the n-gram size on the tuning set of each task.", "labels": [], "entities": [{"text": "OSM", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.751844048500061}]}, {"text": "For the full data, 7-grams were used for the IWSLT and WMT tasks, and 8-grams for BOLT.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.816839873790741}, {"text": "WMT", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.5109795331954956}, {"text": "BOLT", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.8750961422920227}]}, {"text": "When using in-domain data, smaller n-gram sizes were used.", "labels": [], "entities": []}, {"text": "All rescoring experiments used 1000-best lists without duplicates.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results measured in BLEU for the IWSLT  German\u2192English task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.998820960521698}, {"text": "IWSLT  German\u2192English task", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.6061312198638916}]}, {"text": " Table 4: Results measured in BLEU for the BOLT  Chinese\u2192English task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9973472356796265}, {"text": "BOLT  Chinese\u2192English task", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.5730252742767334}]}, {"text": " Table 5: Results measured in BLEU for the WMT  German\u2192English task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9985222220420837}, {"text": "WMT  German\u2192English task", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.8041553735733032}]}]}