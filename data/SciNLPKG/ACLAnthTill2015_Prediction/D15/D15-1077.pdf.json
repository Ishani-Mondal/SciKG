{"title": [{"text": "Name List Only? Target Entity Disambiguation in Short Texts", "labels": [], "entities": [{"text": "Target Entity Disambiguation", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.49463961521784466}]}], "abstractContent": [{"text": "Target entity disambiguation (TED), the task of identifying target entities of the same domain, has been recognized as a critical step in various important applications.", "labels": [], "entities": [{"text": "Target entity disambiguation (TED)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.815383975704511}]}, {"text": "In this paper, we propose a graph-based model called TremenRank to collectively identify target entities in short texts given a name list only.", "labels": [], "entities": []}, {"text": "TremenRank propagates trust within the graph, allowing for an arbitrary number of target entities and texts using inverted index technology.", "labels": [], "entities": []}, {"text": "Furthermore , we design a multi-layer directed graph to assign different trust levels to short texts for better performance.", "labels": [], "entities": []}, {"text": "The experimental results demonstrate that our model outperforms state-of-the-art methods with an average gain of 24.8% inaccuracy and 15.2% in the F1-measure on three datasets in different domains.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 147, "end_pos": 157, "type": "METRIC", "confidence": 0.9985378980636597}]}], "introductionContent": [{"text": "Currently, a growing number of people prefer to express their views and comments online.", "labels": [], "entities": []}, {"text": "These messages, which are updated at the rate of millions per day, become a potentially rich source of information.", "labels": [], "entities": []}, {"text": "From such a large number of texts, entity disambiguation is a critical step when extracting text information from these messages, and for various applications, such as natural language processing and knowledge acquisition.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.7543695271015167}, {"text": "natural language processing", "start_pos": 168, "end_pos": 195, "type": "TASK", "confidence": 0.6372464696566263}, {"text": "knowledge acquisition", "start_pos": 200, "end_pos": 221, "type": "TASK", "confidence": 0.8251251578330994}]}, {"text": "Take the application of customer feedback analysis (CFA) as an example.", "labels": [], "entities": [{"text": "customer feedback analysis (CFA)", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.7772008329629898}]}, {"text": "An enterprise is typically interested in public reviews of its own products as well as those of its competitors'; the identification of these entities is thus critical for further analysis.", "labels": [], "entities": []}, {"text": "The product names comprise a list of entities to be identified.", "labels": [], "entities": []}, {"text": "We refer to these entities of the same domain as target entities, and the identification process is called target entity disambiguation ( ).", "labels": [], "entities": [{"text": "target entity disambiguation", "start_pos": 107, "end_pos": 135, "type": "TASK", "confidence": 0.7081597447395325}]}, {"text": "All of target entities (e.g., car brands) share a common domain, which we refer to as the target domain.", "labels": [], "entities": []}, {"text": "The target domain is the only constraint to target entities, which implies that the entities are in a specific domain, rather than being general things.", "labels": [], "entities": []}, {"text": "In the case of entity recognition from short texts, the disambiguation can be performed on the document level.", "labels": [], "entities": [{"text": "entity recognition from short texts", "start_pos": 15, "end_pos": 50, "type": "TASK", "confidence": 0.8438257336616516}]}, {"text": "Given a collection of short documents, our goal is to determine which documents contain the target entities.", "labels": [], "entities": []}], "datasetContent": [{"text": "Baseline Methods TED in short texts is a relatively new problem, and there are few specific methods for solving it.", "labels": [], "entities": []}, {"text": "To validate the performance of TremenRank and the improvement produced by the MLD graph, we selected the baseline from three different perspectives: (i) a context-based method that identifies target entities separately; (ii) a classic supervised method SVM to classify a document by whether it contains any true mentions; and (iii) the only state-of-the-art MentionRank for TED, which is a collective ranking method.", "labels": [], "entities": []}, {"text": "\u2022 The Context-based method mines a frequent item set of true mentions and identifies the documents that contain more frequent items.", "labels": [], "entities": []}, {"text": "\u2022 SVM classifies the documents into two classes: documents that are in the target domain and those that are not.", "labels": [], "entities": []}, {"text": "Using context words as features, we train and test SVM on the labeled set with a 10-fold cross validation.", "labels": [], "entities": []}, {"text": "\u2022 MentionRank is a graph-based method that disambiguates at the entity level.", "labels": [], "entities": []}, {"text": "Difficult to apply to the entire large datasets directly, it has been applied to the labeled set.", "labels": [], "entities": []}, {"text": "The performance of the disambiguation task is typically evaluated by accuracy, but in TEDs we are also interested in precision, recall and the F1-measure because different applications focus on different aspects.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9993981122970581}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9995015859603882}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9993842840194702}, {"text": "F1-measure", "start_pos": 143, "end_pos": 153, "type": "METRIC", "confidence": 0.9957249164581299}]}, {"text": "For example, in the application of CFA, a company expects a higher recall to collect as many reviews as possible, while in financial news recommendations, users prefer to read more accurate microblogs.", "labels": [], "entities": [{"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9984201192855835}]}, {"text": "Because the entire dataset is too large to evaluate directly, we randomly sampled 800 mentions for each dataset and labelled them manually to calculate the performance metrics .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the datasets.", "labels": [], "entities": []}, {"text": " Table 2: Overall results on the three datasets", "labels": [], "entities": []}]}