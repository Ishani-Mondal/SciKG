{"title": [{"text": "Distributed Representations for Unsupervised Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.6213097671667734}]}], "abstractContent": [{"text": "We present anew approach for unsuper-vised semantic role labeling that leverages distributed representations.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.673619270324707}]}, {"text": "We induce embeddings to represent a predicate , its arguments and their complex interdependence.", "labels": [], "entities": []}, {"text": "Argument embeddings are learned from surrounding contexts involving the predicate and neighboring arguments , while predicate embeddings are learned from argument contexts.", "labels": [], "entities": []}, {"text": "The induced representations are clustered into roles using a linear programming formulation of hierarchical clustering, where we can model task-specific knowledge.", "labels": [], "entities": []}, {"text": "Experiments show improved performance over previous unsupervised semantic role labeling approaches and other distributed word representation models.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7012152870496114}]}], "introductionContent": [{"text": "In recent years, an increasing body of work has been devoted to learning distributed word representations and their successful usage in numerous tasks and real-world applications.", "labels": [], "entities": []}, {"text": "Examples include language modeling, paraphrase detection (), sentiment analysis (, and most notably machine translation).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7582601010799408}, {"text": "paraphrase detection", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.9047307968139648}, {"text": "sentiment analysis", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.9689073860645294}, {"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7844753861427307}]}, {"text": "Distributed word representations (also known as word embeddings) are trained by predicting the contexts in which the words or phrases occur.", "labels": [], "entities": []}, {"text": "In this paper, we present anew approach for unsupervised semantic role labeling that leverages distributed representations.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.6828270157178243}]}, {"text": "The goal of semantic role labeling is to discover the relations that hold between a predicate and its arguments in a given input sentence (e.g., \"who\" did \"what\" to \"whom\", \"when\", \"where\", and \"how\").", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.6312699417273203}]}, {"text": "In sentence (1), A0 represents the Agent of the breaking event, A1 represents the Patient (i.e., the physical object affected by the breaking event) and V determines the boundaries of the predicate.", "labels": [], "entities": []}, {"text": "The semantic roles in the example are labeled in the style of), a broad-coverage human-annotated corpus of semantic roles and their syntactic realizations.", "labels": [], "entities": []}, {"text": "In the unsupervised case, the model must induce such labels from data without access to a predefined set of semantic roles.", "labels": [], "entities": []}, {"text": "Role induction is commonly treated as a clustering problem.", "labels": [], "entities": [{"text": "Role induction", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8661727011203766}]}, {"text": "The input to the model are instances of arguments (e.g., window, the burglar in sentence (1)) and the output is a grouping of these instances into clusters such that each cluster contains arguments corresponding to a specific semantic role and each role corresponds to exactly one cluster.", "labels": [], "entities": []}, {"text": "In other words, the syntactic representations of verbal predicates, and argument positions are observable, whereas the associated semantic roles are latent and need to be inferred.", "labels": [], "entities": []}, {"text": "The task is challenging due to its unsupervised nature -it is difficult to define a learning objective function whose optimization will yield an accurate model -but also because each predicate can allow several alternate mappings or linkings between its semantic roles and their syntactic realization.", "labels": [], "entities": []}, {"text": "Despite occupying different syntactic positions (subject in sentence (1) and object in sentence (2)), the noun phrase the window expresses the same role in both sentences.", "labels": [], "entities": []}, {"text": "To learn such linkings, previous work has made use of syntactic and semantic features (e.g., whether two arguments are in the same position in the parse tree, whether they have the same POS-tags, whether they are lexically similar).", "labels": [], "entities": []}, {"text": "These features are typically defined on argument instances, without taking the predicate into account, and do not interact but instead are sequentially applied.", "labels": [], "entities": []}, {"text": "In this work we propose to learn these features and their complex interactions (e.g., selectional restrictions) automatically from data.", "labels": [], "entities": []}, {"text": "Specifically, we induce embeddings to represent a predicate and its arguments.", "labels": [], "entities": []}, {"text": "Argument embeddings are learned from surrounding contexts involving the predicate and neighboring arguments.", "labels": [], "entities": []}, {"text": "Analogously, predicate embeddings are learned from contexts representing their arguments.", "labels": [], "entities": []}, {"text": "Our model learns a rich feature space which can serve as input to any clustering algorithm.", "labels": [], "entities": []}, {"text": "We use a linear programming formulation of hierarchical clustering which is advantageous for two reasons.", "labels": [], "entities": []}, {"text": "Firstly, expressing clustering as a global optimization problem with an explicit objective function can potentially yield higher quality output compared to greedy algorithms (such as agglomerative clustering).", "labels": [], "entities": []}, {"text": "Secondly, through the use of constraints, we can model task-specific knowledge (e.g., semantic roles are unique within a frame).", "labels": [], "entities": []}, {"text": "Experimental results show improved performance over both previous unsupervised semantic role labeling approaches and other distributed word representation models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present our experimental setup for assessing the performance of the model presented above.", "labels": [], "entities": []}, {"text": "We explain how it was trained and tested, and also briefly introduce the models used for comparison with our approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Purity, collocation and F1 measures (left), and homogeneity, completeness and V1 measures  (right) for CoNLL-2008 data set, using gold syntax information.", "labels": [], "entities": [{"text": "Purity", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9828425049781799}, {"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.986649215221405}, {"text": "completeness", "start_pos": 71, "end_pos": 83, "type": "METRIC", "confidence": 0.9443207383155823}, {"text": "V1", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.7148780226707458}, {"text": "CoNLL-2008 data set", "start_pos": 113, "end_pos": 132, "type": "DATASET", "confidence": 0.9753241737683614}]}, {"text": " Table 2: Purity, collocation and F1 measures, and homogeneity, completeness and V1 measures for  CoNLL-2008 data set using automatic parse syntax information.", "labels": [], "entities": [{"text": "Purity", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.985205352306366}, {"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.988740861415863}, {"text": "completeness", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.9701151251792908}, {"text": "V1", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9449189901351929}, {"text": "CoNLL-2008 data set", "start_pos": 98, "end_pos": 117, "type": "DATASET", "confidence": 0.953916052977244}]}]}