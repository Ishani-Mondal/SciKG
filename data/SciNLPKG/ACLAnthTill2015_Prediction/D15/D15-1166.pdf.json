{"title": [{"text": "Effective Approaches to Attention-based Neural Machine Translation", "labels": [], "entities": [{"text": "Attention-based Neural Machine Translation", "start_pos": 24, "end_pos": 66, "type": "TASK", "confidence": 0.6594584584236145}]}], "abstractContent": [{"text": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.7949121793111166}]}, {"text": "However, there has been little work exploring useful architectures for attention-based NMT.", "labels": [], "entities": []}, {"text": "This paper examines two simple and effective classes of at-tentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions.", "labels": [], "entities": [{"text": "WMT translation tasks between English and German", "start_pos": 59, "end_pos": 107, "type": "TASK", "confidence": 0.9109823703765869}]}, {"text": "With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9994102716445923}]}, {"text": "Our ensemble model using different attention architec-tures yields anew state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.", "labels": [], "entities": [{"text": "WMT'15 English to German translation", "start_pos": 103, "end_pos": 139, "type": "TASK", "confidence": 0.6700393795967102}, {"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9981008172035217}, {"text": "BLEU", "start_pos": 190, "end_pos": 194, "type": "METRIC", "confidence": 0.9974276423454285}, {"text": "NMT", "start_pos": 242, "end_pos": 245, "type": "DATASET", "confidence": 0.9547962546348572}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to) and English to German (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7802805066108703}]}, {"text": "NMT is appealing since it requires minimal domain knowledge and is conceptually simple.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7323024868965149}]}, {"text": "The model by reads through all the source words until the end-of-sentence symbol <eos> is reached.", "labels": [], "entities": []}, {"text": "It then starts emitting one target word at a time, as illustrated in.", "labels": [], "entities": []}, {"text": "NMT is often a large neural network that is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences.", "labels": [], "entities": []}, {"text": "This means the model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT; hence, NMT has a small memory footprint.", "labels": [], "entities": []}, {"text": "Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT (.", "labels": [], "entities": []}, {"text": "In parallel, the concept of \"attention\" has gained popularity recently in training neural networks, allowing models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (), between speech frames and text in the speech recognition task (, or between visual features of a picture and its text description in the image caption generation task ().", "labels": [], "entities": [{"text": "speech recognition task", "start_pos": 285, "end_pos": 308, "type": "TASK", "confidence": 0.7696406741937002}, {"text": "image caption generation task", "start_pos": 384, "end_pos": 413, "type": "TASK", "confidence": 0.8255008459091187}]}, {"text": "In the context of NMT, has successfully applied such attentional mechanism to jointly translate and align words.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there has not been any other work exploring the use of attention-based architectures for NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9226590991020203}]}, {"text": "In this work, we design, with simplicity and effectiveness in mind, two novel types of attention-based models: a global approach in which all source words are attended and a local one whereby only a subset of source words are considered at a time.", "labels": [], "entities": []}, {"text": "The former approach resembles the model of () but is simpler architecturally.", "labels": [], "entities": []}, {"text": "The latter can be viewed as an interesting blend between the hard and soft attention models proposed in (: it is computationally less expensive than the global model or the soft attention; at the same time, unlike the hard attention, the local attention is differentiable, making it easier to implement and train.", "labels": [], "entities": []}, {"text": "Besides, we also examine various alignment functions for our attention-based models.", "labels": [], "entities": []}, {"text": "Experimentally, we demonstrate that both of our approaches are effective in the WMT translation tasks between English and German in both directions.", "labels": [], "entities": [{"text": "WMT translation tasks between English and German", "start_pos": 80, "end_pos": 128, "type": "TASK", "confidence": 0.9154165812901088}]}, {"text": "Our attentional models yield a boost of up to 5.0 BLEU over non-attentional systems which already incorporate known techniques such as dropout.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9995536208152771}]}, {"text": "For English to German translation, we achieve new state-of-the-art (SOTA) results for both WMT'14 and WMT'15, outperforming previous SOTA systems, backed by NMT models and n-gram LM rerankers, by more than 1.0 BLEU.", "labels": [], "entities": [{"text": "WMT'14", "start_pos": 91, "end_pos": 97, "type": "DATASET", "confidence": 0.9245509505271912}, {"text": "WMT'15", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.8508575558662415}, {"text": "BLEU", "start_pos": 210, "end_pos": 214, "type": "METRIC", "confidence": 0.9984521865844727}]}, {"text": "We conduct extensive analysis to evaluate our models in terms of learning, the ability to handle long sentences, choices of attentional architectures, alignment quality, and translation outputs.", "labels": [], "entities": [{"text": "translation", "start_pos": 174, "end_pos": 185, "type": "TASK", "confidence": 0.9546429514884949}]}], "datasetContent": [{"text": "We evaluate the effectiveness of our models on the WMT translation tasks between English and German in both directions.", "labels": [], "entities": [{"text": "WMT translation tasks between English and German", "start_pos": 51, "end_pos": 99, "type": "TASK", "confidence": 0.9214615396090916}]}, {"text": "newstest2013 (3000 sentences) is used as a development set to select our hyperparameters.", "labels": [], "entities": [{"text": "newstest2013", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8747478723526001}]}, {"text": "Translation performances are reported in case-sensitive BLEU () on newstest2014 (2737 sentences) and newstest2015 (2169 sentences).", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.942989706993103}, {"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9830958247184753}, {"text": "newstest2014", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.9599531292915344}, {"text": "newstest2015", "start_pos": 101, "end_pos": 113, "type": "DATASET", "confidence": 0.9611194133758545}]}, {"text": "Following (, we report translation quality using two types of BLEU: (a) tokenized 12 BLEU to be comparable with existing NMT work and (b) NIST 13 BLEU to be comparable with WMT results.", "labels": [], "entities": [{"text": "translation", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.9560948610305786}, {"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9963552951812744}, {"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.926295816898346}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.6995094418525696}, {"text": "WMT", "start_pos": 173, "end_pos": 176, "type": "DATASET", "confidence": 0.6291711926460266}]}], "tableCaptions": [{"text": " Table 1: WMT'14 English-German results -shown are the perplexities (ppl) and the tokenized BLEU  scores of various systems on newstest2014. We highlight the best system in bold and give progressive  improvements in italic between consecutive systems. local-p referes to the local attention with predictive  alignments. We indicate for each attention model the alignment score function used in pararentheses.", "labels": [], "entities": [{"text": "WMT'14", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.7751184701919556}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9859025478363037}]}, {"text": " Table 3: WMT'15 German-English results - performances of various systems (similar to Ta- ble 1). The base system already includes source  reversing on which we add global attention,  dropout, input feeding, and unk replacement.", "labels": [], "entities": [{"text": "WMT'15 German-English results", "start_pos": 10, "end_pos": 39, "type": "DATASET", "confidence": 0.8136632045110067}, {"text": "unk replacement", "start_pos": 212, "end_pos": 227, "type": "TASK", "confidence": 0.6809332817792892}]}, {"text": " Table 3. The attentional mech- anism gives us +2.2 BLEU gain and on top of  that, we obtain another boost of up to +1.0 BLEU  from the input-feeding approach. Using a better  alignment function, the content-based dot product  one, together with dropout yields another gain of  +2.7 BLEU. Lastly, when applying the unknown  word replacement technique, we seize an addi- tional +2.1 BLEU, demonstrating the usefulness  of attention in aligning rare words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9985925555229187}, {"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9969411492347717}, {"text": "BLEU", "start_pos": 283, "end_pos": 287, "type": "METRIC", "confidence": 0.9935694932937622}, {"text": "word replacement", "start_pos": 324, "end_pos": 340, "type": "TASK", "confidence": 0.7319468855857849}, {"text": "BLEU", "start_pos": 382, "end_pos": 386, "type": "METRIC", "confidence": 0.9845194816589355}]}, {"text": " Table 4: Attentional Architectures -perfor- mances of different attentional models. We trained  two local-m (dot) models; both have ppl > 7.0.", "labels": [], "entities": []}]}