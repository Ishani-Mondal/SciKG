{"title": [{"text": "System Combination for Machine Translation through Paraphrasing", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7802449762821198}, {"text": "Paraphrasing", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.6440253853797913}]}], "abstractContent": [{"text": "In this paper, we propose a paraphrasing model to address the task of system combination for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7975718080997467}]}, {"text": "We dynamically learn hierarchical paraphrases from target hypotheses and form asynchronous context-free grammar to guide a series of transformations of target hypotheses into fused translations.", "labels": [], "entities": []}, {"text": "The model is able to exploit phrasal and structural system-weighted consensus and also to utilize existing information about word ordering present in the target hypotheses.", "labels": [], "entities": []}, {"text": "In addition, to consider a diverse set of plausible fused translations, we develop a hybrid combination architecture, where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations.", "labels": [], "entities": []}, {"text": "Our experimental results show that our approach can achieve a significant improvement over combination baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the past several years, many machine translation (MT) combination approaches have been developed.", "labels": [], "entities": [{"text": "machine translation (MT) combination", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.8571425825357437}]}, {"text": "Word-level combination approaches, such as the confusion network decoding model, have been quite successful ().", "labels": [], "entities": []}, {"text": "In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase.", "labels": [], "entities": [{"text": "word-level combination", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7014800012111664}]}, {"text": "The most common phrase-level combination approaches are redecoding methods: by constructing anew phrase table from each MT system's source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table).", "labels": [], "entities": [{"text": "MT system's source-to-target phrase alignments", "start_pos": 120, "end_pos": 166, "type": "TASK", "confidence": 0.7036005059878031}]}, {"text": "One problem with these approaches is that, just with anew phrase table, existing information about word ordering present in the target hypotheses is not utilized; thus the approaches are likely to make new mistakes of word reordering which do not appear in the target hypotheses of MT engines.", "labels": [], "entities": [{"text": "MT", "start_pos": 282, "end_pos": 284, "type": "TASK", "confidence": 0.9458678960800171}]}, {"text": "attacked this issue through a reordering cost function that encourages search along with decoding paths from all MT engines' decoders.", "labels": [], "entities": [{"text": "MT engines' decoders", "start_pos": 113, "end_pos": 133, "type": "TASK", "confidence": 0.7510635256767273}]}, {"text": "Another phrase-level combination approach relies on a lattice decoding model to carryout the combination (Feng et al 2009;.", "labels": [], "entities": []}, {"text": "Ina lattice, each edge is associated with a phrase (a single word or a sequence of words) rather than a single word.", "labels": [], "entities": []}, {"text": "The construction of the lattice is based on the extraction of phrase pairs from word alignments between a selected best MT system hypothesis (the backbone) and the other translation hypotheses.", "labels": [], "entities": []}, {"text": "One challenge of the lattice decoding model is that it is difficult to consider structural consensus among target hypotheses from multiple MT engines, i.e, the consensus among occurrences of discontinuous words.", "labels": [], "entities": []}, {"text": "In this paper, we propose another phrase-level combination approach -a paraphrasing model using hierarchical paraphrases (paraphrases contain subparaphrases), to fuse target hypotheses.", "labels": [], "entities": []}, {"text": "We dynamically learn hierarchical paraphrases from target hypotheses without any syntactic annotations and form asynchronous context-free grammar (SCFG) to guide a series of transformations of target hypotheses into fused translations.", "labels": [], "entities": []}, {"text": "Through these structural transformations, the paraphrasing model is able to exploit phrasal and structural system-weighted consensus and also able to utilize existing information about word ordering present in the target hypotheses.", "labels": [], "entities": []}, {"text": "In addition, to consider a diverse set of plausible fused translations, we develop a hybrid combination architecture, where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations through a sentence-level selection-based model.", "labels": [], "entities": []}, {"text": "In short, compared with other related work, our approach features the following advantages: 1.", "labels": [], "entities": []}, {"text": "It can consider structural system-weighted consensus among target hypotheses from multiple MT engines through its hierarchical paraphrases, which non-hierarchical paraphrases are notable to do.", "labels": [], "entities": []}, {"text": "2. It can utilize existing information about word ordering present in the target hypotheses.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.7000051885843277}]}, {"text": "3. It can retain coherence and consistency between the words in a phrase.", "labels": [], "entities": [{"text": "consistency", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9658629894256592}]}, {"text": "4. The hybrid combination architecture enables us to consider a diverse set of plausible fused translations produced by different fusing techniques.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are conducted and reported on three datasets: The first dataset includes ChineseEnglish system translations and reference translations from DARPA GALE 2008 (GALE ChiEng).", "labels": [], "entities": [{"text": "DARPA GALE 2008 (GALE ChiEng)", "start_pos": 156, "end_pos": 185, "type": "DATASET", "confidence": 0.8954828040940421}]}, {"text": "The second dataset includes ChineseEnglish system translations and reference translations and from NIST 2008 (NIST Chi-Eng).", "labels": [], "entities": [{"text": "NIST 2008 (NIST Chi-Eng)", "start_pos": 99, "end_pos": 123, "type": "DATASET", "confidence": 0.937183270851771}]}, {"text": "And the third dataset includes Arabic-English system translations and reference translations and from NIST 2008 (NIST Ara-Eng).", "labels": [], "entities": [{"text": "NIST 2008 (NIST Ara-Eng)", "start_pos": 102, "end_pos": 126, "type": "DATASET", "confidence": 0.9360408782958984}]}, {"text": "lists distinguishing machine translation approaches of top five MT of GALE ChiEng Dataset.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7151096314191818}, {"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9219391345977783}, {"text": "GALE ChiEng Dataset", "start_pos": 70, "end_pos": 89, "type": "DATASET", "confidence": 0.9263148705164591}]}, {"text": "And \"rwth-pbt-sh\" performs the best in Bleu score.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.7251466810703278}]}], "tableCaptions": [{"text": " Table 1. Part of extracted hierarchical paraphrases to para- phrase", "labels": [], "entities": []}, {"text": " Table 3: Techniques of top five MT of GALE Chi-Eng Da- taset", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9910576343536377}, {"text": "GALE Chi-Eng Da- taset", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.8173402607440948}]}, {"text": " Table 4. Experimental results in Bleu score", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.7616294622421265}]}, {"text": " Table 5. The Bleu score of each MT system, the Bleu  score of paraphrasing each MT system using lattice decod- ing and the Bleu score of paraphrasing each MT system  using paraphrasing model.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9793197214603424}, {"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9465453028678894}, {"text": "Bleu  score", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9731278419494629}, {"text": "Bleu score", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.9680499732494354}]}]}