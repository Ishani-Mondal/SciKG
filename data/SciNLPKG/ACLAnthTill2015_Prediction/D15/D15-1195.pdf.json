{"title": [{"text": "Script Induction as Language Modeling Center for Language and Speech Processing", "labels": [], "entities": [{"text": "Script Induction as Language Modeling", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8115485906600952}]}], "abstractContent": [{"text": "The narrative cloze is an evaluation metric commonly used for work on automatic script induction.", "labels": [], "entities": [{"text": "script induction", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.7266646176576614}]}, {"text": "While prior work in this area has focused on count-based methods from distributional semantics, such as pointwise mutual information, we argue that the narrative cloze can be productively reframed as a language modeling task.", "labels": [], "entities": []}, {"text": "By training a discriminative language model for this task, we attain improvements of up to 27 percent over prior methods on standard narrative cloze metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Although the concept of scripts in artificial intelligence dates back to the 1970s (, interest in this topic has renewed with recent efforts to automatically induce scripts from text on a large scale.", "labels": [], "entities": []}, {"text": "One particularly influential work in this area,, treats the problem of script induction as one of learning narrative chains, which they accomplish using simple textual co-occurrence statistics.", "labels": [], "entities": [{"text": "script induction", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.8625150918960571}]}, {"text": "For the novel task of learning narrative chains, they introduce anew evaluation metric, the narrative cloze test, which involves predicting a missing event from a chain of events drawn from text.", "labels": [], "entities": [{"text": "predicting a missing event from a chain of events drawn from text", "start_pos": 129, "end_pos": 194, "type": "TASK", "confidence": 0.7235402713219324}]}, {"text": "Several follow-up works) employ and extend's methods for learning narrative chains, each using the narrative cloze to evaluate their work.", "labels": [], "entities": []}, {"text": "In this paper, we take the position that the narrative cloze test, which has been treated predom-inantly as a method for evaluating script knowledge, is more productively thought of simply as a language modeling task.", "labels": [], "entities": [{"text": "language modeling task", "start_pos": 194, "end_pos": 216, "type": "TASK", "confidence": 0.762750118970871}]}, {"text": "To support this claim, we demonstrate a marked improvement over previous methods on this task using a powerful discriminative language model -the Log-Bilinear model (LBL).", "labels": [], "entities": []}, {"text": "Based on this finding, we believe one of the following conclusions must follow: either discriminative language models area more effective technique for script induction than previous methods, or the narrative cloze testis not a suitable evaluation for this task.", "labels": [], "entities": [{"text": "script induction", "start_pos": 152, "end_pos": 168, "type": "TASK", "confidence": 0.9075451791286469}]}], "datasetContent": [{"text": "For each of the four metrics, the best overall performance is achieved by one of the two LBL models (context size 2 or 4); the LBL models also achieve the best performance on every chain length.", "labels": [], "entities": []}, {"text": "Not only are the gains achieved by the discriminative LBL consistent across metrics and chain length, they are large.", "labels": [], "entities": []}, {"text": "For average rank, the LBL achieves a 27.0% relative improvement over the best non-discriminative model; for mean reciprocal rank, a 19.9% improvement; for recall at 10, a 22.8% improvement; and for recall at 50, a 17.6% improvement.", "labels": [], "entities": [{"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.998668909072876}, {"text": "recall", "start_pos": 198, "end_pos": 204, "type": "METRIC", "confidence": 0.998794436454773}]}, {"text": "(See) Furthermore, note that both PMI models and the Bigram model have been individually tuned for each metric, while the LBL models have not.", "labels": [], "entities": [{"text": "Bigram model", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.960199773311615}]}, {"text": "(The two LBL models are tuned only for overall perplexity on the development set.)", "labels": [], "entities": []}, {"text": "All models trend toward improved performance on longer chains.", "labels": [], "entities": []}, {"text": "Because the unigram model also improves with chain length, it appears that longer chains contain more frequent events and are thus easier to predict.", "labels": [], "entities": []}, {"text": "However, LBL performance is also likely improving on longer chains because of additional contextual information, as is evident from LBL4's slight relative gains over LBL2 on longer chains.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Narrative cloze results bucketed by chain length for each model and scoring metric with best results in bold. The  models are Unigram Model (UNI), Unordered PMI (UOP), Ordered PMI (OP), Bigram Probability Model (BG), Log-Bilinear  Model N=2 (LBL2), Log-Bilinear Model N=4 (LBL4)", "labels": [], "entities": []}]}