{"title": [], "abstractContent": [{"text": "Compared to tree grammars, graph grammars have stronger generative capacity over structures.", "labels": [], "entities": []}, {"text": "Based on an edge replacement grammar, in this paper we propose to use asynchronous graph-to-string grammar for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 111, "end_pos": 142, "type": "TASK", "confidence": 0.7385045687357584}]}, {"text": "The graph we use is directly converted from a dependency tree by labelling edges.", "labels": [], "entities": []}, {"text": "We build our translation model in the log-linear framework with standard features.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9607434272766113}]}, {"text": "Large-scale experiments on Chinese-English and German-English tasks show that our model is significantly better than the state-of-the-art hierarchical phrase-based (HPB) model and a recently improved dependency tree-to-string model on BLEU, METEOR and TER scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 235, "end_pos": 239, "type": "METRIC", "confidence": 0.9985209107398987}, {"text": "METEOR", "start_pos": 241, "end_pos": 247, "type": "METRIC", "confidence": 0.9414775371551514}, {"text": "TER scores", "start_pos": 252, "end_pos": 262, "type": "METRIC", "confidence": 0.956127941608429}]}, {"text": "Experiments also suggest that our model has better capability to perform long-distance reordering and is more suitable for translating long sentences.", "labels": [], "entities": [{"text": "translating long sentences", "start_pos": 123, "end_pos": 149, "type": "TASK", "confidence": 0.9107904632886251}]}], "introductionContent": [{"text": "Compared to trees, which have dominated the field of natural language processing (NLP) for decades, graphs are more general for modelling natural languages.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.8047138253847758}]}, {"text": "The corresponding grammars for recognizing and producing graphs are more flexible and powerful than tree grammars.", "labels": [], "entities": []}, {"text": "However, because of their high complexity, graph grammars have not been widely used in NLP.", "labels": [], "entities": []}, {"text": "Recently, along with progress on graph-based meaning representation, hyperedge replacement grammars (HRG) () have been revisited, explored and used for semantic-based machine translation (.", "labels": [], "entities": [{"text": "graph-based meaning representation", "start_pos": 33, "end_pos": 67, "type": "TASK", "confidence": 0.6857121686140696}, {"text": "hyperedge replacement grammars", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.7573062777519226}, {"text": "machine translation", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.6983292251825333}]}, {"text": "However, the translation process is rather complex and the resources it relies on, namely abstract meaning corpora, are limited as well.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9693787097930908}]}, {"text": "As most available syntactic resources and tools are tree-based, in this paper we propose to convert dependency trees, which are usually taken as a kind of shallow semantic representation, to dependency graphs by labelling edges.", "labels": [], "entities": []}, {"text": "We then use asynchronous version of edge replacement grammar (ERG) (Section 2), a special case of HRG, to translate these graphs.", "labels": [], "entities": [{"text": "edge replacement grammar", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7901478211085001}]}, {"text": "The resulting translation model has the same order of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB)) under a certain restriction (Section 3).", "labels": [], "entities": []}, {"text": "Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3).", "labels": [], "entities": [{"text": "graph-to-string translation", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.7815789580345154}]}, {"text": "Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques).", "labels": [], "entities": []}, {"text": "Furthermore, compared to the known treelet approach ) and Dep2Str), our method not only uses treelets but also has a full capacity of reordering.", "labels": [], "entities": [{"text": "Dep2Str", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.8290687799453735}]}, {"text": "We define our translation model (Section 4) in the log-linear framework).", "labels": [], "entities": [{"text": "translation", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9584193229675293}]}, {"text": "Large-scale experiments (Section 5) on ChineseEnglish and German-English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (), TER () and METEOR ().", "labels": [], "entities": [{"text": "Dep2Str", "start_pos": 224, "end_pos": 231, "type": "DATASET", "confidence": 0.7926476001739502}, {"text": "BLEU", "start_pos": 248, "end_pos": 252, "type": "METRIC", "confidence": 0.9987754225730896}, {"text": "TER", "start_pos": 257, "end_pos": 260, "type": "METRIC", "confidence": 0.9963445067405701}, {"text": "METEOR", "start_pos": 268, "end_pos": 274, "type": "METRIC", "confidence": 0.9931991696357727}]}, {"text": "We also find that the rules in our model are more suitable for long-distance reordering and translating long sentences.", "labels": [], "entities": [{"text": "translating long sentences", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.9044731458028158}]}], "datasetContent": [{"text": "We conduct experiments on Chinese-English and German-English translation tasks.", "labels": [], "entities": [{"text": "German-English translation tasks", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.783759872118632}]}], "tableCaptions": [{"text": " Table 1: Chinese-English (ZH-EN) and German- English (DE-EN) corpora. For the English side of  dev and test sets, words counts are averaged across  all references.", "labels": [], "entities": []}, {"text": " Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p \u2264 0.01. Moses HPB is significantly better than systems with  *  at p \u2264 0.01.", "labels": [], "entities": [{"text": "MERT", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.6708641648292542}, {"text": "Moses HPB", "start_pos": 238, "end_pos": 247, "type": "DATASET", "confidence": 0.947007954120636}]}, {"text": " Table 3: Statistics of sentence length on four test  sets.", "labels": [], "entities": []}]}