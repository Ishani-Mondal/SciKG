{"title": [{"text": "Navigating the Semantic Horizon using Relative Neighborhood Graphs", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces a novel way to navigate neighborhoods in distributional semantic models.", "labels": [], "entities": []}, {"text": "The approach is based on relative neighborhood graphs, which uncover the topological structure of local neighborhoods in semantic space.", "labels": [], "entities": []}, {"text": "This has the potential to overcome both the problem with selecting a proper kin k-NN search, and the problem that a ranked list of neighbors may conflate several different senses.", "labels": [], "entities": []}, {"text": "We provide both qualitative and quantitative results that support the viability of the proposed method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Nearest neighbor search is a fundamental operation in data mining, in which we are interested in finding the closest points to some given reference point.", "labels": [], "entities": [{"text": "Nearest neighbor search", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6813277502854665}, {"text": "data mining", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.7476594150066376}]}, {"text": "Formally, if we have a reference point rand a set of other points P in a metric space M with some distance function d, the nearest neighbor search task is to find the point p \u2208 P that minimizes d.", "labels": [], "entities": []}, {"text": "In k-Nearest Neighbor search (k-NN), we want to find the k closest points to some given reference point.", "labels": [], "entities": []}, {"text": "Nearest neighbor search is a well-studied task, and in particular the complexity of the task (a linear search has a running time of O(N i) where N is the cardinality of P and i the complexity of the distance function d) has generated a lot of research; suggestions for reducing the complexity of linear nearest neighbor searches include using various types of space partitioning techniques like k-d trees, or various techniques for doing approximate nearest neighbor search (), of which one of the most well-known is locality-sensitive hashing).", "labels": [], "entities": [{"text": "Nearest neighbor search", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7235939105351766}, {"text": "O", "start_pos": 132, "end_pos": 133, "type": "METRIC", "confidence": 0.9592270255088806}]}, {"text": "The problem we are concerned within this paper is not the complexity of nearest neighbor search, but the question of how to identify the internal structure of neighborhoods defined by the nearest neighbors.", "labels": [], "entities": []}, {"text": "The problem with a normal k-NN is that the result -a sorted list of the k nearest neighbors -does not say anything about the internal structure of the neighborhood.", "labels": [], "entities": []}, {"text": "It is quite possible for two neighborhoods with widely different internal structures to produce identical k-NN results.", "labels": [], "entities": []}, {"text": "In the context of Distributional Semantic Models (DSMs), which collect and represent cooccurrence statistics in high-dimensional vector spaces, such structural differences may carry significant semantic information, e.g. about the different senses of terms.", "labels": [], "entities": [{"text": "Distributional Semantic Models (DSMs)", "start_pos": 18, "end_pos": 55, "type": "TASK", "confidence": 0.6601956884066263}]}, {"text": "We argue that the inability of standard k-NN to account for structural properties has been misinterpreted as a shortcoming of the distributional representation).", "labels": [], "entities": []}, {"text": "We will demonstrate in this paper that this is not a shortcoming of the distributional representation, but of the mode of querying the DSM.", "labels": [], "entities": [{"text": "DSM", "start_pos": 135, "end_pos": 138, "type": "DATASET", "confidence": 0.8792931437492371}]}, {"text": "We argue that information about the different usages (i.e. senses) of a term is encoded in the structural properties of the nearest neighborhoods, and we propose the use of relative neighborhood graphs for identifying these structural properties.", "labels": [], "entities": []}, {"text": "Relative neighborhood graphs may also be used for finding a relevant k fora given reference point, which we refer to as the horizon with respect to the reference point.", "labels": [], "entities": []}], "datasetContent": [{"text": "The standard way to evaluate WSI algorithms is to use one the SemEval WSI test collections, which are all designed similarly: systems are expected to first perform WSI and then to assign texts to the induced senses (i.e. in effect doing a word-sense disambiguation step).", "labels": [], "entities": [{"text": "SemEval WSI test collections", "start_pos": 62, "end_pos": 90, "type": "DATASET", "confidence": 0.719878688454628}]}, {"text": "We consider this type of evaluation to be a less useful for our purposes, since the required disambiguation step is a highly non-trivial task in itself.", "labels": [], "entities": []}, {"text": "The RNG method proposed in this paper is a pure WSI algorithm, and as such does not offer a solution to the disambiguation problem.", "labels": [], "entities": []}, {"text": "We therefore opted to focus solely on the hypothesis that relative neighborhoods cover senses that k-NNs do not.", "labels": [], "entities": []}, {"text": "In essence, we investigate whether k-RNG retrieval does a better job at covering different senses than k-NN retrieval.", "labels": [], "entities": []}, {"text": "This was done using pseudowords.", "labels": [], "entities": []}, {"text": "Pseudowords are artificially ambiguous words, created by regarding different words as identical.", "labels": [], "entities": []}, {"text": "We can, for example, say that the pseu- doword <deadeye> is a composite of the two words marksman and loudspeaker.", "labels": [], "entities": []}, {"text": "A corpus with the artificially ambiguous word <deadeye> in it can then be created by replacing all occurrences of the words marksman and loudspeaker with <deadeye>.", "labels": [], "entities": []}, {"text": "Using the pseudowords provided by Pilehvar and Navigli (2013) a corpus with 689 nonoverlapping pseudowords was created, based on the BNC corpus.", "labels": [], "entities": [{"text": "BNC corpus", "start_pos": 133, "end_pos": 143, "type": "DATASET", "confidence": 0.9820353984832764}]}, {"text": "7 Two models were then trained, one on the altered corpus, and one on the unaltered one.", "labels": [], "entities": []}, {"text": "To check whether the neighborhood of a pseudoword contains information about its underlying senses we compared each underlying sense to the words in the neighborhood, taking the minimum of all senses' maximum similarity as a score, as demonstrated in.", "labels": [], "entities": []}, {"text": "The similarities were calculated using the model trained on the unaltered corpus, as the one based on the altered corpus will not contain the underlying senses of pseudowords.", "labels": [], "entities": []}, {"text": "Working through the example in, the neighborhood of the pseudoword <deadeye > consists of the three words shooter, stereo, and sport.", "labels": [], "entities": []}, {"text": "The pseudoword in itself is made up of the two underlying senses marksman and loudspeaker.", "labels": [], "entities": []}, {"text": "The similarities between the words in the neighborhood of the model trained on the unal-7 www.natcorp.ox.ac.uk tered data and the words of the underlying senses are as presented in.", "labels": [], "entities": [{"text": "unal-7 www.natcorp.ox.ac.uk tered data", "start_pos": 83, "end_pos": 121, "type": "DATASET", "confidence": 0.9305103570222855}]}, {"text": "The closest word to marksman is shooter, with a similarity score of 0.7.", "labels": [], "entities": [{"text": "shooter", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.7126035690307617}, {"text": "similarity score", "start_pos": 48, "end_pos": 64, "type": "METRIC", "confidence": 0.9864360988140106}]}, {"text": "The closest word to loudspeaker is stereo, with a score of 0.3.", "labels": [], "entities": [{"text": "stereo", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9890881776809692}]}, {"text": "So the scoring would, in total, be 0.3.", "labels": [], "entities": []}, {"text": "It should be noted that the upper bound for this score is oftentimes significantly lower than 1: The neighborhood could not possibly contain the words marksman or loudspeaker, as those words are not present in the corpus.", "labels": [], "entities": []}, {"text": "This means that the scores are bounded by the similarity of the least similar closest neighbor to the underlying senses.", "labels": [], "entities": []}, {"text": "This score was chosen because of its simplicity and intuitive interpretation: a low score implies that at least one word sense was not represented in the neighborhood whereas a high score means that all senses are represented in the neighborhood.", "labels": [], "entities": []}, {"text": "One can then plot these scores for both relative neighborhoods and k-NN neighborhoods for each pseudoword as is done in.", "labels": [], "entities": []}, {"text": "Each point (x, y) represents a pseudoword, with x and y being the score of the k-NN neighborhood and the k-RNG neighborhood respectively.", "labels": [], "entities": []}, {"text": "shows an aggregate of, plotting the distribution of y \u2212x, i.e. the difference between the scores achieved by the k-RNG and the k-NN.", "labels": [], "entities": []}, {"text": "As seen in, a lot of points lie on the line y = x, meaning both methods achieved the same score.", "labels": [], "entities": []}, {"text": "However, when this is not the case, there is a clear bias for the k-RNG to outperform the k-NN, as demonstrated in.", "labels": [], "entities": []}, {"text": "Here, using the BNC instead of Wikipedia as training data, the GloVe and Skipgram models yielded sparse relative neighborhoods -both with an average of about 8 neighbors -but the PMI model produced quite dense neighborhoods averaging 63 neighbors.", "labels": [], "entities": [{"text": "BNC", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.9306626915931702}]}, {"text": "Since the scoring function does not penalize neighborhood size there is good reason to be skeptical of its viability, and specifically the performance of the PMI-model based on these figures.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: k-RNG for k = 1, 000 of the words \"ser- vice,\" \"bad,\" and \"above\" in three different seman- tic models. The numbers in parenthesis indicate  the k-NN ranks of the neighbors.", "labels": [], "entities": []}, {"text": " Table 4. The closest word to  marksman is shooter, with a similarity score of  0.7. The closest word to loudspeaker is stereo,  with a score of 0.3. So the scoring would, in total,  be 0.3. It should be noted that the upper bound for  this score is oftentimes significantly lower than 1:  The neighborhood could not possibly contain the  words marksman or loudspeaker, as those words  are not present in the corpus. This means that the  scores are bounded by the similarity of the least  similar closest neighbor to the underlying senses.", "labels": [], "entities": []}, {"text": " Table 4: Example scoring of a neighborhood of  the word <deadeye>.", "labels": [], "entities": []}]}