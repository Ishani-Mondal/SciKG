{"title": [], "abstractContent": [{"text": "In this study, we consider a summariza-tion method using the document level similarity based on embeddings, or distributed representations of words, where we assume that an embedding of each word can represent its \"meaning.\"", "labels": [], "entities": []}, {"text": "We formalize our task as the problem of maximizing a sub-modular function defined by the negative summation of the nearest neighbors' distances on embedding distributions, each of which represents a set of word embed-dings in a document.", "labels": [], "entities": []}, {"text": "We proved the sub-modularity of our objective function and that our problem is asymptotically related to the KL-divergence between the probability density functions that correspond to a document and its summary in a continuous space.", "labels": [], "entities": []}, {"text": "An experiment using areal dataset demonstrated that our method performed better than the existing method based on sentence-level similarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Document summarization aims to rephrase a document in a short form called a summary while keeping its \"meaning.\"", "labels": [], "entities": [{"text": "Document summarization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9014489352703094}]}, {"text": "In the present study, we aim to characterize the meaning of a document using embeddings or distributed representations of words in the document, where an embedding of each word is represented as areal valued vector in a Euclidean space that corresponds to the word ().", "labels": [], "entities": [{"text": "characterize the meaning of a document", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.7958352665106455}]}, {"text": "Many previous studies have investigated summarization (), but to the best of our knowledge, only one) considered a direct summarization method using embeddings, where the summarization problem was formalized as maximizing a submodular function defined by the summation of cosine similarities based on sentence embeddings.", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9810444712638855}]}, {"text": "Essentially, this method assumes linear meanings since the objective function is characterized by the summation of sentence-level similarities.", "labels": [], "entities": []}, {"text": "However, this assumption is not always valid in real documents, and thus there maybe a better combination of two other sentences than the best and second best sentences in terms of similarity in a document.", "labels": [], "entities": []}, {"text": "In this study, we consider a summarization method based on document-level similarity, where we assume the non-linearity of meanings.", "labels": [], "entities": [{"text": "summarization", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.9779133200645447}]}, {"text": "First, we examine an objective function defined by a cosine similarity based on document embeddings instead of sentence embeddings.", "labels": [], "entities": []}, {"text": "Unfortunately, in contrast to our intuition, this similarity is not submodular, which we disprove later.", "labels": [], "entities": []}, {"text": "Thus, we propose a valid submodular function based on embedding distributions, each of which represents a set of word embeddings in a document, as the document-level similarity.", "labels": [], "entities": []}, {"text": "Our objective function is calculated based on the nearest neighbors' distances on embedding distributions, which can be proved to be asymptotically related to KLdivergence in a continuous space.", "labels": [], "entities": []}, {"text": "Several studies ( have addressed summarization using KL-divergence, but they calculated KLdivergence based on word distributions in a discrete space.", "labels": [], "entities": [{"text": "summarization", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9910858273506165}]}, {"text": "In other words, our study is the first attempt to summarize by asymptotically estimating KL-divergence based on embedding distributions in a continuous space.", "labels": [], "entities": [{"text": "summarize", "start_pos": 50, "end_pos": 59, "type": "TASK", "confidence": 0.9714595079421997}]}, {"text": "In addition, they involved the inference of complex models, whereas our method is quite simple but still powerful.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compared our two proposed methods DocEmb and EmbDist with two state-of-the-art methods SenEmb and TfIdf.", "labels": [], "entities": [{"text": "DocEmb", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.9094806909561157}]}, {"text": "The first two methods DocEmb and EmbDist represent Algorithm 1 with our proposed objective functions f Cos and f NN , respectively.", "labels": [], "entities": [{"text": "DocEmb", "start_pos": 22, "end_pos": 28, "type": "DATASET", "confidence": 0.9465757012367249}]}, {"text": "TfIdf represents Algorithm 1 with an objective function based on the sum of cosine similarities of tf-idf vectors that correspond to sentences, which was proposed in ().", "labels": [], "entities": []}, {"text": "SenEmb uses a cosine similarity measure based on embeddings instead of tf-idf vectors in the same framework as TfIdf, which was proposed in).", "labels": [], "entities": [{"text": "SenEmb", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8229349255561829}]}, {"text": "We conducted an experiment with almost the same setting as in the previous study, where they used the Opinosis dataset (.", "labels": [], "entities": [{"text": "Opinosis dataset", "start_pos": 102, "end_pos": 118, "type": "DATASET", "confidence": 0.8675976097583771}]}, {"text": "This dataset is a collection of user reviews in 51 different topics such as hotels, cars, and products; thus, it is more appropriate for evaluating summarization of user-generated content than wellknown DUC datasets, which consist of formal news articles.", "labels": [], "entities": [{"text": "summarization of user-generated content", "start_pos": 148, "end_pos": 187, "type": "TASK", "confidence": 0.8282043486833572}, {"text": "DUC datasets", "start_pos": 203, "end_pos": 215, "type": "DATASET", "confidence": 0.7986216843128204}]}, {"text": "Each topic in the collection comprises 50-575 sentences and includes four and five gold standard summaries created by human authors, each of which comprises 1-3 sentences.", "labels": [], "entities": []}, {"text": "We ran an optimization process to choose sentences within 100 words 3 by setting the summary size and weights as = 100 and w s = |s| for any sentence s, respectively.", "labels": [], "entities": []}, {"text": "As for TfIdf and SenEmb, we set a cluster size of k-means ask = |D|/5 and chose the best value fora threshold coefficient \u03b1, trade-off coefficient \u03bb, and the scaling factor r, as in (.", "labels": [], "entities": [{"text": "trade-off coefficient \u03bb", "start_pos": 125, "end_pos": 148, "type": "METRIC", "confidence": 0.8764519691467285}]}, {"text": "Note that our functions DocEmb and EmbDist have only one parameter r, and we similarly chose the best value of r.", "labels": [], "entities": [{"text": "DocEmb", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.9534264206886292}]}, {"text": "Regarding DocEmb, EmbDist, and SenEmb, we used the best embeddings from the CW Vector and W2V Vector for each method, and created document and sentence embeddings by averaging word embeddings with tf-idf weights since it performed better in this experiment.", "labels": [], "entities": []}, {"text": "In the case of EmbDist, we used a variant off NN based R-1 R-2 R-3 R-4 on distributions of sentence embeddings.", "labels": [], "entities": []}, {"text": "In addition, we examined three scaling functions: logarithmic, linear, and exponential functions, i.e., ln x, x, ex , respectively.", "labels": [], "entities": []}, {"text": "We calculated the ROUGE-N metric , which is a widely-used evaluation metric for summarization methods.", "labels": [], "entities": [{"text": "ROUGE-N metric", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.9801218807697296}, {"text": "summarization", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.9856907725334167}]}, {"text": "ROUGE-N is based on the co-occurrence statistics of N-grams, and especially ROUGE-1 has been shown to have the highest correlation with human summaries (.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8316112160682678}, {"text": "ROUGE-1", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.8868010640144348}]}, {"text": "ROUGE-N is similar to the BLEU metric for machine translation, but ROUGE-N is a recall-based metric while BLEU is a precision-based metric.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9644262790679932}, {"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9908491373062134}, {"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8042149841785431}, {"text": "ROUGE-N", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.951264500617981}, {"text": "recall-based", "start_pos": 80, "end_pos": 92, "type": "METRIC", "confidence": 0.9960305094718933}, {"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9941114783287048}, {"text": "precision-based", "start_pos": 116, "end_pos": 131, "type": "METRIC", "confidence": 0.9896740913391113}]}, {"text": "shows the results obtained for ROUGE-N (N \u2264 4) using DocEmb, EmbDist, SenEmb, and TfIdf.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.7619274854660034}, {"text": "DocEmb", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.9556936621665955}]}, {"text": "ApxOpt represents the approximation results of the optimal solution in our problem, where we optimized ROUGE-1 with the gold standard summaries by Algorithm 1.", "labels": [], "entities": [{"text": "ApxOpt", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9615896344184875}, {"text": "ROUGE-1", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.9662835597991943}]}, {"text": "The obtained results indicate that our proposed method EmbDist with exponential scaling performed the best for ROUGE-1, which is the best metric in terms of correlation with human summaries.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9662630558013916}]}, {"text": "The W2V Vector was the best choice for EmbDist.", "labels": [], "entities": [{"text": "EmbDist", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9721301198005676}]}, {"text": "Furthermore, the other proposed method DocEmb performed better than the state-of-the-art methods SenEmb and TfIdf, although DocEmb is not theoretically guaranteed to obtain a near optimal solution.", "labels": [], "entities": [{"text": "DocEmb", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.892193078994751}]}, {"text": "These results imply that our methods based on the document-level similarity can capture more complex meanings than the sentence-level similarity.", "labels": [], "entities": []}, {"text": "On the other hand, TfIdf with tf-idf vectors performed the worst for ROUGE-1.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.46347299218177795}]}, {"text": "A possible reason is that a wide variety of expressions by users made it difficult to calculate similarities.", "labels": [], "entities": []}, {"text": "This also suggests that embedding-based methods naturally have robustness for user-generated content.", "labels": [], "entities": []}, {"text": "In the case of N \u2265 2, TfIdf performed the best for ROUGE-2 and ROUGE-3, while EmbDist with logarithmic scaling is better than TfIdf for ROUGE-4.", "labels": [], "entities": [{"text": "TfIdf", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9422743916511536}]}, {"text": "According to, the higher order ROUGE-N is worse than ROUGE-1 since it tends to score grammaticality rather than content.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9527575373649597}]}, {"text": "Conversely, reports that there is a dataset where the higher order ROUGE-N is correlated with human summaries well.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9894557595252991}]}, {"text": "We may need to conduct human judgments to decide which metric is the best in this dataset for more accurate comparison.", "labels": [], "entities": []}, {"text": "However, it is still important that our simple objective functions can obtain good results competing with the state-of-the-art methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8243997693061829}, {"text": "DocEmb", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.9685143828392029}]}]}