{"title": [{"text": "Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-Level Multimodal Sentiment Analysis", "labels": [], "entities": [{"text": "Utterance-Level Multimodal Sentiment Analysis", "start_pos": 84, "end_pos": 129, "type": "TASK", "confidence": 0.6074458286166191}]}], "abstractContent": [{"text": "We present a novel way of extracting features from short texts, based on the activation values of an inner layer of a deep convolutional neural network.", "labels": [], "entities": []}, {"text": "We use the extracted features in multimodal sentiment analysis of short video clips representing one sentence each.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.7000284592310587}]}, {"text": "We use the combined feature vectors of textual, visual , and audio modalities to train a classi-fier based on multiple kernel learning, which is known to be good at heterogeneous data.", "labels": [], "entities": []}, {"text": "We obtain 14% performance improvement over the state of the art and present a parallelizable decision-level data fusion method, which is much faster, though slightly less accurate.", "labels": [], "entities": [{"text": "decision-level data fusion", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.6720262070496877}]}], "introductionContent": [{"text": "The advent of the Social Web has enabled anyone with a smartphone or computer to easily create and share their ideas, opinions and content with millions of other people around the world.", "labels": [], "entities": []}, {"text": "Much of the content being posted and consumed online is video.", "labels": [], "entities": []}, {"text": "With billions of phones, tablets and PCs shipping today with built-in cameras and a host of new video-equipped wearables like Google Glass on the horizon, the amount of video on the Internet will only continue to increase.", "labels": [], "entities": []}, {"text": "It has become increasingly difficult for researchers to keep up with this deluge of video content, let alone organize or make sense of it.", "labels": [], "entities": []}, {"text": "Mining useful knowledge from video is a critical need that will grow exponentially, in pace with the global growth of content.", "labels": [], "entities": []}, {"text": "This is particularly important in sentiment analysis, as both service and product reviews are gradually shifting from unimodal to multimodal.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9771455526351929}]}, {"text": "We present a method for detecting sentiment polarity in short video clips of a person uttering a sentence.", "labels": [], "entities": [{"text": "detecting sentiment polarity", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.7222004731496176}]}, {"text": "We do it using all three modalities: visual, such as facial expression, audio, such as pitch, and textual, the contents of the uttered sentence.", "labels": [], "entities": []}, {"text": "While the visual and the audio modalities provide additional evidence that improves classification accuracy, we found the textual modality to have the greater impact on the result.", "labels": [], "entities": [{"text": "classification", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.9630586504936218}, {"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9414170980453491}]}, {"text": "In this paper, we propose a novel way for feature extraction from text.", "labels": [], "entities": [{"text": "feature extraction from text", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.8306262344121933}]}, {"text": "Given a training corpus with hand-annotated sentiment polarity labels, following, we train a deep convolutional neural network (CNN) on it.", "labels": [], "entities": []}, {"text": "However, instead of using it as a classifier, as Kim did, we use the values from its hidden layer as features fora much more advanced classifier, which gives superior accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9970760345458984}]}, {"text": "Similar ideas have been suggested in the context of computer vision for dealing with images, but have not been applied in the context of NLP to textual data, and, specifically, for sentiment polarity classification.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 181, "end_pos": 214, "type": "TASK", "confidence": 0.8779099980990092}]}], "datasetContent": [{"text": "We report results for tenfold cross-validation.", "labels": [], "entities": []}, {"text": "We experimented on the dataset described by.", "labels": [], "entities": []}, {"text": "The dataset consists of 498 short video fragments where a person utters one sentence.", "labels": [], "entities": []}, {"text": "The items are manually tagged for sentiment polarity, which can be positive, negative, or neutral.", "labels": [], "entities": [{"text": "sentiment polarity", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7906894981861115}]}, {"text": "We discarded the neutral items from the dataset, which gave us a dataset of 447 clips tagged as positive or negative.", "labels": [], "entities": []}, {"text": "The video in the dataset is present in MP4 format with the resolution of 360 \uf0b4 480, to which the developers converted all videos originally collected in different formats with different resolution.", "labels": [], "entities": []}, {"text": "The duration of the clips is about 5 seconds on average.", "labels": [], "entities": [{"text": "duration", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9612051248550415}]}, {"text": "About 80% of the clips present female speakers.", "labels": [], "entities": []}, {"text": "The developers provided transcription of the text of the sentences, which we used in our textual modality processing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Accuracy of state-of-the-art method compared with our method with feature-level fusion.  The number of features is for our experiments, not for [16]. Shaded cells are shared with Table 2.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9875088334083557}]}, {"text": " Table 2. Accuracy of our method with decision-level fusion and feature selection.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9712404608726501}, {"text": "decision-level fusion", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.8163051009178162}]}]}