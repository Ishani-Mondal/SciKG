{"title": [{"text": "Flexible Domain Adaptation for Automated Essay Scoring Using Correlated Linear Regression", "labels": [], "entities": [{"text": "Flexible Domain Adaptation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.586943914492925}, {"text": "Automated Essay Scoring", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.6810391942660013}]}], "abstractContent": [{"text": "Most of the current automated essay scoring (AES) systems are trained using manually graded essays from a specific prompt.", "labels": [], "entities": [{"text": "essay scoring (AES)", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8102608442306518}]}, {"text": "These systems experience a drop inaccuracy when used to grade an essay from a different prompt.", "labels": [], "entities": []}, {"text": "Obtaining a large number of manually graded essays each time anew prompt is introduced is costly and not viable.", "labels": [], "entities": []}, {"text": "We propose domain adaptation as a solution to adapt an AES system from an initial prompt to anew prompt.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7393187284469604}]}, {"text": "We also propose a novel domain adaptation technique that uses Bayesian linear ridge regression.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7761933505535126}]}, {"text": "We evaluate our domain adaptation technique on the publicly available Automated Student Assessment Prize (ASAP) dataset and show that our proposed technique is a competitive default domain adaptation algorithm for the AES task.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.733653724193573}, {"text": "Automated Student Assessment Prize (ASAP) dataset", "start_pos": 70, "end_pos": 119, "type": "DATASET", "confidence": 0.6937047727406025}, {"text": "AES task", "start_pos": 218, "end_pos": 226, "type": "TASK", "confidence": 0.5236701965332031}]}], "introductionContent": [{"text": "Essay writing is a common task evaluated in schools and universities.", "labels": [], "entities": [{"text": "Essay writing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9510959386825562}]}, {"text": "In this task, students are typically given a prompt or essay topic to write about.", "labels": [], "entities": []}, {"text": "Essay writing is included in high-stakes assessments, such as Test of English as a Foreign Language (TOEFL) and Graduate Record Examination (GRE).", "labels": [], "entities": [{"text": "Essay writing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.82232266664505}]}, {"text": "Manually grading all essays takes a lot of time and effort for the graders.", "labels": [], "entities": []}, {"text": "This is what Automated Essay Scoring (AES) systems are trying to alleviate.", "labels": [], "entities": [{"text": "Automated Essay Scoring (AES)", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.7416012287139893}]}, {"text": "Automated Essay Scoring uses computer software to automatically evaluate an essay written in an educational setting by giving it a score.", "labels": [], "entities": [{"text": "Automated Essay Scoring", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7303509016831716}]}, {"text": "Work related to essay scoring can be traced back to 1966 when Ellis Page created a computer grading software called Project Essay Grade (PEG).", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.8257988095283508}]}, {"text": "Research on AES has continued through the years.", "labels": [], "entities": [{"text": "AES", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.6479514837265015}]}, {"text": "The recent Automated Student Assessment Prize (ASAP) Competition 1 sponsored by the Hewlett Foundation in 2012 has renewed interest on this topic.", "labels": [], "entities": [{"text": "Automated Student Assessment Prize (ASAP) Competition 1", "start_pos": 11, "end_pos": 66, "type": "TASK", "confidence": 0.6003290017445883}, {"text": "Hewlett Foundation", "start_pos": 84, "end_pos": 102, "type": "DATASET", "confidence": 0.9148966073989868}]}, {"text": "The agreement between the scores assigned by state-of-the-art AES systems and the scores assigned by human raters has been shown to be relatively high.", "labels": [], "entities": []}, {"text": "See fora recent overview of AES.", "labels": [], "entities": [{"text": "AES", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.6199368834495544}]}, {"text": "AES is usually treated as a supervised machine learning problem, either as a classification, regression, or rank preference task.", "labels": [], "entities": [{"text": "AES", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7092484831809998}]}, {"text": "Using this approach, a training set in the form of human graded essays is needed.", "labels": [], "entities": []}, {"text": "However, human graded essays are not readily available.", "labels": [], "entities": []}, {"text": "This is perhaps why research in this area was mostly done by commercial organizations.", "labels": [], "entities": []}, {"text": "After the ASAP competition, research interest in this area has been rekindled because of the released dataset.", "labels": [], "entities": [{"text": "ASAP competition", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6251108050346375}]}, {"text": "Most of the recent AES related work is promptspecific.", "labels": [], "entities": [{"text": "AES related", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.6117880940437317}]}, {"text": "That is, an AES system is trained using essays from a specific prompt and tested against essays from the same prompt.", "labels": [], "entities": [{"text": "AES", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.6323885321617126}]}, {"text": "These AES systems will notwork as well when tested against a different prompt.", "labels": [], "entities": [{"text": "AES", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.77018141746521}]}, {"text": "Furthermore, generating the training data each time anew prompt is introduced will be costly and time consuming.", "labels": [], "entities": []}, {"text": "In this paper, we propose domain adaptation as a solution to this problem.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7956607639789581}]}, {"text": "Instead of hiring people to grade new essays each time anew prompt is introduced, domain adaptation can be used to adapt the old prompt-specific system to suit the new prompt.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.723658099770546}]}, {"text": "This way, a smaller number of training essays from the new prompt is needed.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel domain adaptation technique based on Bayesian linear ridge regression.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.762490451335907}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we give an overview of related work on AES and domain adaptation.", "labels": [], "entities": [{"text": "AES", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.7079577445983887}, {"text": "domain adaptation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7215036898851395}]}, {"text": "Section 3 describes the AES task and the features used.", "labels": [], "entities": [{"text": "AES task", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.6196986734867096}]}, {"text": "Section 4 presents our novel domain adaptation algorithm.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7834592461585999}]}, {"text": "Section 5 describes our data, experimental setup, and evaluation metric.", "labels": [], "entities": []}, {"text": "Section 6 presents and discusses the results.", "labels": [], "entities": []}, {"text": "We conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we will give a brief description of the dataset we use, describe our experimental setup, and explain the evaluation metric we use.", "labels": [], "entities": []}, {"text": "We use 5-fold cross validation on the ASAP training data for evaluation.", "labels": [], "entities": [{"text": "ASAP training data", "start_pos": 38, "end_pos": 56, "type": "DATASET", "confidence": 0.7225275039672852}]}, {"text": "This is because the official test data of the competition is not released to the public.", "labels": [], "entities": []}, {"text": "We divide the target domain data randomly into 5 folds.", "labels": [], "entities": []}, {"text": "One fold is used as the test data, while the remaining four folds are collected together and then sub-sampled to obtain the target-domain training data.: Selected details of the ASAP data.", "labels": [], "entities": [{"text": "ASAP data", "start_pos": 178, "end_pos": 187, "type": "DATASET", "confidence": 0.81178879737854}]}, {"text": "For the genre column, ARG denotes argumentative essays, RES denotes response essays, and NAR denotes narrative essays.", "labels": [], "entities": [{"text": "ARG", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9851972460746765}, {"text": "RES", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9845374822616577}, {"text": "NAR", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.972929060459137}]}, {"text": "Our evaluation considers the following four ways in which we train the AES model: SourceOnly Using essays from the source domain only; TargetOnly Using 10, 25, 50, and 100 sampled essays from the target domain only; SharedHyper Using correlated Bayesian linear ridge regression (BLRR) with \u03c1 fixed to 0 on source domain essays and sampled essays from the target domain.", "labels": [], "entities": []}, {"text": "EasyAdapt As SharedHyper, but with \u03c1 = 0.5; Concat As SharedHyper, but with \u03c1 fixed to 1.0; ML-\u03c1 Using correlated BLRR with \u03c1 maximizing the likelihood of the data.", "labels": [], "entities": [{"text": "EasyAdapt", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8893316388130188}]}, {"text": "Since the source and target domain may have different score ranges, we scale the scores linearly to range from \u22121 to 1.", "labels": [], "entities": []}, {"text": "When predicting on the test essays, the predicted scores of our system will be linearly scaled back to the target domain score range and rounded to the nearest integer.", "labels": [], "entities": []}, {"text": "We build upon scikit-learn's implementation of BLRR for our learning algorithm.", "labels": [], "entities": [{"text": "BLRR", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.5858346819877625}]}, {"text": "To ameliorate the effects of different scales of features, we normalize the features: length, POS, and prompt features are linearly scaled to range from 0 to 1 according to the training data; and the feature values for bag-of-words features are log(1 + count) instead of the actual counts.", "labels": [], "entities": [{"text": "length", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9681712985038757}, {"text": "POS", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.8743127584457397}]}, {"text": "We use scikit-learn version 0.15.2, NLTK version 2.0b7, and aspell version 0.60.6.1 in this experiment.", "labels": [], "entities": []}, {"text": "The BLRR code (bayes.py) in scikitlearn is modified to obtain valid likelihoods for use in the outer loop for estimating \u03c1.", "labels": [], "entities": [{"text": "BLRR", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8663842082023621}, {"text": "estimating \u03c1", "start_pos": 110, "end_pos": 122, "type": "TASK", "confidence": 0.8322083055973053}]}, {"text": "We use scikitlearn's default value for the parameters \u03b1 1 , \u03b1 2 , \u03bb 1 , and \u03bb 2 which is 10 \u22126 .: In-domain experimental results.", "labels": [], "entities": []}, {"text": "Quadratic weighted Kappa (QWK) is used to measure the agreement between the human rater and the system.", "labels": [], "entities": [{"text": "Quadratic weighted Kappa (QWK)", "start_pos": 0, "end_pos": 30, "type": "METRIC", "confidence": 0.9341169993082682}]}, {"text": "We choose to use this evaluation metric since it is the official evaluation metric of the ASAP competition.", "labels": [], "entities": [{"text": "ASAP competition", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.6167197823524475}]}, {"text": "Other work such as) that uses the ASAP dataset also uses this evaluation metric.", "labels": [], "entities": [{"text": "ASAP dataset", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.9104864597320557}]}, {"text": "QWK is calculated using where matrices O, (w i,j ), and E are the matrices of observed scores, weights, and expected scores respectively.", "labels": [], "entities": [{"text": "QWK", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7347452044487}, {"text": "O", "start_pos": 39, "end_pos": 40, "type": "METRIC", "confidence": 0.9765365123748779}]}, {"text": "Matrix O i,j corresponds to the number of essays that receive a score i by the first rater and a score j by the second rater.", "labels": [], "entities": []}, {"text": "The weight entries are w i,j = (i \u2212 j) 2 /(N \u2212 1) 2 , where N is the number of possible ratings.", "labels": [], "entities": []}, {"text": "Matrix E is calculated by taking the outer product between the score vectors of the two raters, which are then normalized to have the same sum as O.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Selected details of the ASAP data. For  the genre column, ARG denotes argumentative es- says, RES denotes response essays, and NAR de- notes narrative essays.", "labels": [], "entities": [{"text": "ASAP data", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.8524899482727051}, {"text": "ARG", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9967009425163269}, {"text": "RES", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.990858256816864}]}, {"text": " Table 3: In-domain experimental results.", "labels": [], "entities": []}, {"text": " Table 4: QWK scores of the six methods on four  domain adaptation experiments, ranging from us- ing 10 target-domain essays (second column) to  100 target-domain essays (fifth column). The  scores are the averages over 5 folds. Setting a \u2192 b  means the AES system is trained on essay set a  and tested on essay set b. For each set of six results  comparing the methods, the best score is bold- faced and the second-best score is underlined.", "labels": [], "entities": []}]}