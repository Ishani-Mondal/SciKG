{"title": [{"text": "Building a shared world: Mapping distributional to model-theoretic semantic spaces", "labels": [], "entities": [{"text": "Mapping distributional to model-theoretic semantic spaces", "start_pos": 25, "end_pos": 82, "type": "TASK", "confidence": 0.8230564792950948}]}], "abstractContent": [{"text": "In this paper, we introduce an approach to automatically map a standard distributional semantic space onto a set-theoretic model.", "labels": [], "entities": []}, {"text": "We predict that there is a functional relationship between distributional information and vecto-rial concept representations in which dimensions are predicates and weights are gener-alised quantifiers.", "labels": [], "entities": []}, {"text": "In order to test our prediction , we learn a model of such relationship over a publicly available dataset of feature norms annotated with natural language quan-tifiers.", "labels": [], "entities": []}, {"text": "Our initial experimental results show that, at least for domain-specific data, we can indeed map between formalisms, and generate high-quality vector representations which encapsulate set overlap information.", "labels": [], "entities": []}, {"text": "We further investigate the generation of natural language quantifiers from such vectors.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, the complementarity of distributional and formal semantics has become increasingly evident.", "labels": [], "entities": []}, {"text": "While distributional semantics) has proved very successful in modelling lexical effects such as graded similarity and polysemy, it clearly has difficulties accounting for logical phenomena which are well covered by model-theoretic semantics.", "labels": [], "entities": []}, {"text": "A number of proposals have emerged from these considerations, suggesting that an overarching semantics integrating both distributional and formal aspects would be desirable).", "labels": [], "entities": []}, {"text": "We will use the term 'Formal Distributional Semantics' (FDS) to refer to such proposals.", "labels": [], "entities": [{"text": "Formal Distributional Semantics' (FDS)", "start_pos": 22, "end_pos": 60, "type": "TASK", "confidence": 0.6435603549083074}]}, {"text": "This paper follows this line of work, focusing on one central question: the formalisation of the systematic dependencies between lexical and set-theoretic levels.", "labels": [], "entities": []}, {"text": "Let us consider the following examples.", "labels": [], "entities": []}, {"text": "The preferred reading of 1 has a logical form where the object is treated as an existential, while the object in 2 has a generic reading: \u2022 GEN x[book (x) \u2192 like ( with x * indicating a plurality and GEN the generic quantifier.", "labels": [], "entities": []}, {"text": "It is generally accepted that the appropriate choice of quantifier for an ambiguous bare plural object depends, amongst other things, on the lexical semantics of the verb (e.g.).", "labels": [], "entities": []}, {"text": "This type of interaction implies the existence of systematic influences of the lexicon over logic, which could in principle be formalised.", "labels": [], "entities": []}, {"text": "A model of the lexicon/logic interface would be desirable to explain how speakers resolve standard cases of ambiguity like the bare plural in 1 and 2, but more generally, it could be the basis for answering a more fundamental question: how do speakers construct a model of a sentence for which they have no prior perceptual data?", "labels": [], "entities": []}, {"text": "People can make complex inferences about statements without having access to their real-world reference.", "labels": [], "entities": []}, {"text": "As an example, consider the sentence The kouprey is a mammal.", "labels": [], "entities": []}, {"text": "English speakers have no problem ascertaining that if x is a kouprey, x is a mammal (which set-theoretic semantics would express as \u2200x[kouprey (x) \u2192 mammal (x)]), regardless of whether they have ever encountered a kouprey.", "labels": [], "entities": []}, {"text": "The inference is supported by the lexical semantics of mammal, which applies a property (being a mammal) to all instances of a class.", "labels": [], "entities": []}, {"text": "Much more complex inferences are routinely performed by speakers, down to estimating the cardinality of the entities involved in a particular situation.", "labels": [], "entities": []}, {"text": "Compare e.g. The cats are on the sofa (2 / a few cats?), I picked pears today (a few / a few dozen?) and The protesters were blocking the entire avenue (hundreds/thousands of protesters?).", "labels": [], "entities": []}, {"text": "Understanding how this process works would not only give us an insight into a complex cognitive process, but also make a crucial contribution to NLP tasks relying on inference (e.g. the Recognising Textual Entailment challenge, RTE:).", "labels": [], "entities": [{"text": "Recognising Textual Entailment challenge", "start_pos": 186, "end_pos": 226, "type": "TASK", "confidence": 0.7334039807319641}]}, {"text": "Indeed, while systems have successfully been developed to model entailment between quantifiers, ranging from natural logic approaches to distributional semantics solutions (, they rely on an explicit representation of quantification.", "labels": [], "entities": []}, {"text": "That is, they can model the entailment All koupreys are mammals |= This kouprey is a mammal, but not Koupreys are mammals |= This kouprey is a mammal.", "labels": [], "entities": []}, {"text": "In this work, we assume the existence of a mapping between language (distributional models) and world (set-theoretic models), or to be more precise, between language and a shared set of beliefs about the world, as negotiated by a group of speakers.", "labels": [], "entities": []}, {"text": "To operationalise this mapping, we propose that set-theoretic models, like distributions, can be expressed in terms of vectors -giving us a common representation across formalisms.", "labels": [], "entities": []}, {"text": "Using a publicly available dataset of feature norms annotated with quantifiers 1, we show that human-like intuitions about the quantification of simple subject/predicate pairs can be induced from standard distributional data.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "\u00a72 reviews related work, focusing in turn on approaches to formal distributional semantics, computational work on quantification, and mapping between semantic spaces.", "labels": [], "entities": []}, {"text": "In \u00a73, we describe our dataset.", "labels": [], "entities": []}, {"text": "\u00a74 and \u00a75 describe our experiments, reporting correlation against human annotations.", "labels": [], "entities": []}, {"text": "We discuss our results in \u00a76 and end with an attempt at generating natural language quantifiers from our mapped vectors ( \u00a77).", "labels": [], "entities": []}], "datasetContent": [{"text": "The McRae norms) area set of feature norms elicited from 725 human participants for 541 concepts covering living and non-living entities (e.g. alligator, chair, accordion).", "labels": [], "entities": [{"text": "McRae norms) area set", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.9206279158592224}]}, {"text": "The annotators were given concepts and asked to provide features for them, covering physical, functional and other properties.", "labels": [], "entities": []}, {"text": "The result is a set of 7257 concept-feature pairs such as airplane used-for-passengers or bear is-brown.", "labels": [], "entities": []}, {"text": "In our work, we use the annotation layer produced by for the McRae norms (henceforth QMR): for each concept-feature pair (C, f ), the annotation provides a natural language quantifier expressing the ratio of instances of C having the feature f , as elicited by three coders.", "labels": [], "entities": [{"text": "McRae norms", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.9231714904308319}]}, {"text": "The quantifiers in use are NO, FEW, SOME, MOST, ALL.", "labels": [], "entities": [{"text": "NO", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9297117590904236}, {"text": "FEW", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9924586415290833}, {"text": "SOME", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9080549478530884}, {"text": "MOST", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.988767683506012}, {"text": "ALL", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9923615455627441}]}, {"text": "Table 1 provides example annotations for concept-feature pairs (reproduced from the original paper).", "labels": [], "entities": []}, {"text": "An additional label, KIND, was introduced for usages of the concept as a kind, where quantification does not apply (e.g. beaver symbol-of-Canada).", "labels": [], "entities": [{"text": "KIND", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.7572965025901794}]}, {"text": "A subset of the annotation layer is available for training computational models, corresponding to all instances with a majority label (i.e. those where two or three coders agreed on a label).", "labels": [], "entities": []}, {"text": "The reported average weighted Cohen kappa on this data is \u03ba = 0.59.", "labels": [], "entities": []}, {"text": "In the following, we use a derived gold standard including all 5 quantified classes in QMR (removing the KIND items), with the annotation set to majority opinion (6156 instances).", "labels": [], "entities": [{"text": "QMR", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.7701311111450195}, {"text": "KIND", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.7833105325698853}]}, {"text": "The natural language quantifiers are converted to a numerical format (see \u00a74 for details).", "labels": [], "entities": []}, {"text": "Using the numerical data, we can calculate the mean Spearman rank correlation between the three annotators, which comes to 0.63.", "labels": [], "entities": [{"text": "Spearman rank correlation", "start_pos": 52, "end_pos": 77, "type": "METRIC", "confidence": 0.6261421243349711}]}, {"text": "To map from one semantic representation to another, we learn a function f : DS \u2192 MT that transforms a distributional semantic vector fora concept to its model-theoretic equivalent.", "labels": [], "entities": []}, {"text": "Following previous research showing that similarities amongst word representations can be maintained within linear transformations (, we learn the mapping as a linear relationship between the distributional representation of a word and its model-theoretic representation.", "labels": [], "entities": []}, {"text": "We estimate the coefficients of the function using (multivariate) partial least squares regression (PLSR) as implemented in the R pls package.", "labels": [], "entities": []}, {"text": "We learn a function from the distributional space to each of the model-theoretic spaces (c.f. \u00a74).", "labels": [], "entities": []}, {"text": "The distribution of training and test items is outlined in Table 2, expressed as a number of concept vectors.", "labels": [], "entities": []}, {"text": "We also include the number of quantified instances in the test set (i.e. the number of actual concept-feature pairs that were explicitly annotated in QM R/AD and that No transformations or dimensionality reductions were performed on the MT spaces.", "labels": [], "entities": []}, {"text": "we can thus evaluate -this is a portion of each concept vector in the spaces including QM R data).", "labels": [], "entities": [{"text": "QM R data", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.7351921399434408}]}], "tableCaptions": [{"text": " Table 2: Distribution of training/test items for each  model-theoretic semantic space. We also provide the  number of dimensions for each space, and the actual  number of concept-feature instances tested on.", "labels": [], "entities": []}, {"text": " Table 3: (Spearman) correlations of mapped dimensions with gold annotations for all test items. The table reports  results (\u03c1) when mapped from a distributional space (DS cooc or DS M ikolov ) to each MT space, as well as the  correlation with human annotations when available. The train/test data for the mappings is specified in", "labels": [], "entities": []}, {"text": " Table 2.  For further analysis we report the results when tested only on animal test items (animals), or on all test items but  animals (no-animals). MT animals contains test items from both AD and the animal section of the McRae norms.", "labels": [], "entities": [{"text": "McRae norms", "start_pos": 225, "end_pos": 236, "type": "DATASET", "confidence": 0.9786621928215027}]}, {"text": " Table 4. We find that the gold vector  is among the top 5 nearest neighbours to the predicted  equivalent in nearly 20% of concepts, with the percent- age of gold items in the top neighbours improving as  we increase the size of the neighbourhood. We per- form a more in-depth analysis of the neighbourhoods  for each concept to gain a better understanding of their  behaviour and quality.  We discover that, in many cases, the mapped vector  is close to a similar concept in the gold standard, but not  to itself. So for instance,", "labels": [], "entities": []}, {"text": " Table 7: Distance matrix for the evaluation of the natu- ral language quantifiers generation step.", "labels": [], "entities": [{"text": "natu- ral language quantifiers generation", "start_pos": 52, "end_pos": 93, "type": "TASK", "confidence": 0.5631643931070963}]}, {"text": " Table 8: Confusion matrix for the results of the natural  language quantifiers generation.", "labels": [], "entities": [{"text": "natural  language quantifiers generation", "start_pos": 50, "end_pos": 90, "type": "TASK", "confidence": 0.6752645522356033}]}]}