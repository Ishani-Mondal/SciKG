{"title": [{"text": "Experiments with Generative Models for Dependency Tree Linearization", "labels": [], "entities": []}], "abstractContent": [{"text": "We present experiments with generative models for linearization of unordered labeled syntactic dependency trees (Belz et al., 2011; Rajkumar and White, 2014).", "labels": [], "entities": []}, {"text": "Our linearization models are derived from generative models for dependency structure (Eisner, 1996).", "labels": [], "entities": []}, {"text": "We present a series of generative dependency models designed to capture successively more information about ordering constraints among sister dependents.", "labels": [], "entities": []}, {"text": "We give a dynamic programming algorithm for computing the conditional probability of word orders given tree structures under these models.", "labels": [], "entities": []}, {"text": "The models are tested on corpora of 11 languages using test-set likelihood, and human ratings for generated forms are collected for En-glish.", "labels": [], "entities": []}, {"text": "Our models benefit from representing local order constraints among sisters and from backing off to less sparse distributions , including distributions not conditioned on the head.", "labels": [], "entities": []}], "introductionContent": [{"text": "We explore generative models for producing linearizations of unordered labeled syntactic dependency trees.", "labels": [], "entities": []}, {"text": "This specific task has attracted attention in recent years ( because it forms a useful part of a natural language generation pipeline, especially in machine translation and summarization).", "labels": [], "entities": [{"text": "natural language generation pipeline", "start_pos": 97, "end_pos": 133, "type": "TASK", "confidence": 0.7346284687519073}, {"text": "machine translation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.8117531836032867}, {"text": "summarization", "start_pos": 173, "end_pos": 186, "type": "TASK", "confidence": 0.7853425145149231}]}, {"text": "Closely related tasks are generation of sentences given CCG parses (), bags of words (, and semantic graphs ().", "labels": [], "entities": [{"text": "generation of sentences given CCG parses", "start_pos": 26, "end_pos": 66, "type": "TASK", "confidence": 0.6638477047284445}]}, {"text": "Here we focus narrowly on testing probabilistic generative models for dependency tree linearization.", "labels": [], "entities": [{"text": "dependency tree linearization", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.7285410761833191}]}, {"text": "In contrast, the approach inmost previous work is to apply a variety of scoring functions to trees and linearizations and search for an optimally-scoring tree among some set.", "labels": [], "entities": []}, {"text": "The probabilistic linearization models we investigate are derived from generative models for dependency trees, as most commonly used in unsupervised grammar induction.", "labels": [], "entities": [{"text": "unsupervised grammar induction", "start_pos": 136, "end_pos": 166, "type": "TASK", "confidence": 0.7096042633056641}]}, {"text": "Generative dependency models have typically been evaluated in a parsing task.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 64, "end_pos": 76, "type": "TASK", "confidence": 0.9215135276317596}]}, {"text": "Here, we are interested in the inverse task: inferring a distribution over linear orders given unordered dependency trees.", "labels": [], "entities": []}, {"text": "This is the first work to consider generative dependency models from the perspective of word ordering.", "labels": [], "entities": [{"text": "generative dependency", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.9185087084770203}, {"text": "word ordering", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.7283355593681335}]}, {"text": "The results can potentially shed light on how ordering constraints are best represented in such models.", "labels": [], "entities": []}, {"text": "In addition, the use of probabilistic models means that we can easily define well-motivated normalized probability distributions over orders of dependency trees.", "labels": [], "entities": []}, {"text": "These distributions are useful for answering scientific questions about crosslinguistic word order in quantitative linguistics, where obtaining robust estimates has proven challenging due to data sparsity (.", "labels": [], "entities": [{"text": "crosslinguistic word order", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.567000687122345}]}, {"text": "The remainder of the work is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we present a set of generative linearization models.", "labels": [], "entities": []}, {"text": "In Section 3 we compare the performance of the different models as measured by test-set probability and human acceptability ratings.", "labels": [], "entities": []}, {"text": "We also compare our performance with other systems from the literature.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we empirically evaluate some options for model type and model labelling as described above.", "labels": [], "entities": []}, {"text": "We are interested in how many of the possible orders of a sentence our model can generate (recall), and in how many of our generated orders really are acceptable (precision).", "labels": [], "entities": [{"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9973070621490479}, {"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9993029832839966}]}, {"text": "As a recall-like measure, we quantify the probability of the word orders of held-out test sentences.", "labels": [], "entities": [{"text": "recall-like", "start_pos": 5, "end_pos": 16, "type": "METRIC", "confidence": 0.9925990104675293}]}, {"text": "Low probabil-  ities assigned to held-out sentences indicate that there are possible orders which our model is missing.", "labels": [], "entities": []}, {"text": "As a precision-like measure, we get human acceptability ratings for sentence reorderings generated by our model.", "labels": [], "entities": [{"text": "precision-like", "start_pos": 5, "end_pos": 19, "type": "METRIC", "confidence": 0.9967136383056641}]}, {"text": "We carryout our evaluations using the dependency corpora of the Universal Dependencies project (v1.1), with the train/dev/test splits provided in that dataset.", "labels": [], "entities": []}, {"text": "We remove nodes and edges dealing with punctuation.", "labels": [], "entities": []}, {"text": "Due to space constraints, we only present results from 11 languages here.", "labels": [], "entities": []}, {"text": "We collected human ratings for sentence reorderings sampled from the English models from 54 native American English speakers on Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 128, "end_pos": 150, "type": "DATASET", "confidence": 0.958748439947764}]}, {"text": "We randomly selected a set of 90 sentences from the test set of the English Universal Dependencies corpus.", "labels": [], "entities": [{"text": "English Universal Dependencies corpus", "start_pos": 68, "end_pos": 105, "type": "DATASET", "confidence": 0.952498733997345}]}, {"text": "We generated a reordering of each sentence according to each of 12 model configurations in.", "labels": [], "entities": []}, {"text": "Each participant saw an original sentence and a reordering of it, and was asked to rate how natural each version of the sentence sounded, on a scale of 1 to 5.", "labels": [], "entities": []}, {"text": "The order of presentation of the original and reordered forms was randomized, so that participants were not aware of which form was the original and which was a reordering.", "labels": [], "entities": []}, {"text": "Each participant rated 56 sentence pairs.", "labels": [], "entities": []}, {"text": "Participants were also asked whether the two sentences in a pair meant the same thing, with \"can't tell\" as a possible answer.", "labels": [], "entities": []}, {"text": "shows average human acceptability ratings for reorderings, and the proportion of sentence pairs judged to mean the same thing.", "labels": [], "entities": []}, {"text": "The original sentences have an average acceptability rating of 4.48/5.", "labels": [], "entities": []}, {"text": "The very best performing models are those which do not back off to a distribution not conditioned on the head.", "labels": [], "entities": []}, {"text": "However, in the case of the Observed Orders and other sparse models, we see consistent improvement from this backoff.", "labels": [], "entities": []}, {"text": "shows the acceptability ratings (out of 5) plotted against test set probability.", "labels": [], "entities": []}, {"text": "We see that: Mean acceptability rating out of 5, and proportion of reordered sentences with the same meaning as the original, for English models.", "labels": [], "entities": [{"text": "Mean acceptability rating", "start_pos": 13, "end_pos": 38, "type": "METRIC", "confidence": 0.8651125232378641}]}, {"text": "Figure 2: Comparison of test set probability (Table 1) and acceptability ratings for English across models.", "labels": [], "entities": []}, {"text": "A least-squares linear regression line is shown.", "labels": [], "entities": []}, {"text": "Labels as in the models which yield poor test set probability also have poor acceptability ratings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Mean acceptability rating out of 5, and  proportion of reordered sentences with the same  meaning as the original, for English models. La- bels as in", "labels": [], "entities": [{"text": "Mean acceptability rating", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8704021573066711}]}]}