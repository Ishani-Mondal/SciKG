{"title": [{"text": "Language Understanding for Text-based Games using Deep Reinforcement Learning", "labels": [], "entities": [{"text": "Language Understanding", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6810442060232162}]}], "abstractContent": [{"text": "In this paper, we consider the task of learning control policies for text-based games.", "labels": [], "entities": []}, {"text": "In these games, all interactions in the virtual world are through text and the underlying state is not observed.", "labels": [], "entities": []}, {"text": "The resulting language barrier makes such environments challenging for automatic game players.", "labels": [], "entities": []}, {"text": "We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback.", "labels": [], "entities": []}, {"text": "This framework enables us to map text descriptions into vector representations that capture the semantics of the game states.", "labels": [], "entities": []}, {"text": "We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations.", "labels": [], "entities": []}, {"text": "Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we address the task of learning control policies for text-based strategy games.", "labels": [], "entities": []}, {"text": "These games, predecessors to modern graphical ones, still enjoy a large following worldwide.", "labels": [], "entities": []}, {"text": "They often involve complex worlds with rich interactions and elaborate textual descriptions of the underlying states (see).", "labels": [], "entities": []}, {"text": "Players read descriptions of the current world state and respond with natural language commands to take actions.", "labels": [], "entities": []}, {"text": "Since the underlying state is not directly observable, the player has to understand the text in order to act, making it State 1: The old bridge You are standing very close to the bridge's eastern foundation.", "labels": [], "entities": []}, {"text": "If you go east you will be back on solid ground ...", "labels": [], "entities": []}, {"text": "The bridge sways in the wind.", "labels": [], "entities": []}], "datasetContent": [{"text": "Game Environment For our game environment, we modify Evennia, an open-source library for building online textual MUD games.", "labels": [], "entities": []}, {"text": "Evennia is a Python-based framework that allows one to easily create new games by writing a batch file describing the environment with details of rooms, provides statistics of the game worlds.", "labels": [], "entities": [{"text": "Evennia", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.885912299156189}]}, {"text": "We observe that the Fantasy world is moderately sized with a vocabulary of 1340 words and up to 100 different descriptions fora room.", "labels": [], "entities": []}, {"text": "These descriptions were created manually by the game developers.", "labels": [], "entities": []}, {"text": "These diverse, engaging descriptions are designed to make it interesting and exciting for human players.", "labels": [], "entities": []}, {"text": "Several rooms have many alternative descriptions, invoked randomly on each visit by the player.", "labels": [], "entities": []}, {"text": "Comparatively, the Home world is smaller: it has a very restricted vocabulary of 84 words and the room descriptions are relatively structured.", "labels": [], "entities": []}, {"text": "However, both the room descriptions (which are also varied and randomly provided to the agent) and the quest descriptions were adversarially created with negation and conjunction of facts to force an agent to actually understand the state in order to play well.", "labels": [], "entities": []}, {"text": "Therefore, this domain provides an interesting challenge for language understanding.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.8354353308677673}]}, {"text": "In both worlds, the agent receives a positive reward on completing a quest, and negative rewards forgetting into bad situations like falling off abridge, or losing a battle.", "labels": [], "entities": []}, {"text": "We also add small deterministic negative rewards for each nonterminating step.", "labels": [], "entities": []}, {"text": "This incentivizes the agent to learn policies that solve quests in fewer steps.", "labels": [], "entities": []}, {"text": "The supplementary material has details on the reward structure.", "labels": [], "entities": []}, {"text": "Home World We created Home world to mimic the environment of atypical house.", "labels": [], "entities": []}, {"text": "The world consists of four rooms -a living room, a bedroom, a kitchen and a garden with connecting pathways.", "labels": [], "entities": []}, {"text": "Every room is reachable from every other room.", "labels": [], "entities": []}, {"text": "Each room contains a representative object that the agent can interact with.", "labels": [], "entities": []}, {"text": "For instance, the kitchen has an apple that the player can eat.", "labels": [], "entities": []}, {"text": "Transitions between the rooms are deterministic.", "labels": [], "entities": []}, {"text": "At the start of each game episode, the player is placed in a random room and provided with a randomly selected quest.", "labels": [], "entities": []}, {"text": "The text provided to the player contains both the description of her current state and that of the quest.", "labels": [], "entities": []}, {"text": "Thus, the player can begin in one of 16 different states (4 rooms \u00d7 4 quests), which adds to the world's complexity.", "labels": [], "entities": []}, {"text": "An example of a quest given to the player in text is Not you are sleepy now but you are hungry now.", "labels": [], "entities": []}, {"text": "To complete this quest and obtain a reward, the player has to navigate through the house to reach the kitchen and eat the apple (i.e type in the command eat apple).", "labels": [], "entities": []}, {"text": "More importantly, the player should interpret that the quest does not require her to take a nap in the bedroom.", "labels": [], "entities": []}, {"text": "We created such misguiding quests to make it hard for agents to succeed without having an adequate level of language understanding.", "labels": [], "entities": []}, {"text": "An illustration is provided in the supplementary material.", "labels": [], "entities": []}, {"text": "Fantasy World The Fantasy world is considerably more complex and involves quests such as navigating through a broken bridge or finding the secret tomb of an ancient hero.", "labels": [], "entities": [{"text": "Fantasy World", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9333544075489044}]}, {"text": "This game also has stochastic transitions in addition to varying state descriptions provided to the player.", "labels": [], "entities": []}, {"text": "For instance, there is a possibility of the player falling from the bridge if she lingers too long on it.", "labels": [], "entities": []}, {"text": "Due to the large command space in this game, 7 we make use of cues provided by the game itself to narrow down the set of possible objects to consider in each state.", "labels": [], "entities": []}, {"text": "For instance, in the MUD example in, the game provides a list of possible exits.", "labels": [], "entities": []}, {"text": "If the game does not provide such clues for the current state, we consider all objects in the game.", "labels": [], "entities": []}, {"text": "Evaluation We use two metrics for measuring an agent's performance: (1) the cumulative reward obtained per episode averaged over the episodes and (2) the fraction of quests completed by the agent.", "labels": [], "entities": []}, {"text": "The evaluation procedure is as follows.", "labels": [], "entities": []}, {"text": "In each epoch, we first train the agent on M episodes of T steps each.", "labels": [], "entities": []}, {"text": "At the end of this training, we have a testing phase of running M episodes of the game for T steps.", "labels": [], "entities": []}, {"text": "We use M = 50, T = 20 for the Home world and M = 20, T = 250 for the Fantasy world.", "labels": [], "entities": []}, {"text": "For all evaluation episodes, we run the agent following an -greedy policy with = 0.05, which makes the agent choose the best action according to its Q-values 95% of the time.", "labels": [], "entities": []}, {"text": "We report the agent's performance at each epoch.", "labels": [], "entities": []}, {"text": "Baselines We compare our LSTM-DQN model with three baselines.", "labels": [], "entities": []}, {"text": "The first is a Random agent that chooses both actions and objects uniformly at random from all available choices.", "labels": [], "entities": []}, {"text": "The other two are BOW-DQN and BI-DQN, which use a bagof-words and a bag-of-bigrams representation of the text, respectively, as input to the DQN action scorer.", "labels": [], "entities": [{"text": "BOW-DQN", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9133673906326294}, {"text": "BI-DQN", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9434404969215393}]}, {"text": "These baselines serve to illustrate the importance of having a good representation layer for the task.", "labels": [], "entities": []}, {"text": "Settings For our DQN models, we used D = 100000, \u03b3 = 0.5.", "labels": [], "entities": [{"text": "D", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.9950847029685974}]}, {"text": "We use a learning rate of 0.0005 for RMSprop.", "labels": [], "entities": []}, {"text": "We anneal the for -greedy from 1 to 0.2 over 100000 transitions.", "labels": [], "entities": []}, {"text": "A mini-batch gradient update is performed every 4 steps of the gameplay.", "labels": [], "entities": []}, {"text": "We roll out the LSTM (over words) fora maximum of 30 stepson the Home world and for 100 stepson the Fantasy world.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9158071875572205}, {"text": "Fantasy world", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.9294047057628632}]}, {"text": "For the prioritized sampling, we used \u03c1 = 0.25 for both worlds.", "labels": [], "entities": []}, {"text": "We employed a mini-batch size of 64 and word embedding size d = 20 in all experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Various statistics of the two game worlds", "labels": [], "entities": []}]}