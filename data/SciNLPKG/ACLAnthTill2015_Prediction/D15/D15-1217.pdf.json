{"title": [{"text": "Hierarchical Latent Words Language Models for Robust Modeling to Out-Of Domain Tasks", "labels": [], "entities": [{"text": "Robust Modeling", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7781641781330109}]}], "abstractContent": [{"text": "This paper focuses on language modeling with adequate robustness to support different domain tasks.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7509668469429016}]}, {"text": "To this end, we propose a hierarchical latent word language model (h-LWLM).", "labels": [], "entities": []}, {"text": "The proposed model can be regarded as a generalized form of the standard LWLMs.", "labels": [], "entities": []}, {"text": "The key advance is introducing a multiple latent variable space with hierarchical structure.", "labels": [], "entities": []}, {"text": "The structure can flexibly take account of linguistic phenomena not present in the training data.", "labels": [], "entities": []}, {"text": "This paper details the definition as well as a training method based on layer-wise inference and a practical usage in natural language processing tasks with an approximation technique.", "labels": [], "entities": []}, {"text": "Experiments on speech recognition show the effectiveness of h-LWLM in out-of domain tasks.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8288793563842773}]}], "introductionContent": [{"text": "Language models (LMs) are essential for automatic speech recognition or statistical machine translation).", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6348215540250143}, {"text": "statistical machine translation", "start_pos": 72, "end_pos": 103, "type": "TASK", "confidence": 0.6175423363844553}]}, {"text": "The performance of LMs strongly depends on quality and quantity of their training data.", "labels": [], "entities": []}, {"text": "Superior performance is usually obtained by using enormous domain-matched training data sets to construct.", "labels": [], "entities": []}, {"text": "Unfortunately, in many cases, large amounts of domain-matched training data sets are not available.", "labels": [], "entities": []}, {"text": "Therefore, LM technology that can robustly work for domains that differ from that of the training data is needed).", "labels": [], "entities": []}, {"text": "For robust language modeling, several technologies have been proposed.", "labels": [], "entities": [{"text": "robust language modeling", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.619896282752355}]}, {"text": "Fundamental techniques are smoothing) and clustering).", "labels": [], "entities": []}, {"text": "Other solutions are Bayesian modeling) and ensemble modeling ().", "labels": [], "entities": [{"text": "ensemble modeling", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.8036733567714691}]}, {"text": "Moreover, continuous representation of words in neural network LMs can also support robust modeling ().", "labels": [], "entities": []}, {"text": "However, previous works are focused on maximizing performance in the same domain as that of the training data.", "labels": [], "entities": []}, {"text": "In other words, it is uncertain that these technologies robustly support out-of domain tasks.", "labels": [], "entities": []}, {"text": "In contrast, latent words LMs (LWLMs) are clearly effective for outof domain tasks.", "labels": [], "entities": []}, {"text": "We employed the LWLM to speech recognition and the resulting performance was significantly superior in out-of domain tasks while the performance was comparable in domainmatched task to conventional).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9012481868267059}]}, {"text": "LWLMs are generative models that employ a latent word space.", "labels": [], "entities": []}, {"text": "The latent space can flexibly take into account relationships between words and the modeling helps to efficiently increase the robustness to out-of domain tasks.", "labels": [], "entities": []}, {"text": "In this paper, we focus on LWLMs and aim to make them more flexible for greater robustness to out-of domain tasks.", "labels": [], "entities": []}, {"text": "To this end, this paper takes note of a fact that standard LWLM simply represents the latent space as n-gram model of latent words.", "labels": [], "entities": []}, {"text": "However, function and meaning of words are essentially hierarchical and upper layers ought to be useful to increase the robustness to out-of domain tasks.", "labels": [], "entities": []}, {"text": "The conventional LWLMs do not model the hierarchy, while the latent words are used to represent function and meaning of words.", "labels": [], "entities": []}, {"text": "Thus, we tried to model the hierarchy in the latent space by estimating a latent word of a latent word recursively.", "labels": [], "entities": []}, {"text": "This paper proposes a novel LWLM with multiple latent word spaces that are hierarchically structured; we call it the hierarchical LWLM (h-LWLM).", "labels": [], "entities": []}, {"text": "The proposed model can be regarded as a generalized form of the standard LWLMs.", "labels": [], "entities": []}, {"text": "The hierarchical structure can take into account the abstraction process of function and meaning of words.", "labels": [], "entities": []}, {"text": "Therefore, it can be expected that h-LWLMs flexibly calculate generative probability for unseen words unlike non-hierarchical LWLMs.", "labels": [], "entities": []}, {"text": "To create the hierarchical latent word structure from training data sets, we also propose a layerwise inference.", "labels": [], "entities": []}, {"text": "The inference is inspired by a deep Boltzmann machine) that stacks up restricted Boltzmann machines ().", "labels": [], "entities": []}, {"text": "In addition, we detail an n-gram approximation technique to apply the proposed model to practical natural language processing tasks (see.", "labels": [], "entities": []}, {"text": "In experiments, we construct LMs from spontaneous lecture task data and apply them to a contact center dialogue task and a voicemail task as outof domain tasks.", "labels": [], "entities": []}, {"text": "The effectiveness of the proposed method is shown by perplexity and speech recognition evaluation (Sec. 4).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7329230606555939}]}], "datasetContent": [{"text": "Our basic assumption is domain-matched training data is not available.", "labels": [], "entities": []}, {"text": "Thus, for LM training, we used the Corpus of Spontaneous Japanese (CSJ) whose domain is a spontaneous lecture task ().", "labels": [], "entities": [{"text": "LM training", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9424436986446381}]}, {"text": "We divided CSJ into a training set and a small validation set (Valid).", "labels": [], "entities": []}, {"text": "The validation set was used for optimizing several hyper parameters of LMs.", "labels": [], "entities": []}, {"text": "For evaluation, a contact center dialogue task (Test 1) and a voicemail task (Test 2) were prepared.", "labels": [], "entities": []}, {"text": "In contact center dialogue task, two speakers, an operator and a customer, talked to each other as in call center dialogues.", "labels": [], "entities": []}, {"text": "24 phone calls (24 operator channels and 24 customer channels) were used in the evaluation.", "labels": [], "entities": []}, {"text": "In the voicemail task, a person spoke small voice messages using a smartphone.", "labels": [], "entities": []}, {"text": "237 messages are used in the evaluation.", "labels": [], "entities": []}, {"text": "The training data had about 7M words, the vocabulary size was about 80K.", "labels": [], "entities": [{"text": "vocabulary size", "start_pos": 42, "end_pos": 57, "type": "METRIC", "confidence": 0.9009894132614136}]}, {"text": "The validation data size and test data size (both tasks) were about 20K words.", "labels": [], "entities": []}, {"text": "For speech recognition evaluation, we prepared an acoustic model based on hidden Markov models with deep neural networks (DNN-HMM)).", "labels": [], "entities": [{"text": "speech recognition evaluation", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.9085341890652975}]}, {"text": "The DNN-HMM had 8 hidden layers with 2048 nodes.", "labels": [], "entities": [{"text": "DNN-HMM", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9489362835884094}]}, {"text": "The speech recognizer was a weighted finite state transducer (WFST) decoder (.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.6799622476100922}]}, {"text": "As a baseline, 3-gram LM with interpolated Kneser-Ney smoothing (MKN)) and 3-gram hierarchical Pitman-Yor LM (HPY) were constructed from the training data.", "labels": [], "entities": []}, {"text": "We also trained a classbased recurrent neural network LM with 500 hidden nodes and 500 classes (RNN) for comparison to state-of-the art language modeling).", "labels": [], "entities": []}, {"text": "In addition, we constructed 3-gram standard LWLM and 3-gram h-LWLMs (LW).", "labels": [], "entities": []}, {"text": "LW with 1 layer represents standard LWLM, and LW with 2-5 layers represent proposed h-LWLMs.", "labels": [], "entities": []}, {"text": "The number of instances was set to 10 for each LW.", "labels": [], "entities": []}, {"text": "For their n-gram approximation, we generated one billion words and approximated each as a 3-gram HPYLM.", "labels": [], "entities": [{"text": "HPYLM", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.8967591524124146}]}, {"text": "Moreover, we constructed interpolated model with LW and HPY (LW+HPY).", "labels": [], "entities": []}, {"text": "shows the relation between number of layers in h-LWLM and perplexity (PPL) reduction for each condition.", "labels": [], "entities": [{"text": "perplexity (PPL) reduction", "start_pos": 58, "end_pos": 84, "type": "METRIC", "confidence": 0.9005989193916321}]}, {"text": "In addition, shows speech recognition results in terms of word error rate (WER) for each condition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.744896799325943}, {"text": "word error rate (WER)", "start_pos": 58, "end_pos": 79, "type": "METRIC", "confidence": 0.9130296011765798}]}, {"text": "RNN was only tested in PPL evaluation as RNN cannot be converted into WFST format.", "labels": [], "entities": [{"text": "RNN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8297833800315857}, {"text": "WFST format", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.9024776518344879}]}], "tableCaptions": [{"text": " Table 1: Word error rate (WER) results (%).", "labels": [], "entities": [{"text": "Word error rate (WER)", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8826402028401693}]}]}