{"title": [{"text": "A Discriminative Training Procedure for Continuous Translation Models \u2020 *", "labels": [], "entities": [{"text": "Continuous Translation", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6241080611944199}]}], "abstractContent": [{"text": "Continuous-space translation models have recently emerged as extremely powerful ways to boost the performance of existing translation systems.", "labels": [], "entities": []}, {"text": "A simple, yet effective way to integrate such models in inference is to use them in an N-best rescor-ing step.", "labels": [], "entities": []}, {"text": "In this paper, we focus on this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function.", "labels": [], "entities": []}, {"text": "Our approach is validated on two domains, where it outperforms strong baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated.", "labels": [], "entities": []}, {"text": "Boosted by early successes in language modelling for speech recognition), NNs have since been successufully applied to many other tasks).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7310500591993332}]}, {"text": "In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (, and more recently to implement end-to-end translation systems ().", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 53, "end_pos": 90, "type": "TASK", "confidence": 0.8691210250059763}, {"text": "estimate continuous-space translation models (CTMs)", "start_pos": 101, "end_pos": 152, "type": "TASK", "confidence": 0.7679915726184845}]}, {"text": "In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9917551279067993}]}, {"text": "Since this objective function requires to normalize scores, several alternative training objectives have recently been proposed to speedup training and inference, a popular and effective choice being the Noise Contrastive Estimation (NCE) introduced in (.", "labels": [], "entities": []}, {"text": "In any case, NN training is typically performed (a) in isolation from the other components of the SMT system and (b) using a criterion that is unrelated to the actual performance of the SMT system (as measured for instance by BLEU).", "labels": [], "entities": [{"text": "SMT", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9833320379257202}, {"text": "SMT", "start_pos": 186, "end_pos": 189, "type": "TASK", "confidence": 0.9703171253204346}, {"text": "BLEU", "start_pos": 226, "end_pos": 230, "type": "METRIC", "confidence": 0.9966277480125427}]}, {"text": "It is therefore likely that the resulting NN parameters are sub-optimal with respect to their intended use.", "labels": [], "entities": []}, {"text": "In this paper, we study an alternative training regime aimed at addressing problems (a) and (b).", "labels": [], "entities": []}, {"text": "To this end, we propose anew objective function used to discriminatively train or adapt CTMs, along with a training procedure that enables to take the other components of the system into account.", "labels": [], "entities": []}, {"text": "Our starting point is a non-normalized extension of the n-gram CTM of () that we briefly restate in section 2.", "labels": [], "entities": []}, {"text": "We then introduce our objective function and the associated optimization procedure in section 3.", "labels": [], "entities": []}, {"text": "As will be discussed, our new training criterion is inspired both from maxmargin methods () and from pair-wise ranking (PRO) (.", "labels": [], "entities": [{"text": "pair-wise ranking (PRO)", "start_pos": 101, "end_pos": 124, "type": "METRIC", "confidence": 0.5839206337928772}]}, {"text": "This proposal is evaluated in an N -best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly.", "labels": [], "entities": []}, {"text": "Note, however that it could be used with any phrase-based system.", "labels": [], "entities": []}, {"text": "Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.91605144739151}]}], "datasetContent": [{"text": "Results in measure the impact of discriminative training on top of an NCE-trained model for the two TED Talks conditions.", "labels": [], "entities": [{"text": "NCE-trained", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.8642046451568604}, {"text": "TED Talks", "start_pos": 100, "end_pos": 109, "type": "TASK", "confidence": 0.6165254414081573}]}, {"text": "In the adaptation task, the discriminative training of the CTM gives a large improvement of 0.9 BLEU score over the CTM only trained with NCE and 1.9 over the baseline system.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.9848786294460297}]}, {"text": "However, for the training scenario, these gains are reduced respectively to 0.4 and 1.2 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9991078972816467}]}, {"text": "The BLEU scores (in the train column) measured on the N -best lists used to train the CTM provide an explanation for this difference: in training, the N -best lists contain hypotheses with an overoptimistic BLEU score, to be compared with the ones observed on unseen data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9991452693939209}, {"text": "BLEU", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.9967376589775085}]}, {"text": "As a result, adding the CTM significantly  worsens the performance on the discriminative training data, contrarily to what is observed on the development and test sets.", "labels": [], "entities": []}, {"text": "Even if the results of these two conditions cannot be directly compared (the baselines are different), it seems that the proposed discriminative training has a greater impact on performance in the adaptation scenario, even though the out-of-domain system initially yields lower BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 278, "end_pos": 282, "type": "METRIC", "confidence": 0.9992682337760925}]}, {"text": "The medical translation task represents a different situation, in which a large-scale system is built from multiples but domain-related corpora, among which, one is used to train the CTM.", "labels": [], "entities": [{"text": "medical translation task", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8004396160443624}]}, {"text": "Nevertheless, results reported in exhibit a similar trend.", "labels": [], "entities": []}, {"text": "For both conditions, the discriminative training gives a significant improvement, up to 0.7 BLEU score over the one only trained with NCE and up to 1.7 over the baseline system.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9751404821872711}, {"text": "NCE", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.8741113543510437}]}, {"text": "Arguably, the difference between the two conditions is much smaller than what was observed with the TED Talks task, due to the fact that the PatentAbstract corpus used to discriminatively train the CTM only corresponds to a small subset of the parallel data.", "labels": [], "entities": [{"text": "TED Talks task", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.5794575214385986}, {"text": "PatentAbstract corpus", "start_pos": 141, "end_pos": 162, "type": "DATASET", "confidence": 0.9374301731586456}]}, {"text": "However, the best strategy seems, here again, to exclude the data used for the CTM from the data used to train the baseline system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores for the TED Talkstasks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994195699691772}, {"text": "TED Talkstasks", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.7462608814239502}]}, {"text": " Table 2: BLEU scores for the medical tasks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993434548377991}]}]}