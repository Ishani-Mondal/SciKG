{"title": [{"text": "Graph-Based Collective Lexical Selection for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.8641816178957621}]}], "abstractContent": [{"text": "Lexical selection is of great importance to statistical machine translation.", "labels": [], "entities": [{"text": "Lexical selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.915648490190506}, {"text": "statistical machine translation", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.744161993265152}]}, {"text": "In this paper, we propose a graph-based framework for collective lexical selection.", "labels": [], "entities": [{"text": "collective lexical selection", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.6889146963755289}]}, {"text": "The framework is established on a translation graph that captures not only local associations between source-side content words and their target translations but also target-side global dependencies in terms of relat-edness among target items.", "labels": [], "entities": []}, {"text": "We also introduce a random walk style algorithm to collectively identify translations of source-side content words that are strongly related in translation graph.", "labels": [], "entities": []}, {"text": "We validate the effectiveness of our lexical selection framework on Chinese-English translation.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6711292415857315}]}, {"text": "Experiment results with large-scale training data show that our approach significantly improves lexical selection.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical selection, which selects appropriate translations for lexical items on the source side, is a crucial task in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Lexical selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8890331983566284}, {"text": "statistical machine translation (SMT)", "start_pos": 117, "end_pos": 154, "type": "TASK", "confidence": 0.7961589246988297}]}, {"text": "The task is closely related to two factors: 1) associations of selected translations with lexical items on the source side, including corresponding source items and their neighboring words, and 2) dependencies 1 between selected target translations and other items on the target side.", "labels": [], "entities": []}, {"text": "As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed in-creasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection.", "labels": [], "entities": []}, {"text": "Efforts using source-side associations mainly focus on the exploitation of either sentence-level context or the utilization of document-level context.", "labels": [], "entities": []}, {"text": "In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9978368878364563}]}, {"text": "The most common practice is to use language models to estimate the strength of target-side dependencies ().", "labels": [], "entities": []}, {"text": "However, conventional ngram language models are not good at capturing long-distance dependencies.", "labels": [], "entities": []}, {"text": "Consider the example shown in.", "labels": [], "entities": []}, {"text": "As the translations of polysemous words \"w` ent\u00ed\", \"ch\u00edy\u02c7ch\u00edy\u02c7ou\" and \"l` \u0131ch\u02c7ang\u0131ch\u02c7ang\" are far from each other, our baseline can only correctly translate \"l` \u0131ch\u02c7ang\u0131ch\u02c7ang\" as \"stance\".", "labels": [], "entities": []}, {"text": "It inappropriately translates the other two words as \"problem\" and null, respectively, even with the support of an n-gram language model.", "labels": [], "entities": []}, {"text": "If we could model long-distance dependencies among target translations of source words \"w` ent\u00ed\"(issue), \"ch\u00edy\u02c7ch\u00edy\u02c7ou\"(hold) and \"l` \u0131ch\u02c7ang\u0131ch\u02c7ang\"(stance), these translation errors could be avoided.", "labels": [], "entities": []}, {"text": "In order to model target-side global dependencies, we propose a novel graph-based collective lexical selection framework for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9930556416511536}]}, {"text": "Specifically, \u2022 First, we propose a translation graph to model not only local associations between sourceside content words and their target translations but also global relatedness among target-side items.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The initial and final evidence scores of some source words and their target translations in", "labels": [], "entities": []}, {"text": " Table 2: Experiment results on the test sets with \u03bb=0.15. Avg  = average BLEU scores, GM(LM) and GM(PMI) denote our  model using the measure based on language model and PMI,  respectively.", "labels": [], "entities": [{"text": "Avg", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9975246787071228}, {"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9974439144134521}, {"text": "GM(LM)", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9278087317943573}, {"text": "GM(PMI)", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9370388388633728}]}]}