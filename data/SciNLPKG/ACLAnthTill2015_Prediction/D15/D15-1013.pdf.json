{"title": [{"text": "Re-evaluating Automatic Summarization with BLEU and 192 Shades of ROUGE", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9985815286636353}, {"text": "ROUGE", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.9644816517829895}]}], "abstractContent": [{"text": "We provide an analysis of current evaluation methodologies applied to summariza-tion metrics and identify the following areas of concern: (1) movement away from evaluation by correlation with human assessment ; (2) omission of important components of human assessment from evaluations , in addition to large numbers of metric variants; (3) absence of methods of significance testing improvements over a baseline.", "labels": [], "entities": []}, {"text": "We outline an evaluation methodology that overcomes all such challenges , providing the first method of significance testing suitable for evaluation of summarization metrics.", "labels": [], "entities": [{"text": "summarization", "start_pos": 152, "end_pos": 165, "type": "TASK", "confidence": 0.9426621198654175}]}, {"text": "Our evaluation reveals for the first time which metric variants significantly outperform others, optimal metric variants distinct from current recommended best variants, as well as machine translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summariza-tion systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 208, "end_pos": 212, "type": "METRIC", "confidence": 0.9860338568687439}, {"text": "ROUGE", "start_pos": 245, "end_pos": 250, "type": "METRIC", "confidence": 0.8923602104187012}]}, {"text": "We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclusions about the relative performance of state-of-the-art summarization systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic metrics of summarization evaluation have their origins in machine translation (MT), with ROUGE (, the first and still most widely used automatic summarization metric, comprising an adaption of the BLEU score ().", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.9774293303489685}, {"text": "machine translation (MT)", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.8318698942661286}, {"text": "ROUGE", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.9942481517791748}, {"text": "BLEU score", "start_pos": 207, "end_pos": 217, "type": "METRIC", "confidence": 0.9713718295097351}]}, {"text": "Automatic evaluation in MT and summarization have much in common, as both involve the automatic comparison of systemgenerated texts with one or more human-generated reference texts, contrasting either system-output translations or peer summaries with human reference translations or model summaries, depending on the task.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9894422292709351}, {"text": "summarization", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9890240430831909}]}, {"text": "In both MT and summarization evaluation, any newly proposed automatic metric must be assessed by the degree to which it provides a good substitute of human assessment, and although there are obvious parallels between evaluation of systems in the two areas, when it comes to evaluation of metrics, summarization has diverged considerably from methodologies applied to evaluation of metrics in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9894010424613953}, {"text": "summarization evaluation", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.9380475580692291}, {"text": "summarization", "start_pos": 297, "end_pos": 310, "type": "TASK", "confidence": 0.9879129528999329}, {"text": "MT", "start_pos": 392, "end_pos": 394, "type": "TASK", "confidence": 0.9687618017196655}]}, {"text": "Since the inception of BLEU, evaluation of automatic metrics in MT has been by correlation with human assessment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9392589330673218}, {"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9893381595611572}]}, {"text": "In contrast in summarization, over the years since the introduction of ROUGE, summarization evaluation has seen a variety of different methodologies applied to evaluation of its metrics.", "labels": [], "entities": [{"text": "summarization", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9834657907485962}, {"text": "summarization evaluation", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.9587243497371674}]}, {"text": "Evaluation of summarization metrics has included, for example, the ability of a metric/significance test combination to distinguish between sets of human and system-generated summaries (), or by accuracy of conclusions drawn from metrics when combined with a particular significance test, Wilcoxon ranksum (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.9623395204544067}, {"text": "accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.9966622591018677}]}, {"text": "Besides moving away from well-established methods such as correlation with human judgment, previous summarization metric evaluations have been additionally limited by inclusion of only a small proportion of possible metrics and variants.", "labels": [], "entities": [{"text": "summarization", "start_pos": 100, "end_pos": 113, "type": "TASK", "confidence": 0.9766877293586731}]}, {"text": "For example, although the most commonly used metric ROUGE has a very large number of possible variants, it is common to include only a small range of those in evaluations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.7854214310646057}]}, {"text": "This has the obvious disadvantage that superior variants may exist but remain unidentified due to their omission.", "labels": [], "entities": []}, {"text": "Despite such limitations, however, subsequent evaluations of state-of-the-art summarization systems operate under the assumption that recommended ROUGE variants are optimal and rely on this assumption to draw conclusions about the relative performance of systems (.", "labels": [], "entities": []}, {"text": "This forces us to raise some important questions.", "labels": [], "entities": []}, {"text": "Firstly, to what degree was the divergence away from evaluation methodologies still applied to MT metrics today well-founded?", "labels": [], "entities": [{"text": "MT metrics", "start_pos": 95, "end_pos": 105, "type": "TASK", "confidence": 0.9071389138698578}]}, {"text": "For example, were the original methodology, by correlation with human assessment, to be applied, would a distinct variant of ROUGE emerge as superior and subsequently lead to distinct system rankings?", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 125, "end_pos": 130, "type": "METRIC", "confidence": 0.8912351131439209}]}, {"text": "Secondly, were all variants of ROUGE to be included in evaluations, would a variant originally omitted from the evaluation emerge as superior and lead to further differences in summarization system rankings?", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9630468487739563}]}, {"text": "Furthermore, although methods of statistical significance testing are commonly applied to evaluation of summarization systems, attempts to identify significant differences in performance of metrics are extremely rare, and when they have been applied unfortunately have not used an appropriate test.", "labels": [], "entities": []}, {"text": "This motivates our review of past and current methodologies applied to the evaluation of summarization metrics.", "labels": [], "entities": [{"text": "summarization metrics", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.8763437867164612}]}, {"text": "Since MT evaluation in general has its own imperfections, we do not attempt to indiscriminately impose all MT evaluation methodologies on summarization, but specifically revisit evaluation methodologies applied to one particular area of summarization, evaluation of metrics.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 6, "end_pos": 19, "type": "TASK", "confidence": 0.9594555199146271}, {"text": "MT evaluation", "start_pos": 107, "end_pos": 120, "type": "TASK", "confidence": 0.8825490176677704}]}, {"text": "Correlations with human assessment reveal an extremely wide range in performance among variants, highlighting the importance of an optimal choice of ROUGE variant in system evaluations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 149, "end_pos": 154, "type": "METRIC", "confidence": 0.8983545899391174}]}, {"text": "Since distinct variants of ROUGE achieve significantly stronger correlation with human assessment than previous recommended best variants, we subsequently replicate a recent evaluation of state-of-the-art summarization systems revealing distinct conclusions about the relative performance of systems.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9490652084350586}]}, {"text": "In addition, we include in the evaluation of metrics, an evaluation of BLEU for the purpose of summarization evaluation, and contrary to common belief, precision-based BLEU is on-par with recall-based ROUGE for evaluation of summarization systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9981821775436401}, {"text": "summarization evaluation", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.9594849646091461}, {"text": "precision-based", "start_pos": 152, "end_pos": 167, "type": "METRIC", "confidence": 0.9971330165863037}, {"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.7242007255554199}, {"text": "recall-based ROUGE", "start_pos": 188, "end_pos": 206, "type": "METRIC", "confidence": 0.8753311932086945}]}], "datasetContent": [{"text": "When large-scale human evaluation of summarization systems takes place, human evaluation commonly takes the form of annotation of whether or not system-generated summary units express the meaning of model summary units, annotations subsequently used to compute human coverage scores.", "labels": [], "entities": []}, {"text": "In addition, an evaluation of the linguistic quality of summaries is commonly carried out.", "labels": [], "entities": []}, {"text": "As described in Section 2, when used for the evaluation of metrics, linguistic quality is commonly omitted, however, with metrics only assessed by the degree to which they correlate with human coverage scores.", "labels": [], "entities": []}, {"text": "In contrast, we include all available human assessment data for evaluating metrics.", "labels": [], "entities": []}, {"text": "Moses () multi-bleu 1 was used to compute BLEU () scores for summaries and prepare4rouge 2 applied to summaries before running ROUGE (.", "labels": [], "entities": [{"text": "BLEU () scores", "start_pos": 42, "end_pos": 56, "type": "METRIC", "confidence": 0.9241575797398885}, {"text": "summaries", "start_pos": 61, "end_pos": 70, "type": "TASK", "confidence": 0.9731036424636841}, {"text": "prepare4rouge 2", "start_pos": 75, "end_pos": 90, "type": "METRIC", "confidence": 0.9410350620746613}, {"text": "ROUGE", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.9130489230155945}]}, {"text": "shows the Pearson correlation of each variant of ROUGE with human assessment, in addition to BLEU's correlation with the same human assessment of summaries from DUC-2004.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9942740797996521}, {"text": "ROUGE", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9834229946136475}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9988116025924683}, {"text": "DUC-2004", "start_pos": 161, "end_pos": 169, "type": "DATASET", "confidence": 0.9399847388267517}]}, {"text": "Somewhat surprisingly, BLEU MT evaluation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9449421763420105}, {"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.880023717880249}, {"text": "ROUGE", "start_pos": 142, "end_pos": 147, "type": "METRIC", "confidence": 0.9719606041908264}, {"text": "BLEU", "start_pos": 200, "end_pos": 204, "type": "METRIC", "confidence": 0.9590497016906738}]}, {"text": "For many pairs of metrics, differences in correlation with human judgment are small, however, and prior to concluding superiority in performance of one metric over another, significance tests should be applied.", "labels": [], "entities": []}, {"text": "Since   achieves strongest correlation with human assessment, average ROUGE-2 precision with stemming and stop-words removed.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.997380793094635}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.870322585105896}]}, {"text": "shows ROUGE scores for summarization systems originally presented in.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.983116865158081}, {"text": "summarization", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9852092862129211}]}, {"text": "System rankings diverge considerably from those of the original evaluation.", "labels": [], "entities": []}, {"text": "Notably, the system now taking first place had originally ranked in fourth position.", "labels": [], "entities": []}, {"text": "Since the best variant of ROUGE is based on average ROUGE scores as opposed to median ROUGE scores, a difference of means significance testis appropriate provided the normality assumption of score distributions for systems is not violated.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.8750463128089905}]}, {"text": "In addition, since data used to evaluate systems are not independent, paired tests are also appropriate).", "labels": [], "entities": []}, {"text": "ROUGE score distributions for systems were tested for normality using the Shapiro-Wilk test where score distributions for none of the included systems were shown to be significantly non-normal.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8265936374664307}]}, {"text": "shows outcomes of paired t-tests for summary score distributions of each pair of systems, revealing three summarization systems not significantly outperformed by any other as DPP, ICSISUMM and REGSUM.", "labels": [], "entities": [{"text": "DPP", "start_pos": 175, "end_pos": 178, "type": "DATASET", "confidence": 0.829861044883728}, {"text": "ICSISUMM", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.49464190006256104}]}, {"text": "In addition, as expected, all state-of-the-art systems significantly outperform all baseline systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Proportions of optimal ROUGE variants  attributed to each ROUGE configuration option  (%).", "labels": [], "entities": []}, {"text": " Table 3: Summarization systems originally in- cluded in Hong et al. (2014) evaluated with the  best-performing ROUGE variant (Best): average  ROUGE-2 precision with stemming and stop words  removed; and evaluated with original suboptimal  variant (median ROUGE-2 recall with stemming  and without removal of stop-words)", "labels": [], "entities": [{"text": "precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.8148819804191589}]}, {"text": " Table 4: Correlation of top-ten metric variants for each alternate combination of linguistic quality and  coverage, \u2022 denotes a metric not significantly outperformed by any other under that particular human  evaluation combination, highest correlations highlighted in bold font.", "labels": [], "entities": []}]}