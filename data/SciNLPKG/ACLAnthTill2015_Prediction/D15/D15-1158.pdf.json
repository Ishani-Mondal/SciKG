{"title": [{"text": "Semi-supervised Dependency Parsing using Bilexical Contextual Features from Auto-Parsed Data", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7845732569694519}]}], "abstractContent": [{"text": "We present a semi-supervised approach to improve dependency parsing accuracy by using bilexical statistics derived from auto-parsed data.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.8321478366851807}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9140700101852417}]}, {"text": "The method is based on estimating the attachment potential of head-modifier words, by taking into account not only the head and modifier words themselves, but also the words surrounding the head and the modifier.", "labels": [], "entities": []}, {"text": "When integrating the learned statistics as features in a graph-based parsing model, we observe nice improvements inaccuracy when parsing various English datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "We are concerned with semi-supervised dependency parsing, namely how to leverage large amounts of unannotated data, in addition to annotated Treebank data, to improve dependency parsing accuracy.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7836154997348785}, {"text": "Treebank data", "start_pos": 141, "end_pos": 154, "type": "DATASET", "confidence": 0.9111606180667877}, {"text": "dependency parsing", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.8129499852657318}, {"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.844428300857544}]}, {"text": "Our method (Section 2) is based on parsing large amounts of unannotated text using a baseline parser, extracting word-interaction statistics from the automatically parsed corpus, and using these statistics as the basis of additional parser features.", "labels": [], "entities": [{"text": "parsing large amounts of unannotated text", "start_pos": 35, "end_pos": 76, "type": "TASK", "confidence": 0.8221606413523356}]}, {"text": "The automatically-parsed data is used to acquire statistics about lexical interactions, which are too sparse to estimate well from any realistically-sized Treebank.", "labels": [], "entities": []}, {"text": "Specifically, we attempt to infer a function assoc(head, modif er) measuring the \"goodness\" of head-modifier relations (\"how good is an arc in which black is a modifier of jump\").", "labels": [], "entities": []}, {"text": "A similar approach was taken by and Van Noord et al..", "labels": [], "entities": []}, {"text": "We depart from their work by extending the scoring to include a wider lexical context.", "labels": [], "entities": []}, {"text": "That is, given the sentence fragment in, we score the (incorrect) dependency arc (black, jump) based on the triplets (the black fox, will jump over).", "labels": [], "entities": []}, {"text": "Learning a function between word triplets raises an extreme data sparsity issue, which we deal with by decomposing the interaction between triplets to a sum of interactions between word pairs.", "labels": [], "entities": []}, {"text": "The decomposition we use is inspired by recent work in word-embeddings and dense vector representations (.", "labels": [], "entities": []}, {"text": "Indeed, we initially hoped to leverage the generalization abilities associated with vector-based representations.", "labels": [], "entities": []}, {"text": "However, we find that in our setup, reverting to direct countbased statistics achieve roughly the same results (Section 3).", "labels": [], "entities": []}, {"text": "Our derived features improve the accuracy of a first-order dependency parser by 0.75 UAS points (absolute) when evaluated on the in-domain WSJ test-set, obtaining a final accuracy of 92.32 UAS fora first-order parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9993463158607483}, {"text": "UAS", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9699371457099915}, {"text": "WSJ test-set", "start_pos": 139, "end_pos": 151, "type": "DATASET", "confidence": 0.9560156464576721}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9824540019035339}]}, {"text": "When comparing to the strong baseline of using Brown-clusters based features (, we find that our tripletsbased method outperform them by over 0.27 UAS points.", "labels": [], "entities": [{"text": "UAS", "start_pos": 147, "end_pos": 150, "type": "DATASET", "confidence": 0.4745127856731415}]}, {"text": "This is in contrast to previous works (e.g. () in which improvements over using Brown-clusters features were achieved only by adding to the cluster-based features, not by replacing them.", "labels": [], "entities": []}, {"text": "As expected, combining both our features and the brown-cluster features result in some additional gains.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data Our experiments are based on the Penn Treebank (PTB)) as well as the Google Web Treebanks (LDC2012T13), covering both in-domain and out-of-domain scenarios.", "labels": [], "entities": [{"text": "Penn Treebank (PTB))", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.9737786650657654}, {"text": "Google Web Treebanks (LDC2012T13)", "start_pos": 74, "end_pos": 107, "type": "DATASET", "confidence": 0.9458575447400411}]}, {"text": "We use the Stanford-dependencies representation (.", "labels": [], "entities": []}, {"text": "All the constituent-trees are converted to Stanforddependencies based on the settings of Version 1.0 of the Universal Treebank (.", "labels": [], "entities": [{"text": "Universal Treebank", "start_pos": 108, "end_pos": 126, "type": "DATASET", "confidence": 0.8434629738330841}]}, {"text": "These are based on the Stanford Dependencies converter but use some non-default flags, and change some of the dependency labels.", "labels": [], "entities": [{"text": "Stanford Dependencies converter", "start_pos": 23, "end_pos": 54, "type": "DATASET", "confidence": 0.9105976819992065}]}, {"text": "All of the models are trained on section 2-21 of the WSJ portion of the PTB.", "labels": [], "entities": [{"text": "WSJ portion of the PTB", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.882954478263855}]}, {"text": "For in-domain data, we evaluate on sections 22 (Dev) and 23 (Test).", "labels": [], "entities": []}, {"text": "All of the parameter tuning were performed on the Dev set, and we report test-set numbers only for the \"most interesting\" configurations.", "labels": [], "entities": [{"text": "Dev set", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.9031570255756378}]}, {"text": "For out-ofdomain data, we use the Brown portion of the PTB (Brown), as well as the test-sets of different domains available in the Google Web Treebank: Answers, Blogs, Emails, Reviews and Newsgroups.", "labels": [], "entities": [{"text": "Brown portion of the PTB (Brown)", "start_pos": 34, "end_pos": 66, "type": "DATASET", "confidence": 0.6376285180449486}, {"text": "Google Web Treebank", "start_pos": 131, "end_pos": 150, "type": "DATASET", "confidence": 0.8273100654284159}]}, {"text": "All trees have automatically assigned part-ofspeech tags, assigned by the TurboTagger POStagger.", "labels": [], "entities": [{"text": "TurboTagger POStagger", "start_pos": 74, "end_pos": 95, "type": "DATASET", "confidence": 0.8659154772758484}]}, {"text": "The train-set POS-tags were derived in a 10-fold jacknifing, and the different test datasets receive tags from a tagger trained on sections 2-21.", "labels": [], "entities": []}, {"text": "For auto-parsed data, we parse the text of the BLLIP corpus) using our baseline parser.", "labels": [], "entities": [{"text": "BLLIP corpus", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.8974626064300537}]}, {"text": "This is the same corpus used for deriving Brown clusters for use as features in (.", "labels": [], "entities": []}, {"text": "We use the clusters provided by Terry Koo . Parsing accuracy is measured by unlabeled attachment score (UAS) excluding punctuations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9363531470298767}, {"text": "unlabeled attachment score (UAS)", "start_pos": 76, "end_pos": 108, "type": "METRIC", "confidence": 0.8084880212942759}]}, {"text": "Implementation Details We focus on first-order parsers, as they are the most practical graphbased parsers in terms of running time in realistic parsing scenarios.", "labels": [], "entities": [{"text": "Implementation Details", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7035855799913406}]}, {"text": "Our base model is a reimplementation of a first-order projective Graphbased parser), which we extend to support the semi-supervised \u03c6 lex features.", "labels": [], "entities": []}, {"text": "The parser is trained for 10 iterations of online-training with passive-aggressive updates).", "labels": [], "entities": []}, {"text": "For the Brown-cluster features, we use the feature templates described by ().", "labels": [], "entities": []}, {"text": "The embedding vectors are trained using the freely available word2vecf software 7 , by conjoining each word with its relative position (-1, 0 or 1) and treating the head words as \"words\" and the modifier words to be \"contexts\".", "labels": [], "entities": []}, {"text": "The words are embedded into 300-dimensional vectors.", "labels": [], "entities": []}, {"text": "All code and vectors will be available at the first author's website.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained", "labels": [], "entities": [{"text": "Parsing accuracies", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.6252554953098297}, {"text": "UAS", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.7505016326904297}, {"text": "puctuation", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9800731539726257}]}]}