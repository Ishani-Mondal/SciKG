{"title": [{"text": "Exploring Markov Logic Networks for Question Answering", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7968877255916595}]}], "abstractContent": [{"text": "Elementary-level science exams pose significant knowledge acquisition and reasoning challenges for automatic question answering.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7006087005138397}, {"text": "automatic question answering", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.61294224858284}]}, {"text": "We develop a system that reasons with knowledge derived from textbooks , represented in a subset of first-order logic.", "labels": [], "entities": []}, {"text": "Automatic extraction, while scalable, often results in knowledge that is incomplete and noisy, motivating use of reasoning mechanisms that handle uncertainty.", "labels": [], "entities": [{"text": "Automatic extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7694491446018219}]}, {"text": "Markov Logic Networks (MLNs) seem a natural model for expressing such knowledge , but the exact way of leveraging MLNs is by no means obvious.", "labels": [], "entities": []}, {"text": "We investigate three ways of applying MLNs to our task.", "labels": [], "entities": []}, {"text": "First, we simply use the extracted science rules directly as MLN clauses and exploit the structure present in hard constraints to improve tractability.", "labels": [], "entities": []}, {"text": "Second, we interpret science rules as describing prototypical entities, resulting in a drastically simplified but brittle network.", "labels": [], "entities": []}, {"text": "Our third approach, called Praline, uses MLNs to align lexical elements as well as define and control how inference should be performed in this task.", "labels": [], "entities": [{"text": "Praline", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9700796008110046}]}, {"text": "Praline demonstrates a 15% accuracy boost and a 10x reduction in runtime as compared to other MLN-based methods, and comparable accuracy to word-based baseline approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9989572763442993}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9990704655647278}]}], "introductionContent": [{"text": "We consider the problem of answering questions in standardized science exams), which are used as a benchmark for developing knowledge acquisition and reasoning capabilities.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 124, "end_pos": 145, "type": "TASK", "confidence": 0.7193700671195984}]}, {"text": "The 4th grade science exam dataset from tests fora wide variety of knowledge and its application to specific scenarios.", "labels": [], "entities": [{"text": "4th grade science exam dataset", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.5494570553302764}]}, {"text": "In particular we focus on a subset that test students' understanding of various kinds of general rules and principles (e.g., gravity pulls objects towards the Earth) and their ability to apply these rules to reason about specific situations or scenarios (e.g., which force is responsible fora ball to drop?).", "labels": [], "entities": []}, {"text": "Answering these questions can be naturally formulated as a reasoning task given the appropriate form of knowledge.", "labels": [], "entities": []}, {"text": "Prior work on reasoning based approaches has largely relied on manually input knowledge.", "labels": [], "entities": []}, {"text": "We present an investigation of a reasoning approach that operates over knowledge automatically extracted from text.", "labels": [], "entities": []}, {"text": "In order to effectively reason over knowledge derived from text, a QA system must handle incomplete and potentially noisy knowledge, and allow for reasoning under uncertainty.", "labels": [], "entities": []}, {"text": "We cast QA as a reasoning problem in weighted-first order logic.", "labels": [], "entities": []}, {"text": "While many probabilistic formalisms exist, we use Markov Logic Networks for the ease of specification of weighted rules.", "labels": [], "entities": []}, {"text": "MLNs have been adopted for many NLP tasks.", "labels": [], "entities": []}, {"text": "Recently, and  have shown that MLNs can be used to reason with rules derived from natural language.", "labels": [], "entities": []}, {"text": "While MLNs appear to be a natural fit, it is a priori unclear how to effectively formulate the QA task as an MLN problem.", "labels": [], "entities": []}, {"text": "We find that unique characteristics of this domain pose new challenges in efficient inference.", "labels": [], "entities": []}, {"text": "Moreover, it is unclear how MLNs might perform on automatically extracted noisy rules and how they would fare against simpler baselines that do not rely as much on structured logical representations.", "labels": [], "entities": []}, {"text": "Our goal is to build a high accuracy reasoningbased QA system that can answer a question in near real time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9783305525779724}]}, {"text": "To this end, we investigate three MLN-based formulations: (1) A natural formulation that is intuitive but suffers from inefficient in-ference (e.g., over 10 minutes on 31% of the questions); (2) an extension that improves efficiency by using prototypical constants, but is brittle to variation in structure; and (3) a formulation with improved flexibility to handle variation in text and structure that is 15% more accurate and 10x faster than the other approaches.", "labels": [], "entities": []}, {"text": "Despite significant improvements over two natural MLN formulations, the best reasoning-based configuration still does not outperform a simpler word-based baseline.", "labels": [], "entities": []}, {"text": "We surmise that without effective salience models on text-derived rules, reasoning is unable to leverage the systematic advantages of the MLN-based models.", "labels": [], "entities": []}, {"text": "The improved flexibility in the MLN-based models essentially appears to approximate word-based approaches due to the noisy and incomplete nature of the input knowledge.", "labels": [], "entities": []}, {"text": "Nevertheless, the reasoning based method shows improved performance when adding multiple rules, which provides a principled way to inject additional knowledge and control inference for further improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used Tuffy 0.4 4 (Niu et al., 2011) as the base MLN solver and extended it to incorporate the hard-constraint based grounding reduction technique discussed earlier, implemented using the SAT solver Glucose 3.0 6 (Audemard and Simon, 2009) exploiting its \"solving under assumptions\" capability for efficiency.", "labels": [], "entities": [{"text": "Tuffy 0.4 4 (Niu et al., 2011)", "start_pos": 8, "end_pos": 38, "type": "DATASET", "confidence": 0.9171429932117462}, {"text": "MLN solver", "start_pos": 51, "end_pos": 61, "type": "TASK", "confidence": 0.8378148376941681}, {"text": "SAT solver Glucose 3.0 6 (Audemard and Simon, 2009", "start_pos": 190, "end_pos": 240, "type": "TASK", "confidence": 0.5985437387769873}]}, {"text": "We used a 10 minute timelimit, including a max of 6 minutes for grounding.", "labels": [], "entities": []}, {"text": "Marginal inference was performed using MC-SAT (), with default parameters and 5000 flips per sample to generate 500 samples for marginal estimation.", "labels": [], "entities": [{"text": "Marginal inference", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7895065248012543}]}, {"text": "We used a 2-core 2.5 GHz Amazon EC2 linux machine with 16 GB RAM.", "labels": [], "entities": [{"text": "Amazon EC2 linux machine", "start_pos": 25, "end_pos": 49, "type": "DATASET", "confidence": 0.8807214796543121}]}, {"text": "We selected 108 elementary-level science questions (non-diagram, multiple-choice) from 4th grade New York Regents exam as our benchmark (Dev-108) and used another 68 questions from the same source as a blind test set (Unseen-68)   The KB, representing roughly 47,000 sentences, was generated in advance by processing the New York Regents 4th grade science exam syllabus, the corresponding Barron's study guide, and documents obtained by querying the Internet for relevant terms.", "labels": [], "entities": [{"text": "Barron's study guide", "start_pos": 389, "end_pos": 409, "type": "DATASET", "confidence": 0.9569114148616791}]}, {"text": "Given a question, we use a simple word-overlap based matching algorithm, referred to as the rule selector, to retrieve the top 30 matching sentences to be considered for the question.", "labels": [], "entities": []}, {"text": "Textual entailment scores between words and short phrases were computed using WordNet, and converted to \"desired\" probabilities for soft entails evidence.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9796814918518066}]}, {"text": "The accuracy reported for each approach is computed as the number of multiple-choice questions it answers correctly, with a partial credit of 1/k in case of a k-way tie between the highest scoring options if they include the correct answer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995241165161133}]}, {"text": "compares the effectiveness of our three MLN formulations: FO-MLN, ER-MLN, and Praline.", "labels": [], "entities": [{"text": "FO-MLN", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9813459515571594}, {"text": "Praline", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9781913161277771}]}, {"text": "For each question and approach, we generate an MLN program for each answer option using the most promising KB rule for that answer option.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: QA performance of various MLN formulations. #MLN-Rules, #GroundClauses, and Runtime  per multiple-choice question are averaged over the corresponding dataset. #Answered column indicates  questions where at least one answer option didn't time out (left) and where no answer option timed out  (right). Of the 432 Dev MLNs (108 \u00d7 4), #Atoms and #GroundClauses for FO-MLN are averaged over  the 398 MLNs where grounding finished; 34 remaining MLNs timed out after processing 1.4M clauses.", "labels": [], "entities": [{"text": "Runtime", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.983984112739563}, {"text": "FO-MLN", "start_pos": 371, "end_pos": 377, "type": "METRIC", "confidence": 0.5823327898979187}]}, {"text": " Table 2: QA performance of Praline MLN  variations.", "labels": [], "entities": []}, {"text": " Table 3: QA performance: Praline vs. word-based.", "labels": [], "entities": []}]}