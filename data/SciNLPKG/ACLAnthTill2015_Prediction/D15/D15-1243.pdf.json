{"title": [{"text": "Evaluation of Word Vector Representations by Subspace Alignment", "labels": [], "entities": [{"text": "Subspace Alignment", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7164944112300873}]}], "abstractContent": [{"text": "Unsupervisedly learned word vectors have proven to provide exceptionally effective features in many NLP tasks.", "labels": [], "entities": []}, {"text": "Most common intrinsic evaluations of vector quality measure correlation with similarity judgments.", "labels": [], "entities": []}, {"text": "However, these often correlate poorly with how well the learned representations perform as features in downstream evaluation tasks.", "labels": [], "entities": []}, {"text": "We present QVEC-a computation-ally inexpensive intrinsic evaluation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources-that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision.", "labels": [], "entities": []}, {"text": "Unsupervised word vectors have been shown to benefit parsing (), chunking (, named entity recognition () and sentiment analysis), among others.", "labels": [], "entities": [{"text": "parsing", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.9643668532371521}, {"text": "chunking", "start_pos": 65, "end_pos": 73, "type": "TASK", "confidence": 0.9671713709831238}, {"text": "named entity recognition", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.6142905354499817}, {"text": "sentiment analysis", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.9379549026489258}]}, {"text": "Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks.", "labels": [], "entities": []}, {"text": "This lack of standardized evaluation is due, in part, to word vectors' major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, and, consequently, it is not clear how to score a non-interpretable representation.", "labels": [], "entities": []}, {"text": "Nevertheless, to facilitate development of better word vector models and for better error analysis of word vectors, it is desirable (1) to compare word vector models easily, without recourse to multiple extrinsic applications whose implementation and runtime can be costly; and (2) to understand how features in word vectors contribute to downstream tasks.", "labels": [], "entities": []}, {"text": "We propose a simple intrinsic evaluation measure for word vectors.", "labels": [], "entities": []}, {"text": "Our measure is based on component-wise correlations with manually constructed \"linguistic\" word vectors whose components have well-defined linguistic properties ( \u00a72).", "labels": [], "entities": []}, {"text": "Since vectors are typically used to provide features to downstream learning problems, our measure favors recall (rather than precision), which captures our intuition that meaningless dimensions in induced vector representations are less harmful than important dimensions that are missing.", "labels": [], "entities": [{"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9992589354515076}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9990174770355225}]}, {"text": "We thus align dimensions in a distributional word vector model with the linguistic dimension vectors to maximize the cumulative correlation of the aligned dimensions ( \u00a73).", "labels": [], "entities": []}, {"text": "The resulting sum of correlations of the aligned dimensions is our evaluation score.", "labels": [], "entities": []}, {"text": "Since the dimensions in the linguistic vectors are linguistically-informed, the alignment provides an \"annotation\" of components of the word vector space being evaluated.", "labels": [], "entities": []}, {"text": "To show that our proposed score is meaningful, we compare our intrinsic evaluation model to the standard (semantic) extrinsic evaluation benchmarks ( \u00a74).", "labels": [], "entities": []}, {"text": "For nine off-the-shelf word vector representation models, our model obtains high correlation (0.34 \u2264 r \u2264 0.89) with the extrinsic tasks ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [{"text": "We align dimensions of distributional word vectors to dimensions (linguistic properties) in the linguistic vectors described in \u00a72 to maximize the cumulative correlation of the aligned dimensions.", "labels": [], "entities": []}, {"text": "By projecting linguistic annotations via the alignments, we also obtain plausible annotations of dimensions in the distributional word vectors.", "labels": [], "entities": []}, {"text": "In this section, we formally describe the model, which we call the QVEC.", "labels": [], "entities": [{"text": "QVEC", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.9153599143028259}]}, {"text": "Let the number of common words in the vocabulary of the distributional and linguistic word vectors be N . We define, the distributional vector matrix X \u2208 R D\u00d7N with every row as a dimension vector x \u2208 R 1\u00d7N . D denotes word vector dimensionality.", "labels": [], "entities": []}, {"text": "Similarly, S \u2208 RP \u00d7N is the linguistic property matrix with every row as a linguistic property vector s \u2208 R 1\u00d7N . P denotes linguistic properties obtained from a manually-annotated linguistic resource.", "labels": [], "entities": []}, {"text": "We obtain an alignment between the word vector dimensions and the linguistic dimensions which maximizes the correlation between the aligned dimensions of the two matrices.", "labels": [], "entities": []}, {"text": "This is 1:n alignment: one distributional dimension is aligned to at most one linguistic property, whereas one linguistic property can be aligned ton distributional dimensions; see.", "labels": [], "entities": []}, {"text": "Let A \u2208 {0, 1} D\u00d7P be a matrix of alignments such that a ij = 1 iff xi is aligned to s j , otherwise a ij = 0.", "labels": [], "entities": []}, {"text": "If r(x i , s j ) is the Pearson's correlation between vectors xi and s j , then our objective is defined as: The constraint j a ij \u2264 1, warrants that one distributional dimension is aligned to at most one linguistic dimension.", "labels": [], "entities": []}, {"text": "The total correlation between two matrices QVEC is our intrinsic evaluation measure of a set of word vectors relative to a set of linguistic properties.", "labels": [], "entities": []}, {"text": "The QVEC's underlying hypothesis is that dimensions in distributional vectors correspond to linguistic properties of words.", "labels": [], "entities": []}, {"text": "It is motivated, among others, by the effectiveness of word vectors in linear models implying that linear combinations of features (vector dimensions) produce relevant, salient content.", "labels": [], "entities": []}, {"text": "Via the alignments a ij we obtain labels on dimensions in the distributional word vectors.", "labels": [], "entities": []}, {"text": "The magnitude of the correlation r(x i , s j ) corresponds to the annotation confidence: the higher the correlation, the more salient the linguistic content of the dimension.", "labels": [], "entities": [{"text": "annotation confidence", "start_pos": 66, "end_pos": 87, "type": "METRIC", "confidence": 0.882819265127182}]}, {"text": "Clearly, dimensions in the linguistic matrix S do not capture every possible linguistic property, and low correlations often correspond to the missing information in the linguistic matrix.", "labels": [], "entities": []}, {"text": "Thus, QVEC is a recall-oriented measure: highly-correlated alignments provide evaluation and annotation of vector dimensions, and missing information or noisy dimensions do not significantly affect the score since the correlations are low.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 16, "end_pos": 31, "type": "METRIC", "confidence": 0.995897650718689}]}, {"text": "We compare the QVEC to six standard extrinsic semantic tasks for evaluating word vectors; we now briefly describe the tasks.", "labels": [], "entities": []}, {"text": "We use three different benchmarks to measure word similarity.", "labels": [], "entities": []}, {"text": "The first one is the WS-353 dataset (), which contains 353 pairs of English words that have been assigned similarity ratings by humans.", "labels": [], "entities": [{"text": "WS-353 dataset", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.9432227611541748}]}, {"text": "The second is the MEN dataset () of 3,000 words pairs sampled from words that occur at least 700 times in a large web corpus.", "labels": [], "entities": [{"text": "MEN dataset", "start_pos": 18, "end_pos": 29, "type": "DATASET", "confidence": 0.866343766450882}]}, {"text": "The third dataset is SimLex-999 () which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs.", "labels": [], "entities": [{"text": "WS-353", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.9018843173980713}]}, {"text": "Word similarity is computed using cosine similarity between two words and the performance of word vectors is computed by Spearman's rank correlation between the rankings produced by vector model against the human rankings.", "labels": [], "entities": []}, {"text": "We consider four binary categorization tasks from the 20 Newsgroups (20NG) dataset.", "labels": [], "entities": [{"text": "20 Newsgroups (20NG) dataset", "start_pos": 54, "end_pos": 82, "type": "DATASET", "confidence": 0.649989977478981}]}, {"text": "8 Each task involves categorizing a document according to two related categories with training/dev/test split in accordance with.", "labels": [], "entities": []}, {"text": "For example, a classification task is between two categories of Sports: baseball vs hockey.", "labels": [], "entities": [{"text": "classification task", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.9055411219596863}]}, {"text": "We report the average classification accuracy across the four tasks.", "labels": [], "entities": [{"text": "classification", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.8856431245803833}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9352068305015564}]}, {"text": "Our next downstream semantic task is the sentiment analysis task (Senti)) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and report accuracy on the test set.", "labels": [], "entities": [{"text": "sentiment analysis task (Senti", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.8009228110313416}, {"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.8155491948127747}]}, {"text": "In both cases, we use the average of the word vectors of words in a document (and sentence, respectively) and use them as features in an 2 -regularized logistic regression classifier.", "labels": [], "entities": []}, {"text": "Finally, we evaluate vectors on the metaphor detection (Metaphor) ().", "labels": [], "entities": []}, {"text": "The system uses word vectors as features in a random forest classifier to label adjective-noun pairs as literal/metaphoric.", "labels": [], "entities": []}, {"text": "We report the system accuracy in 5-fold cross validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9992914199829102}]}], "tableCaptions": [{"text": " Table 1: Oracle linguistic word vectors, constructed from a  linguistic resource containing semantic annotations.", "labels": [], "entities": []}, {"text": " Table 2: Intrinsic (QVEC) and extrinsic scores of the 300- dimensional vectors trained using different word vector mod- els and evaluated on the Senti task. Pearson's correlation  between the intrinsic and extrinsic scores is r = 0.87.", "labels": [], "entities": [{"text": "Intrinsic (QVEC)", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9263152033090591}]}, {"text": " Table 4: Pearson's correlations between word similar- ity/QVEC scores and the downstream text classification tasks.", "labels": [], "entities": [{"text": "text classification", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.6977153718471527}]}, {"text": " Table 3: Pearson's correlations between QVEC scores of the 300-dimensional vectors trained using different word vector models  and the scores of the downstream tasks on the same vectors.", "labels": [], "entities": []}, {"text": " Table 5: Spearman's rank-order correlation between the QVEC ranking of the word vector models and the ranking produced by  (1) the Senti task, or (2) the aggregated ranking of all tasks (All). We rank separately models of vectors of different dimensionality", "labels": [], "entities": []}]}