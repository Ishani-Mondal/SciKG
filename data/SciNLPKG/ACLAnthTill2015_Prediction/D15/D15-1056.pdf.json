{"title": [{"text": "Semi-Supervised Bootstrapping of Relationship Extractors with Distributional Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "Semi-supervised bootstrapping techniques for relationship extraction from text iteratively expand a set of initial seed relationships while limiting the semantic drift.", "labels": [], "entities": [{"text": "relationship extraction from text", "start_pos": 45, "end_pos": 78, "type": "TASK", "confidence": 0.8096539378166199}]}, {"text": "We research bootstrapping for relationship extraction using word embeddings to find similar relationships.", "labels": [], "entities": [{"text": "relationship extraction", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.7674145400524139}]}, {"text": "Experimental results show that relying on word embeddings achieves a better performance on the task of extracting four types of relationships from a collection of newswire documents when compared with a baseline using TF-IDF to find similar relationships.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relationship Extraction (RE) transforms unstructured text into relational triples, each representing a relationship between two named-entities.", "labels": [], "entities": [{"text": "Relationship Extraction (RE)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8519103765487671}]}, {"text": "A bootstrapping system for RE starts with a collection of documents and a few seed instances.", "labels": [], "entities": [{"text": "RE", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9533883929252625}]}, {"text": "The system scans the document collection, collecting occurrence contexts for the seed instances.", "labels": [], "entities": []}, {"text": "Then, based on these contexts, the system generates extraction patterns.", "labels": [], "entities": []}, {"text": "The documents are scanned again using the patterns to match new relationship instances.", "labels": [], "entities": []}, {"text": "These newly extracted instances are then added to the seed set, and the process is repeated until a certain stop criteria is met.", "labels": [], "entities": []}, {"text": "The objective of bootstrapping is thus to expand the seed set with new relationship instances, while limiting the semantic drift, i.e. the progressive deviation of the semantics for the extracted relationships from the semantics of the seed relationships.", "labels": [], "entities": []}, {"text": "State-of-the-art approaches rely on word vector representations with TF-IDF weights.", "labels": [], "entities": []}, {"text": "However expanding the seed set by relying on TF-IDF representations to find similar instances has limitations, since the similarity between any two relationship instance vectors of TF-IDF weights is only positive when the instances share at least one term.", "labels": [], "entities": []}, {"text": "For instance, the phrases was founded by and is the co-founder of do not have any common words, but they have the same semantics.", "labels": [], "entities": []}, {"text": "Stemming techniques can aid in these cases, but only for variations of the same root word.", "labels": [], "entities": []}, {"text": "We propose to address this challenge with an approach based on word embeddings.", "labels": [], "entities": []}, {"text": "By relying on word embeddings, the similarity of two phrases can be captured even if no common words exist.", "labels": [], "entities": []}, {"text": "The word embeddings for co-founder and founded should be similar, since these words tend to occur in the same contexts.", "labels": [], "entities": []}, {"text": "Word embeddings can nonetheless also introduce semantic drift.", "labels": [], "entities": []}, {"text": "When using word embeddings, phrases like studied history at can, for instance, have a high similarity with phrases like history professor at.", "labels": [], "entities": []}, {"text": "In our approach, we control the semantic drift by ranking the extracted relationship instances, and by scoring the generated extraction patterns.", "labels": [], "entities": []}, {"text": "We implemented these ideas in BREDS, a bootstrapping system for RE based on word embeddings.", "labels": [], "entities": [{"text": "BREDS", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9363431334495544}, {"text": "RE", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9025899767875671}]}, {"text": "BREDS was evaluated with a collection of 1.2 million sentences from news articles.", "labels": [], "entities": [{"text": "BREDS", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7389880418777466}]}, {"text": "The experimental results show that our method outperforms a baseline bootstrapping system based on the ideas of which relies on TF-IDF representations.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our evaluation we used a set of 5.5 million news articles from AFP and APW).", "labels": [], "entities": [{"text": "AFP", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.9644407033920288}, {"text": "APW", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.8850926160812378}]}, {"text": "Our pre-processing pipeline is based on the models provided by the NLTK toolkit (: sentence segmentation 1 , tokenisation 2 , PoS-tagging 3 and named-entity recognition (NER).", "labels": [], "entities": [{"text": "NLTK toolkit", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.8826568722724915}, {"text": "sentence segmentation", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.748626172542572}, {"text": "tokenisation", "start_pos": 109, "end_pos": 121, "type": "TASK", "confidence": 0.9603211879730225}, {"text": "named-entity recognition (NER)", "start_pos": 144, "end_pos": 174, "type": "TASK", "confidence": 0.7601367771625519}]}, {"text": "The NER module in NLTK is a wrapper over the Stanford NER toolkit ().", "labels": [], "entities": [{"text": "Stanford NER toolkit", "start_pos": 45, "end_pos": 65, "type": "DATASET", "confidence": 0.8804659644762675}]}, {"text": "We performed weak entity-linking by matching entity names in sentences with FreebaseEasy ().", "labels": [], "entities": []}, {"text": "FreebaseEasy is a processed version of Freebase (), which contains a unique meaningful name for every entity, together with canonical binary relations.", "labels": [], "entities": [{"text": "FreebaseEasy", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9359370470046997}]}, {"text": "For our experiments, we selected only the sentences containing at least two entities linked to FreebaseEasy, which corresponded to 1.2 million sentences.", "labels": [], "entities": [{"text": "FreebaseEasy", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.9694570302963257}]}, {"text": "With the full articles set, we computed word embeddings with the skip-gram model 4 using the word2vec 5 implementation from.", "labels": [], "entities": []}, {"text": "The TF-IDF representations used by Snowball were calculated over the same articles set.", "labels": [], "entities": [{"text": "Snowball", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.9879123568534851}]}, {"text": "We adopted a previously proposed framework for the evaluation of large-scale RE systems by, to estimate precision and recall, using FreebaseEasy as the knowledge base.", "labels": [], "entities": [{"text": "RE", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.8323006629943848}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9976450800895691}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9991282820701599}]}, {"text": "We considered entity pairs no further away than 6 tokens, and a window of 2 tokens for the BEF and AFT contexts, ignoring the remaining of the sentence.", "labels": [], "entities": [{"text": "BEF", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.7338778376579285}]}, {"text": "We discarded the clusters with only one relationship instances, and ran a maximum of 4 bootstrapping iterations.", "labels": [], "entities": []}, {"text": "The W unk and W ngt parameters were set to 0.1 and 2, respectively, based on the results reported by.", "labels": [], "entities": []}, {"text": "We compared BREDS against Snowball in four relationship types, shown in We bootstrapped each relationship with two context weighting configurations in Formula (1): where Conf 1 only considers the BET context and Conf 2 uses the three contexts, while giving more importance to the BET context.", "labels": [], "entities": [{"text": "Snowball", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.9696446061134338}]}, {"text": "shows, for each relationship type, the best F 1 score and the corresponding precision and recall, for all combinations of \u03c4 sim and \u03c4 t values, and considering only extracted relationship instances with confidence scores equal or above 0.5.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9749614000320435}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9996582269668579}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.99925297498703}]}, {"text": "shows the results for the BREDS system, while shows the results for Snowball (ReVerb), a modified Snowball in which a relational pattern based on ReVerb is used to select the words for the BET context.", "labels": [], "entities": [{"text": "BREDS", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.642196774482727}, {"text": "Snowball (ReVerb)", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.8610007911920547}, {"text": "Snowball", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.980366051197052}]}, {"text": "Finally, shows the results for Snowball, implemented as described in the original paper.", "labels": [], "entities": [{"text": "Snowball", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.9301472306251526}]}, {"text": "Overall, BREDS achieves better F 1 scores than both versions of Snowball.", "labels": [], "entities": [{"text": "BREDS", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.9431729912757874}, {"text": "F 1 scores", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9836599032084147}, {"text": "Snowball", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.9905293583869934}]}, {"text": "The F 1 score of BREDS is higher, mainly as a consequence of much higher recall scores, which we believe to be due to the relaxed semantic matching caused by using the word embeddings.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9915252923965454}, {"text": "BREDS", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.8903188109397888}, {"text": "recall scores", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9831738770008087}]}, {"text": "For some relationship types, the recall more than doubles when using word embeddings instead of TF-IDF.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9993125200271606}]}, {"text": "For the acquired relationship, when considering Conf 1 , the precision of BREDS drops compared with the other versions of Snowball, but without affecting the F 1 score, since the higher recall compensates for the small loss in precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.999671459197998}, {"text": "BREDS", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9852427244186401}, {"text": "Snowball", "start_pos": 122, "end_pos": 130, "type": "DATASET", "confidence": 0.9822990298271179}, {"text": "F 1 score", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.990086297194163}, {"text": "recall", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.9981333613395691}, {"text": "precision", "start_pos": 227, "end_pos": 236, "type": "METRIC", "confidence": 0.9975544810295105}]}, {"text": "Regarding the context weighting configurations, Conf 2 produces a lower recall when compared to Conf 1 . This might be caused by the sparsity of both BEF and AFT, which contain many different words that do not contribute to capture the relationship between the two entities.", "labels": [], "entities": [{"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9992275238037109}, {"text": "BEF", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.9471847414970398}]}, {"text": "Although, sometimes, the phrase or word that indicates a relationship occurs on the BEF or AFT contexts, it is more often the case that these phrases or words occur in the BET context.", "labels": [], "entities": [{"text": "BEF or AFT contexts", "start_pos": 84, "end_pos": 103, "type": "DATASET", "confidence": 0.7717705070972443}, {"text": "BET context", "start_pos": 172, "end_pos": 183, "type": "DATASET", "confidence": 0.8166496157646179}]}, {"text": "The performance results of Snowball (Classic) and Snowball (ReVerb) suggest that selecting words based on a relational pattern to represent the BET context, instead of using all the words, works better for TF-IDF representations.", "labels": [], "entities": [{"text": "Snowball", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.9632269740104675}, {"text": "Snowball (ReVerb)", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.884883314371109}]}, {"text": "The results also show that word embeddings can generate more extraction patterns.", "labels": [], "entities": []}, {"text": "For instance, for the founder-of relationship, BREDS learns patterns based on words such as founder, cofounder, co-founders or founded, while Snowball only learns patterns that have the word founder, like CEO and founder or founder and chairman.", "labels": [], "entities": [{"text": "BREDS", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9237087368965149}, {"text": "Snowball", "start_pos": 142, "end_pos": 150, "type": "DATASET", "confidence": 0.9623653888702393}]}, {"text": "The implementations of BREDS and Snowball, as described in this paper, are available on-line 6 .", "labels": [], "entities": [{"text": "BREDS", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.6724980473518372}, {"text": "Snowball", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.9554355144500732}]}], "tableCaptions": []}