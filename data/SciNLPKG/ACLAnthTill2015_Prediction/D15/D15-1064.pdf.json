{"title": [{"text": "Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7206365615129471}]}], "abstractContent": [{"text": "We consider the task of named entity recognition for Chinese social media.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.6507880886395773}]}, {"text": "The long line of work in Chinese NER has fo-cused on formal domains, and NER for social media has been largely restricted to English.", "labels": [], "entities": [{"text": "NER", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8853427171707153}]}, {"text": "We present anew corpus of Weibo messages annotated for both name and nominal mentions.", "labels": [], "entities": []}, {"text": "Additionally, we evaluate three types of neural embeddings for representing Chinese text.", "labels": [], "entities": []}, {"text": "Finally, we propose a joint training objective for the embeddings that makes use of both (NER) labeled and unlabeled raw text.", "labels": [], "entities": []}, {"text": "Our methods yield a 9% improvement over a state-of-the-art baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity recognition (NER), and more generally the task of mention detection 1 , is an essential component of information extraction technologies: the first step before tasks such as relation extraction () and entity linking.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7815481175978979}, {"text": "mention detection 1", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7803033788998922}, {"text": "information extraction", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.7368620038032532}, {"text": "relation extraction", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.7539440989494324}, {"text": "entity linking", "start_pos": 214, "end_pos": 228, "type": "TASK", "confidence": 0.7460538148880005}]}, {"text": "A long line of work has focused on NER in both formal and informal domains, with recent efforts turning towards social media (;.", "labels": [], "entities": [{"text": "NER", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9801070690155029}]}, {"text": "While NER has included work on several languages, work on social media NER has largely focused on English language data.", "labels": [], "entities": [{"text": "NER", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.8065187335014343}, {"text": "NER", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.8233353495597839}]}, {"text": "We consider NER on Chinese social media from the popular Sina Weibo service, both because of the popularity of the service (comparable in size to Twitter and previously used in NLP research () and the challenges faced in processing Chinese language data.", "labels": [], "entities": [{"text": "NER", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9835779666900635}]}, {"text": "One approach is to utilize lexical embeddings to improve NER systems, including for Twitter.", "labels": [], "entities": [{"text": "NER", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9305394291877747}]}, {"text": "However, the use of embeddings for Chinese remains a challenge.", "labels": [], "entities": []}, {"text": "Unlike most languages, we cannot easily assign an embedding to each Chinese word without automated segmentation, which maybe unreliable, especially when we want to model informal text.", "labels": [], "entities": []}, {"text": "For this reason, state-of-the-art NER systems for Chinese do not tag words; they instead tag characters directly ().", "labels": [], "entities": []}, {"text": "While work has explored different embeddings for Chinese (, their inclusion in downstream tasks, such as NER, remains untested.", "labels": [], "entities": [{"text": "NER", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.8187990188598633}]}, {"text": "We explore several types of embeddings for Chinese text and their effect on Chinese social media NER.", "labels": [], "entities": [{"text": "Chinese social media NER", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.5371926203370094}]}, {"text": "Specifically, we make the following contributions.", "labels": [], "entities": []}, {"text": "1) We present the first system for NER on Chinese social media using anew corpus based on Weibo messages.", "labels": [], "entities": [{"text": "NER", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9916784763336182}]}, {"text": "We consider both name and nominal mentions, with the goal of supporting downstream systems, such as coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.9564006626605988}]}, {"text": "Notably, our results reveal that the gap between social media and traditional text for Chinese is much larger than similar corpora for English, suggesting this task as an interesting area of future work.", "labels": [], "entities": []}, {"text": "2) We evaluate three types of embeddings for Chinese text based on their inclusion in a downstream task.", "labels": [], "entities": []}, {"text": "We include results with and without fine-tuning.", "labels": [], "entities": []}, {"text": "3) We present a joint ob-3 Word segmentation performance is much worse on social media compared to formal text (.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.6386220455169678}]}, {"text": "Consider the overall F1 scores from, and compared to our best results in.", "labels": [], "entities": [{"text": "F1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9992238283157349}]}, {"text": "This is despite the fact that Chinese NER performance on formal texts is similar to English.", "labels": [], "entities": [{"text": "NER", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.84605872631073}]}, {"text": "jective that trains embeddings simultaneously for both NER and language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.6447685956954956}]}, {"text": "Joint training yields better results than post-hoc fine-tuning.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our methods under two settings: training on only name mentions, and training on both name and nominal mentions.", "labels": [], "entities": []}, {"text": "We re-train the Stanford NER system () as a baseline; besides, we also evaluate our implementation of the CRF from as described in \u00a72 as Baseline Features.", "labels": [], "entities": [{"text": "Stanford NER system", "start_pos": 16, "end_pos": 35, "type": "DATASET", "confidence": 0.8960254391034445}]}, {"text": "To this baseline, we add each of our three embedding models: word, character, character+position (as described in \u00a73), and report results on the modified  CRF model with and without fine-tuning.", "labels": [], "entities": []}, {"text": "We also report results for the joint method trained with the character+position model (cp), which performed the best on dev data for joint training.", "labels": [], "entities": []}, {"text": "General Results shows results for both dev (tuned) and test (held out) splits.", "labels": [], "entities": []}, {"text": "First, we observe that the results for the baseline are significantly below those for SIGHAN shared tasks as well as the reported results on Twitter NER, showing the difficulty of this task.", "labels": [], "entities": [{"text": "Twitter NER", "start_pos": 141, "end_pos": 152, "type": "DATASET", "confidence": 0.8988848328590393}]}, {"text": "In particular, recall is especially challenging.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9846462607383728}]}, {"text": "Second, all embeddings improve the baseline on test data, but the character + position model gets the best results.", "labels": [], "entities": []}, {"text": "Fine-tuning improves embedding results, but seems to overfit on dev data.", "labels": [], "entities": []}, {"text": "Finally, our joint model does the best in both conditions (name and name+nominal) on test data, improving over fine-tuning, yielding up to a 9% (absolute) improvement over a strong baseline.", "labels": [], "entities": []}, {"text": "Effect of Embeddings We expect improvements from embeddings to be larger when there is less training data.", "labels": [], "entities": []}, {"text": "shows F1 on dev data for different amounts of training data, from 200 instances up to 1400, for the character + position embeddings versus the baseline model.", "labels": [], "entities": [{"text": "F1", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.9985544085502625}]}, {"text": "We see that for both settings, we see larger improvements from embeddings for smaller training sets.", "labels": [], "entities": []}, {"text": "in given names, this makes recognizing person names challenging and contributes to 9% of our errors.", "labels": [], "entities": [{"text": "recognizing person names", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.8668911457061768}, {"text": "errors", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9884834885597229}]}, {"text": "The following largest source of error are transliterated foreign names, which contributes to 7% of the errors.", "labels": [], "entities": [{"text": "errors", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9762462973594666}]}, {"text": "Other sources including boundary error, type error, name abbreviation, nicknames, etc.", "labels": [], "entities": [{"text": "boundary error", "start_pos": 24, "end_pos": 38, "type": "METRIC", "confidence": 0.586492583155632}, {"text": "type error", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.6485197991132736}]}], "tableCaptions": [{"text": " Table 2: NER results for name mentions (top) and name + nominal mentions (bottom).", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9015790820121765}]}]}