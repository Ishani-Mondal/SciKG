{"title": [{"text": "Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language", "labels": [], "entities": [{"text": "Question-Answer Driven Semantic Role Labeling", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.5483188688755035}]}], "abstractContent": [{"text": "This paper introduces the task of question-answer driven semantic role labeling (QA-SRL), where question-answer pairs are used to represent predicate-argument structure.", "labels": [], "entities": [{"text": "question-answer driven semantic role labeling (QA-SRL)", "start_pos": 34, "end_pos": 88, "type": "TASK", "confidence": 0.6445768922567368}]}, {"text": "For example, the verb \"intro-duce\" in the previous sentence would be labeled with the questions \"What is in-troduced?\", and \"What introduces some-thing?\", each paired with the phrase from the sentence that gives the correct answer.", "labels": [], "entities": []}, {"text": "Posing the problem this way allows the questions themselves to define the set of possible roles, without the need for prede-fined frame or thematic role ontologies.", "labels": [], "entities": []}, {"text": "It also allows for scalable data collection by annotators with very little training and no linguistic expertise.", "labels": [], "entities": []}, {"text": "We gather data in two domains, newswire text and Wikipedia articles, and introduce simple classifier-based models for predicting which questions to ask and what their answers should be.", "labels": [], "entities": []}, {"text": "Our results show that non-expert anno-tators can produce high quality QA-SRL data, and also establish baseline performance levels for future work on this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic role labeling (SRL) is the widely studied challenge of recovering predicate-argument structure for natural language words, typically verbs.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8577557901541392}]}, {"text": "The goal is to determine \"who does what to whom,\" \"when,\" and \"where,\" etc.", "labels": [], "entities": []}, {"text": "(. However, this intuition is difficult to formalize and fundamental aspects of the task vary across efforts, for example FrameNet () models a large set of interpretable thematic roles (AGENT, PATIENT, etc.) while PropBank () uses a small set of verb-specific roles  (ARG0, ARG1, etc.).", "labels": [], "entities": [{"text": "AGENT", "start_pos": 186, "end_pos": 191, "type": "METRIC", "confidence": 0.8824602365493774}, {"text": "PATIENT", "start_pos": 193, "end_pos": 200, "type": "METRIC", "confidence": 0.7670547366142273}]}, {"text": "Existing task definitions can be complex and require significant linguistic expertise to understand, 1 causing challenges for data annotation and use in many target applications.", "labels": [], "entities": [{"text": "data annotation", "start_pos": 126, "end_pos": 141, "type": "TASK", "confidence": 0.705470621585846}]}, {"text": "In this paper, we introduce anew questionanswer driven SRL task formulation (QA-SRL), which uses question-answer pairs to label verbal predicate-argument structure.", "labels": [], "entities": [{"text": "SRL task formulation", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.871150811513265}]}, {"text": "For example, for the sentence in, we can ask a short question containing a verb, e.g. \"Who finished something?\", and whose answer is a phrase from the original sentence, in this case \"UCD.\"", "labels": [], "entities": [{"text": "UCD", "start_pos": 184, "end_pos": 187, "type": "DATASET", "confidence": 0.9226289391517639}]}, {"text": "The answer tells us that \"UCD\" is an argument of \"finished,\" while the question provides an indirect label on the role that \"UCD\" plays.", "labels": [], "entities": []}, {"text": "Enumerating all such pairs, as we will see later, provides a relatively complete representation of the original verb's arguments and modifiers.", "labels": [], "entities": []}, {"text": "The QA-SRL task formulation has a number of advantages.", "labels": [], "entities": [{"text": "QA-SRL task formulation", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.6124488711357117}]}, {"text": "It can be easily explained to nonexpert annotators with a short tutorial and a few examples.", "labels": [], "entities": []}, {"text": "Moreover, the formulation does not depend on any pre-defined inventory of semantic roles or frames, or build on any existing gram-mar formalisms.", "labels": [], "entities": []}, {"text": "Nonetheless, as we will show, it still represents the argument and modifier attachment decisions that have motivated previous SRL definitions, and which are of crucial importance for semantic understanding in a range of NLP tasks, such as machine translation ( and coreference resolution).", "labels": [], "entities": [{"text": "SRL definitions", "start_pos": 126, "end_pos": 141, "type": "TASK", "confidence": 0.8958756625652313}, {"text": "semantic understanding", "start_pos": 183, "end_pos": 205, "type": "TASK", "confidence": 0.8489479124546051}, {"text": "machine translation", "start_pos": 239, "end_pos": 258, "type": "TASK", "confidence": 0.8467351198196411}, {"text": "coreference resolution", "start_pos": 265, "end_pos": 287, "type": "TASK", "confidence": 0.957718014717102}]}, {"text": "The annotations also, perhaps surprisingly, capture other implicit arguments that cannot be read directly off of the syntax, as was required for previous SRL approaches.", "labels": [], "entities": []}, {"text": "For example, in \"It was his mother's birthday, so he was going to play her favorite tune\", annotators created the QA pair \"When would someone play something?", "labels": [], "entities": []}, {"text": "His mother's birthday\" which describes an implicit temporal relation.", "labels": [], "entities": []}, {"text": "Finally, QA-SRL data can be easily examined, proofread, and improved by anyone who speaks the language and understands the sentence; we use natural language to label the structure of natural language.", "labels": [], "entities": []}, {"text": "We present a scalable approach for QA-SRL annotation and baseline models for predicting QA pairs.", "labels": [], "entities": [{"text": "predicting QA pairs", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7935300270716349}]}, {"text": "Given a sentence and target word (the verb), we ask annotators to provide as many questionanswer pairs as possible, where the question comes from a templated space of wh-questions 2 and the answer is a phrase from the original sentence.", "labels": [], "entities": []}, {"text": "This approach guides annotators to quickly construct high quality questions within a very large space of possibilities.", "labels": [], "entities": []}, {"text": "Given a corpus of QA-SRL annotated sentences, we also train baseline classifiers for both predicting a set of questions to ask, and what their answers should be.", "labels": [], "entities": [{"text": "predicting a set of questions", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.7934727668762207}]}, {"text": "The question generation aspect of QA-SRL is unique to our formulation, and corresponds roughly to identifying what semantic role labels are present in previous formulations of the task.", "labels": [], "entities": [{"text": "question generation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7798229157924652}]}, {"text": "For example, the question \"Who finished something\" in corresponds to the AGENT role in FrameNet.", "labels": [], "entities": [{"text": "AGENT", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.7533438205718994}]}, {"text": "Table 1 also shows examples of similar correspondences for PropBank roles.", "labels": [], "entities": []}, {"text": "Instead of pre-defining the labels, as done in previous work, the questions themselves define the set of possibilities.", "labels": [], "entities": []}, {"text": "Experiments demonstrate high quality data annotation with very little annotator training and establish baseline performance levels for the task.", "labels": [], "entities": []}, {"text": "We hired non-expert, part-time annotators on Upwork (previously oDesk) to label over 3,000 sentences (nearly 8,000 verbs) across two domains (newswire and Wikipedia) at a cost of approximately $0.50 per verb.", "labels": [], "entities": []}, {"text": "We show that the data is high quality, rivaling PropBank in many aspects including coverage, and easily gathered in nonnewswire domains.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.9558547735214233}, {"text": "coverage", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9810749888420105}]}, {"text": "The baseline performance levels for question generation and answering reinforce the quality of the data and highlight the potential for future work on this task.", "labels": [], "entities": [{"text": "question generation and answering", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.7364171743392944}]}, {"text": "In summary, our contributions are: \u2022 We introduce the task of question-answer driven semantic role labeling (QA-SRL), by using question-answer pairs to specify verbal arguments and the roles they play, without predefining an inventory of frames or semantic roles.", "labels": [], "entities": [{"text": "question-answer driven semantic role labeling (QA-SRL)", "start_pos": 62, "end_pos": 116, "type": "TASK", "confidence": 0.7056016102433205}]}, {"text": "\u2022 We present a novel, lightweight templatebased scheme (Section 3) that enables the high quality QA-SRL data annotation with very little training and no linguistic expertise.", "labels": [], "entities": []}, {"text": "\u2022 We define two new QA-SRL sub-tasks, question generation and answer identification, and present baseline learning approaches for both (Sections 4 and 5).", "labels": [], "entities": [{"text": "question generation and answer identification", "start_pos": 38, "end_pos": 83, "type": "TASK", "confidence": 0.7498604595661164}]}, {"text": "The results demonstrate that our data is high-quality and supports the study of better learning algorithms.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes our annotation process in more detail, and discusses agreement between our annotations and PropBank.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 114, "end_pos": 122, "type": "DATASET", "confidence": 0.9625974297523499}]}, {"text": "shows examples provided by non-expert annotators.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Annotated data statistics.", "labels": [], "entities": []}, {"text": " Table 5: Agreement with gold PropBank (CoNLL- 2009) for all roles, core roles, and adjuncts. Preci- sion is the percentage of QA pairs covering exactly  one PropBank relation. Recall is the percentage  of PropBank relations covered by exactly one QA  pair.", "labels": [], "entities": [{"text": "CoNLL- 2009)", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.879314050078392}, {"text": "Preci- sion", "start_pos": 94, "end_pos": 105, "type": "METRIC", "confidence": 0.9465189774831136}, {"text": "Recall", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9954622387886047}]}, {"text": " Table 6: Co-occurrence of wh-words in QA-SRL  annoations and role labels in PropBank.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9566848874092102}]}, {"text": " Table 9: Manual evaluation results for question  generation in two domains, including the averaged  number of distinct questions that are answerable  given the sentence (Ans.) and the averaged num- ber of questions that are grammatical (Gram.).", "labels": [], "entities": [{"text": "question  generation", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7239683270454407}, {"text": "Ans.", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9422692060470581}]}]}