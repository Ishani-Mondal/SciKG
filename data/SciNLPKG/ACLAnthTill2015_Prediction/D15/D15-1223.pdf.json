{"title": [], "abstractContent": [{"text": "Automated text summarization is aimed at extracting essential information from original text and presenting it in a minimal, often predefined, number of words.", "labels": [], "entities": [{"text": "Automated text summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5983784000078837}]}, {"text": "In this paper, we introduce anew approach for unsupervised extractive summariza-tion, based on the Minimum Description Length (MDL) principle, using the Krimp dataset compression algorithm (Vreeken et al., 2011).", "labels": [], "entities": [{"text": "Minimum Description Length (MDL) principle", "start_pos": 99, "end_pos": 141, "type": "METRIC", "confidence": 0.718638905457088}]}, {"text": "Our approach represents a text as a transactional dataset, with sentences as transactions, and then describes it by itemsets that stand for frequent sequences of words.", "labels": [], "entities": []}, {"text": "The summary is then compiled from sentences that compress (and as such, best describe) the document.", "labels": [], "entities": []}, {"text": "The problem of summarization is reduced to the maximal coverage, following the assumption that a summary that best describes the original text, should cover most of the word sequences describing the document.", "labels": [], "entities": [{"text": "summarization", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9921959638595581}, {"text": "maximal coverage", "start_pos": 47, "end_pos": 63, "type": "METRIC", "confidence": 0.9220351278781891}]}, {"text": "We solve it by a greedy algorithm and present the evaluation results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many unsupervised approaches for extractive text summarization follow the maximal coverage principle (, where the extract that maximally covers the information contained in the source text, is selected.", "labels": [], "entities": [{"text": "extractive text summarization", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.6361163755257925}]}, {"text": "Since the exhaustive solution demands an exponential number of tests, approximation techniques, such as a greedy approach or a global optimization of a target function, are utilized.", "labels": [], "entities": []}, {"text": "It is quite common to measure text informativeness by the frequency of its componentswords, phrases, concepts, and soon.", "labels": [], "entities": []}, {"text": "A different approach that received much less attention is based on the Minimum Description Length (MDL) principle, defining the best summary as the one that leads to the best compression of the text by providing its shortest and most concise description.", "labels": [], "entities": []}, {"text": "The MDL principle is widely useful in compression techniques of non-textual data, such as summarization of query results for OLAP applications.", "labels": [], "entities": [{"text": "summarization of query", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.8831413388252258}]}, {"text": "(;) However, only a few works on text summarization using MDL can be found in the literature.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7369722425937653}]}, {"text": "Authors of) used K-means clustering extended with MDL principle for finding diverse topics in the summarized text.", "labels": [], "entities": [{"text": "K-means clustering", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.6547467410564423}]}, {"text": "Nomoto in (2004) also extended the C4.5 classifier with MDL for learning rhetorical relations.", "labels": [], "entities": [{"text": "learning rhetorical relations", "start_pos": 64, "end_pos": 93, "type": "TASK", "confidence": 0.7231081128120422}]}, {"text": "In) the problem of micro-review summarization is formulated within the MDL framework, where the authors view the tips as being encoded by snippets, and seek to find a collection of snippets that produce the encoding with the minimum number of bits.", "labels": [], "entities": [{"text": "micro-review summarization", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.6749025881290436}]}, {"text": "This paper introduces anew MDL-based approach for extracting relevant sentences into a summary.", "labels": [], "entities": []}, {"text": "The approach represents documents as a sequential transactional dataset and then compresses it by replacing frequent sequences of words by codes.", "labels": [], "entities": []}, {"text": "The summary is then compiled from sentences that best compress (or describe) the document content.", "labels": [], "entities": []}, {"text": "The intuition behind this approach says that a summary that best describes the original text should cover its most frequent word sequences.", "labels": [], "entities": []}, {"text": "As such, the problem of summarization is very naturally reduced to the maximal coverage problem.", "labels": [], "entities": [{"text": "summarization", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9905879497528076}]}, {"text": "We solve it by the greedy method which ranks sentences by their coverage of best compressing frequent word sequences and selects the top-ranked sentences to a summary.", "labels": [], "entities": []}, {"text": "There area few works that applied the common data mining techniques for calculating frequent itemsets from transactional data to the text summarization task (), but none of them followed the MDL principle.", "labels": [], "entities": [{"text": "text summarization task", "start_pos": 133, "end_pos": 156, "type": "TASK", "confidence": 0.78319251537323}]}, {"text": "The comparative results on three different corpora show that our approach outperforms other unsupervised state-of-the-art summarizers.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments on three corpora from the Document Understanding Conference (DUC), summarized in  by the ROUGE-1 and ROUGE-2 (Lin, 2004) recall scores, with the word limit indicated in, without stemming and stopword removal.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC)", "start_pos": 51, "end_pos": 90, "type": "TASK", "confidence": 0.48654334247112274}, {"text": "ROUGE-1", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.9737566709518433}, {"text": "ROUGE-2 (Lin, 2004) recall", "start_pos": 126, "end_pos": 152, "type": "METRIC", "confidence": 0.6640621040548597}]}, {"text": "The results of the introduced algorithm (Gamp) maybe affected by several input parameters: minimum support (Supp), codes limit (C), and the maximal gap allowed between frequent words (Gap).", "labels": [], "entities": [{"text": "minimum support (Supp)", "start_pos": 91, "end_pos": 113, "type": "METRIC", "confidence": 0.7085623860359191}, {"text": "codes limit (C)", "start_pos": 115, "end_pos": 130, "type": "METRIC", "confidence": 0.9228333353996276}]}, {"text": "In order to find the best algorithm settings fora general case, we performed experiments that explored the impact of these parameters on the summarization results.", "labels": [], "entities": [{"text": "summarization", "start_pos": 141, "end_pos": 154, "type": "TASK", "confidence": 0.9643644690513611}]}, {"text": "First, we experimented with different values of support count in the range of.", "labels": [], "entities": []}, {"text": "The results show that we get the best summaries using the sequences that occur in at least four document sentences.", "labels": [], "entities": []}, {"text": "A limit on the number of codes is an additional parameter.We explored the impact of this parameter on the quality of generated summaries.", "labels": [], "entities": []}, {"text": "As we could conclude from our experiments, the best summarization results are obtained if this parameter is set to the maximal number of words in the summary, W . Consequently, we used 100 codes for summarizing DUC 2002 and DUC 2004 documents and 250 codes for DUC 2007 documents.", "labels": [], "entities": [{"text": "summarization", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9863160252571106}, {"text": "summarizing", "start_pos": 199, "end_pos": 210, "type": "TASK", "confidence": 0.967517614364624}, {"text": "DUC 2002 and DUC 2004 documents", "start_pos": 211, "end_pos": 242, "type": "DATASET", "confidence": 0.7167429228623708}, {"text": "DUC 2007 documents", "start_pos": 261, "end_pos": 279, "type": "DATASET", "confidence": 0.9498752156893412}]}, {"text": "The maximal gap ratio defines a pattern for generating the frequent sequences and has a direct effect on their structure and number.", "labels": [], "entities": [{"text": "maximal gap ratio", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.8308860460917155}]}, {"text": "Our experiments showed that allowing a small gap between words of a sequence helps to improve slightly the ranking of sentences, but the improvement is not significant.", "labels": [], "entities": []}, {"text": "Thus we used Gap = 0.8 in comparative experiments for all corpora.", "labels": [], "entities": [{"text": "Gap", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9932823181152344}]}, {"text": "The resulting settings for each corpus are shown in  We compared the Gamp algorithm with the two known unsupervised state-of-the-art summarizers denoted by and McDonald.", "labels": [], "entities": []}, {"text": "As a baseline, we used a very simple approach that takes first sentences to a summary (denoted by TopK).", "labels": [], "entities": []}], "tableCaptions": []}