{"title": [], "abstractContent": [{"text": "When applying machine learning to problems in NLP, there are many choices to make about how to represent input texts.", "labels": [], "entities": []}, {"text": "They can have a big effect on performance , but they are often uninteresting to researchers or practitioners who simply need a module that performs well.", "labels": [], "entities": []}, {"text": "We apply sequential model-based optimization over this space of choices and show that it makes standard linear models competitive with more sophisticated, expensive state-of-the-art methods based on latent variables or neural networks on various topic classification and sentiment analysis problems.", "labels": [], "entities": [{"text": "topic classification and sentiment analysis", "start_pos": 246, "end_pos": 289, "type": "TASK", "confidence": 0.8329624652862548}]}, {"text": "Our approach is a first step towards black-box NLP systems that work with raw text and do not require manual tuning.", "labels": [], "entities": []}], "introductionContent": [{"text": "NLP researchers and practitioners spend a considerable amount of time comparing machine-learned models of text that differ in relatively uninteresting ways.", "labels": [], "entities": []}, {"text": "For example, in categorizing texts, should the \"bag of words\" include bigrams, and is tf-idf weighting a good idea?", "labels": [], "entities": []}, {"text": "In learning word embeddings, distributional similarity approaches have been shown to perform competitively with neural network models when the hyperparameters (e.g., context window, subsampling rate, smoothing constant) are carefully tuned ().", "labels": [], "entities": [{"text": "learning word embeddings", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.6605223615964254}]}, {"text": "These choices matter experimentally, often leading to big differences in performance, with little consistency across tasks and datasets in which combination of choices works best.", "labels": [], "entities": []}, {"text": "Unfortunately, these differences tell us little about language or the problems that machine learners are supposed to solve.", "labels": [], "entities": []}, {"text": "We propose that these decisions can be automated in a similar way to hyperparameter selection (e.g., choosing the strength of a ridge or lasso regularizer).", "labels": [], "entities": []}, {"text": "Given a particular text dataset and classification task, we show a technique for optimizing over the space of representational choices, along with other \"nuisances\" that interact with these decisions, like hyperparameter selection.", "labels": [], "entities": []}, {"text": "For example, using higher-order n-grams means more features and a need for stronger regularization and more training iterations.", "labels": [], "entities": []}, {"text": "Generally, these decisions about instance representation are made by humans, heuristically; our work seeks to automate them, not unlike, who proposed to use genetic algorithms to optimize representational choices.", "labels": [], "entities": []}, {"text": "Our technique instantiates sequential modelbased optimization).", "labels": [], "entities": [{"text": "sequential modelbased optimization", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.624331017335256}]}, {"text": "SMBO and other Bayesian optimization approaches have been shown to work well for hyperparameter tuning.", "labels": [], "entities": [{"text": "SMBO", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.8654677271842957}, {"text": "hyperparameter tuning", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.7863835096359253}]}, {"text": "Though popular in computer vision (), these techniques have received little attention in NLP.", "labels": [], "entities": []}, {"text": "We apply it to logistic regression on a range of topic and sentiment classification tasks.", "labels": [], "entities": [{"text": "logistic regression", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7381663620471954}, {"text": "sentiment classification tasks", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.8315191268920898}]}, {"text": "Consistently, our method finds representational choices that perform better than linear baselines previously reported in the literature, and that, in some cases, are competitive with more sophisticated non-linear models trained using neural networks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We fix L to logistic regression.", "labels": [], "entities": []}, {"text": "We optimize text representation based on the types of n-grams used, the type of weighting scheme, and the removal of stopwords; we also optimize the regularizer and training convergence criterion, which interact with the representation.", "labels": [], "entities": []}, {"text": "Note that even with this limited number of options, the number of possible combinations is huge, 3 so exhaustive search is computationally expensive.", "labels": [], "entities": []}, {"text": "In all our experiments for all datasets, we limit ourselves to 30 trials per dataset.", "labels": [], "entities": []}, {"text": "The only preprocessing we applied was downcasing.", "labels": [], "entities": []}, {"text": "We always use a development set to evaluate f (x) during learning and report the final result on an unseen test set.", "labels": [], "entities": []}, {"text": "We summarize the hyperparameters selected by our method, and the accuracies achieved (on test data) in.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.979790985584259}]}, {"text": "We discuss comparisons to baselines for each dataset in turn.", "labels": [], "entities": []}, {"text": "For each of our datasets, we select supervised, nonensemble classification methods from previous literature as baselines.", "labels": [], "entities": []}, {"text": "In each case, we emphasize comparisons with the best-published linear method Stanford sentiment treebank (Socher et al., 2013)-.", "labels": [], "entities": [{"text": "Stanford sentiment treebank", "start_pos": 77, "end_pos": 104, "type": "DATASET", "confidence": 0.8059678276379904}]}, {"text": "A sentence-level sentiment analysis dataset of rottentomatoes.com movie reviews: http://nlp.stanford.edu/sentiment.", "labels": [], "entities": [{"text": "rottentomatoes.com movie reviews", "start_pos": 47, "end_pos": 79, "type": "DATASET", "confidence": 0.9336196184158325}]}, {"text": "We use the binary classification task where the goal is to predict whether a review is positive or negative (no neutral).", "labels": [], "entities": []}, {"text": "Our logistic regression model outperforms the baseline SVM reported by, who used only unigrams but did not specify the weighting scheme for their SVM baseline.", "labels": [], "entities": []}, {"text": "While our result is still below the state-of-the-art based on the the recursive neural tensor networks) and the paragraph vector (), we show that logistic regression is comparable with recursive and matrix-vector neural networks Our method is on par with the secondbest of these, outperforming all of the reported feed-forward neural networks and SVM variants Johnson and Zhang used as baselines.", "labels": [], "entities": []}, {"text": "They varied the representations, and used log term frequency and normalization to unit vectors as the weighting scheme, after finding that this outperformed term frequency.", "labels": [], "entities": []}, {"text": "Our method achieved the best performance with binary weighting, which they did not consider.", "labels": [], "entities": []}, {"text": "IMDB movie reviews (Maas et al., 2011)-.", "labels": [], "entities": [{"text": "IMDB movie reviews", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.928488552570343}]}, {"text": "A binary sentiment analysis dataset of highly polar IMDB movie reviews: http://ai.stanford.edu/~amaas/data/sentiment.", "labels": [], "entities": []}, {"text": "The results parallel those for Amazon electronics; our method comes close to convolutional neural networks, which are state-of-the-art.", "labels": [], "entities": []}, {"text": "It outperforms SVMs and feed-forward neural networks, the restricted Boltzmann machine approach presented by, and compressive feature learning (), we consider the task to predict the vote (\"yea\" or \"nay\") for the speaker of each speech segment (speaker-based speech-segment classification).", "labels": [], "entities": [{"text": "speaker-based speech-segment classification", "start_pos": 245, "end_pos": 288, "type": "TASK", "confidence": 0.5804770489533743}]}, {"text": "Our method outperforms the best results of, which use a multi-level structured model based on a latent-variable SVM.", "labels": [], "entities": []}, {"text": "We show comparisons to two weaker baselines as well.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The set of hyperparameters considered in our ex- periments. The top half are hyperparameters related to text  representation, while the bottom half are logistic regression  hyperparameters, which also interact with the chosen repre- sentation.", "labels": [], "entities": [{"text": "text  representation", "start_pos": 114, "end_pos": 134, "type": "TASK", "confidence": 0.7042995542287827}]}, {"text": " Table 2: Comparisons on the Stanford sentiment treebank  dataset. Scores are as reported by Socher et al. (2013) and Le  and Mikolov (2014). Test size = 6, 920.", "labels": [], "entities": [{"text": "Stanford sentiment treebank  dataset", "start_pos": 29, "end_pos": 65, "type": "DATASET", "confidence": 0.932015672326088}, {"text": "Test size", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.8677809238433838}]}, {"text": " Table 3: Comparisons on the Amazon electronics and IMDB  reviews datasets. SVM results are from Wang and Manning  (2012), the RBM (restricted Bolzmann machine) result is from  Dahl et al. (2012), NN and CNN results are from Johnson  and Zhang (2015), and LR-{1, 2, 3, 4, 5}-grams and compres- sive feature learning results are from Paskov et al. (2013).  Test size = 20, 000 for both datasets.", "labels": [], "entities": [{"text": "LR", "start_pos": 256, "end_pos": 258, "type": "METRIC", "confidence": 0.9619200229644775}]}, {"text": " Table 5: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. \"Acc\" shows  accuracies for our logistic regression model. \"Min\" and \"Max\" correspond to the min n-grams and max n-grams respectively.  \"Reg.\" is the regularization type, \"Strength\" is the regularization strength, and \"Conv.\" is the convergence tolerance. For  regularization strength, we round it to the nearest integer for readability.", "labels": [], "entities": [{"text": "Acc", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9950268864631653}, {"text": "convergence tolerance", "start_pos": 343, "end_pos": 364, "type": "METRIC", "confidence": 0.8687052130699158}]}]}