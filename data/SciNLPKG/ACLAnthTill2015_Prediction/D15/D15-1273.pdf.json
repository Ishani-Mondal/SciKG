{"title": [{"text": "Transducer Disambiguation with Sparse Topological Features", "labels": [], "entities": [{"text": "Transducer Disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9080979824066162}]}], "abstractContent": [{"text": "We describe a simple and efficient algorithm to disambiguate non-functional weighted finite state transducers (WFSTs), i.e. to generate anew WFST that contains a unique, best-scoring path for each hypothesis in the input labels along with the best output labels.", "labels": [], "entities": []}, {"text": "The algorithm uses topological features combined with a tropical sparse tuple vector semiring.", "labels": [], "entities": []}, {"text": "We empirically show that our algorithm is more efficient than previous work in a PoS-tagging disambiguation task.", "labels": [], "entities": [{"text": "PoS-tagging disambiguation task", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.8164534966150919}]}, {"text": "We use our method to rescore very large translation lattices with a bilingual neural network language model, obtaining gains inline with the literature.", "labels": [], "entities": []}], "introductionContent": [{"text": "Weighted finite-state transducers (WFSTs), or lattices, are used in speech and language processing to compactly represent and manipulate a large number of strings.", "labels": [], "entities": []}, {"text": "Applying a finite-state operation (eg. PoS tagging) to a lattice via composition produces a WFST that maps input (eg. words) onto output strings (eg. PoS tags) and preserves the arc-level alignment between each input and output symbol (eg. each arc is labeled with a word-tag pair and has a weight).", "labels": [], "entities": [{"text": "PoS tagging)", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.7340360383192698}]}, {"text": "Typically, the result of such operation is a WFST that is ambiguous because it contains multiple paths with the same input string, and non-functional because it contains multiple output strings fora given input string.", "labels": [], "entities": []}, {"text": "Disambiguating such WFSTs is the task of creating a WFST that encodes only the best-scoring path of each input string, while still maintaining the arc-level mapping between input and output symbols.", "labels": [], "entities": []}, {"text": "This is a non-trivial task , and so far only Unless one enumerates all the possible input strings in one algorithm has been described ; the main steps are: (a) Map the WFST into an equivalent weighted finite-state automata (WFSA) using weights that contain both the WFST weight and output symbols (using a special semiring) (b) Apply WFSA determinization under this semiring to ensure that only one unique path per input string survives (c) Expand the result back to an WFST that preserves arc-level alignments We present anew disambiguation algorithm that can efficiently accomplish this.", "labels": [], "entities": []}, {"text": "In Section 2 we describe how the tropical sparse tuple vector semiring can keep track of individual arcs in the original WFST as topological features during the mapping step (a).", "labels": [], "entities": [{"text": "WFST", "start_pos": 121, "end_pos": 125, "type": "DATASET", "confidence": 0.9384680986404419}]}, {"text": "This allows us to describe in Section 3 an efficient expansion algorithm for step (c).", "labels": [], "entities": []}, {"text": "We show in Section 4 empirical evidence that our algorithm is more efficient than  in their same PoS-tagging task.", "labels": [], "entities": []}, {"text": "We also show how our method can be applied in rescoring translation lattices under a bilingual neuralnetwork model), obtaining BLEU score gains consistent with the literature.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.9804933071136475}]}, {"text": "Section 5 reviews related work and concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our algorithm, henceforth called topological, in two ways: we empirically contrast disambiguation times against previous work, and then apply it to rescore translation lattices with bilingual neural network models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Translation scores in lower-case BLEU.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.867691159248352}, {"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9929458498954773}]}]}