{"title": [{"text": "An Empirical Analysis of Optimization for Max-Margin NLP", "labels": [], "entities": []}], "abstractContent": [{"text": "Despite the convexity of structured max-margin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice.", "labels": [], "entities": []}, {"text": "We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends.", "labels": [], "entities": [{"text": "summarization, parsing", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.6610645552476248}]}, {"text": "First, margin methods do tend to outperform both likelihood and the perceptron.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9863720536231995}]}, {"text": "Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods.", "labels": [], "entities": [{"text": "primal optimization", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.6738179475069046}]}, {"text": "This advantage is most pronounced for tasks with dense or continuous-valued features.", "labels": [], "entities": []}, {"text": "Overall, we argue fora particularly simple online pri-mal subgradient descent method that, despite being rarely mentioned in the literature , is surprisingly effective in relation to its alternatives.", "labels": [], "entities": [{"text": "pri-mal subgradient descent", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.5903908511002859}]}], "introductionContent": [{"text": "Structured discriminative models have proven effective across a range of tasks in NLP including tagging (), reranking parses, and many more.", "labels": [], "entities": [{"text": "reranking parses", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.7904901206493378}]}, {"text": "Common approaches to training such models include margin methods, likelihood methods, and mistake-driven procedures like the averaged perceptron algorithm.", "labels": [], "entities": []}, {"text": "In this paper, we primarily consider the relative empirical behavior of several online optimization methods for margin-based objectives, with secondary attention to other approaches for calibration.", "labels": [], "entities": []}, {"text": "It is increasingly common to train structured models using a max-margin objective that incorporates a loss function that decomposes in the same way as the dynamic program used for inference).", "labels": [], "entities": []}, {"text": "Fortunately, most structured margin objectives are convex, so a range of optimization methods with similar theoretical properties are available -in short, any of these methods will work in the end.", "labels": [], "entities": []}, {"text": "However, in practice, how fast each method converges varies across tasks.", "labels": [], "entities": []}, {"text": "Moreover, some of the most popular methods more loosely associated with the margin objective, such as the MIRA algorithm or even the averaged perceptron) are not global optimizations and can have different properties.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.547319769859314}]}, {"text": "We analyze a range of methods empirically, to understand on which tasks and with which feature types, they are most effective.", "labels": [], "entities": []}, {"text": "We modified six existing, high-performance, systems to enable loss-augmented decoding, and trained these models with six different methods.", "labels": [], "entities": []}, {"text": "We have released our learning code as a Java library.", "labels": [], "entities": []}, {"text": "Our results provide support for the conventional wisdom that margin-based optimization is broadly effective, frequently outperforming likelihood optimization and the perceptron algorithm.", "labels": [], "entities": [{"text": "margin-based optimization", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.6875727623701096}]}, {"text": "We also found that directly optimizing the primal structured margin objective based on subgradients calculated from single training instances is surprisingly effective, performing consistently well across all tasks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of time per iteration relative to the perceptron (or MIRA for the Neural Parser).", "labels": [], "entities": [{"text": "MIRA", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9826329946517944}]}]}