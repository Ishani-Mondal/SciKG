{"title": [{"text": "Extractive Summarization by Maximizing Semantic Volume", "labels": [], "entities": [{"text": "Extractive Summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9263249337673187}, {"text": "Maximizing Semantic Volume", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.898879607518514}]}], "abstractContent": [{"text": "The most successful approaches to extrac-tive text summarization seek to maximize bigram coverage subject to a budget constraint.", "labels": [], "entities": [{"text": "extrac-tive text summarization", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.654706875483195}]}, {"text": "In this work, we propose instead to maximize semantic volume.", "labels": [], "entities": []}, {"text": "We embed each sentence in a semantic space and construct a summary by choosing a subset of sentences whose convex hull maximizes volume in that space.", "labels": [], "entities": []}, {"text": "We provide a greedy algorithm based on the Gram-Schmidt process to efficiently perform volume maximization.", "labels": [], "entities": []}, {"text": "Our method out-performs the state-of-the-art summariza-tion approaches on benchmark datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "In artificial intelligence, changes in representation sometimes suggest new algorithms.", "labels": [], "entities": []}, {"text": "For example, increased attention to distributed meaning representations suggests that existing combinatorial algorithms for NLP might be supplanted by alternatives designed specifically for embeddings.", "labels": [], "entities": []}, {"text": "In this work, we consider summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.9566994905471802}]}, {"text": "Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy, or (b) maximize bigram coverage ().", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.6097268164157867}]}, {"text": "The sentence representation is fundamentally discrete, and a range of greedy, approximate (, and exact optimization algorithms) have been proposed.", "labels": [], "entities": []}, {"text": "Recent studies have explored continuous sentence representations, including the paragraph vector (), a convolutional neural network architecture (, and a dictionary learning approach).", "labels": [], "entities": []}, {"text": "If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document.", "labels": [], "entities": []}, {"text": "We propose that the volume of a summary (i.e., the semantic subspace spanned by the selected sentences) should ideally be large.", "labels": [], "entities": []}, {"text": "We therefore formalize anew objective function for summarization based on semantic volume ( \u00a72), and we provide a fast greedy algorithm that can be used to maximize it ( \u00a73).", "labels": [], "entities": [{"text": "summarization", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9877055287361145}]}, {"text": "We show that our method outperforms competing extractive baselines under similar experimental conditions on benchmark summarization datasets ( \u00a74).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on the TAC-2008 and TAC-2009  datasets. \"Volume\" refers to our method, shown  with two embedding sizes.", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.9211808443069458}, {"text": "TAC-2009  datasets", "start_pos": 38, "end_pos": 56, "type": "DATASET", "confidence": 0.8260685503482819}]}]}