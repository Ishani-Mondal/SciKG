{"title": [{"text": "An Improved Non-monotonic Transition System for Dependency Parsing", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.784478634595871}]}], "abstractContent": [{"text": "Transition-based dependency parsers usually use transition systems that monotoni-cally extend partial parse states until they identify a complete parse tree.", "labels": [], "entities": [{"text": "Transition-based dependency parsers", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6246431370576223}]}, {"text": "(2013) showed that greedy one-best parsing accuracy can be improved by adding additional non-monotonic transitions that permit the parser to \"repair\" earlier parsing mistakes by \"over-writing\" earlier parsing decisions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9187140464782715}]}, {"text": "This increases the size of the set of complete parse trees that each partial parse state can derive, enabling such a parser to escape the \"gar-den paths\" that can trap monotonic greedy transition-based dependency parsers.", "labels": [], "entities": []}, {"text": "We describe anew set of non-monotonic transitions that permits a partial parse state to derive a larger set of completed parse trees than previous work, which allows our parser to escape from a larger set of garden paths.", "labels": [], "entities": []}, {"text": "A parser with our new non-monotonic transition system has 91.85% directed attachment accuracy, an improvement of 0.6% over a comparable parser using the standard monotonic arc-eager transitions .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9090774655342102}]}], "introductionContent": [{"text": "Recent work from and show that neural network models can improve greedy transition-based parsers dramatically, even beyond the 20% error reduction reported by.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 131, "end_pos": 146, "type": "METRIC", "confidence": 0.942899078130722}]}, {"text": "Improvements on beam-search parsing are much more limited, due to the difficulty of applying neural networks to structured prediction.", "labels": [], "entities": [{"text": "beam-search parsing", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.8544266819953918}]}, {"text": "We suggest that the lack of a ready search solution may present the next barrier to further improvements inaccuracy.", "labels": [], "entities": []}, {"text": "Some degree of search flexibility seems inherently necessary, no matter how powerful the local model becomes, as even the human sentence processor can be 'garden pathed' by local structural ambiguities.", "labels": [], "entities": []}, {"text": "We take inspiration from and other psycholinguists and propose repair actions as a light-weight alternative to beamsearch.", "labels": [], "entities": []}, {"text": "Ina transition-based dependency parser, transitions map parse states to parse states, ultimately producing completed parse trees.", "labels": [], "entities": []}, {"text": "This process is non-deterministic, since usually more than one transition can apply to a parse state.", "labels": [], "entities": []}, {"text": "This means that each partial parse state can be associated with a set of complete parse trees (i.e., the complete parses that can be produced by applying sequences of transitions to the partial parse state).", "labels": [], "entities": []}, {"text": "In general adding additional transitions (monotonic or non-monotonic) increases the number of complete parse trees that any given partial parse state can derive.", "labels": [], "entities": []}, {"text": "We explore adding non-monotonic parsing transitions to a greedy arc-eager dependency parser in this paper, in order to permit the parser to recover from attachment errors made early in the parsing process.", "labels": [], "entities": [{"text": "parsing process", "start_pos": 189, "end_pos": 204, "type": "TASK", "confidence": 0.9026423096656799}]}, {"text": "These additional non-monotonic transitions permit the parser to modify what would have been irrevocable parsing decisions in the monotonic arc-eager system when later information justifies this action.", "labels": [], "entities": []}, {"text": "Thus one effect of adding the non-monotonic parsing transitions is to effectively delay the location in the input where the parser must ultimately commit to a particular attachment.", "labels": [], "entities": []}, {"text": "Our transition-system builds on the work of and, who each present modifications to the arc-eager transition system that introduce some non-monotonic behaviour, resulting in small improvements inaccuracy.", "labels": [], "entities": []}, {"text": "However, these systems only apply non-monotonic transitions to a relatively small number of configurations, so they can only have a small impact on parse accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.8410869240760803}]}, {"text": "We introduce a non-monotonic transition system that combines ideas from these two approaches, and allows substantially more repair capability (and hence search flexibility).", "labels": [], "entities": []}, {"text": "We observe a 0.6% improvement inaccuracy on the OntoNotes corpus, which is an error reduction of 6.25% over a competitive baseline.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.9078339040279388}, {"text": "error reduction", "start_pos": 78, "end_pos": 93, "type": "METRIC", "confidence": 0.9733374416828156}]}, {"text": "A parser using our transition system is guaranteed to run in linear time, and the modifications to the algorithm have no negative impact on run-time in our implementation.", "labels": [], "entities": []}, {"text": "Very recently there has been considerable success in applying neural network models to predict which transition to apply in greedy one-best transition-based parsing.", "labels": [], "entities": []}, {"text": "In their preprints, both and report error reductions of around 20-30% for greedy one-best parsing, and much more modest improvements for transition-based parsers with beam search.", "labels": [], "entities": [{"text": "error", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9879167079925537}]}, {"text": "Because the neural network approaches improve the local model that predicts which transition to apply next, while this paper suggests changes to the transition system itself, it is reasonable to expect that the improvements reported here are largely orthogonal to those obtained using the neural network techniques.", "labels": [], "entities": []}, {"text": "In future work we would like to explore integrating such neural network models of transition prediction with the extended transition system proposed here.", "labels": [], "entities": [{"text": "transition prediction", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.7248146086931229}]}], "datasetContent": [{"text": "We implemented a greedy transition-based parser, and used rich contextual features following.", "labels": [], "entities": []}, {"text": "We extended the feature set to include Brown cluster features, using the cluster prefix trick described by.", "labels": [], "entities": []}, {"text": "Brown clusters area standard way to improve the cross-domain performance of supervised linear models.", "labels": [], "entities": []}, {"text": "The use of Brown cluster features accounts for the 0.7% improvement inaccuracy observed between our baseline parser and the Goldberg and Nivre (2012) result shown in.", "labels": [], "entities": []}, {"text": "The two models are otherwise the same.", "labels": [], "entities": []}, {"text": "Part-of-speech tags were predicted using a greedy averaged perceptron model that achieved 97.2% accuracy on the evaluation data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9984244108200073}]}, {"text": "Most previous work uses a n-way jack-knifing to train the stacked tagger/parser model.", "labels": [], "entities": []}, {"text": "For convenience, we instead train the tagger at the same time as the parser, as both allow online learning.", "labels": [], "entities": []}, {"text": "We find this makes no difference to accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9972631931304932}]}, {"text": "Our parsers are trained and evaluated on the same data used by in their recent 'bake-off' of leading dependency parsing models.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.6503934562206268}]}, {"text": "Specifically, we use the OntoNotes corpus converted into dependencies using the ClearNLP 3.1 converter, with the train / dev / test split of the CoNLL 2012 shared task.", "labels": [], "entities": [{"text": "ClearNLP 3.1 converter", "start_pos": 80, "end_pos": 102, "type": "DATASET", "confidence": 0.9234482447306315}, {"text": "CoNLL 2012 shared task", "start_pos": 145, "end_pos": 167, "type": "DATASET", "confidence": 0.9136485904455185}]}], "tableCaptions": [{"text": " Table 2: Our non-monotonic transition system improves", "labels": [], "entities": []}]}