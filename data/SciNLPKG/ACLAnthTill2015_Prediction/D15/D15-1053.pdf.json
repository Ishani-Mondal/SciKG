{"title": [{"text": "Learning a Deep Hybrid Model for Semi-Supervised Text Classification", "labels": [], "entities": [{"text": "Semi-Supervised Text Classification", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.6446416179339091}]}], "abstractContent": [{"text": "We present a novel fine-tuning algorithm in a deep hybrid architecture for semi-supervised text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.6854863166809082}]}, {"text": "During each increment of the online learning process , the fine-tuning algorithm serves as a top-down mechanism for pseudo-jointly modifying model parameters following a bottom-up generative learning pass.", "labels": [], "entities": []}, {"text": "The resulting model, trained under what we call the Bottom-Up-Top-Down learning algorithm , is shown to outperform a variety of competitive models and baselines trained across a wide range of splits between supervised and unsupervised training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent breakthroughs in learning expressive neural architectures have addressed challenging problems in domains such as computer vision, speech recognition, and natural language processing.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.7459978461265564}, {"text": "natural language processing", "start_pos": 161, "end_pos": 188, "type": "TASK", "confidence": 0.6840381622314453}]}, {"text": "This success is owed to the representational power afforded by deeper architectures supported by longstanding theoretical arguments.", "labels": [], "entities": []}, {"text": "These architectures efficiently model complex, highly varying functions via multiple layers of non-linearities, which would otherwise require very \"wide\" shallow models that need large quantities of samples.", "labels": [], "entities": []}, {"text": "However, many of these deeper models have relied on mini-batch training on large-scale, labeled data-sets, either using unsupervised pre-training ( or improved architectural components (such as activation functions).", "labels": [], "entities": []}, {"text": "In an online learning problem, samples are presented to the learning architecture at a given rate (usually with one-time access to these data points), and, as in the case of a web crawling agent, most of these are unlabeled.", "labels": [], "entities": []}, {"text": "Given this, batch training and supervised learning frameworks are no longer applicable.", "labels": [], "entities": []}, {"text": "While incremental approaches such as co-training have been employed to help these models learn in a more update-able fashion, neural architectures can naturally be trained in an online manner through the use of stochastic gradient descent (SGD).", "labels": [], "entities": []}, {"text": "Semi-supervised online learning does not only address practical applications, but it also reflects some challenges of human category acquisition.", "labels": [], "entities": [{"text": "human category acquisition", "start_pos": 118, "end_pos": 144, "type": "TASK", "confidence": 0.6661411921183268}]}, {"text": "Consider the case of a child learning to discriminate between object categories and mapping them to words, given only a small amount of explicitly labeled data (the mother pointing to the object), and a large portion of unsupervised learning, where the child comprehends an adult's speech or experiences positive feedback for his or her own utterances regardless of their correctness.", "labels": [], "entities": []}, {"text": "The original argument in this respect applied to grammar (e.g.,).", "labels": [], "entities": []}, {"text": "While neural networks are not necessarily models of actual cognitive processes, semi-supervised models can show learnability and illustrate possible constraints inherent to the learning process.", "labels": [], "entities": []}, {"text": "The contribution of this paper is the development of the Bottom-Up-Top-Down learning algorithm for training a Stacked Boltzmann Experts Network (SBEN) hybrid architecture.", "labels": [], "entities": []}, {"text": "This procedure combines our proposed top-down fine-tuning procedure for jointly modifying the parameters of a SBEN with a modified form of the model's original layer-wise bottom-up learning pass.", "labels": [], "entities": []}, {"text": "We investigate the performance of the constructed deep model when applied to semi-supervised text classification problems and find that our hybrid architecture outperforms all baselines.", "labels": [], "entities": [{"text": "text classification", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.690596729516983}]}], "datasetContent": [{"text": "We investigate the viability of our deep hybrid architecture for semi-supervised text categorization.", "labels": [], "entities": []}, {"text": "Model performance was evaluated on the WebKB data-set 1 and a small-scale version of the 20News-Group data-set 2 . The original WebKB collection contains pages from a variety of universities (Cornell, Texas, Washington, and Wisconsin as well as miscellaneous pages from others).", "labels": [], "entities": [{"text": "WebKB data-set", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.934226006269455}, {"text": "20News-Group data-set", "start_pos": 89, "end_pos": 110, "type": "DATASET", "confidence": 0.9186641871929169}, {"text": "WebKB collection", "start_pos": 128, "end_pos": 144, "type": "DATASET", "confidence": 0.9337641298770905}]}, {"text": "The 4-class classification problem we defined using this data-set was to determine if a web-page could be identified as one belonging to a Student, Faculty, Course, or a Project, yielding a subset of usable 4,199 samples.", "labels": [], "entities": [{"text": "4-class classification", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.6821508258581161}]}, {"text": "We applied simple pre-processing to the text, namely stop-word removal and stemming, chose to leverage only the k most frequently occurring terms (this varied across the two experiments), and binarized the document low-level representation (only 1 page vector was discarded due to presence of 0 terms).", "labels": [], "entities": []}, {"text": "The 20NewsGroup data-set, on the other hand, contained 16242 total samples and was already pre-processed, containing 100 terms, binary-occurrence low-level representation, with tags for the four top-most highest level domains or meta-topics in the newsgroups array.", "labels": [], "entities": [{"text": "20NewsGroup data-set", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9503947794437408}]}, {"text": "For both data-sets, we evaluated model generalization performance using a stratified 5-fold cross-validation (CV) scheme.", "labels": [], "entities": []}, {"text": "For each possible train/test split, we automatically partitioned the training fold into separate labeled, unlabeled, and validation subsets using stratified random sampling without replacement.", "labels": [], "entities": []}, {"text": "Generalization performance was evaluated by estimating classification error, average precision, average recall, and average F-Measure, where F-Measure was chosen to be the harmonic mean of precision and recall, F 1 = 2(precision \u00b7 recall)/(precision + recall).", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9605939984321594}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9683254361152649}, {"text": "F-Measure", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9957022070884705}, {"text": "F-Measure", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9816873669624329}, {"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.997873067855835}, {"text": "recall", "start_pos": 203, "end_pos": 209, "type": "METRIC", "confidence": 0.986652135848999}, {"text": "precision \u00b7 recall)/(precision + recall)", "start_pos": 219, "end_pos": 259, "type": "METRIC", "confidence": 0.8029890060424805}]}], "tableCaptions": []}