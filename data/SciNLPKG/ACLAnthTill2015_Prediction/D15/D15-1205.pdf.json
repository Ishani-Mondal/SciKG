{"title": [{"text": "Improved Relation Extraction with Feature-Rich Compositional Embedding Models", "labels": [], "entities": [{"text": "Improved Relation Extraction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.857720673084259}]}], "abstractContent": [{"text": "Compositional embedding models build a representation (or embedding) fora linguistic structure based on its component word embeddings.", "labels": [], "entities": []}, {"text": "We propose a Feature-rich Compositional Embedding Model (FCM) for relation extraction that is expressive, generalizes to new domains, and is easy-to-implement.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.8494953215122223}]}, {"text": "The key idea is to combine both (unlexicalized) hand-crafted features with learned word em-beddings.", "labels": [], "entities": []}, {"text": "The model is able to directly tackle the difficulties met by traditional compositional embeddings models, such as handling arbitrary types of sentence annotations and utilizing global information for composition.", "labels": [], "entities": []}, {"text": "We test the proposed model on two relation extraction tasks, and demonstrate that our model outper-forms both previous compositional models and traditional feature rich models on the ACE 2005 relation extraction task, and the SemEval 2010 relation classification task.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7680908143520355}, {"text": "ACE 2005 relation extraction task", "start_pos": 183, "end_pos": 216, "type": "TASK", "confidence": 0.8621618032455445}, {"text": "SemEval 2010 relation classification task", "start_pos": 226, "end_pos": 267, "type": "TASK", "confidence": 0.8062908411026001}]}, {"text": "The combination of our model and a log-linear classifier with hand-crafted features gives state-of-the-art results.", "labels": [], "entities": []}, {"text": "We made our implementation available for general use 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Two common NLP feature types are lexical properties of words and unlexicalized linguistic/structural interactions between words.", "labels": [], "entities": []}, {"text": "Prior work on relation extraction has extensively studied how to design such features by combining discrete lexical properties (e.g. the identity of a word, \u21e4 \u21e4 Gormley and Yu contributed equally.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.9226451814174652}, {"text": "Gormley", "start_pos": 161, "end_pos": 168, "type": "METRIC", "confidence": 0.7205286622047424}]}, {"text": "1 https://github.com/mgormley/pacaya its lemma, its morphological features) with aspects of a word's linguistic context (e.g. whether it lies between two entities or on a dependency path between them).", "labels": [], "entities": []}, {"text": "While these help learning, they make generalization to unseen words difficult.", "labels": [], "entities": [{"text": "generalization to unseen words", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.8671536892652512}]}, {"text": "An alternative approach to capturing lexical information relies on continuous word embeddings 2 as representative of words but generalizable to new words.", "labels": [], "entities": []}, {"text": "Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.8055380284786224}, {"text": "semantic role labeling", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.6911606192588806}, {"text": "relation extraction", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.8622654974460602}]}, {"text": "Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.8316934406757355}]}, {"text": "Our model allows for the composition of embeddings with arbitrary linguistic structure, as expressed by hand crafted features.", "labels": [], "entities": []}, {"text": "In the following sections, we begin with a precise construction of compositional embeddings using word embeddings in conjunction with unlexicalized features.", "labels": [], "entities": []}, {"text": "Various feature sets used in prior work () are cap-", "labels": [], "entities": []}], "datasetContent": [{"text": "Features Our FCM features) use a feature vector f w i over the word w i , the two target entities M 1 , M 2 , and their dependency path.", "labels": [], "entities": []}, {"text": "Here h 1 , h 2 are the indices of the two head words of M 1 , M 2 , \u21e5 refers to the Cartesian product between two sets, t h 1 and t h 2 are entity types (named entity tags for ACE 2005 or WordNet supertags for SemEval 2010) of the head words of two entities, and \ud97b\udf59 stands for the empty feature.", "labels": [], "entities": [{"text": "ACE 2005 or WordNet supertags for SemEval 2010", "start_pos": 176, "end_pos": 222, "type": "DATASET", "confidence": 0.7752371355891228}]}, {"text": "\ud97b\udf59 refers to the conjunction of two elements.", "labels": [], "entities": []}, {"text": "The In-between features indicate whether a word w i is in between two target entities, and the On-path features indicate whether the word is on the dependency path, on which there is a set of words P , between the two entities.", "labels": [], "entities": []}, {"text": "We also use the target entity type as a feature.", "labels": [], "entities": []}, {"text": "Combining this with the basic features results in more powerful compound features, which can help us better distinguish the functions of word embeddings for predicting certain relations.", "labels": [], "entities": []}, {"text": "For example, if we have a person and a vehicle, we know it will be more likely that they have an ART relation.", "labels": [], "entities": []}, {"text": "For the ART relation, we introduce a corresponding weight vector, which is closer to lexical embeddings similar to the embedding of \"drive\".", "labels": [], "entities": []}, {"text": "All linguistic annotations needed for features (POS, chunks 7 , parses) are from Stanford CoreNLP ().", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 81, "end_pos": 97, "type": "DATASET", "confidence": 0.9334100782871246}]}, {"text": "Since SemEval does not have gold entity types we obtained WordNet and named entity tags using.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9568074345588684}]}, {"text": "For all experiments we use 200-d word embeddings trained on the NYT portion of the Gigaword 5.0 corpus (Parker et al., 2011), with word2vec ().", "labels": [], "entities": [{"text": "NYT portion of the Gigaword 5.0 corpus", "start_pos": 64, "end_pos": 102, "type": "DATASET", "confidence": 0.8693345359393528}]}, {"text": "We use the CBOW model with negative sampling (15 negative words).", "labels": [], "entities": []}, {"text": "We set a window size c=5, and remove types occurring less than 5 times.", "labels": [], "entities": []}, {"text": "Models We consider several methods.", "labels": [], "entities": []}, {"text": "(1) FCM in isolation without fine-tuning.", "labels": [], "entities": []}, {"text": "(2) FCM in isolation with fine-tuning (i.e. trained as a log-bilinear model).", "labels": [], "entities": [{"text": "FCM", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.7465595006942749}]}, {"text": "(3) A log-linear model with a rich binary feature set from Sun et al.", "labels": [], "entities": []}, {"text": "(2011) (Baseline)-this consists of all the baseline features of plus several additional carefully-chosen features that have been highly tuned for ACE-style relation extraction over years of research.", "labels": [], "entities": [{"text": "ACE-style relation extraction", "start_pos": 146, "end_pos": 175, "type": "TASK", "confidence": 0.7466225624084473}]}, {"text": "We exclude the Country gazetteer and WordNet features from.", "labels": [], "entities": [{"text": "Country gazetteer", "start_pos": 15, "end_pos": 32, "type": "DATASET", "confidence": 0.9902218580245972}, {"text": "WordNet", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.933164656162262}]}, {"text": "The two remaining methods are hybrid models that integrate FCM as a submodel within the log-linear model ( \u00a7 4).", "labels": [], "entities": []}, {"text": "The feature set of obtained by using the embeddings of heads of two entity mentions (+HeadOnly).", "labels": [], "entities": []}, {"text": "(5) Our full FCM model (+FCM).", "labels": [], "entities": []}, {"text": "All models use L2 regularization tuned on dev data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).", "labels": [], "entities": [{"text": "ACE 2005 out-of-domain test sets", "start_pos": 34, "end_pos": 66, "type": "DATASET", "confidence": 0.9569309473037719}]}, {"text": " Table 4: Comparison of FCM with previously published results for SemEval 2010 Task 8.", "labels": [], "entities": [{"text": "FCM", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.6802730560302734}, {"text": "SemEval 2010 Task 8", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.8170298039913177}]}, {"text": " Table 5: Ablation test of FCM on development set.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9958265423774719}, {"text": "FCM", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.7232726812362671}]}]}