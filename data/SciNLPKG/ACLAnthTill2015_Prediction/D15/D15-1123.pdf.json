{"title": [{"text": "Hierarchical Incremental Adaptation for Statistical Machine Translation", "labels": [], "entities": [{"text": "Hierarchical Incremental Adaptation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6126008033752441}, {"text": "Statistical Machine Translation", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.8520953257878622}]}], "abstractContent": [{"text": "We present an incremental adaptation approach for statistical machine translation that maintains a flexible hierarchical domain structure within a single consistent model.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.7317463258902231}]}, {"text": "Both weights and rules are updated incrementally on a stream of post-edits.", "labels": [], "entities": []}, {"text": "Our multi-level domain hierarchy allows the system to adapt simultaneously towards local context at different levels of granularity, including genres and individual documents.", "labels": [], "entities": []}, {"text": "Our experiments show consistent improvements in translation quality from all components of our approach.", "labels": [], "entities": [{"text": "translation", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.9448000192642212}]}], "introductionContent": [{"text": "Suggestions from a machine translation system can increase the speed and quality of professional human translators.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7381974458694458}, {"text": "speed", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9805849194526672}]}, {"text": "However, querying a single fixed model for all different documents fails to incorporate contextual information that can potentially improve suggestion quality.", "labels": [], "entities": []}, {"text": "We describe a model architecture that adapts simultaneously to multiple genres and individual documents, so that translation suggestions are informed by two levels of contextual information.", "labels": [], "entities": []}, {"text": "Our primary technical contribution is a hierarchical adaptation technique fora post-editing scenario with incremental adaptation, in which users request translations of sentences in corpus order and provide corrected translations of each sentence back to the system (Ortiz-.", "labels": [], "entities": []}, {"text": "Our learning approach resembles Hierarchical Bayesian Domain Adaptation (), but updates both the model weights and translation rules in real time based on these corrected translations ().", "labels": [], "entities": [{"text": "Hierarchical Bayesian Domain Adaptation", "start_pos": 32, "end_pos": 71, "type": "TASK", "confidence": 0.5419368669390678}]}, {"text": "Our adapted system can provide on-demand translations for any genre and document to which it has ever been exposed, using weights and rules for domains associated with each translation request.", "labels": [], "entities": []}, {"text": "Our weight adaptation is performed using a hierarchical extension to fast and adaptive online training (), a technique based on AdaGrad () and forward-backward splitting) that can accurately set weights for both dense and sparse features ().", "labels": [], "entities": [{"text": "weight adaptation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7682689726352692}, {"text": "AdaGrad", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.8849392533302307}]}, {"text": "Rather than adjusting all weights based on each example, our extension adjusts offsets to a fixed baseline system.", "labels": [], "entities": []}, {"text": "In this way, the system can adapt to multiple genres while preventing cross-genre contamination.", "labels": [], "entities": [{"text": "cross-genre contamination", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.7592423558235168}]}, {"text": "In large-scale experiments, we adapt a multigenre baseline system to patents, lectures, and news articles.", "labels": [], "entities": []}, {"text": "Our experiments show that sparse models, hierarchical updates, and rule adaptation all contribute consistent improvements.", "labels": [], "entities": [{"text": "rule adaptation", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.8503114283084869}]}, {"text": "We observe quality gains in all genres, validating our hypothesis that document and genre context are important additional inputs to a machine translation system used for post-editing.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed two sets of German\u2192English experiments; contains the results for both.", "labels": [], "entities": []}, {"text": "Our first set of experiments was performed on the PatTR corpus (.", "labels": [], "entities": [{"text": "PatTR corpus", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.9486784338951111}]}, {"text": "We divided the corpus into training and development data by date and selected 2.4M parallel segments dated before 2000 from the \"claims\" section as bilingual training data, taking equal parts from each of the eight patent types A-H as classified by the Cooperative Patent Classification (CPC).", "labels": [], "entities": []}, {"text": "From each type we further drew separate test sets and a single tune set, selecting documents with at least 10 segments and a maximum of 150 source words per segment, with around 2,100 sentences per test set and 400 sentences per type for the tune set.", "labels": [], "entities": []}, {"text": "The \"claims\" section of this corpus is highly repetitive, which makes it ideal for observing the effects of incremental adaptation techniques.", "labels": [], "entities": []}, {"text": "To train the language and translation model we additionally leveraged all available bilingual and monolingual data provided for the EMNLP 2015 Tenth Workshop on Machine Translation 3 . The total size of the bitext used for rule extraction and feature estimation was 6.4M sentence pairs.", "labels": [], "entities": [{"text": "EMNLP 2015 Tenth Workshop on Machine Translation", "start_pos": 132, "end_pos": 180, "type": "TASK", "confidence": 0.802946252482278}, {"text": "rule extraction", "start_pos": 223, "end_pos": 238, "type": "TASK", "confidence": 0.8189070820808411}, {"text": "feature estimation", "start_pos": 243, "end_pos": 261, "type": "TASK", "confidence": 0.6412197947502136}]}, {"text": "We trained a standard 5-gram language model with modified Kneser-Ney smoothing) using the KenLM toolkit () on 4 billion running words.", "labels": [], "entities": []}, {"text": "The bitext was word-aligned with mgiza, and we used the phrasal decoder () with standard GermanEnglish settings for experimentation.", "labels": [], "entities": []}, {"text": "Our second set of experiments was performed on a mixed-genre corpus containing lectures, patents, and news articles.", "labels": [], "entities": []}, {"text": "The standard dev and test sets of the IWSLT 2014 shared task 4 were used for the lecture genre.", "labels": [], "entities": [{"text": "IWSLT 2014 shared task 4", "start_pos": 38, "end_pos": 62, "type": "DATASET", "confidence": 0.9120566368103027}]}, {"text": "Each document corresponded to an entire lecture.", "labels": [], "entities": []}, {"text": "For the news genre, we used newstest2012 for tuning, newstest2013 for metaparameter optimization, and newstest2014 for testing.", "labels": [], "entities": []}, {"text": "The tune set for the patent genre is identical to the first set of experiments, while the test set consists of the first 300 sentence pairs of each of the patent type specific test sets of the previous experiment.", "labels": [], "entities": []}, {"text": "The documents in the news and patent genres contain around 20 segments on average.", "labels": [], "entities": []}, {"text": "Our evaluation proceeded in multiple stages.", "labels": [], "entities": []}, {"text": "We first trained a set of background weights on the: Results in uncased BBBB.", "labels": [], "entities": [{"text": "BBBB", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.7651893496513367}]}, {"text": "Each component is added on top of the previous line.", "labels": [], "entities": []}, {"text": "All results inline + genre TM and below are statistically significant improvements over the baseline with 95% confidence.", "labels": [], "entities": [{"text": "TM", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9857208728790283}]}, {"text": "We also report the repetition rate of the test corpora as propsed by . concatenated tune sets (baseline).", "labels": [], "entities": [{"text": "repetition rate", "start_pos": 19, "end_pos": 34, "type": "METRIC", "confidence": 0.9860862195491791}]}, {"text": "Keeping these weights fixed, we performed an additional tuning run to estimate genre-level weights (+ genre weights).", "labels": [], "entities": []}, {"text": "In the patent-only setup, we used patent CPC type as genre.", "labels": [], "entities": []}, {"text": "Next, we trained a genrespecific translation model for each genre by first feeding the tune set and then the test set into our incremental adaptation learning method as a continuous stream of simulated post edits (+ genre TM).", "labels": [], "entities": []}, {"text": "After each sentence, we performed an update on the genre-specific weights.", "labels": [], "entities": []}, {"text": "In separate experiments, we also included document-level weights as an additional domain (+ doc.", "labels": [], "entities": []}, {"text": "weights) and included sparse features at the document level (+ sparse features).", "labels": [], "entities": []}, {"text": "6 demonstrates that each component of this approach offered consistent incremental quality gains, but with varying magnitudes.", "labels": [], "entities": []}, {"text": "For the patent experiments we report the average over our eight test sets (A-H) due to lack of space, but total improvement varied from +4.92 to +6.46 BBBB.", "labels": [], "entities": [{"text": "BBBB", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9941669702529907}]}, {"text": "In the mixed-genre experiments, BBBB increased by +2.27 on lectures, +0.97 on news, and +5.33 on patents.", "labels": [], "entities": [{"text": "BBBB", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9882417321205139}]}, {"text": "On all tasks, we observed statistically significant improvements over the baseline (95% confidence level) in the + genre TM, + doc.", "labels": [], "entities": []}, {"text": "weights and + sparse features experiments using bootstrap resampling).", "labels": [], "entities": []}, {"text": "These results demonstrate the efficacy of hierarchical incremental adaptation, although we would like to stress that the patent data was selected specifically for its high level of repetitiveness, and the  large improvement in this genre would only be expected to arise in similarly structured domains.", "labels": [], "entities": [{"text": "hierarchical incremental adaptation", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.7072327931722006}]}, {"text": "This property is quantified by the repetition rate measure (RR) ( ) reported in, which confirms the finding by that RR correlates with the effectiveness of adaptation.", "labels": [], "entities": [{"text": "repetition rate measure (RR)", "start_pos": 35, "end_pos": 63, "type": "METRIC", "confidence": 0.9828068514664968}]}, {"text": "shows BBBB score differences to the baseline + genre weights system for different subsets of the news and patent test sets.", "labels": [], "entities": [{"text": "BBBB score", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9342449903488159}, {"text": "news and patent test sets", "start_pos": 97, "end_pos": 122, "type": "DATASET", "confidence": 0.7079654514789582}]}, {"text": "Each point is computed by document slicing, i.e. on a single segment from each document.", "labels": [], "entities": []}, {"text": "The rightmost data point is the BBBB score we obtain by evaluating on the 20th segment of each document, grouped into a pseudo-corpus.", "labels": [], "entities": [{"text": "BBBB score", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9232289791107178}]}, {"text": "Note that this group does not correspond to any number in, which reports BBBB on the entire test sets.", "labels": [], "entities": [{"text": "BBBB", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9976882934570312}]}, {"text": "Thus, we evaluate on all sentences that have learned from exactly (i\u22121) segments of the same document, with i = 1, . .", "labels": [], "entities": []}, {"text": "Although the graph is naturally very noisy (each score is computed on roughly 150 segments), we can clearly see that incremental adaptation learns on the document level: on average, the improvement over the baseline increases when proceeding further into the document.", "labels": [], "entities": []}, {"text": "In our real-time computerassisted translation scenario, a certain translation speed is required to allow for responsive user interaction.: Decoding speed on the lecture data.", "labels": [], "entities": []}, {"text": "ing run and weight updates.", "labels": [], "entities": [{"text": "run", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9957660436630249}]}, {"text": "Sparse features slows the system down further by a factor of 2.4.", "labels": [], "entities": []}, {"text": "However, the largest part of the computation time incurs only when the user has finalized collaborative translation of one sentence and is busy reading the next source sentence.", "labels": [], "entities": []}, {"text": "Further, the speed/quality tradeoff can be adjusted with pruning parameters.", "labels": [], "entities": [{"text": "speed", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9791698455810547}]}], "tableCaptions": [{"text": " Table 1: Results in uncased BBBB", "labels": [], "entities": [{"text": "BBBB", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.7670795917510986}]}, {"text": " Table 2: Decoding speed on the lecture data.", "labels": [], "entities": [{"text": "Decoding speed", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.5421496629714966}, {"text": "lecture data", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.731949046254158}]}]}