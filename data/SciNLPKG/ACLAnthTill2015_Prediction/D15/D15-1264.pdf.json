{"title": [{"text": "Closing the Gap: Domain Adaptation from Explicit to Implicit Discourse Relations", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7124012559652328}]}], "abstractContent": [{"text": "Many discourse relations are explicitly marked with discourse connectives, and these examples could potentially serve as a plentiful source of training data for recognizing implicit discourse relations.", "labels": [], "entities": []}, {"text": "However , there are important linguistic differences between explicit and implicit discourse relations, which limit the accuracy of such an approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9950436353683472}]}, {"text": "We account for these differences by applying techniques from domain adaptation, treating implicitly and explicitly-marked discourse relations as separate domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7439510822296143}]}, {"text": "The distribution of surface features varies across these two domains, so we apply a marginalized denoising autoencoder to induce a dense, domain-general representation.", "labels": [], "entities": []}, {"text": "The label distribution is also domain-specific, so we apply a resampling technique that is similar to instance weighting.", "labels": [], "entities": []}, {"text": "In combination with a set of automatically-labeled data, these improvements eliminate more than 80% of the transfer loss incurred by training an implicit discourse relation classifier on explicitly-marked discourse relations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse relations reveal the structural organization of text, potentially supporting applications such as summarization (), sentiment analysis, and coherence evaluation ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.9822003841400146}, {"text": "sentiment analysis", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.9662328362464905}, {"text": "coherence evaluation", "start_pos": 150, "end_pos": 170, "type": "TASK", "confidence": 0.6756758242845535}]}, {"text": "While some relations are signaled explicitly with connectives such as however (), many more are implicit.", "labels": [], "entities": []}, {"text": "Expert-annotated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives (.", "labels": [], "entities": []}, {"text": "However, show that models trained on explicitly marked examples generalize poorly to implicit relation identification.", "labels": [], "entities": [{"text": "relation identification", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.7400715202093124}]}, {"text": "They argued that explicit and implicit examples maybe linguistically dissimilar, as writers tend to avoid discourse connectives if the discourse relation could be inferred from context.", "labels": [], "entities": []}, {"text": "Similar observations are made by, who attempt to add automatically-labeled instances to improve supervised classification of implicit discourse relations.", "labels": [], "entities": []}, {"text": "In this paper, we approach this problem from the perspective of domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7133610546588898}]}, {"text": "Specifically, we argue that the reason that automatically-labeled examples generalize poorly is due to domain mismatch from the explicit relations (source domain) to the implicit relations (target domain).", "labels": [], "entities": []}, {"text": "We propose to close the gap by using two simple methods from domain adaptation: (1) feature representation learning: mapping the source domain and target domain to a shared latent feature space; (2) resampling: modifying the relation distribution in the explicit relations to match the distribution over implicit relations.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7585994899272919}, {"text": "feature representation learning", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.7899129192034403}]}, {"text": "Our results on the Penn Discourse Treebank ( show that these two methods improve the performance on unsupervised discourse relation identification by more than 8.4% on average F 1 score across all relation types, an 82% reduction on the transfer loss incurred by training on explicitly-marked discourse relations.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 19, "end_pos": 42, "type": "DATASET", "confidence": 0.9817875623703003}, {"text": "discourse relation identification", "start_pos": 113, "end_pos": 146, "type": "TASK", "confidence": 0.6583915650844574}, {"text": "F 1 score", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.9835349321365356}]}], "datasetContent": [{"text": "Our experiments test the utility of the two domain adaptation methods, using the Penn Discourse Treebank () and some extra-training data collected from a external resource.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 81, "end_pos": 104, "type": "DATASET", "confidence": 0.958097775777181}]}, {"text": "Datasets The test examples are implicit relation instances from section 21-22 in the PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.9799613356590271}]}, {"text": "For the domain adaptation setting, the training set consists of the explicitly-marked examples extracted from sections 02-20 and 23-24, and the development set consists of the explicit relations from sections 21-22.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7708130776882172}]}, {"text": "All relations in the explicit examples are automatically labeled by using the connective-to-relation mapping from in, where we only keep the majority relation type for every connective.", "labels": [], "entities": []}, {"text": "For each identified connective, we use its annotated arguments in the PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.9649378657341003}]}, {"text": "As an upper bound, we also train a supervised discourse relation classifier, using the implicit examples in sections 02-20 and 23-24 as the training set, and using sections 00-01 as the development set.", "labels": [], "entities": []}, {"text": "Following prior work (, we consider the firstlevel discourse relations in the PDTB -Temporal (TEMP.), Comparison (COMP.), Expansion (EXP.) and Contingency (CONT.).", "labels": [], "entities": []}, {"text": "We train binary classifiers and report F 1 score on each binary classification task.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9629337986310323}]}, {"text": "Extension of this approach to multi-class classification is important, but since this is not the setting considered inmost of the prior research, we leave it for future work.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.7904466688632965}]}, {"text": "The true power of learning from automatically labeled examples is that we could leverage much larger datasets than hand-annotated corpora such as the Penn Discourse Treebank.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 150, "end_pos": 173, "type": "DATASET", "confidence": 0.986485222975413}]}, {"text": "To test this idea, we collected 1,000 news articles from CNN.com as extra training data.", "labels": [], "entities": [{"text": "CNN.com", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9354958534240723}]}, {"text": "Explicitly-marked discourse relations from this data are automatically extracted by matching the PDTB discourse connectives (.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.8927279114723206}]}, {"text": "For this data, we also need to extract the arguments of the identified connectives: for every identified connective, the sentence following this connective is labeled as Arg2 and the preceding sentence is labeled as Arg1, as suggested by.", "labels": [], "entities": [{"text": "Arg2", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.8987977504730225}, {"text": "Arg1", "start_pos": 216, "end_pos": 220, "type": "METRIC", "confidence": 0.9653991460800171}]}, {"text": "Ina pilot study we found that larger amounts of additional training data yielded no further improvements, which is consistent with the recent results of.", "labels": [], "entities": []}, {"text": "Model selection We use a linear support vector machine as the classification model.", "labels": [], "entities": [{"text": "Model selection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7563004195690155}]}, {"text": "Our model includes five tunable parameters: the number of pivot features \u03ba, the size of the feature subset K, the noise level for the denoising autoencoder q, the cosine similarity threshold for resampling \u03c4 , and the penalty parameter C for the SVM classifier.", "labels": [], "entities": []}, {"text": "We consider \u03ba \u2208 {1000, 2000, 3000} for pivot features and C \u2208 {0.001, 0.01, 0.1, 1.0, 10.0} for penalty parameters, q \u2208 {0.90, 0.95, 0.99} for noise levels.", "labels": [], "entities": []}, {"text": "To reduce the free parameters, we set K = 5\u03ba and simply fix the cosine similarity threshold \u03c4 =).", "labels": [], "entities": [{"text": "cosine similarity threshold \u03c4", "start_pos": 64, "end_pos": 93, "type": "METRIC", "confidence": 0.7567240744829178}]}, {"text": "We re-implement these features as closely as possible to the cited works, using the Stanford CoreNLP Toolkit to obtain syntactic annotations ().", "labels": [], "entities": [{"text": "Stanford CoreNLP Toolkit", "start_pos": 84, "end_pos": 108, "type": "DATASET", "confidence": 0.9334954420725504}]}, {"text": "In experiments, we start with surface feature representations as baselines, then incorporate the two domain adaptation techniques incrementally.", "labels": [], "entities": []}, {"text": "As shown inline 2 of, the performance is poor if directly applying a model trained on the explicit examples with the FULL feature set, which is consistent with the observations of: there is a 10.28% absolute reduction on average F 1 score from the upper bound obtained with in-domain supervision (line 1).", "labels": [], "entities": [{"text": "FULL feature set", "start_pos": 117, "end_pos": 133, "type": "DATASET", "confidence": 0.7001817027727762}, {"text": "F 1 score", "start_pos": 229, "end_pos": 238, "type": "METRIC", "confidence": 0.9848906993865967}]}, {"text": "With mDA, the overall performance increases by 0.86% (line 4); resampling gives a further 4.16% improvement mainly because of the performance gain on the EXP.", "labels": [], "entities": [{"text": "mDA", "start_pos": 5, "end_pos": 8, "type": "DATASET", "confidence": 0.7644945979118347}, {"text": "EXP", "start_pos": 154, "end_pos": 157, "type": "DATASET", "confidence": 0.9265664219856262}]}, {"text": "relation (line 5).", "labels": [], "entities": []}, {"text": "The resampling method itself (line 3) also gives a better overall performance then mDA (line 4).", "labels": [], "entities": []}, {"text": "However, the F 1 scores on the TEMP. and CONT. are even worse than the baseline (line 2).", "labels": [], "entities": [{"text": "F 1", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9941782057285309}, {"text": "TEMP.", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.6271253228187561}, {"text": "CONT.", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.8414272665977478}]}, {"text": "Surface representations with the FULL feature set were found to cause serious overfitting in the experiments.", "labels": [], "entities": [{"text": "FULL feature set", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.7426002621650696}]}, {"text": "To deal with this problem, we propose to use only \u03ba pivot features, which gives a stronger baseline of the cross-domain relation identification, as shown inline 6.", "labels": [], "entities": [{"text": "cross-domain relation identification", "start_pos": 107, "end_pos": 143, "type": "TASK", "confidence": 0.6083838641643524}]}, {"text": "Then, by incorporating resampling and feature representation learning individually, the average F 1 increases from 32.74% to 35.44% (line 7) and 36.69% (line 8) respectively.", "labels": [], "entities": [{"text": "feature representation learning", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.7480670015017191}, {"text": "F 1", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9808241426944733}]}, {"text": "The combination of these two domain adaptation techniques boosts the average F 1 further to 38.62% (line 9).", "labels": [], "entities": [{"text": "F 1", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9610738754272461}]}, {"text": "The additional CNN training data further improves performance to 39.46% (line 10).", "labels": [], "entities": [{"text": "CNN training data", "start_pos": 15, "end_pos": 32, "type": "DATASET", "confidence": 0.8218820293744405}]}, {"text": "This represents an 8.42% improvement of average F 1 from the original result (line 2), for more than 80% reduction on the transfer loss incurred by training on explicit discourse relations.", "labels": [], "entities": [{"text": "F 1", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.988336592912674}]}, {"text": "An additional experiment is to use automatic argument extraction in both the PDTB and the CNN data, which would correspond to more truly unsupervised domain adaptation.", "labels": [], "entities": [{"text": "argument extraction", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7472139000892639}, {"text": "PDTB", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.9529514908790588}, {"text": "CNN data", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.9410018622875214}]}, {"text": "(Recall that in the CNN data, we used adjacent sentences as argument spans, while in the PDTB data, we use expert annotations.)", "labels": [], "entities": [{"text": "CNN data", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.9456459879875183}, {"text": "PDTB data", "start_pos": 89, "end_pos": 98, "type": "DATASET", "confidence": 0.9703431725502014}]}, {"text": "When using adjacent sentences as argument spans in both datasets, the average F 1 is 38.52% for the combination of representation learning and resampling.", "labels": [], "entities": [{"text": "F 1", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9959128797054291}]}, {"text": "Compared to line 10, this is a 0.94% performance drop, indicating the importance of argument identification in the PDTB data.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.7544555366039276}, {"text": "PDTB data", "start_pos": 115, "end_pos": 124, "type": "DATASET", "confidence": 0.9495061635971069}]}, {"text": "In future work we may consider better heuristics for argument extraction, such as obtaining automatically-labeled examples only from those connectors for whom the arguments usually are the adjacent sentences; for example, the connector nonetheless usually connects adjacent spans (e.g., Bob was hungry.", "labels": [], "entities": [{"text": "argument extraction", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7267644852399826}]}, {"text": "Nonetheless he gave Tina the burger.), while the connector even though may connect two spans that follow the connector in the same sentence (e.g., Even though Bob was hungry, he gave Tina the burger.).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of cross-domain learning for implicit discourse relation identification.", "labels": [], "entities": [{"text": "implicit discourse relation identification", "start_pos": 51, "end_pos": 93, "type": "TASK", "confidence": 0.6375425755977631}]}]}