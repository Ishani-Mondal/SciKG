{"title": [], "abstractContent": [{"text": "We present a novel approach for unsu-pervised induction of a Reordering Grammar using a modified form of permutation trees (Zhang and Gildea, 2007), which we apply to preordering in phrase-based machine translation.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 182, "end_pos": 214, "type": "TASK", "confidence": 0.6133137047290802}]}, {"text": "Unlike previous approaches , we induce in one step both the hierarchical structure and the transduction function over it from word-aligned parallel corpora.", "labels": [], "entities": []}, {"text": "Furthermore, our model (1) handles non-ITG reordering patterns (up to 5-ary branching), (2) is learned from all derivations by treating not only labeling but also bracketing as latent variable, (3) is entirely unlexicalized at the level of reordering rules, and (4) requires no linguistic annotation.", "labels": [], "entities": []}, {"text": "Our model is evaluated both for accuracy in predicting target order, and for its impact on translation quality.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9963789582252502}, {"text": "translation", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.9553895592689514}]}, {"text": "We report significant performance gains over phrase reordering , and over two known preordering baselines for English-Japanese.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7872474789619446}]}], "introductionContent": [{"text": "Preordering () aims at permuting the words of a source sentence s into anew order\u00b4sorder\u00b4order\u00b4s, hopefully close to a plausible target word order.", "labels": [], "entities": []}, {"text": "Preordering is often used to bridge long distance reorderings (e.g., in Japanese-or GermanEnglish), before applying phrase-based models (.", "labels": [], "entities": [{"text": "Preordering", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.759093701839447}]}, {"text": "Preordering is often broken down into two steps: finding a suitable tree structure, and then finding a transduction function over it.", "labels": [], "entities": []}, {"text": "A common approach is to use monolingual syntactic trees and focus on finding a transduction function of the sibling subtrees under the nodes ().", "labels": [], "entities": []}, {"text": "The (direct correspondence) assumption underlying this approach is that permuting the siblings of nodes in a source syntactic tree can produce a plausible target order.", "labels": [], "entities": []}, {"text": "An alternative approach creates reordering rules manually and then learns the right structure for applying these rules).", "labels": [], "entities": []}, {"text": "Others attempt learning the transduction structure and the transduction function in two separate, consecutive steps).", "labels": [], "entities": []}, {"text": "Here we address the challenge of learning both the trees and the transduction functions jointly, in one fell swoop, from word-aligned parallel corpora.", "labels": [], "entities": []}, {"text": "Learning both trees and transductions jointly raises two questions.", "labels": [], "entities": []}, {"text": "How to obtain suitable trees for the source sentence and how to learn a distribution over random variables specifically aimed at reordering in a hierarchical model?", "labels": [], "entities": []}, {"text": "In this work we solve both challenges by using the factorizations of permutations into Permutation Trees (PETs) (.", "labels": [], "entities": []}, {"text": "As we explain next, PETs can be crucial for exposing the hierarchical reordering patterns found in wordalignments.", "labels": [], "entities": []}, {"text": "We obtain permutations in the training data by segmenting every word-aligned source-target pair into minimal phrase pairs; the resulting alignment between minimal phrases is written as a permutation (1:1 and onto) on the source side.", "labels": [], "entities": []}, {"text": "Every permutation can be factorized into a forest of PETs (over the source sentences) which we use as a latent treebank for training a Probabilistic ContextFree Grammar (PCFG) tailor made for preordering as we explain next.", "labels": [], "entities": []}, {"text": "shows two alternative PETs for the same permutation over minimal phrases.", "labels": [], "entities": []}, {"text": "The nodes have labels (like P 3142) which stand for local permutations (called prime permutation) over the child nodes; for example, the root label P 3142 stands for prime permutation 3, 1, 4, 2, which says that the first child of the root becomes 3 rd on the target side, the second becomes 1 st , the third becomes 4 th and the fourth becomes 2 nd . The prime permutations are non-factorizable permutations like 1, 2, 2, 1 and 2, 4, 1, 3.", "labels": [], "entities": []}, {"text": "We think PETs are suitable for learning preordering for two reasons.", "labels": [], "entities": []}, {"text": "Firstly, PETs specify exactly the phrase pairs defined by the permutation.", "labels": [], "entities": []}, {"text": "Secondly, every permutation is factorizable into prime permutations only).", "labels": [], "entities": []}, {"text": "Therefore, PETs expose maximal sharing between different permutations in terms of both phrases and their reordering.", "labels": [], "entities": []}, {"text": "We expect this to be advantageous for learning hierarchical reordering.", "labels": [], "entities": []}, {"text": "For learning preordering, we first extract an initial PCFG from the latent treebank of PETs over the source sentences only.", "labels": [], "entities": []}, {"text": "We initialize the nonterminal set of this PCFG to the prime permutations decorating the PET nodes.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.944682240486145}]}, {"text": "Subsequently we split these coarse labels in the same way as latent variable splitting is learned for treebank parsing).", "labels": [], "entities": [{"text": "latent variable splitting", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.6779899597167969}]}, {"text": "Unlike treebank parsing, however, our training treebank is latent because it consists of a whole forest of PETs per training instance (s).", "labels": [], "entities": [{"text": "treebank parsing", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.617866575717926}]}, {"text": "Learning the splits on a latent treebank of PETs results in a Reordering PCFG which we use to parse input source sentences into split-decorated trees, i.e., the labels are the splits of prime permutations.", "labels": [], "entities": [{"text": "parse input source sentences into split-decorated trees", "start_pos": 94, "end_pos": 149, "type": "TASK", "confidence": 0.8192778485161918}]}, {"text": "After parsing s, we map the splits back on their initial prime permutations, and then retrieve a reordered version\u00b4sversion\u00b4version\u00b4s of s.", "labels": [], "entities": []}, {"text": "In this sense, our latent splits are dedicated to reordering.", "labels": [], "entities": []}, {"text": "We face two technical difficulties alien to work on latent PCFGs in treebank parsing.", "labels": [], "entities": [{"text": "treebank parsing", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.5796469449996948}]}, {"text": "Firstly, as mentioned above, permutations may factorize into more than one PET (a forest) leading to a latent training treebank.", "labels": [], "entities": []}, {"text": "1 And secondly, after we parse a source string s, we are interested in\u00b4sin\u00b4in\u00b4s, the permuted version of s, not in the best derivation/PET.", "labels": [], "entities": []}, {"text": "Exact computation is a known NP-Complete problem).", "labels": [], "entities": []}, {"text": "We solve this by anew Minimum-Bayes Risk decoding approach using Kendall reordering score as loss function, which is an efficient measure over permutations (.", "labels": [], "entities": [{"text": "Minimum-Bayes Risk decoding", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.6268084049224854}]}, {"text": "In summary, this paper contributes: \u2022 A novel latent hierarchical source reordering model working overall derivations of PETs 1 All PETs for the same permutation share the same set of prime permutations but differ only in bracketing structure ().", "labels": [], "entities": []}, {"text": "\u2022 A label splitting approach based on PCFGs over minimal phrases as terminals, learned from an ambiguous treebank, where the label splits start out from prime permutations.", "labels": [], "entities": [{"text": "label splitting", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.767638236284256}]}, {"text": "\u2022 A fast Minimum Bayes Risk decoding over Kendall \u03c4 reordering score for selecting\u00b4sselecting\u00b4selecting\u00b4s.", "labels": [], "entities": [{"text": "Minimum Bayes Risk", "start_pos": 9, "end_pos": 27, "type": "METRIC", "confidence": 0.8834976355234782}]}, {"text": "We report results for extensive experiments on English-Japanese showing that our Reordering PCFG gives substantial improvements when used as preordering for phrase-based models, outperforming two existing baselines for this task.", "labels": [], "entities": [{"text": "Reordering PCFG", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.6173242479562759}]}], "datasetContent": [{"text": "We conduct experiments with three baselines: \u2022 Baseline A: No preordering.", "labels": [], "entities": []}, {"text": "\u2022 Baseline B: Rule based preordering (Isozaki et al., 2010b), which first obtains an HPSG parse tree using Enju parser and after that swaps the children by moving the syntactic head to the final position to account for different head orientation in English and Japanese.", "labels": [], "entities": [{"text": "HPSG parse tree", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.7897080183029175}]}, {"text": "And we test four variants of our model: \u2022 RG left -only canonical left branching PET \u2022 RG right -only canonical right branching PET \u2022 RG ITG-forest -all PETs that are binary (ITG) \u2022 RG PET-forest -all PETs.", "labels": [], "entities": []}, {"text": "We test these models on English-Japanese NTCIR-8 Patent Translation (PATMT) Task.", "labels": [], "entities": [{"text": "NTCIR-8 Patent Translation (PATMT)", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.7280952533086141}]}, {"text": "For tuning we use all NTCIR-7 dev sets and for testing the test set from NTCIR-9 from both directions.", "labels": [], "entities": [{"text": "NTCIR-7 dev sets", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.8894759217898051}, {"text": "NTCIR-9", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9726029634475708}]}, {"text": "All used data was tokenized (English with Moses tokenizer and Japanese with KyTea 5 ) and filtered for sentences between 4 and 50 words.", "labels": [], "entities": []}, {"text": "A subset of this data is used for training the Reordering Grammar, obtained by filtering out sentences that have prime permutations of arity > 5, and for the ITG version arity > 2.", "labels": [], "entities": [{"text": "Reordering Grammar", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.8454374372959137}]}, {"text": "Baseline C was trained on 600 sentences because training is prohibitively slow.", "labels": [], "entities": [{"text": "Baseline C", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8474902510643005}]}, {"text": "The Reordering Grammar was trained for 10 iterations of EM on train RG data.", "labels": [], "entities": [{"text": "RG data", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.6988701522350311}]}, {"text": "We use 30 splits for binary non-terminals and 3 for non-binary.", "labels": [], "entities": []}, {"text": "Training on this dataset takes 2 days and parsing tuning and testing set without any pruning takes 11 and 18 hours respectively.", "labels": [], "entities": [{"text": "parsing tuning", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.7912951409816742}]}, {"text": "We test how well our model predicts gold reorderings before translation by training the alignment model using MGIZA++ 6 on the training corpus and using it to align the test corpus.", "labels": [], "entities": []}, {"text": "Gold reorderings for the test corpus are obtained by sorting words by their average target position and (unaligned words follow their right neighboring word).", "labels": [], "entities": []}, {"text": "We use Kendall \u03c4 score for evaluation (note the difference with Section 3.3 where we defined it as a loss function).", "labels": [], "entities": [{"text": "Kendall \u03c4 score", "start_pos": 7, "end_pos": 22, "type": "METRIC", "confidence": 0.7686927119890848}]}, {"text": "shows that our models outperform all baselines on this task.", "labels": [], "entities": []}, {"text": "The only strange result here is that rule-based preordering obtains a lower score than no preordering, which might bean artifact of the Enju parser changing the tokenization of its input, so the Kendall \u03c4 of this system might not really reflect the real quality of the preordering.", "labels": [], "entities": []}, {"text": "All other systems use the same tokenization.", "labels": [], "entities": []}, {"text": "The reordered output of all the mentioned baselines and versions of our model are translated with phrase-based MT system () (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 \u00b4 s \u2212 t.", "labels": [], "entities": []}, {"text": "The only exception is Baseline A which is trained on original s \u2212 t.", "labels": [], "entities": []}, {"text": "We use a 5-gram language model trained with KenLM 8 , tune 3 times with kb-mira to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (), METEOR) and TER ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 187, "end_pos": 191, "type": "METRIC", "confidence": 0.9984428286552429}, {"text": "METEOR", "start_pos": 196, "end_pos": 202, "type": "METRIC", "confidence": 0.9813663959503174}, {"text": "TER", "start_pos": 208, "end_pos": 211, "type": "METRIC", "confidence": 0.9965565204620361}]}, {"text": "We additionally report RIBES score () that concentrates on word order more than other metrics.", "labels": [], "entities": [{"text": "RIBES score", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.9756835997104645}]}, {"text": "In we see that using all PETs during training makes a big impact on performance.", "labels": [], "entities": []}, {"text": "Only the all PETs variants Earlier work on preordering applies the preordering model to the training data to obtain a parallel corpus of guessed\u00b4sguessed\u00b4guessed\u00b4s \u2212 t pairs, which are the word re-aligned and then used for training the back-end MT system (: Comparison of different preordering models.", "labels": [], "entities": [{"text": "MT", "start_pos": 245, "end_pos": 247, "type": "TASK", "confidence": 0.9472734928131104}]}, {"text": "Superscripts A, B and C signify if the system is significantly better (p < 0.05) than the respective baseline or significantly worse (in which case it is a subscript).", "labels": [], "entities": []}, {"text": "Significance tests were not computed for RIBES.", "labels": [], "entities": [{"text": "Significance", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9670207500457764}, {"text": "RIBES", "start_pos": 41, "end_pos": 46, "type": "TASK", "confidence": 0.4479803740978241}]}, {"text": "Score is bold if the system is significantly better than all the baselines.", "labels": [], "entities": []}, {"text": "(RG ITG-forest and RG PET-forest ) significantly outperform all baselines.", "labels": [], "entities": []}, {"text": "If we are to choose a single PET per training instance, then learning RG from only left-branching PETs (the one usually chosen in other work, e.g. () performs slightly worse than the right-branching PET.", "labels": [], "entities": []}, {"text": "This is possibly because English is mostly rightbranching.", "labels": [], "entities": []}, {"text": "So even though both PETs describe the same reordering, RG right captures reordering over English input better than RG left . All PETs or binary only?", "labels": [], "entities": []}, {"text": "RG PET-forest performs significantly better than RG ITG-forest (p < 0.05).", "labels": [], "entities": []}, {"text": "Non-ITG reordering operators are predicted rarely (in only 99 sentences of the test set), but they make a difference, because these operators often appear high in the predicted PET.", "labels": [], "entities": []}, {"text": "Furthermore, having these operators during training might allow for better fit to the data.", "labels": [], "entities": []}, {"text": "How much reordering is resolved by the Reordering Grammar?", "labels": [], "entities": []}, {"text": "Obviously, completely factorizing out the reordering from the translation process is impossible because reordering depends to a certain degree on target lexical choice.", "labels": [], "entities": []}, {"text": "To quantify the contribution of Reordering Grammar, we tested decoding with different distortion limit values in the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 117, "end_pos": 120, "type": "TASK", "confidence": 0.9804391264915466}]}, {"text": "We compare the phrase-based (PB) system with distance based cost function for reordering () with and without preordering.", "labels": [], "entities": []}, {"text": "shows that Reordering Grammar gives substantial performance improvements at all distortion limits (both BLEU and RIBES).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9991593360900879}, {"text": "RIBES", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.9896606206893921}]}, {"text": "RG PET-forest is less sensitive to changes in decoder distortion limit than standard PBSMT.", "labels": [], "entities": []}, {"text": "The perfor-: Distortion effect on BLEU and RIBES mance of RG PET-forest varies only by 1.1 BLEU points while standard PBSMT by 4.3 BLEU points.", "labels": [], "entities": [{"text": "Distortion", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9754362106323242}, {"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9962369203567505}, {"text": "RIBES", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9457271099090576}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9973583817481995}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9961912631988525}]}, {"text": "Some local reordering in the decoder seems to help RG PET-forest but large distortion limits seem to degrade the preordering choice.", "labels": [], "entities": []}, {"text": "This shows also that the improved performance of RG PET-forest is not only a result of efficiently exploring the full space of permutations, but also a result of improved scoring of permutations.", "labels": [], "entities": []}, {"text": "Does the improvement remain fora decoder with MSD reordering model?", "labels": [], "entities": []}, {"text": "We compare the RG PET-forest preordered model against a decoder that uses the strong MSD model).", "labels": [], "entities": []}, {"text": "shows that using Reordering Grammar as front-end to MSD reordering (full Moses) improves performance by 2.8 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9992634654045105}]}, {"text": "The improvement is confirmed by METEOR, TER and RIBES.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9489248991012573}, {"text": "TER", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9896541237831116}, {"text": "RIBES", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.9243978261947632}]}, {"text": "Our preordering model and MSD are complementary -the Reordering Grammar captures long distance reordering, while MSD possibly does better local reorderings, especially reorderings conditioned on the lexical part of translation units.", "labels": [], "entities": []}, {"text": "Interestingly, the MSD model (BLEU 29.6) improves over distance-based reordering (BLEU 27.8) by (BLEU 1.8), whereas the difference between these systems as back-ends to Reordering Grammar (respectively BLEU 32.4 and 32.0) is far smaller (0.4 BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9730373024940491}, {"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9318848252296448}, {"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9924192428588867}, {"text": "BLEU", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.9318214654922485}, {"text": "BLEU", "start_pos": 242, "end_pos": 246, "type": "METRIC", "confidence": 0.9970694184303284}]}, {"text": "This suggests that a major share of reorderings can be handled well by preordering without conditioning on target lexical choice.", "labels": [], "entities": []}, {"text": "Furthermore, this shows that RG PET-forest preordering is not very sensitive to the decoder's reordering model.", "labels": [], "entities": []}, {"text": "Comparison to a Hierarchical model (Hiero).", "labels": [], "entities": []}, {"text": "Hierarchical preordering is not intended fora hierarchical model as Hiero).", "labels": [], "entities": []}, {"text": "Yet, here we compare our preordering system (PB MSD+RG) to Hiero for completeness, while we should keep in mind that Hiero's reordering model has access to much richer training data.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.8642697334289551}]}, {"text": "We will discuss these differences shortly.", "labels": [], "entities": []}, {"text": "shows that the difference in BLEU is not statistically significant, but there is more difference in METEOR and TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9994018077850342}, {"text": "METEOR", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9907463788986206}, {"text": "TER", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9934338331222534}]}, {"text": "RIBES, which concentrates more on reordering, prefers Reordering Grammar over Hiero.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9681346416473389}]}, {"text": "It is somewhat surprising that a preordering model combined with a phrase-based model succeeds to rival Hiero's performance on English-Japanese.", "labels": [], "entities": []}, {"text": "Especially when looking at the differences between the two: 1.", "labels": [], "entities": []}, {"text": "Reordering Grammar uses only minimal phrases, while Hiero uses composite (longer) phrases which encapsulate internal reorderings, but also non-contiguous phrases.", "labels": [], "entities": [{"text": "Reordering Grammar", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.5177520215511322}, {"text": "Hiero", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.8316609263420105}]}, {"text": "2. Hiero conditions its reordering on the lexical target side, whereas the Reordering Grammar does not (by definition).", "labels": [], "entities": []}, {"text": "3. Hiero uses a range of features, e.g., a language model, while Reordering Grammar is a mere generative PCFG.", "labels": [], "entities": []}, {"text": "The advantages of Hiero can be brought to bear upon Reordering Grammar by reformulating it as a discriminative model.", "labels": [], "entities": [{"text": "Reordering Grammar", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.9476041793823242}]}, {"text": "shows an example PET output showing how our model learns: (1) that the article \"the\" has no equivalent in Japanese, (2) that verbs go after their object, (3) to use postpositions instead of prepositions, and (4) to correctly group certain syntactic units, e.g. NPs and VPs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Comparison of different preordering  models. Superscripts A, B and C signify if the sys- tem is significantly better (p < 0.05) than the re- spective baseline or significantly worse (in which  case it is a subscript). Significance tests were not  computed for RIBES. Score is bold if the system  is significantly better than all the baselines.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 270, "end_pos": 275, "type": "DATASET", "confidence": 0.5171592235565186}, {"text": "Score", "start_pos": 277, "end_pos": 282, "type": "METRIC", "confidence": 0.9591702818870544}]}, {"text": " Table 4: Comparison to MSD and Hiero", "labels": [], "entities": [{"text": "MSD", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.8190637826919556}, {"text": "Hiero", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8757047653198242}]}]}