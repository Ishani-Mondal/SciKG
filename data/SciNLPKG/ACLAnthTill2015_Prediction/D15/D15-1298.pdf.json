{"title": [{"text": "PhraseRNN: Phrase Recursive Neural Network for Aspect-based Sentiment Analysis", "labels": [], "entities": [{"text": "Aspect-based Sentiment Analysis", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.6944968303044637}]}], "abstractContent": [{"text": "This paper presents anew method to identify sentiment of an aspect of an entity.", "labels": [], "entities": [{"text": "identify sentiment of an aspect of an entity", "start_pos": 35, "end_pos": 79, "type": "TASK", "confidence": 0.8416470438241959}]}, {"text": "It is an extension of RNN (Recursive Neu-ral Network) that takes both dependency and constituent trees of a sentence into account.", "labels": [], "entities": []}, {"text": "Results of an experiment show that our method significantly outperforms previous methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Aspect-based sentiment analysis (ABSA) has been found to play a significant role in many applications such as opinion mining on product or restaurant reviews.", "labels": [], "entities": [{"text": "Aspect-based sentiment analysis (ABSA)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7983799030383428}, {"text": "opinion mining on product or restaurant reviews", "start_pos": 110, "end_pos": 157, "type": "TASK", "confidence": 0.8450045585632324}]}, {"text": "It is a task to determine an attitude, opinion and emotions of people toward aspects in a sentence.", "labels": [], "entities": []}, {"text": "For example, given a sentence \"Except the design, the phone is bad for me\", the system should classify positive and negative as the sentiments for the aspects 'design' and 'phone', respectively.", "labels": [], "entities": []}, {"text": "The simple approach is to calculate a sentiment score of a given aspect as the weighted sum of opinion scores, which are defined by a sentiment lexicon, of all words in the sentence (.", "labels": [], "entities": []}, {"text": "This method is further improved by identifying the aspect-opinion relations using tree kernel method).", "labels": [], "entities": []}, {"text": "Other researches have attempted to use unsupervised topic modeling methods.", "labels": [], "entities": []}, {"text": "To identify the sentiment category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA), ASUM (Jo and Oh, 2011), JST ( and FACTS model ().", "labels": [], "entities": [{"text": "TSLDA", "start_pos": 145, "end_pos": 150, "type": "METRIC", "confidence": 0.8072314262390137}]}, {"text": "Recursive Neural Network (RNN) is a kind of deep neural network.", "labels": [], "entities": [{"text": "Recursive Neural Network (RNN)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6085756023724874}]}, {"text": "Using distributed representations of words (aka word embedding) (, RNN merges word representations to represent phrases or sentences.", "labels": [], "entities": []}, {"text": "It is one of the best methods to predict sentiment labels for the phrases.", "labels": [], "entities": [{"text": "predict sentiment labels", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.7385817567507426}]}, {"text": "AdaRNN (Adaptive Recursive Neural Network) is an extension of RNN for Twitter sentiment classification (.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 70, "end_pos": 102, "type": "TASK", "confidence": 0.6487783988316854}]}, {"text": "This paper proposes anew method PhraseRNN for ABSA.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.8749027848243713}]}, {"text": "It is an extended model of RNN and AdaRNN, which are briefly introduced in Section 2.", "labels": [], "entities": [{"text": "RNN", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.7438031435012817}, {"text": "AdaRNN", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.767577052116394}]}, {"text": "The basic idea is to make the representation of the target aspect richer by using syntactic information from both the dependency and constituent trees of the sentence.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the restaurant reviews dataset in SemEval2014 Task 4 consisting of over 3000 English sentences.", "labels": [], "entities": [{"text": "SemEval2014 Task 4", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.5775376756985983}]}, {"text": "For each aspect, \"positive\", \"negative\" or \"neutral\" is annotated as its polarity.", "labels": [], "entities": []}, {"text": "Dataset is divided into three parts: 70% training, 10% development and 20% test.", "labels": [], "entities": [{"text": "development", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9618310928344727}]}, {"text": "We compare the following methods: ASA w/o RE: It defines a sentiment score of a given aspect as the weighted sum of opinion scores of all words in the sentence, where the weight is defined by the distance from the aspect (.", "labels": [], "entities": [{"text": "RE", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.8711512684822083}]}, {"text": "ASA with RE: It improves \"ASA w/o RE\" by firstly identifying the aspect-opinion relations using tree kernel, then integrating them to the sentiment calculation.", "labels": [], "entities": [{"text": "ASA", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9246260523796082}, {"text": "ASA w/o RE", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.6846804857254029}]}, {"text": "RNN: It uses only one global function g 1 over the binary dependency tree.", "labels": [], "entities": [{"text": "RNN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7636168003082275}]}, {"text": "AdaRNN: It uses multi-composition functions G = {g 1 , \u00b7 \u00b7 \u00b7 , g n } over a binary dependency tree ().", "labels": [], "entities": [{"text": "AdaRNN", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.91771000623703}]}, {"text": "PhraseRNN-1: our PhraseRNN with only one global function: G = H = g 1 PhraseRNN-2: our PhraseRNN with two global functions.", "labels": [], "entities": []}, {"text": "One for inner-phrase, the other for outer-phrase: G = g 1 and H = h 1Stanford CoreNLP () is used to parse the sentence and obtain constituent and dependency trees.", "labels": [], "entities": []}, {"text": "For RNN, AdaRNN and PhraseRNN, the optimal parameters, which minimize the error in the development set, are used for the sentiment classification of the test set.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 121, "end_pos": 145, "type": "TASK", "confidence": 0.891688734292984}]}, {"text": "We set \u03b2 = 1 for AdaRNN and PhraseRNN since it is reported that \u03b2 = 1 is the best parameter ().", "labels": [], "entities": [{"text": "AdaRNN", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.9137493968009949}, {"text": "PhraseRNN", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.5583735108375549}]}, {"text": "The optimized number of composition functions n and m = n 2 are selected by grid search with n = {2, 4, 6, 8, 10} on the development set.", "labels": [], "entities": []}, {"text": "\u03bb = 0.0001 is employed.", "labels": [], "entities": []}, {"text": "Accuracy (A), Precision (P), Recall (R) and F-measure (F) are used as evaluation metrics . shows the results of the methods.", "labels": [], "entities": [{"text": "Accuracy (A)", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9238525778055191}, {"text": "Precision (P)", "start_pos": 14, "end_pos": 27, "type": "METRIC", "confidence": 0.9322272390127182}, {"text": "Recall (R)", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9602082222700119}, {"text": "F-measure (F)", "start_pos": 44, "end_pos": 57, "type": "METRIC", "confidence": 0.9571665972471237}]}, {"text": "Differences of PhraseRNN and RNN are verified by statistical significance tests.", "labels": [], "entities": [{"text": "PhraseRNN", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9885278940200806}, {"text": "RNN", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9320007562637329}]}, {"text": "We use the paired randomization test because it does not require additional assumption about distribution of outputs (.", "labels": [], "entities": []}, {"text": "The results indicate that four variations of our PhraseRNN outperform \"ASA w/o RE\", \"ASA with RE\", RNN and AdaRNN methods from 5.35% to 19.44% accuracy and 8% to 16.48% F-measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9993085861206055}, {"text": "F-measure", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.9962298274040222}]}, {"text": "Among four variations, PhraseRNN-2 and PhraseRNN-3 achieved the best performance.", "labels": [], "entities": []}, {"text": "By using different global functions in the inner and outer phrases, PhraseRNN-2 improves PhraseRNN-1 by 2.54% F-measure while keeping the comparable accuracy.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9989891648292542}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9964840412139893}]}, {"text": "Using multi-composition functions is also effective since PhraseRNN-3 was better than PhraseRNN-1 by 1.55% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9982790946960449}]}, {"text": "PhraseRNN-4 improved PhraseRNN-3 by 6.38% precision while keeping comparable in other metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9995505213737488}]}, {"text": "Since our PhraseRNN-1 and PhraseRNN-3 outperform RNN and AdaRNN (the models relying on the binary dependency tree) respectively, we can conclude that our target dependent binary phrase dependency tree is much effective than binary dependency tree for ABSA.", "labels": [], "entities": []}, {"text": "In the data used in (), one sentence contains only one aspect.", "labels": [], "entities": []}, {"text": "On the other hand, two or more aspects can be appeared in one sentence in SemEval 2014 data.", "labels": [], "entities": [{"text": "SemEval 2014 data", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.8202470938364664}]}, {"text": "It is common in the real text.", "labels": [], "entities": []}, {"text": "To examine in which cases our method is better than the others, we conduct an additional experiment by dividing the test set into three disjoint subsets.", "labels": [], "entities": []}, {"text": "The first subset (S1) contains sentences having only one aspect.", "labels": [], "entities": []}, {"text": "The second subset (S2) Precision, Recall and F-measure are the average for three polarity categories weighted by the number of true instances. and third subset (S3) have two or more aspects in each sentence.", "labels": [], "entities": [{"text": "Precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9977108240127563}, {"text": "Recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9913010001182556}, {"text": "F-measure", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9934531450271606}]}, {"text": "All aspects in a sentence in S2 have the same sentiment category, while different sentiment categories in S3.", "labels": [], "entities": []}, {"text": "The number of aspects in S1, S2 and S3 are 200, 323 and 187, respectively.", "labels": [], "entities": []}, {"text": "shows the number of aspects where their sentiments are correctly identified by the methods in the subsets S1, S2 and S3.", "labels": [], "entities": []}, {"text": "The accuracies are also shown in parentheses.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9980692267417908}]}, {"text": "Among three subsets, S3 is the most difficult and ambiguous case.", "labels": [], "entities": []}, {"text": "In all methods, the performance in S3 is worse than S1 and S2.", "labels": [], "entities": []}, {"text": "Comparing with other methods in each subset, PhraseRNN improves the accuracy in S2 more than in S1 and S3.", "labels": [], "entities": [{"text": "PhraseRNN", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.8073567152023315}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9994664788246155}]}], "tableCaptions": [{"text": " Table 1: Results of ABSA", "labels": [], "entities": [{"text": "ABSA", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.8773697018623352}]}, {"text": " Table 2: The Number of Correctly Identified As- pects in Subsets S1, S2 and S3", "labels": [], "entities": []}]}