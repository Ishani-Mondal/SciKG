{"title": [{"text": "Discourse Planning with an N-gram Model of Relations", "labels": [], "entities": [{"text": "Discourse Planning", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7789632380008698}]}], "abstractContent": [{"text": "While it has been established that transitions between discourse relations are important for coherence, such information has not so far been used to aid in language generation.", "labels": [], "entities": [{"text": "language generation", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.7242140918970108}]}, {"text": "We introduce an approach to discourse planning for concept-to-text generation systems which simultaneously determines the order of messages and the discourse relations between them.", "labels": [], "entities": []}, {"text": "This approach makes it straightforward to use statistical transition models, such as n-gram models of discourse relations learned from an annotated corpus.", "labels": [], "entities": []}, {"text": "We show that using such a model significantly improves the quality of the generated text as judged by humans.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse planning is a subtask of Natural Language Generation (NLG), concerned with determining the ordering of messages in a document and the discourse relations that hold among them.", "labels": [], "entities": [{"text": "Discourse planning", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7186932414770126}, {"text": "Natural Language Generation (NLG)", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.8191526432832082}]}, {"text": "Early approaches to discourse planning used manually written rules, often based on schemas or on Rhetorical Structure Theory (RST)).", "labels": [], "entities": [{"text": "discourse planning", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7426195442676544}, {"text": "Rhetorical Structure Theory (RST))", "start_pos": 97, "end_pos": 131, "type": "TASK", "confidence": 0.7091086804866791}]}, {"text": "In the past decade, various statistical approaches have emerged;).", "labels": [], "entities": []}, {"text": "Other relevant statistical approaches to content ordering can also be found in the summarization literature ().", "labels": [], "entities": [{"text": "content ordering", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.775865912437439}]}, {"text": "These approaches overwhelmingly focus on determining the best order of messages using semantic conent, while discourse relations are inmost cases either determined by manuallywritten derivation rules or completely ignored.", "labels": [], "entities": []}, {"text": "Meanwhile, researchers working on discourse relation disambiguation have observed that the sequence of discourse relations itself, independently of content, helps in disambiguating adjacent relations (.", "labels": [], "entities": [{"text": "discourse relation disambiguation", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.6345932682355245}]}, {"text": "Sequential discourse information has been used successfully in discourse parsing, and discourse structure was shown to be as important for text coherence as entity-based content structure).", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7079581171274185}]}, {"text": "Surprisingly, so far, discourse sequential information from existing discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) ( has not been used in generation.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 110, "end_pos": 140, "type": "DATASET", "confidence": 0.9490851163864136}]}, {"text": "In this paper, we present an NLG framework that generates texts from existing semantic web ontologies.", "labels": [], "entities": []}, {"text": "We use an n-gram model of discourse relations to perform discourse planning for these stories.", "labels": [], "entities": [{"text": "discourse planning", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.6946680545806885}]}, {"text": "Through a crowd-sourced human evaluation, we show that the ordering of our documents and the choice of discourse relations is significantly better when using this model.", "labels": [], "entities": []}], "datasetContent": [{"text": "One method for evaluating a discourse plan independently of content is to produce pairs of generated short text documents, each containing the same content, but with different ordering and discourse relations (as dictated by the discourse plan).", "labels": [], "entities": []}, {"text": "The only obvious way to decide which text is better is to have human judges make that decision.", "labels": [], "entities": []}, {"text": "It is important to minimize the effects of other qualities of the texts (differences in content, word choice, grammatical style, etc.) as much as possible, so that the judgement is based only on the differences in order and discourse.", "labels": [], "entities": []}, {"text": "We used DBPedia () -an RDF ontology extracted from Wikipedia -to generate content.", "labels": [], "entities": []}, {"text": "Each document generated was a comparison story of two entities in a single category (i.e., the messages in the stories were selected, as described in Section 2, from the set of triples where one of the entities was the subject).", "labels": [], "entities": []}, {"text": "In order to experiment with different domains, we used four different categories: Office Holder (i.e., a person holding office such as a President or a Judge); River; Television Show; and Military Unit.", "labels": [], "entities": []}, {"text": "The The birthplace of Allen J. Ellender is Montegut, Louisiana, while the death place of Allen J. Ellender is Maryland.", "labels": [], "entities": [{"text": "Allen J. Ellender", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.4879337549209595}]}, {"text": "The birthplace of Robert E. Quinn is Phoenix, Rhode Island.", "labels": [], "entities": [{"text": "Robert E. Quinn", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.890268882115682}]}, {"text": "Subsequently, the death place of Robert E. Quinn is Rhode Island.", "labels": [], "entities": [{"text": "Robert E. Quinn", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.8624083797136942}, {"text": "Rhode Island", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.870282918214798}]}, {"text": "The birthplace of Allen J. Ellender is Montegut, Louisiana.", "labels": [], "entities": [{"text": "Allen J. Ellender", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.5944335560003916}]}, {"text": "In comparison, the birthplace of Robert E. Quinn is Phoenix, Rhode Island.", "labels": [], "entities": [{"text": "Robert E. Quinn", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.857877234617869}]}, {"text": "The death place of Robert E. Quinn is Rhode Island, but the death place of Allen J. Ellender is Maryland.: Sample pair of comparison stories entity pairs from each category were chosen at random but were required to have at least 8 predicates and 3 objects in common, so that they were somewhat semantically related.", "labels": [], "entities": []}, {"text": "To ensure that human judges can easily tell the differences between the stories on a sentential level, we limited the size of each story to 4 messages.", "labels": [], "entities": []}, {"text": "For each pair of stories, everything but the discourse plan (i.e. the content selection, the realization of messages and the lexical choice of connectives) was identical.", "labels": [], "entities": []}, {"text": "shows an example pair of stories from the Office Holder category.", "labels": [], "entities": [{"text": "Office Holder category", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7477420369784037}]}, {"text": "We conducted two crowd sourced experiments on the CrowdFlower platform.", "labels": [], "entities": [{"text": "CrowdFlower platform", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.8808425962924957}]}, {"text": "Each question consisted of two short stories that are completely identical in content, but each generated with a different discourse planner.", "labels": [], "entities": []}, {"text": "The human judge was asked to decide which of the stories has a better flow (or whether they are equally good), and then to give each of the stories a score from 1 to 5, paying specific attention to the ordering of the prepositions and the relations between them.", "labels": [], "entities": []}, {"text": "The stories were presented in a random order and were not given labels, to avoid bias.", "labels": [], "entities": []}, {"text": "We generated 125 pairs of stories from each category -a total of 500 -for each experiment.", "labels": [], "entities": []}, {"text": "Each question was presented to three judges.", "labels": [], "entities": []}, {"text": "In each experiment, there was complete disagreement among the three annotators in approximately 15% of the questions, and those were discarded.", "labels": [], "entities": []}, {"text": "In approximately 20% there was complete agreement, and in the rest of the questions there were two judges who agreed and one who disagreed.", "labels": [], "entities": []}, {"text": "We also computed inter-annotator agreement using Cohen's Kappa for 217 pairs of judges who: Results for the comparison between the Wikipedia model and the PDTB model both answered at least 10 of the same questions.", "labels": [], "entities": []}, {"text": "The average kappa value was 0.5, suggesting reasonable agreement.", "labels": [], "entities": []}, {"text": "In the first experiment, we compared stories generated by a planner using an n-gram model extracted from the PDTB with stories generated by a baseline planner, where all edges have identical probabilities.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 109, "end_pos": 113, "type": "DATASET", "confidence": 0.9315892457962036}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "In the second experiment, we used a PDTB shallow discourse parser we developed to create a discourse-annotated version of the English Wikipedia.", "labels": [], "entities": [{"text": "PDTB shallow discourse parser", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.7334515899419785}, {"text": "English Wikipedia", "start_pos": 126, "end_pos": 143, "type": "DATASET", "confidence": 0.7334973812103271}]}, {"text": "We then compared stories generated by a planner using an ngram model extracted from the parsed Wikipedia corpus with those generated by a planner using the PDTB model.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 95, "end_pos": 111, "type": "DATASET", "confidence": 0.8545221090316772}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The total results in both tables are statistically significant (p < 0.05).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the comparison between the  PDTB n-gram model and the baseline", "labels": [], "entities": [{"text": "PDTB n-gram model", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.84604279200236}]}, {"text": " Table 2: Results for the comparison between the  Wikipedia model and the PDTB model", "labels": [], "entities": [{"text": "Wikipedia model", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.9630703628063202}, {"text": "PDTB", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.9395158290863037}]}]}