{"title": [{"text": "The Forest Convolutional Network: Compositional Distributional Semantics with a Neural Chart and without Binarization", "labels": [], "entities": []}], "abstractContent": [{"text": "According to the principle of composi-tionality, the meaning of a sentence is computed from the meaning of its parts and the way they are syntactically combined.", "labels": [], "entities": []}, {"text": "In practice, however, the syntactic structure is computed by automatic parsers which are far-from-perfect and not tuned to the specifics of the task.", "labels": [], "entities": []}, {"text": "Current re-cursive neural network (RNN) approaches for computing sentence meaning therefore run into a number of practical difficulties, including the need to carefully select a parser appropriate for the task, deciding how and to what extent syntactic context modifies the semantic composition function , as well as on how to transform parse trees to conform to the branching settings (typically, binary branching) of the RNN.", "labels": [], "entities": [{"text": "computing sentence meaning", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.7237003048261007}]}, {"text": "This paper introduces anew model, the Forest Convolutional Network, that avoids all of these challenges, by taking a parse forest as input, rather than a singletree, and by allowing arbitrary branching factors.", "labels": [], "entities": []}, {"text": "We report improvements over the state-of-the-art in sentiment analysis and question classification.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.9706182777881622}, {"text": "question classification", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.808567613363266}]}], "introductionContent": [{"text": "For many natural language processing tasks we need to compute meaning representations for sentences from meaning representations of words.", "labels": [], "entities": []}, {"text": "Ina recent line of research on 'recursive neural networks' (e.g.,), both the word and sentence representations are vectors, and the word vectors (\"embeddings\") are borrowed from work in distributional semantics or neural language modelling.", "labels": [], "entities": []}, {"text": "Sentence representations, in this approach, are computed by recursively applying a neural network that combines two vectors into one (typically according to the syntactic structure provided by an external parser).", "labels": [], "entities": []}, {"text": "The network, which thus implements a 'composition function', is optimized for delivering sentence representations that support a given semantic task: sentiment analysis (, paraphrase detection), semantic relatedness etc.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.890729695558548}, {"text": "paraphrase detection", "start_pos": 172, "end_pos": 192, "type": "TASK", "confidence": 0.8294661343097687}]}, {"text": "Studies with recursive neural networks have yielded promising results on a variety of such tasks.", "labels": [], "entities": []}, {"text": "In this paper, we represent anew recursive neural network architecture that fits squarely in this tradition, but aims to solve a number of difficulties that have arisen in existing work.", "labels": [], "entities": []}, {"text": "In particular, the model we propose addresses three issues: 1.", "labels": [], "entities": []}, {"text": "how to make the composition functions adaptive, in the sense that they operate adequately for the many different types of combinations (e.g., adjective-noun combinations are of a very different type than VP-PP combinations); 2.", "labels": [], "entities": []}, {"text": "how to deal with different branching factors of nodes in the relevant syntactic trees (i.e., we want to avoid having to binarize syntactic trees, 1 but also do not want ternary productions to be completely independent from binary productions); 3.", "labels": [], "entities": []}, {"text": "how to deal with uncertainty about the correct parse inside the neural architecture (i.e., we don't want to work with just the best or k-best parses fora sentence according to an external model, but receive an entire distribution over possible parsers).", "labels": [], "entities": []}, {"text": "To solve these challenges we take inspiration from two other traditions: the convolutional neural networks and classic parsing algorithms based on dynamic programming.", "labels": [], "entities": []}, {"text": "Including convolution in our network provides a direct solution for issue (2), and turns out, somewhat unexpectedly, to also provide a solution for issue (1).", "labels": [], "entities": []}, {"text": "Introducing the chart representation from classic parsing into our architecture then allows us to tackle issue (3).", "labels": [], "entities": []}, {"text": "The resulting model, the Forest Convolutional Network, outperforms all other models on a sentiment analysis and question classification task.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.9548037052154541}, {"text": "question classification task", "start_pos": 112, "end_pos": 140, "type": "TASK", "confidence": 0.7991594870885214}]}], "datasetContent": [{"text": "We evaluate the FCN model with two tasks: question classification and sentiment analysis.", "labels": [], "entities": [{"text": "question classification", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.865403413772583}, {"text": "sentiment analysis", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.9538584053516388}]}, {"text": "The evaluation metric is the classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.9415210485458374}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.7872618436813354}]}, {"text": "Our networks were initialized with the 300-D GloVe word embeddings trained on a corpus of 840B words).", "labels": [], "entities": []}, {"text": "The initial values fora weight matrix were uniformly sampled from the symmetric interval where n is the number of total input units.", "labels": [], "entities": []}, {"text": "In each experiment, a development set was used to tune the model.", "labels": [], "entities": []}, {"text": "We run the model ten times and chose the run with the highest performance on the development set.", "labels": [], "entities": []}, {"text": "We employed early stopping: training is halted if performance on the development set does not improve after three consecutive epochs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracies at sentence level on the SST  dataset. FCN (dep.) and FCN (const.) denote the  FCN with dependency forests and with constituent  forests, respectively. The accuracies of RNTN,  CNN, DCNN, PV, DRNN, LSTM-RNN and CT- LSTM are copied from the corresponding papers  (see text).", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9814183712005615}, {"text": "SST  dataset", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.8645034432411194}]}, {"text": " Table 2: Accuracies on the TREC question type  classification dataset. The accuracies of DCNN,  MaxEnt H , CNN-non-static, and SVM S are copied  from the corresponding papers (see text).", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9871442914009094}, {"text": "TREC question type  classification dataset", "start_pos": 28, "end_pos": 70, "type": "DATASET", "confidence": 0.813905906677246}, {"text": "DCNN", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.9543439745903015}]}]}