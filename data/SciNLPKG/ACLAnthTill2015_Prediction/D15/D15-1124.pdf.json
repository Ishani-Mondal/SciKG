{"title": [{"text": "ReVal: A Simple and Effective Machine Translation Evaluation Metric Based on Recurrent Neural Networks", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.8390285770098368}]}], "abstractContent": [{"text": "Many state-of-the-art Machine Translation (MT) evaluation metrics are complex, involve extensive external resources (e.g. for paraphrasing) and require tuning to achieve best results.", "labels": [], "entities": [{"text": "Machine Translation (MT) evaluation", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.8821985026200613}]}, {"text": "We present a simple alternative approach based on dense vector spaces and recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks.", "labels": [], "entities": []}, {"text": "For WMT-14, our new metric scores best for two out of five language pairs, and overall best and second best on all language pairs, using Spearman and Pearson correlation, respectively.", "labels": [], "entities": [{"text": "WMT-14", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.8290166258811951}, {"text": "Spearman", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.961800217628479}, {"text": "Pearson correlation", "start_pos": 150, "end_pos": 169, "type": "METRIC", "confidence": 0.8140583634376526}]}, {"text": "We also show how training data is computed automatically from WMT ranks data.", "labels": [], "entities": [{"text": "WMT ranks data", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.7901793917020162}]}], "introductionContent": [{"text": "Deep learning approaches have turned out to be successful in many NLP applications such as paraphrasing (), sentiment analysis (), parsing) and machine translation ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.9467488825321198}, {"text": "parsing", "start_pos": 131, "end_pos": 138, "type": "TASK", "confidence": 0.9760253429412842}, {"text": "machine translation", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.8275846540927887}]}, {"text": "While dense vector space representations such as those obtained through Deep Neural Networks (DNNs) or RNNs are able to capture semantic similarity for words (), segments) and documents () naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 212, "end_pos": 225, "type": "TASK", "confidence": 0.9039718508720398}, {"text": "WordNet", "start_pos": 277, "end_pos": 284, "type": "DATASET", "confidence": 0.9260318875312805}]}, {"text": "This paper presents a novel, efficient and compact MT evaluation measure based on RNNs.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9478976726531982}]}, {"text": "Our metric is simple in the sense that it does not require much machinery and resources apart from the dense word vectors.", "labels": [], "entities": []}, {"text": "This cannot be said of most of the state-of-the-art MT evaluation metrics, which tend to be complex and require extensive feature engineering.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9513409435749054}]}, {"text": "Our metric is based on RNNs and particularly on Tree Long Short Term Memory (Tree-LSTM) networks.", "labels": [], "entities": []}, {"text": "LSTM (Hochreiter and) is a sequence learning technique which uses a memory cell to preserve a state over along period of time.", "labels": [], "entities": []}, {"text": "This enables distributed representations of sentences using distributed representations of words.", "labels": [], "entities": []}, {"text": "Tree-LSTM is a recent approach, which is an extension of the simple LSTM framework (.", "labels": [], "entities": []}, {"text": "To provide the required training data, we also show how to automatically convert the WMT-13 () human evaluation rankings into similarity scores between the reference and the translation.", "labels": [], "entities": [{"text": "WMT-13", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.7489551901817322}]}, {"text": "Our metric including training data is available at https://github.com/rohitguptacs/ReVal.", "labels": [], "entities": []}], "datasetContent": [{"text": "We represent both the reference (h ref ) and the translation (h tra ) using an LSTM and predict the similarity scor\u00ea y based on a neural network which considers both distance and angle between h ref and h tra : where, \u03c3 is a sigmoid function, \u02c6 p \u03b8 is the estimated probability distribution vector and The cost function J(\u03b8) is defined over probability distributions p and\u02c6pand\u02c6 and\u02c6p \u03b8 using regularised KullbackLeibler (KL) divergence.", "labels": [], "entities": []}, {"text": "In Equation 3, i represents the index of each training pair, n is the number of training pairs and p is the sparse target distribution such that y = r T p is defined as follows: is the similarity score of a training pair.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 185, "end_pos": 201, "type": "METRIC", "confidence": 0.9494177401065826}]}, {"text": "For example, for y = 2.7, p T = [0 0.3 0.7 0 0].", "labels": [], "entities": []}, {"text": "In our case, the similarity score y is a value between 1 and 5.", "labels": [], "entities": [{"text": "similarity score y", "start_pos": 17, "end_pos": 35, "type": "METRIC", "confidence": 0.9796153903007507}]}, {"text": "For our work, we use glove word vectors) and the simple LSTM, the dependency Tree-LSTM and neural network implementations by.", "labels": [], "entities": []}, {"text": "The system uses the scientific computing framework Torch . Training is performed on the data computed in Section 5.", "labels": [], "entities": [{"text": "Torch", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.8712430596351624}]}, {"text": "The system uses a mini batch size of 25 with learning rate 0.05 and regularization strength 0.0001.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.9619076251983643}, {"text": "regularization", "start_pos": 68, "end_pos": 82, "type": "METRIC", "confidence": 0.9643153548240662}]}, {"text": "The compositional parameters for our Tree-LSTM systems with memory dimensions 150 and 300 are 203,400 and 541,800, respectively.", "labels": [], "entities": []}, {"text": "The training is performed for 10 epochs.", "labels": [], "entities": []}, {"text": "System-level scores are computed by aggregating and normalising segment-level scores.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Variances computed using Algorithm 1", "labels": [], "entities": []}, {"text": " Table 2: Derived Corpus statistics", "labels": [], "entities": [{"text": "Derived Corpus", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8863255679607391}]}, {"text": " Table 3: Results: System-Level Correlations on WMT-14", "labels": [], "entities": [{"text": "WMT-14", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.713507354259491}]}]}