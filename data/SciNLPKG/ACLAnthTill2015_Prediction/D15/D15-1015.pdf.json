{"title": [{"text": "Visual Bilingual Lexicon Induction with Transferred ConvNet Features", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper is concerned with the task of bilingual lexicon induction using image-based features.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.7707536220550537}]}, {"text": "By applying features from a convolutional neural network (CNN), we obtain state-of-the-art performance on a standard dataset, obtaining a 79% relative improvement over previous work which uses bags of visual words based on SIFT features.", "labels": [], "entities": []}, {"text": "The CNN image-based approach is also compared with state-of-the-art linguistic approaches to bilingual lexicon induction , even outperforming these for one of three language pairs on another standard dataset.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.6583231985569}]}, {"text": "Furthermore, we shed new light on the type of visual similarity metric to use for genuine similarity versus re-latedness tasks, and experiment with using multiple layers from the same network in an attempt to improve performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bilingual lexicon induction is the task of finding words that share a common meaning across different languages.", "labels": [], "entities": [{"text": "Bilingual lexicon induction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8382703264554342}]}, {"text": "It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval) and statistical machine translation.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.7999347746372223}, {"text": "natural language processing", "start_pos": 78, "end_pos": 105, "type": "TASK", "confidence": 0.6932353973388672}, {"text": "cross-lingual information retrieval", "start_pos": 117, "end_pos": 152, "type": "TASK", "confidence": 0.630748450756073}, {"text": "statistical machine translation", "start_pos": 158, "end_pos": 189, "type": "TASK", "confidence": 0.7579550544420878}]}, {"text": "Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages, these corpora are either too small or unavailable for many language pairs.", "labels": [], "entities": []}, {"text": "Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed).", "labels": [], "entities": []}, {"text": "These approaches work by mapping language pairs to a shared bilingual space and extracting lexical items from that space.", "labels": [], "entities": []}, {"text": "Bergsma and Van Durme (2011) showed that this bilingual space need not be linguistic in nature: they used labeled images from the Web to obtain bilingual lexical translation pairs based on the visual features of corresponding images.", "labels": [], "entities": []}, {"text": "Local features are computed using SIFT) and color histograms () and aggregated as bags of visual words (BOVW)) to get bilingual representations in a shared visual space.", "labels": [], "entities": [{"text": "BOVW", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9559323191642761}]}, {"text": "Their highest performance is obtained by combining these visual features with normalized edit distance, an orthographic similarity metric).", "labels": [], "entities": []}, {"text": "There are several advantages to having a visual rather than a linguistic intermediate bilingual space: First, while labeled images are readily available for many languages through resources such as Google Images, language pairs that have sizeable comparable, let alone parallel, corpora are relatively scarce.", "labels": [], "entities": []}, {"text": "Second, it has been found that meaning is often grounded in the perceptual system, and that the quality of semantic representations improves significantly when they are grounded in the visual modality.", "labels": [], "entities": []}, {"text": "Having an intermediate visual space means that words in different languages can be grounded in the same space.", "labels": [], "entities": []}, {"text": "Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings.", "labels": [], "entities": []}, {"text": "Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space.", "labels": [], "entities": []}, {"text": "While some approaches to bilingual lexicon induction rely on orthographic properties () or properties of frequency distributions) that will work only for closely related languages, a visual space can work for any language, whether it's English or Chinese, Arabic or Icelandic, or all Greek to you.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.7577216625213623}]}, {"text": "It has recently been shown, however, that much better performance can be achieved on semantic similarity and relatedness tasks by using visual representations from deep convolutional neural networks (CNNs) instead of BOVW features.", "labels": [], "entities": [{"text": "semantic similarity and relatedness tasks", "start_pos": 85, "end_pos": 126, "type": "TASK", "confidence": 0.7525610566139221}]}, {"text": "In this paper we apply such CNN-derived visual features to the task of bilingual lexicon induction.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.6999684274196625}]}, {"text": "To obtain a translation of a word in a source language, we find the nearest neighbours from words in the target language, where words in both languages reside in a shared visual space made up of CNN-based features.", "labels": [], "entities": []}, {"text": "Nearest neighbours are found by applying similarity metrics from both and Bergsma and Van Durme (2011).", "labels": [], "entities": []}, {"text": "In summary, the contributions of this paper are: \u2022 We obtain a relative improvement of 79% over Bergsma and Van Durme (2011) on a standard dataset based on fifteen language pairs.", "labels": [], "entities": []}, {"text": "\u2022 We shed new light on the question of whether genuine similarity versus semantic relatedness tasks require different similarity metrics for optimal performance).", "labels": [], "entities": []}, {"text": "\u2022 We experiment with using different layers of the CNN and find that performance is not affected significantly in either case, obtaining a slight improvement for the relatedness task but no improvement for genuine similarity.", "labels": [], "entities": []}, {"text": "\u2022 Finally, we show that the visual approach outperforms the linguistic approaches on one of the three language pairs on a standard dataset.", "labels": [], "entities": []}, {"text": "To our knowledge this is the first work to provide a comparison of visual and state-of-theart linguistic approaches to bilingual lexicon induction.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 119, "end_pos": 146, "type": "TASK", "confidence": 0.7237959305445353}]}], "datasetContent": [{"text": "Bergsma and Van Durme's primary evaluation dataset consists of a set of five hundred matching lexical items for fifteen language pairs, based on six languages.", "labels": [], "entities": []}, {"text": "(The fifteen pairs results from all ways of pairing six languages).", "labels": [], "entities": []}, {"text": "The data is publicly available online.", "labels": [], "entities": []}, {"text": "In order to get the five hundred lexical items, they first rank nouns by the conditional probability of them occurring in the pattern \"{image,photo,photograph,picture} of {a,an} \" in the web-scale Google N-gram corpus (, and take the top five hundred words as their English lexicon.", "labels": [], "entities": []}, {"text": "For each item CNN-MAX sim(max I(w s ), max I(w t )): Visual similarity metrics between two sets of n images.", "labels": [], "entities": []}, {"text": "I(w s ) represents the set of images fora given source word w s , I(w t ) the set of images fora given target word wt ; max takes a set of vectors and returns the single element-wise maximum vector.", "labels": [], "entities": []}, {"text": "in the English lexicon, they obtain corresponding items in the other languages-Spanish, Italian, French, German and Dutch-through Google Translate.", "labels": [], "entities": []}, {"text": "We call this dataset BERGSMA500.", "labels": [], "entities": [{"text": "BERGSMA500", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9733066558837891}]}, {"text": "In addition to that dataset, we evaluate on a dataset constructed to measure the general performance of bilingual lexicon learning models from comparable Wikipedia data).", "labels": [], "entities": []}, {"text": "The dataset comprises 1, 000 nouns in three languages: Spanish (ES), Italian (IT), and Dutch (NL), along with their one-to-one goldstandard word translations in English (EN) compiled semi-automatically using Google Translate and manual annotators for each language.", "labels": [], "entities": []}, {"text": "We call this dataset VULIC1000 2 . The test set is accompanied with comparable data for training, for the three language pairs ES/IT/NL-EN on which textbased models for bilingual lexicon induction were trained.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 169, "end_pos": 196, "type": "TASK", "confidence": 0.6667562325795492}]}, {"text": "Given the way that the BERGSMA500 dataset was created, in particular the use of the pattern described above, it contains largely concrete linguistic concepts (since, eg, image of a democracy is unlikely to have a high corpus frequency).", "labels": [], "entities": [{"text": "BERGSMA500 dataset", "start_pos": 23, "end_pos": 41, "type": "DATASET", "confidence": 0.8455208241939545}]}, {"text": "In contrast, VULIC1000 was designed to capture general bilingual word correspondences, and contains several highly abstract test examples, such as entendimiento (understanding) and desigualdad (inequality) in Spanish, or scoperta (discovery) and cambiamento (change) in Italian.", "labels": [], "entities": [{"text": "VULIC1000", "start_pos": 13, "end_pos": 22, "type": "DATASET", "confidence": 0.8374796509742737}]}, {"text": "Using the two evaluation datasets can potentially provide: Performance on BERGSMA500 compared to Bergsma and Van Durme (B&VD).", "labels": [], "entities": [{"text": "BERGSMA500", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.7645856738090515}, {"text": "B&VD)", "start_pos": 120, "end_pos": 125, "type": "DATASET", "confidence": 0.7688555642962456}]}, {"text": "some insight into how purely visual models for bilingual lexicon induction behave with respect to both abstract and concrete concepts.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6938051680723826}]}, {"text": "We measure performance in a standard way using mean-reciprocal rank: where rank(w s , wt ) denotes the rank of the correct translation wt (as provided in the gold standard) in the ranked list of translation candidates for w s , and M is the number of test cases.", "labels": [], "entities": []}, {"text": "We also use precision at N (P@N) (, which measures the proportion of test instances where the correct translation is within the top N highest ranked translations.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9994506239891052}]}], "tableCaptions": [{"text": " Table 2: Performance on BERGSMA500 com- pared to Bergsma and Van Durme (B&VD).", "labels": [], "entities": [{"text": "BERGSMA500 com- pared", "start_pos": 25, "end_pos": 46, "type": "METRIC", "confidence": 0.7571255713701248}, {"text": "B&VD)", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.7709937617182732}]}, {"text": " Table 4: Performance on VULIC1000 compared to the linguistic bootstrapping method of Vuli\u00b4cVuli\u00b4c and  Moens (2013b).", "labels": [], "entities": [{"text": "VULIC1000", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.873960554599762}]}, {"text": " Table 3: Spearman \u03c1 s correlation for the visual  similarity metrics on a relatedness (MEN) and a  genuine similarity (SimLex-999) dataset.", "labels": [], "entities": [{"text": "Spearman \u03c1 s correlation", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.8103229850530624}, {"text": "SimLex-999) dataset", "start_pos": 120, "end_pos": 139, "type": "DATASET", "confidence": 0.7747153639793396}]}, {"text": " Table 5: Spearman \u03c1 s correlation for the visual  similarity metrics on a relatedness (MEN) and  a genuine similarity (SimLex-999) dataset using  more than one layer from the CNN.", "labels": [], "entities": [{"text": "Spearman \u03c1 s correlation", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.6446992084383965}, {"text": "SimLex-999) dataset", "start_pos": 120, "end_pos": 139, "type": "DATASET", "confidence": 0.7075122197469076}]}]}