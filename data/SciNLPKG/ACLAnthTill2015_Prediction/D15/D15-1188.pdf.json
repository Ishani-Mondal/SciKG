{"title": [{"text": "Reverse-engineering Language: A Study on the Semantic Compositionality of German Compounds", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we analyze the performance of different composition models on a large dataset of German compound nouns.", "labels": [], "entities": []}, {"text": "Given a vector space model for the German language, we try to reconstruct the observed representation (the corpus-estimated vector) of a compound by composing the observed representations of its two immediate constituents.", "labels": [], "entities": []}, {"text": "We explore the composition models proposed in the literature and also present anew, simple model that achieves the best performance on our dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector space models of language like the ones presented in) create good representations for the individual words of a language.", "labels": [], "entities": []}, {"text": "However, the words in a language can be combined into infinitely many distinct, wellformed phrases and sentences.", "labels": [], "entities": []}, {"text": "Creating meaningful, reusable representations for such longer word sequences is still an open problem.", "labels": [], "entities": []}, {"text": "In this paper we focus on building representations for syntactic units just above the word level, by exploring compositional models for compounds.", "labels": [], "entities": []}, {"text": "Bauer (2001) defines a compound as \"a lexical unit made up of two or more elements, each of which can function as a lexeme independent of the other(s) in other contexts\" (e.g. apple tree).", "labels": [], "entities": []}, {"text": "The vast majority of compounds are compositional, i.e. we can understand the meaning of the compound if we know the meaning of its constituent words.", "labels": [], "entities": []}, {"text": "We would like to equip the vector space model with a composition function able to construct a composite representation for apple tree from the representations of apple and tree.", "labels": [], "entities": []}, {"text": "The composite representation should ideally be indistinguishable from its observed representation, i.e. the representation learned directly by the language model if the compound is part of the dictionary.", "labels": [], "entities": []}, {"text": "We situate our investigations in the context of the German language, a language where compounds represent an important fraction of the vocabulary.", "labels": [], "entities": []}, {"text": "analyzed the 28 million words German APA news corpus and discovered that compounds account for 47% of the word types but only 7% of the overall token count, with 83% of compounds having a corpus frequency of 5 or lower.", "labels": [], "entities": [{"text": "German APA news corpus", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.9207215905189514}]}, {"text": "The high productivity of the compounding process makes the compositional approach the most tractable way to create meaningful representations for all the compounds that have been or will be coined by the speakers of the German language.", "labels": [], "entities": []}, {"text": "German compounds have a strategic advantage for our study: they are generally written as a contiguous word, irrespective of how many constituents they have.", "labels": [], "entities": []}, {"text": "Our example English compound, apple tree, translates into the German compound Apfelbaum, with the head Baum \"tree\" and the modifier Apfel \"apple\".", "labels": [], "entities": []}, {"text": "Because the compound is written as a single word, we can directly learn the representations for the compound and for its constituents.", "labels": [], "entities": []}, {"text": "Given a large dataset of German compounds together with their immediate constituents, and the corresponding distributed representations for each of the individual words, one can try to reverse-engineer the compounding process and learn the parameters of a function that combines the representation of the constituents into the representation of the compound.", "labels": [], "entities": []}, {"text": "More formally, we are interested in learning a composition function f such that where c comp \u2208 Rn is the composite representation of the compound and m obs , h obs \u2208 Rn are the observed representations of its modifier and its head.", "labels": [], "entities": []}, {"text": "The function should minimize J, the mean squared error between the composite (c comp ) and the observed (c obs ) representations of the |C| compounds in the training set: Several compositionality models have already been proposed in the literature (.", "labels": [], "entities": [{"text": "J", "start_pos": 29, "end_pos": 30, "type": "METRIC", "confidence": 0.9721114039421082}]}, {"text": "In this paper we evaluate several of the proposed composition functions and also present anew composition model which outperforms all previous models on a dataset of German compounds.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained 4 vector space language models for German (with 50, 100, 200 and 300 dimensions respectively) using the GloVe package) and a 10 billion token raw-text corpus extracted from the DECOW14AX corpus.", "labels": [], "entities": [{"text": "DECOW14AX corpus", "start_pos": 188, "end_pos": 204, "type": "DATASET", "confidence": 0.9600966572761536}]}, {"text": "We use a vocabulary of 1,029,270 (1M) words, obtained by selecting all the words with a minimum frequency of 100 (the full vocabulary had 50M unique words).", "labels": [], "entities": []}, {"text": "We used the default GloVe training parameters, the only modifications being the use of asymmetric context when constructing the co-occurence matrix (10 words to the left and to the right of the target word) and training each model for 15 iterations.", "labels": [], "entities": []}, {"text": "All the vector spaces were normalized to the L2-norm, first across features then across samples using scikit-learn (Pedregosa et al., 2011).", "labels": [], "entities": []}, {"text": "The German compounds dataset used in the experiments is a subset of the 54759 compounds available in GermaNet 9.0 1 . The compounds in the list were automatically split and manually post-corrected (.", "labels": [], "entities": [{"text": "German compounds dataset", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.788291354974111}, {"text": "GermaNet 9.0 1", "start_pos": 101, "end_pos": 115, "type": "DATASET", "confidence": 0.9143801132837931}]}, {"text": "Each entry in the list is a triple of the form (compound, modifier, head).", "labels": [], "entities": []}, {"text": "We filtered the entries in the list, keeping only those where all three words have a minimum frequency of 500 in the support corpus used to create the vector space representations.", "labels": [], "entities": []}, {"text": "The reason for the filtering step is that a \"well-learned\" representation (based on a sufficiently large number of contexts) should allow fora more accurate reconstruction than a representation based only on a few contexts.", "labels": [], "entities": []}, {"text": "The filtered dataset contains 34497 entries.", "labels": [], "entities": []}, {"text": "This dataset was  The twelve composition models presented in Section 3 were evaluated using word representations of increasing size (described in Section 2).", "labels": [], "entities": []}, {"text": "All the models are trained on the train split and tested on the test split.", "labels": [], "entities": []}, {"text": "We used the rank evaluation method proposed by) fora similar task: first, we generate a composite representation for each of the 6901 compounds in the test set; then, we use the cosine similarity to rank each composite representation with respect to the observed representations of the 41732 unique words in the dataset dictionary.", "labels": [], "entities": []}, {"text": "If the observed representation is the nearest neighbour, the composition is assigned the rank 1.", "labels": [], "entities": []}, {"text": "Similar to (, we assign the rank 1000 (\u22651K) when the observed representation is not one of the nearest 1000 neighbours of the composite representation.", "labels": [], "entities": []}, {"text": "We then compute the first, second and third quartiles (Q1, Q2, Q3) across all the compounds in the test set.", "labels": [], "entities": []}, {"text": "A Q1 value of 2 means that the first 25% of the data was only assigned ranks 1 and 2.", "labels": [], "entities": []}, {"text": "Similarly, Q2 and Q3 refer to the ranks assigned to the first 50% and 75% of data, respectively.", "labels": [], "entities": [{"text": "Q2", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.8805440664291382}]}, {"text": "The results of our evaluation are displayed in.", "labels": [], "entities": []}, {"text": "The observed representation of the head (model 1) was used as a strong baseline for the compound composition task.", "labels": [], "entities": [{"text": "compound composition task", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.8161893089612325}]}, {"text": "Two of the tested models, multiplicative (model 3) and dilation (model 4) score worse than the head baseline, while the additive models (5 and 6) score only slightly above it.", "labels": [], "entities": []}, {"text": "The fact that the worst performing model is the multiplicative model is surprising considering its good performance in previous studies).", "labels": [], "entities": []}, {"text": "This might be either a side-effect of the normalization procedure, or a genuine incompatibility of this compositionality model with the vectorial representations produced by GloVe.", "labels": [], "entities": []}, {"text": "The new Addmask and Wmask models (introduced in Section 3.1) perform very well, with Wmask producing the best results on the test dataset across all dimensions.", "labels": [], "entities": []}, {"text": "It is interesting to note that the linguistically motivated Lexfunc and Fulllex models, which build dedicated representations for each individual constituent, are outperformed by a simple model like Fulladd, that only learns two modification matrices, one for each position.", "labels": [], "entities": [{"text": "Lexfunc", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9326431155204773}]}, {"text": "The explanation is, in our opinion, that the available training material is not enough for training all the parameters of the complex Lexfunc and Fulllex models, but good enough for the more simple Fulladd.", "labels": [], "entities": [{"text": "Lexfunc", "start_pos": 134, "end_pos": 141, "type": "DATASET", "confidence": 0.9660156965255737}, {"text": "Fulllex", "start_pos": 146, "end_pos": 153, "type": "DATASET", "confidence": 0.6286587119102478}, {"text": "Fulladd", "start_pos": 198, "end_pos": 205, "type": "DATASET", "confidence": 0.9866765737533569}]}, {"text": "The mask models are computationally cheaper than models like Lexfunc and Fulllex, as they they only train 2n parameters for each word in the vocabulary, and not n 2 parameters like the aforementioned models.", "labels": [], "entities": [{"text": "Lexfunc", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9318757057189941}, {"text": "Fulllex", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.8695465922355652}]}, {"text": "They manage to strike a balance and learn a dedicated representation for each constituent with a small number of parameters, thus performing better than the more complex models.", "labels": [], "entities": []}, {"text": "We used non-parametric statistical tests to detect significant differences between the results obtained by the models.", "labels": [], "entities": []}, {"text": "We focused our analysis on the best performing 4 models: model 9, which we will label the Matrix model, Fulladd (model 8), Addmask (model 11) and Wmask (model 12).", "labels": [], "entities": [{"text": "Fulladd", "start_pos": 104, "end_pos": 111, "type": "DATASET", "confidence": 0.902496874332428}]}, {"text": "The comparison takes into account two separate factors: (i) differences between the models using representations of the same size; (ii) differences in the performance of the same model using representations of different sizes.", "labels": [], "entities": []}, {"text": "A Friedman test on the ranks obtained by the 4 selected models on representations of size 300 showed that there is a significant difference between the models (p < 0.01).", "labels": [], "entities": []}, {"text": "Pairwise comparisons (using the Wilcoxon signed rank test and Bonferroni corrections) showed that there is a significant difference (p < 0.01) between all but one pair of models, namely the Matrix and the Addmask models (p = 0.9).", "labels": [], "entities": []}, {"text": "The same test confirmed that there are significant differences in the performance of the best model Wmask when using representations of different sizes (p < 0.01).", "labels": [], "entities": []}, {"text": "Pairwise  comparisons showed that Wmask model significantly improves its performance (p < 0.01) when using word representations of increasing size (50, 100, 200 and 300 dimensions).", "labels": [], "entities": []}, {"text": "The twelve composition models were also compared in terms of the mean squared error (MSE) objective function, by computing the MSE between the composite and the observed representation of the compounds in the test set.", "labels": [], "entities": [{"text": "mean squared error (MSE) objective function", "start_pos": 65, "end_pos": 108, "type": "METRIC", "confidence": 0.9459465220570564}, {"text": "MSE", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.9215522408485413}]}, {"text": "The best scoring models in the rank evaluation were also the best in the MSE evaluation.", "labels": [], "entities": [{"text": "MSE", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.6688865423202515}]}, {"text": "However, the difference in performance between the best and the worst models was considerably smaller: the MSE of the multiplicative model is only twice as large as the MSE of the best performing Wmask model.", "labels": [], "entities": [{"text": "MSE", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9859657883644104}]}, {"text": "This is in contrast to the rank evaluation where the multiplicative model assigned the observed representations in the test set only ranks \u2265 1000, while Wmask assigned ranks \u2264 25 to 75% of the test data.", "labels": [], "entities": []}, {"text": "Additional investigations are necessary to estimate the impact of different objective functions on the performance of compositional models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Quartiles for the 6901 composite representations in the test set, ranked with respect to the  observed representations. Best possible rank is 1. D marks the models tested with DISSECT, R marks  reimplementations of existing models and N marks new models.", "labels": [], "entities": []}]}