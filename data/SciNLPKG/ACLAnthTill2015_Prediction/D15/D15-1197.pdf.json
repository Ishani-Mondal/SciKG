{"title": [{"text": "A Strong Lexical Matching Method for the Machine Comprehension Test", "labels": [], "entities": []}], "abstractContent": [{"text": "Machine comprehension of text is the overarching goal of a great deal of research in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 85, "end_pos": 112, "type": "TASK", "confidence": 0.6478171845277151}]}, {"text": "The Machine Comprehension Test (Richard-son et al., 2013) was recently proposed to assess methods on an open-domain, exten-sible, and easy-to-evaluate task consisting of two datasets.", "labels": [], "entities": []}, {"text": "In this paper we develop a lexical matching method that takes into account multiple context windows, question types and coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.9211837351322174}]}, {"text": "We show that the proposed method outper-forms the baseline of Richardson et al.", "labels": [], "entities": []}, {"text": "(2013), and despite its relative simplicity, is comparable to recent work using machine learning.", "labels": [], "entities": []}, {"text": "We hope that our approach will inform future work on this task.", "labels": [], "entities": []}, {"text": "Furthermore , we argue that MC500 is harder than MC160 due to the way question answer pairs were created.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine comprehension of text is the central goal in NLP.", "labels": [], "entities": [{"text": "Machine comprehension of text", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8092529475688934}]}, {"text": "The academic community has proposed a variety of tasks, such as information extraction, semantic parsing and textual entailment.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.841900110244751}, {"text": "semantic parsing", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.7412429600954056}, {"text": "textual entailment", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7602949440479279}]}, {"text": "However, these tasks assess performance on each task individually, rather than on overall progress towards machine comprehension of text.", "labels": [], "entities": []}, {"text": "To this end, proposed the Machine Comprehension Test (MCTest), anew challenge that aims at evaluating machine comprehension.", "labels": [], "entities": []}, {"text": "It does so through an opendomain multiple-choice question answering task on fictional stories requiring the commonsense reasoning typical of a 7-year-old child.", "labels": [], "entities": [{"text": "opendomain multiple-choice question answering task", "start_pos": 22, "end_pos": 72, "type": "TASK", "confidence": 0.6613656401634216}]}, {"text": "It is easy to evaluate as it consists of multiple choice questions.", "labels": [], "entities": []}, {"text": "also showed how the creation of stories and questions can be crowdsourced efficiently, constructing two datasets for the task, namely MC160 and MC500.", "labels": [], "entities": [{"text": "MC160", "start_pos": 134, "end_pos": 139, "type": "DATASET", "confidence": 0.9154304265975952}]}, {"text": "In addition, the authors presented a lexical matching baseline which is combined with the textual entailment recognition system BIUTEE (.", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.612692395846049}, {"text": "BIUTEE", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.7442312836647034}]}, {"text": "In this paper we develop an approach based on lexical matching which we extend by taking into account the type of the question and coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.9330750703811646}]}, {"text": "These components improve the performance on questions that are difficult to handle with pure lexical matching.", "labels": [], "entities": []}, {"text": "When combined with BIUTEE, we achieved 74.27% accuracy on MC160 and 65.96% on MC500, which are significantly better than those reported by.", "labels": [], "entities": [{"text": "BIUTEE", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.962455153465271}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9996968507766724}, {"text": "MC160", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.9231727719306946}, {"text": "MC500", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.9117595553398132}]}, {"text": "Despite the simplicity of our approach, these results are comparable with the recent machine learning-based approaches proposed by, and.", "labels": [], "entities": []}, {"text": "Furthermore, we examine the types of questions and answers in the two datasets.", "labels": [], "entities": []}, {"text": "We argue that some types are relatively simple to answer, partly due to the limited vocabulary used, which explains why simple lexical matching methods can perform well.", "labels": [], "entities": []}, {"text": "On the other hand, some questions require understanding of higher level concepts such as those of the story and its characters, and/or require inference.", "labels": [], "entities": []}, {"text": "This is still beyond the scope of current NLP systems.", "labels": [], "entities": []}, {"text": "However, we believe our analysis will be useful in developing new methods and datasets for the task.", "labels": [], "entities": []}, {"text": "To that extent, we will make our code and analysis publicly available.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance on the MC160 and MC500  test sets, including the results of all previous work.  * denotes statistically significant (p < 0.05) im- provement using McNemar's test, with respect to  the MSR baseline (SW+D)", "labels": [], "entities": [{"text": "MC160 and MC500  test sets", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.8210896492004395}, {"text": "statistically significant (p < 0.05) im- provement", "start_pos": 112, "end_pos": 162, "type": "METRIC", "confidence": 0.7400934398174286}, {"text": "McNemar's test", "start_pos": 169, "end_pos": 183, "type": "DATASET", "confidence": 0.8297447760899862}]}]}