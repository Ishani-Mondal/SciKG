{"title": [{"text": "Fine-grained Opinion Mining with Recurrent Neural Networks and Word Embeddings", "labels": [], "entities": [{"text": "Fine-grained Opinion Mining", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6538742283980051}]}], "abstractContent": [{"text": "The tasks in fine-grained opinion mining can be regarded as either a token-level sequence labeling problem or as a semantic compositional task.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.6905040293931961}, {"text": "token-level sequence labeling", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.647270123163859}]}, {"text": "We propose a general class of discriminative models based on recurrent neural networks (RNNs) and word embeddings that can be successfully applied to such tasks without any task-specific feature engineering effort.", "labels": [], "entities": []}, {"text": "Our experimental results on the task of opinion target identification show that RNNs, without using any hand-crafted features, outperform feature-rich CRF-based models.", "labels": [], "entities": [{"text": "opinion target identification", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.6351823508739471}]}, {"text": "Our framework is flexible, allows us to incorporate other linguistic features, and achieves results that rival the top performing systems in SemEval-2014.", "labels": [], "entities": []}], "introductionContent": [{"text": "Fine-grained opinion mining involves identifying the opinion holder who expresses the opinion, detecting opinion expressions, measuring their intensity and sentiment, and identifying the target or aspect of the opinion ( ).", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7618986666202545}]}, {"text": "For example, in the sentence \"John says, the hard disk is very noisy\", John, the opinion holder, expresses a very negative (i.e., sentiment with intensity) opinion towards the target \"hard disk\" using the opinionated expression \"very noisy\".", "labels": [], "entities": []}, {"text": "A number of NLP applications can benefit from fine-grained opinion mining including opinion summarization and opinion-oriented question answering.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.6967485696077347}, {"text": "opinion summarization", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.8056323826313019}, {"text": "opinion-oriented question answering", "start_pos": 110, "end_pos": 145, "type": "TASK", "confidence": 0.6018359859784445}]}, {"text": "The tasks in fine-grained opinion mining can be regarded as either a token-level sequence labeling problem or as a semantic compositional task at the sequence (e.g., phrase) level.", "labels": [], "entities": [{"text": "fine-grained opinion mining", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6461131771405538}, {"text": "token-level sequence labeling", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.6417384346326193}]}, {"text": "For example, identifying opinion holders, opinion expressions and opinion targets can be formulated as a token-level sequence tagging problem, where the task is to: An example sentence annotated with BIO labels for opinion target (TARG tags) and for opinion expression (EXPR tags) extraction.", "labels": [], "entities": [{"text": "token-level sequence tagging", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.6741505066553751}, {"text": "BIO", "start_pos": 200, "end_pos": 203, "type": "METRIC", "confidence": 0.9652270674705505}, {"text": "opinion expression (EXPR tags) extraction", "start_pos": 250, "end_pos": 291, "type": "TASK", "confidence": 0.640679304088865}]}, {"text": "label each word in a sentence using the conventional BIO tagging scheme.", "labels": [], "entities": [{"text": "BIO tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.7221273183822632}]}, {"text": "For example, shows a sentence tagged with BIO scheme for opinion target (middle row) and for opinion expression (bottom row) identification tasks.", "labels": [], "entities": [{"text": "BIO", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9677176475524902}, {"text": "opinion expression (bottom row) identification tasks", "start_pos": 93, "end_pos": 145, "type": "TASK", "confidence": 0.6547077000141144}]}, {"text": "On the other hand, characterizing intensity and sentiment of an opinionated expression can be regarded as a semantic compositional problem, where the task is to aggregate vector representations of tokens in a meaningful way and later use them for sentiment classification.", "labels": [], "entities": [{"text": "characterizing intensity", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.78861203789711}, {"text": "sentiment classification", "start_pos": 247, "end_pos": 271, "type": "TASK", "confidence": 0.8135946691036224}]}, {"text": "Conditional random fields (CRFs) () have been quite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.7174694389104843}, {"text": "opinion expression extraction", "start_pos": 118, "end_pos": 147, "type": "TASK", "confidence": 0.7983484268188477}]}, {"text": "The state-of-the-art model for opinion target extraction is also based on a CRF ().", "labels": [], "entities": [{"text": "opinion target extraction", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.6412355999151865}]}, {"text": "However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand.", "labels": [], "entities": [{"text": "CRFs", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.9564611315727234}]}, {"text": "An alternative approach of deep learning automatically learns latent features as distributed vectors and have recently been shown to outperform CRFs on similar tasks.", "labels": [], "entities": []}, {"text": "For example, apply deep recurrent neural networks (RNNs) to extract opinion expressions from sentences and show that RNNs outperform CRFs.", "labels": [], "entities": []}, {"text": "propose recursive neural networks fora semantic compositional task to identify the sentiments of phrases and sentences hierarchically using the syntactic parse trees.", "labels": [], "entities": []}, {"text": "Meanwhile, recent advances in word embed-ding induction methods) have benefited researchers in two ways: (i) they have contributed to significant gains when used as extra word features in existing NLP systems, and (ii) they have enabled more effective training of RNNs by providing compact input representations of the words.", "labels": [], "entities": [{"text": "word embed-ding induction", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.6786374449729919}]}, {"text": "Motivated by the recent success of deep learning, in this paper we propose a general class of models based on RNN architecture and word embeddings, that can be successfully applied to finegrained opinion mining tasks without any taskspecific feature engineering effort.", "labels": [], "entities": [{"text": "finegrained opinion mining tasks", "start_pos": 184, "end_pos": 216, "type": "TASK", "confidence": 0.709807500243187}]}, {"text": "We experiment with several important RNN architectures including Elman-RNN, Jordan-RNN, long short term memory (LSTM) and their variations.", "labels": [], "entities": []}, {"text": "We acquire pre-trained word embeddings from several external sources to give better initialization to our RNN models.", "labels": [], "entities": []}, {"text": "The RNN models then fine-tune the word vectors during training to learn task-specific embeddings.", "labels": [], "entities": []}, {"text": "We also present an architecture to incorporate other linguistic features into RNNs.", "labels": [], "entities": []}, {"text": "Our results on the task of opinion target extraction show that word embeddings improve the performance of state-of-the-art CRF models, when included as additional features.", "labels": [], "entities": [{"text": "opinion target extraction", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.6748375296592712}]}, {"text": "They also improve RNNs when used as pre-trained word vectors and fine-tuning them on the task gives the best results.", "labels": [], "entities": []}, {"text": "A comparison between models demonstrates that RNNs outperform CRFs, even when they use word embeddings as the only features.", "labels": [], "entities": []}, {"text": "Incorporating simple linguistic features into RNNs improves the performance even further.", "labels": [], "entities": []}, {"text": "Our best results with LSTM RNN outperform the top performing system on the Laptop dataset and achieve the second best on the Restaurant dataset in SemEval-2014.", "labels": [], "entities": [{"text": "LSTM RNN", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.6212460398674011}, {"text": "Laptop dataset", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.9418397545814514}, {"text": "Restaurant dataset in SemEval-2014", "start_pos": 125, "end_pos": 159, "type": "DATASET", "confidence": 0.8394661992788315}]}, {"text": "We make our source code available.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, after discussing related work in Section 2, we present our RNN models in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, we briefly describe the pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "The experiments and analysis of results are presented in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we summarize our contributions with future directions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present our experimental settings and results for the task of opinion target extraction from customer reviews.", "labels": [], "entities": [{"text": "opinion target extraction from customer reviews", "start_pos": 82, "end_pos": 129, "type": "TASK", "confidence": 0.7568492641051611}]}, {"text": "Datasets: In our experiments, we use the two review datasets provided by the SemEval-2014 task 4: aspect-based sentiment analysis evaluation campaign (), namely the Laptop and the Restaurant datasets.", "labels": [], "entities": [{"text": "SemEval-2014 task 4: aspect-based sentiment analysis evaluation", "start_pos": 77, "end_pos": 140, "type": "TASK", "confidence": 0.5332294851541519}, {"text": "Restaurant datasets", "start_pos": 180, "end_pos": 199, "type": "DATASET", "confidence": 0.805606335401535}]}, {"text": "shows some basic statistics about the datasets.", "labels": [], "entities": []}, {"text": "The majority of aspect terms have only one word, while about one third of them have multiple words.", "labels": [], "entities": []}, {"text": "In both datasets, some sentences have no aspect terms and some have more than one aspect terms.", "labels": [], "entities": []}, {"text": "We use the standard train:test split to compare our results with the SemEval best systems.", "labels": [], "entities": []}, {"text": "In addition, we show a more general performance of our models on the two datasets based on 10-fold cross validation.", "labels": [], "entities": []}, {"text": "Evaluation Metric: The evaluation metric measures the standard precision, recall and F 1 score based on exact matches.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9839320182800293}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9995843768119812}, {"text": "F 1 score", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9928346474965414}]}, {"text": "This means that a candidate aspect term is considered to be correct only if it exactly matches with the aspect term annotated by the human.", "labels": [], "entities": []}, {"text": "In all our experiments when comparing two models, we use paired t-test on the F 1 scores to measure statistical significance and report the corresponding p-value.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9415273269017538}]}], "tableCaptions": [{"text": " Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9627196341753006}]}, {"text": " Table 4: 10-fold cross validation results of the models on the two datasets. Elman-and LSTM-RNNs are  trained using SENNA embeddings.", "labels": [], "entities": [{"text": "Elman-and LSTM-RNNs", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.786436140537262}]}, {"text": " Table 5: Effects of fine-tuning in Elman-RNN and Jordan-RNN.", "labels": [], "entities": []}]}