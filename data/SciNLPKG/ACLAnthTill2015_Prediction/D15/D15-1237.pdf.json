{"title": [{"text": "WIKIQA: A Challenge Dataset for Open-Domain Question Answering", "labels": [], "entities": [{"text": "Open-Domain Question Answering", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.5800549884637197}]}], "abstractContent": [{"text": "We describe the WIKIQA dataset, anew publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering.", "labels": [], "entities": [{"text": "WIKIQA dataset", "start_pos": 16, "end_pos": 30, "type": "DATASET", "confidence": 0.9598976671695709}, {"text": "open-domain question answering", "start_pos": 132, "end_pos": 162, "type": "TASK", "confidence": 0.5954102675120035}]}, {"text": "Most previous work on answer sentence selection focuses on a dataset created using the TREC-QA data, which includes editor-generated questions and candidate answer sentences selected by matching content words in the question.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.8144454956054688}, {"text": "TREC-QA data", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.8695957660675049}]}, {"text": "WIKIQA is constructed using a more natural process and is more than an order of magnitude larger than the previous dataset.", "labels": [], "entities": [{"text": "WIKIQA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9632896184921265}]}, {"text": "In addition, the WIKIQA dataset also includes questions for which there are no correct sentences, enabling researchers to work on answer triggering, a critical component in any QA system.", "labels": [], "entities": [{"text": "WIKIQA dataset", "start_pos": 17, "end_pos": 31, "type": "DATASET", "confidence": 0.9659753441810608}, {"text": "answer triggering", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.7801812589168549}]}, {"text": "We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the WIKIQA dataset.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.7537537614504496}, {"text": "answer triggering", "start_pos": 149, "end_pos": 166, "type": "TASK", "confidence": 0.806639164686203}, {"text": "WIKIQA dataset", "start_pos": 177, "end_pos": 191, "type": "DATASET", "confidence": 0.9814011454582214}]}], "introductionContent": [{"text": "Answer sentence selection is a crucial subtask of the open-domain question answering (QA) problem, with the goal of extracting answers from a set of pre-selected sentences.", "labels": [], "entities": [{"text": "Answer sentence selection", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8520742654800415}, {"text": "open-domain question answering (QA) problem", "start_pos": 54, "end_pos": 97, "type": "TASK", "confidence": 0.8296599558421544}]}, {"text": "In order to conduct research on this important problem, created a dataset, which we refer to by QASENT, based on the TREC-QA data.", "labels": [], "entities": [{"text": "TREC-QA data", "start_pos": 117, "end_pos": 129, "type": "DATASET", "confidence": 0.9082335531711578}]}, {"text": "The QASENT dataset chose questions in TREC 8-13 QA tracks and selected sentences that share one or more non-stopwords from the questions.", "labels": [], "entities": [{"text": "QASENT dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.710318386554718}, {"text": "TREC 8-13 QA tracks", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.8620053380727768}]}, {"text": "Although QASENT has since * Work conducted while interning at Microsoft Research.", "labels": [], "entities": []}, {"text": "become the benchmark dataset for the answer selection problem, its creation process actually introduces a strong bias in the types of answers that are included.", "labels": [], "entities": [{"text": "answer selection problem", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.919458250204722}]}, {"text": "The following example illustrates an answer that does not share any content words with the question and would not be selected: Q: How did Seminole war end?", "labels": [], "entities": []}, {"text": "A: Ultimately, the Spanish Crown ceded the colony to United States rule.", "labels": [], "entities": []}, {"text": "One significant concern with this approach is that the lexical overlap will make sentence selection easier for the QASENT dataset and might inflate the performance of existing systems in more natural settings.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7397310435771942}, {"text": "QASENT dataset", "start_pos": 115, "end_pos": 129, "type": "DATASET", "confidence": 0.7683505415916443}]}, {"text": "For instance, find that simple word matching methods outperform many sophisticated approaches on the dataset.", "labels": [], "entities": [{"text": "word matching", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.7775753438472748}]}, {"text": "We explore this possibility in Section 3.", "labels": [], "entities": []}, {"text": "A second, more subtle challenge for question answering is that it normally assumes that there is at least one correct answer for each question in the candidate sentences.", "labels": [], "entities": [{"text": "question answering", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8792277574539185}]}, {"text": "During the data construction procedures, all the questions without correct answers are manually discarded.", "labels": [], "entities": [{"text": "data construction", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7576490640640259}]}, {"text": "We address anew challenge of answer triggering, an important component in QA systems, where the goal is to detect whether there exist correct answers in the set of candidate sentences for the question, and return a correct answer if there exists such one.", "labels": [], "entities": [{"text": "answer triggering", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.8356803953647614}]}, {"text": "We present WIKIQA, a dataset for opendomain question answering.", "labels": [], "entities": [{"text": "WIKIQA", "start_pos": 11, "end_pos": 17, "type": "DATASET", "confidence": 0.8826128244400024}, {"text": "opendomain question answering", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.6346863110860189}]}, {"text": "The dataset contains 3,047 questions originally sampled from Bing query logs.", "labels": [], "entities": []}, {"text": "Based on the user clicks, each question is associated with a Wikipedia page presumed to be the topic of the question.", "labels": [], "entities": []}, {"text": "In order to eliminate answer sentence biases caused by keyword matching, we consider all the sentences in the summary paragraph of the page as the candidate answer sentences, with labels on whether the sentence is a correct answer to the question provided by crowdsourcing workers.", "labels": [], "entities": [{"text": "keyword matching", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.7154177278280258}]}, {"text": "Among these questions, about one-third of them contain correct answers in the answer sentence set.", "labels": [], "entities": []}, {"text": "We implement several strong baselines to study model behaviors in the two datasets, including two previous state-of-the-art systems) on the QASENT dataset as well as simple lexical matching methods.", "labels": [], "entities": [{"text": "QASENT dataset", "start_pos": 140, "end_pos": 154, "type": "DATASET", "confidence": 0.9388765692710876}]}, {"text": "The results show that lexical semantic methods yield better performance than sentence semantic models on QASENT, while sentence semantic approaches (e.g., convolutional neural networks) outperform lexical semantic models on WIKIQA.", "labels": [], "entities": [{"text": "WIKIQA", "start_pos": 224, "end_pos": 230, "type": "DATASET", "confidence": 0.9423624277114868}]}, {"text": "We propose to evaluate answer triggering using question-level precision, recall and F 1 scores.", "labels": [], "entities": [{"text": "answer triggering", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.8967249989509583}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9455878734588623}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9995728135108948}, {"text": "F 1 scores", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9859559337298075}]}, {"text": "The best F 1 scores are slightly above 30%, which suggests a large room for improvement.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9671695828437805}]}], "datasetContent": [{"text": "In this section, we describe the process of creating our WIKIQA dataset in detail, as well as some comparisons to the QASENT dataset.", "labels": [], "entities": [{"text": "WIKIQA dataset", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.9469223916530609}, {"text": "QASENT dataset", "start_pos": 118, "end_pos": 132, "type": "DATASET", "confidence": 0.949370950460434}]}, {"text": "Many systems have been proposed and tested on the QASENT dataset, including lexical semantic models () and sentence semantic models ().", "labels": [], "entities": [{"text": "QASENT dataset", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.8786549866199493}]}, {"text": "We investigate the performance of several systems on WIKIQA and QASENT.", "labels": [], "entities": [{"text": "WIKIQA", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.9200135469436646}, {"text": "QASENT", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.8244649171829224}]}, {"text": "As discussed in Section 2, WIKIQA offers us the opportunity to evaluate QA systems on answer triggering.", "labels": [], "entities": [{"text": "WIKIQA", "start_pos": 27, "end_pos": 33, "type": "DATASET", "confidence": 0.8448886871337891}, {"text": "answer triggering", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.8841775357723236}]}, {"text": "We propose simple metrics and perform a feature study on the new task.", "labels": [], "entities": []}, {"text": "Finally, we include some error analysis and discussion at the end of this section.", "labels": [], "entities": []}, {"text": "The task of answer sentence selection assumes that there exists at least one correct answer in the candidate answer sentence set.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.7662424445152283}]}, {"text": "Although the assumption simplifies the problem of question answering, it is unrealistic for practical QA systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8729327023029327}]}, {"text": "Modern QA systems rely on an independent component to pre-select candidate answer sentences, which utilizes various signals such as lexical matching and user behaviors.", "labels": [], "entities": []}, {"text": "However, the candidate sentences: Evaluation of answer triggering on the WIKIQA dataset.", "labels": [], "entities": [{"text": "answer triggering", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7332698404788971}, {"text": "WIKIQA dataset", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.981476902961731}]}, {"text": "Question-level precision, recall and F 1 scores are reported. are not guaranteed to contain the correct answers, no matter what kinds of pre-selection components are employed.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9906028509140015}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9995802044868469}, {"text": "F 1 scores", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9895050128300985}]}, {"text": "We propose the answer triggering task, anew challenge for the question answering problem, which requires QA systems to: (1) detect whether there is at least one correct answer in the set of candidate sentences for the question; (2) if yes, select one of the correct answer sentences from the candidate sentence set.", "labels": [], "entities": [{"text": "answer triggering", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7731496691703796}, {"text": "question answering", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.8244431018829346}]}, {"text": "Previous work adopts MAP and MRR to evaluate the performance of a QA system on answer sentence selection.", "labels": [], "entities": [{"text": "MAP", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.896853506565094}, {"text": "MRR", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.7993205189704895}, {"text": "answer sentence selection", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.7801410555839539}]}, {"text": "Both metrics evaluate the relative ranks of correct answers in the candidate sentences of a question, and hence are not suitable for evaluating the task of answer triggering.", "labels": [], "entities": [{"text": "answer triggering", "start_pos": 156, "end_pos": 173, "type": "TASK", "confidence": 0.7120469063520432}]}, {"text": "We need metrics that consider both the presence of answers with respect to a question and the correctness of system predictions.", "labels": [], "entities": []}, {"text": "We employ precision, recall and F 1 scores for answer triggering, at the question level.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9996434450149536}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9992634654045105}, {"text": "F 1 scores", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9860780636469523}, {"text": "answer triggering", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.8429901301860809}]}, {"text": "In particular, we compute these metrics by aggregating all the candidate sentences of a question.", "labels": [], "entities": []}, {"text": "A question is treated as a positive case only if it contains one or more correct answer sentences in its candidate sentence pool.", "labels": [], "entities": []}, {"text": "For the prediction of a question, we only consider the sentence in the candidate set that has the highest model score.", "labels": [], "entities": [{"text": "prediction of a question", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.8794417977333069}]}, {"text": "If the score is above a predefined threshold and the sentence is labeled as a correct answer to the question, then it means that the prediction is correct and the question is answered correctly.", "labels": [], "entities": []}, {"text": "We evaluate the best system CNN-Cnt on the task of answer triggering, and the results are shown in.", "labels": [], "entities": [{"text": "answer triggering", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.8986348211765289}]}, {"text": "We tune the model scores for making predictions with respect to F 1 scores on the dev set, due to the highly skewed class distribution in training data.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9526445666948954}]}, {"text": "The absolute F 1 scores are relative low, which suggests a large room for improvement.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9577482541402181}]}, {"text": "We further study three additional features: the length of question (QLen), the length of sentence (SLen), and the class of the question (QClass).", "labels": [], "entities": [{"text": "length of sentence (SLen)", "start_pos": 79, "end_pos": 104, "type": "METRIC", "confidence": 0.8351006905237833}]}, {"text": "The motivation for adding these features is to capture the hardness of the question and comprehensiveness of the sentence.", "labels": [], "entities": []}, {"text": "Note that the two question features have no effects on MAP and MRR.", "labels": [], "entities": [{"text": "MAP", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.7183806300163269}]}, {"text": "As shown in, the question-level F 1 score is substantially improved by adding a simple QLen feature.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9136625130971273}]}, {"text": "This suggests that designing features to capture question information is very important for this task, which has been ignored in the past.", "labels": [], "entities": []}, {"text": "SLen features also give a small improvement in the performance, and QClass feature has slightly negative influence on the results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the QASENT dataset.", "labels": [], "entities": [{"text": "QASENT dataset", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.8614043593406677}]}, {"text": " Table 2: Statistics of the WIKIQA dataset.", "labels": [], "entities": [{"text": "WIKIQA dataset", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9669350981712341}]}, {"text": " Table 4: Baseline results on both QASENT and  WIKIQA datasets. Questions without correct an- swers in the candidate sentences are removed in  the WIKIQA dataset. The best results are in bold.", "labels": [], "entities": [{"text": "QASENT", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.6727631092071533}, {"text": "WIKIQA datasets", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9611563682556152}, {"text": "WIKIQA dataset", "start_pos": 147, "end_pos": 161, "type": "DATASET", "confidence": 0.9870188236236572}]}, {"text": " Table 5: Evaluation of answer triggering on the  WIKIQA dataset. Question-level precision, recall  and F 1 scores are reported.", "labels": [], "entities": [{"text": "answer triggering", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7840652763843536}, {"text": "WIKIQA dataset", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9807646870613098}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9376237988471985}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.999351441860199}, {"text": "F 1 scores", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9888402422269186}]}]}