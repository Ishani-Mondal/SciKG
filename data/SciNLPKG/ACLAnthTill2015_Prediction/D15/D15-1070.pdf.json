{"title": [{"text": "Image-Mediated Learning for Zero-Shot Cross-Lingual Document Retrieval", "labels": [], "entities": [{"text": "Cross-Lingual Document Retrieval", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.7947195569674174}]}], "abstractContent": [{"text": "We propose an image-mediated learning approach for cross-lingual document retrieval where no or only a few parallel corpora are available.", "labels": [], "entities": [{"text": "cross-lingual document retrieval", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.6684790054957072}]}, {"text": "Using the images in image-text documents of each language as the hub, we derive a common semantic subspace bridging two languages by means of generalized canonical correlation analysis.", "labels": [], "entities": []}, {"text": "For the purpose of evaluation , we create and release anew document dataset consisting of three types of data (English text, Japanese text, and images).", "labels": [], "entities": []}, {"text": "Our approach substantially enhances retrieval accuracy in zero-shot and few-shot scenarios where text-to-text examples are scarce.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9464409351348877}]}], "introductionContent": [{"text": "Cross-lingual document retrieval (CLDR) is the task of finding relevant documents in one language given a query document in another language.", "labels": [], "entities": [{"text": "Cross-lingual document retrieval (CLDR)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7881307850281397}]}, {"text": "While sufficiently large-scale corpora are critical for parallel corpus-based learning methods, manually creating corpora requires huge human effort and is unrealistic in many cases.", "labels": [], "entities": []}, {"text": "A straightforward approach is to crawl bilingual documents from the Web for use as training data.", "labels": [], "entities": []}, {"text": "However, because most documents on the Web are written in one language, it is not always easy to collect a sufficient number of multilingual documents, especially those involving minor languages.", "labels": [], "entities": []}, {"text": "Let us consider the multimedia information in documents.", "labels": [], "entities": []}, {"text": "We can, for example, find abundant pairings of text and images, e.g., text with the ALT property of <IMG> tags in HTML, text with photos posted to social networking sites, and articles on Web news posted with images.", "labels": [], "entities": []}, {"text": "Unlike text, an image is a universal representation; we can easily understand the semantic Our idea is to learn the relation between two languages indirectly by using images attached to text.", "labels": [], "entities": []}, {"text": "If two documents written in different languages include images with similar image features, it is likely that the texts contained in the two documents are similar.", "labels": [], "entities": []}, {"text": "Based on this idea, we seek the relation of texts written in different languages mediated by the similarity between images.", "labels": [], "entities": []}, {"text": "content of images regardless of our mother tongue.", "labels": [], "entities": []}, {"text": "Motivated by this observation, we expect that we can learn the relation of two languages indirectly through images, even if we do not have sufficient bilingual text pairs).", "labels": [], "entities": []}, {"text": "Generally, traditional image recognition techniques (or image features) are very poor compared with those in the natural language processing field.", "labels": [], "entities": [{"text": "image recognition", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7810054123401642}]}, {"text": "In recent years, however, deep learning has resulted in a breakthrough in visual recognition and dramatically improved image recognition accuracy in generic domains, which is rapidly approaching human recognition levels.", "labels": [], "entities": [{"text": "visual recognition", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7466063797473907}, {"text": "image recognition", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7347442209720612}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.8867540955543518}]}, {"text": "We expect that these state-of-the-art image recognition technologies can effectively assist CLDR tasks.", "labels": [], "entities": [{"text": "image recognition", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7329979836940765}, {"text": "CLDR", "start_pos": 92, "end_pos": 96, "type": "TASK", "confidence": 0.9037997126579285}]}, {"text": "We show that hub images enable zero-shot training of CLDR systems and improve retrieval accuracy given only a few parallel text samples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.977846622467041}]}], "datasetContent": [{"text": "The UIUC Pascal Sentence Dataset (Rashtchian et al., 2010) contains 1000 images, each of which is annotated with five English sentences describing its content.", "labels": [], "entities": [{"text": "UIUC Pascal Sentence Dataset (Rashtchian et al., 2010)", "start_pos": 4, "end_pos": 58, "type": "DATASET", "confidence": 0.9307441332123496}]}, {"text": "This dataset was originally created for the study of sentence generation from images, which is one of the current hot topics in computer vision.", "labels": [], "entities": [{"text": "sentence generation from images", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.8282312154769897}]}, {"text": "To establish anew benchmark dataset for image-mediated CLDR, we included a Japanese translation for each English sentence provided by professional translators 1 , as shown in.", "labels": [], "entities": []}, {"text": "In this experiment, we bundled the five sentences attached to each image for use as one text document.", "labels": [], "entities": []}, {"text": "Therefore, in our setup, each of the 1000 documents in the dataset consists of three items: an image, and the corresponding English and Japanese text.", "labels": [], "entities": []}, {"text": "-A family on a boat with across on a river -A happy couple with a young child wearing a life preserver sitting on a boat.", "labels": [], "entities": []}, {"text": "-A man, a woman, and a child sit on boat with a large cross on it.", "labels": [], "entities": []}, {"text": "-A man, women and small child sitting on top of a boat moving along the river.", "labels": [], "entities": []}, {"text": "-Family of three sitting on deck, child wearing red vest, brush and shoes are seen in the foreground.", "labels": [], "entities": []}, {"text": "We randomly sampled data from the dataset for each division in without any overlap; we ignored the modality of each document that was not available in each data division (e.g., Japanese text in [train-E/I]).", "labels": [], "entities": []}, {"text": "We ran experiments with varying sample sizes for [train-E/I] and [train-I/J], that is, 100, 200, 300, and 400.", "labels": [], "entities": []}, {"text": "Furthermore, we gradually increased the number of [train-E/J] samples from 0 to 100 to emulate the few-shot learning scenario.", "labels": [], "entities": []}, {"text": "The size of the test data [test-E/J] was fixed at 100.", "labels": [], "entities": []}, {"text": "Following this setup, we performed imagemediated CLDR based on GCCA, and compared the results with those obtained by standard CLDR using only [train-E/J] data with CCA.", "labels": [], "entities": [{"text": "GCCA", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.8143641352653503}]}, {"text": "We evaluated the performance with respect to the top-1 Japanese to English retrieval accuracy in the test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.7477036118507385}]}, {"text": "Given that we used 100 test samples, the chance rate was 1%.", "labels": [], "entities": [{"text": "chance rate", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9774095416069031}]}, {"text": "For each run, we conducted 50 trials randomly replacing data and used the average score.", "labels": [], "entities": []}, {"text": "All features were compressed into 100 dimensions via PCA and \u03b1 was set to 0.01.", "labels": [], "entities": []}, {"text": "The experimental results, illustrated in, clearly show that better accuracy is obtained with a greater number of text-image data in both zeroshot and few-shot scenarios.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9992659687995911}]}, {"text": "We can expect even better zero-shot accuracy with more text-image data, although, we cannot increase [train-E/I] and [train-I/J] more than 400 each in the current setup because of the restricted dataset size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9828238487243652}]}, {"text": "We summarized results in zero-shot scenario in in several cases.", "labels": [], "entities": []}, {"text": "Although both GCCA and CCA show improved performance as the sample size of [train-E/J] increases, not surprisingly, GCCA is gradually overtaken by CCA when we have enough samples to learn the relevance between English and Japanese texts directly.", "labels": [], "entities": []}, {"text": "However, accuracies of image-mediated learning in the cases when [train-E/J] is scarce are higher than CCA baseline.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9817788600921631}]}, {"text": "Hence, we confirmed that the imagemediated model is also effective in the few-shot learning scenario.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy of zero-shot learning. Image  features are extracted from GoogLeNet and both  the bag-of-words (BoW) and the TF-IDF model  are used as text features.", "labels": [], "entities": [{"text": "GoogLeNet", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.9401642680168152}]}]}