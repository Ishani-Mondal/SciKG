{"title": [{"text": "Discriminative Neural Sentence Modeling by Tree-Based Convolution", "labels": [], "entities": [{"text": "Discriminative Neural Sentence Modeling", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7070591524243355}]}], "abstractContent": [{"text": "This paper proposes a tree-based con-volutional neural network (TBCNN) for discriminative sentence modeling.", "labels": [], "entities": [{"text": "discriminative sentence modeling", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.629032293955485}]}, {"text": "Our model leverages either constituency trees or dependency trees of sentences.", "labels": [], "entities": []}, {"text": "The tree-based convolution process extracts sentences structural features, which are then aggregated by max pooling.", "labels": [], "entities": []}, {"text": "Such architecture allows short propagation paths between the output layer and underlying feature detectors, enabling effective structural feature learning and extraction.", "labels": [], "entities": [{"text": "structural feature learning and extraction", "start_pos": 127, "end_pos": 169, "type": "TASK", "confidence": 0.7366622805595398}]}, {"text": "We evaluate our models on two tasks: sentiment analysis and question classification.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.9744580090045929}, {"text": "question classification", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.8099459409713745}]}, {"text": "In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering.", "labels": [], "entities": []}, {"text": "We also make efforts to visualize the tree-based convo-lution process, shedding light on how our models work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment).", "labels": [], "entities": [{"text": "Discriminative sentence modeling", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8093557556470236}]}, {"text": "It is related to various tasks of interest, and has attracted much attention in the NLP community (.", "labels": [], "entities": []}, {"text": "Feature engineering-for example, n-gram features (), dependency subtree features (, or more dedicated ones)-can play an important role in modeling sentences.", "labels": [], "entities": []}, {"text": "Kernel machines, e.g., SVM, are exploited in and by specifying a certain measure of similarity between sentences, without explicit feature representation.", "labels": [], "entities": []}, {"text": "Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential. and propose unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors in a meaning space.", "labels": [], "entities": []}, {"text": "extend such approaches to learn sentences' and paragraphs' representations.", "labels": [], "entities": []}, {"text": "Compared with human engineering, neural networks serve as away of automatic feature learning (.", "labels": [], "entities": []}, {"text": "Two widely used neural sentence models are convolutional neural networks (CNNs) and recursive neural networks (RNNs).", "labels": [], "entities": []}, {"text": "CNNs can extract words' neighboring features effectively with short propagation paths, but they do not capture inherent sentence structures (e.g., parse trees).", "labels": [], "entities": []}, {"text": "RNNs encode, to some extent, structural information by recursive semantic composition along a parse tree.", "labels": [], "entities": []}, {"text": "However, they may have difficulties in learning deep dependencies because of long propagation paths (.", "labels": [], "entities": []}, {"text": "(CNNs/RNNs and a variant, recurrent networks, will be reviewed in A curious question is whether we can combine the advantages of CNNs and RNNs, i.e., whether we can exploit sentence structures (like RNNs) effectively with short propagation paths (like CNNs).", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel neural architecture for discriminative sentence modeling, called the Tree-Based Convolutional Neural Network (TBCNN).", "labels": [], "entities": [{"text": "discriminative sentence modeling", "start_pos": 58, "end_pos": 90, "type": "TASK", "confidence": 0.6565323968728384}]}, {"text": "Our models can leverage different sentence parse trees, e.g., constituency trees and dependency trees.", "labels": [], "entities": [{"text": "sentence parse", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7511727213859558}]}, {"text": "The model variants are denoted as c-TBCNN and d-TBCNN, respectively.", "labels": [], "entities": []}, {"text": "The idea of tree-based convolution is to apply a set of subtree feature detectors, sliding over the entire parse tree of a sentence; then pooling aggregates these extracted feature vectors by taking the maximum value in each dimension.", "labels": [], "entities": []}, {"text": "One merit of such architecture is that all features, along the tree, have short propagation paths to the output layer, and hence structural information can be learned effectively.", "labels": [], "entities": []}, {"text": "TBCNNs are evaluated on two tasks, sentiment analysis and question classification; our models have outperformed previous state-of-the-art results in both experiments.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.9717959463596344}, {"text": "question classification", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.8213250935077667}]}, {"text": "To understand how TBCNNs work, we also visualize the network by plotting the convolution process.", "labels": [], "entities": []}, {"text": "We make our code and results available on our project website.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our models with two tasks, sentiment analysis and question classification.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.97271928191185}, {"text": "question classification", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.8165644407272339}]}, {"text": "We also conduct quantitative and qualitative model analysis in Subsection 4.3.", "labels": [], "entities": []}, {"text": "Sentiment analysis is a widely studied task for discriminative sentence modeling.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9479508399963379}, {"text": "discriminative sentence modeling", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.6552439431349436}]}, {"text": "The Stanford sentiment treebank consists of more than 10,000 movie reviews.", "labels": [], "entities": [{"text": "Stanford sentiment treebank", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.9649998744328817}]}, {"text": "Two settings are considered for sentiment prediction: (1) fine-grained classification with 5 labels (strongly positive, positive, neutral, negative, and strongly negative), and (2) coarse-gained polarity classification with 2 labels (positive versus negative).", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.9732126593589783}]}, {"text": "Some examples are shown in.", "labels": [], "entities": []}, {"text": "We use the standard split for training, validating, and testing, containing 8544/1101/2210 sentences for 5-class prediction.", "labels": [], "entities": [{"text": "5-class prediction", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.6114712953567505}]}, {"text": "Binary classification does not contain the neutral class.", "labels": [], "entities": [{"text": "Binary classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8218428194522858}]}, {"text": "In the dataset, phrases (sub-sentences) are also tagged with sentiment labels.", "labels": [], "entities": []}, {"text": "RNNs deal with them naturally during the recursive process.", "labels": [], "entities": []}, {"text": "We regard sub-sentences as individual samples during training, like and.", "labels": [], "entities": []}, {"text": "The training set therefore has more than 150,000 entries in total.", "labels": [], "entities": []}, {"text": "For validating and testing, only whole sentences (root labels) are considered in our experiments.", "labels": [], "entities": []}, {"text": "Both c-TBCNN and d-TBCNN use the Stanford parser for data preprocessing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, \" \u2020\" remarks indicate  that the network is transferred directly from that of 5-class.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9936221241950989}, {"text": "sentiment prediction", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.9150974452495575}, {"text": "2-class prediction", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7328726053237915}]}, {"text": " Table 4: Accuracies of different pooling methods,  averaged over 5 random initializations. We chose  sensible hyperparameters manually in advance to  make a fair comparison. This leads to performance  degradation (1-2%) vis-a-vis Table 2.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9578559398651123}]}]}