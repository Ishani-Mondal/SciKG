{"title": [{"text": "Convolutional Sentence Kernel from Word Embeddings for Short Text Categorization", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces a convolutional sentence kernel based on word embeddings.", "labels": [], "entities": []}, {"text": "Our kernel overcomes the sparsity issue that arises when classifying short documents or in case of little training data.", "labels": [], "entities": []}, {"text": "Experiments on six sentence datasets showed statistically significant higher accuracy over the standard linear kernel with n-gram features and other proposed models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9991973042488098}]}], "introductionContent": [{"text": "With the proliferation of text data available online, text categorization emerged as a prominent research topic.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8196798264980316}]}, {"text": "Traditionally, words (unigrams) and phrases (n-grams) have been considered as document features and subsequently fed to a classifier such as an SVM.", "labels": [], "entities": []}, {"text": "In the SVM dual formulation that relies on kernels, i. e. similarity measures between documents, a linear kernel can be interpreted as the number of exact matching n-grams between two documents.", "labels": [], "entities": []}, {"text": "Consequently, for short documents or when little training data is available, sparsity issues due to word synonymy arise, e. g., the sentences 'John likes hot beverages' and 'John loves warm drinks' have little overlap and therefore low linear kernel value (only 1) in the n-gram feature space, even with dependency tree representations and downward paths for n-grams as illustrated in.", "labels": [], "entities": []}, {"text": "We propose to relax the exact matching between words by capitalizing on distances in word embeddings.", "labels": [], "entities": []}, {"text": "We smooth the implicit delta word kernel, i. e. a Dirac similarity function between unigrams, behind the traditional linear document kernel to capture the similarity between words that are different, yet semantically close.", "labels": [], "entities": []}, {"text": "We then aggregate these word and phrase kernels into sentence and documents kernels through convolution resulting in higher kernel values between semantically related sentences (e. g., close to 7 compared to 1 with bigram downward paths in).", "labels": [], "entities": []}, {"text": "Experiments on six standard datasets for sentiment analysis, subjectivity detection and topic spotting showed statistically significant higher accuracy for our proposed kernel over the bigram approaches.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9747248589992523}, {"text": "subjectivity detection", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.7258920818567276}, {"text": "topic spotting", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.8072502017021179}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.999333918094635}]}, {"text": "Our main goal is to demonstrate empirically that word distances from a given word vector space can easily be incorporated in the standard kernel between documents for higher effectiveness and little additional cost in efficiency.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews the related work.", "labels": [], "entities": []}, {"text": "Section 3 gives the detailed formulation of our kernel.", "labels": [], "entities": []}, {"text": "Section 4 describes the experimental settings and the results we obtained on several datasets.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes our paper and mentions future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our kernel with co-occurrence and syntactic phrases on several standard text categorization tasks.", "labels": [], "entities": []}, {"text": "We considered four tasks: (1) binary sentiment analysis with a movie review dataset of 10,662 sentences (PL05) (Pang and) and a product review dataset (Amazon) of 2,000 multiline documents for 4 different product groups () (we will report the average effectiveness over the 4 sub-collections); (2) ternary sentiment analysis with the SemEval 2013 Task B dataset (Twitter) containing 12,348 tweets classified as positive, neutral or negative (Nakov et al., 2013); (3) binary subjectivity detection with a dataset of 10,000 sentences (PL04) () and another of 11,640 sentences (MPQA) (); and (4) seven-class topic spotting with a news dataset (News) of 32,602 one-line news summaries (Vitale et al., 2012).", "labels": [], "entities": [{"text": "binary sentiment analysis", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.7587510049343109}, {"text": "Pang", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.9702057242393494}, {"text": "ternary sentiment analysis", "start_pos": 298, "end_pos": 324, "type": "TASK", "confidence": 0.8890785574913025}, {"text": "SemEval 2013 Task B dataset", "start_pos": 334, "end_pos": 361, "type": "DATASET", "confidence": 0.543418824672699}, {"text": "binary subjectivity detection", "start_pos": 467, "end_pos": 496, "type": "TASK", "confidence": 0.6193418403466543}, {"text": "seven-class topic spotting", "start_pos": 593, "end_pos": 619, "type": "TASK", "confidence": 0.6825223167737325}]}, {"text": "In all our experiments, we used the FANSE parser () to generate dependency trees and the pre-trained version of word2vec 1 , a 300 dimensional representation of 3 million English words trained over a Google News dataset 1 https://code.google.com/p/word2vec of 100 billion words using the Skip-gram model and a context size of 5.", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 200, "end_pos": 219, "type": "DATASET", "confidence": 0.7316949665546417}]}, {"text": "While fine-tuning the embeddings to a specific task or on a given dataset may improve the result for that particular task or dataset (, it makes the expected results less generalizable and the method harder to use as an off-the-shelf solution -re-training the neural network to obtain task-specific embeddings requires a certain amount of training data, admittedly unlabeled, but still not optimal under our scenario with short documents and little task-specific training data available.", "labels": [], "entities": []}, {"text": "Moreover, tuning the hyperparameters to maximize the classification accuracy needs to be carried out on a validation set and therefore requires additional labeled data.", "labels": [], "entities": [{"text": "classification", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.9377065896987915}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9326976537704468}]}, {"text": "Here, we are more interested in showing that distances in a given word vector space can enhance classification in general.", "labels": [], "entities": []}, {"text": "As for the dependency-based word embeddings proposed by, we do not think they are better suited for the problem we are tackling.", "labels": [], "entities": []}, {"text": "As we will see in the results, we do benefit from the dependency tree structure in the phrase kernel but we still want the word kernel to be based on topical similarity rather than functional similarity.", "labels": [], "entities": []}, {"text": "To train and test the SVM classifier, we used the LibSVM library (Chang and Lin, 2011) and employed the one-vs-one strategy for multi-class tasks.", "labels": [], "entities": [{"text": "LibSVM library", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9006810486316681}]}, {"text": "To prevent overfitting, we tuned the parameters using cross-validation on 80% of PL05 dataset (\u03b1 = 5, \u03bb 1 = 1 for PK since there is no need for distortion as the phrases are of the same length by definition, and \u03bb 1 = \u03bb 2 = 0.5 for CK) and used the same set of parameters on the remaining datasets.", "labels": [], "entities": [{"text": "PL05 dataset", "start_pos": 81, "end_pos": 93, "type": "DATASET", "confidence": 0.9056962728500366}]}, {"text": "We performed normalization for our kernel and baselines only when it led to performance improvements on the training set (PL05, News, PL04 and MPQA).", "labels": [], "entities": [{"text": "PL05", "start_pos": 122, "end_pos": 126, "type": "DATASET", "confidence": 0.8703944683074951}, {"text": "PL04", "start_pos": 134, "end_pos": 138, "type": "DATASET", "confidence": 0.8286833167076111}]}, {"text": "We report accuracy on the remaining 20% for PL05, on the standard test split for Twitter (25%) and News (50%) and from 5-fold cross-validation for the other datasets (Amazon, PL04 and MPQA).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997826218605042}, {"text": "PL05", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8847333192825317}, {"text": "MPQA", "start_pos": 184, "end_pos": 188, "type": "DATASET", "confidence": 0.7821874618530273}]}, {"text": "We only report accuracy as the macro-average F1-scores led to similar conclusions (and except for Twitter and News, the class label distributions are balanced).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9994989633560181}, {"text": "F1-scores", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9409779906272888}, {"text": "News", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.9272382855415344}]}, {"text": "Results for phrase lengths longer than two were omitted since they were marginally different at best.", "labels": [], "entities": [{"text": "phrase lengths", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7314168810844421}]}, {"text": "Statistical significance of improvement over the bigram baseline with the same phrase definition was assessed using the micro sign test (p < 0.01) (Yang and Liu, 1999).: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9887920022010803}, {"text": "standard test split", "start_pos": 219, "end_pos": 238, "type": "METRIC", "confidence": 0.9128304521242777}, {"text": "MPQA", "start_pos": 332, "end_pos": 336, "type": "DATASET", "confidence": 0.7663937211036682}]}, {"text": "Bold font marks the best performance in the column.", "labels": [], "entities": []}, {"text": "* indicates statistical significance at p < 0.01 using micro sign test against the bigram baseline (delta word kernel) of the same column and with the same phrase definition.", "labels": [], "entities": []}, {"text": "presents results from our convolutional sentence kernel and the baseline approaches.", "labels": [], "entities": []}, {"text": "Note again that a delta word kernel leads to the typical unigram and bigram baseline approaches (first three rows).", "labels": [], "entities": []}, {"text": "The 3 rd row corresponds to DTK) and the 4 th one to VTK -the difference with our model on the 9 throw lies in the function \u03c6(\u00b7) that enumerates all random walks in the dependency tree representation following whereas we only consider the downward paths.", "labels": [], "entities": [{"text": "DTK", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.7363777756690979}, {"text": "VTK", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.8525899052619934}]}, {"text": "Overall, we obtained better results than the ngram baselines, DTK and VTK, especially with syntactic phrases.", "labels": [], "entities": [{"text": "VTK", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.6085885167121887}]}, {"text": "VTK shows good performance across all datasets but its computation was more than 700% slower than with our kernel.", "labels": [], "entities": [{"text": "VTK", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9342581629753113}]}, {"text": "Regarding the phrase kernels, PK generally produced better results than CK, implying that the semantic linearity and ontological relation encoded in the embedding is not sufficient enough and treating them separately is more beneficial.", "labels": [], "entities": []}, {"text": "However, we believe CK has more room for improvement with the use of more accurate phrase embeddings such as the ones from, and.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9955737590789795}, {"text": "PL05", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.896223783493042}]}]}