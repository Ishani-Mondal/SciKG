{"title": [{"text": "Noise or additional information? Leveraging crowdsource annotation item agreement for natural language tasks", "labels": [], "entities": []}], "abstractContent": [{"text": "In order to reduce noise in training data, most natural language crowdsourcing annotation tasks gather redundant labels and aggregate them into an integrated label, which is provided to the classifier.", "labels": [], "entities": []}, {"text": "However , aggregation discards potentially useful information from linguistically ambiguous instances.", "labels": [], "entities": []}, {"text": "For five natural language tasks, we pass item agreement onto the task classifier via soft labeling and low-agreement filtering of the training dataset.", "labels": [], "entities": []}, {"text": "We find a statistically significant benefit from low item agreement training filtering in four of our five tasks, and no systematic benefit from soft labeling.", "labels": [], "entities": [{"text": "low item agreement training filtering", "start_pos": 49, "end_pos": 86, "type": "TASK", "confidence": 0.5453053534030914}]}], "introductionContent": [{"text": "Crowdsourcing is a cheap and increasinglyutilized source of annotation labels.", "labels": [], "entities": []}, {"text": "Ina typical annotation task, five or ten labels are collected for an instance, and are aggregated together into an integrated label.", "labels": [], "entities": []}, {"text": "The high number of labels is used to compensate for worker bias, task misunderstanding, lack of interest, incompetance, and malicious intent (.", "labels": [], "entities": []}, {"text": "Majority voting for label aggregation has been found effective in filtering noisy labels.", "labels": [], "entities": [{"text": "label aggregation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7355653941631317}, {"text": "filtering noisy labels", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.8407935500144958}]}, {"text": "Labels can be aggregated under weighted conditions reflecting the reliability of the annotator (.", "labels": [], "entities": []}, {"text": "Certain classifiers are also robust to random (unbiased) label noise (.", "labels": [], "entities": []}, {"text": "However, minority label information is discarded by aggregation, and when the labels were gathered under controlled circumstances, these labels may reflect linguistic intuition and contain useful information.", "labels": [], "entities": []}, {"text": "Two alternative strategies that allow the classifier to learn from the item agreement include training instance filtering and soft labeling.", "labels": [], "entities": [{"text": "training instance filtering", "start_pos": 94, "end_pos": 121, "type": "TASK", "confidence": 0.7306728363037109}]}, {"text": "Filtering training instances by item agreement removes low agreement instances from the training set.", "labels": [], "entities": []}, {"text": "Soft labeling assigns a classifier weight to a training instance based on the item agreement.", "labels": [], "entities": [{"text": "Soft labeling", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7258850038051605}]}, {"text": "Consider two Affect Recognition instances and their's \u03b1 item agreement : In, annotators mostly agreed that the headline expresses little sadness.", "labels": [], "entities": [{"text": "Affect Recognition", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7003067135810852}]}, {"text": "But in, the low item agreement maybe caused by instance difficulty (i.e., Is a war zone sad or just bad?): a Hard Case.", "labels": [], "entities": []}, {"text": "Previous work has shown that training strategy may affect Hard and Easy Case test instances differently.", "labels": [], "entities": []}, {"text": "In this work, for five natural language tasks, we examine the impact of passing crowdsource item agreement onto the task classifier, by means of training instance filtering and soft labeling.", "labels": [], "entities": []}, {"text": "We construct classifiers for Biased Text Detection, Stemming Classification, Recognizing Textual Entailment, Twitter POS Tagging, and Affect Recognition, and evaluate the effect of our different training strategies on the accuracy of each task.", "labels": [], "entities": [{"text": "Biased Text Detection", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7110496163368225}, {"text": "Stemming Classification", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.942000538110733}, {"text": "Recognizing Textual Entailment", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.6867891947428385}, {"text": "Twitter POS Tagging", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.5026731888453165}, {"text": "Affect Recognition", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.7048084735870361}, {"text": "accuracy", "start_pos": 222, "end_pos": 230, "type": "METRIC", "confidence": 0.9988569021224976}]}, {"text": "These tasks represent a wide range of machine learning tasks typical in NLP: sentence-level SVM regression using n-grams; word pairs with character-based features and binary SVM classification; pairwise sentence binary SVM classification with similarity score features; CRF sequence word classification with a range of feature types; and sentence-level regression using a token-weight averaging, respectively.", "labels": [], "entities": [{"text": "CRF sequence word classification", "start_pos": 270, "end_pos": 302, "type": "TASK", "confidence": 0.6703756228089333}]}, {"text": "We use preexisting, freely-available crowdsourced datasets and post all our experiment code on GitHub . Contributions This is the first work (1) to apply item-agreement-weighted soft labeling from crowdsourced labels to multiple real natural language tasks; (2) to filter training instances by item agreement from crowdsourced labels, for multiple natural language tasks; (3) to evaluate classifier performance on high item agreement (Easy Case) instances and low item agreement (Hard Case) instances across multiple natural language tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We built systems for the five NLP tasks, and trained them using aggregation, soft labeling, and instance screening strategies.", "labels": [], "entities": []}, {"text": "When labels were numeric, the integrated label was the average 2 . When labels were nominal, the integrated label was majority vote.'s \u03b1 item agreement was used to filter ambiguous training instances.", "labels": [], "entities": []}, {"text": "For soft labeling, percentage item agreement was used to assign instance weights.", "labels": [], "entities": [{"text": "soft labeling", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.6348097026348114}]}, {"text": "We followed's suggested Multiplied Examples procedure: for each unlabeled instance xi and each existing label y i \u2208 Li = {y ij } (as annotated by worker j), we create one replica of xi , assign it y i , and weight the instance according to the count of y i in Li (i.e., the percentage item agrement).", "labels": [], "entities": []}, {"text": "For each training strategy (SoftLabel, etc), the training instances were changed by the strategy, but the test instances were unaffected.", "labels": [], "entities": []}, {"text": "For the division of test instances into Hard and Easy Cases, the training instances were unaffected, but the test instances were filtered by \u03b1 item agreement.", "labels": [], "entities": [{"text": "division of test instances", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.8155256360769272}]}, {"text": "Hard/Easy Case parameters were chosen to divide the corpus by item agreement into roughly equal portions 3 , relative to the corpus, for post-hoc error analysis.", "labels": [], "entities": []}, {"text": "All systems except Affect Recognition were constructed using DKPro Text Classification (, and used Weka's SMO (Platt, 1999) or SMOreg () implementations with default parameters, with 10-fold (or 5-fold, for computationally-intensive POS Tagging) cross-validation.", "labels": [], "entities": [{"text": "Affect Recognition", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7215337306261063}, {"text": "DKPro Text Classification", "start_pos": 61, "end_pos": 86, "type": "DATASET", "confidence": 0.8444409966468811}, {"text": "POS Tagging)", "start_pos": 233, "end_pos": 245, "type": "TASK", "confidence": 0.7680043975512186}]}, {"text": "More details are available in the Supplemental Notes document.", "labels": [], "entities": [{"text": "Supplemental Notes document", "start_pos": 34, "end_pos": 61, "type": "DATASET", "confidence": 0.8368840217590332}]}], "tableCaptions": [{"text": " Table 1: Results (Pearson or micro F1) with different training strategies and all, Hard, and Easy Cases.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.8551135063171387}]}]}