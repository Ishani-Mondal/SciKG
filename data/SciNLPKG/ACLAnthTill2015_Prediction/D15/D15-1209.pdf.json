{"title": [{"text": "Leave-one-out Word Alignment without Garbage Collector Effects", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7042881697416306}]}], "abstractContent": [{"text": "Expectation-maximization algorithms, such as those implemented in GIZA++ pervade the field of unsupervised word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 107, "end_pos": 121, "type": "TASK", "confidence": 0.7166387438774109}]}, {"text": "However, these algorithms have a problem of over-fitting, leading to \"garbage collector effects,\" where rare words tend to be erroneously aligned to untranslated words.", "labels": [], "entities": []}, {"text": "This paper proposes a leave-one-out expectation-maximization algorithm for unsupervised word alignment to address this problem.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.721231997013092}]}, {"text": "The proposed method excludes information derived from the alignment of a sentence pair from the alignment models used to align it.", "labels": [], "entities": []}, {"text": "This prevents erroneous alignments within a sentence pair from supporting themselves.", "labels": [], "entities": []}, {"text": "Experimental results on Chinese-English and Japanese-English corpora show that the F 1 , precision and recall of alignment were consistently increased by 5.0%-17.2%, and BLEU scores of end-to-end translation were raised by 0.03-1.30.", "labels": [], "entities": [{"text": "F 1", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9917376339435577}, {"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9987377524375916}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9992883801460266}, {"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9990399479866028}]}, {"text": "The proposed method also outperformed l 0-normalized GIZA++ and Kneser-Ney smoothed GIZA++.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "Unsupervised word alignment (WA)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7578000475962957}, {"text": "statistical machine translation (SMT)", "start_pos": 113, "end_pos": 150, "type": "TASK", "confidence": 0.7774997601906458}]}, {"text": "A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.999326229095459}, {"text": "WA", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9474157094955444}]}, {"text": "This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA).", "labels": [], "entities": [{"text": "WA", "start_pos": 115, "end_pos": 117, "type": "TASK", "confidence": 0.9693868160247803}]}, {"text": "* The author now is affiliated with Google, Japan.", "labels": [], "entities": []}, {"text": "The EM algorithm for WA has a great influence in SMT.", "labels": [], "entities": [{"text": "WA", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9631646275520325}, {"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9961085915565491}]}, {"text": "Many well-known toolkits including GIZA++, the Berkeley Aligner (, Fast Align ( and SyM-GIZA++), all employ this algorithm.", "labels": [], "entities": []}, {"text": "GIZA++ in particular is frequently used in systems participating in many shared tasks.", "labels": [], "entities": []}, {"text": "However, the EM algorithm for WA is wellknown for introducing \"garbage collector effects.\"", "labels": [], "entities": [{"text": "WA", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9606223702430725}]}, {"text": "Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (.(a) shows areal sentence pair, denoted s, from the GALE ChineseEnglish Word Alignment and Tagging Training corpus (GALE WA corpus) 1 with it's humanannotated word alignment.", "labels": [], "entities": [{"text": "GALE ChineseEnglish Word Alignment and Tagging Training corpus (GALE WA corpus) 1", "start_pos": 177, "end_pos": 258, "type": "DATASET", "confidence": 0.80620995589665}]}, {"text": "The Chinese word \"HE ZHANG,\" denoted w r , which means river custodian, only occurs once in the whole corpus.", "labels": [], "entities": [{"text": "HE ZHANG", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9180979132652283}]}, {"text": "We performed EM training using GIZA++ on this corpus concatenated with 442,967 training sentence pairs from the NIST Open Machine Translation (OpenMT) 2006 evaluation 2 . The resulting alignment is shown in(b).", "labels": [], "entities": [{"text": "NIST Open Machine Translation (OpenMT) 2006 evaluation", "start_pos": 112, "end_pos": 166, "type": "DATASET", "confidence": 0.7705399394035339}]}, {"text": "It can be seen that w r is erroneously aligned to multiple English words.", "labels": [], "entities": []}, {"text": "To find the cause of this, we checked the alignments in each iteration i of s, denoted a i s . We found that in a 1 s , w r together with the other source-side words were aligned with uniform probability to all the target-side words since the alignment models provided no prior information.", "labels": [], "entities": []}, {"text": "However, in a 2 s , w r became erroneously aligned, because the alignment distribution 3 of w r was only learned from a 1 s , thus consisted of non-zero values only for generating the target-side words in s.", "labels": [], "entities": []}, {"text": "Therefore, the alignment probabilities from the rare word w r to the unaligned words in s were extraordinarily high, since almost all of the probability mass was distributed among them.", "labels": [], "entities": []}, {"text": "In other words, the story behind these garbage collector effects is that erroneous alignments are able to provide support for themselves; the probability distribution learned only from sis re-applied to s.", "labels": [], "entities": []}, {"text": "In this way, these \"garbage collector effects\" area form of over-fitting.", "labels": [], "entities": []}, {"text": "Motivated by this observation, we propose a leave-one-out EM algorithm for WA in this paper.", "labels": [], "entities": [{"text": "WA", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9735110998153687}]}, {"text": "Recently this technique has been applied to avoid over-fitting in kernel density estimation (); instead of performing maximum likelihood estimation, maximum leaveone-out likelihood estimation is performed.", "labels": [], "entities": [{"text": "kernel density estimation", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.6709091663360596}]}, {"text": "shows the effect of using our technique on the example.", "labels": [], "entities": []}, {"text": "The garbage collection has not occurred, and the alignment of the word \"HE ZHANG\" is identical to the human annotation.", "labels": [], "entities": [{"text": "garbage collection", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.6996260583400726}, {"text": "HE ZHANG", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.8380554616451263}]}], "datasetContent": [{"text": "The proposed WA method was tested on two language pairs: Chinese-English and JapaneseEnglish.", "labels": [], "entities": [{"text": "WA", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.970893383026123}]}, {"text": "Performance was measured both directly using the agreement with reference to manual WA annotations, and indirectly using the BLEU score in end-to-end machine translation tasks.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 125, "end_pos": 135, "type": "METRIC", "confidence": 0.962205171585083}, {"text": "machine translation tasks", "start_pos": 150, "end_pos": 175, "type": "TASK", "confidence": 0.7145017286141714}]}, {"text": "GIZA++ and our own implementation of standard EM were used as baselines.", "labels": [], "entities": [{"text": "GIZA++", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9291388392448425}]}, {"text": "The Chinese-English experimental data consisted of the GALE WA corpus and the OpenMT corpus.", "labels": [], "entities": [{"text": "GALE WA corpus", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.9242267807324728}, {"text": "OpenMT corpus", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9701152145862579}]}, {"text": "They are from the same domain, both contain newswire texts and web blogs.", "labels": [], "entities": []}, {"text": "The OpenMT evaluation 2005 was used as a development set for MERT tuning, and the OpenMT evaluation 2006 was used as a test set.", "labels": [], "entities": [{"text": "OpenMT evaluation 2005", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.9297998348871866}, {"text": "MERT tuning", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.731973260641098}, {"text": "OpenMT evaluation 2006", "start_pos": 82, "end_pos": 104, "type": "DATASET", "confidence": 0.9309097727139791}]}, {"text": "The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011) . The corpus contains a set of 1,235 sentence pairs that are manually word aligned.", "labels": [], "entities": [{"text": "Kyoto Free Translation Task (Neubig, 2011)", "start_pos": 46, "end_pos": 88, "type": "DATASET", "confidence": 0.7178071869744195}]}, {"text": "The corpora were processed using a standard procedure for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7299754023551941}]}, {"text": "The English texts were tokenized with the tokenization script released with Europarl corpus () and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter ( ; the Japanese texts were segmented into words using the Kyoto Text Analysis Toolkit (KyTea 8 ).", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 76, "end_pos": 91, "type": "DATASET", "confidence": 0.9900888502597809}, {"text": "Kyoto Text Analysis Toolkit (KyTea 8 )", "start_pos": 260, "end_pos": 298, "type": "DATASET", "confidence": 0.84187201410532}]}, {"text": "Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out.", "labels": [], "entities": []}, {"text": "GIZA++ was run with the default Moses settings ().", "labels": [], "entities": [{"text": "GIZA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9360079169273376}]}, {"text": "The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations.", "labels": [], "entities": []}, {"text": "We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4.", "labels": [], "entities": [{"text": "IBM model 1", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.9193159540494283}]}, {"text": "In the original work this combination of models achieved comparable performance to the default Moses settings.", "labels": [], "entities": []}, {"text": "They were run with 5, 5 and 6 iterations.", "labels": [], "entities": []}, {"text": "The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details.", "labels": [], "entities": []}, {"text": "Our implementation is based on the toolkit of CICADA ( . We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS.", "labels": [], "entities": [{"text": "AGRIPPA", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9734288454055786}, {"text": "OCTAVIAN", "start_pos": 131, "end_pos": 139, "type": "DATASET", "confidence": 0.8076829314231873}]}, {"text": "In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages.", "labels": [], "entities": [{"text": "WA", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9933599829673767}]}, {"text": "Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental Data.  \u2020 Each consists of one foreign sentence and four English reference sen- tences.", "labels": [], "entities": []}, {"text": " Table 3: Word alignment accuracy measured by F 1 , precision and recall.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6940872520208359}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.967128336429596}, {"text": "F 1", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9948988854885101}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9997246861457825}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.999575674533844}]}, {"text": " Table 4: End-to-end translation quality measured by BLEU", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.990812361240387}]}, {"text": " Table 6: Effect of training corpus size on end-to-end translation quality measured by BLEU (Chinese- English).  \u2020 the whole manually word aligned corpus", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9963000416755676}]}, {"text": " Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s", "labels": [], "entities": []}]}