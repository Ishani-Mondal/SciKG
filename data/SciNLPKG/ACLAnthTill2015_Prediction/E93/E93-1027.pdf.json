{"title": [{"text": "Linguistic Knowledge Acquisition from Parsing Failures", "labels": [], "entities": [{"text": "Linguistic Knowledge Acquisition from Parsing Failures", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.6474787145853043}]}], "abstractContent": [{"text": "A semi-automatic procedure of linguistic knowledge acquisition is proposed, which combines corpus-based techniques with the conventional rule-based approach.", "labels": [], "entities": [{"text": "linguistic knowledge acquisition", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.6484969953695933}]}, {"text": "The rule-based component generates all the possible hypotheses of defects which the existing linguistic knowledge might contain, when it fails to parse a sentence.", "labels": [], "entities": []}, {"text": "The rule-based component does not try to identify the defects, but generates a set of hypotheses and the corpus-based component chooses the plausible ones among them.", "labels": [], "entities": []}, {"text": "The procedure will be used for adapting or re-using existing linguistic resources for new application domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "While quite a number of useful grammar formalisms for natural language processing now exist, it still remains a time-consuming and hard task to develop grammars and dictionaries with comprehensive coverage.", "labels": [], "entities": []}, {"text": "It is also the case that, though quite a few computational grammars and dictionaries with comprehensive coverage have been used in various application systems, to re-use them for other application domains is not always so easy, even if we use the same formalisms and programs such as parsers, etc.", "labels": [], "entities": []}, {"text": "We usually have to revise, add, and delete grammar rules and lexical entries in order to adapt them to the peculiarities of languages of new application domains.", "labels": [], "entities": []}, {"text": "*also a staff member of Matsushita Electric Industrial Co.,Ltd., Tokyo, JAPAN.", "labels": [], "entities": [{"text": "Matsushita Electric Industrial Co.", "start_pos": 24, "end_pos": 58, "type": "DATASET", "confidence": 0.7984689474105835}, {"text": "JAPAN", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.8302257061004639}]}, {"text": "Such adaptations of existing linguistic knowledge to anew domain are currently performed through rather undisciplined, trial and error processes involving much human effort.", "labels": [], "entities": []}, {"text": "In this paper we show that techniques similar to those in robust parsing of ill-formed input, together with corpus-based techniques, can be used to discover disparities between existing linguistic knowledge and actual language usage in anew domain, and to hypothesize new grammar rules or lexical descriptions.", "labels": [], "entities": []}, {"text": "Although our framework appears similar to grammar learning from corpora, our current goal is far more modest, i.e. to help linguists revise existing grammars by showing possible defects and hypothesizing them through corpus analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "To see what sort of hypotheses are actually generated, and how many of them are reasonable (in other words, how many of them are nonsensical), we have conducted a preliminary experiment with the following six sentences.", "labels": [], "entities": []}, {"text": "(1) The girl in the garden has a bouquet.", "labels": [], "entities": []}, {"text": "We deliberately introduce defects into the existing grammar which are relevant to the analysis of these sentences.", "labels": [], "entities": []}, {"text": "That is, the following rules are removed from the existing grammar for the sake of the experiment.", "labels": [], "entities": []}, {"text": "\u2022 pp-attachment rule for noun phrases.", "labels": [], "entities": []}, {"text": "\u2022 rule for imperative sentences.", "labels": [], "entities": []}, {"text": "\u2022 rule for SO-THAT construction.", "labels": [], "entities": [{"text": "SO-THAT construction", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.9681386947631836}]}, {"text": "\u2022 lexical rule for \"BMW\".", "labels": [], "entities": [{"text": "BMW\"", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.9230112433433533}]}, {"text": "\u2022 lexical description for the plural usage of \"fish\".", "labels": [], "entities": []}, {"text": "The criteria- of redundant hypotheses are included in the basic algorithm of GRHP so that the following lists of hypotheses for these examples do not contain those which are rejected by these criteria.", "labels": [], "entities": [{"text": "GRHP", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8303731679916382}]}, {"text": "The hypotheses marked with '--*' are the plausible hypotheses.", "labels": [], "entities": []}, {"text": "The hypotheses marked by x and \u00ae are the hypotheses removed by adding and as further criteria of redundant hypotheses, respectively.", "labels": [], "entities": []}, {"text": "We do not use the criteria of- in this experiment, partly because these are highly dependent on the completeness of the existing grammar and, though very effective for reducing the number of hypotheses, can be arbitrary.", "labels": [], "entities": []}, {"text": "s => vp GRHP generates only one hypothesis, a rule for imperative sentences.", "labels": [], "entities": []}, {"text": "This rule looks plausible but the fact that the criteria of redundant hypotheses suppresses this rule indicates that a rule for imperative sentences should not be treated as a normal unary (category conversion) rule but rather a whole-sentencial constituent rule.", "labels": [], "entities": []}, {"text": "Although this sentence is short, quite a few hypotheses are generated.", "labels": [], "entities": []}, {"text": "This is partly because both \"do\" and \"dream\" are ambiguous in their parts of speech.", "labels": [], "entities": []}, {"text": "Some of the generated hypotheses are based on the interpretation of \"dream\" as a noun.", "labels": [], "entities": []}, {"text": "However, even in the cases in which the main verb is not ambiguous, GRHP always hypothesizes 'vp =~ vp + vp' as well as the correct DO-emphasis rule, as \"do\" has two parts of speech.", "labels": [], "entities": []}, {"text": "As we discuss in the following section, it is impossible to choose one of these hypotheses on the basis of single parsing failures.", "labels": [], "entities": []}, {"text": "We need corpus-based techniques to rate the plausibility of these two hypotheses.", "labels": [], "entities": []}, {"text": "\"The box is so heavy that I could not move it.\"", "labels": [], "entities": []}, {"text": "In this example, 'vp ~ vp + that_clause' (or 's ~ s + that_clause') could be the appropriate hypothesis.", "labels": [], "entities": []}, {"text": "However, simple addition of such a rule to the existing grammar results in overgeneralization.", "labels": [], "entities": []}, {"text": "The rule should have a condition on the existence of \"so\" in 'vp' (or 's') while a similar effect can also be attained by adding anew lexical entry for \"heavy\" which has a subcategorization frame containing a 'that clause'.", "labels": [], "entities": []}, {"text": "That is, the system has to decide which hypothesis is more plausible, either \"heavy\" can subcategorize a 'that clause' or \"so\" is crucial in making 'vp' to be related with a 'that clause'.", "labels": [], "entities": []}, {"text": "This decision may not be possible, if this sentence is the only one sentence in a corpus which contains this construction.", "labels": [], "entities": []}, {"text": "Like Example 3, we need corpus-based techniques to choose the right one.", "labels": [], "entities": []}, {"text": "(5) \"The student has a BMW.\"", "labels": [], "entities": []}, {"text": "GRHP generates the correct hypothesis of the feature disagreement between the plural determiner \"several\" and the noun \"fish\" as one of possible hypotheses.", "labels": [], "entities": [{"text": "GRHP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8709153532981873}]}, {"text": "summarizes the number of hypotheses generated for each sample sentence.", "labels": [], "entities": []}, {"text": "As can be seen, while appropriate hypotheses are generated, quite a few other hypotheses are also generated, especially in the case of the third and the fourth sentences.", "labels": [], "entities": []}, {"text": "However, as shown in, the criteria and of redundant hypotheses can eliminate significant portions of nonsensical hypotheses ( shows the effects of these criteria on the number of hypothesized new rules).", "labels": [], "entities": []}, {"text": "In Example (4), for example, 31 out of 58 initially hypothesized rules are eliminated by and, while 16 out of 28 rules are eliminated in Example (3).", "labels": [], "entities": []}, {"text": "Furthermore, we expect that introduction of other criteria for redundant elimination based on- will reduce the number of hypotheses significantly and make the succeeding stage of the corpus-based statistical analysis feasible.", "labels": [], "entities": [{"text": "redundant elimination", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.7266606688499451}]}, {"text": "The experiment on another set of sample sentences from the UNIX on-line manual confirms our expectation (See).", "labels": [], "entities": []}, {"text": "The number of hypotheses generated in this experiment is very much similar to that of the experiment on artificial samples (note that Table 4 shows the number of hypotheses generated before elimination by the criteria and'J7]).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of Hypotheses", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9417967200279236}]}, {"text": " Table 4: Number of Hypotheses (Sentences from the UNIX manual)", "labels": [], "entities": [{"text": "UNIX manual", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.789705216884613}]}]}