{"title": [{"text": "Parsing the Wall Street Journal with the Inside-Outside Algorithm", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 12, "end_pos": 31, "type": "DATASET", "confidence": 0.6869236826896667}]}], "abstractContent": [{"text": "We report grammar inference experiments on partially parsed sentences taken from the Wall Street Journal corpus using the inside-outside algorithm for stochastic context-free grammars.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 85, "end_pos": 111, "type": "DATASET", "confidence": 0.972047820687294}]}, {"text": "The initial grammar for the inference process makes no ,assumption of the kinds of structures and their distributions.", "labels": [], "entities": []}, {"text": "The inferred grammar is evaluated by its predicting power and by comparing the bracketing of held out sentences imposed by the inferred grammar with the partial bracketings of these sentences given in the corpus.", "labels": [], "entities": []}, {"text": "Using part-of-speech tags as the only source of lexical information, high bracketing accuracy is achieved even with a small subset of the available training material (1045 sentences): 94.4% for test sentences shorter than 10 words and 90.2% for sentences shorter than 15 words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9849148988723755}]}], "introductionContent": [{"text": "Most broad coverage natural language parsers have been designed by incorporating hand-crafted rules.", "labels": [], "entities": [{"text": "broad coverage natural language parsers", "start_pos": 5, "end_pos": 44, "type": "TASK", "confidence": 0.5716294527053833}]}, {"text": "These rules are also very often further refined by statistical training.", "labels": [], "entities": []}, {"text": "Furthermore, it is widely believed that high performance can only be achieved by disambiguating lexically sensitive phenomena such as prepositional attachment ambiguity, coordination or subcategorizadon.", "labels": [], "entities": []}, {"text": "So far, grammar inference has not been shown to be effective for designing wide coverage parsers.", "labels": [], "entities": [{"text": "grammar inference", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7867117822170258}]}, {"text": "describes a training algorithm for stochastic context-free grammars (SCFG) which can be used for grammar reestimation ( or grammar inference from scratch (.", "labels": [], "entities": [{"text": "grammar reestimation", "start_pos": 97, "end_pos": 117, "type": "TASK", "confidence": 0.7427051067352295}]}, {"text": "However, the application of SCFGs and the original inside-outside algorithm for grammar inference has been inconclusive for two reasons.", "labels": [], "entities": [{"text": "grammar inference", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.754785031080246}]}, {"text": "First, each iteration of the algorithm on a gr,-unmar with n nonterminals requires O(n31wl 3) time per t~ning sentence w.", "labels": [], "entities": [{"text": "O(n31wl 3) time", "start_pos": 83, "end_pos": 98, "type": "METRIC", "confidence": 0.8825515409310659}]}, {"text": "Second, the inferred grammar imposes bracketings which do not agree with linguistic judgments of sentence structure.", "labels": [], "entities": []}, {"text": "extended the inside-outside algorithm for inferring the parameters of a stochastic context-free grammar to take advantage of constituent bracketing information in the training text.", "labels": [], "entities": []}, {"text": "Although they report encouraging experiments (90% bracketing accuracy) on h'mguage transcriptions in the Texas Instrument subset of the Air Travel Information System (ATIS), the small size of the corpus (770 bracketed sentences containing a total of 7812 words), its linguistic simplicity, and the computation time required to vain the grammar were reasons to believe that these results may not scale up to a larger and more diverse corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9290657639503479}, {"text": "Texas Instrument subset of the Air Travel Information System (ATIS)", "start_pos": 105, "end_pos": 172, "type": "DATASET", "confidence": 0.8102750654021899}]}, {"text": "We report grammar inference experiments with this algorithm from the parsed Wall Street Journal corpus.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 76, "end_pos": 102, "type": "DATASET", "confidence": 0.9704449623823166}]}, {"text": "The experiments prove the feasibility and effectiveness of the inside-outside algorithm on a htrge corpus.", "labels": [], "entities": [{"text": "htrge corpus", "start_pos": 93, "end_pos": 105, "type": "DATASET", "confidence": 0.7900006175041199}]}, {"text": "Such experiments are made possible by assumi'ng aright br~mching structure whenever the parsed corpus leaves portions of the parsed tree unspecified.", "labels": [], "entities": []}, {"text": "This preprocessing of the corpus makes it fully bracketed.", "labels": [], "entities": []}, {"text": "By taking adv~mtage of this fact in the implementation of the inside-outside ~dgorithm, its complexity becomes line~tr with respect to the input length (as noted by ,and therefore tractable for large corpora.", "labels": [], "entities": []}, {"text": "We report experiments using several kinds of initial gr~unmars ~md a variety of subsets of the corpus as training data.", "labels": [], "entities": []}, {"text": "When the entire Wall Street Journal corpus was used as training material, the time required for training has been further reduced by using a par~dlel implementation of the inside-outside ~dgorithm.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 16, "end_pos": 42, "type": "DATASET", "confidence": 0.9597754776477814}]}, {"text": "The inferred grammar is evaluated by measuring the percentage of compatible brackets of the bracketing imposed by the inferred grammar with the partial bracketing of held out sentences.", "labels": [], "entities": []}, {"text": "Surprisingly high bracketing accuracy is achieved with only 1042 sentences as train-\u2022 ing materi,'d: 94.4% for test sentences shorter th,-m 10 words ~md 90.2% for sentences shorter than 15 words.", "labels": [], "entities": [{"text": "bracketing", "start_pos": 18, "end_pos": 28, "type": "TASK", "confidence": 0.9324299693107605}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9795589447021484}]}, {"text": "Furthermore, the bracketing accuracy does not drop drastic~dly as longer sentences ,are considered.", "labels": [], "entities": [{"text": "bracketing", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.908812940120697}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9284079670906067}]}, {"text": "These results ,are surprising since the training uses part-ofspeech tags as the only source of lexical information.", "labels": [], "entities": []}, {"text": "This raises questions about the statistical distribution of sentence structures observed in naturally occurring text.", "labels": [], "entities": []}, {"text": "After having described the training material used, we report experiments using several subsets of the available training material ,and evaluate the effect of the training size on the bracketing perform,'mce.", "labels": [], "entities": []}, {"text": "Then, we describe a method for reducing the number of parameters in the inferred gr~unmars.", "labels": [], "entities": []}, {"text": "Finally, we suggest a stochastic model for inferring labels on the produced binary br~mching trees.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}