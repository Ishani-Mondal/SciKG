{"title": [{"text": "Using an Annotated Corpus as a Stochastic Grammar", "labels": [], "entities": []}], "abstractContent": [{"text": "In Data Oriented Parsing (DOP), an annotated corpus is used as a stochastic grammar.", "labels": [], "entities": [{"text": "Data Oriented Parsing (DOP)", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.8093399107456207}]}, {"text": "An input string is parsed by combining subtrees from the corpus.", "labels": [], "entities": []}, {"text": "As a consequence, one parse tree can usually be generated by several derivations that involve different subtrces.", "labels": [], "entities": []}, {"text": "This leads to a statistics where the probability of a parse is equal to the sum of the probabilities of all its derivations.", "labels": [], "entities": []}, {"text": "In (Scha, 1990) an informal introduction to DOP is given, while (Bed, 1992a) provides a formalization of the theory.", "labels": [], "entities": []}, {"text": "In this paper we compare DOP with other stochastic grammars in the context of Formal Language Theory.", "labels": [], "entities": [{"text": "Formal Language Theory", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7533422509829203}]}, {"text": "It it proved that it is not possible to create for every DOP-model a strongly equivalent stochastic CFG which also assigns the same probabilities to the parses.", "labels": [], "entities": []}, {"text": "We show that the maximum probability parse can be estimated in polynomial time by applying Monte Carlo techniques.", "labels": [], "entities": []}, {"text": "The model was tested on a set of hand-parsed strings from the Air Travel Information System (ATIS) spoken language corpus.", "labels": [], "entities": [{"text": "Air Travel Information System (ATIS) spoken language corpus", "start_pos": 62, "end_pos": 121, "type": "DATASET", "confidence": 0.822303831577301}]}, {"text": "Preliminary experiments yield 96% test set parsing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9628890156745911}]}], "introductionContent": [], "datasetContent": [{"text": "For our experiments we used part-of-speech sequences of spoken-language transcriptions from the Air Travel Information System (ATIS) corpus (, with the labeled-bracketings of those sequences in the Penn Treebank.", "labels": [], "entities": [{"text": "Air Travel Information System (ATIS) corpus", "start_pos": 96, "end_pos": 139, "type": "DATASET", "confidence": 0.7019029967486858}, {"text": "Penn Treebank", "start_pos": 198, "end_pos": 211, "type": "DATASET", "confidence": 0.9964545965194702}]}, {"text": "The 750 labeled-bracketings were divided at random into a DOP-corpus of 675 trees and a test set of 75 part-ofspeech sequences.", "labels": [], "entities": []}, {"text": "The following tree is an example from the DOP-corpns, where for reasons of readability the lexical items are added to the part-of-speech tags.", "labels": [], "entities": []}, {"text": "( (S (NP *) fVP (VB Show) .~4 93% .~ 93% ~6 95% ~7 95% unbounded 96% Parsing accuracy for the ATIS corpus, sample size N= I00.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9539604187011719}, {"text": "ATIS corpus", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.9492782652378082}]}, {"text": "The table shows that there is a relatively rapid inc~'~ase in parsing accuracy when enlarging the maximum depth of the subUees to 3.", "labels": [], "entities": [{"text": "parsing", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.963249683380127}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9741576313972473}]}, {"text": "The accuracy keeps increasing, at a slower rate, when the depth is enlarged further.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996926784515381}]}, {"text": "The highest accuracy is obtained by using all subtrees from the corpus: 72 out of the 75 sentences from the test set are parsed correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9992786049842834}]}, {"text": "In the following figure, parsing accuracy is plotted against the sample size Nfor three of our experiments: the experiments where the depth of the subtrees is constrained to 2 and 3, and the experiment where the depth is unconswained.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9608823657035828}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9808710217475891}]}, {"text": "(The maximum depth in the ATIS corpus is 13.)", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9772165417671204}]}, {"text": "In, 90.36% bracketing accuracy was reported using a stochastic CFG trained on bracketings from the ATIS corpus.", "labels": [], "entities": [{"text": "bracketing", "start_pos": 11, "end_pos": 21, "type": "TASK", "confidence": 0.9449009895324707}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9775910377502441}, {"text": "ATIS corpus", "start_pos": 99, "end_pos": 110, "type": "DATASET", "confidence": 0.9735060036182404}]}, {"text": "Though we cannot make a direct c\u00a2~parison, our pilot experiment suggests that our model may have better performance than a stochastic CFG.", "labels": [], "entities": []}, {"text": "However, there is still an error rate of 4%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9930106699466705}]}, {"text": "Although there is no reason to expect 100% accuracy in the absence of any semantic or pragmatic analysis, it seems that the accuracy might be further improved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9932496547698975}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9995648264884949}]}, {"text": "Three limitations of the current experiments are worth mentioning, Fn~t, the Treebank annotations are not rich enough.", "labels": [], "entities": [{"text": "Fn", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9837030172348022}, {"text": "Treebank annotations", "start_pos": 77, "end_pos": 97, "type": "DATASET", "confidence": 0.9218280613422394}]}, {"text": "Although the Treebank uses a relatively rich part-ofspeech system (48 terminal symbols), there are only 15 non-terwinal symbols.", "labels": [], "entities": []}, {"text": "Especially the internal su~cmre of noun phrases is very poor.", "labels": [], "entities": []}, {"text": "Semantic annotations are completely absent.", "labels": [], "entities": []}, {"text": "Secondly, it could be that subtrees which occur only once in the corpus, give bad estimations of their actual probabilities.", "labels": [], "entities": []}, {"text": "The question as to whether reestimation techniques would further improve the accuracy, must be considered in future research.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9996265172958374}]}, {"text": "Thirdly, it could be that our corpus is not large enough.", "labels": [], "entities": []}, {"text": "This brings us to the question as to how much parsing accuracy depends on the size of the corpus.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9619606137275696}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9551481604576111}]}, {"text": "For studying this question, we performed additional experiments with different corpus sizes.", "labels": [], "entities": []}, {"text": "Starting with a corpus of only 50 parse trees (randomly chosen from the initial DOP-corpus of 675 trees), we increased its size with intervals of 50.", "labels": [], "entities": []}, {"text": "As our test set, we took the same 75 p-o-s sequences as used in the previous experiments.", "labels": [], "entities": []}, {"text": "In the next figure the parsing accuracy, for sample size N = 100, is plotted against the corpus size, using all corpus subtrees.", "labels": [], "entities": [{"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9613050222396851}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.8601981997489929}]}], "tableCaptions": []}