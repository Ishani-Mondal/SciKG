{"title": [{"text": "Acquiring Receptive Morphology: A Connectionist Model", "labels": [], "entities": [{"text": "Acquiring Receptive Morphology", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8108859658241272}]}], "abstractContent": [{"text": "This paper describes a modular connectionist model of the acquisition of receptive inflectional morphology.", "labels": [], "entities": [{"text": "acquisition of receptive inflectional morphology", "start_pos": 58, "end_pos": 106, "type": "TASK", "confidence": 0.7740721940994263}]}, {"text": "The model takes inputs in the form of phones one at a time and outputs the associated roots and inflections.", "labels": [], "entities": []}, {"text": "Simulations using artificial language stimuli demonstrate the capacity of the model to learn suffix-ation, prefixation, infixation, circumfixation, mutation, template, and deletion rules.", "labels": [], "entities": []}, {"text": "Separate network modules responsible for syllables enable to the network to learn simple reduplication rules as well.", "labels": [], "entities": []}, {"text": "The model also embodies constraints against association-line crossing.", "labels": [], "entities": [{"text": "association-line crossing", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.6923810690641403}]}], "introductionContent": [{"text": "For many natural languages, a major problem fora language learner, whether human or machine, is the system of bound morphology of the language, which may carry much of the functional load of the grammar.", "labels": [], "entities": []}, {"text": "While the acquisition of morphology has sometimes been seen as the problem of learning how to transform one linguistic form into another form, e.g., by and, from the learner's perspective, the problem is one of learning how forms map onto meanings.", "labels": [], "entities": [{"text": "acquisition of morphology", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.869450847307841}]}, {"text": "Most work which has viewed the acquisition of morphology in this way, e.g.,, has taken tile perspective of production.", "labels": [], "entities": []}, {"text": "But a human language learner almost certainly learns to understand polymorphemic words before learning to produce them, and production may need to build on perception.", "labels": [], "entities": []}, {"text": "Thus it seems reasonable to begin with a model of the acquisition of receptive morphology.", "labels": [], "entities": [{"text": "acquisition of receptive morphology", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.8190614134073257}]}, {"text": "In this paper, I will deal with that component of receptive morphology which takes sequences of phones, each expressed as a vector of phonetic features, and identifies them as particular morphemes.", "labels": [], "entities": []}, {"text": "This process ignores the segmentation of words into phone sequences, the morphological structure of words, and the the semantics of morphemes.", "labels": [], "entities": []}, {"text": "I will refer to this task as root and inflection identification.", "labels": [], "entities": [{"text": "root and inflection identification", "start_pos": 29, "end_pos": 63, "type": "TASK", "confidence": 0.6772225424647331}]}, {"text": "It is assumed that children learn to identify roots and inflections through the presentation of paired forms and sets of morpheme meanings.", "labels": [], "entities": []}, {"text": "They show evidence of generalization when they are able to identify the root and inflection of a novel combination of familiar morphemes.", "labels": [], "entities": []}, {"text": "At a minimum, a model of the acquisition of this capacity should succeed on the full range of morphological rule types attested in the world's languages, it should embody known constraints on what sorts of rules are possible inhuman language, and it should bear a relationship to the production of morphologically complex words.", "labels": [], "entities": []}, {"text": "This paper describes a psychologically motivated connectionist model (Modular Connectionist Network for the Acquisition of Morphology, MC-NAM) which shows evidence of acquiring all of the basic rule types and which also experiences relative difficulty learning rules which seem not to be possible.", "labels": [], "entities": []}, {"text": "In another paper, I show how the representations that develop during the learning of root and inflection identification can support word production.", "labels": [], "entities": [{"text": "root and inflection identification", "start_pos": 85, "end_pos": 119, "type": "TASK", "confidence": 0.6801985502243042}, {"text": "word production", "start_pos": 132, "end_pos": 147, "type": "TASK", "confidence": 0.8183176815509796}]}, {"text": "Although still tentative in several respects, MCNAM appears to be the first computational model of the acquisition of receptive morphology to apply to this diversity of morphological rules.", "labels": [], "entities": [{"text": "acquisition of receptive morphology", "start_pos": 103, "end_pos": 138, "type": "TASK", "confidence": 0.8279830068349838}]}, {"text": "In contrast to symbolic models of language acquisition, it succeeds without built-in symbolic distinctions, for example, the distinction between stem and affix.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "I first provide a brief overview of the categories of morphological rules found in the world's languages.", "labels": [], "entities": []}, {"text": "I then present the model and discuss simulations which demonstrate that it generalizes for most kinds of morphological rules.", "labels": [], "entities": []}, {"text": "Next, focusing on template morphology, I show how the network implements the analogue of autosegments and how the model embodies one constraint on the sorts of rules that can be learned.", "labels": [], "entities": []}, {"text": "Finally, I discuss augmentation of the model with a hierarchical structure reflecting the hierarchy of metrical phonology; this addition is necessary for the acquisition of the most challenging type of morphological rule, reduplication.", "labels": [], "entities": []}], "datasetContent": [{"text": "General Performance of the Model In all of the experiments reported on here, the stimuli presented to the network consisted of words in an artificial language.", "labels": [], "entities": []}, {"text": "The phoneme inventory of the language was made up 19 phones (24 for the mutation rule, which nasalizes vowels).", "labels": [], "entities": []}, {"text": "For each morphological rule, there were 30 roots, 15 each of CVC and CVCVC patterns of phones.", "labels": [], "entities": []}, {"text": "Each word consisted of either two or three morphemes, a root and one or two inflections (referred to as \"tense\" and \"aspect\" for convenience).", "labels": [], "entities": []}, {"text": "Examples of each rule, using the root vibun: For each morphological rule there were either 60 (30 roots x 2 tense inflections) or 120 (30 roots x 2 tense inflections x 2 aspect inflections) different words.", "labels": [], "entities": []}, {"text": "From these 2/3 were selected randomly as training words, and the remaining 1/3 were set aside as test words.", "labels": [], "entities": []}, {"text": "For each rule, ten separate networks with different random initial weights were trained and tested.", "labels": [], "entities": []}, {"text": "Training for the tenseonly rules proceeded for 150 epochs (repetitions of all training patterns); training for the tense-aspect rules lasted 100 epochs.", "labels": [], "entities": []}, {"text": "Following training the performance of the network on the test patterns was assessed.?.", "labels": [], "entities": []}, {"text": "shows the mean performance of the network on the test patterns for each rule following training.", "labels": [], "entities": []}, {"text": "Note that chance performance for the roots was .033 and for the inflections .5 since there were 30 roots and 2 inflections in each category.", "labels": [], "entities": []}, {"text": "For all tasks, including both root and inflection identification the network performs well above chance.", "labels": [], "entities": [{"text": "inflection identification", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.7569502592086792}]}, {"text": "Performance is far from perfect for some of the rule types, but further improvement is possible with optimization of the learning parameters.", "labels": [], "entities": []}, {"text": "Interestingly, template rules, which are problematic for some symbolic approaches to morphology processing and acquisition, are among the easiest for the network.", "labels": [], "entities": [{"text": "morphology processing and acquisition", "start_pos": 85, "end_pos": 122, "type": "TASK", "confidence": 0.6569196507334709}]}, {"text": "Thus it is informative to investigate further how the network solved this task.", "labels": [], "entities": []}, {"text": "For the particular template rule, the two forms of each root shared the same initial and final consonant.", "labels": [], "entities": []}, {"text": "This tended to make root identification relatively easy.", "labels": [], "entities": [{"text": "root identification", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7078207433223724}]}, {"text": "With respect to inflections, the pattern is more like infixation than prefixation or suffixation because all of'the segments relevant to the tense, that is, the/a/s, are between the first and last segment.", "labels": [], "entities": []}, {"text": "But inflection identifation for the template is considerably higher than for infixation, probably because of the redundancy: the present tense is characterized by an /a/ in second position and a consonant in third position, the past tense by a consonant in second position and an/a/in third position.", "labels": [], "entities": []}, {"text": "To gain a better understanding of the way in which the network solves a template morphology task, a further experiment was conducted.", "labels": [], "entities": []}, {"text": "In this experiment, each root consisted of a sequence of three consonants from the set /p, b, m, t, d, s, n, k, g/.", "labels": [], "entities": []}, {"text": "There were three tense morphemes, each characterized by a particular template.", "labels": [], "entities": []}, {"text": "The present template was ClaC2aCaa, the past template aCtC2aaC3, and the future template aClaC2Caa.", "labels": [], "entities": []}, {"text": "Thus the three forms for the root pmn were pamana, apmaan, and apamna.", "labels": [], "entities": []}, {"text": "The network learns to recognize the tense templates very quickly; generalization is over 90% following only 25 epochs of training.", "labels": [], "entities": []}, {"text": "This task is relatively easy since the vowels appear in the same sequential positions for each tense.", "labels": [], "entities": []}, {"text": "More interesting is the performance of the root identification part of the network, which must learn to recognize the commonality among sequences of the same consonants even though, for any pair of forms fora given root, only one of the three consonants appears in the same position.", "labels": [], "entities": [{"text": "root identification", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.742501825094223}]}, {"text": "Performance reaches 72% on the To better visualize the problem, it helps to examine what happens in hidden-layer space for the root layer as a word is processed.", "labels": [], "entities": []}, {"text": "This 15-dimensional space is impossible to observe directly, but we can get an idea of the most significant movements through this space through the use of principal component analysis, a technique which is by now a familiar way of analyzing the behavior of recurrent networks.", "labels": [], "entities": []}, {"text": "Given a set of data vectors, principal component analysis yields a set of orthogonal vectors, or components, which are ranked in terms of how much of the variance in the data they account for.", "labels": [], "entities": []}, {"text": "Principal components for the root identification hidden layer vectors were extracted fora single network following 150 repetitions of the template training patterns.", "labels": [], "entities": [{"text": "root identification hidden layer", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.794008880853653}]}, {"text": "The paths through the space defined by the first two components of the root identification hidden layer as the three forms of the root pds are presented to the network are shown in.", "labels": [], "entities": []}, {"text": "Points marked in the same way represent the same root consonant.", "labels": [], "entities": []}, {"text": "1 What we see is that, as the root hidden layer processes the word, it passes through roughly similar regions in hidden-layer space as it encounters the consonants of the root, inde1Only two points appear for the first root consonant because the first two segments of the past and future forms of a given root are the same.", "labels": [], "entities": []}, {"text": "pendent of their sequential position.", "labels": [], "entities": []}, {"text": "Ina sense these regions correspond to the autosegments of autosegmental phonological and morphological analyses.", "labels": [], "entities": []}], "tableCaptions": []}