{"title": [{"text": "Extracting Noun Phrases from Large-Scale Texts: A Hybrid Approach and Its Automatic Evaluation", "labels": [], "entities": [{"text": "Extracting Noun Phrases from Large-Scale Texts", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.9072431822617849}]}], "abstractContent": [{"text": "To acquire noun phrases from running texts is useful for many applications, such as word grouping, terminology indexing, etc.", "labels": [], "entities": [{"text": "word grouping", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.7842473387718201}, {"text": "terminology indexing", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.873346209526062}]}, {"text": "The reported literatures adopt pure probabilistic approach, or pure rule-based noun phrases grammar to tackle this problem.", "labels": [], "entities": []}, {"text": "In this paper, we apply a probabilistic chunker to deciding the implicit boundaries of constituents and utilize the linguistic knowledge to extract the noun phrases by a finite state mechanism.", "labels": [], "entities": []}, {"text": "The test texts are SUSANNE Corpus and the results are evaluated by comparing the parse field of SUSANNE Corpus automatically.", "labels": [], "entities": [{"text": "SUSANNE Corpus", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.9136106073856354}, {"text": "SUSANNE Corpus", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.8613990247249603}]}, {"text": "The results of this preliminary experiment are encouraging.", "labels": [], "entities": []}], "introductionContent": [{"text": "From the cognitive point of view, human being must recognize, learn and understand the entities or concepts (concrete or abstract) in the texts for natural language comprehension.", "labels": [], "entities": []}, {"text": "These entities or concepts are usually described by noun phrases.", "labels": [], "entities": []}, {"text": "The evidences from the language learning of children also show the belief.", "labels": [], "entities": []}, {"text": "Therefore, if we can grasp the noun phases of the texts, we will understand the texts to some extent.", "labels": [], "entities": []}, {"text": "This consideration is also captured by theories of discourse analysis, such as Discourse Representation Theory.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7318775802850723}, {"text": "Discourse Representation Theory", "start_pos": 79, "end_pos": 110, "type": "TASK", "confidence": 0.7544186313947042}]}, {"text": "Traditionally, to make out the noun phrases in a text means to parse the text and to resolve the attachment relations among the constituents.", "labels": [], "entities": []}, {"text": "However, parsing the text completely is very difficult, since various ambiguities cannot be resolved solely by syntactic or semantic information.", "labels": [], "entities": [{"text": "parsing the text", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.8680037260055542}]}, {"text": "Do we really need to fully parse the texts in every application?", "labels": [], "entities": []}, {"text": "Some researchers apply shallow or partial parsers to acquiring specific patterns from texts.", "labels": [], "entities": []}, {"text": "These tell us that it is not necessary to completely parse the texts for some applications.", "labels": [], "entities": []}, {"text": "This paper will propose a probabilistic partial parser and incorporate linguistic knowledge to extract noun phrases.", "labels": [], "entities": []}, {"text": "The partial parser is motivated by an intuition: (1) When we read a sentence, we read it chunk by chunk.", "labels": [], "entities": []}, {"text": "Abney uses two level grammar rules to implement the parser through pure LR parsing technique.", "labels": [], "entities": [{"text": "Abney", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9312565326690674}, {"text": "LR parsing", "start_pos": 72, "end_pos": 82, "type": "TASK", "confidence": 0.717997282743454}]}, {"text": "The first level grammar rule takes care of the chunking process.", "labels": [], "entities": []}, {"text": "The second level grammar rule tackles the attachment problems among chunks.", "labels": [], "entities": []}, {"text": "Historically, our statisticsbased partial parser is called chunker.", "labels": [], "entities": []}, {"text": "The chunker receives tagged texts and outputs a linear chunk sequences.", "labels": [], "entities": []}, {"text": "We assign a syntactic head and a semantic head to each chunk.", "labels": [], "entities": []}, {"text": "Then, we extract the plausible maximal noun phrases according to the information of syntactic head and semantic head, and a finite state mechanism with only 8 states.", "labels": [], "entities": []}, {"text": "Section 2 will give a brief review of the works for the acquisition of noun phrases.", "labels": [], "entities": []}, {"text": "Section 3 will describe the language model for chunker.", "labels": [], "entities": []}, {"text": "Section 4 will specify how to apply linguistic knowledge to assigning heads to each chunk.", "labels": [], "entities": []}, {"text": "Section 5 will list the experimental results of chunker.", "labels": [], "entities": []}, {"text": "Following Section 5, Section 6 will give the performance of our work on the retrieval of noun phrases.", "labels": [], "entities": [{"text": "retrieval of noun phrases", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.8407026529312134}]}, {"text": "The possible extensions of the proposed work will be discussed in Section 7.", "labels": [], "entities": []}, {"text": "Section 8 will conclude the remarks.", "labels": [], "entities": []}, {"text": "proposes apart of speech tagger and a simple noun phrase extractor.", "labels": [], "entities": [{"text": "noun phrase extractor", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.6755696932474772}]}, {"text": "His noun phrase extractor brackets the noun phrases of input tagged texts according to two probability matrices: one is starting noun phrase matrix; the other is ending noun phrase matrix.", "labels": [], "entities": []}, {"text": "The methodology is a simple version of.", "labels": [], "entities": []}, {"text": "Church lists a sample text in the Appendix of his paper to show the performance of his work.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.7447008490562439}]}, {"text": "It demonstrates only 5 out of 248 noun phrases are omitted.", "labels": [], "entities": []}, {"text": "Because the tested text is too small to assess the results, the experiment for large volume of texts is needed.", "labels": [], "entities": []}, {"text": "reports a tool, LEXTER, for extracting terminologies from texts.", "labels": [], "entities": [{"text": "LEXTER", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9794451594352722}]}, {"text": "LEXTER triggers two-stage processing: 1) analysis (by identification of frontiers), which extracts the maximal-length noun phrase: 2) parsing (the maximal-length noun phrases), which, furthermore, acquires the terminology embedded in the noun phrases.", "labels": [], "entities": []}, {"text": "Bourigault declares the LEXTER extracts 95\u00b0/'0 maximal-length noun phrases, that is, 43500 out of 46000 from test corpus.", "labels": [], "entities": [{"text": "LEXTER", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.7352138757705688}]}, {"text": "The result is validated by an expert.", "labels": [], "entities": []}, {"text": "However, the precision is not reported in the Boruigault's paper.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.999666690826416}, {"text": "Boruigault's paper", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.7700260480244955}]}], "datasetContent": [{"text": "Following the procedures depicted in, we should train a chunker firstly.", "labels": [], "entities": [{"text": "chunker", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.963134765625}]}, {"text": "This is done by using the SUSANNE Corpus as the training texts.", "labels": [], "entities": [{"text": "SUSANNE Corpus", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.8900763988494873}]}, {"text": "The SUSANNE Corpus is a modified and condensed version of Brown Corpus   In order to avoid the errors introduced by tagger, the SUSANNE corpus is used as the training and testing texts.", "labels": [], "entities": [{"text": "SUSANNE Corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8450108170509338}, {"text": "Brown Corpus", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9851993024349213}, {"text": "SUSANNE corpus", "start_pos": 128, "end_pos": 142, "type": "DATASET", "confidence": 0.8092994093894958}]}, {"text": "Note the tags of SUSANNE corpus are mapped to LOB corpus.", "labels": [], "entities": [{"text": "SUSANNE corpus", "start_pos": 17, "end_pos": 31, "type": "DATASET", "confidence": 0.8983095288276672}, {"text": "LOB corpus", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.9413299262523651}]}, {"text": "The 3/4 of texts of each categories of SUSANNE Corpus are both for training the chunker and testing the chunker (inside test).", "labels": [], "entities": [{"text": "SUSANNE Corpus", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.7403583824634552}]}, {"text": "The rest texts are only for testing (outside test).", "labels": [], "entities": []}, {"text": "Every tree structure contained in the parse field is extracted to form a potential chunk grammar and the adjacent tree structures are also extracted to form a potential context chunk grammar.", "labels": [], "entities": []}, {"text": "After the training process, total 10937 chunk grammar rules associated with different scores and 37198 context chunk grammar rules are extracted.", "labels": [], "entities": []}, {"text": "These chunk grammar rules are used in the chunking process.", "labels": [], "entities": []}, {"text": "lists the time taken for processing SUSANNE corpus.", "labels": [], "entities": [{"text": "SUSANNE corpus", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.7162151634693146}]}, {"text": "This experiment is executed on the Sun Sparc 10, model 30 workstation, T denotes time, W word, C chunk, and S sentence.", "labels": [], "entities": [{"text": "Sun Sparc 10", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.9158464471499125}]}, {"text": "Therefore, T/W means the time taken to process a word on average.", "labels": [], "entities": [{"text": "T/W", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9224992990493774}]}, {"text": "This criterion is based on an observation that each nonterminal node has a chance to dominate a chunk. is the experimental results of testing the SUSANNE Corpus according to the specified criterion.", "labels": [], "entities": [{"text": "SUSANNE Corpus", "start_pos": 146, "end_pos": 160, "type": "DATASET", "confidence": 0.7972066104412079}]}, {"text": "As usual, the symbol C denotes chunk and S denotes sentence.", "labels": [], "entities": []}, {"text": "shows the chunker has more than 98% chunk correct rate and 94% sentence correct rate in outside test, and 99% chunk correct rate and 97% sentence correct rate in inside test.", "labels": [], "entities": [{"text": "chunk correct rate", "start_pos": 36, "end_pos": 54, "type": "METRIC", "confidence": 0.7384958664576212}, {"text": "sentence correct rate", "start_pos": 63, "end_pos": 84, "type": "METRIC", "confidence": 0.707294096549352}, {"text": "chunk correct rate", "start_pos": 110, "end_pos": 128, "type": "METRIC", "confidence": 0.7896423935890198}, {"text": "sentence correct rate", "start_pos": 137, "end_pos": 158, "type": "METRIC", "confidence": 0.7176933288574219}]}, {"text": "Note that once a chunk is mischopped, the sentence is also mischopped.", "labels": [], "entities": []}, {"text": "Therefore, sentence correct rate is always less than chunk correct rate.", "labels": [], "entities": [{"text": "sentence correct rate", "start_pos": 11, "end_pos": 32, "type": "METRIC", "confidence": 0.6118331154187521}, {"text": "chunk correct rate", "start_pos": 53, "end_pos": 71, "type": "METRIC", "confidence": 0.8288441101710001}]}, {"text": "gives a direct view of the correct rate of this chunker.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. The Overview of SUSANNE Corpus", "labels": [], "entities": [{"text": "Overview of SUSANNE", "start_pos": 14, "end_pos": 33, "type": "DATASET", "confidence": 0.5641509095827738}]}, {"text": " Table 3. The Processing Time", "labels": [], "entities": []}, {"text": " Table 5. Time for Acquisition of Noun Phrases", "labels": [], "entities": [{"text": "Time", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9843995571136475}, {"text": "Acquisition of Noun Phrases", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.7346220761537552}]}, {"text": " Table 7. The Number of Noun Phrases in Corpus", "labels": [], "entities": []}, {"text": " Table 8. The average precision is 95%.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9987332224845886}]}, {"text": " Table 8. Precision of Our System", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8381308913230896}]}, {"text": " Table 9. The limitation of Values for Recall", "labels": [], "entities": [{"text": "limitation", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9005889892578125}, {"text": "Recall", "start_pos": 39, "end_pos": 45, "type": "TASK", "confidence": 0.3548736870288849}]}]}