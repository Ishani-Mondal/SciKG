{"title": [{"text": "GENERALIZED CHART ALGORITHM: AN EFFICIENT PROCEDURE FOR COST-BASED ABDUCTION", "labels": [], "entities": [{"text": "GENERALIZED CHART ALGORITHM", "start_pos": 0, "end_pos": 27, "type": "METRIC", "confidence": 0.5663140813509623}, {"text": "AN EFFICIENT PROCEDURE FOR COST-BASED ABDUCTION", "start_pos": 29, "end_pos": 76, "type": "METRIC", "confidence": 0.7027800579865774}]}], "abstractContent": [{"text": "We present an efficient procedure for cost-based abduction , which is based on the idea of using chart parsers as proof procedures.", "labels": [], "entities": []}, {"text": "We discuss in detail three features of our algorithm-goal-driven bottom-up derivation, tabulation of the partial results , and agenda control mechanism-and report the results of the preliminary experiments, which show how these features improve the computational efficiency of cost-based abduction.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken language understanding is one of the most challenging research areas in natural language processing.", "labels": [], "entities": [{"text": "Spoken language understanding", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9191685716311137}, {"text": "natural language processing", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.6604934632778168}]}, {"text": "Since spoken language is incomplete in various ways, i.e., containing speech errors, ellipsis, metonymy, etc., spoken language understanding systems should have the ability to process incomplete inputs by hypothesizing the underlying information.", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 111, "end_pos": 140, "type": "TASK", "confidence": 0.691359281539917}]}, {"text": "The abduction-based approach has provided a simple and elegant way to realize such a task.", "labels": [], "entities": []}, {"text": "Consider the following 3apanese sentence: (1) Sfseki katta (a famous writer) buy PAST This sentence contains two typical phenomena arising in spoken language, i.e., metonymy and the ellipsis of a particle.", "labels": [], "entities": [{"text": "PAST", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9227358102798462}]}, {"text": "When this sentence is uttered under the situation where the speaker reports his experience, its natural interpretation is the speaker bought a SSseki novel.", "labels": [], "entities": []}, {"text": "To derive this interpretation, we need to resolve the following problems: \u2022 The metonymy implied by the noun phrase S6seki is expanded to a S6seki novel, based on the pragmatic knowledge that the name of a writer is sometimes used to refer to his novel.", "labels": [], "entities": []}, {"text": "\u2022 The particle-less thematic relation between the verb katta and the noun phrase SSseki is determined to be the object case relation, based on the semantic knowledge that the object case relation between a trading action and a commodity can be linguistically expressed as a thematic relation.", "labels": [], "entities": []}, {"text": "This interpretation is made by abduction.", "labels": [], "entities": []}, {"text": "For instance, the above semantic knowledge is stated, in terms of the predicate logic, as follows: sem(e,x) C trade(e) A commodity(x) A obj(e,x) Then, the inference process derives the consequent sem(e,x) by hypothesizing an antecedent obj(e,x), which is never proved from the observed facts.", "labels": [], "entities": []}, {"text": "This process is called abduction.", "labels": [], "entities": []}, {"text": "Of course, there maybe several other possibilities that support the thematic relation sem(e,x).", "labels": [], "entities": []}, {"text": "For instance, the thematic relation being determined to be the agent case relation, sentence (1) can have another interpretation, i.e., Sfseki bought something, which, under some other situations, might be more feasible than the first interpretation.", "labels": [], "entities": []}, {"text": "To cope with feasibility, the abduction-based model usually manages the mechanism for evaluating the goodness of the interpretation.", "labels": [], "entities": []}, {"text": "This is known as cost-based abduction.", "labels": [], "entities": []}, {"text": "In cost-based abduction, each assumption bears a certain cost.", "labels": [], "entities": []}, {"text": "For instance, the assumption obj(e,x), introduced by applying rule (2), is specified to have a cost of, say, $2.", "labels": [], "entities": []}, {"text": "The goodness of the interpretation is evaluated by accumulating the costs of all the assumptions involved.", "labels": [], "entities": []}, {"text": "The whole process of interpreting an utterance is depicted in the following schema: 1.", "labels": [], "entities": [{"text": "interpreting an utterance", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.8877695600191752}]}, {"text": "Find all possible interpretations, and 2.", "labels": [], "entities": []}, {"text": "Select the one that has the lowest cost.", "labels": [], "entities": []}, {"text": "In our example, the interpretation that assumes the thematic relation to be the object case relation, with the metonymy being expanded to a S6seki novel, is cheaper than the interpretation that assumes the thematic relation to be the agent case relation; hence, the former is selected.", "labels": [], "entities": []}, {"text": "An apparent problem here is the high computational cost; because abduction allows many possibilities, the schema involves very heavy computation.", "labels": [], "entities": []}, {"text": "Particularly in the spoken language understanding task, we need to consider a great number of possibilities when hypothesizing various underlying information.", "labels": [], "entities": [{"text": "spoken language understanding task", "start_pos": 20, "end_pos": 54, "type": "TASK", "confidence": 0.7415178716182709}]}, {"text": "This makes the abduction process computationally demanding, and reduces the practicality of abduction-based systems.", "labels": [], "entities": []}, {"text": "The existing models do not provide any basic solution to this problem.) dealt with the problem, but those solutions are applicable only to the propositional case, where the search space is represented as a directed graph overground formulas.", "labels": [], "entities": []}, {"text": "In other words, they did not provide away to build such graphs from rules, which, in general, contain variables and can be recursive.", "labels": [], "entities": []}, {"text": "This paper provides a basic and practical solution to the computation problem of cost-based abduction.", "labels": [], "entities": []}, {"text": "The basic idea comes from the natural language parsing literature.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6705364485581716}]}, {"text": "As Pereira and pointed out, there is a strong connection between parsing and deduction.", "labels": [], "entities": []}, {"text": "They showed that parsing of DCG can be seen as a special case of deduction of Horn clauses; conversely, deduction can be seen as a generalization of parsing.", "labels": [], "entities": [{"text": "parsing of DCG", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7937978704770406}]}, {"text": "Their idea of using chart parsers as deductive-proof procedures can easily be extended to the idea of using chart parsers as abductive-proof procedures.", "labels": [], "entities": []}, {"text": "Because chart parsers have many advantages from the viewpoint of computational efficiency, chart-based abductive-proof procedures are expected to nicely solve the computation problem.", "labels": [], "entities": [{"text": "chart parsers", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.7433926463127136}]}, {"text": "Our algorithm, proposed in this paper, has the following features, which considerably enhance the computational efficiency of cost-based abduction: 1.", "labels": [], "entities": []}, {"text": "Goal-driven bottom-up derivation, which reduces the search space.", "labels": [], "entities": []}, {"text": "2. Tabulation of the partial results, which avoids the recomputation of the same goal.", "labels": [], "entities": [{"text": "Tabulation", "start_pos": 3, "end_pos": 13, "type": "TASK", "confidence": 0.8643028736114502}]}, {"text": "3. Agenda control mechanism, which realizes various search strategies to find the best solution efficiently.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, we explain the basic idea of our algorithm, and then present the details of the algorithm along with simple examples.", "labels": [], "entities": []}, {"text": "Next, we report the results of the preliminary experiments, which clearly show how the above features of our algorithm improve the computational efficiency.", "labels": [], "entities": []}, {"text": "Then, we compare our algorithm with Pereira and Warren's algorithm, and finally conclude the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted preliminary experiments to compare four methods of cost-based abduction: top-down algorithm (TD), head-driven algorithm (HD), generalized chart algorithm with full-search (GCF), and generalized chart algorithm with ordered search (GCO).", "labels": [], "entities": []}, {"text": "The rules used for the experiments are in the spoken language understanding task, and they are rather small (51 chain rules + 35 non-chain rules).", "labels": [], "entities": [{"text": "spoken language understanding task", "start_pos": 46, "end_pos": 80, "type": "TASK", "confidence": 0.7434571832418442}]}, {"text": "The test sentences include one verb and 1-4 noun phrases, e.g., sentence (1).", "labels": [], "entities": []}, {"text": "The performance of each method is measured by the number of computation steps, i.e., the number of derivation steps in TD and HD, and the number of passive and active edges in GCF and GCO.", "labels": [], "entities": []}, {"text": "The decimals in parentheses show the ratio of the performance of each method to the performance of TD.", "labels": [], "entities": []}, {"text": "The table clearly shows how the three features of our algorithm improve the computational efficiency.", "labels": [], "entities": []}, {"text": "The improvement from TD to HD is due to the first feature, i.e., goal-driven bottom-up derivation, which eliminates about 50% of the computation steps; the improvement from HD to GCF is due to the second feature, i.e., tabulation of the partial results, which decreases the number of steps another 13%-23%; the improvement from GCF to GCO is due to the last feature, i.e., the agenda control mechanism, which decreases the number of steps another 4%-8%.", "labels": [], "entities": []}, {"text": "In short, the efficiency is improved, maximally, about four times.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comp. among TD, HD, GCF, and GCO", "labels": [], "entities": [{"text": "GCO", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.6078609228134155}]}]}