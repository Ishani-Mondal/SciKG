{"title": [{"text": "OPTIMIZING THE COMPUTATIONAL LEXICALIZATION OF LARGE GRAMMARS", "labels": [], "entities": [{"text": "OPTIMIZING THE COMPUTATIONAL LEXICALIZATION OF LARGE GRAMMARS", "start_pos": 0, "end_pos": 61, "type": "METRIC", "confidence": 0.6819981506892613}]}], "abstractContent": [{"text": "The computational lexicalization of a grammar is the optimization of the links between lexicalized rules and lexical items in order to improve the quality of the bottom-up filtering during parsing.", "labels": [], "entities": []}, {"text": "This problem is N P-complete and untractable on large grammars.", "labels": [], "entities": []}, {"text": "An approximation algorithm is presented.", "labels": [], "entities": []}, {"text": "The quality of the suboptimal solution is evaluated on real-world grammars as well as on randomly generated ones.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexicalized grammar formalisms and more specifically Lexicalized Tree Adjoining Grammars (LTAGs) give a lexical account of phenomena which cannot be considered as purely syntactic.", "labels": [], "entities": [{"text": "Lexicalized Tree Adjoining Grammars (LTAGs", "start_pos": 53, "end_pos": 95, "type": "TASK", "confidence": 0.6506680995225906}]}, {"text": "A formalism is said to be lexicalized if it is composed of structures or rules associated with each lexical item and operations to derive new structures from these elementary ones.", "labels": [], "entities": []}, {"text": "The choice of the lexical anchor of a rule is supposed to be determined on purely linguistic grounds.", "labels": [], "entities": []}, {"text": "This is the linguistic side of lexicalization which links to each lexical head a set of minimal and complete structures.", "labels": [], "entities": []}, {"text": "But lexicalization also has a computational aspect because parsing algorithms for lexicalized grammars can take advantage of lexical links through a two-step strategy (.", "labels": [], "entities": []}, {"text": "The first step is the selection of the set of rules or elementary structures associated with the lexical items in the input sentence ~.", "labels": [], "entities": []}, {"text": "In the second step, the parser uses the rules filtered by the first step.", "labels": [], "entities": []}, {"text": "The two kinds of anchors corresponding to these two aspects of lexicalization can be considered separately : \u2022 The linguistic anchors are used to access the grammar, update the data, gather together items with similar structures, organize the grammar into a hierarchy...", "labels": [], "entities": []}, {"text": "\u2022 The computational anchors are used to select the relevant rules during the first step of parsing and to improve computational and conceptual tractability of the parsing algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 91, "end_pos": 98, "type": "TASK", "confidence": 0.9795629978179932}]}, {"text": "Unlike linguistic lexicalization, computational anchoring concerns any of the lexical items found in a rule and is only motivated by the quality of the induced filtering.", "labels": [], "entities": [{"text": "computational anchoring", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7591179013252258}]}, {"text": "For example, the systematic linguistic anchoring of the rules describing \"Nmetal alloy\" to their head noun \"alloy\" should be avoided and replaced by a more distributed lexicalization.", "labels": [], "entities": []}, {"text": "Then, only a few rules \"Nmetal alloy\" will be activated when encountering the word \"alloy\" in the input.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the problem of the optimization of computational lexicalization.", "labels": [], "entities": []}, {"text": "We study how to choose the computational anchors of a lexicalized grammar so that the distribution of the rules onto the lexical items is the most uniform possible The computational anchor of a rule should not be optional (viz included in a disjunction) to make sure that it will be encountered in any string derived from this rule. with respect to rule weights.", "labels": [], "entities": []}, {"text": "Although introduced with reference to LTAGs, this optimization concerns any portion of a grammar where rules include one or more potential lexical anchors such as Head Driven Phrase Structure or Lexicalized Context-Free Grammar.", "labels": [], "entities": []}, {"text": "This algorithm is currently used to good effect in FASTR a unification-based parser for terminology extraction from large corpora.", "labels": [], "entities": [{"text": "FASTR", "start_pos": 51, "end_pos": 56, "type": "TASK", "confidence": 0.4038432538509369}, {"text": "terminology extraction from large corpora", "start_pos": 88, "end_pos": 129, "type": "TASK", "confidence": 0.888140332698822}]}, {"text": "In this framework, terms are represented by rules in a lexicalized constraint-based formalism.", "labels": [], "entities": []}, {"text": "Due to the large size of the grammar, the quality of the lexicalization is a determining factor for the computational tractability of the application.", "labels": [], "entities": []}, {"text": "FASTR is applied to automatic indexing on industrial data and lays a strong emphasis on the handling of term variations.", "labels": [], "entities": [{"text": "FASTR", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.35265496373176575}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the following part, we prove that the problem of the Lexicalization of a Grammar is NP-complete and hence that there is no better algorithm known to solve it than an exponential exhaustive search.", "labels": [], "entities": [{"text": "Lexicalization of a Grammar", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.7625607401132584}]}, {"text": "As this solution is untractable on large data, an approximation algorithm is presented which has a computational-time complexity proportional to the cubic size of the grammar.", "labels": [], "entities": []}, {"text": "In the last part, an evaluation of this algorithm on real-world grammars of 6,622 and 71,623 rules as well as on randomly generated ones confirms its computational tractability and the quality of the lexicalization.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Bench marks of the approximation algorithm on eight randomly generated grammars.", "labels": [], "entities": []}]}