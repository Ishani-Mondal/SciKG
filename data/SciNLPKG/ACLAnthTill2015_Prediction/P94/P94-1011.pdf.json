{"title": [{"text": "PRECISE N-GRAM PROBABILITIES FROM STOCHASTIC CONTEXT-FREE GRAMMARS", "labels": [], "entities": [{"text": "FROM STOCHASTIC CONTEXT-FREE GRAMMARS", "start_pos": 29, "end_pos": 66, "type": "METRIC", "confidence": 0.7923002690076828}]}], "abstractContent": [{"text": "We present an algorithm for computing n-gram probabilities from stochastic context-free grammars, a procedure that can alleviate some of the standard problems associated with n-grams (estimation from sparse data, lack of linguistic structure, among others).", "labels": [], "entities": []}, {"text": "The method operates via the computation of substring expectations, which in turn is accomplished by solving systems of linear equations derived from the grammar.", "labels": [], "entities": []}, {"text": "The procedure is fully implemented and has proved viable and useful in practice.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic language modeling with n-gram grammars (particularly bigram and trigram) has proven extremely useful for such tasks as automated speech recognition, part-ofspeech tagging, and word-sense disambiguation, and lead to simple, efficient algorithms.", "labels": [], "entities": [{"text": "automated speech recognition", "start_pos": 133, "end_pos": 161, "type": "TASK", "confidence": 0.6459860404332479}, {"text": "part-ofspeech tagging", "start_pos": 163, "end_pos": 184, "type": "TASK", "confidence": 0.7990642189979553}, {"text": "word-sense disambiguation", "start_pos": 190, "end_pos": 215, "type": "TASK", "confidence": 0.7326839566230774}]}, {"text": "Unfortunately, working with these grammars can be problematic for several reasons: they have large numbers of parameters, so reliable estimation requires a very large training corpus and/or sophisticated smoothing techniques; it is very hard to directly model linguistic knowledge (and thus these grammars are practically incomprehensible to human inspection); and the models are not easily extensible, i.e., if anew word is added to the vocabulary, none of the information contained in an existing n-gram will tell anything about the n-grams containing the new item.", "labels": [], "entities": []}, {"text": "Stochastic context-free grammars (SCFGs), on the other hand, are not as susceptible to these problems: they have many fewer parameters (so can be reasonably trained with smaller corpora); they capture linguistic generalizations, and are easily understood and written, by linguists; and they can be extended straightforwardly based on the underlying linguistic knowledge.", "labels": [], "entities": [{"text": "Stochastic context-free grammars (SCFGs)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7169585824012756}]}, {"text": "In this paper, we present a technique for computing an n-gram grammar from an existing SCFG--an attempt to get the best of both worlds.", "labels": [], "entities": []}, {"text": "Besides developing the mathematics involved in the computation, we also discuss efficiency and implementation issues, and briefly report on our experience confirming its practical feasibility and utility.", "labels": [], "entities": []}, {"text": "The technique of compiling higher-level grammatical models into lower-level ones has precedents: report building a word-pair grammar from more elaborate language models to achieve good coverage, by random generation of sentences.", "labels": [], "entities": []}, {"text": "In our own group, the current approach was predated by an alternative one that essentially relied on approximating bigram probabilities through Monte-Carlo sampling from SCFGs.", "labels": [], "entities": []}], "datasetContent": [{"text": "The algorithm described here has been implemented, and is being used to generate bigrams fora speech recognizer that is part of the BeRP spoken-language system).", "labels": [], "entities": []}, {"text": "An early prototype of BeRP was used in an experiment to assess the benefit of using bigram probabilities obtained through SCFGs versus estimating them directly from the available training corpus.", "labels": [], "entities": []}, {"text": "4 The system's domain are inquiries about restaurants in the city of Berkeley.", "labels": [], "entities": []}, {"text": "The training corpus used had only 2500 sentences, with an average length of about 4.8 words/sentence.", "labels": [], "entities": []}, {"text": "Our experiments made use of a context-free grammar hand-written for the BeRP domain.", "labels": [], "entities": [{"text": "BeRP domain", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.8917238414287567}]}, {"text": "With 1200 rules and a vocabulary of 1 I00 words, this grammar was able to parse 60% of the training corpus.", "labels": [], "entities": []}, {"text": "Computing the bigram probabilities from this SCFG takes about 24 hours on a SPARCstation 2-class machine.", "labels": [], "entities": []}, {"text": "5 In experiment 1, the recognizer used bigrams that were estimated directly from the training corpus, without any smoothing, resulting in a word error rate of 35.1%.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 140, "end_pos": 155, "type": "METRIC", "confidence": 0.7436116933822632}]}, {"text": "In experiment 2, a different set of bigram probabilities was used, computed from the context-free grammar, whose probabilities had previously been estimated from the same training corpus, using standard EM techniques.", "labels": [], "entities": []}, {"text": "This resulted in a word error rate of 35.3%.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 19, "end_pos": 34, "type": "METRIC", "confidence": 0.7448029319445292}]}, {"text": "This may seem surprisingly good given the low coverage of the underlying CFGs, but notice that the conversion into bigrams is bound to result in a less constraining language model, effectively increasing coverage.", "labels": [], "entities": []}, {"text": "Finally, in experiment 3, the bigrams generated from the SCFG were augmented by those from the raw training data, in a proportion of 200,000 : 2500.", "labels": [], "entities": []}, {"text": "We have not attempted to optimize this mixture proportion, e.g., by deleted interpolation.", "labels": [], "entities": []}, {"text": "6 With the bigram estimates thus obtained, the word error rate dropped to 33.5%.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.7237241069475809}]}, {"text": "(All error rates were measured on a separate test corpus.)", "labels": [], "entities": [{"text": "error rates", "start_pos": 5, "end_pos": 16, "type": "METRIC", "confidence": 0.961595207452774}]}, {"text": "The experiment therefore supports our earlier argument that more sophisticated language models, even if far from perfect, can improve n-gram estimates obtained directly from sample data.", "labels": [], "entities": []}, {"text": "4Corpus and grammar sizes, as well as the recognition performance figures reported here are not up-to-date with respect to the latest version of BeRP.", "labels": [], "entities": []}, {"text": "For ACL-94 we expect to have revised results available that reflect the current performance of the system.", "labels": [], "entities": [{"text": "ACL-94", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.8648275136947632}]}, {"text": "5Unlike the rest of BeRP, this computation is implemented in Lisp/CLOS and could be speeded up considerably if necessary.", "labels": [], "entities": []}, {"text": "6This proportion comes about because in the original system, predating the method described in this paper, bigrams had to be estimated from the SCFG by random sampling.", "labels": [], "entities": []}, {"text": "Generating 200,000 sentence samples was found to give good converging estimates for the bigrams.", "labels": [], "entities": []}, {"text": "The bigrams from the raw training sentences were then simply added to the randomly generated ones.", "labels": [], "entities": []}, {"text": "We later verified that the bigrams estimated from the SCFG were indeed identical to the ones computed directly using the method described here.", "labels": [], "entities": []}], "tableCaptions": []}