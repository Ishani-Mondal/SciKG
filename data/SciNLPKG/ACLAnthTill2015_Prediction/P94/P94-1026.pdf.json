{"title": [], "abstractContent": [{"text": "Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees.", "labels": [], "entities": [{"text": "Explanation-based generalization", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6367084234952927}]}, {"text": "This allows very much faster parsing and gives a lower error rate, at the price of a small loss in coverage.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9823684096336365}, {"text": "error rate", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9850417077541351}, {"text": "coverage", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9415068030357361}]}, {"text": "Previously, it has been necessary to specify the tree-cutting criteria (or operationality criteria) manually; here they are derived automatically from the training set and the desired coverage of the specialized grammar.", "labels": [], "entities": []}, {"text": "This is done by assigning an en-tropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values.", "labels": [], "entities": []}, {"text": "BACKGROUND Previous work by Manny Rayner and the author, see [Samuelsson &~ Rayner 1991] attempts to tailor an existing natural-language system to a specific application domain by extracting a specialized grammar from the original one using a large set of training examples.", "labels": [], "entities": [{"text": "BACKGROUND", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.5838441252708435}]}, {"text": "The training set is a treebank consisting of implicit parse trees that each specify a verified analysis of an input sentence.", "labels": [], "entities": []}, {"text": "The parse trees are implicit in the sense that each node in the tree is the (mnemonic) name of the grammar rule resolved on at that point, rather than the syntactic category of the LHS of the grammar rule as is the casein an ordinary parse tree.", "labels": [], "entities": []}, {"text": "Figure 1 shows five examples of implicit parse trees.", "labels": [], "entities": []}, {"text": "The analyses are verified in the sense that each analysis has been judged to be the preferred one for that input sentence by a human evaluator using a semi-automatic evaluation method.", "labels": [], "entities": []}, {"text": "A new grammar is created by cutting up each implicit parse tree in the treebank at appropriate points, creating a set of new rules that consist of chunks of original grammar rules.", "labels": [], "entities": []}, {"text": "The LHS of each new rule will be the LHS phrase of the original grammar rule at the root of the tree chunk and the RHS will be the RHS phrases of the rules in the leaves of the tree chunk.", "labels": [], "entities": [{"text": "RHS", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9860388040542603}]}, {"text": "For example, cutting up the first parse tree of Figure 1 at the NP of the rule vp_v_np yields rules 2 and 3 of Figure 3.", "labels": [], "entities": []}, {"text": "The idea behind this is to create a specialized grammar that retains a high coverage but allows very much faster parsing.", "labels": [], "entities": []}, {"text": "This has turned out to be possible-speedups compared to using the original grammar of", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "A module realizing this scheme has been implemented and applied to the very setup used for the previous experiments with the hand-coded tree-cutting criteria, see].", "labels": [], "entities": []}, {"text": "2100 of the verified parse trees constituted the training set, while 230 of them were used for the test set.", "labels": [], "entities": []}, {"text": "The table below summarizes the results for some grammars of different coverage extracted using: 1.", "labels": [], "entities": []}, {"text": "2. Induced tree-cutting criteria where the node entropy was taken to be the phrase entropy of the RHIS phrase of the dominating grammar rule.", "labels": [], "entities": []}, {"text": "3. Induced tree-cutting criteria where the node entropy was the sum of the phrase entropy of the RHS phrase of the dominating grammar rule and the weighted sum of the phrase entropies of the LHSs of the alternative choices of grammar rules to resolve on.", "labels": [], "entities": []}, {"text": "In the latter two cases experiments were carried out both with and without the restrictions on neighbouring cutnodes discussed in the previous section.", "labels": [], "entities": []}, {"text": "With the mixed entropy scheme it seems important to include the restrictions on neighbouring cutnodes, while this does not seem to be the case with the RHS phrase entropy scheme.", "labels": [], "entities": []}, {"text": "A potential explanation for the significantly higher average parsing times for all grammars extracted using the induced tree-cutting criteria is that these are in general recursive, while the handcoded criteria do not allow recursion, and thus only produce grammars that generate finite languages.", "labels": [], "entities": []}, {"text": "Although the hand-coded tree-cutting criteria are substantially better than the induced ones, we must remember that the former produce a grammar that in median allows 60 times faster processing than the original grammar and parser do.", "labels": [], "entities": []}, {"text": "This means that even if the induced criteria produce grammars that area factor two or three slower than this, they are still approximately one and a half order of magnitude faster than the original setup.", "labels": [], "entities": []}, {"text": "Also, this is by no means a closed research issue, but merely a first attempt to realize the scheme, and there is no doubt in my mind that it can be improved on most substantially.", "labels": [], "entities": []}], "tableCaptions": []}