{"title": [{"text": "PART-OF-SPEECH TAGGING USING A VARIABLE MEMORY MARKOV MODEL", "labels": [], "entities": [{"text": "PART-OF-SPEECH TAGGING USING A VARIABLE MEMORY MARKOV MODEL", "start_pos": 0, "end_pos": 59, "type": "METRIC", "confidence": 0.6308338679373264}]}], "abstractContent": [{"text": "We present anew approach to disambiguating syntactically ambiguous words in context, based on Variable Memory Markov (VMM) models.", "labels": [], "entities": [{"text": "disambiguating syntactically ambiguous words in context", "start_pos": 28, "end_pos": 83, "type": "TASK", "confidence": 0.817247748374939}]}, {"text": "In contrast to fixed-length Markov models, which predict based on fixed-length histories, variable memory Markov models dynamically adapt their history length based on the training data, and hence may use fewer parameters.", "labels": [], "entities": []}, {"text": "Ina test of a VMM based tagger on the Brown corpus, 95.81% of tokens are correctly classified.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9851205348968506}]}], "introductionContent": [{"text": "Many words in English have several parts of speech (POS).", "labels": [], "entities": []}, {"text": "For example \"book\" is used as a noun in \"She read a book.\" and as a verb in \"She didn't book a trip.\"", "labels": [], "entities": []}, {"text": "Part-of-speech tagging is the problem of determining the syntactic part of speech of an occurrence of a word in context.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7878054678440094}, {"text": "determining the syntactic part of speech of an occurrence of a word in context", "start_pos": 41, "end_pos": 119, "type": "TASK", "confidence": 0.5122427195310593}]}, {"text": "In any given English text, most tokens are syntactically ambiguous since most of the high-frequency English words have several parts of speech.", "labels": [], "entities": []}, {"text": "Therefore, a correct syntactic classification of words in context is important for most syntactic and other higherlevel processing of natural language text.", "labels": [], "entities": [{"text": "syntactic classification of words in context", "start_pos": 21, "end_pos": 65, "type": "TASK", "confidence": 0.8104787369569143}]}, {"text": "Two stochastic methods have been widely used for POS tagging: fixed order Markov models and Bidden Markov models.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9668434858322144}]}, {"text": "Fixed order Markov models are used in and.", "labels": [], "entities": []}, {"text": "Since the order of the model is assumed to be fixed, a short memory (small order) is typically used, since the number of possible combinations grows exponentially.", "labels": [], "entities": []}, {"text": "For example, assuming there are 184 different tags, as in the Brown corpus, there are 1843 = 6,229,504 different order 3 combinations of tags (of course not all of these will actually occur, see).", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.933113157749176}]}, {"text": "Because of the large number of parameters higher-order fixed length models are hard to estimate.", "labels": [], "entities": []}, {"text": "(See) fora rule-based approach to incorporating higher-order information.)", "labels": [], "entities": []}, {"text": "Ina Hidden iarkov Model (HMM), a different state is defined for each POS tag and the transition probabilities and the output probabilities are estimated using the EM algorithm, which guarantees convergence to.a local minimum.", "labels": [], "entities": []}, {"text": "The advantage of an HMM is that it can be trained using untagged text.", "labels": [], "entities": []}, {"text": "On the other hand, the training procedure is time consuming, and a fixed model (topology) is assumed.", "labels": [], "entities": []}, {"text": "Another disadvantage is due to the local convergence properties of the EM algorithm.", "labels": [], "entities": []}, {"text": "The solution obtained depends on the initial setting of the model's parameters, and different solutions are obtained for different parameter initialization schemes.", "labels": [], "entities": []}, {"text": "This phenomenon discourages linguistic analysis based on the output of the model.", "labels": [], "entities": [{"text": "linguistic analysis", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7341856956481934}]}, {"text": "We present anew method based on variable memory Markov models (VMM).", "labels": [], "entities": []}, {"text": "The VMM is an approximation of an unlimited order Markov source.", "labels": [], "entities": [{"text": "VMM", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.84739750623703}]}, {"text": "It can incorporate both the static (order 0) and dynamic (higher-order) information systematically, while keeping the ability to change the model due to future observations.", "labels": [], "entities": []}, {"text": "This approach is easy to implement, the learning algorithm and classification of new tags are computationally efficient, and the results achieved, using simplified assumptions for the static tag probabilities, are encouraging.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Estimates for tag conversion", "labels": [], "entities": [{"text": "tag conversion", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.9679161906242371}]}, {"text": " Table 3: Most common errors.", "labels": [], "entities": [{"text": "errors", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9748262763023376}]}]}