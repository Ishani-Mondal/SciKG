{"title": [{"text": "A STOCHASTIC FINITE-STATE WORD-SEGMENTATION ALGORITHM FOR CHINESE", "labels": [], "entities": [{"text": "A STOCHASTIC FINITE-STATE WORD-SEGMENTATION ALGORITHM FOR CHINESE", "start_pos": 0, "end_pos": 65, "type": "METRIC", "confidence": 0.8142710838999067}]}], "abstractContent": [{"text": "We present a stochastic finite-state model for segmenting Chinese text into dictionary entries and productively derived words, and providing pronunciations for these words; the method incorporates a class-based model in its treatment of personal names.", "labels": [], "entities": []}, {"text": "We also evaluate the system's performance, taking into account the fact that people often do not agree on a single seg-mentation.", "labels": [], "entities": []}, {"text": "THE PROBLEM The initial step of any text analysis task is the tok-enization of the input into words.", "labels": [], "entities": [{"text": "text analysis task", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8043694694836935}]}, {"text": "For many writing systems, using whitespace as a delimiter for words yields reasonable results.", "labels": [], "entities": []}, {"text": "However, for Chinese and other systems where whitespace is not used to delimit words, such trivial schemes will notwork.", "labels": [], "entities": []}, {"text": "Chinese writing is morphosyllabic (DeFrancis, 1984), meaning that each hanzi-'Chinese character'-(nearly always) represents a single syllable that is (usually) also a single morpheme.", "labels": [], "entities": []}, {"text": "Since in Chinese, as in English, words maybe polysyllabic, and since hanzi are written with no intervening spaces, it is not trivial to reconstruct which hanzi to group into words.", "labels": [], "entities": []}, {"text": "While for some applications it maybe possible to bypass the word-segmentation problem and work straight from hanzi, there are several reasons why this approach will notwork in a text-to-speech (TI'S) system for Mandarin Chinese-the primary intended application of our segmenter.", "labels": [], "entities": []}, {"text": "These reasons include: 1.", "labels": [], "entities": []}, {"text": "Many hanzi are homographs whose pronunciation depends upon word affiliation.", "labels": [], "entities": []}, {"text": "So, ~ is pronounced deO ~ when it is a prenominal modification marker, but di4 in the word [] ~ mu4di4 'goal'; ~ is normally ganl 'dry',but qian2 in a person's given name.", "labels": [], "entities": []}, {"text": "2. Some phonological rules depend upon correct word-segmentation, including Third Tone Sandhi (Shih, 1986), which changes a 3 tone into a 2 tone before another 3 tone: ,J~]~ xiao3 [lao3 shu3] 'lit-t We use pinyin transliteration with numbers representing tones.", "labels": [], "entities": []}, {"text": "66 tie rat', becomes xiao3 [ lao2-shu3 ], rather than xiao2 [ lao2-shu3 ], because the rule first applies within the word lao3-shu3, blocking its phrasal application.", "labels": [], "entities": []}, {"text": "While a minimal requirement for building a Chi-nese word-segmenter is a dictionary, a dictionary is insufficient since there are several classes of words that are not generally found in dictionaries.", "labels": [], "entities": []}, {"text": "Morphologically Derived Words: PJ~l~f{l xiao3-jiang4-menO (little general-plural) 'little generals'.", "labels": [], "entities": []}, {"text": "2. Personal Names: ~,~ zhoul enl-lai2 'Zhou Enlai'.", "labels": [], "entities": []}, {"text": "3. Transliterated Foreign Names: ~i~::,,~ bu4-lang 3-shi4-wei2-ke4 'Brunswick'.", "labels": [], "entities": [{"text": "Brunswick", "start_pos": 68, "end_pos": 77, "type": "DATASET", "confidence": 0.9567192196846008}]}, {"text": "We present a stochastic finite-state model for segmenting Chinese text into dictionary entries and words derived via the above-mentioned productive processes; as part of the treatment of personal names, we discuss a class-based model which uses the Good-Turing method to estimate costs of previously unseen personal names.", "labels": [], "entities": []}, {"text": "The segmenter handles the grouping of hanzi into words and outputs word pronunciations, with default pronunciations for hanzi it cannot group; we focus here primarily on the system's ability to segment text appropriately (rather than on its pronunciation abilities).", "labels": [], "entities": [{"text": "grouping of hanzi into words", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.8435077905654907}]}, {"text": "We evaluate various specific aspects of the seg-mentation, and provide an evaluation of the overall segmentation performance: this latter evaluation compares the performance of the system with that of several human judges, since even people do not agree on a single correct way to segment a text, PREVIOUS WORK There is a sizable literature on Chinese word segmenta-tion: recent reviews include (Wang et al., 1990; Wu and Tseng, 1993).", "labels": [], "entities": []}, {"text": "Roughly, previous work can be classified into purely statistical approaches (Sproat and Shih, 1990), statistical approaches which incorporate lexical knowledge (Fan and Tsai, 1988; Lin et al., 1993), and approaches that include lexical knowledge combined with heuristics (Chen and Liu, 1992).", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In this section we present a partial evaluation of the current system in three parts.", "labels": [], "entities": []}, {"text": "The first is an evaluation of the system's ability to mimic humans at the task of segmenting text into word-sized units; the second evaluates the proper name identification; the third measures the performance on morphological analysis.", "labels": [], "entities": [{"text": "proper name identification", "start_pos": 146, "end_pos": 172, "type": "TASK", "confidence": 0.605865478515625}]}, {"text": "To date we have not done a separate evaluation of foreign name recognition.", "labels": [], "entities": [{"text": "foreign name recognition", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.7157808144887289}]}, {"text": "Evaluation of the Segmentation as a Whole: Previous reports on Chinese segmentation have invariably 4The current model is too simplistic in several respects.", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.5089974403381348}]}, {"text": "For instance, the common 'suffixes', -nia (e.g., Virginia) and -sia are normally transliterated as ~=~ ni2-ya3 and ~]~ ~n~ xil-ya3, respectively.", "labels": [], "entities": []}, {"text": "The interdependence between ]:~ or ~, and ~r~ is not captured by our model, but this could easily be remedied.", "labels": [], "entities": []}, {"text": "cited performance either in terms of a single percentcorrect score, or else a single precision-recall pair.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 85, "end_pos": 101, "type": "METRIC", "confidence": 0.983954668045044}]}, {"text": "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.", "labels": [], "entities": []}, {"text": "Thus, rather than give a single evaluative score, we prefer to compare the performance of our method with the judgments of several human subjects.", "labels": [], "entities": []}, {"text": "To this end, we picked 100 sentences at random containing 4372 total hanzi from a test corpus.", "labels": [], "entities": []}, {"text": "We asked six native speakers --three from Taiwan (T1-T3), and three from the Mainland (M1-M3) --to segment the corpus.", "labels": [], "entities": []}, {"text": "Since we could not bias the subjects towards a particular segmentation and did not presume linguistic sophistication on their part, the instructions were simple: subjects were to mark all places they might plausibly pause if they were reading the text aloud.", "labels": [], "entities": []}, {"text": "An examination of the subjects' bracketings confirmed that these instructions were satisfactory in yielding plausible word-sized units.", "labels": [], "entities": []}, {"text": "Various segmentation approaches were then compared with human performance: 1.", "labels": [], "entities": []}, {"text": "A greedy algorithm, GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.", "labels": [], "entities": []}, {"text": "2. An 'anti-greedy' algorithm, AG: instead of the longest match, take the shortest match at each point.", "labels": [], "entities": [{"text": "AG", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9767532348632812}]}, {"text": "3. The method being described --henceforth ST.", "labels": [], "entities": []}, {"text": "Two measures that can be used to compare judgments are: 1.", "labels": [], "entities": []}, {"text": "For each pair of judges consider one judge as the standard, computing the precision of the other's judgments relative to this standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.998863697052002}]}, {"text": "For each pair of judges, consider one judge as the standard, computing the recall of the other's judgments relative to this standard.", "labels": [], "entities": [{"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9994654059410095}]}, {"text": "Obviously, for judges J1 and J2, taking ,/1 as standard and computing the precision and recall for J2 yields the same results as taking J2 as the standard, and computing for Jr, respectively, the recall and precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.998791515827179}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9891710877418518}, {"text": "recall", "start_pos": 196, "end_pos": 202, "type": "METRIC", "confidence": 0.9991726279258728}, {"text": "precision", "start_pos": 207, "end_pos": 216, "type": "METRIC", "confidence": 0.9928020238876343}]}, {"text": "We therefore used the arithmetic mean of each interjudge precision-recall pair as a single measure of interjudge similarity..", "labels": [], "entities": []}, {"text": "In addition to the automatic methods, AG, GR and ST, just discussed, we also added to the plot the values for the current algorithm using only dictionary entries (i.e., no productively derived words, or names).", "labels": [], "entities": [{"text": "AG", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9465207457542419}, {"text": "ST", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9577991366386414}]}, {"text": "This is to allow for fair comparison between the statistical method, and GR, which is also purely dictionary-based.", "labels": [], "entities": [{"text": "GR", "start_pos": 73, "end_pos": 75, "type": "DATASET", "confidence": 0.5682201385498047}]}, {"text": "As can be seen, GR and this 'pared-down' statistical method perform quite similarly, though the statistical method is still slightly better.", "labels": [], "entities": [{"text": "GR", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.48604342341423035}]}, {"text": "AG clearly performs much less like humans than these methods, whereas the full statistical algorithm, including morphological derivatives and names, performs most closely to humans among the automatic methods.", "labels": [], "entities": []}, {"text": "It can be also seen clearly in this plot, two of the Taiwan speakers cluster very closely together, and the third Taiwan speaker is also close in the most significant dimension (the z axis).", "labels": [], "entities": []}, {"text": "Two of the Mainlanders also cluster close together but, interestingly, not particularly close to the Taiwan speakers; the third Mainlander is much more similar to the Taiwan speakers.", "labels": [], "entities": [{"text": "Mainlanders", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.9795489311218262}, {"text": "Mainlander", "start_pos": 128, "end_pos": 138, "type": "DATASET", "confidence": 0.9686784148216248}]}, {"text": "Personal Name Identification: To evaluate personal name identification, we randomly selected 186 sentences containing 12,000 hanzi from our test corpus, and segmented the text automatically, tagging personal names; note that for names there is always a single unambiguous answer, unlike the more general question of which segmentation is correct.", "labels": [], "entities": [{"text": "Personal Name Identification", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7256499926249186}, {"text": "personal name identification", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.6469866434733073}]}, {"text": "The performance was 80.99% recall and 61.83% precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9998165965080261}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9995243549346924}]}, {"text": "Interestingly, Chang et al. reported 80.67% recall and 91.87% precision on an 11,000 word corpus: seemingly, our system finds as many names as their system, but with four times as many false hits.", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9991680383682251}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9983653426170349}]}, {"text": "However, we have reason to doubt Chang et al.'s performance claims.", "labels": [], "entities": []}, {"text": "Without using the same test corpus, direct comparison is obviously difficult; fortunately Chang et al. included a list of about 60 example sentence fragments that exemplified various categories of performance for their system.", "labels": [], "entities": []}, {"text": "The performance of our system on those sentences appeared rather better than theirs.", "labels": [], "entities": []}, {"text": "Now, on a set of 11 sentence fragments where they reported 100% recall and precision for name identification, we had 80% precision and 73% recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.998984158039093}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9994420409202576}, {"text": "name identification", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.8763690888881683}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9996764659881592}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9982434511184692}]}, {"text": "However, they listed two sets, one consisting of 28 fragments and the other of 22 fragments in which they had 0% precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9996368885040283}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9987816214561462}]}, {"text": "On the first of these our system had 86% precision and 64% recall; on the second it had 19% precision and 33% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9994223117828369}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9995013475418091}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9993415474891663}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.998578667640686}]}, {"text": "Note that it is in precision that our overall performance would appear to be poorer than that of Chang et al., yet based on their published examples, our we present results from small test corpora for some productive affixes; as with names, the segmentation of morphologically derived words is generally either right or wrong.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9995012283325195}]}, {"text": "The first four affixes are so-called resultative affixes: they denote some property of the resultant state of an verb, as in ~,,:;~ ~\" wang4-bu4-1iao3 (forget-notattain) 'cannot forget'.", "labels": [], "entities": []}, {"text": "The last affix is the nominal plural.", "labels": [], "entities": []}, {"text": "Note that ~ in ~,:~: ]\" is normally pronounced as leO, but when part of a resultative it is liao3.", "labels": [], "entities": []}, {"text": "In the table are the (typical) classes of words to which the affix attaches, the number found in the test corpus by the method, the number correct (with a precision measure), and the number missed (with a recall measure).", "labels": [], "entities": [{"text": "precision measure", "start_pos": 155, "end_pos": 172, "type": "METRIC", "confidence": 0.9776795208454132}, {"text": "recall measure", "start_pos": 205, "end_pos": 219, "type": "METRIC", "confidence": 0.9823972880840302}]}], "tableCaptions": [{"text": " Table 2: Similarity matrix for segmentation judgments  Judges AG GR  ST  M1  M2  M3  T1  T2  T3  AG  0.70 0.70 0.43 0.42 0.60 0.60 0.62 0.59  GR  0.99 0.62 0.64 0.79 0.82 0.81 0.72  ST  0.64 0.67 0.80 0.84 0.82 0.74  0.77  M1  M2  M3  T1  T2", "labels": [], "entities": [{"text": "segmentation judgments  Judges AG GR  ST  M1  M2  M3  T1  T2  T3  AG", "start_pos": 32, "end_pos": 100, "type": "TASK", "confidence": 0.8580108147401077}]}, {"text": " Table 3: Performance on morphological analysis.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7753004133701324}]}]}