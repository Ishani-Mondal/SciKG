{"title": [{"text": "A Corpus-based Approach to Automatic Compound Extraction", "labels": [], "entities": [{"text": "Automatic Compound Extraction", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.6071920891602834}]}], "abstractContent": [{"text": "An automatic compound retrieval method is proposed to extract compounds within a text message.", "labels": [], "entities": []}, {"text": "It uses n-gram mutual information, relative frequency count and parts of speech as the features for compound extraction.", "labels": [], "entities": [{"text": "compound extraction", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7849021852016449}]}, {"text": "The problem is mod-eled as a two-class classification problem based on the distributional characteristics of n-gram tokens in the compound and the non-compound clusters.", "labels": [], "entities": []}, {"text": "The recall and precision using the proposed approach are 96.2% and 48.2% for bigram compounds and 96.6% and 39.6% for trigram compounds fora testing corpus of 49,314 words.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9996073842048645}, {"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9993379712104797}]}, {"text": "A significant cutdown in processing time has been observed.", "labels": [], "entities": []}], "introductionContent": [{"text": "In technical manuals, technical compounds are very common.", "labels": [], "entities": []}, {"text": "Therefore, the quality of their translations greatly affects the performance of a machine translation system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7093075215816498}]}, {"text": "If a compound is not in the dictionary, it would be translated incorrectly in many cases; the reason is: many compounds are not compositional, which means that the translation of a compound is not the composite of the respective translations of the individual words [Chen and.", "labels": [], "entities": []}, {"text": "For example, the translation of 'green house' into Chinese is not the composite of the Chinese ~anslations of 'green' and 'house'.", "labels": [], "entities": [{"text": "translation of 'green house'", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.8679817914962769}]}, {"text": "Under such circumstances, the number of parsing ambiguities will also increase due to the large number of possible parts of speech combinations for the individual words.", "labels": [], "entities": []}, {"text": "It will then reduce the accuracy rate in disambiguation and also increase translation time.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 24, "end_pos": 37, "type": "METRIC", "confidence": 0.9873650372028351}, {"text": "translation", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.930916428565979}]}, {"text": "In practical operations, a computer-translated \u2022 manual is usually concurrently processed by several posteditors; thus, to maintain the consistency of translated terminologies among different posteditors is very important, because terminological consistency is a major advaatage of machine translation over human translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 282, "end_pos": 301, "type": "TASK", "confidence": 0.7310902774333954}]}, {"text": "If all the terminologies can be entered into the dictionary before translation, the consistency can be automatically maintained, the translation quality can be greatly improved, and lots of postediting time and consistency maintenance cost can be saved.", "labels": [], "entities": [{"text": "consistency", "start_pos": 84, "end_pos": 95, "type": "METRIC", "confidence": 0.9962856769561768}]}, {"text": "Since compounds are rather productive and new compounds are created from day today, it is impossible to exhaustively store all compounds in a dictionary.", "labels": [], "entities": []}, {"text": "Also, it is too costly and timeconsuming to inspect the manual by people for the compound candidates and update the dictionary beforehand.", "labels": [], "entities": []}, {"text": "Therefore, it is important that the compounds be found and entered into the dictionary before translation without much human effort; an automatic and quantitative tool for extracting compounds from the text is thus seriously required.", "labels": [], "entities": []}, {"text": "Several compound extracting approaches have been proposed in the literature.", "labels": [], "entities": [{"text": "compound extracting", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7414193749427795}]}, {"text": "Traditional rule-based systems are to encode some sets of rules to extract likely compounds from the text.", "labels": [], "entities": []}, {"text": "However, a lot of compounds obtained with such approaches may not be desirable since they are not assigned objective preferences.", "labels": [], "entities": []}, {"text": "Thus, it is not clear how likely one candidate is considered a compound.", "labels": [], "entities": []}, {"text": "In LEXTER, for example, a text corpus is analyzed and parsed to produce a list of likely terminological units to be validated by an expert.", "labels": [], "entities": []}, {"text": "While it allows the test to be done very quickly due to the use of simple analysis and parsing rules, instead of complete syntactic analysis, it does not suggest quantitatively to what extent a unit is considered a terminology and how often such a unit is used in real text.", "labels": [], "entities": []}, {"text": "It might therefore extract many inappropriate terminologies with high false alarm.", "labels": [], "entities": [{"text": "false alarm", "start_pos": 70, "end_pos": 81, "type": "METRIC", "confidence": 0.8340952694416046}]}, {"text": "In another statistical approach by, the association ratio of a word pair and the dispersion of the second word are used to decide if it. is a fixed phrase (a compound).", "labels": [], "entities": [{"text": "association ratio", "start_pos": 40, "end_pos": 57, "type": "METRIC", "confidence": 0.9248779118061066}]}, {"text": "The drawback is that it does not take the number of occurrences of the word pair into account; therefore, it is not.", "labels": [], "entities": []}, {"text": "known if the word pair is commonly or rarely used.", "labels": [], "entities": []}, {"text": "Since there is no performance evaluation reported in both frameworks, it is not clear how well they work.", "labels": [], "entities": []}, {"text": "A previous framework by [Wu and shows that the mutual information measure and the relative frequency information are discriminative for extracting highly associated and frequently encountered n-gram as compound.", "labels": [], "entities": []}, {"text": "However, many non-compound n-grams, like 'is a', which have high mutual information and high relative frequency of occurrence are also recognized as compounds.", "labels": [], "entities": []}, {"text": "Such n-grams can be rejected if syntactic constraints are applied.", "labels": [], "entities": []}, {"text": "In this paper, we thus incorporate parts of speech of the words as a third feature for compound extraction.", "labels": [], "entities": [{"text": "compound extraction", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8480738699436188}]}, {"text": "An automatic compound retrieval method combining the joint features of n-gram mutual information, relative frequency count and parts of speech is proposed.", "labels": [], "entities": []}, {"text": "A likelihood ratio test method, designed fora two-class classification task, is used to check whether an n-gram is a compound.", "labels": [], "entities": [{"text": "likelihood ratio test", "start_pos": 2, "end_pos": 23, "type": "METRIC", "confidence": 0.895054300626119}, {"text": "two-class classification task", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.7767059803009033}]}, {"text": "Those n-grams that pass the test are then listed in the order of significance for the lexicographers to build these entries into the dictionary.", "labels": [], "entities": []}, {"text": "It is found that, by incorporating parts of speech information, both the recall and precision for compound extraction is improved.", "labels": [], "entities": [{"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9997269511222839}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9995942711830139}, {"text": "compound extraction", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7317958623170853}]}, {"text": "The simulation result shows that the proposed approach works well.", "labels": [], "entities": []}, {"text": "A significant cutdown of the postediting time has been observed when using this tool in an MT system, and the translation quality is greatly improved.", "labels": [], "entities": [{"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.9624147415161133}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Distribution statistics of  pounds", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.8588275909423828}]}, {"text": " Table 2: Distribution statistics of non- compounds", "labels": [], "entities": []}, {"text": " Table 3: Performance for bigrams", "labels": [], "entities": [{"text": "bigrams", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.6794764399528503}]}, {"text": " Table 4: Performance for trigrams", "labels": [], "entities": [{"text": "Performance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9746860265731812}]}]}