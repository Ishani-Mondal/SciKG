{"title": [{"text": "A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation", "labels": [], "entities": [{"text": "Phrase-based String-to-Dependency Translation", "start_pos": 37, "end_pos": 82, "type": "TASK", "confidence": 0.769864559173584}]}], "abstractContent": [{"text": "We introduce a shift-reduce parsing algorithm for phrase-based string-to-dependency translation.", "labels": [], "entities": [{"text": "phrase-based string-to-dependency translation", "start_pos": 50, "end_pos": 95, "type": "TASK", "confidence": 0.6481179694334666}]}, {"text": "As the algorithm generates dependency trees for partial translations left-to-right in decoding, it allows for efficient integration of both n-gram and dependency language models.", "labels": [], "entities": []}, {"text": "To resolve conflicts in shift-reduce parsing, we propose a maximum entropy model trained on the derivation graph of training data.", "labels": [], "entities": []}, {"text": "As our approach combines the merits of phrase-based and string-to-dependency models, it achieves significant improvements over the two baselines on the NIST Chinese-English datasets.", "labels": [], "entities": [{"text": "NIST Chinese-English datasets", "start_pos": 152, "end_pos": 181, "type": "DATASET", "confidence": 0.9589686195055643}]}], "introductionContent": [{"text": "Modern statistical machine translation approaches can be roughly divided into two broad categories: phrase-based and syntax-based.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 7, "end_pos": 38, "type": "TASK", "confidence": 0.6335808535416921}]}, {"text": "Phrase-based approaches treat phrase, which is usually a sequence of consecutive words, as the basic unit of translation ().", "labels": [], "entities": []}, {"text": "As phrases are capable of memorizing local context, phrase-based approaches excel at handling local word selection and reordering.", "labels": [], "entities": []}, {"text": "In addition, it is straightforward to integrate n-gram language models into phrase-based decoders in which translation always grows left-to-right.", "labels": [], "entities": []}, {"text": "As a result, phrase-based decoders only need to maintain the boundary words on one end to calculate language model probabilities.", "labels": [], "entities": []}, {"text": "However, as phrase-based decoding usually casts translation as a string concatenation problem and permits arbitrary permutation, it proves to be NP-complete.", "labels": [], "entities": []}, {"text": "Syntax-based approaches, on the other hand, model the hierarchical structure of natural languages (;.", "labels": [], "entities": []}, {"text": "As syntactic information can be exploited to provide linguistically-motivated reordering rules, predicting non-local permutation is computationally tractable in syntax-based approaches.", "labels": [], "entities": [{"text": "predicting non-local permutation", "start_pos": 96, "end_pos": 128, "type": "TASK", "confidence": 0.8795064687728882}]}, {"text": "Unfortunately, as syntax-based decoders often generate target-language words in a bottom-up way using the CKY algorithm, integrating n-gram language models becomes more expensive because they have to maintain target boundary words at both ends of a partial translation.", "labels": [], "entities": []}, {"text": "Moreover, syntax-based approaches often suffer from the rule coverage problem since syntactic constraints rule out a large portion of nonsyntactic phrase pairs, which might help decoders generalize well to unseen data ( ).", "labels": [], "entities": [{"text": "rule coverage", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.7400406301021576}]}, {"text": "Furthermore, the introduction of nonterminals makes the grammar size significantly bigger than phrase tables and leads to higher memory requirement.", "labels": [], "entities": []}, {"text": "As a result, incremental decoding with hierarchical structures has attracted increasing attention in recent years.", "labels": [], "entities": []}, {"text": "While some authors try to integrate syntax into phrase-based decoding, others develop incremental algorithms for syntax-based models (.", "labels": [], "entities": []}, {"text": "Despite these successful efforts, challenges still remain for both directions.", "labels": [], "entities": []}, {"text": "While parsing algorithms can be used to parse partial translations in phrase-based decoding, the search space is significantly enlarged since there are exponentially many parse trees for exponentially many translations.", "labels": [], "entities": [{"text": "parse partial translations", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.8552477558453878}]}, {"text": "On the other hand, although target words can be generated left-to-right by altering the way of tree transversal in syntaxbased models, it is still difficult to reach full rule coverage as compared with phrase: A training example consisting of a (romanized) Chinese sentence, an English dependency tree, and the word alignment between them.", "labels": [], "entities": []}, {"text": "Each translation rule is composed of a source phrase, a target phrase with a set of dependency arcs.", "labels": [], "entities": []}, {"text": "Following, we distinguish between fixed, floating, and ill-formed structures.", "labels": [], "entities": []}, {"text": "In this paper, we propose a shift-reduce parsing algorithm for phrase-based string-to-dependency translation.", "labels": [], "entities": [{"text": "phrase-based string-to-dependency translation", "start_pos": 63, "end_pos": 108, "type": "TASK", "confidence": 0.6640444397926331}]}, {"text": "The basic unit of translation in our model is string-to-dependency phrase pair, which consists of a phrase on the source side and a dependency structure on the target side.", "labels": [], "entities": []}, {"text": "The algorithm generates well-formed dependency structures for partial translations left-to-right using string-todependency phrase pairs.", "labels": [], "entities": []}, {"text": "Therefore, our approach is capable of combining the advantages of both phrase-based and syntax-based approaches: 1 of the original string-to-dependency grammar () by excluding rules with non-terminals.", "labels": [], "entities": []}, {"text": "2. full rule coverage: all phrase pairs, both syntactic and non-syntactic, can be used in our algorithm.", "labels": [], "entities": []}, {"text": "This is the same with Moses ().", "labels": [], "entities": []}, {"text": "3. efficient integration of n-gram language model: as translation grows left-to-right in our algorithm, integrating n-gram language models is straightforward.", "labels": [], "entities": []}, {"text": "4. exploiting syntactic information: as the shift-reduce parsing algorithm generates target language dependency trees in decoding, dependency language models ( can be used to encourage linguistically-motivated reordering.", "labels": [], "entities": []}, {"text": "5. resolving local parsing ambiguity: as dependency trees for phrases are memorized in rules, our approach avoids resolving local parsing ambiguity and explores in a smaller search space than parsing word-by-word on the fly in decoding ().", "labels": [], "entities": [{"text": "resolving local parsing ambiguity", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.6745840311050415}]}, {"text": "We evaluate our method on the NIST ChineseEnglish translation datasets.", "labels": [], "entities": [{"text": "NIST ChineseEnglish translation datasets", "start_pos": 30, "end_pos": 70, "type": "DATASET", "confidence": 0.8589202165603638}]}, {"text": "Experiments show that our approach significantly outperforms both phrase-based ( and string-todependency approaches) in terms of BLEU and TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9971952438354492}, {"text": "TER", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.935795783996582}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparison with Moses (", "labels": [], "entities": []}, {"text": " Table 3: Contribution of maximum entropy shift- reduce parsing model. \"standard\" denotes us- ing standard features of phrase-based system.  Adding dependency language model (\"depLM\")  and the maximum entropy shift-reduce parsing  model (\"maxent\") significantly improves BLEU  and TER on the development set, both separately  and jointly.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 271, "end_pos": 275, "type": "METRIC", "confidence": 0.9983949065208435}, {"text": "TER", "start_pos": 281, "end_pos": 284, "type": "METRIC", "confidence": 0.9963151812553406}]}]}