{"title": [{"text": "Smatch: an Evaluation Metric for Semantic Feature Structures", "labels": [], "entities": []}], "abstractContent": [{"text": "The evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.7854292690753937}, {"text": "large-scale semantic structure annotation", "start_pos": 101, "end_pos": 142, "type": "TASK", "confidence": 0.6205360293388367}]}, {"text": "However, there is no widely-used metric to evaluate whole-sentence semantic structures.", "labels": [], "entities": []}, {"text": "In this paper , we present smatch, a metric that calculates the degree of overlap between two semantic feature structures.", "labels": [], "entities": []}, {"text": "We give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of semantic parsing is to generate all semantic relationships in a text.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7404983639717102}]}, {"text": "Its output is often represented by whole-sentence semantic structures.", "labels": [], "entities": []}, {"text": "Evaluating such structures is necessary for semantic parsing tasks, as well as semantic annotation tasks which create linguistic resources for semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.769866019487381}, {"text": "semantic parsing", "start_pos": 143, "end_pos": 159, "type": "TASK", "confidence": 0.7184306681156158}]}, {"text": "However, there is no widely-used evaluation method for whole-sentence semantic structures.", "labels": [], "entities": [{"text": "whole-sentence semantic structures", "start_pos": 55, "end_pos": 89, "type": "TASK", "confidence": 0.6679680148760477}]}, {"text": "Current whole-sentence semantic parsing is mainly evaluated in two ways: 1.", "labels": [], "entities": [{"text": "whole-sentence semantic parsing", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.7648539741834005}]}, {"text": "task correctness), which evaluates on an NLP task that uses the parsing results; 2.", "labels": [], "entities": [{"text": "task correctness", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6242607980966568}]}, {"text": "whole-sentence accuracy), which counts the number of sentences parsed completely correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9901740550994873}]}, {"text": "Nevertheless, it is worthwhile to explore evaluation methods that use scores which range from 0 to 1 (\"partial credit\") to measure whole-sentence semantic structures.", "labels": [], "entities": []}, {"text": "By using such methods, we are able to differentiate between two similar wholesentence semantic structures regardless of specific tasks or domains.", "labels": [], "entities": []}, {"text": "In this work, we provide an evaluation metric that uses the degree of overlap between two whole-sentence semantic structures as the partial credit.", "labels": [], "entities": []}, {"text": "In this paper, we observe that the difficulty of computing the degree of overlap between two whole-sentence semantic feature structures comes from determining an optimal variable alignment between them, and further prove that finding such alignment is NP-complete.", "labels": [], "entities": []}, {"text": "We investigate how to compute this metric and provide several practical and replicable computing methods by using Integer Linear Programming (ILP) and hill-climbing method.", "labels": [], "entities": []}, {"text": "We show that our metric can be used for measuring the annotator agreement in largescale linguistic annotation, and evaluating semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 126, "end_pos": 142, "type": "TASK", "confidence": 0.7090900987386703}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Inter-annotator smatch agreement for 5 groups of sentences, as computed with seven different  methods (Base, ILP, R, 10R, S, S+4R, S+9R). The number 1-5 indicate the sentence group number. Bold  scores are search errors.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy and running time (seconds) of  various computing methods of smatch over 200  AMR pairs.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9950048327445984}, {"text": "running time", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9528533816337585}]}]}