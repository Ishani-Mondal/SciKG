{"title": [{"text": "Using Supervised Bigram-based ILP for Extractive Summarization", "labels": [], "entities": [{"text": "Extractive Summarization", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6913319081068039}]}], "abstractContent": [{"text": "In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework.", "labels": [], "entities": [{"text": "extractive document summarization", "start_pos": 63, "end_pos": 96, "type": "TASK", "confidence": 0.6302457849184672}]}, {"text": "For each bigram, a regression model is used to estimate its frequency in the reference summary.", "labels": [], "entities": []}, {"text": "The regression model uses a variety of indicative features and is trained dis-criminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary.", "labels": [], "entities": []}, {"text": "During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7506088018417358}]}, {"text": "We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations.", "labels": [], "entities": [{"text": "TAC data sets", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.8843865593274435}]}, {"text": "We also conducted various analysis to show the impact of bigram selection, weight estimation , and ILP setup.", "labels": [], "entities": [{"text": "weight estimation", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.6475122272968292}]}], "introductionContent": [{"text": "Extractive summarization is a sentence selection problem: identifying important summary sentences from one or multiple documents.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9006146788597107}, {"text": "sentence selection", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7159578502178192}]}, {"text": "Many methods have been developed for this problem, including supervised approaches that use classifiers to predict summary sentences, graph based approaches to rank the sentences, and recent global optimization methods such as integer linear programming (ILP) and submodular methods.", "labels": [], "entities": []}, {"text": "These global optimization methods have been shown to be quite powerful for extractive summarization, because they try to select important sentences and remove redundancy at the same time under the length constraint.) introduced the concept-based ILP for summarization.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.5754644572734833}, {"text": "summarization", "start_pos": 254, "end_pos": 267, "type": "TASK", "confidence": 0.9807066917419434}]}, {"text": "Their system achieved the best result in the TAC 09 summarization task based on the ROUGE evaluation metric.", "labels": [], "entities": [{"text": "TAC 09 summarization task", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.5711220055818558}, {"text": "ROUGE", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.9464461803436279}]}, {"text": "In this approach the goal is to maximize the sum of the weights of the language concepts that appear in the summary.", "labels": [], "entities": []}, {"text": "They used bigrams as such language concepts.", "labels": [], "entities": []}, {"text": "The association between the language concepts and sentences serves as the constraints.", "labels": [], "entities": []}, {"text": "This ILP method is formally represented as below (see) for more details): ) c i \u2208 {0, 1} \u2200i (5) s j \u2208 {0, 1} \u2200j (6) c i and s j are binary variables (shown in (5) and) that indicate the presence of a concept and a sentence respectively.", "labels": [], "entities": []}, {"text": "w i is a concept's weight and Occ ij means the occurrence of concept i in sentence j.", "labels": [], "entities": [{"text": "Occ", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9941446185112}]}, {"text": "Inequalities (2)(3) associate the sentences and concepts.", "labels": [], "entities": []}, {"text": "They ensure that selecting a sentence leads to the selection of all the concepts it contains, and selecting a concept only happens when it is present in at least one of the selected sentences.", "labels": [], "entities": []}, {"text": "There are two important components in this concept-based ILP: one is how to select the concepts (c i ); the second is how to setup their weights (w i ).) used bigrams as concepts, which are selected from a subset of the sentences, and their document frequency as the weight in the objective function.", "labels": [], "entities": []}, {"text": "In this paper, we propose to find a candidate summary such that the language concepts (e.g., bigrams) in this candidate summary and the reference summary can have the same frequency.", "labels": [], "entities": []}, {"text": "We expect this restriction is more consistent with the ROUGE evaluation metric used for summarization).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9413547515869141}, {"text": "summarization", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.9855851531028748}]}, {"text": "In addition, in the previous conceptbased ILP method, the constraints are with respect to the appearance of language concepts, hence it cannot distinguish the importance of different language concepts in the reference summary.", "labels": [], "entities": []}, {"text": "Our method can decide not only which language concepts to use in ILP, but also the frequency of these language concepts in the candidate summary.", "labels": [], "entities": []}, {"text": "To estimate the bigram frequency in the summary, we propose to use a supervised regression model that is discriminatively trained using a variety of features.", "labels": [], "entities": []}, {"text": "Our experiments on several TAC summarization data sets demonstrate this proposed method outperforms the previous ILP system and often the best performing TAC system.", "labels": [], "entities": [{"text": "TAC summarization data sets", "start_pos": 27, "end_pos": 54, "type": "DATASET", "confidence": 0.8790176659822464}]}], "datasetContent": [{"text": "Based on the above analysis, we can seethe impact of the bigram set and their weights.", "labels": [], "entities": [{"text": "bigram set", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.7260981351137161}]}, {"text": "The following experiments are designed to demonstrate the best system performance we can achieve if we have access to good quality bigrams and weights.", "labels": [], "entities": []}, {"text": "Here we use the information from the reference summary.", "labels": [], "entities": []}, {"text": "The first is an oracle experiment, where we use all the bigrams from the reference summaries that are also in the original text.", "labels": [], "entities": []}, {"text": "In the ICSI ILP system, the weights are the document frequency from the multiple reference summaries.", "labels": [], "entities": [{"text": "ICSI ILP system", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.8659062385559082}]}, {"text": "In our ILP module, we use the term frequency of the bigram.", "labels": [], "entities": []}, {"text": "The oracle results are shown in.", "labels": [], "entities": []}, {"text": "We can see these are significantly better than the automatic systems.", "labels": [], "entities": []}, {"text": "From, we notice that ICSI's ILP performs marginally better than our proposed ILP.", "labels": [], "entities": [{"text": "ICSI's ILP", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.757339616616567}]}, {"text": "We hypothesize that one reason maybe that many bigrams in the summary reference only appear once.", "labels": [], "entities": []}, {"text": "shows the frequency of the bigrams in the summary.", "labels": [], "entities": []}, {"text": "Indeed 85% of bigram only appear once", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE-2 summarization results.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8585799932479858}, {"text": "summarization", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.6281737685203552}]}, {"text": " Table 2: Results using different weighting meth- ods on the top 100 bigrams generated from our  proposed system.", "labels": [], "entities": []}, {"text": " Table 3: Results using different weighting meth- ods based on the initial bigram sets. The average  number of bigrams is around 80 for each topic.", "labels": [], "entities": []}, {"text": " Table 6: Average number of bigrams for each term  frequency in one topic's reference summary.", "labels": [], "entities": []}, {"text": " Table 7: Summarization results when using the es- timated weights and only keeping the bigrams that  are in the reference summary.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9056736826896667}]}, {"text": " Table 8: Summarization performance when using  different training corpora.", "labels": [], "entities": []}]}