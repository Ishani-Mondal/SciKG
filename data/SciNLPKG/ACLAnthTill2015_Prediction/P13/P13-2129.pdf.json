{"title": [{"text": "Diathesis alternation approximation for verb clustering", "labels": [], "entities": [{"text": "Diathesis alternation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7628127932548523}, {"text": "verb clustering", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7417668104171753}]}], "abstractContent": [{"text": "Although diathesis alternations have been used as features for manual verb classification , and there is recent work on incorporating such features in computational models of human language acquisition , work on large scale verb classification has yet to examine the potential for using diathesis alternations as input features to the clustering process.", "labels": [], "entities": [{"text": "manual verb classification", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.6293674608071645}, {"text": "large scale verb classification", "start_pos": 212, "end_pos": 243, "type": "TASK", "confidence": 0.7517141401767731}]}, {"text": "This paper proposes a method for approximating diathesis alternation behaviour in corpus data and shows, using a state-of-the-art verb clustering system, that features based on alternation approximation outperform those based on independent subcategoriza-tion frames.", "labels": [], "entities": []}, {"text": "Our alternation-based approach is particularly adept at leveraging information from less frequent data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Diathesis alternations (DAs) are regular alternations of the syntactic expression of verbal arguments, sometimes accompanied by a change in meaning.", "labels": [], "entities": [{"text": "Diathesis alternations (DAs) are regular alternations of the syntactic expression of verbal arguments", "start_pos": 0, "end_pos": 101, "type": "TASK", "confidence": 0.6112408757209777}]}, {"text": "For example, The man broke the window \u2194 The window broke.", "labels": [], "entities": []}, {"text": "The syntactic phenomena are triggered by the underlying semantics of the participating verbs.'s seminal book provides a manual inventory both of DAs and verb classes where membership is determined according to participation in these alternations.", "labels": [], "entities": []}, {"text": "For example, most of the COOK verbs (e.g. bake, cook, fry . .", "labels": [], "entities": []}, {"text": ") can all take various DAs, such as the causative alternation, middle alternation and instrument subject alternation.", "labels": [], "entities": []}, {"text": "In computational linguistics, work inspired by Levin's classification has exploited the link between syntax and semantics for producing classifications of verbs.", "labels": [], "entities": []}, {"text": "Such classifications are useful fora wide variety of purposes such as semantic role labelling (), predicting unseen syntax, argument zoning) and metaphor identification (.", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.6724796195824941}, {"text": "predicting unseen syntax", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.9064168731371561}, {"text": "metaphor identification", "start_pos": 145, "end_pos": 168, "type": "TASK", "confidence": 0.8953596651554108}]}, {"text": "While Levin's classification can be extended manually), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages.", "labels": [], "entities": [{"text": "Levin's classification", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.6441522638003031}, {"text": "automatic verb classification", "start_pos": 107, "end_pos": 136, "type": "TASK", "confidence": 0.6461574037869772}]}, {"text": "Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im).", "labels": [], "entities": [{"text": "automatic classification", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.7453644871711731}]}, {"text": "There has also been some success incorporating selectional preferences.", "labels": [], "entities": []}, {"text": "Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information).", "labels": [], "entities": [{"text": "verb classification", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7428939938545227}]}, {"text": "Exceptions to this include, and.", "labels": [], "entities": []}, {"text": "used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop).", "labels": [], "entities": []}, {"text": "used similar features to classify verbs on a much larger scale.", "labels": [], "entities": []}, {"text": "They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes.) used hierarchical Bayesian models on slot frequency data obtained from childdirected speech parsed with a dependency parser to model acquisition of SCF, alternations and ultimately verb classes which provided predictions for unseen syntactic behaviour of class members.", "labels": [], "entities": []}, {"text": "In this paper, like; we seek to automatically classify verbs into abroad range of classes.", "labels": [], "entities": []}, {"text": "Like Joanis et al., we include evidence of DA, but we do not manually select features attributed to specific alternations but rather experiment with syntactic evidence for alternation approximation.", "labels": [], "entities": [{"text": "DA", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.7730244994163513}, {"text": "alternation approximation", "start_pos": 172, "end_pos": 197, "type": "TASK", "confidence": 0.8284282088279724}]}, {"text": "We use the verb clustering system presented in because it achieves state-of-theart results on several datasets, including those of Joanis et al., even without the additional boost in performance from the selectional preference data.", "labels": [], "entities": [{"text": "verb clustering", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.6335504800081253}]}, {"text": "We are interested in the improvement that can be achieved to verb clustering using approximations for DAs, rather than the DA per se.", "labels": [], "entities": [{"text": "verb clustering", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.784542977809906}]}, {"text": "As such we make the simple assumption that if a pair of SCFs tends to occur with the same verbs, we have a potential occurrence of DA.", "labels": [], "entities": [{"text": "DA", "start_pos": 131, "end_pos": 133, "type": "METRIC", "confidence": 0.9595556259155273}]}, {"text": "Although this approximation can give rise to false positives (pairs of frames that co-occur frequently but are not DA) we are nevertheless interested in investigating its potential usefulness for verb classification.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 196, "end_pos": 215, "type": "TASK", "confidence": 0.7848048508167267}]}, {"text": "One attractive aspect of this method is that it does not require a pre-defined list of possible alternations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated this model by performing verb clustering experiments using three feature sets: F1: SCF parameterized with preposition.", "labels": [], "entities": [{"text": "verb clustering", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7478280067443848}, {"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9977097511291504}]}, {"text": "F2: The frame pair features built from F1 with the frame independence assumption (equation 1).", "labels": [], "entities": [{"text": "F1", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9629899263381958}]}, {"text": "This feature is not a DA feature as it ignores the inter-dependency of the frames.", "labels": [], "entities": []}, {"text": "F3: The frame pair features (DAs) built from F1 with the frame dependency assumption (equation 4).", "labels": [], "entities": []}, {"text": "This is the DA feature which considers the correlation of the two frames which are generated from the alternation.", "labels": [], "entities": [{"text": "DA", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.7401748895645142}]}, {"text": "F3 implicitly includes F1, as a frame can pair with itself.", "labels": [], "entities": [{"text": "F3", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.939587414264679}, {"text": "F1", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9976251721382141}]}, {"text": "In the example in, the frame pair \"PP(on) PP(on)\" will always have the same value as the \"PP(on)\" frame in F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.9065355062484741}]}, {"text": "We extracted the SCFs using the system of which classifies each corpus occurrence of a verb as a member of one of the 168 SCFs on the basis of grammatical relations identified by the RASP () parser.", "labels": [], "entities": []}, {"text": "We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in, and the 17 classes set in.", "labels": [], "entities": [{"text": "verb clustering", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.7133572697639465}]}, {"text": "We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but adopted the Bhattacharyya kernel to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space.", "labels": [], "entities": [{"text": "spectral clustering (SPEC)", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.8179046034812927}]}, {"text": "The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel () which is frequently used in verb clustering experiments (.", "labels": [], "entities": [{"text": "verb clustering", "start_pos": 124, "end_pos": 139, "type": "TASK", "confidence": 0.6783715933561325}]}, {"text": "To further reduce computational complexity, we restricted our scope to the more frequent features.", "labels": [], "entities": []}, {"text": "In the experiment described in this section we used the 50 most frequent features for the 3-6 way classifications (Joanis et al.'s test set 7-9) and 100 features for the 7-17 way classifications.", "labels": [], "entities": []}, {"text": "In the next section, we will demonstrate that F3 outperforms F1 regardless of the feature number setting.", "labels": [], "entities": [{"text": "F3", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9172531962394714}, {"text": "F1", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9980251789093018}]}, {"text": "The features are normalized to sum 1.", "labels": [], "entities": []}, {"text": "The clustering results are evaluated using FMeasure as in which provides the harmonic mean of precision (P ) and recall (R) P is calculated using modified purity -a global measure which evaluates the mean precision of clusters.", "labels": [], "entities": [{"text": "FMeasure", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.7310104966163635}, {"text": "harmonic mean of precision (P )", "start_pos": 77, "end_pos": 108, "type": "METRIC", "confidence": 0.7831501109259469}, {"text": "recall (R) P", "start_pos": 113, "end_pos": 125, "type": "METRIC", "confidence": 0.9720404505729675}]}, {"text": "Each cluster (k i \u2208 K) is associated with the gold-standard class to which the majority of its members belong.", "labels": [], "entities": []}, {"text": "The number of verbs in a cluster (k i ) that take this class is denoted by n prevalent (k i ).", "labels": [], "entities": []}, {"text": "|verbs| R is calculated using weighted class accuracy: the proportion of members of the dominant cluster DOM-CLUST i within each of the gold-standard classes c i \u2208 C.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.8661365509033203}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The result of F2 is lower than that of F3, and even lower than that of F1 for 3-6 way classification.", "labels": [], "entities": [{"text": "F2", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9110990166664124}, {"text": "F1", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9984512329101562}]}, {"text": "This indicates that the frame independence assumption is a poor assumption.", "labels": [], "entities": []}, {"text": "F3 yields substantially better result than F2 and F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.9944585561752319}]}, {"text": "The result of F3 is 6.4% higher than the result (F=63.28) reported in Sun and Korhonen (2009) using the F1 feature.", "labels": [], "entities": [{"text": "F3", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9831546545028687}, {"text": "F", "start_pos": 49, "end_pos": 50, "type": "METRIC", "confidence": 0.9972666501998901}, {"text": "F1", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.9881359338760376}]}, {"text": "This experiment shows, on two datasets, that DA features are clearly more effective than the frame features for verb clustering, even when relaxations are used.", "labels": [], "entities": [{"text": "verb clustering", "start_pos": 112, "end_pos": 127, "type": "TASK", "confidence": 0.7357530444860458}]}], "tableCaptions": [{"text": " Table 1: Example frames for verb spray", "labels": [], "entities": []}, {"text": " Table 1. The feature  value of a single frame feature is the frequency  of the SCF. Given two frames f v (i), f v (j) of a  verb v, they can be transformed into a feature pair  (f v (i), f v (j)) as an approximation to a DA. The  feature value of the DA feature (f v (i), f v (j)) is ap- proximated by the joint probability of the pair of  frames p(f v (i), f v (j)|v), obtained by integrating  all the possible DAs. The key assumption is that  the joint probability of two SCFs has a strong cor- relation with a DA on the grounds that the DA gives  rise to both SCFs in the pair. We use the DA feature  (f v (i), f v (j)) with its value p(f v (i), f v (j)|v) as a  new feature for verb clustering. As a comparison  point, we can ignore the DA and make a frame in- dependence assumption. The joint probability is", "labels": [], "entities": [{"text": "verb clustering", "start_pos": 683, "end_pos": 698, "type": "TASK", "confidence": 0.7311646193265915}]}, {"text": " Table 3: Results when using F3 (DA), F2 (pair of independent  frames) and F1 (single frame) features with Bhattacharyya  kernel on Joanis et al. and Sun et al. datasets", "labels": [], "entities": [{"text": "F1", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9928380846977234}]}]}