{"title": [{"text": "Fast and Adaptive Online Training of Feature-Rich Translation Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a fast and scalable online method for tuning statistical machine translation models with large feature sets.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.5825649301211039}]}, {"text": "The standard tuning algorithm-MERT-only scales to tens of features.", "labels": [], "entities": []}, {"text": "Recent discrimi-native algorithms that accommodate sparse features have produced smaller than expected translation quality gains in large systems.", "labels": [], "entities": []}, {"text": "Our method, which is based on stochastic gradient descent with an adaptive learning rate, scales to millions of features and tuning sets with tens of thousands of sentences, while still converging after only a few epochs.", "labels": [], "entities": []}, {"text": "Large-scale experiments on Arabic-English and Chinese-English show that our method produces significant translation quality gains by exploiting sparse features.", "labels": [], "entities": []}, {"text": "Equally important is our analysis, which suggests techniques for mitigating overfitting and domain mismatch, and applies to other recent discriminative methods for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.7760812342166901}]}], "introductionContent": [{"text": "Sparse, overlapping features such as words and ngram contexts improve many NLP systems such as parsers and taggers.", "labels": [], "entities": []}, {"text": "Adaptation of discriminative learning methods for these types of features to statistical machine translation (MT) systems, which have historically used idiosyncratic learning techniques fora few dense features, has been an active research area for the past half-decade.", "labels": [], "entities": [{"text": "Adaptation of discriminative learning", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8015669286251068}, {"text": "statistical machine translation (MT)", "start_pos": 77, "end_pos": 113, "type": "TASK", "confidence": 0.7652045836051306}]}, {"text": "However, despite some research successes, feature-rich models are rarely used in annual MT evaluations.", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9923490285873413}]}, {"text": "For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features).", "labels": [], "entities": [{"text": "WMT and IWSLT 2012 shared tasks", "start_pos": 42, "end_pos": 73, "type": "DATASET", "confidence": 0.8605395555496216}]}, {"text": "Slow uptake of these methods maybe due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (, inter alia).", "labels": [], "entities": []}, {"text": "We introduce anew method for training featurerich MT systems that is effective yet comparatively easy to implement.", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9756155610084534}]}, {"text": "The algorithm scales to millions of features and large tuning sets.", "labels": [], "entities": []}, {"text": "It optimizes a logistic objective identical to that of PRO () with stochastic gradient descent, although other objectives are possible.", "labels": [], "entities": []}, {"text": "The learning rate is set adaptively using AdaGrad), which is particularly effective for the mixture of dense and sparse features present in MT models.", "labels": [], "entities": [{"text": "MT", "start_pos": 140, "end_pos": 142, "type": "TASK", "confidence": 0.9689533710479736}]}, {"text": "Finally, feature selection is implemented as efficient L 1 regularization in the forward-backward splitting (FOBOS) framework.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7347896695137024}, {"text": "forward-backward splitting (FOBOS)", "start_pos": 81, "end_pos": 115, "type": "TASK", "confidence": 0.6659599304199219}]}, {"text": "Experiments show that our algorithm converges faster than batch alternatives.", "labels": [], "entities": []}, {"text": "To learn good weights for the sparse features, most algorithms-including ours-benefit from more tuning data, and the natural source is the training bitext.", "labels": [], "entities": []}, {"text": "However, the bitext presents two problems.", "labels": [], "entities": []}, {"text": "First, it has a single reference, sometimes of lower quality than the multiple references in tuning sets from MT competitions.", "labels": [], "entities": [{"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.9643779397010803}]}, {"text": "Second, large bitexts often comprise many text genres), a virtue for classical dense MT models but a curse for high dimensional models: bitext tuning can lead to a significant domain adaptation problem when evaluating on standard test sets.", "labels": [], "entities": []}, {"text": "Our analysis separates and quantifies these two issues.", "labels": [], "entities": []}, {"text": "We conduct large-scale translation quality experiments on Arabic-English and Chinese-English.", "labels": [], "entities": []}, {"text": "As baselines we use MERT, PRO, and the Moses ( ) implementation of k-best MIRA, which recently showed to work as well as online MIRA) for feature-rich models.", "labels": [], "entities": [{"text": "MERT", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.8569965362548828}]}, {"text": "The first experiment uses standard tuning and test sets from the NIST OpenMT competitions.", "labels": [], "entities": [{"text": "NIST OpenMT competitions", "start_pos": 65, "end_pos": 89, "type": "DATASET", "confidence": 0.9029299020767212}]}, {"text": "The second experiment uses tuning and test sets sampled from the large bitexts.", "labels": [], "entities": []}, {"text": "The new method yields significant improvements in both experiments.", "labels": [], "entities": []}, {"text": "Our code is included in the Phrasal () toolkit, which is freely available.", "labels": [], "entities": [{"text": "Phrasal () toolkit", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.866221010684967}]}], "datasetContent": [{"text": "We built Arabic-English and Chinese-English MT systems with Phrasal (), a phrasebased system based on alignment templates ().", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9157525897026062}]}, {"text": "The corpora 3 in our experiments) derive from several LDC sources from 2012 and earlier.", "labels": [], "entities": []}, {"text": "We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets.", "labels": [], "entities": []}, {"text": "We produced alignments with the Berkeley aligner () with standard settings and symmetrized via the grow-diag heuristic.", "labels": [], "entities": []}, {"text": "For each language we used SRILM) to estimate 5-gram LMs with modified Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.8396124839782715}]}, {"text": "We included the monolingual English data and the respective target bitexts.", "labels": [], "entities": []}, {"text": "The first experiment evaluates our algorithm when tuning and testing on standard test sets, each with four references.", "labels": [], "entities": []}, {"text": "When we add features, our algorithm tends to overfit to a standard-sized tuning set like MT06.", "labels": [], "entities": [{"text": "MT06", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.9679238796234131}]}, {"text": "We thus concatenated MT05, MT06, and MT08 to create a larger tuning set.", "labels": [], "entities": [{"text": "MT05", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.9524720311164856}, {"text": "MT06", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.9168143272399902}, {"text": "MT08", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.950545072555542}]}, {"text": "Our algorithm is competitive with MERT in the low dimensional \"dense\" setting, and compares favorably to PRO with the PT feature set.", "labels": [], "entities": [{"text": "PT feature set", "start_pos": 118, "end_pos": 132, "type": "DATASET", "confidence": 0.763269305229187}]}, {"text": "PRO does not benefit from additional features, whereas our algorithm improves with both additional features and data.", "labels": [], "entities": [{"text": "PRO", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6506620049476624}]}, {"text": "The underperformance of kb-MIRA may result from a difference between Moses and Phrasal: Moses MERT achieves only 45.62 on MT09.", "labels": [], "entities": [{"text": "MERT", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9937434792518616}, {"text": "MT09", "start_pos": 122, "end_pos": 126, "type": "DATASET", "confidence": 0.9059861302375793}]}, {"text": "Moses PRO with the PT feature set is slightly worse, e.g.,.52 on MT09.", "labels": [], "entities": [{"text": "Moses PRO", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7107285857200623}, {"text": "PT feature set", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.7213845650355021}, {"text": "MT09", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.9649245142936707}]}, {"text": "Nevertheless, kb-MIRA does not improve significantly over MERT, and also selects an unnecessarily large model.", "labels": [], "entities": [{"text": "MERT", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.785757839679718}]}, {"text": "For Ar-En, our algorithm thus has the desirable property of benefiting from more and better features, and more data.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly our algorithm improves over MERT in the dense setting.", "labels": [], "entities": [{"text": "MERT", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.5023027062416077}]}, {"text": "When we add the discriminative phrase table, our algorithm improves over kb-MIRA, and over batch PRO on two evaluation sets.", "labels": [], "entities": []}, {"text": "With all features and the MT05/6/8 tuning set, we improve significantly overall other models.", "labels": [], "entities": [{"text": "MT05/6/8 tuning set", "start_pos": 26, "end_pos": 45, "type": "DATASET", "confidence": 0.8769560882023403}]}, {"text": "PRO learns a smaller model with the PT+AL+LO feature set which is surprising given that it applies L 2 regularization (AdaGrad uses L 1 ).", "labels": [], "entities": [{"text": "PT+AL+LO feature set", "start_pos": 36, "end_pos": 56, "type": "METRIC", "confidence": 0.7486449905804226}]}, {"text": "We speculate that this maybe an consequence of stochastic learning.", "labels": [], "entities": []}, {"text": "Our algorithm decodes each example with anew weight vector, thus exploring more of the search space for the same tuning set.", "labels": [], "entities": []}, {"text": "show that adding tuning examples improves translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9709854125976562}]}, {"text": "Nevertheless, even the larger tuning set is small relative to the bitext from which rules were extracted.", "labels": [], "entities": []}, {"text": "He and Deng (2012) and showed significant translation quality gains by tuning on the bitext.", "labels": [], "entities": []}, {"text": "However, their bitexts matched the genre of their test sets.", "labels": [], "entities": []}, {"text": "Our bitexts, like those of most large-scale systems, do not.", "labels": [], "entities": []}, {"text": "Domain mismatch matters for the dense feature set ().", "labels": [], "entities": []}, {"text": "We show that it also matters for feature-rich MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9073442220687866}]}, {"text": "Before aligning each bitext, we randomly sampled and sequestered 5k and 15k sentence tuning sets, and a 5k test set.", "labels": [], "entities": []}, {"text": "We prevented overlap be- 10.5k 5ktest bitext5k 82k 67k 5.6k 5ktest bitext15k 82k 310k 14k tween the tuning sets and the test set.", "labels": [], "entities": [{"text": "overlap", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9875954389572144}]}, {"text": "We then tuned a dense model with MERT on MT06, and feature-rich models on both MT05/6/8 and the bitext tuning set.", "labels": [], "entities": [{"text": "MERT", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9906713962554932}, {"text": "MT06", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.9618462324142456}, {"text": "MT05/6/8", "start_pos": 79, "end_pos": 87, "type": "DATASET", "confidence": 0.9248849630355835}]}, {"text": "When tuned on bitext5k the translation quality gains are significant for bitext5k-test relative to tuning on MT05/6/8, which has multiple references.", "labels": [], "entities": [{"text": "MT05/6/8", "start_pos": 109, "end_pos": 117, "type": "DATASET", "confidence": 0.9499134540557861}]}, {"text": "However, the bitext5k models do not generalize as well to the NIST evaluation sets as represented by the MT04 result.", "labels": [], "entities": [{"text": "NIST evaluation sets", "start_pos": 62, "end_pos": 82, "type": "DATASET", "confidence": 0.8736598491668701}, {"text": "MT04 result", "start_pos": 105, "end_pos": 116, "type": "DATASET", "confidence": 0.929606705904007}]}, {"text": "shows similar trends for Zh-En.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Ar-En results [BLEU-4 % uncased] for the NIST tuning experiment. The tuning and test sets  each have four references. MT06 has 1,717 sentences, while the concatenated MT05/6/8 set has 4,213  sentences. Bold indicates statistical significance relative to the best baseline in each block at p < 0.001;  bold-italic at p < 0.05. We assessed significance with the permutation test of Riezler and Maxwell (2005).  (*) Chiang (2012) used a similar-sized bitext, but two LMs trained on twice as much monolingual data.", "labels": [], "entities": [{"text": "Ar-En", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9976455569267273}, {"text": "BLEU-4", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9970834851264954}, {"text": "NIST tuning experiment", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.8490093549092611}, {"text": "MT06", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.8901541233062744}, {"text": "MT05/6/8 set", "start_pos": 177, "end_pos": 189, "type": "DATASET", "confidence": 0.8976334929466248}]}, {"text": " Table 3: Zh-En results [BLEU-4 % uncased] for the NIST tuning experiment. MT05/6/8 has 4,103  sentences. OpenMT 2009 did not include Zh-En, hence the asymmetry with Table 2.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9976192116737366}, {"text": "NIST tuning experiment", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.8366726835568746}, {"text": "MT05/6/8", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9293877601623535}, {"text": "OpenMT 2009", "start_pos": 106, "end_pos": 117, "type": "DATASET", "confidence": 0.9130229949951172}]}, {"text": " Table 4: Ar-En results [BLEU-4 % uncased] for the bitext tuning experiment. Statistical significance is  relative to the Dense baseline. We include MT04 for comparison to the NIST genre.", "labels": [], "entities": [{"text": "Ar-En", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.99828040599823}, {"text": "BLEU-4", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9981778860092163}, {"text": "Statistical significance", "start_pos": 77, "end_pos": 101, "type": "METRIC", "confidence": 0.887563943862915}, {"text": "MT04", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.8756547570228577}, {"text": "NIST genre", "start_pos": 176, "end_pos": 186, "type": "DATASET", "confidence": 0.9463486075401306}]}, {"text": " Table 5: Zh-En results [BLEU-4 % uncased] for the bitext tuning experiment.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9987521171569824}]}, {"text": " Table 7: Top: comparison of token counts in two  Ar-En tuning sets for programme and program. Bot- tom: rule counts in the discriminative phrase table  (PT) for models tuned on the two tuning sets. Both", "labels": [], "entities": [{"text": "Bot- tom", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9292884866396586}]}, {"text": " Table 8: Epochs to convergence (\"epochs\") and  approximate runtime per epoch in minutes (\"min.\")  for selected Zh-En experiments tuned on MT06.  All runs executed on the same dedicated system  with the same number of threads. (*) Moses and  kb-MIRA are written in C++, while all other rows  refer to Java implementations in Phrasal.", "labels": [], "entities": [{"text": "MT06", "start_pos": 139, "end_pos": 143, "type": "DATASET", "confidence": 0.962525486946106}, {"text": "Phrasal", "start_pos": 325, "end_pos": 332, "type": "DATASET", "confidence": 0.9279109239578247}]}]}