{"title": [{"text": "Hierarchical Phrase Table Combination for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7697092294692993}]}], "abstractContent": [{"text": "Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.6268090506394705}]}, {"text": "With the growth of the available data across different domains, it is computationally demanding to perform batch training every time when new data comes.", "labels": [], "entities": []}, {"text": "In face of the problem, we propose an efficient phrase table combination method.", "labels": [], "entities": [{"text": "phrase table combination", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.6928616960843405}]}, {"text": "In particular , we train a Bayesian phrasal inversion transduction grammars for each domain separately.", "labels": [], "entities": []}, {"text": "The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process.", "labels": [], "entities": []}, {"text": "The performance measured by BLEU is at least as comparable to the traditional batch training method.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9680225849151611}]}, {"text": "Furthermore , each phrase table is trained separately in each domain, and while computational overhead is significantly reduced by training them in parallel.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation (SMT) systems usually achieve 'crowd-sourced' improvements with batch training.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.835331787665685}]}, {"text": "Phrase pair extraction, the key step to discover translation knowledge, heavily relies on the scale of training data.", "labels": [], "entities": [{"text": "Phrase pair extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9098973075548807}, {"text": "discover translation knowledge", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.7017343242963155}]}, {"text": "Typically, the more parallel corpora used, the more phrase pairs and more accurate parameters will be learned, which can obviously be beneficial to improving translation performances.", "labels": [], "entities": []}, {"text": "Today, more parallel sentences are drawn from divergent domains, and the size keeps growing.", "labels": [], "entities": []}, {"text": "Consequently, how to effectively use those data and improve translation performance becomes a challenging issue.", "labels": [], "entities": [{"text": "translation", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.9673445820808411}]}, {"text": "This joint work was done while the first author visited NICT.", "labels": [], "entities": [{"text": "NICT", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9685127139091492}]}, {"text": "Batch retraining is not acceptable for this case, since it demands serious computational overhead when training on a large data set, and it requires us to re-train every time new training data is available.", "labels": [], "entities": [{"text": "Batch retraining", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8431493043899536}]}, {"text": "Even if we can handle the large computation cost, improvement is not guaranteed every time we perform batch tuning on the newly updated training data obtained from divergent domains.", "labels": [], "entities": []}, {"text": "Traditional domain adaption methods for SMT are also not adequate in this scenario.", "labels": [], "entities": [{"text": "SMT", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9964697360992432}]}, {"text": "Most of them have been proposed in order to make translation systems perform better for resource-scarce domains when most training data comes from resourcerich domains, and ignore performance on a more generic domain without domain bias ( . As an alternative, incremental learning may resolve the gap by incrementally adding data sentence-by-sentence into the training data.", "labels": [], "entities": []}, {"text": "Since SMT systems trend to employ very large scale training data for translation knowledge extraction, updating several sentence pairs each time will be annihilated in the existing corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9922001957893372}, {"text": "translation knowledge extraction", "start_pos": 69, "end_pos": 101, "type": "TASK", "confidence": 0.9343806107838949}]}, {"text": "This paper proposes anew phrase table combination method.", "labels": [], "entities": [{"text": "phrase table combination", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.6679913302262624}]}, {"text": "First, phrase pairs are extracted from each domain without interfering with other domains.", "labels": [], "entities": []}, {"text": "In particular, we employ the nonparametric Bayesian phrasal inversion transduction grammar (ITG) of to perform phrase table extraction.", "labels": [], "entities": [{"text": "phrase table extraction", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.7494157155354818}]}, {"text": "Second, extracted phrase tables are combined as if they are drawn from a hierarchical Pitman-Yor process, in which the phrase tables represented as tables in the Chinese restaurant process (CRP) are hierarchically chained by treating each of the previously learned phrase tables as prior to the current one.", "labels": [], "entities": []}, {"text": "Thus, we can easily update the chain of phrase tables by appending the newly extracted phrase table and by treating the chain of the previous ones as its prior.", "labels": [], "entities": []}, {"text": "Experiment results indicate that our method can achieve better translation performance when there exists a large divergence in domains, and can achieve at least comparable results to batch training methods, with a significantly less computational overhead.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce related work.", "labels": [], "entities": []}, {"text": "In section 3, we briefly describe the translation model with phrasal ITGs and Pitman-Yor process.", "labels": [], "entities": [{"text": "translation", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.9793881177902222}]}, {"text": "In section 4, we explain our hierarchical combination approach and give experiment results in section 5.", "labels": [], "entities": []}, {"text": "We conclude the paper in the last section.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed approach on the Chinese-to-English translation task with three data sets with different scales.: The sentence pairs used in each data set.", "labels": [], "entities": [{"text": "Chinese-to-English translation task", "start_pos": 41, "end_pos": 76, "type": "TASK", "confidence": 0.7638225754102071}]}, {"text": "The first data set comes from the IWSLT2012 OLYMPICS task consisting of two training sets: the HIT corpus, which is closely related to the Beijing 2008 Olympic Games, and the BTEC corpus, which is a multilingual speech corpus containing tourism-related sentences.", "labels": [], "entities": [{"text": "IWSLT2012 OLYMPICS task", "start_pos": 34, "end_pos": 57, "type": "DATASET", "confidence": 0.694088359673818}, {"text": "HIT corpus", "start_pos": 95, "end_pos": 105, "type": "DATASET", "confidence": 0.8341928422451019}, {"text": "BTEC corpus", "start_pos": 175, "end_pos": 186, "type": "DATASET", "confidence": 0.9380994737148285}]}, {"text": "The second data set, the FBIS corpus, is a collection of news articles and does not have domain information itself, so a Latent Dirichlet Allocation (LDA) tool, PLDA 1 , is used to divide the whole corpus into 5 different sub-domains according to the concatenation of the source side and target side as a single sentence (Liu et al., 2011).", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.7788520157337189}, {"text": "Latent Dirichlet Allocation (LDA", "start_pos": 121, "end_pos": 153, "type": "METRIC", "confidence": 0.9382436394691467}]}, {"text": "The third data set is composed of 5 corpora 2 from LDC with various domains, including news, magazine, and finance.", "labels": [], "entities": [{"text": "LDC", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.9209595322608948}]}, {"text": "The details are shown in.", "labels": [], "entities": []}, {"text": "In order to evaluate our approach, four phrase pair extraction methods are performed: 1.", "labels": [], "entities": [{"text": "phrase pair extraction", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.750525544087092}]}, {"text": "GIZA-linear: Phase pairs are extracted in each domain by GIZA++ ( and the \"grow-diag-final-and\" method with a maximum length 7.", "labels": [], "entities": []}, {"text": "The phrase tables from various domains are linearly combined by averaging the feature values.", "labels": [], "entities": []}, {"text": "2. Pialign-linear: Similar to GIZA-linear, but we employed the phrasal ITG method described in Section 3 using the pialign toolkit: BLEU scores and phrase table size by alignment method and probabilities estimation method.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 132, "end_pos": 143, "type": "METRIC", "confidence": 0.9690906703472137}]}, {"text": "Pialign was run with five samples.", "labels": [], "entities": [{"text": "Pialign", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9057504534721375}]}, {"text": "Because of computational overhead, the baseline Pialign-batch and Pialign-adaptive were not run on the largest data set.", "labels": [], "entities": []}, {"text": "Extracted phrase pairs are linearly combined by averaging the feature values.", "labels": [], "entities": []}, {"text": "3. GIZA-batch: Instead of splitting into each domain, the data set is merged as a single corpus and then a heuristic GZA-based phrase extraction is performed, similar as GIZA-linear.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.6505792737007141}]}, {"text": "4. Pialign-batch: Similar to the GIZA-batch, a single model is estimated from a single, merged corpus.", "labels": [], "entities": [{"text": "GIZA-batch", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.891398012638092}]}, {"text": "Since pialign cannot handle large data, we did not experiment on the largest LDC data set.", "labels": [], "entities": [{"text": "LDC data set", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.8420132001241049}]}, {"text": "5. Pialign-adaptive: Alignment and phrase pairs extraction are same to Pialign-batch, while translation probabilities are estimated by the adaptive method with monolingual topic information ().", "labels": [], "entities": [{"text": "phrase pairs extraction", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.6564532419045767}]}, {"text": "The method established the relationship between the out-ofdomain bilingual corpus and in-domain monolingual corpora via topic distribution to estimate the translation probability.", "labels": [], "entities": []}, {"text": "where \u00f8(\u02dc e|t f , \u02dc f ) is the probability of translating\u02dcftranslating\u02dc translating\u02dcf int\u00f5 e given the source-side topic\u02dcftopic\u02dc topic\u02dcf , P (t f | \u02dc f ) is the phrase-topic distribution off.", "labels": [], "entities": [{"text": "\u00f8", "start_pos": 6, "end_pos": 7, "type": "METRIC", "confidence": 0.996669590473175}]}, {"text": "The method we proposed is named Hiercombin.", "labels": [], "entities": []}, {"text": "It extracts phrase pairs in the same way as the Pialign-linear.", "labels": [], "entities": []}, {"text": "In the phrase table combination process, the translation probability of each phrase pair is estimated by the Hier-combin and the other features are also linearly combined by averaging the feature values.", "labels": [], "entities": [{"text": "phrase table combination", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.6936373511950175}]}, {"text": "Pialign is used with default parameters.", "labels": [], "entities": []}, {"text": "The parameter 'samps' is set to 5, which indicates 5 samples are generated fora sentence pair.", "labels": [], "entities": []}, {"text": "The IWSLT data consists of roughly 2, 000 sentences and 3, 000 sentences each from the HIT and BTEC for development purposes, and the test data consists of 1, 000 sentences.", "labels": [], "entities": [{"text": "IWSLT data", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.7565740942955017}, {"text": "HIT", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.9536576271057129}, {"text": "BTEC", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.7970849871635437}]}, {"text": "For the FBIS and LDC task, we used NIST MT 2002 and 2004 for development and testing purposes, consisting of 878 and 1, 788 sentences respectively.", "labels": [], "entities": [{"text": "FBIS", "start_pos": 8, "end_pos": 12, "type": "TASK", "confidence": 0.5019002556800842}, {"text": "NIST MT 2002", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.8930340011914571}]}, {"text": "We employ Moses, an open-source toolkit for our experiment (.", "labels": [], "entities": []}, {"text": "SRILM Toolkit) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used.", "labels": [], "entities": [{"text": "SRILM Toolkit", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9144273996353149}, {"text": "Xinhua portion of Gigaword corpus", "start_pos": 66, "end_pos": 99, "type": "DATASET", "confidence": 0.647346842288971}, {"text": "IWLST2012 data set", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.9628317753473917}]}, {"text": "We use batch-MIRA) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric ().", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9872336387634277}]}, {"text": "The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9989356398582458}]}], "tableCaptions": [{"text": " Table 1: The sentence pairs used in each data set.", "labels": [], "entities": []}, {"text": " Table 2: BLEU scores and phrase table size by alignment method and probabilities estimation method.  Pialign was run with five samples. Because of computational overhead, the baseline Pialign-batch and  Pialign-adaptive were not run on the largest data set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.996871292591095}]}, {"text": " Table 3: Minutes used for alignment and phase  pair extraction in the FBIS data set.", "labels": [], "entities": [{"text": "alignment", "start_pos": 27, "end_pos": 36, "type": "TASK", "confidence": 0.9821171164512634}, {"text": "phase  pair extraction", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.5837687849998474}, {"text": "FBIS data set", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9204500317573547}]}, {"text": " Table 4: BLEU scores for the hierarchical model  with different integrating orders. Here Pialign was  run without multi-samples.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999258816242218}]}]}