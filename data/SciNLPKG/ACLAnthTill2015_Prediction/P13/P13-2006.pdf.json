{"title": [{"text": "Learning Entity Representation for Entity Disambiguation", "labels": [], "entities": [{"text": "Learning Entity Representation for Entity Disambiguation", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.6571039309104284}]}], "abstractContent": [{"text": "We propose a novel entity disambigua-tion model, based on Deep Neural Network (DNN).", "labels": [], "entities": []}, {"text": "Instead of utilizing simple similarity measures and their disjoint combinations , our method directly optimizes document and entity representations fora given similarity measure.", "labels": [], "entities": []}, {"text": "Stacked Denois-ing Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage.", "labels": [], "entities": []}, {"text": "A supervised fine-tuning stage follows to optimize the representation towards the similarity measure.", "labels": [], "entities": []}, {"text": "Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Entity linking or disambiguation has recently received much attention in natural language processing community (.", "labels": [], "entities": [{"text": "Entity linking or disambiguation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8530718833208084}]}, {"text": "It is an essential first step for succeeding sub-tasks in knowledge base construction () like populating attribute to entities.", "labels": [], "entities": []}, {"text": "Given a sentence with four mentions, \"The [] of] was a creature with the body of a snake.", "labels": [], "entities": []}, {"text": "This creature dwelled on [, in central].\"", "labels": [], "entities": []}, {"text": "How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado?", "labels": [], "entities": []}, {"text": "A most straightforward method is to compare the context of the mention and the definition of candidate entities.", "labels": [], "entities": []}, {"text": "Previous work has explored many ways of measuring the relatedness of context * Corresponding author d and entity e, such as dot product, cosine similarity, Kullback-Leibler divergence, Jaccard distance, or more complicated ones ().", "labels": [], "entities": [{"text": "Jaccard distance", "start_pos": 185, "end_pos": 201, "type": "METRIC", "confidence": 0.6297724097967148}]}, {"text": "However, these measures are often duplicate or over-specified, because they are disjointly combined and their atomic nature determines that they have no internal structure.", "labels": [], "entities": []}, {"text": "Another line of work focuses on collective disambiguation ().", "labels": [], "entities": [{"text": "collective disambiguation", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.6669404208660126}]}, {"text": "Ambiguous mentions within the same context are resolved simultaneously based on the coherence among decisions.", "labels": [], "entities": []}, {"text": "Collective approaches often undergo a non-trivial decision process.", "labels": [], "entities": []}, {"text": "In fact, show that even though global approaches can be improved, local methods based on only similarity sim(d, e) of context d and entity e are hard to beat.", "labels": [], "entities": []}, {"text": "This somehow reveals the importance of a good modeling of sim(d, e).", "labels": [], "entities": []}, {"text": "Rather than learning context entity association at word level, topic model based approaches can learn it in the semantic space.", "labels": [], "entities": []}, {"text": "However, the one-topic-perentity assumption makes it impossible to scale to large knowledge base, as every entity has a separate word distribution P (w|e); besides, the training objective does not directly correspond with disambiguation performances.", "labels": [], "entities": []}, {"text": "To overcome disadvantages of previous approaches, we propose a novel method to learn context entity association enriched with deep architecture.", "labels": [], "entities": []}, {"text": "Deep neural networks () are builtin a hierarchical manner, and allow us to compare context and entity at some higher level abstraction; while at lower levels, general concepts are shared across entities, resulting in compact models.", "labels": [], "entities": []}, {"text": "Moreover, to make our model highly correlated with disambiguation performance, our method directly optimizes doc-ument and entity representations fora fixed similarity measure.", "labels": [], "entities": []}, {"text": "In fact, the underlying representations for computing similarity measure add internal structure to the given similarity measure.", "labels": [], "entities": []}, {"text": "Features are learned leveraging large scale annotation of Wikipedia, without any manual design efforts.", "labels": [], "entities": []}, {"text": "Furthermore, the learned model is compact compared with topic model based approaches, and can be trained discriminatively without relying on expensive sampling strategy.", "labels": [], "entities": []}, {"text": "Despite its simplicity, it beats all complex collective approaches in our experiments.", "labels": [], "entities": []}, {"text": "The learned similarity measure can be readily incorporated into any existing collective approaches, which further boosts performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "Training settings: In pre-training stage, input layer has 100,000 units, all hidden layers have 1,000 units with rectifier function max(0, x).", "labels": [], "entities": []}, {"text": "Following (Glorot et al., 2011), for the first reconstruction layer, we use sigmoid activation function and cross-entropy error function.", "labels": [], "entities": []}, {"text": "For higher reconstruction layers, we use sof tplus (log(1 + exp(x))) as activation function and squared loss as error function.", "labels": [], "entities": []}, {"text": "For corruption process, we use a masking noise probability in {0.1,0.4,0.7} for the first layer, a Gaussian noise with standard deviation of 0.1 for higher layers.", "labels": [], "entities": []}, {"text": "For reconstruction sampling, we set the reconstruction rate to 0.01.", "labels": [], "entities": [{"text": "reconstruction sampling", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.9770247936248779}, {"text": "reconstruction rate", "start_pos": 40, "end_pos": 59, "type": "METRIC", "confidence": 0.9483863115310669}]}, {"text": "In fine-tuning stage, the final layer has 200 units with sigmoid activation function.", "labels": [], "entities": []}, {"text": "The learning rate is set to 1e-3.", "labels": [], "entities": []}, {"text": "The mini-batch size is set to 20.", "labels": [], "entities": []}, {"text": "We run all our experiments on a Linux machine with 72GB memory 6 core Xeon CPU.", "labels": [], "entities": []}, {"text": "The model is implemented in Python with C extensions, numpy configured with Openblas library.", "labels": [], "entities": [{"text": "Openblas library", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.9461154639720917}]}, {"text": "Thanks to reconstruction sampling and refined mini-batch arrangement, it takes about 1 day to converge for pre-training and 3 days for finetuning, which is fast given our training set size.", "labels": [], "entities": [{"text": "reconstruction", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9640049338340759}, {"text": "finetuning", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.9614790678024292}]}, {"text": "Datasets: We use half of Wikipedia 1 plain text (\u02dc1.5M articles split into sections) for pre-training.", "labels": [], "entities": []}, {"text": "We collect a total of 40M hyperlinks grouped by name string m for fine-tuning stage.", "labels": [], "entities": []}, {"text": "We holdout a subset of hyperlinks for model selection, and we find that 3 layers network with a higher masking noise rate (0.7) always gives best performance.", "labels": [], "entities": []}, {"text": "We select TAC-KBP 2010 (Ji and Grishman, 2011) dataset for non-collective approaches, and AIDA 2 dataset for collective approaches.", "labels": [], "entities": [{"text": "TAC-KBP 2010 (Ji and Grishman, 2011) dataset", "start_pos": 10, "end_pos": 54, "type": "DATASET", "confidence": 0.6970283448696136}, {"text": "AIDA 2 dataset", "start_pos": 90, "end_pos": 104, "type": "DATASET", "confidence": 0.7235398093859354}]}, {"text": "For both datasets, we evaluate the non-NIL queries.", "labels": [], "entities": []}, {"text": "The TAC-KBP and AIDA testb dataset contains 1020 and 4485 non-NIL queries respectively.", "labels": [], "entities": [{"text": "TAC-KBP", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.7748209834098816}, {"text": "AIDA testb dataset", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.9456203182538351}]}, {"text": "For candidate generation, mention-to-entity dictionary is built by mining Wikipedia structures, following.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.707771509885788}]}, {"text": "We keep top 30 candidates by prominence P (e|m) for speed consideration.", "labels": [], "entities": [{"text": "prominence P", "start_pos": 29, "end_pos": 41, "type": "METRIC", "confidence": 0.9199908077716827}, {"text": "speed", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.963480532169342}]}, {"text": "The candidate generation recall are 94.0% and 98.5% for TAC and AIDA respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9961463212966919}, {"text": "TAC", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.5132942199707031}, {"text": "AIDA", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.476826548576355}]}, {"text": "Analysis: shows evaluation results across several best performing systems.) is a collective approach, using Personalized PageRank to propagate evidence between different decisions.", "labels": [], "entities": []}, {"text": "To our surprise, our method with only local evidence even beats several complex collective methods with simple word similarity.", "labels": [], "entities": []}, {"text": "This reveals the importance of context modeling in semantic space.", "labels": [], "entities": [{"text": "context modeling", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7360244989395142}]}, {"text": "Collective approaches can improve performance only when local evidence is not confident enough.", "labels": [], "entities": []}, {"text": "When embedding our similarity measure sim(d, e) into, we achieve the best results on AIDA.", "labels": [], "entities": [{"text": "AIDA", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.8858439922332764}]}, {"text": "A close error analysis shows some typical errors due to the lack of prominence feature and name matching feature.", "labels": [], "entities": [{"text": "name matching", "start_pos": 91, "end_pos": 104, "type": "TASK", "confidence": 0.7829227149486542}]}, {"text": "Some queries accidentally link to rare candidates and some link to entities with completely different names.", "labels": [], "entities": []}, {"text": "We will add these features as mentioned in Eq.", "labels": [], "entities": []}, {"text": "We will also add NIL-detection module, which is required by more realistic application scenarios.", "labels": [], "entities": []}, {"text": "A first thought is to construct pseudo-NIL with Wikipedia annotations and automatically learn the threshold and feature weight as in (", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation on TAC and AIDA dataset.", "labels": [], "entities": [{"text": "TAC", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.5397145748138428}, {"text": "AIDA dataset", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.8430233001708984}]}]}