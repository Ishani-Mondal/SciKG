{"title": [{"text": "A System for Summarizing Scientific Topics Starting from Keywords", "labels": [], "entities": [{"text": "Summarizing Scientific Topics", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.9287051955858866}]}], "abstractContent": [{"text": "In this paper, we investigate the problem of automatic generation of scientific surveys starting from keywords provided by a user.", "labels": [], "entities": [{"text": "automatic generation of scientific surveys", "start_pos": 45, "end_pos": 87, "type": "TASK", "confidence": 0.7561793506145478}]}, {"text": "We present a system that can take a topic query as input and generate a survey of the topic by first selecting a set of relevant documents, and then selecting relevant sentences from those documents.", "labels": [], "entities": []}, {"text": "We discuss the issues of robust evaluation of such systems and describe an evaluation corpus we generated by manually extracting factoids, or information units, from 47 gold standard documents (surveys and tutorials) on seven topics in Natural Language Processing.", "labels": [], "entities": []}, {"text": "We have manually annotated 2,625 sentences with these fac-toids (around 375 sentences per topic) to build an evaluation corpus for this task.", "labels": [], "entities": []}, {"text": "We present evaluation results for the performance of our system using this annotated data.", "labels": [], "entities": []}], "introductionContent": [{"text": "The rise of the number of publications in all scientific fields is making it more and more difficult to get quickly acquainted with the new developments in anew area.", "labels": [], "entities": []}, {"text": "One way to wade through this huge amount of scholarly information is to consult topical surveys written by experts in an area.", "labels": [], "entities": []}, {"text": "For example, for machine translation, one might read ( . Such surveys can be very helpful when available, but unfortunately, may not be available for all areas.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.8040268123149872}]}, {"text": "Additionally, the manual surveys quickly go out of date within a few years of publication as additional papers are published in the field.", "labels": [], "entities": []}, {"text": "Thus, a system that can generate such surveys automatically would be a useful tool.", "labels": [], "entities": []}, {"text": "Short summaries in the form of abstracts are available for individual papers, but no such information is available for scientific topics.", "labels": [], "entities": []}, {"text": "In this paper, we explore strategies for generating and evaluating such surveys of scientific topics automatically starting from a phrase representing a topic area.", "labels": [], "entities": []}, {"text": "We evaluate our system on a set of topics in the field of Natural Language Processing.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.638038694858551}]}, {"text": "In earlier work, () have examined the problem of summarizing scientific articles using rhetorical analysis of sentences.", "labels": [], "entities": [{"text": "summarizing scientific articles", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.9267991383870443}]}, {"text": "have also discussed the problem of generating surveys of multiple papers.", "labels": [], "entities": []}, {"text": "presented experiments on generating surveys of scientific topics starting from papers to be summarized.", "labels": [], "entities": []}, {"text": "More recently, have presented initial results on automatically generating related work section fora target paper by taking a hierarchical topic tree as an input.", "labels": [], "entities": []}, {"text": "In this paper, we tackle the more challenging problem of summarizing a topic starting from a topic query.", "labels": [], "entities": [{"text": "summarizing a topic", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8985786239306132}]}, {"text": "Our system takes as an input a string describing the topic area, selects the relevant papers from a corpus of papers, and then selects sentences from the citing sentences to these papers to generate a survey of the topic.", "labels": [], "entities": []}, {"text": "A sample output of our system for the topic of \"Word Sense Disambiguation\" is shown in.", "labels": [], "entities": [{"text": "Word Sense Disambiguation\"", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.7296469435095787}]}], "datasetContent": [{"text": "We use the ACL Anthology Network (AAN) as the corpus for our experiments ().", "labels": [], "entities": [{"text": "ACL Anthology Network (AAN)", "start_pos": 11, "end_pos": 38, "type": "DATASET", "confidence": 0.9329870541890463}]}, {"text": "We built a factoid inventory for seven topics in NLP based on manual written surveys in the following way.", "labels": [], "entities": []}, {"text": "For each topic, we found at least 3 recent tutorials and 3 recent surveys on the topic and extracted the factoids that are covered in each of them.", "labels": [], "entities": []}, {"text": "shows the complete list of material collected for the topic of \"Word Sense Disambiguation\".", "labels": [], "entities": [{"text": "Word Sense Disambiguation\"", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.8090736642479897}]}, {"text": "We found around 80 factoids per topic on an average.", "labels": [], "entities": []}, {"text": "Once the factoids were extracted, each factoid was assigned a weight based on the number of documents it appears in, and any factoids with weight one were removed.", "labels": [], "entities": []}, {"text": "shows the top ten factoids in the topic of Word Sense Disambiguation along with their distribution across the different surveys and tutorials and final weight.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.6870963374773661}]}, {"text": "For each of the topics, we used the method described in Section 2 to create a candidate document set and extracted the candidate citing sentences to be used as the input for the content selection component.", "labels": [], "entities": []}, {"text": "Each sentence in each topic was then annotated by a human judge against the factoid list for that topic.", "labels": [], "entities": []}, {"text": "A sentence is allowed to have zero or more than one factoid.", "labels": [], "entities": []}, {"text": "The human assessors were graduate students in Computer Science who have taken a basic \"Natural Language Processing\" course or an equivalent course.", "labels": [], "entities": []}, {"text": "On an average, 375 citing sentences were annotated for each topic, with 2,625 sentences being annotated in total.", "labels": [], "entities": []}, {"text": "We present all our experimental results on this large annotated corpora which is also available for download 2 .  To do an evaluation of our different content selection methods, we first select the documents using our Restricted Expansion method, and then pick  the citing sentences to be used as the input to the summarization module as described in Section 2.", "labels": [], "entities": [{"text": "summarization", "start_pos": 314, "end_pos": 327, "type": "TASK", "confidence": 0.9633784294128418}]}, {"text": "Given this input, we generate 500 word summaries for each of the seven topics using the four methods: Centroid, Lexrank, C-Lexrank and a random baseline.", "labels": [], "entities": [{"text": "Lexrank", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.8517797589302063}]}, {"text": "For each summary, we compute two evaluation metrics.", "labels": [], "entities": []}, {"text": "The first is the Pyramid score () computed by treating the factoids as Summary Content Units (SCU's).", "labels": [], "entities": [{"text": "Pyramid score", "start_pos": 17, "end_pos": 30, "type": "METRIC", "confidence": 0.740324854850769}]}, {"text": "The Pyramid scores for each summary is shown in Table 4.", "labels": [], "entities": []}, {"text": "The second metric is an Unnormalized Relative Utility score, computed using the factoid scores of sentences based on the method presented in.", "labels": [], "entities": [{"text": "Unnormalized Relative Utility score", "start_pos": 24, "end_pos": 59, "type": "METRIC", "confidence": 0.7302592992782593}]}, {"text": "We call this Unnormalized RU since we are notable to normalize the scores with human generated gold summaries.", "labels": [], "entities": [{"text": "RU", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.7226254940032959}]}, {"text": "The results for Unnormalized RU are shown in.", "labels": [], "entities": [{"text": "RU", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.37016019225120544}]}, {"text": "The parameter \u03b1 is the RU penalty for including a redundant sentence subsumed by an earlier sentence.", "labels": [], "entities": [{"text": "RU", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9896532893180847}]}, {"text": "If the summary chooses a sentence s i with score w orig that is subsumed by an earlier summary sentence, the score is reduced as w subsumed = (\u03b1 * w orig ).", "labels": [], "entities": []}, {"text": "We approximate subsumption by marking a sentence s j as being subsumed by s i if F j \u2282 F i , where F i and F j are sets of factoids covered in each sentence.: Results of Unnormalized Relative Utility evaluation for the three methods and random baseline using \u03b1 = 0.5.", "labels": [], "entities": []}, {"text": "The reason for the relatively high scores for the random baseline is that our process to select the initial set of sentences eliminates many bad sentences.", "labels": [], "entities": []}, {"text": "For example, fora subset of 5 topics, the total input set contains 1508 sentences, out of which 922 of the sentences (60%) have at least one factoid.", "labels": [], "entities": []}, {"text": "This makes it highly likely to pick good content sentences even when we are picking sentences at random.", "labels": [], "entities": []}, {"text": "We find that the Lexrank method outperforms other sentence selection methods on both evaluation metrics.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7028310149908066}]}, {"text": "The higher performance of Lexrank compared to Centroid is consistent with earlier published results ().", "labels": [], "entities": [{"text": "Lexrank", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9028682112693787}]}, {"text": "The reason for the low performance of C-Lexrank as compared to Lexrank on this data set can be attributed to the fact that the input sentence set is derived from a much more diverse set of papers which can have a high diversity in lexical choice when describing the same factoid.", "labels": [], "entities": []}, {"text": "Thus simple lexical similarity is not enough to find good clusters in this sentence set.", "labels": [], "entities": []}, {"text": "The lower Unnormalized RU scores compared to Pyramid scores indicate that we are selecting sentences containing highly weighted factoids, but we do not select the most informative sentences that contain a large number of factoids.", "labels": [], "entities": [{"text": "Unnormalized RU", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7906589210033417}]}, {"text": "This also shows that we select some redundant factoids, since Unnormalized RU contains a penalty for redundancy.", "labels": [], "entities": []}, {"text": "This is again, explained by the fact that the simple lexical diversity based model in CLexrank is notable to detect the same factoids being present in two sentences.", "labels": [], "entities": []}, {"text": "Despite these shortcomings, our system works quite well in terms of content selection for unseen topics, shows the top 5 sentences for the query \"Conditional Random Fields\".", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of different methods for  document selection by measuring the Cumulative  Gain (CG) of top 5, 10 and 20 results.", "labels": [], "entities": [{"text": "document selection", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7404062449932098}, {"text": "Cumulative  Gain (CG)", "start_pos": 83, "end_pos": 104, "type": "METRIC", "confidence": 0.8518453478813172}]}, {"text": " Table 2: The set of surveys and tutorials col- lected for the topic of \"Word Sense Disambigua- tion\". Sizes for surveys are expressed in number  of pages, sizes for tutorials are expressed in num- ber of slides.", "labels": [], "entities": [{"text": "Word Sense Disambigua- tion", "start_pos": 73, "end_pos": 100, "type": "TASK", "confidence": 0.711582362651825}]}, {"text": " Table 4: Results of pyramid evaluation for each  of the three methods and the random baseline on  each topic.", "labels": [], "entities": []}, {"text": " Table 5. The parameter \u03b1 is the RU  penalty for including a redundant sentence sub- sumed by an earlier sentence. If the summary  chooses a sentence s i with score w orig that is sub- sumed by an earlier summary sentence, the score  is reduced as w subsumed = (\u03b1  *  w orig ). We ap- proximate subsumption by marking a sentence s j  as being subsumed by s i if F j \u2282 F i , where F i and  F j are sets of factoids covered in each sentence.", "labels": [], "entities": [{"text": "RU", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9851986765861511}]}, {"text": " Table 5: Results of Unnormalized Relative Utility  evaluation for the three methods and random base- line using \u03b1 = 0.5.", "labels": [], "entities": [{"text": "Unnormalized Relative Utility", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.507638563712438}]}]}