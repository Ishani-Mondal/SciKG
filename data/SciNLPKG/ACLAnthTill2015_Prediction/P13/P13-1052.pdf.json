{"title": [{"text": "GlossBoot: Bootstrapping Multilingual Domain Glossaries from the Web", "labels": [], "entities": []}], "abstractContent": [{"text": "We present GlossBoot, an effective minimally-supervised approach to acquiring wide-coverage domain glossaries for many languages.", "labels": [], "entities": []}, {"text": "For each language of interest, given a small number of hypernymy relation seeds concerning a target domain, we bootstrap a glossary from the Web for that domain by means of iteratively acquired term/gloss extraction patterns.", "labels": [], "entities": []}, {"text": "Our experiments show high performance in the acquisition of domain terminologies and glossaries for three different languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Much textual content, such as that available on the Web, contains a great deal of information focused on specific areas of knowledge.", "labels": [], "entities": []}, {"text": "However, it is not infrequent that, when reading a domainspecific text, we humans do not know the meaning of one or more terms.", "labels": [], "entities": []}, {"text": "To help the human understanding of specialized texts, repositories of textual definitions for technical terms, called glossaries, are compiled as reference resources within each domain of interest.", "labels": [], "entities": []}, {"text": "Interestingly, electronic glossaries have been shown to be key resources not only for humans, but also in Natural Language Processing (NLP) tasks such as Question Answering (, Word Sense Disambiguation ( and ontology learning.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.8722119629383087}, {"text": "Word Sense Disambiguation", "start_pos": 176, "end_pos": 201, "type": "TASK", "confidence": 0.6623049378395081}]}, {"text": "Today large numbers of glossaries are available on the Web.", "labels": [], "entities": []}, {"text": "However most such glossaries are small-scale, being made up of just some hundreds of definitions.", "labels": [], "entities": []}, {"text": "Consequently, individual glossaries typically provide a partial view of a given domain.", "labels": [], "entities": []}, {"text": "Moreover, there is no easy way of retrieving the subset of Web glossaries which appertains to a domain of interest.", "labels": [], "entities": []}, {"text": "Although online services such as Google Define allow the user to retrieve definitions for an input term, such definitions are extracted from Web glossaries and put together for the given term regardless of their domain.", "labels": [], "entities": []}, {"text": "As a result, gathering a large-scale, full-fledged domain glossary is not a speedy operation.", "labels": [], "entities": []}, {"text": "Collaborative efforts are currently producing large-scale encyclopedias, such as Wikipedia, which are proving very useful in NLP (.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.9526010751724243}]}, {"text": "Interestingly, wikipedias also include manually compiled glossaries.", "labels": [], "entities": []}, {"text": "However, such glossaries still suffer from the same above-mentioned problems, i.e., being incomplete or over-specific, and hard to customize according to a user's needs.", "labels": [], "entities": []}, {"text": "To automatically obtain large domain glossaries, over recent years computational approaches have been developed which extract textual definitions from corpora () or the Web).", "labels": [], "entities": []}, {"text": "The former methods start from a given set of terms (possibly automatically extracted from a domain corpus) and then harvest textual definitions for these terms from the input corpus using a supervised system.", "labels": [], "entities": []}, {"text": "Webbased methods, instead, extract text snippets from Web pages which match pre-defined lexical patterns, such as \"X is a Y\", along the lines of.", "labels": [], "entities": []}, {"text": "These approaches typically perform with high precision and low recall, because they fall short of detecting the high variability of the syntactic structure of textual definitions.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9973337650299072}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9992948770523071}]}, {"text": "To address the low-recall issue, recurring cue terms occurring within dictionary and encyclopedic resources can be automatically extracted and incorporated into lexical patterns).", "labels": [], "entities": []}, {"text": "However, this approach is term-specific and does not scale to arbitrary terminologies and domains.", "labels": [], "entities": []}, {"text": "In this paper we propose GlossBoot, a novel approach which reduces human intervention to a bare minimum and exploits the Web to learn a Figure 1: The GlossBoot bootstrapping process for glossary learning.", "labels": [], "entities": []}, {"text": "Given a domain and a language of interest, we bootstrap the glossary learning process with just a few hypernymy relations (such as computer is-a device), with the only condition that the (term, hypernym) pairs must be specific enough to implicitly identify the domain in the target language.", "labels": [], "entities": []}, {"text": "Hence we drop the requirement of a large domain corpus, and also avoid the use of training data or a manually defined set of lexical patterns.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first approach which jointly acquires large amounts of terms and glosses from the Web with minimal supervision for any target domain and language.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments to evaluate the quality of both terms and glosses, as jointly extracted by GlossBoot.", "labels": [], "entities": [{"text": "GlossBoot", "start_pos": 100, "end_pos": 109, "type": "DATASET", "confidence": 0.9573103189468384}]}, {"text": "For each domain and language we calculated coverage, extra-coverage and precision of the acquired terms T . Coverage is the ratio of extracted terms in T also contained in the gold standard\u02c6Tdard\u02c6 dard\u02c6T to the size of\u02c6Tof\u02c6 of\u02c6T . Extra-coverage is calculated as the ratio of the additional extracted terms in T \\ \u02c6 T over the number of gold standard terms\u02c6T terms\u02c6 terms\u02c6T . Finally, precision is the ratio of extracted terms in T deemed to be within the domain.", "labels": [], "entities": [{"text": "coverage", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9912543892860413}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9994902610778809}, {"text": "precision", "start_pos": 385, "end_pos": 394, "type": "METRIC", "confidence": 0.9995183944702148}]}, {"text": "To calculate precision we randomly sampled 5% of the retrieved terms and asked two human annotators to manually tag their domain pertinence (with adjudication in case of disagreement; \u03ba = .62, indicating substantial agreement).", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9992823004722595}]}, {"text": "Note that by sampling on the entire set T , we calculate the precision of both terms in T \u2229 \u02c6 T , i.e., in the gold standard, and terms in T \\ \u02c6 T , i.e., not in the gold standard, which are not necessarily outside the domain.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9993360638618469}]}, {"text": "We calculated the precision of the extracted glosses as the ratio of glosses which were both well-formed textual definitions and specific  to the target domain.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9991287589073181}]}, {"text": "Precision was determined on a random sample of 5% of the acquired glosses for each domain and language.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9785560369491577}]}, {"text": "The annotation was made by two annotators, with \u03ba = .675, indicating substantial agreement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Examples of extracted terms, glosses and hypernyms (seeds are in bold, domain terms, i.e., in  T k\u22121", "labels": [], "entities": []}, {"text": " Table 3: Hypernymy relation seeds used to bootstrap glossary learning in the four domains for the English  language.", "labels": [], "entities": []}, {"text": " Table 4:  Size of the gold-standard and  automatically-acquired glossaries for the four  domains in the three languages of interest.", "labels": [], "entities": []}, {"text": " Table 6: Precision, coverage and extra-coverage of  the term extraction phase after 5 iterations.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9975671768188477}, {"text": "coverage", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9847632050514221}, {"text": "term extraction phase", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.7516606648763021}]}, {"text": " Table 8: Precision of the glosses for the four do- mains and for the three languages.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9862359166145325}]}, {"text": " Table 9: Number of domain glosses (from a ran- dom sample of 100 gold standard terms per do- main) retrieved using Google Define and Gloss- Boot.", "labels": [], "entities": []}]}