{"title": [{"text": "Distortion Model Considering Rich Context for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.8244071205457052}]}], "abstractContent": [{"text": "This paper proposes new distortion models for phrase-based SMT.", "labels": [], "entities": [{"text": "phrase-based SMT", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.63983353972435}]}, {"text": "In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP).", "labels": [], "entities": []}, {"text": "We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously.", "labels": [], "entities": []}, {"text": "Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates.", "labels": [], "entities": []}, {"text": "It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data.", "labels": [], "entities": []}, {"text": "In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.999332845211029}, {"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9985852241516113}]}], "introductionContent": [{"text": "Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Estimating appropriate word order in a target language", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.8458893895149231}, {"text": "statistical machine translation (SMT)", "start_pos": 97, "end_pos": 134, "type": "TASK", "confidence": 0.8195273776849111}]}, {"text": "This is particularly true when translating between languages with widely different word orders.", "labels": [], "entities": []}, {"text": "To address this problem, there has been a lot of research done into word reordering: lexical reordering model), which is one of the distortion models, reordering constraint (), pre-ordering (), hierarchical phrase-based SMT, and syntax-based SMT).", "labels": [], "entities": [{"text": "SMT", "start_pos": 220, "end_pos": 223, "type": "TASK", "confidence": 0.8013879656791687}, {"text": "SMT", "start_pos": 242, "end_pos": 245, "type": "TASK", "confidence": 0.7871894240379333}]}, {"text": "In general, source language syntax is useful for handling long distance word reordering.", "labels": [], "entities": [{"text": "long distance word reordering", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.6595265343785286}]}, {"text": "However, obtaining syntax requires a syntactic parser, which is not available for many languages.", "labels": [], "entities": []}, {"text": "Phrase-based SMT () is a widely used SMT method that does not use a parser.", "labels": [], "entities": [{"text": "Phrase-based SMT", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5792862474918365}, {"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.990992546081543}]}, {"text": "Phrase-based SMT mainly 1 estimates word reordering using distortion models 2 . Therefore, distortion models are one of the most important components for phrase-based SMT.", "labels": [], "entities": [{"text": "Phrase-based SMT", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5794217884540558}, {"text": "word reordering", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7399455308914185}, {"text": "phrase-based SMT", "start_pos": 154, "end_pos": 170, "type": "TASK", "confidence": 0.5713389217853546}]}, {"text": "On the other hand, there are methods other than distortion models for improving word reordering for phrase-based SMT, such as pre-ordering or reordering constraints.", "labels": [], "entities": [{"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.7786412835121155}]}, {"text": "However, these methods also use distortion models when translating by phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.7122290134429932}]}, {"text": "Therefore, distortion models do not compete against these methods and are commonly used with them.", "labels": [], "entities": []}, {"text": "If there is a good distortion model, it will improve the translation quality of phrase-based SMT and benefit to the methods using distortion models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.6523266434669495}]}, {"text": "In this paper, we propose two distortion models for phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.641485869884491}]}, {"text": "In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP).", "labels": [], "entities": []}, {"text": "The proposed models are the pair model and the sequence model.", "labels": [], "entities": []}, {"text": "The pair model utilizes the word at the CP, a word at an NP candidate site, and the words surrounding the CP and the NP candidates (context) simultaneously.", "labels": [], "entities": []}, {"text": "In addition, the sequence model, which is the further improved model, considers richer context by identifying the label sequence that specify the span from the CP to the NP.", "labels": [], "entities": []}, {"text": "It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data.", "labels": [], "entities": []}, {"text": "Our model learns the preference relations among NP kinou kare wa pari de hon wo katta he bought books in Paris yesterday Source: Target: Figure 1: An example of left-to-right translation for Japanese-English.", "labels": [], "entities": []}, {"text": "Boxes represent phrases and arrows indicate the translation order of the phrases. candidates.", "labels": [], "entities": []}, {"text": "Our model consists of one probabilistic model and does not require a parser.", "labels": [], "entities": []}, {"text": "Experiments confirmed the effectiveness of our method for Japanese-English and Chinese-English translation, using NTCIR-9 Patent Machine Translation Task data sets).", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.7411676943302155}, {"text": "NTCIR-9 Patent Machine Translation Task data sets", "start_pos": 114, "end_pos": 163, "type": "DATASET", "confidence": 0.83506007705416}]}], "datasetContent": [{"text": "In order to confirm the effects of our distortion model, we conducted a series of Japanese to English (JE) and Chinese to English (CE) translation experiments.", "labels": [], "entities": [{"text": "Chinese to English (CE) translation", "start_pos": 111, "end_pos": 146, "type": "TASK", "confidence": 0.563365808555058}]}], "tableCaptions": [{"text": " Table 2: The \"C, I, and N\" label set.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation results for each method. The values are case-insensitive BLEU scores. Bold numbers  indicate no significant difference from the best result in each language pair using the bootstrap resampling  test at a significance level \u03b1 = 0.01 (Koehn, 2004).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9818634986877441}]}]}