{"title": [{"text": "Supervised Model Learning with Feature Grouping based on a Discrete Constraint", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a framework of supervised model learning that realizes feature grouping to obtain lower complexity models.", "labels": [], "entities": []}, {"text": "The main idea of our method is to integrate a discrete constraint into model learning with the help of the dual decomposition technique.", "labels": [], "entities": []}, {"text": "Experiments on two well-studied NLP tasks, dependency parsing and NER, demonstrate that our method can provide state-of-the-art performance even if the degrees of freedom in trained models are surprisingly small, i.e., 8 or even 2.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.8766118884086609}, {"text": "NER", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.7800417542457581}]}, {"text": "This significant benefit enables us to provide compact model representation, which is especially useful in actual use.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper focuses on the topic of supervised model learning, which is typically represented as the following form of the optimization problem: where Dis supervised training data that consists of the corresponding input x and output y pairs, that is, (x, y) \u2208 D. w is an N -dimensional vector representation of a set of optimization variables, which are also interpreted as feature weights.", "labels": [], "entities": [{"text": "supervised model learning", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.6484574576218923}]}, {"text": "L(w; D) and \u2126(w) represent a loss function and a regularization term, respectively.", "labels": [], "entities": []}, {"text": "Nowadays, we, inmost cases, utilize a supervised learning method expressed as the above optimization problem to estimate the feature weights of many natural language processing (NLP) tasks, such as text classification, POS-tagging, named entity recognition, dependency parsing, and semantic role labeling.", "labels": [], "entities": [{"text": "text classification", "start_pos": 198, "end_pos": 217, "type": "TASK", "confidence": 0.7911690175533295}, {"text": "named entity recognition", "start_pos": 232, "end_pos": 256, "type": "TASK", "confidence": 0.6149267852306366}, {"text": "dependency parsing", "start_pos": 258, "end_pos": 276, "type": "TASK", "confidence": 0.8316669166088104}, {"text": "semantic role labeling", "start_pos": 282, "end_pos": 304, "type": "TASK", "confidence": 0.6556416352589926}]}, {"text": "In the last decade, the L 1 -regularization technique, which incorporates L 1 -norm into \u2126(w), has become popular and widely-used in many NLP tasks ().", "labels": [], "entities": []}, {"text": "The reason is that L 1 -regularizers encourage feature weights to be zero as much as possible in model learning, which makes the resultant model a sparse solution (many zero-weights exist).", "labels": [], "entities": []}, {"text": "We can discard all features whose weight is zero from the trained model 1 without any loss.", "labels": [], "entities": []}, {"text": "Therefore, L 1 -regularizers have the ability to easily and automatically yield compact models without strong concern over feature selection.", "labels": [], "entities": []}, {"text": "Compact models generally have significant and clear advantages in practice: instances are faster loading speed to memory, less memory occupation, and even faster decoding is possible if the model is small enough to be stored in cache memory.", "labels": [], "entities": []}, {"text": "Given this background, our aim is to establish a model learning framework that can reduce the model complexity beyond that possible by simply applying L 1 -regularizers.", "labels": [], "entities": []}, {"text": "To achieve our goal, we focus on the recently developed concept of automatic feature grouping (.", "labels": [], "entities": [{"text": "automatic feature grouping", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.6781147917111715}]}, {"text": "We introduce a model learning framework that achieves feature grouping by incorporating a discrete constraint during model learning.", "labels": [], "entities": [{"text": "feature grouping", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7381971776485443}]}], "datasetContent": [{"text": "We conducted experiments on two well-studied NLP tasks, namely named entity recognition (NER) and dependency parsing (DEPAR).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.7226915260155996}, {"text": "dependency parsing (DEPAR)", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.687595522403717}]}, {"text": "Basic settings: We simply reused the settings of most previous studies.", "labels": [], "entities": []}, {"text": "We used CoNLL'03 data for NER, and the Penn Treebank (PTB) III corpus ( converted to dependency trees for DEPAR ().", "labels": [], "entities": [{"text": "CoNLL'03 data", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9119064509868622}, {"text": "NER", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8140485882759094}, {"text": "Penn Treebank (PTB) III corpus", "start_pos": 39, "end_pos": 69, "type": "DATASET", "confidence": 0.9741233331816537}, {"text": "DEPAR", "start_pos": 106, "end_pos": 111, "type": "DATASET", "confidence": 0.896282970905304}]}, {"text": "Our decoding models are the Viterbi algorithm on CRF (, and the secondorder parsing model proposed by for NER and DEPAR, respectively.", "labels": [], "entities": [{"text": "CRF", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8949770927429199}, {"text": "DEPAR", "start_pos": 114, "end_pos": 119, "type": "DATASET", "confidence": 0.8976072072982788}]}, {"text": "Features are automatically generated according to the predefined feature templates widely-used in the previous studies.", "labels": [], "entities": []}, {"text": "We also integrated the cluster features obtained by the method explained in ( as additional features for evaluating our method in the range of the current best systems.", "labels": [], "entities": []}, {"text": "Evaluation measures: The purpose of our experiments is to investigate the effectiveness of our proposed method in terms of both its performance and the complexity of the trained model.", "labels": [], "entities": []}, {"text": "Therefore, our evaluation measures consist of two axes.", "labels": [], "entities": []}, {"text": "Task performance was mainly evaluated in terms of the complete sentence accuracy (COMP) since the objective of all model learning methods evaluated in our experiments is to maximize COMP.", "labels": [], "entities": [{"text": "complete sentence accuracy (COMP)", "start_pos": 54, "end_pos": 87, "type": "METRIC", "confidence": 0.7083381116390228}]}, {"text": "We also report the F \u03b2=1 score (F-sc) for NER, and the unlabeled attachment score (UAS) for DE-PAR for comparison with previous studies.", "labels": [], "entities": [{"text": "F \u03b2=1 score (F-sc)", "start_pos": 19, "end_pos": 37, "type": "METRIC", "confidence": 0.8933719918131828}, {"text": "NER", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.7410297393798828}, {"text": "unlabeled attachment score (UAS)", "start_pos": 55, "end_pos": 87, "type": "METRIC", "confidence": 0.8042400379975637}]}, {"text": "Model complexity is evaluated by the number of non-zero active features (#nzF) and the degree of freedom (#DoF) (.", "labels": [], "entities": []}, {"text": "#nzF is the number of features whose corresponding feature weight is non-zero in the trained model, and #DoF is the number of unique non-zero feature weights.", "labels": [], "entities": []}, {"text": "Baseline methods: Our main baseline is L 1 -regularized sparse modeling.", "labels": [], "entities": []}, {"text": "To cover both batch and online leaning, we selected L 1 -regularized CRF (L1CRF) () optimized by OWL-QN (  for the NER experiment, and the L 1 -regularized regularized dual averaging (L1RDA) method for DEPAR.", "labels": [], "entities": [{"text": "OWL-QN", "start_pos": 97, "end_pos": 103, "type": "DATASET", "confidence": 0.8116133809089661}, {"text": "DEPAR", "start_pos": 202, "end_pos": 207, "type": "DATASET", "confidence": 0.8212237358093262}]}, {"text": "Additionally, we also evaluated L 2 -regularized CRF (L2CRF) with L-BFGS ( for NER, and passive-aggressive algorithm (L2PA)) for DE-PAR since L 2 -regularizer often provides better results than L 1 -regularizer ( ).", "labels": [], "entities": []}, {"text": "For a fair comparison, we applied the procedure of Step2 as a simple quantization method to trained models obtained from L 1 -regularized model learning, which we refer to as (QT).", "labels": [], "entities": []}, {"text": "RDA provided better results at least in our experiments than L1-regularized FOBOS (, and its variant (, which are more familiar to the NLP community.", "labels": [], "entities": [{"text": "FOBOS", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.8340755701065063}]}, {"text": "5 L2PA is also known as a loss augmented variant of onebest MIRA, well-known in DEPAR ().", "labels": [], "entities": [{"text": "MIRA", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.7941364049911499}, {"text": "DEPAR", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.8217484354972839}]}], "tableCaptions": [{"text": " Table 1: Comparison results of the methods on test  data (K: thousand, M: million)", "labels": [], "entities": []}]}