{"title": [{"text": "Domain-Specific Coreference Resolution with Lexicalized Features", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.8414218425750732}]}], "abstractContent": [{"text": "Most coreference resolvers rely heavily on string matching, syntactic properties, and semantic attributes of words, but they lack the ability to make decisions based on individual words.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 5, "end_pos": 26, "type": "TASK", "confidence": 0.939035177230835}]}, {"text": "In this paper, we explore the benefits of lexicalized features in the setting of domain-specific corefer-ence resolution.", "labels": [], "entities": [{"text": "corefer-ence resolution", "start_pos": 97, "end_pos": 120, "type": "TASK", "confidence": 0.8079370856285095}]}, {"text": "We show that adding lexicalized features to off-the-shelf coref-erence resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures.", "labels": [], "entities": [{"text": "coref-erence resolvers", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7357199490070343}, {"text": "coreference resolution", "start_pos": 174, "end_pos": 196, "type": "TASK", "confidence": 0.8257521986961365}]}], "introductionContent": [{"text": "Coreference resolvers are typically evaluated on collections of news articles that cover a wide range of topics, such as the ACE) and OntoNotes () data sets.", "labels": [], "entities": [{"text": "Coreference resolvers", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9046553075313568}, {"text": "ACE) and OntoNotes () data sets", "start_pos": 125, "end_pos": 156, "type": "DATASET", "confidence": 0.7516214294092995}]}, {"text": "Many NLP applications, however, involve text analysis for specialized domains, such as clinical medicine, legal text analysis, and biological literature).", "labels": [], "entities": [{"text": "legal text analysis", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.6248903175195059}]}, {"text": "Learning-based coreference resolvers can be easily retrained fora specialized domain given annotated training texts for that domain.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.8127896189689636}]}, {"text": "However, we found that retraining an off-the-shelf coreference resolver with domainspecific texts showed little benefit.", "labels": [], "entities": [{"text": "coreference resolver", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.9060985743999481}]}, {"text": "This surprising result led us to question the nature of the feature sets used by noun phrase (NP) coreference resolvers.", "labels": [], "entities": [{"text": "noun phrase (NP) coreference resolvers", "start_pos": 81, "end_pos": 119, "type": "TASK", "confidence": 0.6322636519159589}]}, {"text": "Nearly all of the features employed by recent systems fall into three categories: string match and word overlap, syntactic properties (e.g., appositives, predicate nominals, parse features, etc.), and semantic matching (e.g., gender agreement, WordNet similarity, named entity classes, etc.).", "labels": [], "entities": []}, {"text": "Conspicuously absent from most systems are lexical features that allow the classifier to consider the specific words when making a coreference decision.", "labels": [], "entities": []}, {"text": "A few researchers have experimented with lexical features, but they achieved mixed results in evaluations on broad-coverage corpora.", "labels": [], "entities": []}, {"text": "We hypothesized that lexicalized features can have a more substantial impact in domain-specific settings.", "labels": [], "entities": []}, {"text": "Lexical features can capture domainspecific knowledge and subtle semantic distinctions that maybe important within a domain.", "labels": [], "entities": []}, {"text": "For example, based on the resolutions found in domain-specific training sets, our lexicalized features captured the knowledge that \"tomcat\" can be coreferent with \"plane\", \"UAW\" can be coreferent with \"union\", and \"anthrax\" can be coreferent with \"diagnosis\".", "labels": [], "entities": []}, {"text": "Capturing these types of domain-specific information is often impossible using only general-purpose resources.", "labels": [], "entities": [{"text": "Capturing these types of domain-specific information", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7839526832103729}]}, {"text": "For example, WordNet defines \"tomcat\" only as an animal, does not contain an entry for \"UAW\", and categorizes \"anthrax\" and \"diagnosis\" very differently.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.944122850894928}]}, {"text": "In this paper, we evaluate the impact of lexicalized features on 4 domains: management succession (MUC-6 data), vehicle launches (MUC-7 data), disease outbreaks (ProMed texts), and terrorism (MUC-4 data).", "labels": [], "entities": [{"text": "MUC-6 data", "start_pos": 99, "end_pos": 109, "type": "DATASET", "confidence": 0.9036411345005035}, {"text": "MUC-7 data", "start_pos": 130, "end_pos": 140, "type": "DATASET", "confidence": 0.9373778402805328}, {"text": "ProMed texts", "start_pos": 162, "end_pos": 174, "type": "DATASET", "confidence": 0.9267050921916962}, {"text": "MUC-4 data", "start_pos": 192, "end_pos": 202, "type": "DATASET", "confidence": 0.9366455078125}]}, {"text": "We incorporate lexicalized feature sets into two different coreference architectures: Reconcile (), a pairwise coreference classifier, and Sieve (), a rule-based system.", "labels": [], "entities": []}, {"text": "Our results show that lexicalized features significantly improve performance in all four domains and in both types of coreference architectures.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first row shows the results without the lexicalized features (from).", "labels": [], "entities": []}, {"text": "All F scores for Reconcile with lexicalized features are significantly better than without these features based on the Paired Permutation test) with p \u2264 0.05.", "labels": [], "entities": [{"text": "F", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.9934936165809631}]}, {"text": "MUC-4 showed the largest gain for Reconcile, with the F score increasing from 69.5 to over 74.", "labels": [], "entities": [{"text": "MUC-4", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9815380573272705}, {"text": "Reconcile", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.7683678865432739}, {"text": "F score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9896592199802399}]}, {"text": "For most domains, adding the lexical features to Reconcile substantially increased precision with comparable levels of recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9993809461593628}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9983677268028259}]}, {"text": "The bottom half of contains the results of adding a lexical heuristic to Sieve.", "labels": [], "entities": []}, {"text": "The first row shows the default system with no lexical information.", "labels": [], "entities": []}, {"text": "All F scores with the lexical heuristic are significantly better than without it.", "labels": [], "entities": [{"text": "F", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.9958685636520386}]}, {"text": "In Sieve's high-precision coreference architecture, the lexical heuristic yields additional recall gains without sacrificing much precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9980164766311646}, {"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9886658191680908}]}, {"text": "shows the results for Reconcile and Sieve when training and testing on the ACE 2004 data.", "labels": [], "entities": [{"text": "Reconcile", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.6661584377288818}, {"text": "Sieve", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.8304646611213684}, {"text": "ACE 2004 data", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.9916969935099283}]}, {"text": "Here, we see little improvement from adding lexical information.", "labels": [], "entities": []}, {"text": "For Reconcile, the small differences in F scores are not statistically significant.", "labels": [], "entities": [{"text": "F scores", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9822276532649994}]}, {"text": "For Sieve, the unlexicalized system yields a significantly higher F score than when adding the lexical heuristic.", "labels": [], "entities": [{"text": "F score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9931120276451111}]}, {"text": "These results support our hypothesis that lexicalized information can be beneficial for capturing domain-specific word associations, but may not be as helpful in a broad-coverage setting where the language covers a diverse set of topics.", "labels": [], "entities": []}, {"text": "shows a re-evaluation of the crossdomain experiments from for Reconcile with the LexSet features added.", "labels": [], "entities": [{"text": "LexSet", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.9734717011451721}]}, {"text": "The bottom half of the table shows cross-domain experiments for Sieve using the lexical heuristic at the end of its rule set (LexEnd).", "labels": [], "entities": [{"text": "LexEnd", "start_pos": 126, "end_pos": 132, "type": "DATASET", "confidence": 0.9429747462272644}]}, {"text": "Results are presented using both the B 3 metric and the MUC Score (.", "labels": [], "entities": [{"text": "B 3 metric", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9509276350339254}, {"text": "MUC Score", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.9315391778945923}]}, {"text": "Training and testing on the same domain always produced the highest recall scores for MUC-7, ProMed, and MUC-4 when utilizing lexical features.", "labels": [], "entities": [{"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9993162155151367}, {"text": "ProMed", "start_pos": 93, "end_pos": 99, "type": "DATASET", "confidence": 0.9206727147102356}]}, {"text": "In all cases, lexical features acquired from same-domain texts yield results that are either clearly the best or not significantly different from the best.", "labels": [], "entities": []}, {"text": "For MUC-6 and MUC-7, the highest F score results almost always come from training on samedomain texts, although in some cases these results are not significantly different from training on other domains.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7209831476211548}, {"text": "MUC-7", "start_pos": 14, "end_pos": 19, "type": "DATASET", "confidence": 0.8524768948554993}, {"text": "F score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9936375021934509}]}, {"text": "Lexical features can yield improvements when training on a different domain if there is overlap in the vocabulary across the domains.", "labels": [], "entities": []}, {"text": "For the ProMed domain, the Sieve system performs significantly better, under both metrics, with same-domain lexical features than with lexical features acquired from a different domain.", "labels": [], "entities": [{"text": "ProMed domain", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9282886385917664}]}, {"text": "For Reconcile, there is not a significant difference in the F score for ProMed when training on ProMed, MUC-4, or MUC-7.", "labels": [], "entities": [{"text": "F score", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9925088882446289}, {"text": "ProMed", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.9454948306083679}, {"text": "ProMed", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.9872941374778748}, {"text": "MUC-4", "start_pos": 104, "end_pos": 109, "type": "DATASET", "confidence": 0.8938718438148499}, {"text": "MUC-7", "start_pos": 114, "end_pos": 119, "type": "DATASET", "confidence": 0.9564565420150757}]}, {"text": "In the MUC-4 domain, using same-domain lexical information always produces the best F score, under both metrics and in both coreference systems.", "labels": [], "entities": [{"text": "F score", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9893283843994141}]}], "tableCaptions": [{"text": " Table 1: Cross-domain B 3 (Bagga and Baldwin, 1998) results for Reconcile with its general feature set.  The Paired Permutation test", "labels": [], "entities": []}, {"text": " Table 2: B 3 results for baselines and lexicalized feature sets across four domains.", "labels": [], "entities": []}, {"text": " Table 3: B 3 results for baselines and lexicalized  feature sets on the broad-coverage ACE 2004 data  set.", "labels": [], "entities": [{"text": "ACE 2004 data  set", "start_pos": 88, "end_pos": 106, "type": "DATASET", "confidence": 0.9265083521604538}]}, {"text": " Table 4: Cross-domain B 3 and MUC results for Reconcile and Sieve with lexical features. Gray cells  represent results that are not significantly different from the best results in the column at the 0.05 p-level.", "labels": [], "entities": [{"text": "MUC", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.6129392981529236}]}]}