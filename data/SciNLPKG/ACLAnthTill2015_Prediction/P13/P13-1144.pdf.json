{"title": [{"text": "Text-Driven Toponym Resolution using Indirect Supervision", "labels": [], "entities": [{"text": "Text-Driven Toponym Resolution", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5502138137817383}]}], "abstractContent": [{"text": "Toponym resolvers identify the specific locations referred to by ambiguous place-names in text.", "labels": [], "entities": [{"text": "Toponym resolvers", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.846388190984726}]}, {"text": "Most resolvers are based on heuristics using spatial relationships between multiple toponyms in a document, or metadata such as population.", "labels": [], "entities": [{"text": "resolvers", "start_pos": 5, "end_pos": 14, "type": "TASK", "confidence": 0.9670938849449158}]}, {"text": "This paper shows that text-driven disambiguation for toponyms is far more effective.", "labels": [], "entities": []}, {"text": "We exploit document-level geotags to indirectly generate training instances for text classi-fiers for toponym resolution, and show that textual cues can be straightforwardly integrated with other commonly used ones.", "labels": [], "entities": [{"text": "toponym resolution", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7510088086128235}]}, {"text": "Results are given for both 19th century texts pertaining to the American Civil War and 20th century newswire articles.", "labels": [], "entities": [{"text": "19th century texts pertaining to the American Civil War", "start_pos": 27, "end_pos": 82, "type": "TASK", "confidence": 0.7519987225532532}]}], "introductionContent": [{"text": "It has been estimated that at least half of the world's stored knowledge, both printed and digital, has geographic relevance, and that geographic information pervades many more aspects of humanity than previously thought).", "labels": [], "entities": []}, {"text": "Thus, there is value in connecting linguistic references to places (e.g. placenames) to formal references to places (coordinates)).", "labels": [], "entities": []}, {"text": "Allowing for the querying and exploration of knowledge in a geographically informed way requires more powerful tools than a keyword-based search can provide, in part due to the ambiguity of toponyms (placenames).", "labels": [], "entities": []}, {"text": "Toponym resolution is the task of disambiguating toponyms in natural language contexts to geographic locations.", "labels": [], "entities": [{"text": "Toponym resolution", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8801905512809753}]}, {"text": "It plays an essential role in automated geographic indexing and information retrieval.", "labels": [], "entities": [{"text": "automated geographic indexing", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.5900396406650543}, {"text": "information retrieval", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.8252144157886505}]}, {"text": "This is useful for historical research that combines age-old geographic issues like territoriality with modern computational tools, studies of the effect of historically recorded travel costs on the shaping of empires (, and systems that convey the geographic content in news articles) and microblogs.", "labels": [], "entities": []}, {"text": "Entity disambiguation systems such as those of and disambiguate references to people and organizations as well as locations, but these systems do not take into account any features or measures unique to geography such as physical distance.", "labels": [], "entities": []}, {"text": "Here we demonstrate the utility of incorporating distance measurements in toponym resolution systems.", "labels": [], "entities": [{"text": "toponym resolution", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7932405769824982}]}, {"text": "Most work on toponym resolution relies on heuristics and hand-built rules.", "labels": [], "entities": [{"text": "toponym resolution", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.8565185070037842}]}, {"text": "Some use simple rules based on information from a gazetteer, such as population or administrative level (city, state, country, etc.), resolving every instance of the same toponym type to the same location regardless of context (.", "labels": [], "entities": []}, {"text": "Others use relationships between multiple toponyms in a context (local or whole document) and look for containment relationships, e.g. London and England occurring in the same paragraph or as the bigram London,.", "labels": [], "entities": [{"text": "bigram London", "start_pos": 196, "end_pos": 209, "type": "DATASET", "confidence": 0.8240425884723663}]}, {"text": "Still others first identify unambiguous toponyms and then disambiguate other toponyms based on geopolitical relationships with or distances to the unambiguous ones ().", "labels": [], "entities": []}, {"text": "Many favor resolutions of toponyms within a local context or document that cover a smaller geographic area over those that are more dispersed).", "labels": [], "entities": []}, {"text": "use relationships learned between people, organizations, and locations from Wikipedia to aid in toponym resolution when such named entities are present, but do not exploit any other textual context.", "labels": [], "entities": [{"text": "toponym resolution", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7482982575893402}]}, {"text": "Most of these approaches suffer from a major weakness: they rely primarily on spatial relationships and metadata about locations (e.g., population).", "labels": [], "entities": []}, {"text": "As such, they often require nearby toponyms (including unambiguous or containing toponyms) to resolve ambiguous ones.", "labels": [], "entities": []}, {"text": "This reliance can result in poor coverage when the required information is missing in the context or when a document mentions locations that are neither nearby geographically nor in a geopolitical relationship.", "labels": [], "entities": [{"text": "coverage", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9662895798683167}]}, {"text": "There is a clear opportunity that most ignore: use non-toponym textual context.", "labels": [], "entities": []}, {"text": "Spatially relevant words like downtown that are not explicit toponyms can be strong cues for resolution.", "labels": [], "entities": [{"text": "resolution", "start_pos": 93, "end_pos": 103, "type": "TASK", "confidence": 0.9680352210998535}]}, {"text": "Furthermore, the connection between non-spatial words and locations has been successfully exploited in data-driven approaches to document geolocation () and other tasks (.", "labels": [], "entities": []}, {"text": "In this paper, we learn resolvers that use all words in local or document context.", "labels": [], "entities": []}, {"text": "For example, the word lobster appearing near the toponym Portland indicates the location is Portland in Maine rather than Oregon or Michigan.", "labels": [], "entities": []}, {"text": "Essentially, we learn a text classifier per toponym.", "labels": [], "entities": []}, {"text": "There are no massive collections of toponyms labeled with locations, so we train models indirectly using geotagged Wikipedia articles.", "labels": [], "entities": []}, {"text": "Our results show these text classifiers are far more accurate than algorithms based on spatial proximity or metadata.", "labels": [], "entities": []}, {"text": "Furthermore, they are straightforward to combine with such algorithms and lead to error reductions for documents that match those algorithms' assumptions.", "labels": [], "entities": []}, {"text": "Our primary focus is toponym resolution, so we evaluate on toponyms identified by human annotators.", "labels": [], "entities": [{"text": "toponym resolution", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8737578094005585}]}, {"text": "However, it is important to consider the utility of an end-to-end toponym identification and resolution system, so we also demonstrate that performance is still strong when toponyms are detected with a standard named entity recognizer.", "labels": [], "entities": [{"text": "toponym identification and resolution", "start_pos": 66, "end_pos": 103, "type": "TASK", "confidence": 0.7581062018871307}]}, {"text": "We have implemented all the models discussed in this paper in an open source software package called Fieldspring, which is available on GitHub: http://github.com/utcompling/fieldspring Explicit instructions are provided for preparing data and running code to reproduce our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Many prior efforts use a simple accuracy metric: the fraction of toponyms whose predicted location is the same as the gold location.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9980280995368958}]}, {"text": "Such a metric can be problematic, however.", "labels": [], "entities": []}, {"text": "The gazetteer used by a resolver may not contain, fora given toponym, a location whose latitude and longitude exactly match the gold label for the toponym).", "labels": [], "entities": []}, {"text": "Also, some errors are worse than others, e.g. predicting a toponym's location to be on the other side of the world versus predicting it to be a different city in the same country-accuracy does not reflect this difference.", "labels": [], "entities": []}, {"text": "We choose a metric that instead measures the distance between the correct and predicted location for each toponym and compute the mean and median of all such error distances.", "labels": [], "entities": []}, {"text": "This is used in document geolocation work and is related to the root mean squared distance metric discussed by.", "labels": [], "entities": [{"text": "root mean squared distance metric", "start_pos": 64, "end_pos": 97, "type": "METRIC", "confidence": 0.7121057152748108}]}, {"text": "It is important to understand performance on plain text (without gold toponyms), which is the typical use case for applications using toponym resolvers.", "labels": [], "entities": []}, {"text": "Both the accuracy metric and the errordistance metric encounter problems when the set of predicted toponyms is not the same as the set of gold toponyms (regardless of locations), e.g. when a named entity recognizer is used to identify toponyms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9989941716194153}, {"text": "errordistance", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.9628308415412903}]}, {"text": "In this case, we can use precision and recall, where a true positive is defined as the prediction of a correctly identified toponym's location to be as close as possible to its gold label, given the gazetteer used.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9995778203010559}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9991509914398193}]}, {"text": "False positives occur when the NER incorrectly predicts a toponym, and false negatives occur when it fails to predict a toponym identified by the annotator.", "labels": [], "entities": [{"text": "NER incorrectly predicts a toponym", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.6171096384525299}]}, {"text": "When a correctly identified toponym receives an incorrect location prediction, this counts as both a false negative and a false positive.", "labels": [], "entities": []}, {"text": "We primarily present results from experiments with gold toponyms but include an accuracy measure for comparability with results from experiments run on plain text with a named entity recognizer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9988514184951782}]}, {"text": "This accuracy metric simply computes the fraction of toponyms that were resolved as close as possible to their gold label given the gazetteer.   from GEONAMES closest to the annotated location is always selected.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9991235136985779}, {"text": "GEONAMES", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.8020524978637695}]}, {"text": "The ORACLE mean and median error values on TR-CONLL are nonzero due to errors in the annotations and inconsistencies stemming from the fact that coordinates from GEONAMES were not used in the annotation of TR-CONLL.", "labels": [], "entities": [{"text": "ORACLE mean and median error", "start_pos": 4, "end_pos": 32, "type": "METRIC", "confidence": 0.9024928212165833}, {"text": "GEONAMES", "start_pos": 162, "end_pos": 170, "type": "DATASET", "confidence": 0.9127541780471802}, {"text": "TR-CONLL", "start_pos": 206, "end_pos": 214, "type": "DATASET", "confidence": 0.824272096157074}]}, {"text": "On both datasets, SPIDER achieves errors and accuracies much better than RANDOM, validating the intuition that authors tend to discuss places near each other more often than not, while some locations are more prominent in a given corpus despite violating the minimality heuristic.", "labels": [], "entities": []}, {"text": "The text-driven resolvers vastly outperform SPIDER, showing the effectiveness of textual cues for toponym resolution.", "labels": [], "entities": [{"text": "toponym resolution", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.8411510288715363}]}], "tableCaptions": [{"text": " Table 1: Statistics of the corpora used for evaluation. Columns subscripted by top give figures for  toponyms. The last two columns give the average number of candidate locations per toponym token and  the number of candidate locations for the most ambiguous toponym.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy and error distance metrics on test sets with gold toponyms.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9945530295372009}, {"text": "error distance metrics", "start_pos": 23, "end_pos": 45, "type": "METRIC", "confidence": 0.8786520759264628}]}, {"text": " Table 3: Precision, recall, and F-score of resolvers  on TRC-TEST with NER-identified toponyms.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.998487114906311}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9979497790336609}, {"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9993112087249756}, {"text": "TRC-TEST", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.7382760047912598}]}, {"text": " Table 4: Toponyms with the greatest total error  distances in kilometers from TRC-DEV with gold  toponyms resolved by TRAWL. N is the number  of instances, and the mean error for each toponym  type is also given.", "labels": [], "entities": [{"text": "TRC-DEV", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.9349173903465271}, {"text": "TRAWL", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.6950667500495911}]}, {"text": " Table 5: Top errors from CWAR-DEV resolved by  TRAWL+SPIDER.", "labels": [], "entities": [{"text": "TRAWL", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.984548807144165}, {"text": "SPIDER", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.597616970539093}]}]}