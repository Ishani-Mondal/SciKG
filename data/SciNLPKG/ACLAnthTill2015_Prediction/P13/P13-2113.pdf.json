{"title": [], "abstractContent": [{"text": "Supervised NLP tools and on-line services are often used on data that is very different from the manually annotated data used during development.", "labels": [], "entities": []}, {"text": "The performance loss observed in such cross-domain applications is often attributed to covari-ate shifts, with out-of-vocabulary effects as an important subclass.", "labels": [], "entities": []}, {"text": "Many discrim-inative learning algorithms are sensitive to such shifts because highly indicative features may swamp other indicative features.", "labels": [], "entities": []}, {"text": "Regularized and adversarial learning algorithms have been proposed to be more robust against covariate shifts.", "labels": [], "entities": []}, {"text": "We present anew perceptron learning algorithm using antagonistic adversaries and compare it to previous proposals on 12 multilingual cross-domain part-of-speech tagging datasets.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 146, "end_pos": 168, "type": "TASK", "confidence": 0.6977495104074478}]}, {"text": "While previous approaches do not improve on our supervised baseline, our approach is better across the board with an average 4% error reduction.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 128, "end_pos": 143, "type": "METRIC", "confidence": 0.9734339714050293}]}], "introductionContent": [{"text": "Most learning algorithms assume that training and test data are governed by identical distributions; and more specifically, in the case of part-of-speech (POS) tagging, that training and test sentences were sampled at random and that they are identically and independently distributed.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 139, "end_pos": 167, "type": "TASK", "confidence": 0.6488498389720917}]}, {"text": "Significance is usually tested across data points in standard NLP test sets.", "labels": [], "entities": []}, {"text": "Such datasets typically contain running text rather than independently sampled sentences, thereby violating the assumption that data points are independently distributed and sampled at random.", "labels": [], "entities": []}, {"text": "More importantly, significance across data points only says something about the likelyhood of observing the same effect on more data sampled the same way, but says nothing about likely performance on sentences sampled from different sources or different domains.", "labels": [], "entities": []}, {"text": "This paper considers the POS tagging problem, i.e. where we have training and test data consisting of sentences in which all words are assigned a label y chosen from a finite set of class labels {NOUN, VERB, DET,.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.9059312343597412}, {"text": "NOUN", "start_pos": 196, "end_pos": 200, "type": "METRIC", "confidence": 0.9307582974433899}, {"text": "VERB", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.9561993479728699}, {"text": "DET", "start_pos": 208, "end_pos": 211, "type": "METRIC", "confidence": 0.8939392566680908}]}, {"text": "}. We assume that we are interested in performance across data sets or domains rather than just performance across data points, but that we do not know the target domain in advance.", "labels": [], "entities": []}, {"text": "This is often the case when we develop NLP tools and on-line services.", "labels": [], "entities": []}, {"text": "We will do crossdomain experiments using several target domains in order to compute significance across domains, enabling us to say something about likely performance on new domains.", "labels": [], "entities": []}, {"text": "Several authors have noted how POS tagging performance is sensitive to cross-domain shifts, and while most authors have assumed known target distributions and pool unlabeled target data in order to automatically correct cross-domain bias, methods such as feature bagging (), learning with random adversaries () and L \u221e -regularization have been proposed to improve performance on unknown target distributions.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.8633650839328766}]}, {"text": "These methods explicitly or implicitly try to minimize average or worst-case expected error across a set of possible test distributions in various ways.", "labels": [], "entities": []}, {"text": "These algorithms are related because of the intimate relationship between adversarial corruption and regularization (.", "labels": [], "entities": []}, {"text": "This paper presents anew method based on learning with antagonistic adversaries.", "labels": [], "entities": []}, {"text": "Section 2 introduces previous work on robust perceptron learning, as well as the methods dicussed in the paper.", "labels": [], "entities": []}, {"text": "Section 3 motivates and introduces learning with antagonistic adversaries.", "labels": [], "entities": []}, {"text": "Section 4 presents experiments on POS tagging and discusses how to evaluate cross-domain performance.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.8808450996875763}]}, {"text": "Learning with antagonistic adversaries is superior to the other approaches across 10/12 datasets with an average error reduction of 4% over a supervised baseline.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 113, "end_pos": 128, "type": "METRIC", "confidence": 0.9751552939414978}]}, {"text": "The problem with out-of-vocabulary effects can be illustrated using a small labeled data set: Say we train our model on x 1\u22123 and evaluate it on the fourth data point.", "labels": [], "entities": []}, {"text": "Most discriminate learning algorithms only update parameters when training examples are misclassified.", "labels": [], "entities": []}, {"text": "In this example, a model initialized by zero weights would misclassify x 1 , update the parameter associated with feature x 2 at a fixed rate \u03b1, and the returned model would then classify all data points correctly.", "labels": [], "entities": []}, {"text": "Hence the parameter associated with feature x 3 would never be updated, although this feature is also correlated with class.", "labels": [], "entities": []}, {"text": "If x 2 is missing in our test data (out-of-vocabulary), we end up classifying all data points as negative.", "labels": [], "entities": []}, {"text": "In this case, we would wrongly predict that x 4 is negative.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider part-of-speech (POS) tagging, i.e. the problem of assigning syntactic categories to word tokens in running text.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6301098942756653}, {"text": "assigning syntactic categories to word tokens in running text", "start_pos": 62, "end_pos": 123, "type": "TASK", "confidence": 0.7763836781183878}]}, {"text": "POS tagging accuracy is known to be very sensitive to domain shifts.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7945941686630249}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9048243165016174}]}, {"text": "report a POS tagging accuracy on social media data of 84% using a tagger that acchieves an accuracy of about 97% on newspaper data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.6858338862657547}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9435335993766785}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9991033673286438}]}, {"text": "In the case of social media data, many errors occur due to different spelling and capitalization conventions.", "labels": [], "entities": []}, {"text": "The main source of error, though, is the increased out-of-vocabulary rate, i.e. the many unknown words.", "labels": [], "entities": [{"text": "error", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9644932746887207}]}, {"text": "While POS taggers can often recover the part of speech of a previously unseen word from the context it occurs in, this is harder than for previously seen words.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.8860567212104797}]}, {"text": "We use the LXMLS toolkit 3 as our baseline with the default feature model, but use the PTB tagset rather than the Google tagset) used by default in the LXMLS toolkit.", "labels": [], "entities": [{"text": "LXMLS toolkit 3", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.9113525748252869}, {"text": "PTB tagset", "start_pos": 87, "end_pos": 97, "type": "DATASET", "confidence": 0.927372008562088}]}, {"text": "We use four groups of datasets.", "labels": [], "entities": []}, {"text": "The first group comes from the English Web Treebank (EWT), also used in the Parsing the Web shared task.", "labels": [], "entities": [{"text": "English Web Treebank (EWT)", "start_pos": 31, "end_pos": 57, "type": "DATASET", "confidence": 0.9065466324488322}, {"text": "Parsing the Web shared task", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.8938849329948425}]}, {"text": "We train our tagger on Sections 2-21 of the WSJ data in the Penn-III Treebank (PTB), Ontonotes 4.0 release.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9641708731651306}, {"text": "Penn-III Treebank (PTB)", "start_pos": 60, "end_pos": 83, "type": "DATASET", "confidence": 0.9690791964530945}, {"text": "Ontonotes 4.0 release", "start_pos": 85, "end_pos": 106, "type": "DATASET", "confidence": 0.8922668099403381}]}, {"text": "The EWT contains development and test data for five domains: answers (from Yahoo!), emails (from the Enron corpus), BBC newsgroups, Amazon reviews, and weblogs.", "labels": [], "entities": [{"text": "EWT", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7890051603317261}]}, {"text": "We use the emails development section for development and test on the remaining four test sets.", "labels": [], "entities": []}, {"text": "We also do experiments with additional data from PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.9797256588935852}]}, {"text": "For these experiments we use the 0th even split of the biomedical section (PTB-biomedical) as development data, the 9th split and the chemistry section (PTBchemistry) as test data, and the remaining biomedical data (splits 1-8) as training data.", "labels": [], "entities": []}, {"text": "This data was also used for developing and testing in the.", "labels": [], "entities": []}, {"text": "Our third group of datasets also comes from Ontonotes 4.0.", "labels": [], "entities": [{"text": "Ontonotes 4.0", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8782811462879181}]}, {"text": "We use the Chinese Ontonotes (CHO) data, covering five different domains.", "labels": [], "entities": [{"text": "Chinese Ontonotes (CHO) data", "start_pos": 11, "end_pos": 39, "type": "DATASET", "confidence": 0.782366414864858}]}, {"text": "We use newswire for training data and randomly sampled broadcasted news for development.", "labels": [], "entities": []}, {"text": "Finally we do experiments with the Danish section of the Copenhagen Dependency Treebank (CDT).", "labels": [], "entities": [{"text": "Danish section of the Copenhagen Dependency Treebank (CDT)", "start_pos": 35, "end_pos": 93, "type": "DATASET", "confidence": 0.8977805376052856}]}, {"text": "For CDT we rely on the treebank meta-data and sin-: Results (in %).", "labels": [], "entities": []}, {"text": "gle out the newspaper section as training data and use held-out newspaper data for development.", "labels": [], "entities": [{"text": "newspaper section", "start_pos": 12, "end_pos": 29, "type": "DATASET", "confidence": 0.9686377644538879}]}, {"text": "We observe two characteristics about our datasets: (a) The class distributions are relatively stable across domains.", "labels": [], "entities": []}, {"text": "For CDT, for example, we see almost identical distributions of parts of speech, except literature has more prepositions.", "labels": [], "entities": []}, {"text": "(b) The OOV rate is significantly higher across domains than within domains.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9769388735294342}]}, {"text": "This holds even for the PTB datasets, where the OOV rate is 14.6% on the biomedical test data, but 43.3% on the chemistry test data.", "labels": [], "entities": [{"text": "PTB datasets", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9776866137981415}, {"text": "OOV rate", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9896121621131897}]}, {"text": "These two observations confirm that cross-domain data is primarily biased by covariate shifts.", "labels": [], "entities": []}, {"text": "All learning algorithms do the same number of passes over each training data set.", "labels": [], "entities": []}, {"text": "The number of iterations was set optimizing baseline system performance on development data.", "labels": [], "entities": []}, {"text": "For EWT and CHO, we do 10 passes over the data.", "labels": [], "entities": [{"text": "EWT", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.5487974286079407}]}, {"text": "For PTB, we do 15 passes over the data, and for CDT, we do 25 passes over the data.", "labels": [], "entities": [{"text": "PTB", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.5469799637794495}, {"text": "CDT", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.8743561506271362}]}, {"text": "The deletion rate in adversarial learning was fixed to 0.1% (optimized on the EWT emails data; not optimized on PTB, CHO or CDT).", "labels": [], "entities": [{"text": "EWT emails data", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.96401979525884}, {"text": "PTB, CHO or CDT", "start_pos": 112, "end_pos": 127, "type": "DATASET", "confidence": 0.7276195287704468}]}, {"text": "In L \u221e -regularization, the parameter C was optimized the same way and set to 20.", "labels": [], "entities": []}, {"text": "Results are averages over five runs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results (in %).", "labels": [], "entities": []}]}