{"title": [{"text": "SenseSpotting: Never let your parallel data tie you to an old domain", "labels": [], "entities": []}], "abstractContent": [{"text": "Words often gain new senses in new domains.", "labels": [], "entities": []}, {"text": "Being able to automatically identify , from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.8155516684055328}]}, {"text": "We define a task, SENSESPOTTING, in which we build systems to spot tokens that have new senses in new domain text.", "labels": [], "entities": [{"text": "SENSESPOTTING", "start_pos": 18, "end_pos": 31, "type": "METRIC", "confidence": 0.6979157328605652}]}, {"text": "Instead of difficult and expensive annotation, we build a gold-standard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 163, "end_pos": 180, "type": "TASK", "confidence": 0.7287836968898773}, {"text": "machine translation", "start_pos": 185, "end_pos": 204, "type": "TASK", "confidence": 0.7582482397556305}]}, {"text": "Our system is able to achieve F-measures of as much as 80%, when applied to word types it has never seen before.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.992745041847229}]}, {"text": "Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "As observed, the domain of the text that a word occurs in is a useful signal for performing word sense disambiguation (e.g. in a text about finance, bank is likely to refer to a financial institution while in a text about geography, it is likely to refer to a river bank).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 92, "end_pos": 117, "type": "TASK", "confidence": 0.7005308270454407}]}, {"text": "However, in the classic WSD task, ambiguous word types and a set of possible senses are known in advance.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.935718297958374}]}, {"text": "In this work, we focus on the setting where we observe texts in two different domains and want to identify words in the second text that have a sense that did not appear in the first text, without any lexical knowledge in the new domain.", "labels": [], "entities": []}, {"text": "To  (and is translated as) \"report.\"", "labels": [], "entities": []}, {"text": "However, in moving to a medical or scientific domain, the word gains anew sense: \"ratio\", which simply does not exist in the parliament domain.", "labels": [], "entities": [{"text": "ratio", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9460205435752869}]}, {"text": "Ina science domain, the \"report\" sense exists, but it is dominated about 12:1 by \"ratio.\"", "labels": [], "entities": []}, {"text": "Ina medical domain, the \"report\" sense remains dominant (about 2:1), but the new \"ratio\" sense appears frequently.", "labels": [], "entities": []}, {"text": "In this paper we define anew task that we call SENSESPOTTING.", "labels": [], "entities": [{"text": "SENSESPOTTING", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.6886032223701477}]}, {"text": "The goal of this task is to identify words in anew domain monolingual text that appeared in old domain text but which have anew, previously unseen sense . We operate under the framework of phrase sense disambiguation, in which we take automatically align parallel data in an old domain to generate an initial old-domain sense inventory.", "labels": [], "entities": [{"text": "phrase sense disambiguation", "start_pos": 189, "end_pos": 216, "type": "TASK", "confidence": 0.7183008193969727}]}, {"text": "This sense inventory provides the set of \"known\" word senses in the form of phrasal translations.", "labels": [], "entities": []}, {"text": "Concrete examples are shown in.", "labels": [], "entities": []}, {"text": "One of our key contributions is the development of a rich set of features based on monolingual text that are indicative of new word senses.", "labels": [], "entities": []}, {"text": "This work is driven by an application need.", "labels": [], "entities": []}, {"text": "When machine translation (MT) systems are applied in anew domain, many errors area result of: (1) previously unseen (OOV) source language words, or (2) source language words that appear with anew sense and which require new transla-tions 2.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 5, "end_pos": 29, "type": "TASK", "confidence": 0.8410400211811065}]}, {"text": "Given monolingual text in anew domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques, or active learning.", "labels": [], "entities": [{"text": "dictionary extraction", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.6865486204624176}]}, {"text": "However, previously seen (even frequent) words which require new translations are harder to spot.", "labels": [], "entities": []}, {"text": "Because our motivation is translation, one significant point of departure between our work and prior related work ( \u00a73) is that we focus on word tokens.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9763318300247192}]}, {"text": "That is, we are not interested only in the question of \"has this known word (type) gained anew sense?\", but the much more specific question of \"is this particular (token) occurrence of this known word being used in anew sense?\"", "labels": [], "entities": []}, {"text": "Note that for both the dictionary mining setting and the active learning setting, it is important to consider words in context when acquiring their translations.", "labels": [], "entities": [{"text": "dictionary mining", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.8432717621326447}]}], "datasetContent": [{"text": "Our goal in evaluation is to be able to understand what our approach is realistically capable of.", "labels": [], "entities": []}, {"text": "One challenge is that the distribution of representative words is highly skewed.", "labels": [], "entities": []}, {"text": "We present results in terms of area under the ROC curve (AUC), 5 micro-averaged precision/recall/fmeasure and macro-averaged precision/recall/fmeasure.", "labels": [], "entities": [{"text": "ROC curve (AUC)", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.9473847389221192}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.8692324757575989}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.6823613047599792}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.8513636589050293}, {"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.5287827849388123}]}, {"text": "For macro-averaging, we compute a single confusion matrix overall the test data and determining P/R/F from that matrix.", "labels": [], "entities": [{"text": "P/R/F", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.8510264158248901}]}, {"text": "For microaveraging, we compute a separate confusion matrix for each word type on the French side, compute P/R/F for each of these separately, and then average the results.", "labels": [], "entities": [{"text": "P/R/F", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.8446234941482544}]}, {"text": "(Thus, micro-F is not a function of micro-P and micro-R.)", "labels": [], "entities": []}, {"text": "The AUC and macro-averaged scores give a sense of how well the system is doing on a type-level basis (essentially weighted by type frequency), while the micro-averaged scores give a sense as to how well the system is doing on individual types, not taking into account their frequencies.", "labels": [], "entities": [{"text": "AUC", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.7849372029304504}]}, {"text": "For most of our results, we present standard deviations to help assess significance (\u00b12\u03c3 is roughly a 90% confidence interval).", "labels": [], "entities": [{"text": "significance", "start_pos": 71, "end_pos": 83, "type": "METRIC", "confidence": 0.9852215647697449}]}, {"text": "For our results, in which we use new-domain training data, we compute these results via 16-fold cross validation.", "labels": [], "entities": []}, {"text": "The folds are split across types so the system is never being tested on a word type that it has seen before.", "labels": [], "entities": []}, {"text": "We do this because it more closely resembles our application goals.", "labels": [], "entities": []}, {"text": "We do 16-fold for convenience, because we divide the data into binary folds recursively (thus having a power-of-two is easier), with an attempt to roughly balance the size of the training sets in each fold (this is tricky because of the skewed nature of the data).", "labels": [], "entities": []}, {"text": "This entire 16-fold cross-validation procedure is repeated 10 times and averages and standard deviations are over the 160 replicates.", "labels": [], "entities": []}, {"text": "We evaluate performance using our type-level features only, TYPEONLY, our token-level features only, TOKENONLY, and using both our type and our token level features, ALLFEATURES.", "labels": [], "entities": [{"text": "TYPEONLY", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9834135174751282}, {"text": "TOKENONLY", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.8525061011314392}, {"text": "ALLFEATURES", "start_pos": 166, "end_pos": 177, "type": "METRIC", "confidence": 0.9835680723190308}]}, {"text": "We compare our results with two baselines: RANDOM and CONSTANT.", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.8595640063285828}, {"text": "CONSTANT", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.6659241914749146}]}, {"text": "RANDOM predicts new-sense or not-new-sense randomly and with equal probability.", "labels": [], "entities": []}, {"text": "CONSTANT always predicts new-sense, achieving 100% recall and a macrolevel precision that is equal to the percent of representative words which do have anew sense, modulo cross-validation splits (see).", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9980512857437134}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.8858170509338379}]}, {"text": "Addi-tionally, we compare our results with a type-level oracle, TYPEORACLE.", "labels": [], "entities": [{"text": "TYPEORACLE", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9122349619865417}]}, {"text": "For all tokens of a given word type, the oracle predicts the majority label (new-sense or not-new-sense) for that word type.", "labels": [], "entities": []}, {"text": "These results correspond to an upper bound for the TYPEONLY experiments.", "labels": [], "entities": [{"text": "TYPEONLY", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.7461761832237244}]}], "tableCaptions": [{"text": " Table 2: Basic characteristics of the parallel data.", "labels": [], "entities": []}, {"text": " Table 3: Statistics about representative words and  the size of the development sets. The columns  show: the total amount of parallel development  data (# of sentences and tokens in French), # of  representative types that appear in this corpus, the  corresponding # of tokens, and the percentage of  these tokens that correspond to \"new senses.\"", "labels": [], "entities": []}, {"text": " Table 4: Complete SENSESPOTTING results for all domains. The scores are from cross-validation on  a single domain; in all cases, higher is better. Two standard deviations of performance over the cross- validation are shown in small type. For all domains and metrics, the highest (not necessarily statistically  significant) non-oracle results are bolded.", "labels": [], "entities": [{"text": "SENSESPOTTING", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.5166227221488953}]}, {"text": " Table 5: Feature ablation results for all three corpora. Selection criteria is AUC, but Macro-F is presented  for completeness. Feature selection is run independently on each of the three datasets. The features  toward the bottom were the first selected.", "labels": [], "entities": [{"text": "AUC", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9981368780136108}]}, {"text": " Table 6: Cross-domain test results on the SENS- ESPOTTING task. Two standard deviations are  shown in small type. Only AUC, Macro-F and  Micro-F are shown for brevity.", "labels": [], "entities": [{"text": "SENS- ESPOTTING task", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.8844042867422104}, {"text": "AUC", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.680585503578186}]}, {"text": " Table 7: Cross-validation results on the MOST- FREQSENSECHANGE task. Two standard devia- tions are shown in small type.", "labels": [], "entities": [{"text": "MOST", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.7647684216499329}, {"text": "FREQSENSECHANGE", "start_pos": 48, "end_pos": 63, "type": "METRIC", "confidence": 0.6715013980865479}]}]}