{"title": [{"text": "A joint model of word segmentation and phonological variation for English word-final /t/-deletion", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7328454405069351}, {"text": "English word-final /t/-deletion", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.5698056668043137}]}], "abstractContent": [{"text": "Word-final /t/-deletion refers to a common phenomenon in spoken English where words such as /wEst/ \"west\" are pronounced as [wEs] \"wes\" in certain contexts.", "labels": [], "entities": [{"text": "Word-final /t/-deletion", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5646116018295289}]}, {"text": "Phonological variation like this is common in naturally occurring speech.", "labels": [], "entities": []}, {"text": "Current computational models of unsu-pervised word segmentation usually assume idealized input that is devoid of these kinds of variation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7292220443487167}]}, {"text": "We extend a non-parametric model of word segmenta-tion by adding phonological rules that map from underlying forms to surface forms to produce a mathematically well-defined joint model as a first step towards handling variation and segmentation in a single model.", "labels": [], "entities": []}, {"text": "We analyse how our model handles /t/-deletion on a large corpus of transcribed speech, and show that the joint model can perform word segmentation and recover underlying /t/s.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.7196430265903473}]}, {"text": "We find that Bi-gram dependencies are important for performing well on real data and for learning appropriate deletion probabilities for different contexts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computational models of word segmentation try to solve one of the first problems language learners have to face: breaking an unsegmented stream of sound segments into individual words.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7245412021875381}]}, {"text": "Currently, most such models assume that the input consists of sequences of phonemes with no pronunciation variation across different occurrences of the same word type.", "labels": [], "entities": []}, {"text": "In this paper we describe an extension of the Bayesian models of that incorporates phonological rules to \"explain away\" surface variation.", "labels": [], "entities": []}, {"text": "As a concrete example, we focus on word-final /t/-deletion in English, although our approach is not limited to this case.", "labels": [], "entities": [{"text": "word-final /t/-deletion", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.6099386811256409}]}, {"text": "We choose /t/-deletion because it is a very common and well-studied phenomenon (see Coetzee (2004, Chapter 5) fora review) and segmental deletion is an interesting test-case for our architecture.", "labels": [], "entities": []}, {"text": "Recent work has found that /t/-deletion (among other things) is indeed common in child-directed speech (CDS) and, importantly, that its distribution is similar to that in adult-directed speech (ADS) (.", "labels": [], "entities": []}, {"text": "This justifies our using ADS to evaluate our model, as discussed below.", "labels": [], "entities": []}, {"text": "Our experiments are consistent with longstanding and recent findings in linguistics, in particular that /t/-deletion heavily depends on the immediate context and that models ignoring context work poorly on real data.", "labels": [], "entities": []}, {"text": "We also examine how well our models identify the probability of /t/-deletion in different contexts.", "labels": [], "entities": []}, {"text": "We find that models that capture bigram dependencies between underlying forms provide considerably more accurate estimates of those probabilities than corresponding unigram or \"bag of words\" models of underlying forms.", "labels": [], "entities": []}, {"text": "In section 2 we discuss related work on handling variation in computational models and on /t/-deletion.", "labels": [], "entities": []}, {"text": "Section 3 describes our computational model and section 4 discusses its performance for recovering deleted /t/s.", "labels": [], "entities": [{"text": "recovering deleted /t/s", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.8106325368086497}]}, {"text": "We look at both a situation where word boundaries are pre-specified and only inference for underlying forms has to be performed; and the problem of jointly finding the word boundaries and recovering deleted underlying /t/s.", "labels": [], "entities": []}, {"text": "Section 5 discusses our findings, and section 6 concludes with directions for further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test our Gibbs sampling inference procedure, we ran it on artificial data generated according to the model itself.", "labels": [], "entities": []}, {"text": "If our inference procedure fails to recover the underlying /t/s accurately in this setting, we should not expect it to work well on actual data.", "labels": [], "entities": []}, {"text": "We generated our artificial data as follows.", "labels": [], "entities": []}, {"text": "We transformed the sequence of canonical pronunciations in the Buckeye corpus (which we take to be underlying forms here) by randomly deleting final /t/s using empirical probabilities as shown in to generate a sequence of artificial surface forms that serve as input to our models.", "labels": [], "entities": [{"text": "Buckeye corpus", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.9654219150543213}]}, {"text": "We: F-score of /t/-recovery with known word boundaries on artificial data, each condition tested on data that corresponds to the assumption, averaged over two runs (standard errors less than 2% except (+) = 3.68%)).: /t/-recovery F-scores when performing joint word segmention in the left-right setting, averaged over two runs (standard errors less than 2%).", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9985910058021545}, {"text": "joint word segmention", "start_pos": 255, "end_pos": 276, "type": "TASK", "confidence": 0.6970857183138529}]}, {"text": "See for the corresponding segmentation F-scores.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.7709900736808777}]}, {"text": "Finally, we are also interested to learn how well we can do word segmentation and underlying /t/-recovery jointly.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7384368181228638}, {"text": "underlying /t/-recovery", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.6770226001739502}]}, {"text": "Again, we look at both the LEARN-\u03c1 and GOLD-\u03c1 conditions but focus on the left-right setting as this worked best in the experiments above.", "labels": [], "entities": [{"text": "LEARN-\u03c1", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9955376386642456}, {"text": "GOLD-\u03c1", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9319355487823486}]}, {"text": "For these experiments, we perform simulated annealing throughout the initial 2000 iterations, gradually cooling the temperature from 5 to 1, following the observation by  that without annealing, the Bigram model gets stuck in sub-optimal parts of the solution space early on.", "labels": [], "entities": []}, {"text": "During the annealing stage, we prevent the model from performing inference for underlying /t/s so that the annealing stage can be seen as an elaborate initialisation scheme, and we perform joint inference for the remaining 500 iterations, evaluating on the last sample and averaging over two runs.", "labels": [], "entities": []}, {"text": "As neither the Unigram nor the Bigram model performs \"perfect\" word segmentation, we expect to see a degradation in /t/-recovery performance and this is what we find indeed.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7554179728031158}]}, {"text": "To give an impression of the impact of /t/-deletion, we also report numbers for running only the segmentation model on the Buckeye data with no deleted /t/s and on the data with deleted /t/s.", "labels": [], "entities": [{"text": "Buckeye data", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.9836589097976685}]}, {"text": "The /t/-recovery scores are given in and segmentation scores in.", "labels": [], "entities": [{"text": "recovery", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9743258953094482}, {"text": "segmentation", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.9542936682701111}]}, {"text": "Again the Unigram model's /t/-recovery score degrades dramatically in the LEARN-\u03c1 condition.", "labels": [], "entities": [{"text": "recovery score", "start_pos": 30, "end_pos": 44, "type": "METRIC", "confidence": 0.9725496470928192}, {"text": "LEARN-\u03c1", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9946032166481018}]}, {"text": "Looking at the segmentation performance this isn't too surprising: the Unigram model's poorer token Fscore, the standard measure of segmentation performance on a word token level, suggests that it misses many more boundaries than the Bigram model to begin with and, consequently, can't recover any potential underlying /t/s at these boundaries.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.7765443325042725}]}, {"text": "Also note that in the GOLD-\u03c1 condition, our joint Bigram model performs almost as well on data with /t/-deletions as the word segmentation model on data that includes no variation at all.", "labels": [], "entities": [{"text": "GOLD-\u03c1", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.6259278655052185}, {"text": "word segmentation", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.6960342973470688}]}, {"text": "The generally worse performance of handling variation as measured by /t/-recovery F-score when performing joint segmentation is consistent with the finding of who report considerable performance drops for their phonological learner when working with induced boundaries (note, however, that their model does not perform joint inference, rather the induced boundaries are given to their phonological learner as groundtruth).", "labels": [], "entities": [{"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.5279210209846497}, {"text": "joint segmentation", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7113515883684158}]}], "tableCaptions": [{"text": " Table 2: F-score of recovered /t/s with known  word boundaries on real data for the three differ- ent context settings, averaged over two runs (all  standard errors below 2%). Note how the Uni- gram model always suffers in the LEARN-\u03c1 condi- tion whereas the Bigram model's performance is  actually best for LEARN-\u03c1 in the left-right setting.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9987967014312744}]}, {"text": " Table 3: Inferred rule-probabilities for different  contexts in the left-right setting from one of the  runs. \"C C\" stands for the context where the  deleted /t/ is preceded and followed by a conso- nant, \"V $\" stands for the context where it is pre- ceded by a vowel and followed by the utterance  boundary. Note how the Unigram model severely  under-estimates and the Bigram model slightly  over-estimates the probabilities.", "labels": [], "entities": []}, {"text": " Table 4: F-score of /t/-recovery with known word  boundaries on artificial data, each condition tested  on data that corresponds to the assumption, aver- aged over two runs (standard errors less than 2%  except (+) = 3.68%)).", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9984124898910522}]}, {"text": " Table 5: /t/-recovery F-scores when performing  joint word segmention in the left-right setting, av- eraged over two runs (standard errors less than  2%). See", "labels": [], "entities": [{"text": "recovery", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9795377850532532}, {"text": "F-scores", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.5694650411605835}, {"text": "joint word segmention", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.6622491975625356}]}, {"text": " Table 6: Word segmentation F-scores for the /t/- recovery F-scores in", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7048918008804321}, {"text": "F-scores", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.683399498462677}]}]}