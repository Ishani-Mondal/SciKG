{"title": [{"text": "The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing", "labels": [], "entities": [{"text": "Phrase-Structure Parsing", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.6471606343984604}]}], "abstractContent": [{"text": "Higher-order dependency features are known to improve dependency parser accuracy.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7034582495689392}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.915107250213623}]}, {"text": "We investigate the incorporation of such features into a cube decoding phrase-structure parser.", "labels": [], "entities": []}, {"text": "We find considerable gains inaccuracy on the range of standard metrics.", "labels": [], "entities": []}, {"text": "What is especially interesting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ).", "labels": [], "entities": [{"text": "dependency recovery", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7941989302635193}, {"text": "WSJ", "start_pos": 144, "end_pos": 147, "type": "DATASET", "confidence": 0.8597208857536316}]}, {"text": "This suggests that higher-order dependency features are not simply over-fitting the training material.", "labels": [], "entities": []}], "introductionContent": [{"text": "Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.", "labels": [], "entities": []}, {"text": "The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.8348453640937805}, {"text": "parse", "start_pos": 104, "end_pos": 109, "type": "TASK", "confidence": 0.9615974426269531}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.783216118812561}]}, {"text": "This finding suggests that the same benefits might be observed in phrase-structure parsing.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.7853638529777527}]}, {"text": "But, this is not necessarily implied.", "labels": [], "entities": []}, {"text": "Phrasestructure parsers are generally stronger than dependency parsers (, and make use of more kinds of information.", "labels": [], "entities": [{"text": "Phrasestructure parsers", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8598313331604004}]}, {"text": "So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case.", "labels": [], "entities": []}, {"text": "To investigate this issue, we experiment using cube decoding algorithm.", "labels": [], "entities": []}, {"text": "This algorithm allows structured prediction with nonlocal features, as discussed in \u00a72.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.6236438155174255}]}, {"text": "strategy of expanding the phrase-structure parser's dynamic program to incorporate head-modifier dependency information would not scale to the complex kinds of dependencies we will consider.", "labels": [], "entities": []}, {"text": "Using Huang's algorithm, we can indeed incorporate arbitrary types of dependency feature, using a single, simple dynamic program.", "labels": [], "entities": []}, {"text": "Compared to the baseline, non-local feature set of and, we find that higher-order dependencies do in fact tend to improve performance significantly on both dependency and constituency accuracy metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.7878002524375916}]}, {"text": "Our most interesting finding, though, is that higher-order dependency features show a consistent and unambiguous contribution to the dependency accuracy, both labelled and unlabelled, of our phrase-structure parsers on outof-domain tests (which means, here, trained on WSJ, but tested on BROWN).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9774568676948547}, {"text": "WSJ", "start_pos": 269, "end_pos": 272, "type": "DATASET", "confidence": 0.9037599563598633}, {"text": "BROWN", "start_pos": 288, "end_pos": 293, "type": "METRIC", "confidence": 0.48547983169555664}]}, {"text": "In fact, the gains are even stronger on out-of-domain tests than on indomain tests.", "labels": [], "entities": []}, {"text": "One might have thought that higherorder dependencies, being rather specific by nature, would tend to pick out only very rare events, and so only serve to over-fit the training material, but this is not what we find.", "labels": [], "entities": []}, {"text": "We speculate as to what this might mean in \u00a75.2.", "labels": [], "entities": []}, {"text": "The cube decoding paradigm requires a firststage parser to prune the output space.", "labels": [], "entities": []}, {"text": "For this, we use the generative parser of.", "labels": [], "entities": [{"text": "generative parser", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.9063821136951447}]}, {"text": "We can use this parser's model score as a feature in our discriminative model at no additional cost.", "labels": [], "entities": []}, {"text": "However, doing so conflates the contribution to accuracy of the generative model, on the one hand, and the discriminatively trained, hand-written, features, on the other.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9991557598114014}, {"text": "generative", "start_pos": 64, "end_pos": 74, "type": "TASK", "confidence": 0.9678597450256348}]}, {"text": "Future systems might use the same or a similar feature set to ours, but in an architecture that does not include any generative parser.", "labels": [], "entities": []}, {"text": "On the other hand, some systems might indeed incorporate this generative model's score.", "labels": [], "entities": []}, {"text": "So, we need to know exactly what the generative model is contributing to the accuracy of a generative-discriminative model combination.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9989481568336487}]}, {"text": "Thus, we conduct experiments in sets: in some cases the generative model score is used, and in others it is not used.", "labels": [], "entities": []}, {"text": "Compared to the faster and more psychologically plausible shift-reduce parsers, cube decoding is a computationally expensive method.", "labels": [], "entities": []}, {"text": "But, cube decoding provides a relatively exact environment with which to compare different feature sets, has close connections with modern phrasebased machine translation methods, and produces very accurate parsers.", "labels": [], "entities": [{"text": "phrasebased machine translation", "start_pos": 139, "end_pos": 170, "type": "TASK", "confidence": 0.6301502088705698}]}, {"text": "In some cases, one might want to use a slower, but more accurate, parser during the training stage of a semi-supervised parser training strategy.", "labels": [], "entities": []}, {"text": "For example, have shown that a fast parser ( can be profitably trained from the output of a slower but more accurate one (), in a strategy they call uptraining.", "labels": [], "entities": []}, {"text": "We make the source code for these experiments available.", "labels": [], "entities": []}, {"text": "2 Phrase-Structure Parsing with Non-Local Features", "labels": [], "entities": [{"text": "Phrase-Structure Parsing", "start_pos": 2, "end_pos": 26, "type": "TASK", "confidence": 0.7945894598960876}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance of the various models in cube decoding experiments, on the WSJ test set (in- domain) and the BROWN test set (out-of-domain). G abbreviates generative, D abbreviates discrim- inative, and G+D a combination. Some cells are empty because \u03a6 deps features are only sensitive to  unlabelled dependencies. Best results in D and G+D conditions appear in bold face.", "labels": [], "entities": [{"text": "WSJ test set", "start_pos": 81, "end_pos": 93, "type": "DATASET", "confidence": 0.9720214207967123}, {"text": "BROWN test set", "start_pos": 115, "end_pos": 129, "type": "DATASET", "confidence": 0.7777062257130941}]}, {"text": " Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9277624487876892}]}]}