{"title": [], "abstractContent": [{"text": "Beam search incremental parsers are accurate , but not as fast as they could be.", "labels": [], "entities": [{"text": "Beam search incremental parsers", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.677898496389389}]}, {"text": "We demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in O(n 2), rather than linear time, because each state-transition is actually implemented as an O(n) operation.", "labels": [], "entities": []}, {"text": "We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in areal linear-time algorithm, which is verified empirically.", "labels": [], "entities": []}, {"text": "We further improve parsing speed by sharing feature-extraction and dot-product across beam items.", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.9850834608078003}]}, {"text": "Practically, our methods combined offer a speedup of \u223c2x over strong baselines on Penn Tree-bank sentences, and are orders of magnitude faster on much longer sentences.", "labels": [], "entities": [{"text": "Penn Tree-bank sentences", "start_pos": 82, "end_pos": 106, "type": "DATASET", "confidence": 0.9836024443308512}]}], "introductionContent": [{"text": "Beam search incremental parsers) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars).", "labels": [], "entities": []}, {"text": "In terms of purning strategies, they can be broadly divided into two categories: the first group) uses soft (aka probabilistic) beams borrowed from bottom-up parsers) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation which guarantee (as they claim) a linear runtime O(kn) where k is the beam width.", "labels": [], "entities": [{"text": "O(kn)", "start_pos": 365, "end_pos": 370, "type": "METRIC", "confidence": 0.9000727236270905}]}, {"text": "However, we will demonstrate below that, contrary to popular * Supported in part by DARPA FA8750-13-2-0041 belief, inmost standard implementations their actual runtime is in fact O(kn 2 ) rather than linear.", "labels": [], "entities": [{"text": "DARPA FA8750-13-2-0041 belief", "start_pos": 84, "end_pos": 113, "type": "DATASET", "confidence": 0.7466636697451273}, {"text": "O", "start_pos": 179, "end_pos": 180, "type": "METRIC", "confidence": 0.9689434766769409}]}, {"text": "Although this argument in general also applies to dynamic programming (DP) parsers, 1 in this paper we only focus on the standard, non-dynamic programming approach since it is arguably still the dominant practice (e.g. it is easier with the popular arc-eager parser with a rich feature set () and it benefits more from our improved algorithms.", "labels": [], "entities": [{"text": "dynamic programming (DP) parsers", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.688299298286438}]}, {"text": "The dependence on the beam-size k is because one needs to do k-times the number of basic operations (feature-extractions, dot-products, and statetransitions) relative to a greedy parser (.", "labels": [], "entities": []}, {"text": "Note that in abeam setting, the same state can expand to several new states in the next step, which is usually achieved by copying the state prior to making a transition, whereas greedy search only stores one state which is modified in-place.", "labels": [], "entities": []}, {"text": "Copying amounts to a large fraction of the slowdown of beam-based with respect to greedy parsers.", "labels": [], "entities": [{"text": "Copying", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9053938388824463}]}, {"text": "Copying is expensive, because the state keeps track of (a) a stack and (b) the set of dependency-arcs added so far.", "labels": [], "entities": []}, {"text": "Both the arc-set and the stack can grow to O(n) size in the worst-case, making the state-copy (and hence state-transition) an O(n) operation.", "labels": [], "entities": []}, {"text": "Thus, beam search implementations that copy the entire state are in fact quadratic O(kn 2 ) and not linear, with a slowdown factor of O(kn) with respect to greedy parsers, which is confirmed empirically in.", "labels": [], "entities": [{"text": "beam search", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.8165432810783386}, {"text": "O", "start_pos": 83, "end_pos": 84, "type": "METRIC", "confidence": 0.9496825933456421}, {"text": "O", "start_pos": 134, "end_pos": 135, "type": "METRIC", "confidence": 0.9921259880065918}]}, {"text": "We present away of decreasing the O(n) transition cost to O(1) achieving strictly linear-time parsing, using a data structure of Tree-Structured Stack (TSS) that is inspired by but simpler than the graph-structured stack (GSS) of used in dynamic programming.", "labels": [], "entities": []}, {"text": "On average Treebank sentences, the TSS input:.", "labels": [], "entities": [{"text": "TSS", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.7209347486495972}]}, {"text": "The stack S is a list of heads, j is the index of the token at the front of the buffer, and is the step number (beam index).", "labels": [], "entities": []}, {"text": "A is the arc-set of dependency arcs accumulated so far, which we will get rid of in Section 4.1.", "labels": [], "entities": []}, {"text": "version, being linear time, leads to a speedup of 2x\u223c2.7x over the naive implementation, and about 1.3x\u223c1.7x over the optimized baseline presented in Section 5.", "labels": [], "entities": []}, {"text": "Having achieved efficient state-transitions, we turn to feature extraction and dot products (Section 6).", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7500969469547272}]}, {"text": "We present a simple scheme of sharing repeated scoring operations across different beam items, resulting in an additional 7 to 25% speed increase.", "labels": [], "entities": [{"text": "speed", "start_pos": 131, "end_pos": 136, "type": "METRIC", "confidence": 0.9567616581916809}]}, {"text": "On Treebank sentences, the methods combined lead to a speedup of \u223c2x over strong baselines (\u223c10x over naive ones), and on longer sentences they are orders of magnitude faster.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implemented beam-based parsers using the traditional approach as well as with our proposed extension and compared their runtime.", "labels": [], "entities": []}, {"text": "The first experiment highlights the non-linear behavior of the standard implementation, compared to the linear behavior of the TSS method.", "labels": [], "entities": []}, {"text": "As parsing time is dominated by score computation, the effect is too small to be measured on natural language sentences, but it is noticeable for longer sentences.", "labels": [], "entities": [{"text": "parsing", "start_pos": 3, "end_pos": 10, "type": "TASK", "confidence": 0.9814722537994385}]}, {"text": "plots the runtime for synthetic examples with lengths ranging from 50 to 1000 tokens, which are generated by concatenating sentences from Sections 22-24 of Penn Treebank (PTB), and demonstrates the non-linear behavior (dataset included).", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 156, "end_pos": 175, "type": "DATASET", "confidence": 0.9720235109329224}]}, {"text": "We argue parsing longer sentences is by itself an interesting and potentially important problem (e.g. for other languages such as Arabic and Chinese where word or sentence boundaries are vague, and for parsing beyond sentence-level, e.g. discourse parsing or parsing with inter-sentence dependencies).", "labels": [], "entities": [{"text": "parsing longer sentences", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.9160819053649902}, {"text": "discourse parsing", "start_pos": 238, "end_pos": 255, "type": "TASK", "confidence": 0.6940186470746994}]}, {"text": "Our next set of experiments compares the actual speedup observed on English sentences.", "labels": [], "entities": [{"text": "speedup", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9612915515899658}]}, {"text": "shows the speed of the parsers (sentences/second) with the various proposed optimization techniques.", "labels": [], "entities": []}, {"text": "We first train our parsers on Sections 02-21 of PTB, using Section 22 as the test set.", "labels": [], "entities": [{"text": "PTB", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.8568041920661926}]}, {"text": "The accuracies of all our parsers are at the state-ofthe-art level.", "labels": [], "entities": []}, {"text": "The final speedups are up to 10x against naive baselines and \u223c2x against the lazytransitions baselines.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parsing speeds for the different tech- niques measured in sentences/sec (beam size 8).  All parsers are implemented in Python, with dot- products in C. ArcS/ArcE denotes arc-standard  vs. arc-eager, L/U labeled (stanford deps, 49 la- bels) vs. unlabeled parsing. ArcS use feature set  of Huang and Sagae (2010) (50 templates), and ArcE  that of Zhang and Nivre (2011) (72 templates).", "labels": [], "entities": []}]}