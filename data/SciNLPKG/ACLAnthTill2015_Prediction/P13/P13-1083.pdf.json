{"title": [{"text": "Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation", "labels": [], "entities": [{"text": "Part-of-Speech Induction", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7865820229053497}, {"text": "Statistical Machine Translation", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.8159011403719584}]}], "abstractContent": [{"text": "This paper proposes a nonparametric Bayesian method for inducing Part-of-Speech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 141, "end_pos": 178, "type": "TASK", "confidence": 0.7930991997321447}]}, {"text": "In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model).", "labels": [], "entities": []}, {"text": "Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forest-to-string SMT system.", "labels": [], "entities": [{"text": "Japanese-to-English translation", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.6587868928909302}, {"text": "NTCIR-9 data", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.9874005913734436}, {"text": "SMT", "start_pos": 174, "end_pos": 177, "type": "TASK", "confidence": 0.9379114508628845}]}, {"text": "Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.997346043586731}]}], "introductionContent": [{"text": "In recent years, syntax-based SMT has made promising progress by employing either dependency parsing or constituency parsing (;;) on the source side, the target side, or both.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8357335925102234}, {"text": "dependency parsing", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.848061203956604}, {"text": "constituency parsing", "start_pos": 104, "end_pos": 124, "type": "TASK", "confidence": 0.7455112636089325}]}, {"text": "However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.8916437923908234}, {"text": "constituency parsing", "start_pos": 194, "end_pos": 214, "type": "TASK", "confidence": 0.7990967333316803}]}, {"text": "Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9959002137184143}]}, {"text": "The Japanese noun \"\" in Japanese POS: Japanese POS:", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our proposed models under the NTCIR-9 Japanese-to-English patent translation task), consisting of approximately 3.2 million bilingual sentences.", "labels": [], "entities": [{"text": "NTCIR-9 Japanese-to-English patent translation task", "start_pos": 40, "end_pos": 91, "type": "TASK", "confidence": 0.8338180422782898}]}, {"text": "Both the development data and the test data consist of 2,000 sentences.", "labels": [], "entities": []}, {"text": "We also used the NTCIR-7 development data consisting of 2,741 sentences for development testing purposes.", "labels": [], "entities": [{"text": "NTCIR-7 development data", "start_pos": 17, "end_pos": 41, "type": "DATASET", "confidence": 0.9526537855466207}]}, {"text": "We evaluated our bilingual infinite tree model for POS induction using an in-house developed syntax-based forest-to-string SMT system.", "labels": [], "entities": [{"text": "POS induction", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9526483118534088}, {"text": "SMT", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.865006148815155}]}, {"text": "In the training process, the following steps are performed sequentially: preprocessing, inducing a POS tagset fora source language, training a POS tagger and a dependency parser, and training a forest-to-string MT model.", "labels": [], "entities": []}, {"text": "In, REF s are at least comparable to, or better than, IN Ds except for M ono.", "labels": [], "entities": [{"text": "REF", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9683494567871094}, {"text": "IN Ds", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.8400900661945343}]}, {"text": "This shows that REF achieves better performance by preserving the clues from the original POS tagset.", "labels": [], "entities": [{"text": "REF", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.7559418678283691}, {"text": "POS tagset", "start_pos": 90, "end_pos": 100, "type": "DATASET", "confidence": 0.8506188690662384}]}, {"text": "However, REF may suffer sever overfitting problem for M ono since no bilingual information was incorporated.", "labels": [], "entities": [{"text": "REF", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.666957437992096}]}, {"text": "Further, when the full-level IPA POS tags were used in BS, the system achieved a 27.49% BLEU score, which is worse than the result using the second-level IPA POS tags.", "labels": [], "entities": [{"text": "BS", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.6966109275817871}, {"text": "BLEU score", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9858149886131287}]}, {"text": "This means that manual refinement without bilingual information may also cause an overfitting problem in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9865707755088806}]}, {"text": "shows the number of the IPA POS tags used in the experiments and the POS tags induced by the proposed models.", "labels": [], "entities": []}, {"text": "This table shows that each induced tagset contains more POS tags than the IPA POS tagset.", "labels": [], "entities": []}, {"text": "In the experimental data, some of Japanese verbs correspond to genuine English verbs, some are nominalized, and others correspond to English past participle verbs or present participle verbs which modify other words.", "labels": [], "entities": []}, {"text": "Respective examples are \"I use a card.\", \"U sing the index is faster.\", and \"I explain using an example.\", where all the underlined words correspond to the same Japanese word, \"\", whose IPA POS tag is a verb.", "labels": [], "entities": []}, {"text": "Ind in REF generated the POS tagset where the three types are assigned to separate POS groups.", "labels": [], "entities": [{"text": "POS tagset", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8014208674430847}]}], "tableCaptions": [{"text": " Table 1: Performance on Japanese-to-English  Translation Measured by BLEU (%)", "labels": [], "entities": [{"text": "Japanese-to-English  Translation", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.6241999268531799}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9973641037940979}]}, {"text": " Table 2: The Number of POS Tags", "labels": [], "entities": []}, {"text": " Table 3: Tagging and Dependency Accuracy (%)", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9822412133216858}, {"text": "Accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.8581134080886841}]}]}