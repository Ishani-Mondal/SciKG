{"title": [{"text": "BRAINSUP: Brainstorming Support for Creative Sentence Generation", "labels": [], "entities": [{"text": "BRAINSUP", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9366459846496582}]}], "abstractContent": [{"text": "We present BRAINSUP, an extensible framework for the generation of creative sentences in which users are able to force several words to appear in the sentences and to control the generation process across several semantic dimensions, namely emotions, colors, domain related-ness and phonetic properties.", "labels": [], "entities": [{"text": "BRAINSUP", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9912394881248474}]}, {"text": "We evaluate its performance on a creative sentence generation task, showing its capability of generating well-formed, catchy and effective sentences that have all the good qualities of slogans produced by human copywriters .", "labels": [], "entities": [{"text": "sentence generation task", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.8112728198369344}]}], "introductionContent": [{"text": "A variety of real-world scenarios involve talented and knowledgable people in a time-consuming process to write creative, original sentences generated according to well-defined requisites.", "labels": [], "entities": []}, {"text": "For instance, to advertise anew product it could be desirable to have its name appearing in a punchy sentence together with some keywords relevant for marketing, e.g. \"fresh\", or \"thirst\" for the advertisement of a drink.", "labels": [], "entities": []}, {"text": "Besides, it could be interesting to characterize the sentence with respect to a specific color, like \"blue\" to convey the idea of freshness, or to a color more related to the brand of the company, e.g. \"red\" fora new Ferrari.", "labels": [], "entities": []}, {"text": "Moreover, making the slogan evoke \"joy\" or \"satisfaction\" could make the advertisement even more catchy for customers.", "labels": [], "entities": []}, {"text": "On the other hand, there are many examples of provocative slogans in which copywriters try to impress their readers by suscitating strong negative feelings, as in the case of antismoke campaigns (e.g., \"there are cooler ways to die than smoking\" or \"cancer cures smoking\"), or the famous beer motto \"Guinness is not good for you\".", "labels": [], "entities": []}, {"text": "As another scenario, creative sentence generation is also a useful teaching device.", "labels": [], "entities": [{"text": "creative sentence generation", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.6664588650067648}]}, {"text": "For example, the keyword or linkword method used for second language learning links the translation of a foreign (target) word to one or more keywords in the native language which are phonologically or lexically similar to the target word ().", "labels": [], "entities": []}, {"text": "To illustrate, for teaching the Italian word \"tenda\", which means \"curtain\" in English, the learners are asked to imagine \"rubbing a tender part of their leg with a curtain\".", "labels": [], "entities": []}, {"text": "These words should co-occur in the same sentence, but constructing such sentences by hand can be a difficult and very time-consuming process.", "labels": [], "entities": []}, {"text": "\u00a8, who attempted to automate the process, conclude that the inability to retrieve from the web a good sentence for all cases is a major bottleneck.", "labels": [], "entities": []}, {"text": "Although state of the art computational models of creativity often produce remarkable results, e.g.,,,, just to name a few, to our best knowledge there is no attempt to develop an unified framework for the generation of creative sentences in which users can control all the variables involved in the creative process to achieve the desired effect.", "labels": [], "entities": []}, {"text": "In this paper, we advocate the use of syntactic information to generate creative utterances by describing a methodology that accounts for lexical and phonetic constraints and multiple semantic dimensions at the same time.", "labels": [], "entities": []}, {"text": "We present BRAIN-SUP, an extensible framework for creative sentence generation in which users can control all the parameters of the creative process, thus generating sentences that can be used for practical applications.", "labels": [], "entities": [{"text": "BRAIN-SUP", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9895718097686768}, {"text": "creative sentence generation", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.7496042648951212}]}, {"text": "First, users can define a set of keywords which must appear in the final sentence.", "labels": [], "entities": []}, {"text": "Second, they can slant the output towards a spe-  cific emotion, color or domain.", "labels": [], "entities": []}, {"text": "At the same time, they can require a sentence to include desired phonetic properties, such as rhymes, alliteration or plosives.", "labels": [], "entities": []}, {"text": "The combination of these features allows for the generation of potentially catchy and memorable sentences by establishing connections between linguistic, emotional (), echoic and visual) memory, as exemplified by the system outputs showcased in.", "labels": [], "entities": []}, {"text": "Other creative dimensions can easily be plugged in, due to the inherently modular structure of the system.", "labels": [], "entities": []}, {"text": "BRAINSUP supports the creative process by greedily exploring a huge solution space to produce completely novel utterances responding to user requisites.", "labels": [], "entities": [{"text": "BRAINSUP", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.7884086966514587}]}, {"text": "It exploits syntactic constraints to dramatically cut the size of the search space, thus making it possible to focus on the creative aspects of sentence generation.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.7201693207025528}]}], "datasetContent": [{"text": "We evaluated our model on a creative sentence generation task.", "labels": [], "entities": [{"text": "sentence generation task", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.8206089933713278}]}, {"text": "The objective of the evaluation is twofold: we wanted to demonstrate 1) the effectiveness of our approach for creative sentence generation, in general, and 2) the potential of BRAIN-SUP to support the brainstorming process behind slogan generation.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7502543330192566}, {"text": "BRAIN-SUP", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.9323386549949646}, {"text": "slogan generation", "start_pos": 230, "end_pos": 247, "type": "TASK", "confidence": 0.8159974813461304}]}, {"text": "To this end, the annotation template included one question asking the annotators to rate the quality of the generated sentences as slogans.", "labels": [], "entities": []}, {"text": "Five experienced annotators were asked to rate 432 creative sentences according to the following criteria, namely: 1) Catchiness: is the sentence attractive, catchy or memorable?", "labels": [], "entities": [{"text": "Catchiness", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9857571721076965}]}, {"text": "In these last two cases, the annotators were instructed to select the middle option only in cases where the gap with a correct/successful sentence could be filled just by performing minor editing.", "labels": [], "entities": []}, {"text": "The annotation form had no default values, and the annotators did not know how the evaluated sentences were generated, or whether they were the outcome of one or more systems.", "labels": [], "entities": []}, {"text": "We started by collecting slogans from an online repository of slogans 5 . Then, we randomly selected a subset of these slogans and for each of them we generated an input specification U for the system.", "labels": [], "entities": []}, {"text": "We used the commercial domain of the advertised product as the target domain d.", "labels": [], "entities": []}, {"text": "Two or three content words appearing in each slogan were randomly selected as the target words t.", "labels": [], "entities": []}, {"text": "We did so to simulate the brainstorming phase behind the slogan generation process, where copywriters start with a set of relevant keywords to come up with a catchy slogan.", "labels": [], "entities": [{"text": "slogan generation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7948589622974396}]}, {"text": "In all cases, we set the target emotion to \"positive\" as we could not establish a generally valid criteria to associate a specific emotion to a product.", "labels": [], "entities": []}, {"text": "Concerning chromatic slanting, for target domains having a strong chromatic correlation we allowed the system to slant the generated sentences accordingly.", "labels": [], "entities": []}, {"text": "In the other cases, a random color association was selected.", "labels": [], "entities": []}, {"text": "In this manner, we produced 10 tuples t, d, c, e, p.", "labels": [], "entities": []}, {"text": "Then, from each tuple we produced 5 complete user specifications by enabling or disabling different feature function combinations . The four combinations of features are: base: Target-word scorer + N-gram likelihood + Dependency likelihood + Variety scorer + Unusual-words scorer + Semantic cohesion; base+D: all the scorers in base + Domain relatedness; base+D+C: all the scorers in base+D + Chromatic connotation; base+D+E: all the scorers in base+D + Emotional connotation; base+D+P: all the scorers in base+D + Phonetic features.", "labels": [], "entities": [{"text": "N-gram likelihood + Dependency likelihood", "start_pos": 198, "end_pos": 239, "type": "METRIC", "confidence": 0.6691683888435364}]}, {"text": "For each of the resulting 50 input configurations, we generated up to 10 creative sentences.", "labels": [], "entities": []}, {"text": "As the system could not generate exactly 10 solutions in all the cases, we ended up with a set of 432 items to annotate.", "labels": [], "entities": []}, {"text": "The weights of the feature functions were set heuristically, due to the lack of an annotated dataset suitable to learn an opti-: Majority classes (%) for the five dimensions of the annotation.", "labels": [], "entities": []}, {"text": "We started by assigning the highest weight to the Target Word scorer (i.e., 1.0), followed by the Variety and Unusual Word scorers (0.99), the Phonetic Features, Chromatic/Emotional Connotation and Semantic Cohesion scorers (0.98) and finally the Domain, Ngram and Dependency Likelihood scorers (0.97).", "labels": [], "entities": []}, {"text": "These settings allow us to enforce an order of precedence among the scorers during slot-filling, while giving them virtually equal relevance for solution ranking.", "labels": [], "entities": [{"text": "solution ranking", "start_pos": 145, "end_pos": 161, "type": "TASK", "confidence": 0.9035625159740448}]}, {"text": "As discussed in Section 3 we use two different treebanks to learn the syntactic patterns (P) and the dependency operators (L).", "labels": [], "entities": []}, {"text": "For these experiments, patterns were learned from a corpus of 16,000 proverbs (), which offers a good selection of short sentences with a good potential to be used for slogan generation.", "labels": [], "entities": [{"text": "slogan generation", "start_pos": 168, "end_pos": 185, "type": "TASK", "confidence": 0.9617662131786346}]}, {"text": "This choice seemed to be a good compromise as, to our best knowledge, there is no published slogan dataset with an adequate size.", "labels": [], "entities": []}, {"text": "Besides, using existing slogans might have legal implications that we might not be aware of.", "labels": [], "entities": []}, {"text": "Dependency operators were learned by dependency parsing the British National Corpus . To reduce the amount of noise introduced by the automatic parses, we only considered sentences having less than 20 words.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7859968841075897}, {"text": "British National Corpus", "start_pos": 60, "end_pos": 83, "type": "DATASET", "confidence": 0.9363091786702474}]}, {"text": "Furthermore, we only considered sentences in which all the content words are listed in WordNet with the observed part of speech.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9735880494117737}]}, {"text": "The LSA space used for the semantic feature functions was also learned on BNC data, but in this case no filtering was applied.", "labels": [], "entities": [{"text": "BNC data", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.8954828381538391}]}], "tableCaptions": [{"text": " Table 2: Majority classes (%) for the five dimen- sions of the annotation.", "labels": [], "entities": []}, {"text": " Table 3: Majority decisions (%) for each annota- tion dimension.", "labels": [], "entities": []}]}