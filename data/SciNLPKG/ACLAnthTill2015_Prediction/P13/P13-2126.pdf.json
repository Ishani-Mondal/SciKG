{"title": [{"text": "Typesetting for Improved Readability using Lexical and Syntactic Information", "labels": [], "entities": []}], "abstractContent": [{"text": "We present results from our study of which uses syntactically and semantically motivated information to group segments of sentences into unbreakable units for the purpose of typesetting those sentences in a region of a fixed width, using an otherwise standard dynamic programming line breaking algorithm, to minimize ragged-ness.", "labels": [], "entities": [{"text": "dynamic programming line breaking", "start_pos": 260, "end_pos": 293, "type": "TASK", "confidence": 0.6402188464999199}]}, {"text": "In addition to a rule-based base-line segmenter, we use a very modest size text, manually annotated with positions of breaks, to train a maximum entropy clas-sifier, relying on an extensive set of lexical and syntactic features, which can then predict whether or not to break after a certain word position in a sentence.", "labels": [], "entities": []}, {"text": "We also use a simple genetic algorithm to search fora subset of the features optimizing F 1 , to arrive at a set of features that delivers 89.2% Precision, 90.2% Recall (89.7% F 1) on a test set, improving the rule-based baseline by about 11 points and the classi-fier trained on all features by about 1 point in F 1 .", "labels": [], "entities": [{"text": "Precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9988478422164917}, {"text": "Recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9980536699295044}, {"text": "F 1)", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9595169027646383}]}], "introductionContent": [], "datasetContent": [{"text": "Our data set consists of a modest set of 150 sentences (3918 tokens) selected from four different documents and manually annotated by a human expert relying on the 30 or so rules.", "labels": [], "entities": []}, {"text": "The annotation consists of marking after each token whether one is allowed to break at that position or not.", "labels": [], "entities": []}, {"text": "We developed three systems for predicting breaks: a rule-based baseline system, a maximumentropy classifier that learns to classify breaks using about 100 lexical, syntactic and collocational features, and a maximum entropy classifier that uses a subset of these features selected by a simple genetic algorithm in a hill-climbing fashion.", "labels": [], "entities": []}, {"text": "We evaluated our classifiers intrinsically using the usual measures: We expect to make our annotated data available upon the publication of the paper.", "labels": [], "entities": []}, {"text": "\u2022 Precision: Percentage of the breaks posited that were actually correct breaks in the goldstandard hand-annotated data.", "labels": [], "entities": [{"text": "Precision", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9963310360908508}, {"text": "goldstandard hand-annotated data", "start_pos": 87, "end_pos": 119, "type": "DATASET", "confidence": 0.7840341528256735}]}, {"text": "It is possible to get 100% precision by putting a single break at the end.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9996119141578674}]}, {"text": "\u2022 Recall: Percentage of the actual breaks correctly posited.", "labels": [], "entities": [{"text": "Recall", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.99702388048172}, {"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9715619683265686}]}, {"text": "It is possible to get 100% recall by positing a break after each token.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9915332198143005}]}, {"text": "\u2022 F 1 : The geometric mean of precision and recall divided by their average.", "labels": [], "entities": [{"text": "F 1", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9903076887130737}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9976231455802917}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9984627962112427}]}, {"text": "It should be noted that when a text is typeset into an area of width of a certain number of characters, an erroneous break need not necessarily lead to an actual break in the final output, that is an error may not be too bad.", "labels": [], "entities": []}, {"text": "On the other hand, a missed break while not hurting the readability of the text may actually lead to along segment that may eventually worsen raggedness in the final typesetting.", "labels": [], "entities": []}, {"text": "Baseline Classifier We implemented a subset of the rules (those that rely only on lexical and partof-speech information), as a baseline rule-based break classifier.", "labels": [], "entities": []}, {"text": "The baseline classifier avoids breaks: \u2022 after the first word in a sentence, quote or parentheses, \u2022 before the last word in a sentence, quote or parentheses, and \u2022 between a punctuation mark following a word or between two consecutive punctuation marks.", "labels": [], "entities": []}, {"text": "It posits breaks (i) before a word following a punctuation, and (ii) before prepositions, auxiliary verbs, coordinating conjunctions, subordinate conjunctions, relative pronouns, relative adverbs, conjunctive adverbs, and correlative conjunctions.", "labels": [], "entities": []}, {"text": "Maximum Entropy Classifier We used the CRF++ Tool 2 but with the option to run it only as a maximum entropy classifier, to train a classifier.", "labels": [], "entities": [{"text": "CRF++ Tool 2", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9105285108089447}]}, {"text": "We used a large set of about 100 features grouped into the following categories: \u2022 Lexical features: These features include the token and the POS tag for the previous, current and the next word.", "labels": [], "entities": []}, {"text": "We also encode whether the word is part of a compound noun or a verb, or is an adjective that subcategorizes a specific preposition in WordNet, (e.g., familiar with).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 135, "end_pos": 142, "type": "DATASET", "confidence": 0.9534938335418701}]}, {"text": "\u2022 Constituency structure features: These are unlexicalized features that take into account in the parse tree, fora word and its previous and next words, the labels of the parent, the grandparent and their siblings, and number of siblings they have.", "labels": [], "entities": []}, {"text": "We also consider the label of the closest common ancestor fora word and its next word.", "labels": [], "entities": []}, {"text": "\u2022 Dependency structure features: These are unlexicalized features that essentially capture the number of dependency relation links that cross-over a given word boundary.", "labels": [], "entities": []}, {"text": "The motivation for these comes from the desire to limit the amount of information that would need to be carried over that boundary, assuming this would be captured by the number of dependency links over the break point.", "labels": [], "entities": []}, {"text": "\u2022 Baseline feature: This feature reflects whether the rule-based baseline break classifier posits a break at this point or not.", "labels": [], "entities": []}, {"text": "We use the following tools to process the sentences to extract some of these features: \u2022 Stanford constituency and dependency parsers,;), \u2022 lemmatization tool in NLTK), \u2022 WordNet for compound nouns and verbs.", "labels": [], "entities": []}, {"text": "Baseline ME-All ME-GA Precision Maximum Entropy Classifier with GA Feature Selection We used a genetic algorithm on a development data set, to select a subset of the features above.", "labels": [], "entities": []}, {"text": "Basically, we start with a randomly selected set of features and through mutation and crossover try to obtain feature combinations that perform better over the development set in terms of F 1 score.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 188, "end_pos": 197, "type": "METRIC", "confidence": 0.9862972100575765}]}, {"text": "After a few hundred generations of this kind of hill-climbing, we get a subset of features that perform the best.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results from Baseline and Maximum En- tropy break classifiers", "labels": [], "entities": []}]}