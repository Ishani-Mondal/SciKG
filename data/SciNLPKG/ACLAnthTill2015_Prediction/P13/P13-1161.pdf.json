{"title": [{"text": "Joint Inference for Fine-grained Opinion Extraction", "labels": [], "entities": [{"text": "Joint Inference", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5836530476808548}, {"text": "Fine-grained Opinion Extraction", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.5658407708009084}]}], "abstractContent": [{"text": "This paper addresses the task of fine-grained opinion extraction-the identification of opinion-related entities: the opinion expressions, the opinion holders , and the targets of the opinions, and the relations between opinion expressions and their targets and holders.", "labels": [], "entities": [{"text": "fine-grained opinion extraction-the identification", "start_pos": 33, "end_pos": 83, "type": "TASK", "confidence": 0.7705683410167694}]}, {"text": "Most existing approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner, where the inter-dependencies among different extraction stages are not captured.", "labels": [], "entities": []}, {"text": "We propose a joint inference model that leverages knowledge from predictors that optimize subtasks of opinion extraction, and seeks a globally optimal solution.", "labels": [], "entities": [{"text": "opinion extraction", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7559870183467865}]}, {"text": "Experimental results demonstrate that our joint inference approach significantly outperforms traditional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction.", "labels": [], "entities": [{"text": "opinion extraction", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.7989991307258606}]}], "introductionContent": [{"text": "Fine-grained opinion analysis is concerned with identifying opinions in text at the expression level; this includes identifying the subjective (i.e., opinion) expression itself, the opinion holder and the target of the opinion ().", "labels": [], "entities": [{"text": "opinion analysis", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.8293017446994781}]}, {"text": "The task has received increasing attention as many natural language processing applications would benefit from the ability to identify text spans that correspond to these key components of opinions.", "labels": [], "entities": []}, {"text": "In question-answering systems, for example, users may submit questions in the form \"What does entity A think about target B?\"; opinion-oriented summarization systems also need to recognize opinions and their targets and holders.", "labels": [], "entities": []}, {"text": "In this paper, we address the task of identifying opinion-related entities and opinion relations.", "labels": [], "entities": []}, {"text": "We consider three types of opinion entities: opinion expressions or direct subjective expressions as defined in -expressions that explicitly indicate emotions, sentiment, opinions or other private states or speech events expressing private states; opinion targets -expressions that indicate what the opinion is about; and opinion holders -mentions of whom or what the opinion is from.", "labels": [], "entities": []}, {"text": "Consider the following examples in which opinion expressions (O) are underlined and targets (T) and holders (H) of the opinion are bracketed. were irked by [the government report] and were worried as they went about their daily chores.", "labels": [], "entities": []}, {"text": "S2: From the very start it could be predicted ] that on the subject of economic globalization, [the developed states] were going to come across fierce opposition .", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation, we used version 2.0 of the MPQA corpus (, a widely used data set for fine-grained opinion analysis.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9784135818481445}, {"text": "fine-grained opinion analysis", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.6478187640508016}]}, {"text": "We considered the subset of 482 documents that contain attitude and target annotations.", "labels": [], "entities": []}, {"text": "There area total of 9,471 sentences with opinionrelated labels at the phrase level.", "labels": [], "entities": []}, {"text": "We set aside 132 documents as a development set and use 350 documents as the evaluation set.", "labels": [], "entities": []}, {"text": "All experiments employ 10-fold cross validation on the evaluation set; the average over the 10 runs is reported.", "labels": [], "entities": []}, {"text": "Our gold standard opinion expressions, opinion targets and opinion holders correspond to the direct subjective annotations, target annotations and agent annotations, respectively.", "labels": [], "entities": []}, {"text": "The IS-FROM relation is obtained from the agent attribute of each opinion expression.", "labels": [], "entities": []}, {"text": "The IS-ABOUT relation is obtained from the attitude annotations: each opinion expression is annotated with attitude frames and each attitude frame is associated with a list of targets.", "labels": [], "entities": []}, {"text": "The relations may overlap: for example, in the following sentence, the target of relation 1 contains relation 2.", "labels": [], "entities": []}, {"text": "[ We discard relations that contain sub-relations because we believe that identifying the sub-relations usually is sufficient to recover the discarded relations.", "labels": [], "entities": []}, {"text": "(Prediction of overlapping relations is considered as future work.)", "labels": [], "entities": []}, {"text": "In the example above, we will identify (loves, being at Enderly Park) as an IS-ABOUT relation and happy as an opinion expression associated with an implicit target.", "labels": [], "entities": [{"text": "Enderly Park", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9654027223587036}]}, {"text": "shows some statistics of the corpus.", "labels": [], "entities": []}, {"text": "We adopted the evaluation metrics for entity and relation extraction from, which include precision, recall, and F1-measure according to overlap and exact matching metrics.", "labels": [], "entities": [{"text": "entity and relation extraction", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.6353999823331833}, {"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9993876218795776}, {"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9983794689178467}, {"text": "F1-measure", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.99950110912323}]}, {"text": "will focus our discussion on results obtained using overlap matching, since the exact boundaries of opinion entities are hard to define even for human annotators ().", "labels": [], "entities": [{"text": "overlap matching", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.6655478030443192}]}, {"text": "We trained CRFs for opinion entity identification using the following features: indicators for words, POS tags, and lexicon features (the subjectivity strength of the word in the Subjectivity Lexicon).", "labels": [], "entities": [{"text": "opinion entity identification", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.6682380139827728}]}, {"text": "All features are computed for the current token and tokens in a [\u22121, +1] window.", "labels": [], "entities": []}, {"text": "We used L2-regularization; the regularization parameter was tuned using the development set.", "labels": [], "entities": []}, {"text": "We trained the classifiers for relation extraction using L1-regularized logistic regression with default parameters using the LIBLINEAR package.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.9309159219264984}]}, {"text": "For joint inference, we used GLPK 9 to provide the optimal ILP solution.", "labels": [], "entities": [{"text": "joint inference", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8016314506530762}, {"text": "GLPK", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.8190712928771973}]}, {"text": "The parameter \u03bb was tuned using the development set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data Statistics of the MPQA Corpus.", "labels": [], "entities": [{"text": "MPQA Corpus", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.9505822062492371}]}, {"text": " Table 2: Performance on opinion entity extraction using overlap and exact matching metrics (the top table uses overlap and", "labels": [], "entities": [{"text": "opinion entity extraction", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.6332578261693319}]}, {"text": " Table 3: Performance on opinion relation extraction using the overlap metric.", "labels": [], "entities": [{"text": "opinion relation extraction", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.7805615663528442}]}, {"text": " Table 4: Comparison between our approach and ILP baselines that omit some potentials in our approach.", "labels": [], "entities": []}]}