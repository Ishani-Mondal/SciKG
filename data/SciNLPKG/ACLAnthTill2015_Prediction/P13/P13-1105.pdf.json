{"title": [], "abstractContent": [{"text": "This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency.", "labels": [], "entities": []}, {"text": "By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction.", "labels": [], "entities": []}, {"text": "We induced dependency grammar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilingually-guided method achieves a significant improvement of 28.5% over the unsuper-vised baseline and 3.0% over the best projection baseline on average.", "labels": [], "entities": []}], "introductionContent": [{"text": "In past decades supervised methods achieved the state-of-the-art in constituency parsing) and dependency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.8355083763599396}, {"text": "dependency parsing", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.9096314907073975}]}, {"text": "For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build.", "labels": [], "entities": []}, {"text": "As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts, and semi-supervised methods () which use both raw texts and annotated corpus.", "labels": [], "entities": []}, {"text": "And there area lot of efforts have also been devoted to bilingual projection (, which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (.", "labels": [], "entities": [{"text": "bilingual projection", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.7860423624515533}]}, {"text": "In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.8348069389661154}]}, {"text": "Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost.", "labels": [], "entities": []}, {"text": "On the contrary, bilingual projection) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language.", "labels": [], "entities": [{"text": "bilingual projection", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.7202994376420975}]}, {"text": "By projecting syntactic structures directly ( across bilingual texts or indirectly across multilingual texts), a better dependency grammar can be easily induced, if syntactic isomorphism is largely maintained between target and source languages.", "labels": [], "entities": []}, {"text": "Unsupervised induction and bilingual projection run according to totally different principles, the former mines the underlying structure of the monolingual language, while the latter leverages the syntactic knowledge of the parsed counter- part language.", "labels": [], "entities": [{"text": "bilingual projection", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7013697475194931}]}, {"text": "Considering this, we propose a novel strategy for automatically inducing a monolingual dependency grammar under the guidance of bilingually-projected dependency information, which integrates the advantage of bilingual projection into the unsupervised framework.", "labels": [], "entities": []}, {"text": "A randomly-initialized monolingual treebank evolves in a self-training iterative procedure, and the grammar parameters are tuned to simultaneously maximize both the monolingual likelihood and bilingually-projected likelihood of the evolving treebank.", "labels": [], "entities": []}, {"text": "The monolingual likelihood is similar to the optimization objectives of conventional unsupervised models, while the bilinguallyprojected likelihood is the product of the projected probabilities of dependency trees.", "labels": [], "entities": []}, {"text": "By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, we can automatically induce a monolingual dependency grammar which is much better than previous models only using bilingual projection or unsupervised induction.", "labels": [], "entities": []}, {"text": "In addition, since both likelihoods are fundamentally factorized into dependency edges (of the hypothesis tree), the computational complexity approaches to unsupervised models, while with much faster convergence.", "labels": [], "entities": []}, {"text": "We evaluate the final automatically-induced dependency parsing model on 5 languages.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.6911454200744629}]}, {"text": "Experimental results show that our method significantly outperforms previous work based on unsupervised method or indirect/direct dependency projection, where we see an average improvement of 28.5% over unsupervised baseline on all languages, and the improvements are 3.9%/3.0% over indirect/direct baselines.", "labels": [], "entities": []}, {"text": "And our model achieves the most significant gains on Chinese, where the improvements are 12.0%, 4.5% over indirect and direct projection baselines respectively.", "labels": [], "entities": []}, {"text": "In the rest of the paper, we first describe the unsupervised dependency grammar induction framework in section 2 (where the unsupervised optimization objective is given), and introduce the bilingual projection method for dependency parsing in section 3 (where the projected optimization objective is given); Then in section 4 we present the bilingually-guided induction strategy for dependency grammar (where the two objectives above are jointly optimized, as shown in.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 221, "end_pos": 239, "type": "TASK", "confidence": 0.797600120306015}, {"text": "dependency grammar", "start_pos": 383, "end_pos": 401, "type": "TASK", "confidence": 0.8333500325679779}]}, {"text": "After giving a brief introduction of previous work in section 5, we finally give the experimental results in section 6 and conclude our work in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the performance of the MST dependency parser ( which is trained by our bilingually-guided model on 5 languages.", "labels": [], "entities": [{"text": "MST dependency parser", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.9117326736450195}]}, {"text": "And the features used in our experiments are summarized in.", "labels": [], "entities": []}, {"text": "Datasets and Evaluation Our experiments are run on five different languages: Chinese(ch), Danish(da), Dutch(nl), Portuguese(pt) and Swedish(sv) (da, nl, pt and sv are free data sets distributed for the 2006 CoNLL Shared Tasks ().", "labels": [], "entities": []}, {"text": "For all languages, we only use English-target parallel data: we take the FBIS English-Chinese bitext as bilingual corpus for English-Chinese dependency projection which contains 239K sentence pairs with about 8.9M/6.9M words in English/Chinese, and for other languages we use the readily available data in the Europarl corpus.", "labels": [], "entities": [{"text": "FBIS English-Chinese bitext", "start_pos": 73, "end_pos": 100, "type": "DATASET", "confidence": 0.8668952584266663}, {"text": "Europarl corpus", "start_pos": 310, "end_pos": 325, "type": "DATASET", "confidence": 0.9952157735824585}]}, {"text": "Then we run tests on the Penn Chinese Treebank (CTB) and CoNLL-X test sets.", "labels": [], "entities": [{"text": "Penn Chinese Treebank (CTB)", "start_pos": 25, "end_pos": 52, "type": "DATASET", "confidence": 0.9690460463364919}, {"text": "CoNLL-X test sets", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.9077855745951334}]}, {"text": "English sentences are tagged by the implementations of the POS tagger of, which is trained on WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.9509276151657104}]}, {"text": "The source sentences are then parsed by an implementation of 2nd-ordered MST model of , which is trained on dependency trees extracted from Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 140, "end_pos": 153, "type": "DATASET", "confidence": 0.9939042627811432}]}, {"text": "As the evaluation metric, we use parsing accuracy which is the percentage of the words which have found their correct parents.", "labels": [], "entities": [{"text": "parsing", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9435788989067078}, {"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.8587105870246887}]}, {"text": "We evaluate on sentences with all length for our method.", "labels": [], "entities": []}, {"text": "Training Regime In experiments, we use the projection method proposed by to provide the projection instances.", "labels": [], "entities": []}, {"text": "And we train the projection part \u03b1 = 0 first for initialization, on which the whole model will be trained.", "labels": [], "entities": []}, {"text": "Availing of the initialization method, the model can converge very fast (about 3 iterations is sufficient) and the results are more stable than the ones trained on random initialization.", "labels": [], "entities": [{"text": "initialization", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.9752825498580933}]}, {"text": "Baselines We compare our method against three kinds of different approaches: unsupervised method (; singlesource direct projection methods (; multi-source indirect projection methods with multi-sources (M-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The directed dependency accuracy with  different parameter of our model and the base- lines. The first section of the table (row 3-7)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9005593061447144}]}]}