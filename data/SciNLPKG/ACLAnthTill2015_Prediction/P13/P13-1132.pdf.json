{"title": [{"text": "Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity", "labels": [], "entities": []}], "abstractContent": [{"text": "Semantic similarity is an essential component of many Natural Language Processing applications.", "labels": [], "entities": [{"text": "Semantic similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8170526623725891}]}, {"text": "However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each datatype.", "labels": [], "entities": [{"text": "computing semantic similarity", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.6668040057023367}]}, {"text": "We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents.", "labels": [], "entities": []}, {"text": "Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data.", "labels": [], "entities": []}, {"text": "This unified representation shows state-of-the-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.7338796257972717}, {"text": "word sense coarsening", "start_pos": 129, "end_pos": 150, "type": "TASK", "confidence": 0.6348464091618856}]}], "introductionContent": [{"text": "Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment), Semantic Role Labeling (, and Question Answering ().", "labels": [], "entities": [{"text": "Semantic similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8745740652084351}, {"text": "Semantic Role Labeling", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.8055485089619955}, {"text": "Question Answering", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.855485200881958}]}, {"text": "For example, textual similarity enables relevant documents to be identified for information retrieval (), while identifying similar words enables tasks such as paraphrasing, lexical substitution (, lexical simplification, and Web search result clustering.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 80, "end_pos": 101, "type": "TASK", "confidence": 0.7363509833812714}, {"text": "lexical substitution", "start_pos": 174, "end_pos": 194, "type": "TASK", "confidence": 0.6768246293067932}, {"text": "lexical simplification", "start_pos": 198, "end_pos": 220, "type": "TASK", "confidence": 0.7473898530006409}, {"text": "Web search result clustering", "start_pos": 226, "end_pos": 254, "type": "TASK", "confidence": 0.6954760402441025}]}, {"text": "Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (.", "labels": [], "entities": []}, {"text": "Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text; for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information ().", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7417949736118317}, {"text": "SemEval-2012 task on textual similarity", "start_pos": 199, "end_pos": 238, "type": "TASK", "confidence": 0.7450765013694763}]}, {"text": "We propose a unified approach to semantic similarity across multiple representation levels from senses to documents, which offers two significant advantages.", "labels": [], "entities": []}, {"text": "First, the method is applicable independently of the input type, which enables meaningful similarity comparisons across different scales of text or lexical levels.", "labels": [], "entities": []}, {"text": "Second, by operating at the sense level, a unified approach is able to identify the semantic similarities that exist independently of the text's lexical forms and any semantic ambiguity therein.", "labels": [], "entities": []}, {"text": "For example, consider the sentences: t1.", "labels": [], "entities": []}, {"text": "A manager fired the worker. t2.", "labels": [], "entities": []}, {"text": "An employee was terminated from work by his boss.", "labels": [], "entities": []}, {"text": "A surface-based approach would label the sentences as dissimilar due to the minimal lexical overlap.", "labels": [], "entities": []}, {"text": "However, a sense-based representation enables detection of the similarity between the meanings of the words, e.g., fire and terminate.", "labels": [], "entities": []}, {"text": "Indeed, an accurate, sense-based representation is essential for cases where different words are used to convey the same meaning.", "labels": [], "entities": []}, {"text": "The contributions of this paper are threefold.", "labels": [], "entities": []}, {"text": "First, we propose anew unified representation of the meaning of an arbitrarily-sized piece of text, referred to as a lexical item, using a sense-based probability distribution.", "labels": [], "entities": []}, {"text": "Second, we propose a novel alignment-based method for word sense dis-ambiguation during semantic comparison.", "labels": [], "entities": [{"text": "word sense dis-ambiguation during semantic comparison", "start_pos": 54, "end_pos": 107, "type": "TASK", "confidence": 0.6319175213575363}]}, {"text": "Third, we demonstrate that this single representation can achieve state-of-the-art performance on three similarity tasks, each operating at a different lexical level: (1) surpassing the highest scores on the SemEval-2012 task on textual similarity) that compares sentences, (2) achieving a near-perfect performance on the TOEFL synonym selection task proposed by, which measures word pair similarity, and also obtaining state-of-the-art performance in terms of the correlation with human judgments on the RG-65 dataset, and finally (3) surpassing the performance of in a sensecoarsening task that measures sense similarity.", "labels": [], "entities": [{"text": "TOEFL synonym selection task", "start_pos": 322, "end_pos": 350, "type": "TASK", "confidence": 0.8257342129945755}, {"text": "RG-65 dataset", "start_pos": 505, "end_pos": 518, "type": "DATASET", "confidence": 0.9660884141921997}]}], "datasetContent": [{"text": "Measuring semantic similarity of textual items has applications in a wide variety of NLP tasks.", "labels": [], "entities": []}, {"text": "As our benchmark, we selected the recent SemEval-2012 task on Semantic Textual Similarity (STS), which was concerned with measuring the semantic similarity of sentence pairs.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.7523297270139059}]}, {"text": "The task received considerable interest by facilitating a meaningful comparison between approaches.", "labels": [], "entities": []}, {"text": "We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli: F-score sense merging evaluation on three hand-labeled datasets: OntoNotes (Onto), Senseval-2 (SE-2), and combined (Onto+SE-2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9992170333862305}, {"text": "F-score sense merging", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.8202001452445984}]}, {"text": "Results are reported for all three of our signature comparison measures and also for two previous works (last two rows).", "labels": [], "entities": []}, {"text": "as either merged or not-merged.", "labels": [], "entities": []}, {"text": "We constructed a simple threshold-based classifier to perform the same binary classification.", "labels": [], "entities": []}, {"text": "To this end, we calculated the semantic similarity of each sense pair and then used a threshold value t to classify the pair as merged if similarity \u2265 t and not-merged otherwise.", "labels": [], "entities": []}, {"text": "We sampled out 10% of the dataset for tuning the value oft, thus adapting our classifier to the fine granularity of the dataset.", "labels": [], "entities": []}, {"text": "We used the same held-out instances to perform a tuning of the k value used for Jaccard index, over the same values of k as in Experiment 1 (cf. Section 3).", "labels": [], "entities": []}, {"text": "We now proceed from the sentence level to the word level.", "labels": [], "entities": []}, {"text": "Word similarity has been a key problem for lexical semantics, with significant efforts being made by approaches in distributional semantics to accurately identify synonymous words.", "labels": [], "entities": [{"text": "Word similarity", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6854984760284424}]}, {"text": "Different evaluation methods exist in the literature for evaluating the performance of a word-level semantic similarity measure; we adopted two well-established benchmarks: synonym recognition and correlating word similarity judgments with those from human annotators.", "labels": [], "entities": [{"text": "synonym recognition", "start_pos": 173, "end_pos": 192, "type": "TASK", "confidence": 0.8964594006538391}]}, {"text": "For synonym recognition, we used the TOEFL dataset created by.", "labels": [], "entities": [{"text": "synonym recognition", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9775749444961548}, {"text": "TOEFL dataset", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.8096846640110016}]}, {"text": "The dataset consists of 80 multiple-choice synonym questions from the TOEFL test; a word is paired with four options, one of which is a valid synonym.", "labels": [], "entities": [{"text": "TOEFL test", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.8024975657463074}]}, {"text": "Test takers with English as a second language averaged 64.5% correct.", "labels": [], "entities": []}, {"text": "Despite multiple approaches, only recently has the test been answered perfectly, underscoring the challenge of synonym recognition.  that of Rapp uses word senses, an approach that is outperformed by our method.", "labels": [], "entities": [{"text": "synonym recognition.", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.9253604710102081}]}, {"text": "The errors produced by our system were largely the result of sense locality in the WordNet network.", "labels": [], "entities": [{"text": "WordNet network", "start_pos": 83, "end_pos": 98, "type": "DATASET", "confidence": 0.9640825092792511}]}, {"text": "The synonym mistakes reveal cases where senses of the two words are close in WordNet, indicating some relatedness.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.9669431447982788}]}, {"text": "For example, percentage maybe interpreted colloquially as monetary value (e.g., \"give me my percentage\") and elicits the synonym of profit in the economic domain, which ADW incorrectly selects as a synonym.", "labels": [], "entities": [{"text": "ADW", "start_pos": 169, "end_pos": 172, "type": "DATASET", "confidence": 0.8824214339256287}]}, {"text": "shows the Spearman's \u03c1 rank correlation coefficients with human judgments on the RG-65 dataset.", "labels": [], "entities": [{"text": "Spearman's \u03c1 rank correlation", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.6566916942596436}, {"text": "RG-65 dataset", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9814918339252472}]}, {"text": "As can be seen from the Table, our approach with the Weighted Overlap signature comparison improves over the similar approach of which, however, does not involve the disambiguation step and considers a word as a whole unit as represented by the set of its senses.", "labels": [], "entities": []}, {"text": "WordNet is known to be a fine-grained sense inventory with many related word senses.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9646101593971252}]}, {"text": "Accordingly, multiple approaches have attempted to identify highly similar senses in order to produce a coarse-grained sense inventory.", "labels": [], "entities": []}, {"text": "We adopt this task as away of evaluating our similarity measure at the sense level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the provided datasets for the  SemEval-2012 Semantic Textual Similarity task.", "labels": [], "entities": [{"text": "SemEval-2012 Semantic Textual Similarity task", "start_pos": 55, "end_pos": 100, "type": "TASK", "confidence": 0.8691797375679016}]}, {"text": " Table 2: Performance of our system (ADW) and the 3 top-ranking participating systems (out of 88) in  the SemEval-2012 Semantic Textual Similarity task. Rightmost columns report the corresponding Pear- son correlation r for individual datasets, i.e., MSRpar (Mpar), MSRvid (Mvid), SMTeuroparl (SMTe),  OnWN (OnWN) and SMTnews (SMTn). We also provide scores according to the three official evalua- tion metrics (i.e., ALL, ALLnrm, and Mean). Rankings are also presented based on the three metrics.", "labels": [], "entities": [{"text": "SemEval-2012 Semantic Textual Similarity task", "start_pos": 106, "end_pos": 151, "type": "TASK", "confidence": 0.8175712823867798}, {"text": "Pear- son correlation r", "start_pos": 196, "end_pos": 219, "type": "METRIC", "confidence": 0.9031094908714294}, {"text": "Mean", "start_pos": 434, "end_pos": 438, "type": "METRIC", "confidence": 0.9914637207984924}]}, {"text": " Table 3: Performance of our main-feature sys- tem with conventional WSD (DW) and with the  alignment-based disambiguation approach (ADW- MF) vs. four other similarity measures, using 10- fold cross validation on the training datasets MSR- par (Mpar), MSRvid (Mvid), and SMTeuroparl  (SMTe).", "labels": [], "entities": []}, {"text": " Table 4: Accuracy on the 80-question TOEFL  Synonym test. ADW Jac , ADW W O , and ADW Cos  correspond to results with the Jaccard, Weighted  Overlap and Cosine signature comparison mea- sures, respectively.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9858021140098572}, {"text": "TOEFL  Synonym test", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.48719433943430585}, {"text": "ADW Jac", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.6535308957099915}, {"text": "ADW W O", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.5713754196961721}, {"text": "ADW Cos", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.6510833501815796}, {"text": "Cosine signature comparison mea- sures", "start_pos": 154, "end_pos": 192, "type": "METRIC", "confidence": 0.6804387221733729}]}, {"text": " Table 6: Spearman's \u03c1 correlation coefficients  with human judgments on the RG-65 dataset.  ADW Jac , ADW W O , and ADW Cos correspond to  results with the Jaccard, Weighted Overlap and  Cosine signature comparison measures respec- tively.", "labels": [], "entities": [{"text": "RG-65 dataset", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.9758289754390717}, {"text": "ADW Jac , ADW W O", "start_pos": 93, "end_pos": 110, "type": "METRIC", "confidence": 0.5566747834285101}, {"text": "ADW Cos", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.6814126968383789}]}]}