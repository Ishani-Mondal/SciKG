{"title": [{"text": "Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling", "labels": [], "entities": [{"text": "Code-Switching Language Modeling", "start_pos": 74, "end_pos": 106, "type": "TASK", "confidence": 0.6270981629689535}]}], "abstractContent": [{"text": "In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech.", "labels": [], "entities": []}, {"text": "We present away to integrate partof-speech tags (POS) and language information (LID) into these models which leads to significant improvements in terms of perplexity.", "labels": [], "entities": []}, {"text": "Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed.", "labels": [], "entities": []}, {"text": "Finally, we show that recurrent neural networks and factored language models can be combined using linear interpolation to achieve the best performance.", "labels": [], "entities": []}, {"text": "The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model.", "labels": [], "entities": [{"text": "SEAME development set", "start_pos": 100, "end_pos": 121, "type": "DATASET", "confidence": 0.8067802588144938}]}, {"text": "Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models", "labels": [], "entities": [{"text": "code switching", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.7273036539554596}, {"text": "language modeling", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7165616154670715}]}], "introductionContent": [{"text": "Code-Switching (CS) speech is defined as speech that contains more than one language ('code').", "labels": [], "entities": []}, {"text": "It is a common phenomenon in multilingual communities.", "labels": [], "entities": []}, {"text": "For the automated processing of spoken communication in these scenarios, a speech recognition system must be able to handle code switches.", "labels": [], "entities": [{"text": "automated processing of spoken communication", "start_pos": 8, "end_pos": 52, "type": "TASK", "confidence": 0.6185468554496765}, {"text": "speech recognition", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7550955414772034}]}, {"text": "However, the components of speech recognition systems are usually trained on monolingual data.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8006498217582703}]}, {"text": "Furthermore, there is alack of bilingual training data.", "labels": [], "entities": []}, {"text": "While there have been promising research results in the area of acoustic modeling, only few approaches so far address Code-Switching in the language model.", "labels": [], "entities": [{"text": "acoustic modeling", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.8324494957923889}]}, {"text": "Recently, it has been shown that recurrent neural network language models (RNNLMs) can improve perplexity and error rates in speech recognition systems in comparison to traditional n-gram approaches ().", "labels": [], "entities": [{"text": "error rates", "start_pos": 110, "end_pos": 121, "type": "METRIC", "confidence": 0.9413635730743408}, {"text": "speech recognition", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.6737867742776871}]}, {"text": "One reason for that is their ability to handle longer contexts.", "labels": [], "entities": []}, {"text": "Furthermore, the integration of additional features as input is rather straightforward due to their structure.", "labels": [], "entities": []}, {"text": "On the other hand, factored language models (FLMs) have been used successfully for languages with rich morphology due to their ability to process syntactical features, such as word stems or part-of-speech tags.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is the application of RNNLMs and FLMs to the challenging task of Code-Switching.", "labels": [], "entities": []}, {"text": "Furthermore, the two different models are combined using linear interpolation.", "labels": [], "entities": []}, {"text": "In addition, a comparison between them is provided including a detailed analysis to explain their results.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation, we compute the perplexity of each language model on the SEAME development and evaluation set und perform an analysis of the different back-off levels to understand in detail the behavior of each language model.", "labels": [], "entities": [{"text": "SEAME development and evaluation set", "start_pos": 72, "end_pos": 108, "type": "DATASET", "confidence": 0.6703367948532104}]}, {"text": "A traditional 3-gram LM trained with the SEAME transcriptions serves as baseline.", "labels": [], "entities": [{"text": "SEAME transcriptions", "start_pos": 41, "end_pos": 61, "type": "DATASET", "confidence": 0.4521050453186035}]}], "tableCaptions": [{"text": " Table 1: Statistics of the SEAME corpus", "labels": [], "entities": [{"text": "SEAME", "start_pos": 28, "end_pos": 33, "type": "TASK", "confidence": 0.5226989984512329}]}, {"text": " Table 4: Perplexities after interpolation", "labels": [], "entities": [{"text": "Perplexities", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.8740822076797485}]}]}