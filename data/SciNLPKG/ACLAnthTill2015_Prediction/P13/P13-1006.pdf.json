{"title": [{"text": "Grounded Language Learning from Video Described with Sentences", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a method that learns representations for word meanings from short video clips paired with sentences.", "labels": [], "entities": []}, {"text": "Unlike prior work on learning language from symbolic input, our input consists of video of people interacting with multiple complex objects in outdoor environments.", "labels": [], "entities": []}, {"text": "Unlike prior computer-vision approaches that learn from videos with verb labels or images with noun labels, our labels are sentences containing nouns, verbs, prepositions , adjectives, and adverbs.", "labels": [], "entities": []}, {"text": "The correspondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts simultaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences.", "labels": [], "entities": []}, {"text": "The learned word meanings can be subsequently used to automatically generate description of new video.", "labels": [], "entities": []}], "introductionContent": [{"text": "People learn language through exposure to a rich perceptual context.", "labels": [], "entities": []}, {"text": "Language is grounded by mapping words, phrases, and sentences to meaning representations referring to the world. has shown that even with referential uncertainty and noise, a system based on crosssituational learning can robustly acquire a lexicon, mapping words to word-level meanings from sentences paired with sentence-level meanings.", "labels": [], "entities": []}, {"text": "However, it did so only for symbolic representations of word-and sentence-level meanings that were not perceptually grounded.", "labels": [], "entities": []}, {"text": "An ideal system would not require detailed word-level labelings to acquire word meanings from video but rather could learn language in a largely unsupervised fashion, just as a child does, from video paired with sentences.", "labels": [], "entities": []}, {"text": "There has been recent research on grounded language learning.", "labels": [], "entities": []}, {"text": "pairs training sentences with vectors of real-valued features extracted from synthesized images which depict 2D blocks-world scenes, to learn a specific set of features for adjectives, nouns, and adjuncts.", "labels": [], "entities": []}, {"text": "paired training images containing multiple objects with spoken name candidates for the objects to find the correspondence between lexical items and visual features.", "labels": [], "entities": []}, {"text": "paired narrated sentences with symbolic representations of their meanings, automatically extracted from video, to learn object names, spatial-relation terms, and event names as a mapping from the grammatical structure of a sentence to the semantic structure of the associated meaning representation.", "labels": [], "entities": []}, {"text": "learned the language of sportscasting by determining the mapping between game commentaries and the meaning representations output by a rulebased simulation of the game.", "labels": [], "entities": []}, {"text": "present an approach that learns Montaguegrammar representations of word meanings together with a combinatory categorial grammar (CCG) from child-directed sentences paired with first-order formulas that represent their meaning.", "labels": [], "entities": [{"text": "Montaguegrammar representations of word meanings", "start_pos": 32, "end_pos": 80, "type": "TASK", "confidence": 0.5706571578979492}]}, {"text": "Although most of these methods succeed in learning word meanings from sentential descriptions they do so only for symbolic or simple visual input (often synthesized); they fail to bridge the gap between language and computer vision, i.e., they do not attempt to extract meaning representations from complex visual scenes.", "labels": [], "entities": []}, {"text": "On the other hand, there has been research on training object and event models from large corpora of complex images and video in the computer-vision community (.", "labels": [], "entities": []}, {"text": "However, most such work requires training data that labels individual concepts with individual words (i.e., ob-jects delineated via bounding boxes in images as nouns and events that occur in short video clips as verbs).", "labels": [], "entities": []}, {"text": "There is no attempt to model phrasal or sentential meaning, let alone acquire the objector event models from training data labeled with phrasal or sentential annotations.", "labels": [], "entities": []}, {"text": "Moreover, such work uses distinct representations for different parts of speech; i.e., object and event recognizers use different representations.", "labels": [], "entities": []}, {"text": "In this paper, we present a method that learns representations for word meanings from short video clips paired with sentences.", "labels": [], "entities": []}, {"text": "Our work differs from prior work in three ways.", "labels": [], "entities": []}, {"text": "First, our input consists of realistic video filmed in an outdoor environment.", "labels": [], "entities": []}, {"text": "Second, we learn the entire lexicon, including nouns, verbs, prepositions, adjectives, and adverbs, simultaneously from video described with whole sentences.", "labels": [], "entities": []}, {"text": "Third we adopt a uniform representation for the meanings of words in all parts of speech, namely Hidden Markov Models (HMMs) whose states and distributions allow for multiple possible interpretations of a word or a sentence in an ambiguous perceptual context.", "labels": [], "entities": []}, {"text": "We employ the following representation to ground the meanings of words, phrases, and sentences in video clips.", "labels": [], "entities": []}, {"text": "We first run an object detector on each video frame to yield a set of detections, each a subregion of the frame.", "labels": [], "entities": []}, {"text": "In principle, the object detector need just detect the objects rather than classify them.", "labels": [], "entities": []}, {"text": "In practice, we employ a collection of class-, shape-, pose-, and viewpoint-specific detectors and pool the detections to account for objects whose shape, pose, and viewpoint may vary overtime.", "labels": [], "entities": []}, {"text": "Our methods can learn to associate a single noun with detections produced by multiple detectors.", "labels": [], "entities": []}, {"text": "We then string together detections from individual frames to yield tracks for objects that temporally span the video clip.", "labels": [], "entities": []}, {"text": "We associate a feature vector with each frame (detection) of each such track.", "labels": [], "entities": []}, {"text": "This feature vector can encode image features (including the identity of the particular detector that produced that detection) that correlate with object class; region color, shape, and size features that correlate with object properties; and motion features, such as linear and angular object position, velocity, and acceleration, that correlate with event properties.", "labels": [], "entities": []}, {"text": "We also compute features between pairs of tracks to encode the relative position and motion of the pairs of objects that participate in events that involve two participants.", "labels": [], "entities": []}, {"text": "In principle, we can also compute features between tuples of any number of tracks.", "labels": [], "entities": []}, {"text": "Following,, and, we represent the meaning of an intransitive verb, like jump, as a two-state HMM over the velocity-direction feature, modeling the requirement that the participant move upward then downward.", "labels": [], "entities": []}, {"text": "We represent the meaning of a transitive verb, like pickup, as a two-state HMM over both single-object and object-pair features: the agent moving toward the patient while the patient is as rest, followed by the agent moving together with the patient.", "labels": [], "entities": []}, {"text": "We extend this general approach to other parts of speech.", "labels": [], "entities": []}, {"text": "Nouns, like person, can be represented as one-state HMMs over image features that correlate with the object classes denoted by those nouns.", "labels": [], "entities": []}, {"text": "Adjectives, like red, round, and big, can be represented as one-state HMMs over region color, shape, and size features that correlate with object properties denoted by such adjectives.", "labels": [], "entities": []}, {"text": "Adverbs, like quickly, can be represented as one-state HMMs over object-velocity features.", "labels": [], "entities": []}, {"text": "Intransitive prepositions, like leftward, can be represented as one-state HMMs over velocity-direction features.", "labels": [], "entities": []}, {"text": "Static transitive prepositions, like to the left of, can be represented as one-state HMMs over the relative position of a pair of objects.", "labels": [], "entities": []}, {"text": "Dynamic transitive prepositions, like towards, can be represented as HMMs over the changing distance between a pair of objects.", "labels": [], "entities": []}, {"text": "Note that with this formulation, the representation of a verb, like approach, might be the same as a dynamic transitive preposition, like towards.", "labels": [], "entities": []}, {"text": "While it might seem like overkill to represent the meanings of words as one-stateHMMs, in practice, we often instead encode such concepts with multiple states to allow for temporal variation in the associated features due to changing pose and viewpoint as well as deal with noise and occlusion.", "labels": [], "entities": []}, {"text": "Moreover, the general framework of modeling word meanings as temporally variant time series via multi-state HMMs allows one to model denominalized verbs, i.e., nouns that denote events, as in The jump was fast.", "labels": [], "entities": []}, {"text": "Our HMMs are parameterized with varying arity.", "labels": [], "entities": []}, {"text": "Some, like jump(\u03b1), person(\u03b1), red(\u03b1), round(\u03b1), big(\u03b1), quickly(\u03b1), and leftward(\u03b1) have one argument, while others, like pick-up(\u03b1, \u03b2), to-the-left-of(\u03b1, \u03b2), and towards(\u03b1, \u03b2), have two arguments (In principle, any arity can be supported.).", "labels": [], "entities": []}, {"text": "HMMs are instantiated by mapping their arguments to tracks.", "labels": [], "entities": []}, {"text": "This involves computing the associated feature vector for that HMM over the detections in the tracks chosen to fill its arguments.", "labels": [], "entities": []}, {"text": "This is done with a two-step process to support compositional semantics.", "labels": [], "entities": []}, {"text": "The meaning of a multi-word phrase or sentence is represented as a joint likelihood of the HMMs for the words in that phrase or sentence.", "labels": [], "entities": []}, {"text": "Compositionality is handled by linking or coindexing the arguments of the conjoined HMMs.", "labels": [], "entities": [{"text": "Compositionality", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9430547952651978}]}, {"text": "Thus a sentence like The person to the left of the backpack approached the trashcan would be represented as a conjunction of person(p 0 ), to-the-left-of(p 0 , p 1 ), backback(p 1 ), approached(p 0 , p 2 ), and trash-can(p 2 ) over the three participants p 0 , p 1 , and p 2 . This whole sentence is then grounded in a particular video by mapping these participants to particular tracks and instantiating the associated HMMs over those tracks, by computing the feature vectors for each HMM from the tracks chosen to fill its arguments.", "labels": [], "entities": []}, {"text": "Our algorithm makes six assumptions.", "labels": [], "entities": []}, {"text": "First, we assume that we know the part of speech Cm associated with each lexical entry m, along with the part-of-speech dependent number of states I c in the HMMs used to represent word meanings in that part of speech, the part-of-speech dependent number of features N c in the feature vectors used by HMMs to represent word meanings in that part of speech, and the part-of-speech dependent feature-vector computation \u03a6 c used to compute the features used by HMMs to represent word meanings in that part of speech.", "labels": [], "entities": []}, {"text": "Second, we pair individual sentences each with a short video clip that depicts that sentence.", "labels": [], "entities": []}, {"text": "The algorithm is notable to determine the alignment between multiple sentences and longer video segments.", "labels": [], "entities": []}, {"text": "Note that there is no requirement that the video depict only that sentence.", "labels": [], "entities": []}, {"text": "Other objects maybe present and other events may occur.", "labels": [], "entities": []}, {"text": "In fact, nothing precludes a training corpus with multiple copies of the same video, each paired with a different sentence describing a different aspect of that video.", "labels": [], "entities": []}, {"text": "Moreover, our algorithm potentially can handle a small amount of noise, where a video clip is paired with an incorrect sentence that the video does not depict.", "labels": [], "entities": []}, {"text": "Third, we assume that we already have (pre-trained) low-level object detectors capable of detecting instances of our target event participants in individual frames of the video.", "labels": [], "entities": []}, {"text": "We allow such detections to be unreliable; our method can handle a moderate amount of false positives and false negatives.", "labels": [], "entities": []}, {"text": "We do not need to know the mapping from these object-detection classes to words; our algorithm determines that.", "labels": [], "entities": []}, {"text": "Fourth, we assume that we know the arity of each word in the corpus, i.e., the number of arguments that that word takes.", "labels": [], "entities": []}, {"text": "For example, we assume that we know that the word person(\u03b1) takes one argument and the word approached(\u03b1, \u03b2) takes two arguments.", "labels": [], "entities": []}, {"text": "Fifth, we assume that we know the total number of distinct participants that collectively fill all of the arguments for all of the words in each training sentence.", "labels": [], "entities": []}, {"text": "For example, for the sentence The person to the left of the backpack approached the trash-can, we assume that we know that there are three distinct objects that participate in the event denoted.", "labels": [], "entities": []}, {"text": "Sixth, we assume that we know the argument-to-participant mapping for each training sentence.", "labels": [], "entities": []}, {"text": "Thus, for example, for the above sentence we would know person(p 0 ), to-the-left-of(p 0 , p 1 ), backback(p 1 ), approached(p 0 , p 2 ), and trash-can(p 2 ).", "labels": [], "entities": []}, {"text": "The latter two items can be determined by parsing the sentence, which is what we do.", "labels": [], "entities": []}, {"text": "One can imagine learning the ability to automatically perform the latter two items, and even the fourth item above, by learning the grammar and the part of speech of each word, such as done by.", "labels": [], "entities": []}, {"text": "We leave such for future work.", "labels": [], "entities": []}, {"text": "illustrates a single frame from a potential training sample provided as input to our learner.", "labels": [], "entities": []}, {"text": "It consists of a video clip paired with a sentence, where the arguments of the words in the sentence are mapped to participants.", "labels": [], "entities": []}, {"text": "From a sequence of such training samples, our learner determines the objects tracks and the mapping from participants to those tracks, together with the meanings of the words.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 generally describes our problem of lexical acquisition from video.", "labels": [], "entities": [{"text": "lexical acquisition from video", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.8025096952915192}]}, {"text": "Section 3 introduces our work on the sentence tracker, a method for jointly tracking the motion of multiple objects in a video that participate in a sententiallyspecified event.", "labels": [], "entities": [{"text": "sentence tracker", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.7049573510885239}]}, {"text": "Section 4 elaborates on the details of our problem formulation in the context of this sentence tracker.", "labels": [], "entities": []}, {"text": "Section 5 describes how to generalize and extend the sentence tracker so that it can be used to support lexical acquisition.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7445053160190582}]}, {"text": "We demonstrate this lexical acquisition algorithm on a small example in Section 6.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7011427581310272}]}, {"text": "Finally, we conclude with a discussion in Section 7.", "labels": [], "entities": []}, {"text": "Figure 1: An illustration of our problem.", "labels": [], "entities": []}, {"text": "Each word in the sentence has one or more arguments (\u03b1 and possibly \u03b2), each argument of each word is assigned to a participant (p 0 , . .", "labels": [], "entities": []}, {"text": ", p 3 ) in the event described by the sentence, and each participant can be assigned to any object track in the video.", "labels": [], "entities": []}, {"text": "This figure shows a possible (but erroneous) interpretation of the sentence where the mapping is: and p 3 \u2192 Track 2, which might (incorrectly) lead the learner to conclude that the word person maps to the backpack, the word backpack maps to the chair, the word trash-can maps to the trash-can, and the word chair maps to the person.", "labels": [], "entities": []}], "datasetContent": [{"text": "We filmed 61 video clips (each 3-5 seconds at 640\u00d7480 resolution and 40 fps) that depict a variety of different compound events.", "labels": [], "entities": []}, {"text": "Each clip depicts multiple simultaneous events between some: The grammar used for our annotation and generation.", "labels": [], "entities": []}, {"text": "Our lexicon contains 1 determiner, 4 nouns, 2 spatial relation prepositions, 4 verbs, 2 adverbs, and 2 motion prepositions fora total of 15 lexical entries over 6 parts of speech.", "labels": [], "entities": []}, {"text": "subset of four objects: a person, a backpack, a chair, and a trash-can.", "labels": [], "entities": []}, {"text": "These clips were filmed in three different outdoor environments which we use for cross validation.", "labels": [], "entities": [{"text": "cross validation", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.8731544315814972}]}, {"text": "We manually annotated each video with several sentences that describe what occurs in that video.", "labels": [], "entities": []}, {"text": "The sentences were constrained to conform to the grammar in.", "labels": [], "entities": []}, {"text": "Our corpus of 159 training samples pairs some videos with more than one sentence and some sentences with more than one video, with an average of 2.6 sentences per video 1 . We model and learn the semantics of all words except determiners.", "labels": [], "entities": []}, {"text": "specifies the arity, the state number I c , and the features computed by \u03a6 c for the semantic models for words of each part of speech c.", "labels": [], "entities": []}, {"text": "While we specify a different subset of features for each part of speech, we presume that, in principle, with enough training data, we could include all features in all parts of speech and automatically learn which ones are noninformative and lead to uniform distributions.", "labels": [], "entities": []}, {"text": "We use an off-the-shelf object detector) which outputs detections in the form of scored axis-aligned rectangles.", "labels": [], "entities": []}, {"text": "We trained four object detectors, one for each of the four object classes in our corpus: person, backpack, chair, and trashcan.", "labels": [], "entities": [{"text": "object detectors", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.6816955804824829}]}, {"text": "For each frame, we pick the two highestscoring detections produced by each object detector and pool the results yielding eight detections per frame.", "labels": [], "entities": []}, {"text": "Having a larger pool of detections per frame can better compensate for false negatives in the object detection and potentially yield smoother tracks but it increases the size of the lattice and the concomitant running time and does not lead to appreciably better performance on our corpus.", "labels": [], "entities": [{"text": "object detection", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.720547541975975}]}, {"text": "We compute continuous features, such as velocity, distance, size ratio, and x-position solely from the detection rectangles and quantize the features into bins as follows: velocity To reduce noise, we compute the velocity of a participant by averaging the optical flow in the detection rectangle.", "labels": [], "entities": []}, {"text": "The velocity magnitude is quantized into 5 levels: absolutely stationary, stationary, moving, fast moving, and quickly.", "labels": [], "entities": []}, {"text": "The velocity orientation is quantized into 4 directions: left, up, right, and down.", "labels": [], "entities": []}, {"text": "distance We compute the Euclidean distance between the detection centers of two participants, which is quantized into 3 levels: near, normal, and faraway.", "labels": [], "entities": []}, {"text": "size ratio We compute the ratio of detection area of the first participant to the detection area of the second participant, quantized into 2 possibilities: larger/smaller than.", "labels": [], "entities": []}, {"text": "x-position We compute the difference between the x-coordinates of the participants, quantized into 2 possibilities: to the left/right of.", "labels": [], "entities": []}, {"text": "The binning process was determined by a preprocessing step that clustered a subset of the training data.", "labels": [], "entities": [{"text": "binning", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9541454315185547}]}, {"text": "We also incorporate the index of the detector that produced the detection as a feature.", "labels": [], "entities": []}, {"text": "The particular features computed for each part of speech are given in.", "labels": [], "entities": []}, {"text": "Note that while we use English phrases, like to the left of, to refer to particular bins of particular features, and we have object detectors which we train on samples of a particular object class such as backpack, such phrases are only mnemonic of the clustering and object-detector training process.", "labels": [], "entities": []}, {"text": "We do not have a fixed correspondence between the lexical entries and any particular feature value.", "labels": [], "entities": []}, {"text": "Moreover, that correspondence need not be oneto-one: a given lexical entry may correspond to a (time variant) constellation of feature values and any given feature value may participate in the meaning of multiple lexical entries.", "labels": [], "entities": []}, {"text": "We perform a three-fold cross validation, taking the test data for each fold to be the videos filmed in a given outdoor environment and the training data for that fold to be all training samples that contain other videos.", "labels": [], "entities": []}, {"text": "For testing, we hand selected 24 sentences generated by the grammar in, where each sentence is true for at least one test video.", "labels": [], "entities": []}, {"text": "Half of these sentences (designated NV) contain only nouns and verbs while the other half (designated ALL) contain other parts of speech.", "labels": [], "entities": []}, {"text": "The latter are longer and more complicated than the former.", "labels": [], "entities": []}, {"text": "We score each testing video paired with every sentence in both NV and ALL.", "labels": [], "entities": []}, {"text": "To evaluate our results, we manually annotated the correctness of each such pair.", "labels": [], "entities": []}, {"text": "Video-sentence pairs could be scored with Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.8777648210525513}]}, {"text": "4. However, the score depends on the sentence length, the collective numbers of states and features in the HMMs for words in that sentence, and the length of the video clip.", "labels": [], "entities": []}, {"text": "To render the scores comparable across such variation we incorporate a sentence prior to the per-frame score: In the above, Z C S r,l ,n is the number of bins for the nth feature of S r,l of part of speech C S r,l and   The scores are thresholded to decide hits, which together with the manual annotations, can generate TP, TN, FP, and FN counts.", "labels": [], "entities": [{"text": "TP", "start_pos": 320, "end_pos": 322, "type": "METRIC", "confidence": 0.9546170830726624}, {"text": "FP", "start_pos": 328, "end_pos": 330, "type": "METRIC", "confidence": 0.8237794041633606}, {"text": "FN counts", "start_pos": 336, "end_pos": 345, "type": "METRIC", "confidence": 0.9581093788146973}]}, {"text": "We select the threshold that leads to the maximal F1 score on the training set, use this threshold to compute F1 scores on the test set in each fold, and average F1 scores across the folds.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9588266313076019}, {"text": "F1", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.9973260164260864}, {"text": "F1", "start_pos": 162, "end_pos": 164, "type": "METRIC", "confidence": 0.9974310994148254}]}, {"text": "The F1 scores are listed in the column labeled Our in.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9681337475776672}]}, {"text": "For comparison, we also report F1 scores for three baselines: Chance, Blind, and Hand.", "labels": [], "entities": [{"text": "F1", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9996291399002075}, {"text": "Blind", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.8946518301963806}, {"text": "Hand", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9439973831176758}]}, {"text": "The Chance baseline randomly classifies a video-sentence pair as a hit with probability 0.5.", "labels": [], "entities": [{"text": "Chance baseline", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7851809859275818}]}, {"text": "The Blind baseline determines hits by potentially looking at the sentence but never looking at the video.", "labels": [], "entities": [{"text": "Blind baseline", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9241837561130524}]}, {"text": "We can find an upper bound on the F1 score that any blind method could have on each of our test sets by solving a 0-1 fractional programming problem (see Appendix A for details).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.985853761434555}]}, {"text": "The Hand baseline determines hits with hand-coded HMMs, carefully designed to yield what we believe is near-optimal performance.", "labels": [], "entities": [{"text": "Hand baseline", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.6133694350719452}]}, {"text": "As can be seen from, our trained models perform substantially better than the Chance and Blind baselines and approach the performance of the ideal Hand baseline.", "labels": [], "entities": [{"text": "Hand baseline", "start_pos": 147, "end_pos": 160, "type": "DATASET", "confidence": 0.8916691839694977}]}, {"text": "One can further see from the ROC curves in, comparing the trained and hand-written models on both NV and ALL, that the trained models are close to optimal.", "labels": [], "entities": [{"text": "ROC", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.8210262060165405}]}, {"text": "Note that performance on ALL exceeds that on NV with the trained models.", "labels": [], "entities": []}, {"text": "This is because longer sentences with varied parts of speech incorporate more information into the scoring process.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: F1 scores of different methods.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9687356650829315}]}]}