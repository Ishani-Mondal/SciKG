{"title": [{"text": "Shallow Local Multi Bottom-up Tree Transducers in Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.7594455083211263}]}], "abstractContent": [{"text": "We present anew translation model integrating the shallow local multi bottom-up tree transducer.", "labels": [], "entities": []}, {"text": "We perform a large-scale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree base-line on the WMT 2009 English \u2192 German translation task.", "labels": [], "entities": [{"text": "WMT 2009 English \u2192 German translation task", "start_pos": 158, "end_pos": 200, "type": "DATASET", "confidence": 0.80766042641231}]}, {"text": "As an additional contribution we make the developed software and complete tool-chain publicly available for further experimentation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Besides phrase-based machine translation systems (, syntax-based systems have become widely used because of their ability to handle non-local reordering.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.6959908405939738}]}, {"text": "Those systems use synchronous context-free grammars, synchronous tree substitution grammars or even more powerful formalisms like synchronous tree-sequence substitution grammars.", "labels": [], "entities": []}, {"text": "However, those systems use linguistic syntactic annotation at different levels.", "labels": [], "entities": []}, {"text": "For example, the systems proposed by and use no linguistic information and are syntactic in a structural sense only. and use syntactic annotations on the source language side and show significant improvements in translation quality.", "labels": [], "entities": []}, {"text": "Using syntax exclusively on the target language side has also been successfully tried by and.", "labels": [], "entities": []}, {"text": "Nowadays, open-source toolkits such as Moses () offer syntax-based components (, which allow experiments without expert knowledge.", "labels": [], "entities": []}, {"text": "The improvements observed for systems using syntactic annotation on either the source or the target language side naturally led to experiments with models that use syntactic annotations on both sides.", "labels": [], "entities": []}, {"text": "However, as noted by,, and, the integration of syntactic information on both sides tends to decrease translation quality because the systems become too restrictive.", "labels": [], "entities": [{"text": "translation", "start_pos": 101, "end_pos": 112, "type": "TASK", "confidence": 0.9543339610099792}]}, {"text": "Several strategies such as (i) using parse forests instead of single parses () or (ii) soft syntactic constraints) have been developed to alleviate this problem.", "labels": [], "entities": []}, {"text": "Another successful approach has been to switch to more powerful formalisms, which allow the extraction of more general rules.", "labels": [], "entities": []}, {"text": "A particularly powerful model is the non-contiguous version of synchronous tree-sequence substitution grammars (STSSG) of,, and, which allows sequences of trees on both sides of the rules [see also].", "labels": [], "entities": [{"text": "synchronous tree-sequence substitution grammars (STSSG)", "start_pos": 63, "end_pos": 118, "type": "TASK", "confidence": 0.7364194393157959}]}, {"text": "The multi bottom-up tree transducer (MBOT) of and offers a middle ground between traditional syntax-based models and STSSG.", "labels": [], "entities": [{"text": "multi bottom-up tree transducer (MBOT)", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.6831795232636588}]}, {"text": "Roughly speaking, an MBOT is an STSSG, in which all the discontinuities must occur on the target language side.", "labels": [], "entities": []}, {"text": "This restriction yields many algorithmic advantages over both the traditional models as well as STSSG as demonstrated by.", "labels": [], "entities": []}, {"text": "Formally, they are expressive enough to express all sensible translations (Maletti, 2012) . displays sample rules of the MBOT variant, called MBOT, that we use (in a graphical representation of the trees and the alignment).", "labels": [], "entities": [{"text": "MBOT", "start_pos": 121, "end_pos": 125, "type": "DATASET", "confidence": 0.8468145728111267}, {"text": "MBOT", "start_pos": 142, "end_pos": 146, "type": "DATASET", "confidence": 0.8505959510803223}]}, {"text": "In this contribution, we report on our novel statistical machine translation system that uses an MBOT-based translation model.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6112485826015472}, {"text": "MBOT-based translation", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.6997970640659332}]}, {"text": "The theoretical foundations of MBOT and their integration into our translation model are presented in Sections 2 and 3.", "labels": [], "entities": [{"text": "MBOT", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.49790629744529724}]}, {"text": "In order to empirically evaluate the MBOT model, we implemented a machine trans-: Example tree t with indicated positions.", "labels": [], "entities": [{"text": "MBOT", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.7101345062255859}]}, {"text": "We have t(21) = VBD and t| 221 is the subtree marked in red.", "labels": [], "entities": [{"text": "VBD", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.7360134720802307}]}, {"text": "lation system that we are going to make available to the public.", "labels": [], "entities": []}, {"text": "We implemented MBOT inside the syntax-based component of the Moses open source toolkit.", "labels": [], "entities": [{"text": "Moses open source toolkit", "start_pos": 61, "end_pos": 86, "type": "DATASET", "confidence": 0.9083085656166077}]}, {"text": "Section 4 presents the most important algorithms of our MBOT decoder.", "labels": [], "entities": [{"text": "MBOT decoder", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9008042216300964}]}, {"text": "We evaluate our new system on the WMT 2009 shared translation task English \u2192 German.", "labels": [], "entities": [{"text": "WMT 2009 shared translation task English \u2192 German", "start_pos": 34, "end_pos": 83, "type": "TASK", "confidence": 0.7287026643753052}]}, {"text": "The translation quality is automatically measured using BLEU scores, and we confirm the findings by providing linguistic evidence (see Section 5).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9970784187316895}]}, {"text": "Note that in contrast to several previous approaches, we perform large scale experiments by training systems with approx. 1.5 million parallel sentences.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Number of rules used in decoding test  (lex: only lexical items; non-term: at least one  nonterminal).", "labels": [], "entities": []}, {"text": " Table 3: Number of k-discontiguous rules.", "labels": [], "entities": []}]}