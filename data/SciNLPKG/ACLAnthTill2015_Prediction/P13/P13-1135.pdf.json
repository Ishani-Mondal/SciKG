{"title": [{"text": "Dirt Cheap Web-Scale Parallel Text from the Common Crawl", "labels": [], "entities": []}], "abstractContent": [{"text": "Parallel text is the fuel that drives modern machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7365525960922241}]}, {"text": "The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies.", "labels": [], "entities": []}, {"text": "We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon's Elastic Cloud.", "labels": [], "entities": []}, {"text": "Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500.", "labels": [], "entities": [{"text": "STRAND", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.7308104634284973}]}, {"text": "Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets.", "labels": [], "entities": []}, {"text": "Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 223, "end_pos": 227, "type": "METRIC", "confidence": 0.9986384510993958}]}, {"text": "We make our code and data available for other researchers seeking to mine this rich new data resource.", "labels": [], "entities": []}], "introductionContent": [{"text": "A key bottleneck in porting statistical machine translation (SMT) technology to new languages and domains is the lack of readily available parallel corpora beyond curated datasets.", "labels": [], "entities": [{"text": "porting statistical machine translation (SMT)", "start_pos": 20, "end_pos": 65, "type": "TASK", "confidence": 0.7931009403296879}]}, {"text": "For a handful of language pairs, large amounts of parallel data are readily available, ordering in the hundreds of millions of words for Chinese-English and ArabicEnglish, and in tens of millions of words for many European languages).", "labels": [], "entities": []}, {"text": "In each case, much of this data consists of government and news text.", "labels": [], "entities": []}, {"text": "However, for most language pairs and domains there is little to no curated parallel data available.", "labels": [], "entities": []}, {"text": "Hence discovery of parallel data is an important first step for translation between most of the world's languages.", "labels": [], "entities": [{"text": "translation", "start_pos": 64, "end_pos": 75, "type": "TASK", "confidence": 0.9775784015655518}]}, {"text": "The Web is an important source of parallel text.", "labels": [], "entities": []}, {"text": "Many websites are available in multiple languages, and unlike other potential sourcessuch as multilingual news feeds (Munteanu and) or Wikipedia (it is common to find document pairs that are direct translations of one another.", "labels": [], "entities": [{"text": "Munteanu", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.924267053604126}]}, {"text": "This natural parallelism simplifies the mining task, since few resources or existing corpora are needed at the outset to bootstrap the extraction process.", "labels": [], "entities": []}, {"text": "Parallel text mining from the Web was originally explored by individuals or small groups of academic researchers using search engines.", "labels": [], "entities": [{"text": "Parallel text mining from the Web", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8483713269233704}]}, {"text": "However, anything more sophisticated generally requires direct access to web-crawled documents themselves along with the computing power to process them.", "labels": [], "entities": []}, {"text": "For most researchers, this is prohibitively expensive.", "labels": [], "entities": []}, {"text": "As a consequence, web-mined parallel text has become the exclusive purview of large companies with the computational resources to crawl, store, and process the entire Web.", "labels": [], "entities": []}, {"text": "To put web-mined parallel text back in the hands of individual researchers, we mine parallel text from the Common Crawl, a regularly updated 81-terabyte snapshot of the public internet hosted on Amazon's Elastic Cloud (EC2) service.", "labels": [], "entities": []}, {"text": "Using the Common Crawl completely removes the bottleneck of web crawling, and makes it possible to run algorithms on a substantial portion of the web at very low cost.", "labels": [], "entities": [{"text": "web crawling", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.7495100200176239}]}, {"text": "Starting from nothing other than a set of language codes, our extension of the STRAND algorithm identifies potentially parallel documents using cues from URLs and document content ( \u00a72).", "labels": [], "entities": [{"text": "STRAND", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.8923385739326477}]}, {"text": "We conduct an extensive empirical exploration of the web-mined data, demonstrating coverage in a wide variety of languages and domains ( \u00a73).", "labels": [], "entities": [{"text": "coverage", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9400252103805542}]}, {"text": "Even without extensive pre-processing, the data improves translation performance on strong baseline news translation systems in five different language pairs ( \u00a74).", "labels": [], "entities": [{"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9652611613273621}]}, {"text": "On general domain and speech translation tasks where test conditions substantially differ from standard government and news training text, web-mined training data improves performance substantially, resulting in improvements of up to 1.5 BLEU on standard test sets, and 5 BLEU on test sets outside of the news domain.", "labels": [], "entities": [{"text": "general domain and speech translation", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.6211334466934204}, {"text": "BLEU", "start_pos": 238, "end_pos": 242, "type": "METRIC", "confidence": 0.9982534050941467}, {"text": "BLEU", "start_pos": 272, "end_pos": 276, "type": "METRIC", "confidence": 0.9975281357765198}]}], "datasetContent": [{"text": "For our SMT experiments, we use the Moses toolkit (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9955357313156128}]}, {"text": "In these experiments, a baseline system is trained on an existing parallel corpus, and the experimental system is trained on the baseline corpus plus the mined parallel data.", "labels": [], "entities": []}, {"text": "In all experiments we include the target side of the mined parallel data in the language model, in order to distinguish whether results are due to influences from parallel or monolingual data.", "labels": [], "entities": []}, {"text": "Index Most Likely Tokens 1 glitter graphics profile comments share love size girl friends happy blingee cute anime twilight sexy emo 2 subtitles online web users files rar movies prg akas dwls xvid dvdrip avi results download eng cd movie 3 miles hotels city search hotel homepage list overview select tokyo discount destinations china japan 4 english language students details skype american university school languages words england british college 5 translation japanese english chinese dictionary french german spanish korean russian italian dutch 6 products services ni system power high software design technology control national applications industry 7 en de el instructions amd hyper riv saab kfreebsd poland user fr pln org wikimedia pl commons fran norway 8 information service travel services contact number time account card site credit company business terms 9 people time life day good years work make god give lot long world book today great year end things 10 show km map hotels de hotel beach spain san italy resort del mexico rome portugal home santa berlin la 11 rotary international world club korea foundation district business year global hong kong president ri 12 hotel reviews stay guest rooms service facilities room smoking submitted customers desk score united hour 13 free site blog views video download page google web nero internet http search news links category tv 14 casino game games play domaine ago days music online poker free video film sports golf live world tags bet 15 water food attribution health mango japan massage medical body baby natural yen commons traditional 16 file system windows server linux installation user files set debian version support program install type 17 united kingdom states america house london street park road city inn paris york st france home canada 18 km show map hotels hotel featured search station museum amsterdam airport centre home city rue germany 19 hotel room location staff good breakfast rooms friendly nice clean great excellent comfortable helpful 20 de la en le el hotel es het del und die il est der les des das du para: A list of 20 topics generated using the MALLET toolkit and their most likely tokens.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Automatic evaluation of precision  through language identification for several lan- guages paired with English.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9986737966537476}, {"text": "language identification", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.6938054114580154}]}, {"text": " Table 6: A sample of topics along with the number of Europarl and CommonCrawl documents where  they are the most likely topic in the mixture. We include topics that are mostly found in Europarl or  CommonCrawl, and some that are somewhat prominent in both.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.9641157388687134}, {"text": "CommonCrawl documents", "start_pos": 67, "end_pos": 88, "type": "DATASET", "confidence": 0.8396325707435608}, {"text": "Europarl", "start_pos": 186, "end_pos": 194, "type": "DATASET", "confidence": 0.970346212387085}, {"text": "CommonCrawl", "start_pos": 199, "end_pos": 210, "type": "DATASET", "confidence": 0.8575927019119263}]}, {"text": " Table 9: BLEU scores for French-English and  English-French before and after adding the mined  parallel data to systems trained on data from  WMT data including the French-English Giga- word", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992132186889648}, {"text": "WMT data", "start_pos": 143, "end_pos": 151, "type": "DATASET", "confidence": 0.830875426530838}, {"text": "French-English Giga- word", "start_pos": 166, "end_pos": 191, "type": "DATASET", "confidence": 0.6045777872204781}]}, {"text": " Table 11: n-gram coverage percentages (up to 4- grams) of the source side of our test sets given our  different parallel training corpora computed at the  type level.", "labels": [], "entities": []}, {"text": " Table 12: BLEU scores for Spanish-English be- fore and after adding the mined parallel data to a  baseline Europarl system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9991981387138367}, {"text": "Europarl system", "start_pos": 108, "end_pos": 123, "type": "DATASET", "confidence": 0.9586361944675446}]}]}