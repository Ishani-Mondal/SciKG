{"title": [{"text": "Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures", "labels": [], "entities": [{"text": "Combining Referring Expression Generation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7594982087612152}, {"text": "Surface Realization", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.739578515291214}]}], "abstractContent": [{"text": "We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization.", "labels": [], "entities": [{"text": "discourse-level referring expression generation", "start_pos": 45, "end_pos": 92, "type": "TASK", "confidence": 0.621020644903183}, {"text": "sentence-level surface realization", "start_pos": 97, "end_pos": 131, "type": "TASK", "confidence": 0.6690002282460531}]}, {"text": "We present a data set of Ger-man articles annotated with deep syntax and referents, including some types of implicit referents.", "labels": [], "entities": []}, {"text": "Our experiments compare several architectures varying the order of a set of trainable modules.", "labels": [], "entities": []}, {"text": "The results suggest that a revision-based pipeline, with intermediate linearization, significantly out-performs standard pipelines or a parallel architecture.", "labels": [], "entities": []}], "introductionContent": [{"text": "Generating well-formed linguistic utterances from an abstract non-linguistic input involves making a multitude of conceptual, discourse-level as well as sentence-level, lexical and syntactic decisions.", "labels": [], "entities": [{"text": "Generating well-formed linguistic utterances", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7278751879930496}]}, {"text": "Work on rule-based natural language generation (NLG) has explored a number of ways to combine these decisions in an architecture, ranging from integrated systems where all decisions happen jointly to strictly sequential pipelines).", "labels": [], "entities": [{"text": "rule-based natural language generation (NLG)", "start_pos": 8, "end_pos": 52, "type": "TASK", "confidence": 0.7832424896103996}]}, {"text": "While integrated or interactive systems typically face issues with efficiency and scalability, they can directly account for interactions between discourse-level planning and linguistic realization.", "labels": [], "entities": []}, {"text": "For instance, Rubinoff (1992) mentions Example (1) where the sentence planning component needs to have access to the lexical knowledge that \"order\" and not \"home\" can be realized as a verb in English.", "labels": [], "entities": []}, {"text": "*John homed him with an order. b. John ordered him home.", "labels": [], "entities": []}, {"text": "In recent data-driven generation research, the focus has somewhat shifted from full data-to-text systems to approaches that isolate well-defined subproblems from the NLG pipeline.", "labels": [], "entities": []}, {"text": "In particular, the tasks of surface realization and referring expression generation (REG) have received increasing attention using a number of available annotated data sets).", "labels": [], "entities": [{"text": "surface realization", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.730824813246727}, {"text": "referring expression generation (REG)", "start_pos": 52, "end_pos": 89, "type": "TASK", "confidence": 0.7817224562168121}]}, {"text": "While these single-task approaches have given rise to many insights about algorithms and corpus-based modelling for specific phenomena, they can hardly deal with aspects of the architecture and interaction between generation levels.", "labels": [], "entities": []}, {"text": "This paper suggests a middle ground between full data-to-text and single-task generation, combining two well-studied NLG problems.", "labels": [], "entities": [{"text": "single-task generation", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.7869091033935547}]}, {"text": "We integrate a discourse-level approach to REG with sentence-level surface realization in a data-driven framework.", "labels": [], "entities": [{"text": "REG", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9641764760017395}, {"text": "sentence-level surface realization", "start_pos": 52, "end_pos": 86, "type": "TASK", "confidence": 0.665073573589325}]}, {"text": "We address this integrated task with a set of components that can be trained on flexible inputs which allows us to systematically explore different ways of arranging the components in a generation architecture.", "labels": [], "entities": []}, {"text": "Our main goal is to investigate how different architectural set-ups account for interactions between generation decisions at the level of referring expressions (REs), syntax and word order.", "labels": [], "entities": []}, {"text": "Our basic set-up is inspired from the Generating Referring Expressions in Context (GREC) tasks, where candidate REs have to be assigned to instances of a referent in a Wikipedia article.", "labels": [], "entities": []}, {"text": "We have created a dataset of German texts with annotations that extend this standard in three substantial ways: (i) our domain consists of articles about robbery events that mainly involve two main referents, a victim and a perpetrator (perp), (ii) annotations include deep and shallow syntactic relations similar to the representations used in) (iii) annotations include empty referents, as e.g. in passives and nominalizations directing attention to the phenomenon of implicit reference, which is largely understudied in NLG.", "labels": [], "entities": [{"text": "NLG", "start_pos": 523, "end_pos": 526, "type": "DATASET", "confidence": 0.897929847240448}]}, {"text": "presents an example fora deep syntax tree with underspecified RE Applying a strictly sequential pipeline on our data, we observe incoherent system output that is related to an interaction of generation levels, very similar to the interleaving between sentence planning and lexicalization in Example (1).", "labels": [], "entities": [{"text": "RE", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.9360741376876831}]}, {"text": "A pipeline that first inserts REs into the underspecified tree in, then generates syntax and finally linearizes, produces inappropriate sentences like (2-a).", "labels": [], "entities": []}, {"text": "Sentence (2-a) is incoherent because the syntactic surface obscurs the intended meaning that \"two italians\" and \"the two men\" refer to the same referent.", "labels": [], "entities": []}, {"text": "In order to generate the natural Sentence (2-b), the RE component needs information about linear precedence of the two perp instances and the nominalization of \"attack\".", "labels": [], "entities": []}, {"text": "These types of interactions between referential and syntactic realization have been thoroughly discussed in theoretical accounts of textual coherence, as e.g. Centering Theory (.", "labels": [], "entities": [{"text": "referential and syntactic realization", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.6292496249079704}]}, {"text": "The integrated modelling of REG and surface realization leads to a considerable expansion of the choice space.", "labels": [], "entities": [{"text": "REG", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.8975434303283691}, {"text": "surface realization", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7349531054496765}]}, {"text": "Ina sentence with 3 referents that each have 10 RE candidates and can be freely ordered, the number of surface realizations increases from 6 to 6\u00b710 3 , assuming that the remaining words cannot be syntactically varied.", "labels": [], "entities": []}, {"text": "Thus, even when the generation problem is restricted to these tasks, a fully integrated architecture faces scalability issues on realistic corpus data.", "labels": [], "entities": []}, {"text": "In this work, we assume a modular set-up of the generation system that allows fora flexible ordering of the single components.", "labels": [], "entities": []}, {"text": "Our experiments vary 3 parameters of the generation architecture: 1) the sequential order of the modules, 2) parallelization of modules, 3) joint vs. separate modelling of implicit referents.", "labels": [], "entities": []}, {"text": "Our results suggest that the interactions between RE and syntax can be modelled in sequential generation architecture where the RE component has access to information about syntactic realization and an approximative, intermediate linearization.", "labels": [], "entities": []}, {"text": "Such a system is reminiscent of earlier work in rulebased generation that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation).", "labels": [], "entities": [{"text": "rulebased generation", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7264593839645386}]}], "datasetContent": [{"text": "In this experimental section, we provide a corpusbased evaluation of the generation components and architectures introduced in Section 4.", "labels": [], "entities": []}, {"text": "In the following, Section 5.1 presents the details of our evaluation methodology.", "labels": [], "entities": []}, {"text": "In Section 5.2, we discuss the first experiment that evaluates the pipeline architectures and the single components on oracle inputs.", "labels": [], "entities": []}, {"text": "Section 5.3 describes an experiment which compares the parallel and the revision-based architecture against the pipeline.", "labels": [], "entities": []}, {"text": "In Section 5.4, we compare two methods for dealing with the implicit referents in our data.", "labels": [], "entities": []}, {"text": "Section 5.5 provides some general discussion of the results.", "labels": [], "entities": []}, {"text": "We split our data set into 10 splits of 20 articles.", "labels": [], "entities": []}, {"text": "We use one split as the development set, and crossvalidate on the remaining splits.", "labels": [], "entities": []}, {"text": "In each case, the downstream modules of the pipeline will be trained on the jackknifed training set.", "labels": [], "entities": []}, {"text": "Text normalization: We carryout automatic evaluation calculated on lemmatized text without punctuation, excluding additional effects that would be introduced from a morphology generation component.", "labels": [], "entities": [{"text": "Text normalization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7365278601646423}]}, {"text": "Measures: First, we use a number of evaluation measures familiar from previous generation shared tasks: 3.", "labels": [], "entities": []}, {"text": "RE Accuracy on String, proportion of REs selected by the system with a string identical to the RE string in the original corpus, as in ( 4.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.6001038551330566}]}, {"text": "RE Accuracy on Type, proportion of REs selected by the system with an RE type identical to the RE type in the original corpus, as in ( Second, we define a number of measures motivated by our specific set-up of the task: 1.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.6056896448135376}]}, {"text": "BLEUr, sentence-level BLEU computed on postprocessed output where predicted referring expressions for victim and perp are replaced in the sentences (both gold and predicted) by their original role label, this score does not penalize lexical mismatches between corpus and system REs 2.", "labels": [], "entities": [{"text": "BLEUr", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9805259108543396}, {"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9646616578102112}]}, {"text": "RE Accuracy on Impl, proportion of REs predicted correctly as implicit/non-implicit 3.", "labels": [], "entities": [{"text": "RE", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.6464941501617432}, {"text": "Accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.7080999612808228}, {"text": "Impl", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.8681671619415283}]}, {"text": "SYN Accuracy on String, proportion of shallow verb candidates selected by the system with a string identical to the verb string in the original corpus 4.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.7681648135185242}]}, {"text": "SYN Accuracy on Type, proportion of shallow verb candidates selected by the system with a syntactic category identical to the category in the original corpus", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.8369153738021851}]}], "tableCaptions": [{"text": " Table 1: Basic annotation statistics", "labels": [], "entities": []}, {"text": " Table 2: Evaluating pipeline architectures against the baseline and upper bounds", "labels": [], "entities": []}, {"text": " Table 4: RE generation from different input layers", "labels": [], "entities": [{"text": "RE generation", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8622685968875885}]}, {"text": " Table 5: Implicit reference and architectures", "labels": [], "entities": []}]}