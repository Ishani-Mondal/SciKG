{"title": [{"text": "Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction", "labels": [], "entities": [{"text": "Distant Supervision of Relation Extraction", "start_pos": 32, "end_pos": 74, "type": "TASK", "confidence": 0.9397832036018372}]}], "abstractContent": [{"text": "Distant supervision has attracted recent interest for training information extraction systems because it does not require any human annotation but rather employs existing knowledge bases to heuristically label a training corpus.", "labels": [], "entities": [{"text": "training information extraction", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6496695478757223}]}, {"text": "However, previous work has failed to address the problem of false negative training examples misla-beled due to the incompleteness of knowledge bases.", "labels": [], "entities": []}, {"text": "To tackle this problem, we propose a simple yet novel framework that combines a passage retrieval model using coarse features into a state-of-the-art relation extractor using multi-instance learning with fine features.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.8432228863239288}]}, {"text": "We adapt the information retrieval technique of pseudo-relevance feedback to expand knowledge bases, assuming entity pairs in top-ranked passages are more likely to express a relation.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.7382935881614685}]}, {"text": "Our proposed technique significantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.733809769153595}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9993374943733215}, {"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9950360655784607}]}], "introductionContent": [{"text": "A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision).", "labels": [], "entities": [{"text": "training information extraction", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6388751665751139}]}, {"text": "To combat the noisy training data produced by heuristic labeling in distant supervision, researchers () exploited multi-instance learning models.", "labels": [], "entities": []}, {"text": "Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it).", "labels": [], "entities": []}, {"text": "However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives.", "labels": [], "entities": []}, {"text": "aligned mentions true mentions.5% 2.7% 1.7% false negatives false positives In reality, knowledge bases are often incomplete, giving rise to numerous false negatives in the training data.", "labels": [], "entities": []}, {"text": "We sampled 1834 sentences that contain two entities in the New York Times 2006 corpus and manually evaluated whether they express any of a set of 50 common Freebase 1 relations.", "labels": [], "entities": [{"text": "New York Times 2006 corpus", "start_pos": 59, "end_pos": 85, "type": "DATASET", "confidence": 0.8489767432212829}]}, {"text": "As shown in, of the 133 (7.3%) sentences that truly express one of these relations, only 32 (1.7%) are covered by Freebase, leaving 101 (5.5%) false negatives.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 114, "end_pos": 122, "type": "DATASET", "confidence": 0.9626609086990356}]}, {"text": "Even for one of the most complete relations in Freebase, Employee-of (with more than 100,000 entity pairs), 6 out of 27 sentences with the pattern 'PERSON executive of ORGANIZATION' contain a fact that is not included in Freebase and are thus mislabeled as negative.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.9470650553703308}, {"text": "PERSON executive of ORGANIZATION", "start_pos": 148, "end_pos": 180, "type": "METRIC", "confidence": 0.7942468822002411}, {"text": "Freebase", "start_pos": 221, "end_pos": 229, "type": "DATASET", "confidence": 0.9715700745582581}]}, {"text": "These mislabelings dilute the discriminative capability of useful features and confuse the models.", "labels": [], "entities": []}, {"text": "In this paper, we will show how reducing this source of noise can significantly improve the performance of distant supervision.", "labels": [], "entities": []}, {"text": "In fact, our system corrects the relation labels of the above 6 sentences before training the relation extractor.", "labels": [], "entities": []}, {"text": "Encouraged by the recent success of simple methods for coreference resolution) and inspired by pseudo-relevance feedback () in the field of information retrieval, which expands or reformulates query terms based on the highest ranked documents of an initial query, we propose to increase the quality and quantity of training data generated by distant supervision for information extraction task using pseudo feedback.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.9528442323207855}, {"text": "information retrieval", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.73038549721241}, {"text": "information extraction task", "start_pos": 366, "end_pos": 393, "type": "TASK", "confidence": 0.82642928759257}]}, {"text": "As shown in, we expand an original knowledge base with possibly missing relation instances with information from the highest ranked sentences returned by a passage retrieval model ( ) trained on the same data.", "labels": [], "entities": []}, {"text": "We use coarse features for our passage retrieval model to aggressively expand the knowledge base for maximum recall; at the same time, we exploit a multi-instance learning model with fine features for relation extraction to handle the newly introduced false positives and maintain high precision.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8526058495044708}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9949310421943665}, {"text": "relation extraction", "start_pos": 201, "end_pos": 220, "type": "TASK", "confidence": 0.8830382227897644}, {"text": "precision", "start_pos": 286, "end_pos": 295, "type": "METRIC", "confidence": 0.997244119644165}]}, {"text": "Similar to iterative bootstrapping techniques, this mechanism uses the outputs of the first trained model to expand training data for the second model, but unlike bootstrapping it does not require iteration and avoids the problem of semantic drift.", "labels": [], "entities": []}, {"text": "We further note that iterative bootstrapping over a single distant supervision system is difficult, because state-of-the-art systems (, detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by.", "labels": [], "entities": []}, {"text": "We present a reliable and novel way to address these issues and achieve significant improvement over the MULTIR system), increasing recall from 47.7% to 61.2% at comparable precision.", "labels": [], "entities": [{"text": "MULTIR", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.528268039226532}, {"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9996141195297241}, {"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.997495710849762}]}, {"text": "The key to this success is the combination of two different views as in co-training: an information extraction technique with fine features for high precision and an information retrieval technique with coarse features for high recall.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.8083843886852264}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9871416091918945}, {"text": "recall", "start_pos": 228, "end_pos": 234, "type": "METRIC", "confidence": 0.9945043325424194}]}, {"text": "Our work is developed in parallel with, who take a very different approach by adding additional latent variables to a multi-instance multi-label model () to solve this same problem.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluating extraction accuracy, we follow the experimental setup of, and use their implementation of MULTIR 4 with 50 training iterations as our baseline.", "labels": [], "entities": [{"text": "extraction", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.9444900155067444}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.8949552774429321}, {"text": "MULTIR 4", "start_pos": 105, "end_pos": 113, "type": "DATASET", "confidence": 0.706638902425766}]}, {"text": "Our complete system, which we call IRMIE, combines our passage retrieval component with MULTIR.", "labels": [], "entities": [{"text": "IRMIE", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.8263360857963562}, {"text": "passage retrieval", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.8905299603939056}, {"text": "MULTIR", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.870540201663971}]}, {"text": "We use the same datasets as in and, which include 3-years of New York Times articles aligned with Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.9208676815032959}]}, {"text": "The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and: Overall sentential extraction performance evaluated on the original test set of and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall.", "labels": [], "entities": [{"text": "sentential extraction", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8910721838474274}, {"text": "sentential extraction", "start_pos": 158, "end_pos": 179, "type": "TASK", "confidence": 0.7371124625205994}, {"text": "recall", "start_pos": 333, "end_pos": 339, "type": "METRIC", "confidence": 0.999489426612854}]}, {"text": "We define Se as the sentences where some system extracted a relation and SF as the sentences that match the arguments of a fact in \u2206.", "labels": [], "entities": [{"text": "SF", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9534914493560791}]}, {"text": "The sentential precision and recall is computed on a randomly sampled set of sentences from Se \u222a SF , in which each sentence is manually labeled whether it expresses any relation in R. shows the precision/recall curves for MULTIR with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.8540558815002441}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9990435242652893}, {"text": "precision", "start_pos": 195, "end_pos": 204, "type": "METRIC", "confidence": 0.9923458099365234}, {"text": "recall", "start_pos": 205, "end_pos": 211, "type": "METRIC", "confidence": 0.7766114473342896}, {"text": "MULTIR", "start_pos": 223, "end_pos": 229, "type": "DATASET", "confidence": 0.4496926963329315}]}, {"text": "With the pseudo-relevance feedback from passage retrieval, IRMIE achieves significantly higher recall at a consistently high level of precision.", "labels": [], "entities": [{"text": "IRMIE", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.6801784038543701}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.999327540397644}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9974530339241028}]}, {"text": "At the highest recall point, IRMIE reaches 78.5% precision and 59.2% recall, for an F1 score of 68.9%.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.999663233757019}, {"text": "IRMIE", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9310423731803894}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9969778060913086}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9995866417884827}, {"text": "F1 score", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9849176704883575}]}, {"text": "Because the two types of lexical features used in our passage retrieval models are not used in MUL-TIR, we created another baseline MULTIRLEX by adding these features into MULTIR in order to rule out the improvement from additional information.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7942643761634827}, {"text": "MUL-TIR", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.8213088512420654}, {"text": "MULTIR", "start_pos": 172, "end_pos": 178, "type": "DATASET", "confidence": 0.8793329000473022}]}, {"text": "Note that the sentences are sampled from the union of Freebase matches and sentences from which some systems in extracted a relation.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.947696328163147}]}, {"text": "It underestimates the improvements of the newly developed systems in this paper.", "labels": [], "entities": []}, {"text": "We therefore also created anew test set of 1000 sentences by sampling from the union of Freebase matches and sentences where MULTIR-LEX or IRMIELEX extracted a relation.", "labels": [], "entities": [{"text": "IRMIELEX", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.8515869975090027}]}, {"text": "shows the overall precision and recall computed against these two test datasets, with and without adding lexical features into multi-instance learning models.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9995286464691162}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9993366599082947}]}, {"text": "The performance improvement by using pseudo-feedback is significant (p < 0.05) in McNemar's test for both datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overall sentential extraction performance evaluated on the original test set of", "labels": [], "entities": [{"text": "sentential extraction", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.9292932152748108}]}]}