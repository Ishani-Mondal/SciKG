{"title": [{"text": "A Structured Distributional Semantic Model for Event Co-reference", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods.", "labels": [], "entities": []}, {"text": "We argue that our model approximates meaning in compositional configurations more effectively than standard dis-tributional vectors or bag-of-words models.", "labels": [], "entities": []}, {"text": "We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outper-forms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature.", "labels": [], "entities": [{"text": "judging event coreferentiality", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.8073392311731974}]}], "introductionContent": [{"text": "Distributional Semantic Models (DSM) are popular in computational semantics.", "labels": [], "entities": [{"text": "Distributional Semantic Models (DSM)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7325553596019745}]}, {"text": "DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood.", "labels": [], "entities": []}, {"text": "They have been successfully used in a variety of NLP tasks including information retrieval (, question answering (, wordsense discrimination and disambiguation (), semantic similarity computation and selectional preference modeling.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.8115929961204529}, {"text": "question answering", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.8390595316886902}, {"text": "wordsense discrimination and disambiguation", "start_pos": 116, "end_pos": 159, "type": "TASK", "confidence": 0.7991146147251129}, {"text": "selectional preference modeling", "start_pos": 200, "end_pos": 231, "type": "TASK", "confidence": 0.7881241043408712}]}, {"text": "A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words.", "labels": [], "entities": []}, {"text": "Composing the * *Equally contributing authors distributions for \"Lincoln\", \"Booth\", and \"killed\" gives the same result regardless of whether the input is \"Booth killed Lincoln\" or \"Lincoln killed Booth\".", "labels": [], "entities": []}, {"text": "But as suggested by and others, modeling the distribution over preferential attachments for each syntactic relation separately yields greater expressive power.", "labels": [], "entities": []}, {"text": "Thus, to remedy the bag-of-words failing, we extend the generic DSM model to several relation-specific distributions over syntactic neighborhoods.", "labels": [], "entities": []}, {"text": "In other words, one can think of the Structured DSM (SDSM) representation of a word/phrase as several vectors defined over the same vocabulary, each vector representing the word's selectional preferences for its various syntactic arguments.", "labels": [], "entities": [{"text": "Structured DSM (SDSM) representation of a word/phrase", "start_pos": 37, "end_pos": 90, "type": "TASK", "confidence": 0.692105079239065}]}, {"text": "We argue that this representation not only captures individual word semantics more effectively than the standard DSM, but is also better able to express the semantics of compositional units.", "labels": [], "entities": []}, {"text": "We prove this on the task of judging event coreference.", "labels": [], "entities": [{"text": "judging event coreference", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.8648338119188944}]}, {"text": "Experimental results indicate that our model achieves greater predictive accuracy on the task than models that employ weaker forms of composition, as well as a baseline that relies on stateof-the-art window based word embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9619738459587097}]}, {"text": "This suggests that our formalism holds the potential of greater expressive power in problems that involve underlying semantic compositionality.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on two datasets and compare it against four baselines, two of which use window based distributional vectors and two that employ weaker forms of composition.", "labels": [], "entities": []}, {"text": "IC Event Coreference Corpus: The dataset ( , drawn from 100 news articles about violent events, contains manually created annotations for 2214 pairs of co-referent and noncoreferent events each.", "labels": [], "entities": [{"text": "IC Event Coreference Corpus", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.8786283731460571}]}, {"text": "Where available, events' semantic role-fillers for agent and patient are annotated as well.", "labels": [], "entities": []}, {"text": "When missing, empirical substitutes were obtained by querying the PropStore for the preferred word attachments.", "labels": [], "entities": [{"text": "PropStore", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.9371545314788818}]}, {"text": "EventCorefBank (ECB) corpus: This corpus (Bejan and Harabagiu, 2010) of 482 documents from Google News is clustered into 45 topics, with event coreference chains annotated over each topic.", "labels": [], "entities": [{"text": "Bejan and Harabagiu, 2010) of 482 documents from Google News", "start_pos": 42, "end_pos": 102, "type": "DATASET", "confidence": 0.6039264127612114}]}, {"text": "The event mentions are enriched with semantic roles to obtain the canonical event structure described above.", "labels": [], "entities": []}, {"text": "Positive instances are obtained by taking pairwise event mentions within each chain, and negative instances are generated from pairwise event mentions across chains, but within the same topic.", "labels": [], "entities": []}, {"text": "This results in 11039 positive instances and 33459 negative instances.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cross-validation Performance on IC and ECB dataset", "labels": [], "entities": [{"text": "ECB dataset", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.7815234065055847}]}]}