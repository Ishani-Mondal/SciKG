{"title": [{"text": "Minimum Bayes Risk based Answer Re-ranking for Question Answering", "labels": [], "entities": [{"text": "Minimum Bayes Risk based Answer Re-ranking", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.5421363363663355}, {"text": "Question Answering", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7715402841567993}]}], "abstractContent": [{"text": "This paper presents two minimum Bayes risk (MBR) based Answer Re-ranking (MBRAR) approaches for the question answering (QA) task.", "labels": [], "entities": [{"text": "Bayes risk (MBR) based Answer Re-ranking (MBRAR)", "start_pos": 32, "end_pos": 80, "type": "METRIC", "confidence": 0.9275878288529136}, {"text": "question answering (QA) task", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.8941230674584707}]}, {"text": "The first approach re-ranks single QA system's outputs by using a traditional MBR model, by measuring correlations between answer candidates ; while the second approach re-ranks the combined outputs of multiple QA systems with heterogenous answer extraction components by using a mixture model-based MBR model.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 240, "end_pos": 257, "type": "TASK", "confidence": 0.7260962128639221}]}, {"text": "Evaluations are performed on factoid questions selected from two different domains: Jeopardy ! and Web, and significant improvements are achieved on all data sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (), automatic speech recognition), parsing), etc.", "labels": [], "entities": [{"text": "Minimum Bayes Risk (MBR", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.7177645027637481}, {"text": "statistical machine translation", "start_pos": 129, "end_pos": 160, "type": "TASK", "confidence": 0.7118955254554749}, {"text": "automatic speech recognition", "start_pos": 165, "end_pos": 193, "type": "TASK", "confidence": 0.6712600986162821}, {"text": "parsing", "start_pos": 196, "end_pos": 203, "type": "TASK", "confidence": 0.9557415843009949}]}, {"text": "This work makes further exploration along this line of research, by applying MBR technique to question answering (QA).", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.8990026831626892}]}, {"text": "The function of atypical factoid question answering system is to automatically give answers to questions inmost case asking about entities, which usually consists of three key components: question understanding, passage retrieval, and answer extraction.", "labels": [], "entities": [{"text": "factoid question answering", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.577446440855662}, {"text": "question understanding", "start_pos": 188, "end_pos": 210, "type": "TASK", "confidence": 0.7917172908782959}, {"text": "passage retrieval", "start_pos": 212, "end_pos": 229, "type": "TASK", "confidence": 0.862067312002182}, {"text": "answer extraction", "start_pos": 235, "end_pos": 252, "type": "TASK", "confidence": 0.8557906746864319}]}, {"text": "In this paper, we propose two MBRbased Answer Re-ranking (MBRAR) approaches, aiming to re-rank answer candidates from either single and multiple QA systems.", "labels": [], "entities": [{"text": "MBRbased Answer Re-ranking (MBRAR)", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.6250577519337336}]}, {"text": "The first one re-ranks answer outputs from single QA system based on a traditional MBR model by measuring the correlations between each answer candidates and all the other candidates; while the second one re-ranks the combined answer outputs from multiple QA systems based on a mixture model-based MBR model.", "labels": [], "entities": []}, {"text": "The key contribution of this work is that, our MBRAR approaches assume little about QA systems and can be easily applied to QA systems with arbitrary sub-components.", "labels": [], "entities": [{"text": "MBRAR", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.6511343121528625}]}, {"text": "The remainder of this paper is organized as follows: Section 2 gives a brief review of the QA task and describe two types of QA systems with different pros and cons.", "labels": [], "entities": [{"text": "QA task", "start_pos": 91, "end_pos": 98, "type": "TASK", "confidence": 0.9078304171562195}]}, {"text": "Section 3 presents two MBRAR approaches that can re-rank the answer candidates from single and multiple QA systems respectively.", "labels": [], "entities": [{"text": "MBRAR", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.6491421461105347}]}, {"text": "The relationship between our approach and previous work is discussed in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 evaluates our methods on large scale questions selected from two domains (Jeopardy! and Web) and shows promising results.", "labels": [], "entities": []}, {"text": "Section 6 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Impacts of MBRAR for single QA system  on Jeopardy! questions.", "labels": [], "entities": [{"text": "MBRAR", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.778296172618866}, {"text": "Jeopardy! questions", "start_pos": 52, "end_pos": 71, "type": "DATASET", "confidence": 0.8054405053456625}]}, {"text": " Table 2: Impacts of MBRAR for single QA system  on web questions.", "labels": [], "entities": [{"text": "MBRAR", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.45845741033554077}]}, {"text": " Table 3: Impacts of MBRAR for multiple QA sys- tems on Jeopardy! questions.", "labels": [], "entities": [{"text": "MBRAR", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.709074854850769}, {"text": "Jeopardy! questions", "start_pos": 56, "end_pos": 75, "type": "DATASET", "confidence": 0.8238454461097717}]}, {"text": " Table 4: Impacts of MBRAR for multiple QA sys- tems on web questions.", "labels": [], "entities": [{"text": "MBRAR", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.5246278047561646}]}]}