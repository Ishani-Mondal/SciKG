{"title": [{"text": "Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents novel methods for modeling numerical common sense: the ability to infer whether a given number (e.g., three billion) is large, small, or normal fora given context (e.g., number of people facing a water shortage).", "labels": [], "entities": [{"text": "numerical common sense", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.6067873934904734}]}, {"text": "We first discuss the necessity of numerical commonsense in solving textual entailment problems.", "labels": [], "entities": []}, {"text": "We explore two approaches for acquiring numerical commonsense.", "labels": [], "entities": []}, {"text": "Both approaches start with extracting numerical expressions and their context from the Web.", "labels": [], "entities": []}, {"text": "One approach estimates the distribution of numbers co-occurring within a context and examines whether a given value is large, small, or normal, based on the distribution.", "labels": [], "entities": []}, {"text": "Another approach utilizes textual patterns with which speakers explicitly expresses their judgment about the value of a numerical expression.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate the effectiveness of both approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Textual entailment recognition (RTE) involves a wide range of semantic inferences to determine whether the meaning of a hypothesis sentence (h) can be inferred from another text (t) ().", "labels": [], "entities": [{"text": "Textual entailment recognition (RTE)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.870392049352328}]}, {"text": "Although several evaluation campaigns (e.g., PASCAL/TAC RTE challenges) have made significant progress, the RTE community recognizes the necessity of a deeper understanding of the core phenomena involved in textual inference.", "labels": [], "entities": [{"text": "PASCAL/TAC RTE", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.4262981042265892}, {"text": "RTE", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.9454475045204163}, {"text": "textual inference", "start_pos": 207, "end_pos": 224, "type": "TASK", "confidence": 0.6698890626430511}]}, {"text": "Such recognition comes from the ideas that crucial progress may derive from decomposing the complex RTE task into basic phenomena and from solving each basic phenomenon separately.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 100, "end_pos": 108, "type": "TASK", "confidence": 0.8920157253742218}]}, {"text": "Given this background, we focus on solving one of the basic phenomena in RTE: semantic inference related to numerical expressions.", "labels": [], "entities": [{"text": "RTE", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.945814847946167}]}, {"text": "The specific problem we address is acquisition of numerical commonsense.", "labels": [], "entities": [{"text": "acquisition of numerical commonsense", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.7910135835409164}]}, {"text": "For example, (1) t : Before long, 3b people will face a water shortage in the world.", "labels": [], "entities": []}, {"text": "h : Before long, a serious water shortage will occur in the world.", "labels": [], "entities": []}, {"text": "Although recognizing the entailment relation between t and h is frustratingly difficult, we assume this inference is decomposable into three phases: 3b people face a water shortage.", "labels": [], "entities": []}, {"text": "\u21d4 3,000,000,000 people face a water shortage.", "labels": [], "entities": []}, {"text": "|= many people face a water shortage.", "labels": [], "entities": []}, {"text": "|= a serious water shortage.", "labels": [], "entities": []}, {"text": "In the first phase, it is necessary to recognize 3b as a numerical expression and to resolve the expression 3b into the exact amount 3,000,000,000.", "labels": [], "entities": []}, {"text": "The second phase is much more difficult because we need subjective but common-sense knowledge that 3,000,000,000 people is a large number.", "labels": [], "entities": []}, {"text": "In this paper, we address the first and second phases of inference as an initial step towards semantic processing with numerical expressions.", "labels": [], "entities": [{"text": "semantic processing", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7844218909740448}]}, {"text": "The contributions of this paper are four-fold.", "labels": [], "entities": []}, {"text": "1. We examine instances in existing RTE corpora, categorize them into groups in terms of the necessary semantic inferences, and discuss the impact of this study for solving RTE problems with numerical expressions.", "labels": [], "entities": [{"text": "RTE", "start_pos": 173, "end_pos": 176, "type": "TASK", "confidence": 0.9601201415061951}]}, {"text": "2. We describe a method of normalizing numerical expressions referring to the same amount in text into a unified semantic representation.", "labels": [], "entities": []}, {"text": "3. We present approaches for aggregating numerical commonsense from examples of numerical expressions and for judging whether a given amount is large, small, or normal.", "labels": [], "entities": []}, {"text": "4. We demonstrate the effectiveness of this approach, reporting experimental results and analyses in detail.", "labels": [], "entities": []}, {"text": "Although it would be ideal to evaluate the impact of this study on the overall RTE task, we evaluate each phase separately.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 79, "end_pos": 87, "type": "TASK", "confidence": 0.8840992450714111}]}, {"text": "We do this because the existing RTE data sets tend to exhibit very diverse linguistic phenomena, and it is difficult to employ such data for evaluating the real impact of this study.", "labels": [], "entities": [{"text": "RTE data sets", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.8997339606285095}]}], "datasetContent": [{"text": "We built a gold-standard data set for numerical commonsense.", "labels": [], "entities": [{"text": "numerical commonsense", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.6463912129402161}]}, {"text": "We applied the method in Section 4.1 to sentences sampled at random from the Japanese Web corpus (), and we extracted 2,000 numerical expressions.", "labels": [], "entities": [{"text": "Japanese Web corpus", "start_pos": 77, "end_pos": 96, "type": "DATASET", "confidence": 0.9620341459910074}]}, {"text": "We asked three human judges to annotate every numerical expression with one of six labels, small, relatively small, normal, relatively large, large, and unsure.", "labels": [], "entities": []}, {"text": "The label relatively small could be applied to a numerical expression when the judge felt that the amount was rather small (below the normal) but hesitated to label it small.", "labels": [], "entities": []}, {"text": "The label relatively large was defined analogously.", "labels": [], "entities": []}, {"text": "We gave the following criteria for labeling an item as unsure: when the judgment was highly dependent on the context; when the sentence was incomprehensible; and when it was a non-numerical expressions (false positives of the method are discussed in Section 4.1).", "labels": [], "entities": []}, {"text": "For the evaluation of numerical expressions in the data set, we used those for which at least two annotators assigned the same label.", "labels": [], "entities": []}, {"text": "After removing the unsure instances, we obtained 640 numerical expressions (20 small, 35 relatively small, 152 normal, 263 relatively large, and 170 large) as the evaluation set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Frequency and simple definitions for each category of the entailment phenomena in the survey.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9902409315109253}]}, {"text": " Table 2: Normalized representation examples", "labels": [], "entities": []}, {"text": " Table 6: Output example and error analysis. We present translations of the sentences, which were origi- nally in Japanese.", "labels": [], "entities": []}, {"text": " Table 5: Precision (P), recall (R), F1 score (F1),  and accuracy (Acc) of the acquisition of numerical  common sense.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9484268426895142}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.956289678812027}, {"text": "F1 score (F1)", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.9403304100036621}, {"text": "accuracy (Acc)", "start_pos": 57, "end_pos": 71, "type": "METRIC", "confidence": 0.923459455370903}, {"text": "acquisition of numerical  common sense", "start_pos": 79, "end_pos": 117, "type": "TASK", "confidence": 0.7907691717147827}]}]}