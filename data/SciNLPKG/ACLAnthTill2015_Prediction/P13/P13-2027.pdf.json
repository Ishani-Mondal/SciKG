{"title": [{"text": "Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?", "labels": [], "entities": [{"text": "Ad Hoc Information Retrieval", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.5763033851981163}]}], "abstractContent": [{"text": "The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.84054034948349}]}, {"text": "More, the semantic coherence of the topics has never been considered in this field.", "labels": [], "entities": []}, {"text": "We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics.", "labels": [], "entities": []}, {"text": "We perform a first experimental evaluation using two major TREC test collections.", "labels": [], "entities": [{"text": "TREC test collections", "start_pos": 59, "end_pos": 80, "type": "DATASET", "confidence": 0.8415098190307617}]}, {"text": "Results show that retrieval performances tend to be better when using topics with higher semantic coherence.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing documents as mixtures of \"topics\" has always been a challenge and an objective for researchers working in text-related fields.", "labels": [], "entities": [{"text": "Representing documents as mixtures of \"topics\"", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7974116876721382}]}, {"text": "Based on the words used within a document, topic models learn topic level relations by assuming that the document covers a small set of concepts.", "labels": [], "entities": []}, {"text": "Learning the topics from a document collection can help to extract high level semantic information, and help humans to understand the meaning of documents.", "labels": [], "entities": []}, {"text": "Latent Semantic Indexing) (LSI), probabilistic Latent Semantic Analysis) (pLSA) and Latent Dirichlet Allocation () (LDA) are the most famous approaches that tried to tackle this problem throughout the years.", "labels": [], "entities": []}, {"text": "Topics produced by these methods are generally fancy and appealing, and often correlate well with human concepts.", "labels": [], "entities": []}, {"text": "This is one of the reasons of the intensive use of topic models (and especially LDA) in current research in Natural Language Processing (NLP) related areas.", "labels": [], "entities": []}, {"text": "One main problem in ad hoc Information Retrieval (IR) is the difficulty for users to translate a complex information need into a keyword query.", "labels": [], "entities": [{"text": "ad hoc Information Retrieval (IR)", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.832961482661111}]}, {"text": "The most popular and effective approach to overcome this problem is to improve the representation of the query by adding query-related \"concepts\".", "labels": [], "entities": []}, {"text": "This approach mostly relies on pseudorelevance feedback, where these so-called \"concepts\" are the most frequent words occurring in the top-ranked documents retrieved by the retrieval system (.", "labels": [], "entities": []}, {"text": "From that perspective, topic models seem attractive in the sense that they can provide a descriptive and intuitive representation of concepts.", "labels": [], "entities": []}, {"text": "But how can we quantify the usefulness of these topics with respect to an IR system?", "labels": [], "entities": []}, {"text": "Recently, researchers developed measures which evaluate the semantic coherence of topic models).", "labels": [], "entities": []}, {"text": "We adopt their view of semantic coherence and apply one of these measures to query-oriented topics.", "labels": [], "entities": []}, {"text": "Several studies concentrated on improving the quality of document ranking using topic models, especially probabilistic ones.", "labels": [], "entities": []}, {"text": "The approach by was the first to leverage LDA topics to improve the estimate of document language models and achieved good empirical results.", "labels": [], "entities": []}, {"text": "Following this pioneering work, several studies explored the use of pLSA and LDA under different experimental settings).", "labels": [], "entities": []}, {"text": "The reported results suggest that the words and the probability distributions learned by probabilistic topic models are effective for query expansion.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 134, "end_pos": 149, "type": "TASK", "confidence": 0.8931166231632233}]}, {"text": "The main drawback of these approaches is that topics are learned on the whole target document collection prior to retrieval, thus leading to a static topical representation of the collection.", "labels": [], "entities": []}, {"text": "Depending on the query and on its specificity, topics may either be too coarse or too fine to accurately represent the latent concepts of the query.", "labels": [], "entities": []}, {"text": "Recently, proposed a method which uses LDA and learns topics directly on a limited set of documents.", "labels": [], "entities": []}, {"text": "While this approach is a first step towards modeling query-oriented topics, it lacks some theoretic principles and only aims to heuristically construct a \"best\" topic (from all learned topics) before expanding the query with its most probable words.", "labels": [], "entities": []}, {"text": "More, none of the aforementioned works studied the semantic coherence of those generated topics.", "labels": [], "entities": []}, {"text": "We tackle these issues by making the following contributions: \u2022 we introduce Topic-Driven Relevance Models, a model-based feedback approach) for integrating topic models into relevance models by learning topics on pseudo-relevant feedback documents (as opposed to the entire document collection), \u2022 we explore the coherence of those generated topics using the queries of two major and well-established TREC test collections, \u2022 we evaluate the effects coherent topics have on ad hoc IR using the same test collections.", "labels": [], "entities": [{"text": "TREC test collections", "start_pos": 402, "end_pos": 423, "type": "DATASET", "confidence": 0.8558907707532247}]}], "datasetContent": [{"text": "We performed our evaluation using two main TREC 2 collections: Robust04 and WT10g.", "labels": [], "entities": [{"text": "TREC 2 collections", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.714055081208547}, {"text": "Robust04", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.7556508183479309}, {"text": "WT10g", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.9317656755447388}]}, {"text": "Robust04 is composed 528,155 of news articles coming from three newspapers and the FBIS.", "labels": [], "entities": [{"text": "Robust04", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8552635312080383}, {"text": "FBIS", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.9232050776481628}]}, {"text": "It supported the TREC 2004 Robust track, from which we used the 250 query topics (numbers: 301-450, 601-700).", "labels": [], "entities": [{"text": "TREC 2004 Robust track", "start_pos": 17, "end_pos": 39, "type": "DATASET", "confidence": 0.8436519950628281}]}, {"text": "The WT10g collection is composed of 1,692,096 web pages, and supported the TREC Web track for four years).", "labels": [], "entities": [{"text": "WT10g collection", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.976222038269043}, {"text": "TREC Web track", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.9053091208140055}]}, {"text": "We focus on the 2000 and 2001 ad-hoc query topics (numbers: 451-550).", "labels": [], "entities": []}, {"text": "We used the open-source indexing and retrieval system Indri 3 to run our experiments.", "labels": [], "entities": []}, {"text": "We indexed the two collections with the exact same parameters: tokens were stemmed with the well-known light Krovetz stemmer and stopwords were removed using the standard English stoplist embedded with Indri (417 words).", "labels": [], "entities": []}, {"text": "Most coherent topics are composed of rare words that do not often occur in the reference corpus, but  co-occur at lot together.", "labels": [], "entities": []}, {"text": "We see on that very coherent topics are identified in the top 5 and 10 feedback documents for the WT10g collection, suggesting that closely related documents are retrieved in the top ranks.", "labels": [], "entities": [{"text": "WT10g collection", "start_pos": 98, "end_pos": 114, "type": "DATASET", "confidence": 0.944739431142807}]}, {"text": "Results are quite different on the Robust04 collection, where topic models with 20 topics on 5 documents are the least coherent.", "labels": [], "entities": [{"text": "Robust04 collection", "start_pos": 35, "end_pos": 54, "type": "DATASET", "confidence": 0.8790809810161591}]}, {"text": "However, when looking at the Robust04 documents, we see that they are on average almost twice smaller than the WT10g web pages.", "labels": [], "entities": [{"text": "Robust04 documents", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.9142011106014252}, {"text": "WT10g web pages", "start_pos": 111, "end_pos": 126, "type": "DATASET", "confidence": 0.9743708769480387}]}, {"text": "We hypothesize that the heterogeneous nature of the web allows to model very different topics covering several aspects of the query, while news articles are contributions focused on a single subject.", "labels": [], "entities": []}, {"text": "Overall, the more coherent topic models contain a reasonable amount of topics (10-15), thus allowing to fit with variable amounts of documents.", "labels": [], "entities": []}, {"text": "The attentive reader will notice that the topic coherence scores are very high compared to those previously reported in the literature ().", "labels": [], "entities": []}, {"text": "The TDRM approach captures topics that are centered around a specific information need, often with a limited vocabulary, which favors word co-occurrence.", "labels": [], "entities": []}, {"text": "On the other hand, topics learned on entire collections are coarser than ours, which leads to lower coherence scores.", "labels": [], "entities": []}], "tableCaptions": []}