{"title": [{"text": "A Two Level Model for Context Sensitive Inference Rules", "labels": [], "entities": [{"text": "Context Sensitive Inference", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.860506534576416}]}], "abstractContent": [{"text": "Automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words, operating at the word space level.", "labels": [], "entities": []}, {"text": "A recent line of work, which addresses context sensitivity of rules, represented contexts in a latent topic space and computed similarity over topic vectors.", "labels": [], "entities": []}, {"text": "We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations.", "labels": [], "entities": []}, {"text": "Evaluations on a naturally-distributed dataset show that our model significantly outperforms prior word-level and topic-level models.", "labels": [], "entities": []}, {"text": "We also release a first context-sensitive inference rule set.", "labels": [], "entities": []}], "introductionContent": [{"text": "Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA)) and Information Extraction (IE)).", "labels": [], "entities": [{"text": "Question Answering (QA))", "start_pos": 112, "end_pos": 136, "type": "TASK", "confidence": 0.8269398987293244}, {"text": "Information Extraction (IE))", "start_pos": 141, "end_pos": 169, "type": "TASK", "confidence": 0.8309496760368347}]}, {"text": "For example, the inference rule 'X treat Y \u2192 X relieve Y' can be useful to extract pairs of drugs and the illnesses which they relieve, or to answer a question like \"Which drugs relieve headache?\".", "labels": [], "entities": []}, {"text": "Along this vein, such inference rules constitute a crucial component in generic modeling of textual inference, under the Textual Entailment paradigm;.", "labels": [], "entities": []}, {"text": "Motivated by these needs, substantial research was devoted to automatic learning of inference rules from corpora, mostly in an unsupervised distributional setting.", "labels": [], "entities": []}, {"text": "This research line was mainly initiated by the highly-cited DIRT algorithm), which learns inference for binary predicates with two argument slots (like the rule in the example above).", "labels": [], "entities": []}, {"text": "DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus.", "labels": [], "entities": []}, {"text": "Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors.", "labels": [], "entities": []}, {"text": "This general scheme was further enhanced in several directions, e.g. directional similarity (  and meta-classification over similarity values).", "labels": [], "entities": []}, {"text": "Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (.", "labels": [], "entities": []}, {"text": "The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities.", "labels": [], "entities": []}, {"text": "Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score.", "labels": [], "entities": []}, {"text": "However, the validity of an inference rule may depend on the context in which it is applied, such as the context specified by the given predicate's arguments.", "labels": [], "entities": []}, {"text": "For example, 'AT&T acquire TMobile \u2192 AT&T purchase T-Mobile', is a valid application of the rule 'X acquire Y \u2192 X purchase Y', while 'Children acquire skills \u2192 Children purchase skills' is not.", "labels": [], "entities": []}, {"text": "To address this issue, a line of works emerged which computes a contextsensitive reliability score for each rule application, based on the given context.", "labels": [], "entities": [{"text": "reliability score", "start_pos": 81, "end_pos": 98, "type": "METRIC", "confidence": 0.9220766127109528}]}, {"text": "The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling ().", "labels": [], "entities": []}, {"text": "In particular, the more recent methods () modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo-cation (LDA) model.", "labels": [], "entities": []}, {"text": "Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the given context, yielding a context-sensitive score for each particular rule application.", "labels": [], "entities": [{"text": "similarity", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9899691939353943}]}, {"text": "We notice at this point that while contextinsensitive methods represent predicates by argument vectors in the original fine-grained word space, context-sensitive methods represent them as vectors at the level of latent topics.", "labels": [], "entities": []}, {"text": "This raises the question of whether such coarse-grained topic vectors might be less informative in determining the semantic similarity between the two predicates.", "labels": [], "entities": []}, {"text": "To address this hypothesized caveat of prior context-sensitive rule scoring methods, we propose a novel generic scheme that integrates wordlevel and topic-level representations.", "labels": [], "entities": []}, {"text": "Our scheme can be applied on top of any context-insensitive \"base\" similarity measure for rule learning, which operates at the word level, such as Cosine or Lin.", "labels": [], "entities": [{"text": "rule learning", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.8017109930515289}]}, {"text": "Rather than computing a single context-insensitive rule score, we compute a distinct word-level similarity score for each topic in an LDA model.", "labels": [], "entities": [{"text": "word-level similarity score", "start_pos": 85, "end_pos": 112, "type": "METRIC", "confidence": 0.6820982297261556}]}, {"text": "Then, when applying a rule in a given context, these different scores are weighed together based on the specific topic distribution under the given context.", "labels": [], "entities": []}, {"text": "This way, we calculate similarity over vectors in the original word space, while biasing them towards the given context via a topic model.", "labels": [], "entities": []}, {"text": "In order to promote replicability and equal-term comparison with our results, we based our experiments on publicly available datasets, both for unsupervised learning of the evaluated models and for testing them over a random sample of rule applications.", "labels": [], "entities": []}, {"text": "We apply our two-level scheme over three state-of-the-art context-insensitive similarity measures.", "labels": [], "entities": []}, {"text": "The evaluation compares performances both with the original context-insensitive measures and with recent LDA-based contextsensitive methods, showing consistent and robust advantages of our scheme.", "labels": [], "entities": []}, {"text": "Finally, we release a context-sensitive rule resource comprising over 2,000 frequent verbs and one million rules.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our model, we compare it both to context-insensitive similarity measures as well as to prior context-sensitive methods.", "labels": [], "entities": []}, {"text": "Furthermore, to better understand its applicability in typical NLP tasks, we focus on an evaluation setting that corresponds to a natural distribution of examples from a large corpus.", "labels": [], "entities": []}, {"text": "0.974 0.000, along with the topic relevance for the given context p(t|d v , w), which weighs the topic-biased scores in the Lin W T calculation.", "labels": [], "entities": []}, {"text": "The context-insensitive Lin score is shown for comparison.", "labels": [], "entities": [{"text": "Lin score", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.94651198387146}]}, {"text": "To evaluate the performance of the different methods we chose the dataset constructed by.", "labels": [], "entities": []}, {"text": "This publicly available dataset contains about 6,500 manually annotated predicate template rule applications, each one labeled as corrector incorrect.", "labels": [], "entities": []}, {"text": "For example, 'Jack agree with Jill Jack feel sorry for Jill' is a rule application in this dataset, labeled as incorrect, and 'Registration open this month \u2192 Registration begin this month' is another rule application, labeled as correct.", "labels": [], "entities": []}, {"text": "Rule applications were generated by randomly sampling extractions from ReVerb, such as ('Jack','agree with','Jill') and then sampling possible rules for each, such as 'agree with \u2192 feel sorry for'.", "labels": [], "entities": []}, {"text": "Hence, this dataset provides naturally distributed rule inferences with respect to ReVerb.", "labels": [], "entities": [{"text": "ReVerb", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.9224467277526855}]}, {"text": "Whenever we evaluated a distributional similarity measure (namely Lin, BInc, or Cosine), we discarded instances from Zeichner et al.'s dataset in which the assessed rule is not in the contextinsensitive rule-set learned for this measure or the argument instantiation of the rule is not in the LDA lexicon.", "labels": [], "entities": [{"text": "BInc", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.850553035736084}, {"text": "LDA lexicon", "start_pos": 293, "end_pos": 304, "type": "DATASET", "confidence": 0.8885295689105988}]}, {"text": "We refer to the remaining instances as the test set per measure, e.g. Lin's test set.", "labels": [], "entities": [{"text": "Lin's test set", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.7280728220939636}]}, {"text": "details the size of each such test set in our experiment.", "labels": [], "entities": []}, {"text": "Finally, the task under which we assessed the tested models is to rank all rule applications in each test set, aiming to rank the valid rule applications above the invalid ones.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Two characteristic topics for the Y slot of  'acquire', along with their topic-biased Lin sim- ilarities scores Lin t , compared with the original  Lin similarity, for two rules. The relevance of each  topic to different arguments of 'acquire' is illus- trated by showing the top 5 words in the argument  vector v y  acquire for which the illustrated topic is the  most likely one.", "labels": [], "entities": [{"text": "topic-biased Lin sim- ilarities scores Lin t", "start_pos": 83, "end_pos": 127, "type": "METRIC", "confidence": 0.734919685870409}]}, {"text": " Table 3: Sizes of rule application test set for each  learned rule-set.", "labels": [], "entities": []}, {"text": " Table 4: MAP values on corresponding test set ob- tained by each method. Figures in parentheses in- dicate optimal number of LDA topics.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9561325907707214}]}, {"text": " Table 5: MAP results for the two split Lin test- sets.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.919556200504303}, {"text": "split Lin test- sets", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.7408168733119964}]}]}