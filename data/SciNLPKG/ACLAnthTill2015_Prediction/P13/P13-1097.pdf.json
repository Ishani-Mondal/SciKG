{"title": [{"text": "Probabilistic Sense Sentiment Similarity through Hidden Emotions", "labels": [], "entities": [{"text": "Probabilistic Sense Sentiment Similarity", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.5551011636853218}]}], "abstractContent": [{"text": "Sentiment Similarity of word pairs reflects the distance between the words regarding their underlying sentiments.", "labels": [], "entities": [{"text": "Sentiment Similarity of word pairs", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8602744698524475}]}, {"text": "This paper aims to infer the sentiment similarity between word pairs with respect to their senses.", "labels": [], "entities": []}, {"text": "To achieve this aim, we propose a probabilistic emotion-based approach that is built on a hidden emotional model.", "labels": [], "entities": []}, {"text": "The model aims to predict a vector of basic human emotions for each sense of the words.", "labels": [], "entities": []}, {"text": "The resultant emotional vectors are then employed to infer the sentiment similarity of word pairs.", "labels": [], "entities": []}, {"text": "We apply the proposed approach to address two main NLP tasks, namely , Indirect yes/no Question Answer Pairs inference and Sentiment Orientation prediction.", "labels": [], "entities": [{"text": "Indirect yes/no Question Answer Pairs inference", "start_pos": 71, "end_pos": 118, "type": "TASK", "confidence": 0.6412245519459248}, {"text": "Sentiment Orientation prediction", "start_pos": 123, "end_pos": 155, "type": "TASK", "confidence": 0.9413265585899353}]}, {"text": "Extensive experiments demonstrate the effectiveness of the proposed approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment similarity reflects the distance between words based on their underlying sentiments.", "labels": [], "entities": [{"text": "Sentiment similarity", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9176284968852997}]}, {"text": "Semantic similarity measures such as Latent Semantic Analysis (LSA) ( can effectively capture the similarity between semantically related words like \"car\" and \"automobile\", but they are less effective in relating words with similar sentiment orientation like \"excellent\" and \"superior\".", "labels": [], "entities": []}, {"text": "For example, the following relations show the semantic similarity between some sentiment words computed by LSA: : \ud97b\udf59, \ud97b\udf59 = 0.40 < \ud97b\udf59, \ud97b\udf59 = 0.46 < \ud97b\udf59, \ud97b\udf59 = 0.65 Clearly, the sentiment similarity between the above words should be in the reversed order.", "labels": [], "entities": []}, {"text": "In fact, the sentiment intensity in \"excellent\" is closer to \"superior\" than \"good\".", "labels": [], "entities": []}, {"text": "Furthermore, sentiment similarity between \"good\" and \"bad\" should be 0.", "labels": [], "entities": [{"text": "sentiment similarity", "start_pos": 13, "end_pos": 33, "type": "METRIC", "confidence": 0.7933807075023651}]}, {"text": "In this paper, we propose a probabilistic approach to detect the sentiment similarity of words regarding their senses and underlying sentiments.", "labels": [], "entities": []}, {"text": "For this purpose, we propose to model the hidden emotions of word senses.", "labels": [], "entities": []}, {"text": "We show that our approach effectively outperforms the semantic similarity measures in two NLP tasks: Indirect yes/no Question Answer Pairs (IQAPs) Inference and Sentiment Orientation (SO) prediction that are described as follows: In IQAPs, answers do not explicitly contain the yes or no keywords, but rather provide context information to infer the yes or no answer (e.g. Q: Was she the best one on that old show? A: She was simply funny).", "labels": [], "entities": [{"text": "Sentiment Orientation (SO) prediction", "start_pos": 161, "end_pos": 198, "type": "TASK", "confidence": 0.7246647377808889}]}, {"text": "Clearly, the sentiment words in IQAPs are the pivots to infer the yes or no answers.", "labels": [], "entities": []}, {"text": "We show that sentiment similarity between such words (e.g., here the adjectives best and Funny) can be used effectively to infer the answers.", "labels": [], "entities": []}, {"text": "The second application (SO prediction) aims to determine the sentiment orientation of individual words.", "labels": [], "entities": [{"text": "SO prediction", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9780515730381012}, {"text": "sentiment orientation of individual words", "start_pos": 61, "end_pos": 102, "type": "TASK", "confidence": 0.810810124874115}]}, {"text": "Previous research utilized the semantic relations between words obtained from WordNet ( and semantic similarity measures (e.g. for this purpose.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9452711939811707}]}, {"text": "In this paper, we show that sentiment similarity between word pairs can be effectively utilized to compute SO of words.", "labels": [], "entities": [{"text": "SO", "start_pos": 107, "end_pos": 109, "type": "TASK", "confidence": 0.9125111103057861}]}, {"text": "The contributions of this paper are follows: \u2022 We propose an effective approach to predict the sentiment similarity between word pairs through hidden emotions at the sense level, \u2022 We show the utility of sentiment similarity prediction in IQAP inference and SO prediction tasks, and \u2022 Our hidden emotional model can infer the type and number of hidden emotions in a corpus.", "labels": [], "entities": [{"text": "sentiment similarity prediction", "start_pos": 204, "end_pos": 235, "type": "TASK", "confidence": 0.7214007476965586}, {"text": "SO prediction tasks", "start_pos": 258, "end_pos": 277, "type": "TASK", "confidence": 0.9531423449516296}]}], "datasetContent": [{"text": "To evaluate our PSSS model, we perform experiments on the SO prediction and IQAPs inference tasks.", "labels": [], "entities": [{"text": "SO prediction", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.942552775144577}]}, {"text": "Here, we consider six emotions for both bridged and series models.", "labels": [], "entities": []}, {"text": "We study the effect of emotion numbers in Section 7.1.", "labels": [], "entities": []}, {"text": "Also, we set a threshold of 0.3 for the confidence value in Equation (16), i.e. we set the confidence values smaller than the threshold to 0.", "labels": [], "entities": [{"text": "Equation", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.8095917105674744}]}, {"text": "We explain the effect of this parameter in Section 7.3.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our PSSS models in the SO prediction task using the algorithm explained in by setting our PSSS as similarity function (A).", "labels": [], "entities": [{"text": "SO prediction task", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.9728484948476156}, {"text": "similarity function (A)", "start_pos": 129, "end_pos": 152, "type": "METRIC", "confidence": 0.8964044928550721}]}, {"text": "The results on SO prediction are presented in PMI extracts the semantic similarity between words using their co-occurrences.", "labels": [], "entities": [{"text": "SO prediction", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9927751123905182}]}, {"text": "As shows, it leads to poor performance.", "labels": [], "entities": []}, {"text": "This is mainly due to the relatively small size of the development dataset which affects the quality of the co-occurrence information used by the PMI.", "labels": [], "entities": []}, {"text": "ER computes the expected rating of a word based on the distribution of the word across rating categories.", "labels": [], "entities": []}, {"text": "The value of ER indicates the SO of the word.", "labels": [], "entities": [{"text": "ER", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9981521964073181}, {"text": "SO", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9885239005088806}]}, {"text": "As shown in the two last rows of the table, the results of PSSS approach are higher than PMI and ER.", "labels": [], "entities": [{"text": "PMI", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.632050633430481}, {"text": "ER", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.98700350522995}]}, {"text": "The reason is that PSSS is based on the combination between sentiment space (through using ratings, and matrices W\u00d7R in BHEM, D\u00d7R in SHEM) and semantic space (through the input W\u00d7D in SHEM and enriched matrix W\u00d7W in both hidden models).", "labels": [], "entities": [{"text": "PSSS", "start_pos": 19, "end_pos": 23, "type": "TASK", "confidence": 0.9386516809463501}, {"text": "BHEM", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.616740345954895}]}, {"text": "However, the PMI employs only the semantic space (i.e., the co-occurrence of the words) and ER uses occurrence of the words in rating categories.", "labels": [], "entities": []}, {"text": "Furthermore, the PSSS model achieves higher performance with BHEM rather than SHEM.", "labels": [], "entities": [{"text": "BHEM", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9673161506652832}]}, {"text": "This is because the emotional vectors of the words are directly computed from the EM steps of BHEM.", "labels": [], "entities": [{"text": "BHEM", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.5099791288375854}]}, {"text": "However, the emotional vectors of SHEM are computed after finishing the EM steps using Equation.", "labels": [], "entities": [{"text": "SHEM", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.9814654588699341}]}, {"text": "This causes the SHEM model to estimate the number and type of the hidden emotions with a lower performance as compared to BHEM, although the performances of SHEM and BHEM are comparable as explained in Section 7.1.", "labels": [], "entities": [{"text": "BHEM", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.6922006607055664}]}, {"text": "To apply our PSSS on IQAPs inference task, we use it as the sentiment similarity measure in the algorithm explained in.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "The first and second rows are baselines.", "labels": [], "entities": []}, {"text": "The first row is the result obtained by approach.", "labels": [], "entities": []}, {"text": "This approach is based on the similarity between the SO of the adjectives in question and answer.", "labels": [], "entities": []}, {"text": "The second row of show the results of using a popular semantic similarity measure, PMI, as the sentiment similarity (SS) measure in.", "labels": [], "entities": [{"text": "sentiment similarity (SS) measure", "start_pos": 95, "end_pos": 128, "type": "METRIC", "confidence": 0.8123554189999899}]}, {"text": "The result shows that PMI is less effective to capture the sentiment similarity.", "labels": [], "entities": [{"text": "PMI", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.5230573415756226}]}], "tableCaptions": [{"text": " Table 2. The algo- rithm employs document-rating, term-document  and term-rating matrices to infer the unknown  probabilities. This algorithm can be used with  both bridged or series models. Our goal is to in- fer the emotional vector for each word w that can  be obtained by the probability P(w|e). Note that,  this probability can be simply computed for the  SHEM model using P(d|e) as follows:", "labels": [], "entities": []}, {"text": " Table 2. Constructing emotional vectors via P(w|e)", "labels": [], "entities": [{"text": "Constructing emotional vectors", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.8748924930890402}]}, {"text": " Table 3. Performance on SO prediction task", "labels": [], "entities": [{"text": "SO prediction", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.9892142713069916}]}, {"text": " Table 4. Performance on IQAP inference task", "labels": [], "entities": []}]}