{"title": [{"text": "Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?", "labels": [], "entities": [{"text": "Minimal Translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.6739829331636429}, {"text": "Phrase-Based SMT", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.6439136862754822}]}], "abstractContent": [{"text": "The phrase-based and N-gram-based SMT frameworks complement each other.", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.8959397077560425}]}, {"text": "While the former is better able to memorize , the latter provides a more principled model that captures dependencies across phrasal boundaries.", "labels": [], "entities": []}, {"text": "Some work has been done to combine insights from these two frameworks.", "labels": [], "entities": []}, {"text": "A recent successful attempt showed the advantage of using phrase-based search on top of an N-gram-based model.", "labels": [], "entities": []}, {"text": "We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption.", "labels": [], "entities": []}, {"text": "A large scale evaluation over 8 language pairs shows that performance does significantly improve.", "labels": [], "entities": []}], "introductionContent": [{"text": "Phrase-based models () learn local dependencies such as reorderings, idiomatic collocations, deletions and insertions by memorization.", "labels": [], "entities": []}, {"text": "A fundamental drawback is that phrases are translated and reordered independently of each other and contextual information outside of phrasal boundaries is ignored.", "labels": [], "entities": []}, {"text": "The monolingual language model somewhat reduces this problem.", "labels": [], "entities": []}, {"text": "However i) often the language model cannot overcome the dispreference of the translation model for nonlocal dependencies, ii) source-side contextual dependencies are still ignored and iii) generation of lexical translations and reordering is separated.", "labels": [], "entities": []}, {"text": "The N-gram-based SMT framework addresses these problems by learning Markov chains over sequences of minimal translation units (MTUs) also known as tuples ) or over operations coupling lexical generation and reordering ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9003534913063049}]}, {"text": "Because the models condition the MTU probabilities on the previous MTUs, they capture non-local dependencies and both source and target contextual information across phrasal boundaries.", "labels": [], "entities": [{"text": "MTU", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9347409009933472}]}, {"text": "In this paper we study the effect of integrating tuple-based N-gram models (TSM) and operationbased N-gram models (OSM) into the phrasebased model in Moses, a state-of-the-art phrasebased system.", "labels": [], "entities": []}, {"text": "Rather than using POS-based rewrite rules ( ) to form a search graph, we use the ability of the phrasebased system to memorize larger translation units to replicate the effect of source linearization as done in the TSM model.", "labels": [], "entities": []}, {"text": "We also show that using phrase-based search with MTU N-gram translation models helps to address some of the search problems that are nontrivial to handle when decoding with minimal translation units.", "labels": [], "entities": [{"text": "MTU N-gram translation", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.6022616028785706}]}, {"text": "An important limitation of the OSM N-gram model is that it does not handle unaligned or discontinuous target MTUs and requires post-processing of the alignment to remove these.", "labels": [], "entities": [{"text": "OSM N-gram", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.7403007745742798}]}, {"text": "Using phrases during search enabled us to make novel changes to the OSM generative story (also applicable to the TSM model) to handle unaligned target words and to use target linearization to deal with discontinuous target MTUs.", "labels": [], "entities": [{"text": "OSM generative", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.7623556852340698}]}, {"text": "We performed an extensive evaluation, carrying out translation experiments from French, Spanish, Czech and Russian to English and in the opposite direction.", "labels": [], "entities": []}, {"text": "Our integration of the OSM model into Moses and our modification of the OSM model to deal with unaligned and discontinuous target tokens consistently improves BLEU scores over the baseline system, and shows statistically significant improvements in seven out of eight cases.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.999154806137085}]}], "datasetContent": [{"text": "Corpus: We ran experiments with data made available for the translation task of the Eighth Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "Corpus", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8954319357872009}, {"text": "translation task", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.928202897310257}, {"text": "Eighth Workshop on Statistical Machine Translation", "start_pos": 84, "end_pos": 134, "type": "TASK", "confidence": 0.6516559223333994}]}, {"text": "The sizes of bitext used for the estimation of translation and monolingual language models are reported in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Translating into and from English. Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline", "labels": [], "entities": [{"text": "Statistically", "start_pos": 51, "end_pos": 64, "type": "METRIC", "confidence": 0.9739100933074951}]}]}