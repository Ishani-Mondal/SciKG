{"title": [{"text": "Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation", "labels": [], "entities": [{"text": "Adaptation Data Selection", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7742876013120016}, {"text": "Machine Translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.735843762755394}]}], "abstractContent": [{"text": "Data selection is an effective approach to domain adaptation in statistical machine translation.", "labels": [], "entities": [{"text": "Data selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7745868861675262}, {"text": "domain adaptation", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.736802726984024}, {"text": "statistical machine translation", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.662866602341334}]}, {"text": "The idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora, which are then incorporated into the training data.", "labels": [], "entities": []}, {"text": "Substantial gains have been demonstrated in previous works, which employ standard n-gram language models.", "labels": [], "entities": []}, {"text": "Here, we explore the use of neural language models for data selection.", "labels": [], "entities": [{"text": "data selection", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.8096621036529541}]}, {"text": "We hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text.", "labels": [], "entities": []}, {"text": "Ina comprehensive evaluation of 4 language pairs (En-glish to German, French, Russian, Span-ish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outper-form conventional n-grams.", "labels": [], "entities": [{"text": "data selection", "start_pos": 163, "end_pos": 177, "type": "TASK", "confidence": 0.6483731865882874}, {"text": "BLEU", "start_pos": 239, "end_pos": 243, "type": "METRIC", "confidence": 0.9973715543746948}]}], "introductionContent": [{"text": "A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.8566666742165884}]}, {"text": "An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora.", "labels": [], "entities": [{"text": "adaptation data selection", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.7869461973508199}]}, {"text": "The selected sentences are then incorporated into the SMT training data.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 54, "end_pos": 66, "type": "TASK", "confidence": 0.8918784856796265}]}, {"text": "Analyses have shown that this augmented data can lead to better statistical estimation or word coverage (.", "labels": [], "entities": [{"text": "statistical estimation", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.7093577086925507}]}, {"text": "Although previous works in data selection have shown substantial gains, we suspect that the commonly-used n-gram LMs maybe sub-optimal.", "labels": [], "entities": []}, {"text": "The small size of the in-domain text implies that a large percentage of generaldomain sentences will contain words not observed in the LM training data.", "labels": [], "entities": [{"text": "LM training data", "start_pos": 135, "end_pos": 151, "type": "DATASET", "confidence": 0.6972836852073669}]}, {"text": "In fact, as many as 60% of general-domain sentences contain at least one unknown word in our experiments.", "labels": [], "entities": []}, {"text": "Although the LM probabilities of these sentences could still be computed by resorting to back-off and other smoothing techniques, a natural question remains: will alternative, more robust LMs do better?", "labels": [], "entities": []}, {"text": "We hypothesize that the neural language model () is a viable alternative, since its continuous vector representation of words is well-suited for modeling sentences with frequent unknown words, providing smooth probability estimates of unseen but similar contexts.", "labels": [], "entities": []}, {"text": "Neural LMs have achieved positive results in speech recognition and SMT reranking (.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8208922445774078}, {"text": "SMT reranking", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.9574453830718994}]}, {"text": "To the best of our knowledge, this paper is the first work that examines neural LMs for adaptation data selection.", "labels": [], "entities": [{"text": "adaptation data selection", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.8403012156486511}]}], "datasetContent": [{"text": "We experimented with four language pairs in the WIT 3 corpus (, with English (en) as source and German (de), Spanish (es), French (fr), Russian (ru) as target.", "labels": [], "entities": [{"text": "WIT 3 corpus", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.9174118638038635}]}, {"text": "This is the in-domain corpus, and consists of TED Talk transcripts covering topics in technology, entertainment, and design.", "labels": [], "entities": [{"text": "TED Talk transcripts", "start_pos": 46, "end_pos": 66, "type": "DATASET", "confidence": 0.8541678587595621}]}, {"text": "As general-domain corpora, we collected bitext from the WMT2013 campaign, including CommonCrawl and NewsCommentary for all 4 languages, Europarl for de/es/fr, UN for es/fr, Gigaword for fr, and Yandex for ru.", "labels": [], "entities": [{"text": "WMT2013 campaign", "start_pos": 56, "end_pos": 72, "type": "DATASET", "confidence": 0.8523482084274292}, {"text": "CommonCrawl", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.9710744023323059}, {"text": "Europarl", "start_pos": 136, "end_pos": 144, "type": "DATASET", "confidence": 0.945382833480835}]}, {"text": "The indomain data is divided into a training set (for SMT For each language pair, we built a baseline indata SMT system trained only on in-domain data, and an alldata system using combined in-domain and general-domain data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9866917729377747}, {"text": "SMT", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.839542031288147}]}, {"text": "We then built 3 systems from augmented data selected by different LMs: \u2022 neuralnet: Data selection by Recurrent neural LM, with the RNNLM Toolkit.", "labels": [], "entities": [{"text": "RNNLM Toolkit", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.9560431838035583}]}, {"text": "5 \u2022 combine: Data selection by interpolated LM using n-gram & neuralnet (equal weight).", "labels": [], "entities": [{"text": "Data selection", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7237463593482971}]}, {"text": "All systems are built using standard settings in the Moses toolkit (GIZA++ alignment, grow-diagfinal-and, lexical reordering models, and SRILM).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 137, "end_pos": 142, "type": "METRIC", "confidence": 0.8736813068389893}]}, {"text": "Note that standard n-grams are used as LMs for SMT; neural LMs are only used for data selection.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9949615001678467}, {"text": "data selection", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.7015306353569031}]}, {"text": "Multiple SMT systems are trained by thresholding on {10k,50k,100k,500k,1M} general-domain sentence subsets, and we empirically determine the single system for testing based on results on a separate validation set (in practice, 500k was chosen for fr and 1M for es, de, ru.).", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9880949258804321}]}, {"text": "The original data are provided by http://wit3.fbk.eu and http://www.statmt.org/wmt13/.", "labels": [], "entities": []}, {"text": "Our domain adaptation scenario is similar to the IWSLT2012 campaign but we used our own random train/test splits, since we wanted to ensure the testset for all languages had identical source sentences for comparison purposes.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7113979458808899}, {"text": "IWSLT2012", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.8058229684829712}]}, {"text": "For replicability, our software is available at http://cl.naist.jp/\u223ckevinduh/a/acl2013.", "labels": [], "entities": [{"text": "replicability", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9838247299194336}]}, {"text": "More advanced phrase table adaptation methods are possible.", "labels": [], "entities": [{"text": "phrase table adaptation", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.7784249583880106}]}, {"text": "but our interest is in comparing data selection methods.", "labels": [], "entities": []}, {"text": "The conclusions should transfer to advanced methods such as  Second, we show that the usual concern of neural LM training time is not so critical for the indomain data sizes used domain adaptation.", "labels": [], "entities": []}, {"text": "The complexity of training is dominated by computing Eq.", "labels": [], "entities": []}, {"text": "3 and scales as O(|W | \u00d7 |S|) in the number of tokens.", "labels": [], "entities": [{"text": "O", "start_pos": 16, "end_pos": 17, "type": "METRIC", "confidence": 0.9862486720085144}]}, {"text": "Since |W | can be large, one practical trick is to cluster the vocabulary so that the output dimension is reduced.", "labels": [], "entities": []}, {"text": "shows the training times on a 3.3GHz XeonE5 CPU by varying these two main hyper-parameters (|S| and cluster size).", "labels": [], "entities": []}, {"text": "Note that the setting |S| = 200 and cluster size of 100 already gives good perplexity in reasonable training time.", "labels": [], "entities": [{"text": "cluster size", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.9369302093982697}]}, {"text": "All neural LMs in this paper use this setting, without additional tuning.: Training time (in minutes) for various neural LM architectures (Task: en-de de).", "labels": [], "entities": []}, {"text": "shows translation results in terms of BLEU (), RIBES (, and TER ().", "labels": [], "entities": [{"text": "translation", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.9363017082214355}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9991424083709717}, {"text": "RIBES", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9943758249282837}, {"text": "TER", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9981703758239746}]}, {"text": "We observe that all three data selection methods essentially outperform alldata and indata for all language pairs, and neuralnet tend to be the best in all metrics.", "labels": [], "entities": []}, {"text": "E.g., BLEU improvements over ngram are in the range of 0.4 for en-de, 0.5 for en-es, 0.1 for en-fr, and 1.7 for en-ru.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9990203380584717}]}, {"text": "Although not all improvements are large in absolute terms, many are statistically significant (95% confidence).", "labels": [], "entities": []}, {"text": "We therefore believe that neural LMs are generally worthwhile to try for data selection, as it rarely underperform n-grams.", "labels": [], "entities": [{"text": "data selection", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.7825101017951965}]}, {"text": "The open question is: what can explain the significant improvements in, for example Russian, Spanish, German, but the lack thereof in French?", "labels": [], "entities": []}, {"text": "One conjecture is that neural LMs succeeded in lowering testset out-ofvocabulary (OOV) rate, but we found that OOV reduction is similar across all selection methods.", "labels": [], "entities": [{"text": "testset out-ofvocabulary (OOV) rate", "start_pos": 56, "end_pos": 91, "type": "METRIC", "confidence": 0.80215884745121}, {"text": "OOV reduction", "start_pos": 111, "end_pos": 124, "type": "METRIC", "confidence": 0.9086368083953857}]}, {"text": "The improvements appear to be due to better probability estimates of the translation/reordering models.", "labels": [], "entities": []}, {"text": "We performed a diagnostic by decoding the testset using LMs trained on the same testset, while varying the translation/reordering tables with those of ngram and neuralnet; this is a kind of pseudo forced-decoding that can inform us about which table has better coverage.", "labels": [], "entities": []}, {"text": "We found that across all language pairs, BLEU differences of translations under this diagnostic become insignificant, implying that the raw probability value is the differentiating factor between ngram and neuralnet.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9989516735076904}]}, {"text": "Manual inspection of en-de revealed that many improvements come from lexical choice in morphological variants (\"meinen Sohn\" vs. \"mein Sohn\"), segmentation changes (\"baking soda\" \u2192 \"Backpulver\" vs. \"baken Soda\"), and handling of unaligned words at phrase boundaries.", "labels": [], "entities": []}, {"text": "Finally, we measured the intersection between the sentence set selected by ngram vs neural-: End-to-end Translation Results.", "labels": [], "entities": []}, {"text": "The best results are bold-faced.", "labels": [], "entities": []}, {"text": "We also compare neural LMs to ngram using pairwise bootstrap): \"+\" means statistically significant improvement and \"\u2212\" means significant degradation. net.", "labels": [], "entities": []}, {"text": "They share 60-75% of the augmented training data.", "labels": [], "entities": []}, {"text": "This high overlap means that ngram and neuralnet are actually not drastically different systems, and neuralnet with its slightly better selections represent an incremental improvement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data statistics. \"%unknown\"=fraction of  general-domain sentences with unknown words.", "labels": [], "entities": []}, {"text": " Table 2: Perplexity of various LMs. Number in  parenthesis is percentage improvement vs. ngram.", "labels": [], "entities": [{"text": "Number", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9897610545158386}, {"text": "ngram", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.9454535841941833}]}, {"text": " Table 3: Training time (in minutes) for various  neural LM architectures (Task: en-de de).", "labels": [], "entities": []}, {"text": " Table 4: End-to-end Translation Results. The best  results are bold-faced. We also compare neural  LMs to ngram using pairwise bootstrap", "labels": [], "entities": [{"text": "Translation", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.8047108054161072}]}]}