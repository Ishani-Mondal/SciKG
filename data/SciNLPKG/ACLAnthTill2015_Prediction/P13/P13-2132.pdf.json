{"title": [], "abstractContent": [{"text": "We introduce a scheme for optimally allocating a variable number of bits per LSH hyperplane.", "labels": [], "entities": []}, {"text": "Previous approaches assign a constant number of bits per hyper-plane.", "labels": [], "entities": []}, {"text": "This neglects the fact that a subset of hyperplanes maybe more informative than others.", "labels": [], "entities": []}, {"text": "Our method, dubbed Variable Bit Quantisation (VBQ), provides a data-driven non-uniform bit allocation across hyperplanes.", "labels": [], "entities": [{"text": "Variable Bit Quantisation (VBQ)", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.7125783860683441}]}, {"text": "Despite only using a fraction of the available hyperplanes, VBQ outper-forms uniform quantisation by up to 168% for retrieval across standard text and image datasets.", "labels": [], "entities": [{"text": "VBQ", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.7046130895614624}]}], "introductionContent": [{"text": "The task of retrieving the nearest neighbours to a given query document permeates the field of Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 95, "end_pos": 128, "type": "TASK", "confidence": 0.7142801781495413}]}, {"text": "Nearest neighbour search has been used for applications as diverse as automatically detecting document translation pairs for the purposes of training a statistical machine translation system (SMT) (, the large-scale generation of noun similarity lists () to an unsupervised method for extracting domain specific lexical variants.", "labels": [], "entities": [{"text": "Nearest neighbour search", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6676356593767802}, {"text": "automatically detecting document translation pairs", "start_pos": 70, "end_pos": 120, "type": "TASK", "confidence": 0.7408121645450592}, {"text": "statistical machine translation system (SMT)", "start_pos": 152, "end_pos": 196, "type": "TASK", "confidence": 0.6760807846273694}, {"text": "extracting domain specific lexical variants", "start_pos": 285, "end_pos": 328, "type": "TASK", "confidence": 0.8078375577926635}]}, {"text": "There are two broad approaches to nearest neighbour based search: exact and approximate techniques, which are differentiated by their ability to return completely correct nearest neighbours (the exact approach) or have some possibility of returning points that are not true nearest neighbours (the approximate approach).", "labels": [], "entities": [{"text": "nearest neighbour based search", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.6371760666370392}]}, {"text": "Approximate nearest neighbour (ANN) search using hashing techniques has recently gained prominence within NLP.", "labels": [], "entities": [{"text": "Approximate nearest neighbour (ANN) search", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.6546295625822884}]}, {"text": "The hashing-based approach maps the data into a substantially more compact representation referred to as a fingerprint, that is more efficient for performing similarity computations.", "labels": [], "entities": []}, {"text": "The resulting compact binary representation radically reduces memory requirements while also permitting fast sub-linear time retrieval of approximate nearest neighbours.", "labels": [], "entities": []}, {"text": "Hashing-based ANN techniques generally comprise two main steps: a projection stage followed by a quantisation stage.", "labels": [], "entities": [{"text": "Hashing-based ANN", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7954235076904297}]}, {"text": "The projection stage performs a neighbourhood preserving embedding, mapping the input data into a lower-dimensional representation.", "labels": [], "entities": []}, {"text": "The quantisation stage subsequently reduces the cardinality of this representation by converting the real-valued projections to binary.", "labels": [], "entities": []}, {"text": "Quantisation is a lossy transformation which can have a significant impact on the resulting quality of the binary encoding.", "labels": [], "entities": []}, {"text": "Previous work has quantised each projected dimension into a uniform number of bits) ( ).", "labels": [], "entities": []}, {"text": "We demonstrate that uniform allocation of bits is sub-optimal and propose a data-driven scheme for variable bit allocation.", "labels": [], "entities": [{"text": "variable bit allocation", "start_pos": 99, "end_pos": 122, "type": "TASK", "confidence": 0.7485776742299398}]}, {"text": "Our approach is distinct from previous work in that it provides a general objective function for bit allocation.", "labels": [], "entities": [{"text": "bit allocation", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.7155285477638245}]}, {"text": "VBQ makes no assumptions on the data and, in addition to LSH, it applies to abroad range of other projection functions.", "labels": [], "entities": [{"text": "VBQ", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9534077048301697}, {"text": "LSH", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.963640034198761}]}], "datasetContent": [{"text": "Our text datasets are Reuters-21578 and TDT-2., which consists of 60,000 images represented as 512 dimensional Gist descriptors).", "labels": [], "entities": [{"text": "Reuters-21578", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.9652920365333557}, {"text": "TDT-2.", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.9481977820396423}]}, {"text": "All of the datasets are identical to those that have been used in previous ANN hashing work () (Kong and Li, 2012) and are publicly available on the Internet.", "labels": [], "entities": [{"text": "ANN hashing", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.9348763227462769}]}, {"text": "We adopt the standard Hamming ranking evaluation paradigm ( ).", "labels": [], "entities": []}, {"text": "We randomly select 1000 query data points per run.", "labels": [], "entities": []}, {"text": "Our results are averaged over 10 runs, and the average reported.", "labels": [], "entities": [{"text": "average", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.989489734172821}]}, {"text": "The \u01eb-neighbours of each query point [3] [5] [2] [4] [6] form the ground truth for evaluation.", "labels": [], "entities": []}, {"text": "The threshold \u01eb is computed by sampling 100 training datapoints at random from the training dataset and determining the distance at which these points have 50 nearest neighbours on average.", "labels": [], "entities": []}, {"text": "Positive pairs and negative pairs for F-measure computation are computed by thresholding the training dataset Euclidean distance matrix by \u01eb.", "labels": [], "entities": []}, {"text": "We adopt the Manhattan distance and multi-bit binary encoding method as suggested in ( ).", "labels": [], "entities": []}, {"text": "The F-measure we use for threshold optimisation is: We select the parameter \u03b2 on a held-out validation dataset.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9952540397644043}]}, {"text": "The area under the precision-recall curve (AUPRC) is used to evaluate the quality of retrieval.", "labels": [], "entities": [{"text": "precision-recall curve (AUPRC)", "start_pos": 19, "end_pos": 49, "type": "METRIC", "confidence": 0.9448244452476502}]}, {"text": "(top row) illustrates that VBQ is effective across a range of bit budgets.", "labels": [], "entities": []}, {"text": "(bottom row) presents the precision-recall (PR) curves at 32 bits (CIFAR-10) and 128 bits (TDT-2, Reuters-21578).", "labels": [], "entities": [{"text": "precision-recall (PR)", "start_pos": 26, "end_pos": 47, "type": "METRIC", "confidence": 0.951689288020134}, {"text": "TDT-2", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.8957638144493103}, {"text": "Reuters-21578", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.8388473987579346}]}, {"text": "We confirm our hypothesis that judicious allocation of variable bits is significantly more effective than uniform allocation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Area under the Precision Recall curve (AUPRC) for all five projection methods. Results are for  32 bits (images) and at 128 bits (text). The best overall score for each dataset is shown in bold face.", "labels": [], "entities": [{"text": "Area", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9669498205184937}, {"text": "Precision Recall curve (AUPRC)", "start_pos": 25, "end_pos": 55, "type": "METRIC", "confidence": 0.916308730840683}]}]}