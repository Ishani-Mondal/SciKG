{"title": [{"text": "Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm", "labels": [], "entities": [{"text": "PageRank Algorithm", "start_pos": 87, "end_pos": 105, "type": "DATASET", "confidence": 0.8976582288742065}]}], "abstractContent": [{"text": "In this paper, we propose a method to raise the accuracy of text classification based on latent topics, reconsidering the techniques necessary for good classification-for example, to decide important sentences in a document, the sentences with important words are usually regarded as important sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9992166757583618}, {"text": "text classification", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.6836816668510437}]}, {"text": "In this case, tf.idf is often used to decide important words.", "labels": [], "entities": []}, {"text": "On the other hand, we apply the PageRank algorithm to rank important words in each document.", "labels": [], "entities": []}, {"text": "Furthermore, before clustering documents , we refine the target documents by representing them as a collection of important sentences in each document.", "labels": [], "entities": []}, {"text": "We then classify the documents based on latent information in the documents.", "labels": [], "entities": []}, {"text": "As a clustering method, we employ the k-means algorithm and investigate how our proposed method works for good clustering.", "labels": [], "entities": []}, {"text": "We conduct experiments with Reuters-21578 corpus under various conditions of important sentence extraction, using latent and surface information for clustering, and have confirmed that our proposed method provides better result among various conditions for clustering.", "labels": [], "entities": [{"text": "Reuters-21578 corpus", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.977045476436615}, {"text": "sentence extraction", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7409214824438095}]}], "introductionContent": [{"text": "Text classification is an essential issue in the field of natural language processing and many techniques using latent topics have so far been proposed and used under many purposes.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8619129657745361}, {"text": "natural language processing", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6556623379389445}]}, {"text": "In this paper, we aim to raise the accuracy of text classification using latent information by reconsidering elemental techniques necessary for good classification in the following three points: 1) important words extraction -to decide important words in documents is a crucial issue for text classification, tf.idf is often used to decide them.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9989719390869141}, {"text": "text classification", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7647528350353241}, {"text": "important words extraction", "start_pos": 198, "end_pos": 224, "type": "TASK", "confidence": 0.638680229584376}, {"text": "text classification", "start_pos": 288, "end_pos": 307, "type": "TASK", "confidence": 0.8451178073883057}]}, {"text": "Whereas, we apply the PageRank algorithm) for the issue, because the algorithm scores the centrality of anode in a graph, and important words should be regarded as having the centrality (.", "labels": [], "entities": []}, {"text": "Besides, the algorithm can detect centrality in any kind of graph, so we can find important words for any purposes.", "labels": [], "entities": []}, {"text": "In our study, we express the relation of word cooccurrence in the form of a graph.", "labels": [], "entities": []}, {"text": "This is because we use latent information to classify documents, and documents with high topic coherence tend to have high PMI of words in the documents ().", "labels": [], "entities": [{"text": "PMI", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9959130883216858}]}, {"text": "So, we construct a graph from a viewpoint of text classification based on latent topics.", "labels": [], "entities": [{"text": "text classification", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7616216838359833}]}, {"text": "2) Refinement of the original documents -we recompile the original documents with a collection of the extracted important sentences in order to refine the original documents for more sensitive to be classified.", "labels": [], "entities": [{"text": "Refinement", "start_pos": 3, "end_pos": 13, "type": "TASK", "confidence": 0.9675683975219727}]}, {"text": "3) Information used for classification -we use latent information estimated by latent Dirichlet allocation (LDA) () to classify documents, and compare the results of the cases using both surface and latent information.", "labels": [], "entities": [{"text": "classification", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.970855712890625}, {"text": "latent Dirichlet allocation (LDA)", "start_pos": 79, "end_pos": 112, "type": "METRIC", "confidence": 0.8400093813737234}]}, {"text": "We experiment text classification with Reuters-21578 corpus; evaluate the result of our method with the results of those which have various other settings for classification; and show the usefulness of our proposed method.", "labels": [], "entities": [{"text": "text classification", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7388376891613007}, {"text": "Reuters-21578 corpus", "start_pos": 39, "end_pos": 59, "type": "DATASET", "confidence": 0.9854692816734314}]}], "datasetContent": [{"text": "We evaluate our proposed method by comparing the accuracy of document clustering between our method and the method using tf.idf for extracting important words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9994217157363892}, {"text": "document clustering", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.6648828685283661}]}, {"text": "As the documents for experiments, we use Reuters-21578 dataset 1 collected from the Reuters newswire in 1987.In our proposed method, the refined documents consisting of important sentences extracted from the original documents are classified, therefore, if there are not many sentences in a document, we will not be able to verify the usefulness of our proposed method.", "labels": [], "entities": [{"text": "Reuters-21578 dataset 1 collected from the Reuters newswire", "start_pos": 41, "end_pos": 100, "type": "DATASET", "confidence": 0.9459487497806549}]}, {"text": "So, we use the documents which have more than 5 sentences in themselves.", "labels": [], "entities": []}, {"text": "Of the 135 potential topic categories in Reuters-21578, referring to other clustering study), we also use the most frequent 10 categories: i.e., earn, acq, grain, wheat, money, crude, trade, interest, ship, corn.", "labels": [], "entities": [{"text": "Reuters-21578", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.9843667149543762}]}, {"text": "In the sequel, we use 792 documents whose number of words is 15,835 for experiments -the 792 documents are the all documents which have more than 5 sentences in themselves in the corpus.", "labels": [], "entities": []}, {"text": "For each document, stemming and stop-word removal processes are adopted.", "labels": [], "entities": [{"text": "stop-word removal", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7197476327419281}]}, {"text": "Furthermore, the hyper-parameters for topic probability distribution and word probability distribution in LDA are \u03b1=0.5 and \u03b2=0.5, respectively.", "labels": [], "entities": []}, {"text": "We use Gibbs sampling and the number of iteration is 200.", "labels": [], "entities": []}, {"text": "The number of latent topics is decided by perplexity, and we decide the optimal number of topics by the minimum value of the average of 10 times trial, changing the number of topics ranging from 1 to 30.", "labels": [], "entities": []}, {"text": "As the first step for clustering with our method, in this study we employ the k-means clustering algorithm because it is a representative and a simple clustering algorithm.", "labels": [], "entities": [{"text": "clustering", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.9675218462944031}]}, {"text": "For evaluation, we use both accuracy and F-value, referring to the methods used in).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9996464252471924}, {"text": "F-value", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9985659718513489}]}, {"text": "As fora document d i , l i is the label provided to d i by the clustering algorithm, and \u03b1 i is the correct label ford i . The accuracy is expressed in equation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9995342493057251}]}, {"text": "\u03b4 (x, y) is 1 if x = y, otherwise 0.", "labels": [], "entities": []}, {"text": "map (l i ) is the label provided to d i by the k-means clustering algorithm.", "labels": [], "entities": []}, {"text": "For evaluation, the F-value of each category is computed and then the average of the F-values of the whole categories, used as an index for evaluation, is computed (see, equation).", "labels": [], "entities": [{"text": "F-value", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9839551448822021}]}, {"text": "As the initial data for the k-means clustering algorithm, a correct document of each category is randomly selected and provided.", "labels": [], "entities": []}, {"text": "By this, the category of classified data can be identified as in).", "labels": [], "entities": []}, {"text": "To obtain the final result of the experiment, we applied the k-means clustering algorithm for 10 times for the data set and averaged the results.", "labels": [], "entities": []}, {"text": "Here, in the case of clustering the documents based on the topic probabilistic distribution by LDA, the topic distribution over documents \u03b8 is changed in every estimation.", "labels": [], "entities": []}, {"text": "Therefore, we estimated \u03b8 for 8 times and then applied the k-means clustering algorithm with each \u03b8 for 10 times.", "labels": [], "entities": []}, {"text": "We averaged the results of the 10 trials and finally evaluated it.", "labels": [], "entities": []}, {"text": "The number of latent topics was estimated as 11 by perplexity.", "labels": [], "entities": []}, {"text": "We used it in the experiments.", "labels": [], "entities": []}, {"text": "To measure the latent similarity among documents, we construct topic vectors with the topic probabilistic distribution, and then adopt the Jensen-Shannon divergence to measures it, on the other hand, in the case of using document vectors we adopt cosine similarity. and show the cases of with and without refining the original documents by recompiling the original documents with the important sentences., 4 show the number of words and sentences after applying each method to decide important words.", "labels": [], "entities": []}, {"text": "Furthermore, show the accuracy and F-value of both methods, i.e., PageRank scores and tf.idf, in the case that we use the same number of sentences in the experiment to experiment under the same conditions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9997326731681824}, {"text": "F-value", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9992080330848694}]}], "tableCaptions": [{"text": " Table 1: Extracting important sentences  Methods  Measure  Accuracy F-value  PageRank Jenshen-Shannon  0.567  0.485  Cosine similarity  0.287  0.291  tf.idf  Jenshen-Shannon  0.550  0.435  Cosine similarity  0.275  0.270", "labels": [], "entities": [{"text": "Extracting important sentences", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.8893579244613647}, {"text": "Accuracy F-value  PageRank Jenshen-Shannon  0.567  0.485  Cosine similarity  0.287  0.291  tf.idf  Jenshen-Shannon  0.550  0.435  Cosine similarity  0.275  0.270", "start_pos": 60, "end_pos": 221, "type": "METRIC", "confidence": 0.7451769759257635}]}, {"text": " Table 2: Without extracting important sentences  Similarity measure Accuracy F-value  Jenshen-Shannon  0.518  0.426  Cosine similarity  0.288  0.305", "labels": [], "entities": [{"text": "Accuracy F-value  Jenshen-Shannon  0.518  0.426  Cosine similarity  0.288  0.305", "start_pos": 69, "end_pos": 149, "type": "METRIC", "confidence": 0.87042326397366}]}, {"text": " Table 3: Change of number of words", "labels": [], "entities": [{"text": "Change", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9278177618980408}]}, {"text": " Table 4: Change of number of sentences", "labels": [], "entities": [{"text": "Change", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9377899765968323}]}, {"text": " Table 5: Accuracy to the number of topics", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9978512525558472}]}, {"text": " Table 6: F-value to the number of topics", "labels": [], "entities": [{"text": "F-value", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9985180497169495}]}]}