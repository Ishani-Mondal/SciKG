{"title": [{"text": "Nonparametric Bayesian Inference and Efficient Parsing for Tree-adjoining Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "In the line of research extending statistical parsing to more expressive grammar formalisms, we demonstrate for the first time the use of tree-adjoining grammars (TAG).", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.783154159784317}]}, {"text": "We present a Bayesian non-parametric model for estimating a proba-bilistic TAG from a parsed corpus, along with novel block sampling methods and approximation transformations for TAG that allow efficient parsing.", "labels": [], "entities": []}, {"text": "Our work shows performance improvements on the Penn Treebank and finds more compact yet linguistically rich representations of the data, but more importantly provides techniques in grammar transformation and statistical inference that make practical the use of these more expressive systems, thereby enabling further experimentation along these lines.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9959748387336731}, {"text": "grammar transformation", "start_pos": 181, "end_pos": 203, "type": "TASK", "confidence": 0.7361046820878983}]}], "introductionContent": [{"text": "There is a deep tension in statistical modeling of grammatical structure between providing good expressivity -to allow accurate modeling of the data with sparse grammars -and low complexity -making induction of the grammars (say, from a treebank) and parsing of novel sentences computationally practical.", "labels": [], "entities": [{"text": "statistical modeling of grammatical structure", "start_pos": 27, "end_pos": 72, "type": "TASK", "confidence": 0.8283445239067078}, {"text": "parsing of novel sentences", "start_pos": 251, "end_pos": 277, "type": "TASK", "confidence": 0.8541027754545212}]}, {"text": "Tree-substitution grammars (TSG), by expanding the domain of locality of context-free grammars (CFG), can achieve better expressivity, and the ability to model more contextual dependencies; the payoff would be better modeling of the data or smaller (sparser) models or both.", "labels": [], "entities": []}, {"text": "For instance, constructions that go across levels, like the predicate-argument structure of a verb and its arguments can be modeled by TSGs.", "labels": [], "entities": []}, {"text": "Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the daunting model selection problem of segmenting training data trees into appropriate elementary fragments to form the grammar ().", "labels": [], "entities": []}, {"text": "The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) \"split them up\", preventing their reuse, leading to less sparse grammars than might be ideal.", "labels": [], "entities": []}, {"text": "TSGs area special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (.", "labels": [], "entities": [{"text": "TSGs", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5049625039100647}]}, {"text": "TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for \"splicing in\" of syntactic fragments within trees.", "labels": [], "entities": []}, {"text": "This functionality allows for better modeling of linguistic phenomena such as the distinction between modifiers and arguments).", "labels": [], "entities": []}, {"text": "Unfortunately, TAG's expressivity comes at the cost of greatly increased complexity.", "labels": [], "entities": [{"text": "TAG", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.935123860836029}]}, {"text": "Parsing complexity for unconstrained TAG scales as O(n 6 ), impractical as compared to CFG and TSG's O(n 3 ).", "labels": [], "entities": [{"text": "O", "start_pos": 51, "end_pos": 52, "type": "METRIC", "confidence": 0.9935437440872192}, {"text": "CFG", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.9220026731491089}]}, {"text": "In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators.", "labels": [], "entities": [{"text": "TAG", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.8503903150558472}]}, {"text": "This has led researchers to resort to manual () or heuristic techniques.", "labels": [], "entities": []}, {"text": "For example, one can consider \"outsourcing\" the auxiliary trees, use template rules and a very small number of grammar categories, or rely on head-words and force lexicalization in order to constrain the problem (.", "labels": [], "entities": []}, {"text": "However a solution has not been put forward by which a model that maximizes a principled probabilistic objective is sought after.", "labels": [], "entities": []}, {"text": "Recent work by argued that under highly expressive grammars such as TSGs where exponentially many derivations maybe hypothesized of the data, local Gibbs sampling is insufficient for effective inference and global blocked sampling strategies will be necessary.", "labels": [], "entities": []}, {"text": "For TAG, this problem is only more severe due to its mild context-sensitivity and even richer combinatorial nature.", "labels": [], "entities": [{"text": "TAG", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9637629389762878}]}, {"text": "Therefore in previous work, and Yamangil and Shieber (2012) used tree-insertion grammar (TIG) as a kind of expressive compromise between TSG and TAG, as a substrate on which to build nonparametric inference.", "labels": [], "entities": []}, {"text": "However TIG has the constraint of disallowing wrapping adjunction (coordination between material that falls to the left and right of the point of adjunction, such as parentheticals and quotations) as well as left adjunction along the spine of aright auxiliary tree and vice versa.", "labels": [], "entities": [{"text": "TIG", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.7536510229110718}]}, {"text": "In this work we formulate a blocked sampling strategy for TAG that is effective and efficient, and prove its superiority against the local Gibbs sampling approach.", "labels": [], "entities": [{"text": "TAG", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.8162806034088135}]}, {"text": "We show via nonparametric inference that TAG, which contains TSG as a subset, is a better model for treebank data than TSG and leads to improved parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 145, "end_pos": 152, "type": "TASK", "confidence": 0.9665974378585815}]}, {"text": "TAG achieves this by using more compact grammars than TSG and by providing the ability to make finer-grained linguistic distinctions.", "labels": [], "entities": []}, {"text": "We explain how our parameter refinement scheme for TAG allows for cubic-time CFG parsing, which is just as efficient as TSG parsing.", "labels": [], "entities": [{"text": "CFG parsing", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.7609875202178955}, {"text": "TSG parsing", "start_pos": 120, "end_pos": 131, "type": "TASK", "confidence": 0.6523915529251099}]}, {"text": "Our presentation assumes familiarity with prior work on block sampling of TSG and TIG).", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the standard Penn treebank methodology of training on sections 2-21 and testing on section 23.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.9836302101612091}]}, {"text": "All our data is head-binarized, all hyperparameters are resampled under appropriate vague gamma and beta priors.", "labels": [], "entities": []}, {"text": "Samplers are run 1000 iterations each; all reported numbers are averages over 5 runs.", "labels": [], "entities": []}, {"text": "For simplicity, parsing results are based on the maximum probability derivation (Viterbi algorithm).", "labels": [], "entities": [{"text": "parsing", "start_pos": 16, "end_pos": 23, "type": "TASK", "confidence": 0.9790893197059631}]}, {"text": "In, we compare TAG inference schemes and TSG.", "labels": [], "entities": [{"text": "TAG inference", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.8286793231964111}, {"text": "TSG", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.4589845836162567}]}, {"text": "TAG Gibbs operates by locally adding/removing potential adjunctions, similar to.", "labels": [], "entities": []}, {"text": "TAG is the O(n 2 ) algorithm that disallows spine adjunction.", "labels": [], "entities": [{"text": "TAG", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8395606875419617}]}, {"text": "We see that TAG has the best parsing performance, while TAG provides the most compact representation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.973463773727417}]}], "tableCaptions": []}