{"title": [], "abstractContent": [{"text": "While the resolution of term ambiguity is important for information extraction (IE) systems, the cost of resolving each instance of an entity can be prohibitively expensive on large datasets.", "labels": [], "entities": [{"text": "resolution of term ambiguity", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8432258665561676}, {"text": "information extraction (IE)", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.8644429206848144}]}, {"text": "To combat this, this work looks at ambiguity detection at the term, rather than the instance, level.", "labels": [], "entities": []}, {"text": "By making a judgment about the general ambiguity of a term, a system is able to handle ambiguous and unambigu-ous cases differently, improving through-put and quality.", "labels": [], "entities": []}, {"text": "To address the term ambiguity detection problem, we employ a model that combines data from language models, ontologies, and topic mod-eling.", "labels": [], "entities": [{"text": "ambiguity detection", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.8073413670063019}]}, {"text": "Results over a dataset of entities from four product domains show that the proposed approach achieves significantly above baseline F-measure of 0.96.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9891014695167542}]}], "introductionContent": [{"text": "Many words, phrases, and referring expressions are semantically ambiguous.", "labels": [], "entities": []}, {"text": "This phenomenon, commonly referred to as polysemy, represents a problem for NLP applications, many of which inherently assume a single sense.", "labels": [], "entities": []}, {"text": "It can be particularly problematic for information extraction (IE), as IE systems often wish to extract information about only one sense of polysemous terms.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.8644163191318512}]}, {"text": "If nothing is done to account for this polysemy, frequent mentions of unrelated senses can drastically harm performance.", "labels": [], "entities": []}, {"text": "Several NLP tasks, such as word sense disambiguation, word sense induction, and named entity disambiguation, address this ambiguity problem to varying degrees.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.7235225240389506}, {"text": "word sense induction", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7791224718093872}, {"text": "named entity disambiguation", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.6349202791849772}]}, {"text": "While the goals and initial data assumptions vary between these tasks, all of them attempt to map an instance of a term seen in context to an individual sense.", "labels": [], "entities": []}, {"text": "While making a judgment for every instance maybe appropriate for small or medium sized data sets, the cost of applying these ambiguity resolution procedures becomes prohibitively expensive on large data sets of tens to hundreds of million items.", "labels": [], "entities": []}, {"text": "To combat this, this work zooms out to examine the ambiguity problem at a more general level.", "labels": [], "entities": []}, {"text": "To do so, we define an IE-centered ambiguity detection problem, which ties the notion of ambiguity to a given topical domain.", "labels": [], "entities": [{"text": "IE-centered ambiguity detection", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.8675963282585144}]}, {"text": "For instance, given that the terms Call of Juarez and A New Beginning can both reference video games, we would like to discover that only the latter case is likely to appear frequently in non-video game contexts.", "labels": [], "entities": []}, {"text": "The goal is to make a binary decision as to whether, given a term and a domain, we can expect every instance of that term to reference an entity in that domain.", "labels": [], "entities": []}, {"text": "By doing so, we segregate ambiguous terms from their unambiguous counterparts.", "labels": [], "entities": []}, {"text": "Using this segregation allows ambiguous and unambiguous instances to be treated differently while saving the processing time that might normally be spent attempting to disambiguate individual instances of unambiguous terms.", "labels": [], "entities": []}, {"text": "Previous approaches to handling word ambiguity employ a variety of disparate methods, variously relying on structured ontologies, gleaming insight from general word usage patterns via language models, or clustering the contexts in which words appear.", "labels": [], "entities": [{"text": "handling word ambiguity", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.6766570607821146}]}, {"text": "This work employs an ambiguity detection pipeline that draws inspiration from all of these methods to achieve high performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "Effectiveness To understand the contribution of the n-gram (NG), ontology (ON), and clustering (CL) based modules, we ran each separately, as well as every possible combination.", "labels": [], "entities": []}, {"text": "Results are shown in, where they are compared to a majority class (ambiguous) baseline.", "labels": [], "entities": []}, {"text": "As shown, all configurations outperform the baseline.", "labels": [], "entities": []}, {"text": "Of the three individual modules, the ngram and clustering methods achieve F-measure of around 0.9, while the ontology-based module performs only modestly above baseline.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9995342493057251}]}, {"text": "Unsurprisingly, the ontology method is affected heavily by its coverage, so its poor performance is primarily attributable to low recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9985103011131287}]}, {"text": "As noted, many IE tasks may involve sets of entities that are not found in common ontologies, limiting the ability of the ontology-based method alone.", "labels": [], "entities": [{"text": "IE tasks", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.9276218414306641}]}, {"text": "Additionally, ontologies maybe apt to list cases of strict ambiguity, rather than practical ambiguity.", "labels": [], "entities": []}, {"text": "That is, an ontology may list a term as ambiguous if there are several potential named entities it could refer to, even if the vast majority of references were to only a single entity.", "labels": [], "entities": []}, {"text": "Combining any two methods produced substantial performance increases over any of the individual runs.", "labels": [], "entities": []}, {"text": "The final system that employed all modules produced an F-measure of 0.960, a significant (p < 0.01) absolute increase of 15.4% over the baseline.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.99920254945755}]}, {"text": "Usefulness To establish that term ambiguity detection is actually helpful for IE, we conducted a preliminary study by integrating our pipeline into a commercially available rule-based IE system ().", "labels": [], "entities": [{"text": "term ambiguity detection", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.8048871755599976}, {"text": "IE", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.9920435547828674}]}, {"text": "The system takes a list of product names as input and outputs tweets associated with each product.", "labels": [], "entities": []}, {"text": "It utilizes rules that employ more conservative extraction for ambiguous entities.", "labels": [], "entities": []}, {"text": "Experiments were conducted over several million tweets using the terms from the video game and camera domains.", "labels": [], "entities": []}, {"text": "When no ambiguity detection was performed, all terms were treated as unambiguous.", "labels": [], "entities": []}, {"text": "The system produced very poor precision of 0.16 when no ambiguity detection was used, due to the extraction of irrelevant instances of ambiguous objects.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9991017580032349}]}, {"text": "In contrast, the system produced precision of 0.96 when ambiguity detection was employed.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9994049072265625}, {"text": "ambiguity detection", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7098365724086761}]}, {"text": "However, the inclusion of disambiguation did reduce the overall recall; the system that employed disambiguation returned only about 57% of the true positives returned by the system that did not employ disambiguation.", "labels": [], "entities": [{"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9992907047271729}]}, {"text": "Although this reduction in recall is significant, the overall impact of disambiguation is clearly positive, due to the stark difference in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9994919300079346}, {"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9975185394287109}]}, {"text": "Nonetheless, this limited study suggests that there is substantial room for improvement in the extraction system, although this is out of the scope of the current work.", "labels": [], "entities": [{"text": "extraction", "start_pos": 95, "end_pos": 105, "type": "TASK", "confidence": 0.9408828020095825}]}], "tableCaptions": [{"text": " Table 1: Example tweet annotations.", "labels": [], "entities": []}, {"text": " Table 1. If all individual tweet judg- ments for a term were marked as referring to a", "labels": [], "entities": []}]}