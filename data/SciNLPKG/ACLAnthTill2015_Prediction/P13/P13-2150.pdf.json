{"title": [{"text": "Explicit and Implicit Syntactic Features for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7669147551059723}]}], "abstractContent": [{"text": "Syntactic features are useful for many text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.8337748646736145}]}, {"text": "Among these, tree kernels (Collins and Duffy, 2001) have been perhaps the most robust and effective syntactic tool, appealing for their empirical success, but also because they do not require an answer to the difficult question of which tree features to use fora given task.", "labels": [], "entities": []}, {"text": "We compare tree kernels to different explicit sets of tree features on five diverse tasks, and find that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9989948868751526}]}, {"text": "Since explicit features are easy to generate and use (with publicly available tools), we suggest they should always be included as baseline comparisons in tree kernel method evaluations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Features computed over parse trees are useful fora range of discriminative tasks, including authorship attribution (, parse reranking (), language modeling, and native-language detection (.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.8135570883750916}, {"text": "language modeling", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.7852761745452881}, {"text": "native-language detection", "start_pos": 161, "end_pos": 186, "type": "TASK", "confidence": 0.7595613896846771}]}, {"text": "A major distinction among these uses of syntax is how the features are represented.", "labels": [], "entities": []}, {"text": "The implicit approach uses tree kernels, which make predictions with inner products between tree pairs.", "labels": [], "entities": []}, {"text": "These products can be computed efficiently with a dynamic program that produces weighted counts of all the shared tree fragments between a pair of trees, essentially incorporating all fragments without representing any of them explicitly.", "labels": [], "entities": []}, {"text": "Tree kernel approaches have been applied successfully in many areas of NLP (.", "labels": [], "entities": []}, {"text": "Tree kernels were inspired in part by ideas from Data-Oriented Parsing (, which was in turn motivated by uncertainty about which fragments to include in a grammar.", "labels": [], "entities": []}, {"text": "However, manual and automatic approaches to inducing tree fragments have recently been found to be useful in an explicit approach to text classification, which employs specific tree fragments as features in standard classifiers).", "labels": [], "entities": [{"text": "text classification", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7994573712348938}]}, {"text": "These feature sets necessarily represent only a small subset of all possible tree patterns, leaving open the question of what further gains might be had from the unusued fragments.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, explicit and implicit syntactic features have been explored largely independently.", "labels": [], "entities": []}, {"text": "Here, we compare them on a range of classification tasks: (1,2) grammatical classification (is a sentence written by a human?), (3) question classification (what type of answer is sought by this question?), and (4,5) native language prediction (what is the native language of a text's author?).", "labels": [], "entities": [{"text": "grammatical classification", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.7018788456916809}, {"text": "question classification", "start_pos": 132, "end_pos": 155, "type": "TASK", "confidence": 0.834736168384552}, {"text": "native language prediction (what is the native language of a text's author", "start_pos": 217, "end_pos": 291, "type": "TASK", "confidence": 0.6613826389823642}]}, {"text": "Our main contribution is to show that an explicit syntactic feature set performs as well or better than tree kernels on each tested task, and in orders of magnitude less time.", "labels": [], "entities": []}, {"text": "Since explicit features are simple to generate (with publicly available tools) and flexible to use, we recommend they be included as baseline comparisons in tree kernel method evaluations.", "labels": [], "entities": []}, {"text": "CFG rules Counts of depth-one contextfree grammar (CFG) productions obtained from the Berkeley parser ().", "labels": [], "entities": [{"text": "CFG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8452221751213074}]}, {"text": "C&J features The parse-tree reranking feature set of, extracted from the Berkeley parse trees.", "labels": [], "entities": [{"text": "C&J", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8887592355410258}]}, {"text": "TSG features We also parsed with a Bayesian tree substitution grammar (Post and Gildea, 2009, TSG) 2 and extracted fragment counts from Viterbi derivations.", "labels": [], "entities": [{"text": "TSG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7276661396026611}]}, {"text": "We build classifiers with Liblinear.", "labels": [], "entities": []}, {"text": "We divided each dataset into training, dev, and test sets.", "labels": [], "entities": []}, {"text": "We then trained an L2-regularized L1-loss support vector machine (-s 3) with a bias parameter of 1 (-B 1), optimizing the regularization parameter (-c) on the dev set over the range {0.0001 . .", "labels": [], "entities": []}, {"text": "100} by multiples of 10.", "labels": [], "entities": []}, {"text": "The best model was then used to classify the test set.", "labels": [], "entities": []}, {"text": "A sentence length feature was included for every sentence.", "labels": [], "entities": []}, {"text": "For tree kernels, we used SVM-light-TK 4) with the default settings (-t 5 -D 1 -L 0.4), 5 which also solves an L2-regularized L1-loss SVM optimization problem.", "labels": [], "entities": []}, {"text": "We tuned the regularization parameter (-c) on the dev set in the same manner as described above, providing 4 GB of memory to the kernel cache (-m 4000).", "labels": [], "entities": []}, {"text": "We used subset tree kernels, which compute the similarity between two trees by implicitly enumerating all possible fragments of the trees (in contrast with subtree kernels, where all fragments fully extend to the leaves).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Fine-grained classification accuracy  (the Wong and Dras (2010) score is the highest  score from the last column of their Table 3).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9740402698516846}]}, {"text": " Table 4: Question classification (6 classes).", "labels": [], "entities": [{"text": "Question classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8775647282600403}]}]}