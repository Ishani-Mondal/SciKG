{"title": [{"text": "Easy-First POS Tagging and Dependency Parsing with Beam Search", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.6895118057727814}, {"text": "Dependency Parsing", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.5765215158462524}]}], "abstractContent": [{"text": "In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.6756855994462967}, {"text": "POS tagging", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.722680076956749}]}, {"text": "We propose a simple variant of \"early-update\" to ensure valid update in the training process.", "labels": [], "entities": [{"text": "early-update", "start_pos": 32, "end_pos": 44, "type": "METRIC", "confidence": 0.9460576176643372}]}, {"text": "The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity.", "labels": [], "entities": [{"text": "beam search", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.8327059149742126}]}, {"text": "On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width.", "labels": [], "entities": [{"text": "CTB", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.8752228021621704}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9837586283683777}, {"text": "attachment score", "start_pos": 64, "end_pos": 80, "type": "METRIC", "confidence": 0.8639807105064392}]}, {"text": "On PTB, we also achieve state-of-the-art performance.", "labels": [], "entities": [{"text": "PTB", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.7782953977584839}]}], "introductionContent": [{"text": "The easy-first dependency parsing algorithm) is attractive due to its good accuracy, fast speed and simplicity.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.6181124448776245}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9991738200187683}]}, {"text": "The easy-first parser has been applied to many applications (.", "labels": [], "entities": []}, {"text": "By processing the input tokens in an easyto-hard order, the algorithm could make use of structured information on both sides of the hard token thus making more indicative predictions.", "labels": [], "entities": []}, {"text": "However, rich structured information also causes exhaustive inference intractable.", "labels": [], "entities": []}, {"text": "As an alternative, greedy search which only explores a tiny fraction of the search space is adopted.", "labels": [], "entities": []}, {"text": "To enlarge the search space, a natural extension to greedy search is beam search.", "labels": [], "entities": []}, {"text": "Recent work also shows that beam search together with perceptron-based global learning enable the use of non-local features that are helpful to improve parsing performance without overfitting (.", "labels": [], "entities": [{"text": "beam search", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.8437901139259338}]}, {"text": "Due to these advantages, beam search and global learning has been applied to many NLP tasks.", "labels": [], "entities": [{"text": "beam search", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.8439408540725708}]}, {"text": "However, to the best of our knowledge, no work in the literature has ever applied the two techniques to easy-first dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.797185868024826}]}, {"text": "While applying beam-search is relatively straightforward, the main difficulty comes from combining easy-first dependency parsing with perceptron-based global learning.", "labels": [], "entities": []}, {"text": "In particular, one needs to guarantee that each parameter update is valid, i.e., the correct action sequence has lower model score than the predicted one 1 . The difficulty in ensuring validity of parameter update for the easy-first algorithm is caused by its spurious ambiguity, i.e., the same result might be derived by more than one action sequences.", "labels": [], "entities": []}, {"text": "For algorithms which do not exhibit spurious ambiguity, \"early update\") is always valid: at the k-th step when the single correct action sequence falls off the beam, As shown by, only valid update guarantees the convergence of any perceptron-based training.", "labels": [], "entities": [{"text": "early update", "start_pos": 57, "end_pos": 69, "type": "METRIC", "confidence": 0.9657067954540253}]}, {"text": "Invalid update may lead to bad learning or even make the learning not converge at all.", "labels": [], "entities": []}, {"text": "its model score must be lower than those still in the beam (as illustrated in, also seethe proof in ().", "labels": [], "entities": []}, {"text": "While for easyfirst dependency parsing, there could be multiple action sequences that yield the gold result (C 1 and C 2 in).", "labels": [], "entities": [{"text": "easyfirst dependency parsing", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.5517022212346395}]}, {"text": "When all correct sequences falloff the beam, some may indeed have higher model score than those still in the beam (C 2 in), causing invalid update.", "labels": [], "entities": []}, {"text": "For the purpose of valid update, we present a simple solution which is based on early update.", "labels": [], "entities": []}, {"text": "The basic idea is to use one of the correct action sequences that were pruned right at the k-th step (C 1 in) for parameter update.", "labels": [], "entities": []}, {"text": "The proposed solution is general and can also be applied to other algorithms that exhibit spurious ambiguity, such as easy-first POS tagging () and transition-based dependency parsing with dynamic oracle.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 129, "end_pos": 140, "type": "TASK", "confidence": 0.8144720196723938}, {"text": "transition-based dependency parsing", "start_pos": 148, "end_pos": 183, "type": "TASK", "confidence": 0.6157294313112894}]}, {"text": "In this paper, we report experimental results on both easy-first dependency parsing and POS tagging ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7233692705631256}, {"text": "POS tagging", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.8446080386638641}]}, {"text": "We show that both easy-first POS tagging and dependency parsing can be improved significantly from beam search and global learning.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.859213262796402}, {"text": "dependency parsing", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8115902841091156}]}, {"text": "Specifically, on CTB we achieve 94.01% tagging accuracy which is the best result to date 2 fora single tagging model.", "labels": [], "entities": [{"text": "CTB", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.8773406147956848}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9834377765655518}]}, {"text": "With a relatively small beam, we achieve 86.33% unlabeled score (assume gold tags), better than state-of-the-art transition-based parsers).", "labels": [], "entities": []}, {"text": "On PTB, we also achieve good results that are comparable to the state-of-the-art.", "labels": [], "entities": [{"text": "PTB", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.5902348160743713}]}], "datasetContent": [{"text": "For English, we use PTB as our data set.", "labels": [], "entities": [{"text": "PTB", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.6210029721260071}]}, {"text": "We use the standard split for dependency parsing and the split used by for POS tagging.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8857971429824829}, {"text": "POS tagging", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.6864736080169678}]}, {"text": "Penn2Malt 5 is used to convert the bracketed structure into dependencies.", "labels": [], "entities": [{"text": "Penn2Malt 5", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9429682493209839}]}, {"text": "For dependency parsing, POS tags of the training set are generated using 10-fold jack-knifing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8720974326133728}]}, {"text": "For Chinese, we use CTB 5.1 and the split suggested by) for both tagging and dependency parsing.", "labels": [], "entities": [{"text": "CTB 5.1", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.8916845321655273}, {"text": "tagging", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.9650598168373108}, {"text": "dependency parsing", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.8391903042793274}]}, {"text": "We also use Penn2Malt and the head-finding rules of ( to convert constituency trees into dependencies.", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 12, "end_pos": 21, "type": "DATASET", "confidence": 0.9769570827484131}]}, {"text": "For dependency parsing, we assume gold segmentation and POS tags for the input.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8738340735435486}]}, {"text": "Features used in English dependency parsing are listed in table 1.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8238354027271271}]}, {"text": "Besides the features in, we also include some trigram features and valency features which are useful for transition-based dependency parsing ().", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 105, "end_pos": 140, "type": "TASK", "confidence": 0.5766249497731527}]}, {"text": "For English POS tagging, we use the same features as in).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.8160074353218079}]}, {"text": "For Chinese POS tagging and dependency parsing, we use the same features as ().", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.7100770026445389}, {"text": "dependency parsing", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8908964097499847}]}, {"text": "All of our experiments are conducted on a Core i7 (2.93GHz) machine, both the tagger and parser are implemented using C++.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Tagging accuracy vs beam width vs. Speed is  evaluated using the number of sentences that can be  processed in one second", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9761133790016174}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9239914417266846}, {"text": "Speed", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9950272440910339}]}, {"text": " Table 3: Parsing accuracy vs beam width. 'uas' and  'compl' denote unlabeled score and complete match  rate respectively (all excluding punctuations).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9660077095031738}, {"text": "compl", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9392549395561218}, {"text": "complete match  rate", "start_pos": 88, "end_pos": 108, "type": "METRIC", "confidence": 0.85010959704717}]}, {"text": " Table 4: Tagging results on the test set. '  \u2020 ' denotes  statistically significant over the greedy baseline by  McNemar's test (  )", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9803142547607422}, {"text": "McNemar's test", "start_pos": 114, "end_pos": 128, "type": "DATASET", "confidence": 0.8440543214480082}]}, {"text": " Table 5: Parsing results on CTB test set.", "labels": [], "entities": [{"text": "CTB test set", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.9609027703603109}]}]}