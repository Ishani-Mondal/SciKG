{"title": [{"text": "Online Relative Margin Maximization for Statistical Machine Translation", "labels": [], "entities": [{"text": "Relative Margin Maximization", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.9646106362342834}, {"text": "Statistical Machine Translation", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.8717430432637533}]}], "abstractContent": [{"text": "Recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization, such as the spread of the data.", "labels": [], "entities": []}, {"text": "However, these solutions are impractical in complex struc-tured prediction problems such as statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 92, "end_pos": 123, "type": "TASK", "confidence": 0.7117878297964731}]}, {"text": "We present an online gradient-based algorithm for relative margin maximization, which bounds the spread of the projected data while maximizing the margin.", "labels": [], "entities": [{"text": "relative margin maximization", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.6625409324963888}]}, {"text": "We evaluate our op-timizer on Chinese-English and Arabic-English translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant improvements of 1.2-2 BLEU and 1.7-4.3 TER on average over state-of-the-art opti-mizers with the large feature set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 203, "end_pos": 207, "type": "METRIC", "confidence": 0.9957401752471924}, {"text": "TER", "start_pos": 220, "end_pos": 223, "type": "METRIC", "confidence": 0.9822225570678711}]}], "introductionContent": [{"text": "The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT), and toward other discriminative methods that can optimize more features.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 79, "end_pos": 116, "type": "TASK", "confidence": 0.7797655711571375}, {"text": "Minimum Error Rate Training (MERT", "start_pos": 161, "end_pos": 194, "type": "METRIC", "confidence": 0.7590455909570059}]}, {"text": "Examples include minimum risk), pairwise ranking (PRO) (), RAMPION (, and variations of the margin-infused relaxation algorithm (MIRA) (.", "labels": [], "entities": [{"text": "pairwise ranking (PRO)", "start_pos": 32, "end_pos": 54, "type": "METRIC", "confidence": 0.7232310056686402}, {"text": "RAMPION", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9884374737739563}, {"text": "margin-infused relaxation algorithm (MIRA)", "start_pos": 92, "end_pos": 134, "type": "METRIC", "confidence": 0.8020665347576141}]}, {"text": "While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses.", "labels": [], "entities": []}, {"text": "In every SMT system, and in machine learning in general, the goal of learning is to find a model that generalizes well, i.e. one that will yield good translations for previously unseen sentences.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9907106757164001}]}, {"text": "However, as the dimension of the feature space increases, generalization becomes increasingly difficult.", "labels": [], "entities": [{"text": "generalization", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.968752384185791}]}, {"text": "Since only a small portion of all (sparse) features maybe observed in a relatively small fixed set of instances during tuning, we are prone to overfit the training data.", "labels": [], "entities": []}, {"text": "An alternative approach for solving this problem is estimating discriminative feature weights directly on the training bitext (, which is usually substantially larger than the tuning set, but this is complementary to our goal hereof better generalization given a fixed size tuning set.", "labels": [], "entities": []}, {"text": "In order to achieve that goal, we need to carefully choose what objective to optimize, and how to perform parameter estimation of w for this objective.", "labels": [], "entities": []}, {"text": "We focus on large-margin methods such as SVM and passive-aggressive algorithms such as MIRA.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.5050621032714844}]}, {"text": "Intuitively these seek aw such that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting the incorrect one.", "labels": [], "entities": []}, {"text": "This criterion performs well in practice at finding a linear separator in high-dimensional feature spaces ().", "labels": [], "entities": []}, {"text": "Now, recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information, as in the Second Order Perceptron (), Gaussian Margin Machines (), confidenceweighted learning), AROW ( and Relative Margin Machines (RMM)).", "labels": [], "entities": [{"text": "AROW", "start_pos": 254, "end_pos": 258, "type": "METRIC", "confidence": 0.7088727951049805}]}, {"text": "The latter, RMM, was introduced as an effective and less computationally expensive way to incorporate the spread of the data -second order information about the distance between hypotheses when projected onto the line defined by the weight vector w.", "labels": [], "entities": []}, {"text": "Unfortunately, not all advances in machine learning are easy to apply to structured prediction problems such as SMT; the latter often involve latent variables and surrogate references, resulting in loss functions that have not been well explored in machine learning (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9911710619926453}]}, {"text": "Although Shivaswamy and Jebara extended RMM to handle sequential structured prediction (Shivaswamy and Jebara, 2009a), their batch approach to quadratic optimization, using existing off-the-shelf QP solvers, does not provide a practical solution: as observe, \"off-the-shelf QP solvers tend to scale poorly with problem and training sample size\" for structured prediction problems..", "labels": [], "entities": [{"text": "sequential structured prediction", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.6797822713851929}, {"text": "quadratic optimization", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.7082057595252991}, {"text": "QP solvers", "start_pos": 274, "end_pos": 284, "type": "TASK", "confidence": 0.7510460019111633}]}, {"text": "This motivates an online gradient-based optimization approach-an approach that is particularly attractive because its simple update is well suited for efficiently processing structured objects with sparse features.", "labels": [], "entities": [{"text": "online gradient-based optimization", "start_pos": 18, "end_pos": 52, "type": "TASK", "confidence": 0.660571277141571}]}, {"text": "The contributions of this paper include (1) introduction of a loss function for structured RMM in the SMT setting, with surrogate reference translations and latent variables; (2) an online gradientbased solver, RM, with a closed-form parameter update to optimize the relative margin loss; and (3) an efficient implementation that integrates well with the open source cdec SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9934849143028259}, {"text": "SMT", "start_pos": 372, "end_pos": 375, "type": "TASK", "confidence": 0.7875069379806519}]}, {"text": "In addition, (4) as our solution is not dependent on any specific QP solver, it can be easily incorporated into practically any gradientbased learning algorithm.", "labels": [], "entities": []}, {"text": "After background discussion on learning in SMT ( \u00a72), we introduce a novel online learning algorithm for relative margin maximization suitable for SMT ( \u00a73).", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9770814776420593}, {"text": "relative margin maximization", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.6085953116416931}, {"text": "SMT", "start_pos": 147, "end_pos": 150, "type": "TASK", "confidence": 0.9867677092552185}]}, {"text": "First, we introduce RMM ( \u00a73.1) and propose a latent structured relative margin objective which incorporates cost-augmented hypothesis selection and latent variables.", "labels": [], "entities": [{"text": "RMM", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.6750157475471497}]}, {"text": "Then, we derive a simple closed-form online update necessary to create a large margin solution while simultaneously bounding the spread of the projection of the data ( \u00a73.2).", "labels": [], "entities": []}, {"text": "Chinese-English translation experiments show that our algorithm, RM, significantly outperforms strong state-of-the-art optimizers, in both a basic feature setting and high-dimensional (sparse) feature space ( \u00a74).", "labels": [], "entities": []}, {"text": "Additional ArabicEnglish experiments further validate these results, 1 https://github.com/veidel/cdec even where previously MERT was shown to be advantageous ( \u00a75).", "labels": [], "entities": [{"text": "MERT", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.7193665504455566}]}, {"text": "Finally, we discuss the spread and other key issues of RM ( \u00a76), and conclude with discussion of future work ( \u00a77).", "labels": [], "entities": [{"text": "spread", "start_pos": 24, "end_pos": 30, "type": "TASK", "confidence": 0.9625109434127808}, {"text": "RM", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.7855810523033142}]}], "datasetContent": [{"text": "In order to explore the applicability of our approach to a wider range of languages, we also evaluated its performance on Arabic-English translation.", "labels": [], "entities": [{"text": "Arabic-English translation", "start_pos": 122, "end_pos": 148, "type": "TASK", "confidence": 0.7011464536190033}]}, {"text": "All experimental details were the same as above, except those noted below.", "labels": [], "entities": []}, {"text": "For training, we used the non-UN portion of the NIST training corpora, which was segmented using an HMM segmenter (.", "labels": [], "entities": [{"text": "NIST training corpora", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.9222994446754456}]}, {"text": "Dataset statistics are given in the bottom part of.", "labels": [], "entities": [{"text": "Dataset", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8207063674926758}]}, {"text": "The sparse feature templates resulted herein a total of 4.9 million possible features, of which again only a fraction were active, as shown in.", "labels": [], "entities": []}, {"text": "As can be seen in, in the smaller feature set, RM and MERT were the best performers, with the exception that on MT08, MIRA yielded somewhat better (+0.7) BLEU but a somewhat worse (-0.9) TER score than RM.", "labels": [], "entities": [{"text": "MERT", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9803316593170166}, {"text": "MT08", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.9134529232978821}, {"text": "MIRA", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9874445199966431}, {"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.998250424861908}, {"text": "TER score", "start_pos": 187, "end_pos": 196, "type": "METRIC", "confidence": 0.9848340153694153}]}, {"text": "On the large feature set, RM is again the best performer, except, perhaps, a tied BLEU score with MIRA on MT08, but with a clear 1.8 TER gain.", "labels": [], "entities": [{"text": "RM", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.8044455647468567}, {"text": "BLEU score", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9802646934986115}, {"text": "MIRA", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.992155134677887}, {"text": "MT08", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.8813021183013916}, {"text": "TER", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9940052628517151}]}, {"text": "In both Arabic-English feature sets, MIRA seems to take the second place, while RAMPION lags behind, unlike in Chinese-English ( \u00a74).", "labels": [], "entities": [{"text": "MIRA", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9720206260681152}, {"text": "RAMPION", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9909561276435852}]}, {"text": "Interestingly, RM achieved substantially higher BLEU precision scores in all tests for both language pairs.", "labels": [], "entities": [{"text": "RM", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9310712814331055}, {"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9994171857833862}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.7653152942657471}]}, {"text": "However, this was also usually coupled had a higher brevity penalty (BP) than MIRA, with the BP increasing slightly when moving to the sparse setting.", "labels": [], "entities": [{"text": "brevity penalty (BP)", "start_pos": 52, "end_pos": 72, "type": "METRIC", "confidence": 0.9021108984947205}, {"text": "MIRA", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9213089942932129}, {"text": "BP", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9973207116127014}]}], "tableCaptions": [{"text": " Table 3: Performance on Zh-En with basic (left) and sparse (right) feature sets on MT03 and MT05.", "labels": [], "entities": [{"text": "MT03", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.962242603302002}, {"text": "MT05", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.8699279427528381}]}, {"text": " Table 4: Performance on Ar-En with basic (left) and sparse (right) feature sets on MT05 and MT08.", "labels": [], "entities": [{"text": "MT05", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.963850200176239}, {"text": "MT08", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.8675935864448547}]}, {"text": " Table 5: RM gain over other optimizers averaged  over all test sets.", "labels": [], "entities": [{"text": "RM gain", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7474729716777802}]}]}