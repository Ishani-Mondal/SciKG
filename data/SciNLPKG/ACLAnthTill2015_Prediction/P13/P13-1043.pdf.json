{"title": [{"text": "Fast and Accurate Shift-Reduce Constituent Parsing", "labels": [], "entities": [{"text": "Accurate", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9880353808403015}, {"text": "Shift-Reduce Constituent Parsing", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.7528992096583048}]}], "abstractContent": [{"text": "Shift-reduce dependency parsers give comparable accuracies to their chart-based counterparts, yet the best shift-reduce constituent parsers still lag behind the state-of-the-art.", "labels": [], "entities": [{"text": "Shift-reduce dependency parsers", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6849812169869741}]}, {"text": "One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input.", "labels": [], "entities": []}, {"text": "This turns out to have a large empirical impact on the framework of global training and beam search.", "labels": [], "entities": [{"text": "beam search", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.900324136018753}]}, {"text": "We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search.", "labels": [], "entities": []}, {"text": "Our parser gives comparable accuracies to the state-of-the-art chart parsers.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9714798331260681}]}, {"text": "With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions.", "labels": [], "entities": []}, {"text": "The pioneering models rely on a classifier to make local decisions, and search greedily fora transition sequence to build a parse tree.", "labels": [], "entities": []}, {"text": "Greedy, classifier-based parsers have been developed for both dependency grammars () and phrase-structure grammars).", "labels": [], "entities": []}, {"text": "With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers).", "labels": [], "entities": []}, {"text": "Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.8199737668037415}]}, {"text": "While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases ().", "labels": [], "entities": []}, {"text": "This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features.", "labels": [], "entities": []}, {"text": "With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (.", "labels": [], "entities": []}, {"text": "In addition, processing tens of sentences per second (), these transition-based parsers can be a favorable choice for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.8504915535449982}]}, {"text": "The above global-learning and beam-search framework can be applied to transition-based phrase-structure (constituent) parsing also (, maintaining all the aforementioned benefits.", "labels": [], "entities": [{"text": "transition-based phrase-structure (constituent) parsing", "start_pos": 70, "end_pos": 125, "type": "TASK", "confidence": 0.5860545635223389}]}, {"text": "However, the effects were not as significant as for transition-based dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.6823360621929169}]}, {"text": "The best reported accuracies of transition-based constituent parsers still lag behind the state-of-the-art (.", "labels": [], "entities": []}, {"text": "One difference between phrasestructure parsing and dependency parsing is that for the former, parse trees with different numbers of unary rules require different numbers of actions to build.", "labels": [], "entities": [{"text": "phrasestructure parsing", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.7744787931442261}, {"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8127470016479492}]}, {"text": "Hence the scoring model needs to disambiguate between transitions sequences with different sizes.", "labels": [], "entities": []}, {"text": "For the same sentence, the largest output can take twice as many as actions to build as the smallest one.", "labels": [], "entities": []}, {"text": "This turns out to have a significant empirical impact on parsing with beam-search.", "labels": [], "entities": [{"text": "parsing", "start_pos": 57, "end_pos": 64, "type": "TASK", "confidence": 0.9873794317245483}]}, {"text": "We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the parsing accuracies.", "labels": [], "entities": [{"text": "parsing", "start_pos": 121, "end_pos": 128, "type": "TASK", "confidence": 0.9798509478569031}]}, {"text": "Our method is conceptually simple, requiring only one additional transition action to eliminate size differences between different candidate outputs.", "labels": [], "entities": []}, {"text": "On standard evaluations using both the Penn Treebank and the Penn Chinese Treebank, our parser gave higher accuracies than the Berkeley parser, a state-of-the-art chart parser.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.9933220446109772}, {"text": "Penn Chinese Treebank", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.9680276910463969}, {"text": "accuracies", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.993669331073761}]}, {"text": "In addition, our parser runs with over 89 sentences per second, which is 14 times faster than the Berkeley parser, and is the fastest that we are aware of for phrase-structure parsing.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 159, "end_pos": 183, "type": "TASK", "confidence": 0.7401517629623413}]}, {"text": "An open source release of our parser (version 0.6) is freely available on the Web.", "labels": [], "entities": []}, {"text": "In addition to the above contributions, we apply a variety of semi-supervised learning techniques to our transition-based parser.", "labels": [], "entities": []}, {"text": "These techniques have been shown useful to improve chart-based parsing (), but little work has been done for transition-based parsers.", "labels": [], "entities": []}, {"text": "We therefore fill a gap in the literature by reporting empirical results using these methods.", "labels": [], "entities": []}, {"text": "Experimental results show that semi-supervised methods give a further improvement of 0.9% in F-score on the English data and 2.4% on the Chinese data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9995235204696655}, {"text": "English data", "start_pos": 108, "end_pos": 120, "type": "DATASET", "confidence": 0.7888002097606659}]}, {"text": "Our Chinese results are the best that we are aware of on the standard CTB data.", "labels": [], "entities": [{"text": "CTB data", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.9696617424488068}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Statistics on sentence and word numbers  of the experimental data.", "labels": [], "entities": []}, {"text": " Table 3: Semi-supervised features designed on the base of word clusters, lexical dependencies, and  dependency language models. Here the symbol s i denotes a stack item, q i denotes a queue item, w  represents a word, and t represents a POS tag.", "labels": [], "entities": []}, {"text": " Table 5: Experimental results on the English and  Chinese development sets with the padding tech- nique and new supervised features added incre- mentally.", "labels": [], "entities": [{"text": "English and  Chinese development sets", "start_pos": 38, "end_pos": 75, "type": "DATASET", "confidence": 0.6188072144985199}]}, {"text": " Table 7: Comparison of our parsers and related  work on the English test set.  *  Shift-reduce  parsers.  \u2020 The results of self-training with a sin- gle latent annotation grammar.", "labels": [], "entities": [{"text": "English test set", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.8145611683527628}]}, {"text": " Table 8: Comparison of our parsers and related  work on the test set of CTB5.1.  *  Huang (2009)  adapted the parsers to Chinese parsing on CTB5.1.   \u2020 We run the parser on CTB5.1 to get the results.", "labels": [], "entities": [{"text": "CTB5.1", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.960393488407135}, {"text": "CTB5.1", "start_pos": 141, "end_pos": 147, "type": "DATASET", "confidence": 0.9427955150604248}, {"text": "CTB5.1", "start_pos": 174, "end_pos": 180, "type": "DATASET", "confidence": 0.9814515709877014}]}, {"text": " Table 8  reports the comparison on the Chinese test set.  From the results we can see that our extended  parser (baseline + padding + supervised features)  outperforms the Berkeley parser by 0.3% on En- glish, and is comparable with the Berkeley parser  on Chinese (\u22120.1% less). Here +padding means  the padding technique and the features in Table 2.  After integrating semi-supervised features, the  parsing accuracy on English is improved to 91.3%.  We note that the performance is on the same level", "labels": [], "entities": [{"text": "Chinese test set", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.8528249065081278}, {"text": "accuracy", "start_pos": 410, "end_pos": 418, "type": "METRIC", "confidence": 0.9290910363197327}]}, {"text": " Table 10: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parsers  on different phrase types.", "labels": [], "entities": []}]}