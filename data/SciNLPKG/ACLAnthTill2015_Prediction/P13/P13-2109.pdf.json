{"title": [{"text": "Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers", "labels": [], "entities": []}], "abstractContent": [{"text": "We present fast, accurate, direct non-projective dependency parsers with third-order features.", "labels": [], "entities": []}, {"text": "Our approach uses AD 3 , an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models.", "labels": [], "entities": []}, {"text": "Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-of-the-art accuracies for the largest datasets (English, Czech, and German).", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8814217746257782}]}, {"text": "In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming.", "labels": [], "entities": [{"text": "projective parsing", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.6541026830673218}]}, {"text": "This results in cubic-time decoders for arc-factored and sibling second-order models), and quartic-time for grandparent models and third-order models . Recently, trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems.", "labels": [], "entities": []}, {"text": "Third-order features have also been included in transition systems () and graph-based parsers with cube-pruning (.", "labels": [], "entities": []}, {"text": "Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances.", "labels": [], "entities": []}, {"text": "The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models.", "labels": [], "entities": []}, {"text": "Approximate parsers have therefore been introduced, based on belief propagation, dual decomposition ( , or multi-commodity flows).", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7478901147842407}]}, {"text": "These are all instances of turbo parsers, as shown by: the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects.", "labels": [], "entities": []}, {"text": "While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9986686706542969}]}, {"text": "This paper bridges the gap above by presenting the following contributions: \u2022 We apply the third-order feature models of  to non-projective parsing.", "labels": [], "entities": [{"text": "non-projective parsing", "start_pos": 125, "end_pos": 147, "type": "TASK", "confidence": 0.6109278202056885}]}, {"text": "\u2022 This extension is non-trivial since exact dynamic programming is not applicable.", "labels": [], "entities": []}, {"text": "Instead, we adapt AD 3 , the dual decomposition algorithm proposed by, to handle third-order features, by introducing specialized head automata.", "labels": [], "entities": []}, {"text": "\u2022 We make our parser substantially faster than the many-components approach of.", "labels": [], "entities": []}, {"text": "While AD 3 requires solving quadratic subproblems as an intermediate step, recent results ( show that they can be addressed with the same oracles used in the subgradient method ( ).", "labels": [], "entities": []}, {"text": "This enables AD 3 to exploit combinatorial subproblems like the the head automata above.", "labels": [], "entities": []}, {"text": "Along with this paper, we provide a free distribution of our parsers, including training code.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first evaluated our non-projective parser in a projective English dataset, to see how its speed and accuracy compares with recent projective parsers, which can take advantage of dynamic programming.", "labels": [], "entities": [{"text": "speed", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.9601308107376099}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9968112111091614}]}, {"text": "To this end, we converted the Penn Treebank to dependencies through (i) the head rules of Yamada and Matsumoto (2003) (PTB-YM) and (ii) basic dependencies from the Stanford parser 2.0.5 (PTB-S).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.990261971950531}, {"text": "PTB-YM", "start_pos": 119, "end_pos": 125, "type": "DATASET", "confidence": 0.8686968684196472}, {"text": "PTB-S", "start_pos": 187, "end_pos": 192, "type": "DATASET", "confidence": 0.8624430298805237}]}, {"text": "11 We trained by running 10 epochs of cost-augmented MIRA ().", "labels": [], "entities": [{"text": "MIRA", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.5722914934158325}]}, {"text": "To ensure valid parse trees attest time, we rounded fractional solutions as inyet, solutions were integral \u2248 95% of the time.", "labels": [], "entities": []}, {"text": "2 shows the results in the dev-set (top block) and in the test-set (two bottom blocks).", "labels": [], "entities": []}, {"text": "In the dev-set, we see consistent gains when more expressive features are added, the best accuracies being achieved with the full third-order model; this comes at the cost of a 6-fold drop in runtime compared with a first-order model.", "labels": [], "entities": []}, {"text": "By looking at the two bottom blocks, we observe that our parser has slightly better accuracies than recent projective parsers, with comparable speed levels (with the exception of the highly optimized vine cascade approach of   In our second experiment (Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 253, "end_pos": 257, "type": "DATASET", "confidence": 0.9058265388011932}]}, {"text": "3), we used 14 datasets, most of which are non-projective, from the CoNLL 2006 and 2008 shared tasks.", "labels": [], "entities": [{"text": "CoNLL 2006 and 2008 shared tasks", "start_pos": 68, "end_pos": 100, "type": "DATASET", "confidence": 0.9194467862447103}]}, {"text": "Our third-order model achieved the best reported scores for English, Czech, German, and Dutchwhich includes the three largest datasets and the ones with the most non-projective dependenciesand is on par with the state of the art for the remaining languages.", "labels": [], "entities": []}, {"text": "To our knowledge, the speeds are the highest reported among higherorder non-projective parsers, and only about 3-4 times slower than the vine parser of, which has lower accuracies.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for the projective English dataset.  We report unlabeled attachment scores (UAS) ig- noring punctuation, and parsing speeds in tokens  per second. Our speeds include the time necessary  for pruning, evaluating features, and decoding, as  measured on a Intel Core i7 processor @3.4 GHz.  The others are speeds reported in the cited papers;  those marked with  \u2020 were converted from times per  sentence.", "labels": [], "entities": [{"text": "unlabeled attachment scores (UAS) ig- noring punctuation", "start_pos": 65, "end_pos": 121, "type": "METRIC", "confidence": 0.7250940829515458}]}]}