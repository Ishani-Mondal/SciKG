{"title": [{"text": "Advancements in Reordering Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Reordering", "start_pos": 16, "end_pos": 26, "type": "TASK", "confidence": 0.9910921454429626}, {"text": "Statistical Machine Translation", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.8376513322194418}]}], "abstractContent": [{"text": "In this paper, we propose a novel reordering model based on sequence labeling techniques.", "labels": [], "entities": []}, {"text": "Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task.", "labels": [], "entities": [{"text": "sequence labeling problem", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.7273815274238586}, {"text": "tagging task", "start_pos": 83, "end_pos": 95, "type": "TASK", "confidence": 0.9062084555625916}]}, {"text": "Results on five Chinese-English NIST tasks show that our model improves the baseline system by 1.32 BLEU and 1.53 TER on average.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9995535016059875}, {"text": "TER", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.998781144618988}]}, {"text": "Results of comparative study with other seven widely used reordering models will also be reported.", "labels": [], "entities": []}], "introductionContent": [{"text": "The systematic word order difference between two languages poses a challenge for current statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 89, "end_pos": 126, "type": "TASK", "confidence": 0.7774129112561544}]}, {"text": "The system has to decide in which order to translate the given source words.", "labels": [], "entities": []}, {"text": "This problem is known as the reordering problem.", "labels": [], "entities": []}, {"text": "As shown in, if arbitrary reordering is allowed, the search problem is NP-hard.", "labels": [], "entities": []}, {"text": "Many ideas have been proposed to address the reordering problem.", "labels": [], "entities": [{"text": "reordering", "start_pos": 45, "end_pos": 55, "type": "TASK", "confidence": 0.9616804718971252}]}, {"text": "Within the phrase-based SMT framework there are mainly three stages where improved reordering could be integrated: In the preprocessing: the source sentence is reordered by heuristics, so that the word order of source and target sentences is similar.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8751366138458252}]}, {"text": "() use manually designed rules to reorder parse trees of the source sentences.", "labels": [], "entities": []}, {"text": "Based on shallow syntax, () use rules to reorder the source sentences on the chunk level and provide a source-reordering lattice instead of a single reordered source sentence as input to the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 191, "end_pos": 194, "type": "TASK", "confidence": 0.9809935092926025}]}, {"text": "Designing rules to reorder the source sentence is conceptually clear and usually easy to implement.", "labels": [], "entities": []}, {"text": "In this way, syntax information can be incorporated into phrase-based SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.8365645408630371}]}, {"text": "However, one disadvantage is that the reliability of the rules is often language pair dependent.", "labels": [], "entities": [{"text": "reliability", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9747130870819092}]}, {"text": "In the decoder: we can add constraints or models into the decoder to reward good reordering options or penalize bad ones.", "labels": [], "entities": []}, {"text": "For reordering constraints, early work includes ITG constraints and IBM constraints.", "labels": [], "entities": []}, {"text": "( did comparative study over different reordering constraints.", "labels": [], "entities": []}, {"text": "This paper focuses on reordering models.", "labels": [], "entities": []}, {"text": "For reordering models, we can further roughly divide the existing methods into three genres: \u2022 The reordering is a classification problem.", "labels": [], "entities": []}, {"text": "The classifier will make decision on next phrase's relative position with current phrase.", "labels": [], "entities": []}, {"text": "The classifier can be trained with maximum likelihood like Moses lexicalized reordering ( ) and hierarchical lexicalized reordering model ( or be trained under maximum entropy framework ().", "labels": [], "entities": []}, {"text": "\u2022 The reordering is a decoding order problem.", "labels": [], "entities": []}, {"text": "() present a translation model that constitutes a language model of a sort of bilanguage composed of bilingual units.", "labels": [], "entities": []}, {"text": "From the reordering point of view, the idea is that the correct reordering is a suitable order of translation units.", "labels": [], "entities": []}, {"text": "() present a simpler version of)'s model which utilize only source words to model the decoding order.", "labels": [], "entities": []}, {"text": "\u2022 The reordering can be solved by outside heuristics.", "labels": [], "entities": []}, {"text": "We can put human knowledge into the decoder.", "labels": [], "entities": []}, {"text": "For example, the simple jump model using linear distance tells the decoder that usually the long range reordering should be avoided.", "labels": [], "entities": []}, {"text": "uses information from dependency trees to make the decoding process keep syntactic cohesion.) present a method that utilizes predicate-argument structures from semantic role labeling results as soft constraints.", "labels": [], "entities": []}, {"text": "In the reranking framework: in principle, all the models in previous category can be used in the reranking framework, because in the reranking we have all the information (source and target words/phrases, alignment) about the translation process.", "labels": [], "entities": []}, {"text": "() describe the use of syntactic features in the rescoring step.", "labels": [], "entities": []}, {"text": "However, they report the syntactic features contribute very small gains.", "labels": [], "entities": []}, {"text": "One disadvantage of carrying out reordering in reranking is the representativeness of the N-best list is often a question mark.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel tagging style reordering model which is under the category \"The reordering is a decoding order problem\".", "labels": [], "entities": []}, {"text": "Our model converts the decoding order problem into a sequence labeling problem, i.e. a tagging task.", "labels": [], "entities": [{"text": "tagging task", "start_pos": 87, "end_pos": 99, "type": "TASK", "confidence": 0.9089741408824921}]}, {"text": "The remainder of this paper is organized as follows: Section 2 introduces the basement of this research: the principle of statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 122, "end_pos": 153, "type": "TASK", "confidence": 0.7149432301521301}]}, {"text": "Section 3 describes the proposed model.", "labels": [], "entities": []}, {"text": "Section 4 briefly describes several reordering models with which we compare our method.", "labels": [], "entities": []}, {"text": "Section 5 provides the experimental configuration and results.", "labels": [], "entities": []}, {"text": "Conclusion will be given in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the baseline setup, the CRFs training results, the RNN training results and translation experimental results.", "labels": [], "entities": []}, {"text": "Our baseline is a phrase-based decoder, which includes the following models: an n-gram targetside language model (LM), a phrase translation model and a word-based lexicon model.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7917583584785461}]}, {"text": "The latter two models are used for both directions: p(f |e) and p(e|f ).", "labels": [], "entities": []}, {"text": "Additionally we use phrase count features, word and phrase penalty.", "labels": [], "entities": []}, {"text": "The reordering model for the baseline system is the distancebased jump model which uses linear distance.", "labels": [], "entities": []}, {"text": "This model does not have hard limit.", "labels": [], "entities": []}, {"text": "We list the important information regarding the experimental setup below.", "labels": [], "entities": []}, {"text": "All those conditions have been kept same in this work.", "labels": [], "entities": []}, {"text": "\u2022 lowercased training data from the GALE task contains the data statistics used for translation model and LM.", "labels": [], "entities": [{"text": "GALE task", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.6072885096073151}, {"text": "translation", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.9780711531639099}]}, {"text": "For the reordering model, we take two further filtering steps.", "labels": [], "entities": []}, {"text": "Firstly, we delete the sentence pairs if the source sentence length is one.", "labels": [], "entities": []}, {"text": "When the source sentence has only one word, the translation will be always monotonic and the reordering model does not need to learn this.", "labels": [], "entities": []}, {"text": "Secondly, we delete the sentence pairs if the source sentence contains more than three contiguous unaligned words.", "labels": [], "entities": []}, {"text": "When this happens, the sentence pair is usually low quality hence not suitable for learning.", "labels": [], "entities": []}, {"text": "The main purpose of the two filtering steps is to further lay down the computational burden.", "labels": [], "entities": []}, {"text": "The label distribution is depicted in.", "labels": [], "entities": []}, {"text": "We can see that most words are monotonic.", "labels": [], "entities": []}, {"text": "We then divide the corpus to three parts: train, validation and test.", "labels": [], "entities": []}, {"text": "The source side data statistics for the reordering model training is given in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: translation model and LM training data statistics", "labels": [], "entities": [{"text": "translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9799239039421082}, {"text": "LM training", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.8647468090057373}]}, {"text": " Table 2: tagging-style model training data statistics", "labels": [], "entities": [{"text": "tagging-style model training", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.910589357217153}]}, {"text": " Table 7: Experimental results. CRFs and RNN mean the tagging-style model trained with CRFs or RNN; LRM for lexicalized  reordering model (Koehn et al., 2007) ; MERO for maximum entropy reordering model (Zens and Ney, 2006) ; BILM for  bilingual language model (Mari\u00f1o et al., 2006) and SRCLM for its simpler version source decoding sequence model (Feng et  al., 2010) ; SC for syntactic cohesion model (Cherry, 2008) ; SRL for semantic cohesion model (Feng et al., 2012); JUMPTREE  for our tree-based jump model based on (Wang et al., 2007).", "labels": [], "entities": [{"text": "LRM", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9197646975517273}, {"text": "MERO", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.98919278383255}, {"text": "BILM", "start_pos": 226, "end_pos": 230, "type": "METRIC", "confidence": 0.984955370426178}]}]}