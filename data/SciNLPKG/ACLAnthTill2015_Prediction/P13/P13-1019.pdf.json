{"title": [{"text": "Improved Bayesian Logistic Supervised Topic Models with Data Augmentation", "labels": [], "entities": [{"text": "Data Augmentation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.740617036819458}]}], "abstractContent": [{"text": "Supervised topic models with a logistic likelihood have two issues that potentially limit their practical use: 1) response variables are usually over-weighted by document word counts; and 2) existing variational inference methods make strict mean-field assumptions.", "labels": [], "entities": []}, {"text": "We address these issues by: 1) introducing a regularization constant to better balance the two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables.", "labels": [], "entities": []}, {"text": "Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily paral-lelized.", "labels": [], "entities": []}, {"text": "Empirical results demonstrate significant improvements on prediction performance and time efficiency.", "labels": [], "entities": []}], "introductionContent": [{"text": "As widely adopted in supervised latent Dirichlet allocation (sLDA) models, one way to improve the predictive power of LDA is to define a likelihood model for the widely available documentlevel response variables, in addition to the likelihood model for document words.", "labels": [], "entities": [{"text": "supervised latent Dirichlet allocation (sLDA)", "start_pos": 21, "end_pos": 66, "type": "TASK", "confidence": 0.7261055367333549}]}, {"text": "For example, the logistic likelihood model is commonly used for binary or multinomial responses.", "labels": [], "entities": []}, {"text": "By imposing some priors, posterior inference is done with the Bayes' rule.", "labels": [], "entities": []}, {"text": "Though powerful, one issue that could limit the use of existing logistic supervised LDA models is that they treat the document-level response variable as one additional word via a normalized likelihood model.", "labels": [], "entities": []}, {"text": "Although some special treatment is carried out on defining the likelihood of the single response variable, it is normally of a much smaller scale than the likelihood of the usually tensor hundreds of words in each document.", "labels": [], "entities": []}, {"text": "As noted by) and observed in our experiments, this model imbalance could result in a weak influence of response variables on the topic representations and thus non-satisfactory prediction performance.", "labels": [], "entities": []}, {"text": "Another difficulty arises when dealing with categorical response variables is that the commonly used normal priors are no longer conjugate to the logistic likelihood and thus lead to hard inference problems.", "labels": [], "entities": []}, {"text": "Existing approaches rely on variational approximation techniques which normally make strict mean-field assumptions.", "labels": [], "entities": [{"text": "variational approximation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.7509129345417023}]}, {"text": "To address the above issues, we present two improvements.", "labels": [], "entities": []}, {"text": "First, we present a general framework of Bayesian logistic supervised topic models with a regularization parameter to better balance response variables and words.", "labels": [], "entities": []}, {"text": "Technically, instead of doing standard Bayesian inference via Bayes' rule, which requires a normalized likelihood model, we propose to do regularized Bayesian inference () via solving an optimization problem, where the posterior regularization is defined as an expectation of a logistic loss, a surrogate loss of the expected misclassification error; and a regularization parameter is introduced to balance the surrogate classification loss (i.e., the response log-likelihood) and the word likelihood.", "labels": [], "entities": []}, {"text": "The general formulation subsumes standard sLDA as a special case.", "labels": [], "entities": []}, {"text": "Second, to solve the intractable posterior inference problem of the generalized Bayesian logistic supervised topic models, we present a simple Gibbs sampling algorithm by exploring the ideas of data augmentation).", "labels": [], "entities": []}, {"text": "More specifically, we extend Polson's method for Bayesian logistic regression () to the generalized logistic supervised topic models, which are much more challeng-ing due to the presence of non-trivial latent variables.", "labels": [], "entities": [{"text": "Bayesian logistic regression", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.5991291205088297}]}, {"text": "Technically, we introduce a set of PolyaGamma variables, one per document, to reformulate the generalized logistic pseudo-likelihood model (with the regularization parameter) as a scale mixture, where the mixture component is conditionally normal for classifier parameters.", "labels": [], "entities": []}, {"text": "Then, we develop a simple and efficient Gibbs sampling algorithms with analytic conditional distributions without Metropolis-Hastings accept/reject steps.", "labels": [], "entities": []}, {"text": "For Bayesian LDA models, we can also explore the conjugacy of the Dirichlet-Multinomial priorlikelihood pairs to collapse out the Dirichlet variables (i.e., topics and mixing proportions) to do collapsed Gibbs sampling, which can have better mixing rates ().", "labels": [], "entities": []}, {"text": "Finally, our empirical results on real data sets demonstrate significant improvements on time efficiency.", "labels": [], "entities": []}, {"text": "The classification performance is also significantly improved by using appropriate regularization parameters.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9545125961303711}]}, {"text": "We also provide a parallel implementation with GraphLab ( , which shows great promise in our preliminary studies.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "2 introduces logistic supervised topic models as a general optimization problem.", "labels": [], "entities": []}, {"text": "3 presents Gibbs sampling algorithms with data augmentation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present empirical results and sensitivity analysis to demonstrate the efficiency and prediction performance 3 of the generalized logistic supervised topic models on the 20Newsgroups (20NG) data set, which contains about 20,000 postings within 20 news groups.", "labels": [], "entities": [{"text": "20Newsgroups (20NG) data set", "start_pos": 172, "end_pos": 200, "type": "DATASET", "confidence": 0.6897063702344894}]}, {"text": "We follow the same setting as in ( ) and remove a standard list of stop words for both binary and multiclass classification.", "labels": [], "entities": []}, {"text": "For all the experiments, we use the standard normal prior p 0 (\u03b7) (i.e., \u03bd 2 = 1) and the symmetric Dirichlet priors \u03b1 = \u03b1 K 1, \u03b2 = 0.01 \u00d7 1, where 1 is a vector with all entries being 1.", "labels": [], "entities": []}, {"text": "For each setting, we report the average performance and the standard deviation with five randomly initialized runs.", "labels": [], "entities": [{"text": "standard", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9843957424163818}]}], "tableCaptions": [{"text": " Table 1: Split of training time over various steps.", "labels": [], "entities": [{"text": "Split", "start_pos": 10, "end_pos": 15, "type": "TASK", "confidence": 0.9528145790100098}]}]}