{"title": [{"text": "Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition", "labels": [], "entities": [{"text": "Implicit Discourse Relation Recognition", "start_pos": 64, "end_pos": 103, "type": "TASK", "confidence": 0.750615581870079}]}], "abstractContent": [{"text": "To overcome the shortage of labeled data for implicit discourse relation recognition , previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 45, "end_pos": 84, "type": "TASK", "confidence": 0.6520127430558205}]}, {"text": "However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data.", "labels": [], "entities": []}, {"text": "In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition.", "labels": [], "entities": [{"text": "discourse relation recognition", "start_pos": 136, "end_pos": 166, "type": "TASK", "confidence": 0.6447946031888326}]}, {"text": "Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F 1 improvement of 5.86% over baseline models.", "labels": [], "entities": [{"text": "PDTB data", "start_pos": 11, "end_pos": 20, "type": "DATASET", "confidence": 0.9383339285850525}, {"text": "F 1", "start_pos": 200, "end_pos": 203, "type": "METRIC", "confidence": 0.9930059611797333}]}], "introductionContent": [{"text": "The task of implicit discourse relation recognition is to identify the type of discourse relation (a.k.a. rhetorical relation) hold between two spans of text, where there is no discourse connective (a.k.a. discourse marker, e.g., but, and) in context to explicitly mark their discourse relation (e.g., Contrast or Explanation).", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 12, "end_pos": 51, "type": "TASK", "confidence": 0.6944762393832207}]}, {"text": "It can be of great benefit to many downstream NLP applications, such as question answering (QA) (, information extraction (IE) (), and machine translation (MT), etc.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.8758681297302247}, {"text": "information extraction (IE)", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.8451247572898865}, {"text": "machine translation (MT)", "start_pos": 135, "end_pos": 159, "type": "TASK", "confidence": 0.8504027605056763}]}, {"text": "This task is quite challenging due to two reasons.", "labels": [], "entities": []}, {"text": "First, without discourse connective in text, the task is quite difficult in itself.", "labels": [], "entities": []}, {"text": "Second, implicit discourse relation is quite frequent in text.", "labels": [], "entities": []}, {"text": "For example, almost half the sentences in the British National Corpus held implicit discourse relations).", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 46, "end_pos": 69, "type": "DATASET", "confidence": 0.9479665756225586}]}, {"text": "Therefore, the task of implicit discourse relation recognition is the key to improving end-to-end discourse parser performance.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 23, "end_pos": 62, "type": "TASK", "confidence": 0.6943255290389061}]}, {"text": "To overcome the shortage of manually annotated training data, () proposed a pattern-based approach to automatically generate training data from raw corpora.", "labels": [], "entities": []}, {"text": "This line of research was followed by and.", "labels": [], "entities": []}, {"text": "In these works, sentences containing certain words or phrases (e.g. but, although) were selected out from raw corpora using a patternbased approach and then these words or phrases were removed from these sentences.", "labels": [], "entities": []}, {"text": "Thus the resulting sentences were used as synthetic training examples for implicit discourse relation recognition.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 74, "end_pos": 113, "type": "TASK", "confidence": 0.7051266580820084}]}, {"text": "Since there is ambiguity of a word or phrase serving for discourse connective (i.e., the ambiguity between discourse and non-discourse usage or the ambiguity between two or more discourse relations if the word or phrase is used as a discourse connective), the synthetic implicit data would contain a lot of noises.", "labels": [], "entities": []}, {"text": "Later, with the release of manually annotated corpus, such as Penn Discourse Treebank 2.0 (PDTB) (, recent studies performed implicit discourse relation recognition on natural (i.e., genuine) implicit discourse data () () () with the use of linguistically informed features and machine learning algorithms.", "labels": [], "entities": [{"text": "Penn Discourse Treebank 2.0 (PDTB)", "start_pos": 62, "end_pos": 96, "type": "DATASET", "confidence": 0.9504408240318298}, {"text": "implicit discourse relation recognition", "start_pos": 125, "end_pos": 164, "type": "TASK", "confidence": 0.5886698663234711}]}, {"text": "() conducted a study of the pattern-based approach presented by and showed that the model built on synthetical implicit data has not generalize well on natural implicit data.", "labels": [], "entities": []}, {"text": "They found some evidence that this behavior is largely independent of the classifiers used and seems to lie in the data itself (e.g., marked and unmarked examples maybe too dissimilar linguistically and removing unambiguous markers in the automatic labelling process may lead to a meaning shift in the examples).", "labels": [], "entities": []}, {"text": "We state that in some cases it is true while in other cases it may not always be so.", "labels": [], "entities": []}, {"text": "A simple example is given here: (E1) a.", "labels": [], "entities": []}, {"text": "We can't win. b. [but] We must keep trying.", "labels": [], "entities": []}, {"text": "We may find that in this example whether the insertion or the removal of connective but would not lead to a redundant or missing information between the above two sentences.", "labels": [], "entities": []}, {"text": "That is, discourse connectives can be inserted between or removed from two sentences without changing the semantic relations between them in some cases.", "labels": [], "entities": []}, {"text": "Another similar observation is in the annotation procedure of PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.9104174375534058}]}, {"text": "To label implicit discourse relation, annotators inserted connective which can best express the relation between sentences without any redundancy . We see that there should be some linguistical similarities between explicit and implicit discourse examples.", "labels": [], "entities": []}, {"text": "Therefore, the first question arises: can we exploit this kind of linguistic similarity between explicit and implicit discourse examples to improve implicit discourse relation recognition?", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 148, "end_pos": 187, "type": "TASK", "confidence": 0.7051387429237366}]}, {"text": "In this paper, we propose a multi-task learning based method to improve the performance of implicit discourse relation recognition (as main task) with the help of relevant auxiliary tasks.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 91, "end_pos": 130, "type": "TASK", "confidence": 0.5842282101511955}]}, {"text": "Specifically, the main task is to recognize the implicit discourse relations based on genuine implicit discourse data and the auxiliary task is to recognize the implicit discourse relations based on synthetic implicit discourse data.", "labels": [], "entities": []}, {"text": "According to the principle of multi-task learning, the learning model can be optimized by the shared part of the main task and the auxiliary tasks without bring unnecessary noise.", "labels": [], "entities": []}, {"text": "That means, the model can learn from synthetic implicit data while it would not bring unnecessary noise from synthetic implicit data.", "labels": [], "entities": []}, {"text": "Although) did not mention, we speculate that another possible reason for the reported worse performance may result from noises in synthetic implicit discourse data.", "labels": [], "entities": []}, {"text": "These synthetic data can be generated from two sources: (1) raw corpora with the use of pattern-based approach in (Marcu and Echihabi, According to the PDTB Annotation Manual (PDTB-, if the insertion of connective leads to \"redundancy\", the relation is annotated as Alternative lexicalizations (AltLex), not implicit. and, and (2) manually annotated explicit data with the removal of explicit discourse connectives.", "labels": [], "entities": [{"text": "PDTB Annotation Manual", "start_pos": 152, "end_pos": 174, "type": "DATASET", "confidence": 0.8342998822530111}]}, {"text": "Obviously, the data generated from the second source is cleaner and more reliable than that from the first source.", "labels": [], "entities": []}, {"text": "Therefore, the second question to address in this work is: whether synthetic implicit discourse data generated from explicit discourse data source (i.e., the second source) can lead to a better performance than that from raw corpora (i.e., the first source)?", "labels": [], "entities": []}, {"text": "To answer this question, we will make a comparison of synthetic discourse data generated from two corpora, i.e., the BILLIP corpus and the explicit discourse data annotated in PDTB.", "labels": [], "entities": [{"text": "BILLIP corpus", "start_pos": 117, "end_pos": 130, "type": "DATASET", "confidence": 0.8326298594474792}, {"text": "PDTB", "start_pos": 176, "end_pos": 180, "type": "DATASET", "confidence": 0.9167526960372925}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews related work on implicit discourse relation classification and multi-task learning.", "labels": [], "entities": [{"text": "implicit discourse relation classification", "start_pos": 34, "end_pos": 76, "type": "TASK", "confidence": 0.7334791272878647}]}, {"text": "Section 3 presents our proposed multi-task learning method for implicit discourse relation classification.", "labels": [], "entities": [{"text": "implicit discourse relation classification", "start_pos": 63, "end_pos": 105, "type": "TASK", "confidence": 0.7513116598129272}]}, {"text": "Section 4 provides the implementation technique details of the proposed multi-task method.", "labels": [], "entities": []}, {"text": "Section 5 presents experiments and discusses results.", "labels": [], "entities": []}, {"text": "Section 6 concludes this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Although previous work has been done on PDTB and (), we cannot make a direct comparison with them because various experimental conditions, such as, different classification strategies (multi-class classification, multiple binary classification), different data preparation (feature extraction and selection), different benchmark data collections (different sections for training and test, different levels of discourse relations), different classifiers with various parameters (MaxEnt, Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, SVM, etc) and even different evaluation methods (F 1 , accuracy) have been adopted by different researchers.", "labels": [], "entities": [{"text": "F 1", "start_pos": 555, "end_pos": 558, "type": "METRIC", "confidence": 0.9913296103477478}, {"text": "accuracy", "start_pos": 561, "end_pos": 569, "type": "METRIC", "confidence": 0.9933278560638428}]}, {"text": "Therefore, to address the two questions raised in Section 1 and to make the comparison reliable and reasonable, we performed experiments on the top and second level of PDTB using single task learning and multi-task learning, respectively.", "labels": [], "entities": []}, {"text": "The systems using single task learning serve as baseline systems.", "labels": [], "entities": []}, {"text": "Under the single task learning, various combinations of expand BLLIP data are incorporated with imp data for the implicit discourse relation classification task.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.8149422407150269}, {"text": "implicit discourse relation classification task", "start_pos": 113, "end_pos": 160, "type": "TASK", "confidence": 0.6634211242198944}]}, {"text": "We hypothesize that synthetical implicit data would contribute to the main task, i.e., the implicit discourse relation classification.", "labels": [], "entities": [{"text": "discourse relation classification", "start_pos": 100, "end_pos": 133, "type": "TASK", "confidence": 0.6167488495508829}]}, {"text": "Specifically, the natural implicit data (i.e., imp) are used to create main task and the synthetical implicit data (exp or BLLIP) are used to create auxiliary tasks for the purpose of optimizing the objective functions of main task.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.9820213317871094}]}, {"text": "If the hypothesis is correct, the performance of main task would be improved by auxiliary tasks created from synthetical implicit data.", "labels": [], "entities": []}, {"text": "Thus in the experiments of multi-task learning, only natural implicit examples (i.e., imp) data are used for main task training while different combinations of synthetical implicit examples (exp and BLLIP) are used for auxiliary task training.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 199, "end_pos": 204, "type": "METRIC", "confidence": 0.9924315214157104}]}, {"text": "We adopt precision, recall and their combination F 1 for performance evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9997547268867493}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9996098875999451}, {"text": "F 1", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9096356928348541}]}, {"text": "We also perform one-tailed t-test to validate if there is significant difference between two methods in terms of F 1 performance analysis.", "labels": [], "entities": []}, {"text": "summarizes the experimental results under single and multi-task learning on the top level of four PDTB relations with respect to different combinations of synthetic implicit data.", "labels": [], "entities": []}, {"text": "For each relation, the first three rows indicate the results of using different single training data under single task learning and the last three rows indicate the results using different combinations of training data under single task and multi-task learning.", "labels": [], "entities": []}, {"text": "The best F 1 for every relation is shown in bold font.", "labels": [], "entities": [{"text": "F 1", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9760594666004181}]}, {"text": "From this table, we can find that on four relations, our multi-task learning systems achieved the best performance using the combination of expand BLLIP synthetic data.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 147, "end_pos": 152, "type": "METRIC", "confidence": 0.8755976557731628}]}, {"text": "summarizes the best single task and the best multi-task learning results on the second level of PDTB.", "labels": [], "entities": []}, {"text": "For four relations, i.e., Synchrony, Con-: Performance of precision, recall and F 1 for 10 Level 2 relation types.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9983603358268738}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9993434548377991}, {"text": "F 1", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.991437554359436}]}], "tableCaptions": [{"text": " Table 1: Distribution of implicit discourse rela- tions in the top and second level of PDTB", "labels": [], "entities": [{"text": "PDTB", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.5858428478240967}]}, {"text": " Table 2: Performance of precision, recall and F 1 for 4 Level 1 relation classes. \"-\" indicates N.A.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9983592629432678}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9995262622833252}, {"text": "F 1", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9920258522033691}]}, {"text": " Table 3: Performance of precision, recall and F 1 for 10 Level 2 relation types. \"-\" indicates 0.00.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.998776376247406}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9995842576026917}, {"text": "F 1", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9926277697086334}]}]}