{"title": [], "abstractContent": [{"text": "We propose a Name-aware Machine Translation (MT) approach which can tightly integrate name processing into MT model, by jointly annotating parallel corpora , extracting name-aware translation grammar and rules, adding name phrase table and name translation driven decoding.", "labels": [], "entities": [{"text": "Name-aware Machine Translation (MT)", "start_pos": 13, "end_pos": 48, "type": "TASK", "confidence": 0.7850453307231268}]}, {"text": "Additionally, we also propose anew MT metric to appropriately evaluate the translation quality of informative words, by assigning different weights to different words according to their importance values in a document.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9379448294639587}]}, {"text": "Experiments on Chinese-English translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation, name translation and word alignment over a high-quality MT baseline 1 .", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.7015327513217926}, {"text": "name translation", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.8246934413909912}, {"text": "word alignment", "start_pos": 160, "end_pos": 174, "type": "TASK", "confidence": 0.7586711347103119}]}], "introductionContent": [{"text": "A shrinking fraction of the world's Web pages are written in English, therefore the ability to access pages across a range of languages is becoming increasingly important.", "labels": [], "entities": []}, {"text": "This need can be addressed in part by cross-lingual information access tasks such as entity linking), event extraction (, slot filling) and question answering (.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.7388169765472412}, {"text": "event extraction", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.7931993901729584}, {"text": "slot filling", "start_pos": 122, "end_pos": 134, "type": "TASK", "confidence": 0.72465880215168}, {"text": "question answering", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.892815500497818}]}, {"text": "A key bottleneck of highquality cross-lingual information access lies in the performance of Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.8548685789108277}]}, {"text": "Traditional MT approaches focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content words including critical information, especially names.", "labels": [], "entities": [{"text": "MT", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9866929650306702}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9975283741950989}]}, {"text": "A typical statistical MT system can only translate 60% person names correctly ( . Incorrect segmentation and translation of names which often carry central meanings of a sentence can also yield incorrect translation of long contexts.", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9633204340934753}, {"text": "Incorrect segmentation", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.649500235915184}]}, {"text": "Names have been largely neglected in the prior MT research due to the following reasons: \u2022 The current dominant automatic MT scoring metrics (such as Bilingual Evaluation Understudy (BLEU) ()) treat all words equally, but names have relative low frequency in text (about 6% in newswire and only 3% in web documents) and thus are vastly outnumbered by function words and common nouns, etc..", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9892174005508423}, {"text": "MT", "start_pos": 122, "end_pos": 124, "type": "TASK", "confidence": 0.9760370254516602}, {"text": "Bilingual Evaluation Understudy (BLEU)", "start_pos": 150, "end_pos": 188, "type": "METRIC", "confidence": 0.7716190417607626}]}, {"text": "\u2022 Name translations pose a greater complexity because the set of names is open and highly dynamic.", "labels": [], "entities": [{"text": "Name translations", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.8221249282360077}]}, {"text": "It is also important to acknowledge that there are many fundamental differences between the translation of names and other tokens, depending on whether a name is rendered phonetically, semantically, or a mixture of both ( ).", "labels": [], "entities": []}, {"text": "\u2022 The artificial settings of assigning low weights to information translation (compared to overall word translation) in some largescale government evaluations have discouraged MT developers to spend time and explore resources to tackle this problem.", "labels": [], "entities": [{"text": "information translation", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.7745981514453888}, {"text": "word translation", "start_pos": 99, "end_pos": 115, "type": "TASK", "confidence": 0.7236423194408417}, {"text": "MT", "start_pos": 176, "end_pos": 178, "type": "TASK", "confidence": 0.9787230491638184}]}, {"text": "We propose a novel Name-aware MT (NAMT) approach which can tightly integrate name processing into the training and decoding processes of an end-to-end MT pipeline, and anew name-aware metric to evaluate MT which can assign different weights to different tokens according to their importance values in a document.", "labels": [], "entities": [{"text": "Name-aware MT (NAMT)", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.79159494638443}, {"text": "MT", "start_pos": 203, "end_pos": 205, "type": "TASK", "confidence": 0.9633932709693909}]}, {"text": "Compared to previous methods, the novel contributions of our approach are: 1.", "labels": [], "entities": []}, {"text": "Tightly integrate joint bilingual name tagging into MT training by coordinating tagged names in parallel corpora, updating word segmentation, word alignment and grammar extraction (Section 3.1).", "labels": [], "entities": [{"text": "bilingual name tagging", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.6258071462313334}, {"text": "MT training", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9280692338943481}, {"text": "word segmentation", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.6948585957288742}, {"text": "word alignment", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.7841885387897491}, {"text": "grammar extraction", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.7117487341165543}]}, {"text": "2. Tightly integrate name tagging and translation into MT decoding via name-aware grammar (Section 3.2).", "labels": [], "entities": [{"text": "name tagging", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.8263001441955566}, {"text": "MT decoding", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.8889026939868927}]}, {"text": "3. Optimize name translation and context translation simultaneously and conduct name translation driven decoding with language model (LM) based selection (Section 3.2).", "labels": [], "entities": [{"text": "name translation", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.8289737701416016}, {"text": "context translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.6903514266014099}, {"text": "name translation", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.7186850011348724}]}], "datasetContent": [{"text": "can discriminate names and non-informative words (Section 4).", "labels": [], "entities": []}, {"text": "Traditional MT evaluation metrics such as BLEU () and Translation Edit Rate (TER)) assign the same weights to all tokens equally.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.9093065857887268}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.998606264591217}, {"text": "Translation Edit Rate (TER))", "start_pos": 54, "end_pos": 82, "type": "METRIC", "confidence": 0.8742514451344808}]}, {"text": "For example, incorrect translations of \"the\" and \"Bush\" will receive the same penalty.", "labels": [], "entities": []}, {"text": "However, for crosslingual information processing applications, we should acknowledge that certain informationally critical words are more important than other common words.", "labels": [], "entities": []}, {"text": "In order to properly evaluate the translation quality of NAMT methods, we propose to modify the BLEU metric so that they can dynamically assign more weights to names during evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9980652928352356}]}, {"text": "BLEU considers the correspondence between a system translation and a human translation: where BP is brevity penalty defined as follows: where w n is a set of positive weights summing to one and usually uniformly set as w n = 1/N , c is the length of the system translation and r is the length of reference translation, and p n is modified n-gram precision defined as: where C and C are translation candidates in the candidate sentence set, if a source sentence is translated to many candidate sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7337969541549683}, {"text": "system translation", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7183410972356796}, {"text": "BP", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9904413223266602}, {"text": "precision", "start_pos": 346, "end_pos": 355, "type": "METRIC", "confidence": 0.8465858101844788}]}, {"text": "As in BLEU metric, we first count the maximum number of times an n-gram occurs in any single reference translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.8587781190872192}]}, {"text": "The total count of each candidate n-gram is clipped at sentence level by its maximum reference count.", "labels": [], "entities": []}, {"text": "Then we add up the weights of clipped n-grams and divide them by the total weight of all n-grams.", "labels": [], "entities": []}, {"text": "Based on BLEU score, we design a name-aware BLEU metric as follows.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9709409773349762}, {"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9642192125320435}]}, {"text": "Depending on whether a token t is contained in a name in reference translation, we assign a weight weight t tot as follows: , if t never appears in names where PE is the sum of penalties of non-name tokens and Z is the number of tokens within all names: In this paper, the tf \u00b7 idf score is computed at sentence level, therefore, Dis the sentence set and each d \u2208 Dis a sentence.", "labels": [], "entities": []}, {"text": "The weight of an n-gram in reference translation is the sum of weights of all tokens it contains.", "labels": [], "entities": []}, {"text": "Next, we compute the weighted modified ngram precision Count weight\u2212clip (n-gram) as follows: Count weight\u2212clip (n-gram) = if the ngram i is correctly translated weight ngram i The Count clip (n-gram) in the equation 3 is substituted with above Count weight\u2212clip (n-gram).", "labels": [], "entities": [{"text": "Count", "start_pos": 94, "end_pos": 99, "type": "METRIC", "confidence": 0.9827086329460144}]}, {"text": "When we sum up the total weight of all n-grams of a candidate translation, some n-grams may contain tokens which do not exist in reference translation.", "labels": [], "entities": []}, {"text": "We assign the lowest weight of tokens in reference translation to these rare tokens.", "labels": [], "entities": []}, {"text": "We also add an item, name penalty NP , to penalize the output sentences which contain too many or too few names: where u is the number of name tokens in system translation and v is the number of name tokens in reference translation.", "labels": [], "entities": []}, {"text": "Finally the name-aware BLEU score is defined as: wn log wpn This new metric can also be applied to evaluate MT approaches which emphasize other types of facts such as events, by simply replacing name tokens by other fact tokens.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9701785147190094}, {"text": "MT", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.9959716200828552}]}, {"text": "In this section we present the experimental results of NAMT compared to the baseline MT.", "labels": [], "entities": [{"text": "NAMT", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.5956454873085022}, {"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.8222375512123108}]}, {"text": "In order to investigate the correlation between name-aware BLEU scores and human judgment results, we asked three bi-lingual speakers to judge our translation output from the baseline system and the NAMT system, on a Chinese subset of 250 sentences (each sentence has two corresponding translations from baseline and NAMT) extracted randomly from 7 test corpora.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9713191986083984}]}, {"text": "The annotators rated each translation from 1 (very bad) to 5 (very good) and made their judgments based on whether the translation is understandable and conveys the same meaning.", "labels": [], "entities": []}, {"text": "We computed the name-aware BLEU scores on the subset and also the aggregated average scores from human judgments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9847710728645325}]}, {"text": "shows that NAMT consistently achieved higher scores with both name-aware BLEU metric and human judgement.", "labels": [], "entities": [{"text": "NAMT", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.6363092660903931}, {"text": "BLEU metric", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.9613323509693146}]}, {"text": "Furthermore, we calculated three Pearson product-moment correlation coefficients between human judgment scores and name-aware BLEU scores of these two MT systems.", "labels": [], "entities": [{"text": "Pearson product-moment correlation", "start_pos": 33, "end_pos": 67, "type": "METRIC", "confidence": 0.9057596127192179}, {"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9784886837005615}, {"text": "MT", "start_pos": 151, "end_pos": 153, "type": "TASK", "confidence": 0.9631426930427551}]}, {"text": "Give the sample size and the correlation coefficient value, the high significance value of 0.99 indicates that nameaware BLEU tracks human judgment well.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 29, "end_pos": 52, "type": "METRIC", "confidence": 0.9793186187744141}, {"text": "nameaware", "start_pos": 111, "end_pos": 120, "type": "DATASET", "confidence": 0.40044912695884705}, {"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9004355669021606}]}], "tableCaptions": [{"text": " Table 1: Statistics and Name Distribution of Test Data Sets.", "labels": [], "entities": []}, {"text": " Table 2: Translation Performance (%).", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9175481796264648}]}, {"text": " Table 3: Impact of Joint Bilingual Name Tagging on Word  Alignment (%).", "labels": [], "entities": [{"text": "Joint Bilingual Name Tagging", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.596955344080925}, {"text": "Word  Alignment", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.7174464464187622}]}]}