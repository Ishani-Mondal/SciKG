{"title": [{"text": "Cross-lingual Transfer of Semantic Role Labeling Models", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.5943884452184042}]}], "abstractContent": [{"text": "Semantic Role Labeling (SRL) has become one of the standard tasks of natural language processing and proven useful as a source of information fora number of other applications.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8296584486961365}, {"text": "natural language processing", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.6348826388518015}]}, {"text": "We address the problem of transferring an SRL model from one language to another using a shared feature representation.", "labels": [], "entities": [{"text": "SRL model", "start_pos": 42, "end_pos": 51, "type": "TASK", "confidence": 0.8660500645637512}]}, {"text": "This approach is then evaluated on three language pairs, demonstrating competitive performance as compared to a state-of-the-art unsuper-vised SRL system and a cross-lingual annotation projection baseline.", "labels": [], "entities": []}, {"text": "We also consider the contribution of different aspects of the feature representation to the performance of the model and discuss practical applicability of this method.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We use the F 1 measure as a metric for the argument identification stage and accuracy as an aggregate measure of argument classification performance.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.9729458888371786}, {"text": "argument identification", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.8085078001022339}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9992450475692749}, {"text": "argument classification", "start_pos": 113, "end_pos": 136, "type": "TASK", "confidence": 0.7062042653560638}]}, {"text": "When comparing to the unsupervised SRL system the clustering evaluation measures are used instead.", "labels": [], "entities": []}, {"text": "These are purity and collocation where Ci is the set of arguments in the i-th induced cluster, G j is the set of arguments in the jth gold cluster and N is the total number of arguments.", "labels": [], "entities": [{"text": "purity", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9967856407165527}]}, {"text": "We report the harmonic mean of the two (Lang and Lapata, 2011) and denote it F c 1 to avoid confusing it with the supervised metric.", "labels": [], "entities": [{"text": "F c 1", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9429571628570557}]}, {"text": "Evaluation of the cross-lingual model transfer requires a rather specific kind of dataset.", "labels": [], "entities": [{"text": "cross-lingual model transfer", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.6720568339029948}]}, {"text": "Namely, the data in both languages has to be annotated with the same set of semantic roles following the same (or compatible) guidelines, which is seldom the case.", "labels": [], "entities": []}, {"text": "We have identified three language pairs for which such resources are available: EnglishChinese, English-Czech and English-French.", "labels": [], "entities": [{"text": "EnglishChinese", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.9273937940597534}]}, {"text": "The evaluation datasets for English and Chinese are those from the CoNLL Shared Task) (henceforth CoNLL-ST).", "labels": [], "entities": [{"text": "CoNLL Shared Task)", "start_pos": 67, "end_pos": 85, "type": "DATASET", "confidence": 0.8310860991477966}]}, {"text": "Their annotation in the CoNLL-ST is not identical, but the guidelines for \"core\" semantic roles are similar), so we evaluate only on core roles here.", "labels": [], "entities": [{"text": "CoNLL-ST", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.8935661911964417}]}, {"text": "The data for the second language pair is drawn from the Prague Czech-English Dependency Treebank 2.0 (Haji\u010d et al., 2012), which we converted to a format similar to that of CoNLL-ST 1 . The original annotation uses the tectogrammatical representation) and an inventory of semantic roles (or functors), most of which are interpretable across various predicates.", "labels": [], "entities": [{"text": "Prague Czech-English Dependency Treebank 2.0", "start_pos": 56, "end_pos": 100, "type": "DATASET", "confidence": 0.8722773671150208}]}, {"text": "Also note that the syntactic annotation of English and Czech in PCEDT 2.0 is quite similar (to the extent permitted by the difference in the structure of the two languages) and we can use the dependency relations in our experiments.", "labels": [], "entities": []}, {"text": "For English-French, the English CoNLL-ST dataset was used as a source and the model was evaluated on the manually annotated dataset from van der.", "labels": [], "entities": [{"text": "CoNLL-ST dataset", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.8048252165317535}]}, {"text": "The latter contains one thousand sentences from the French part of the Europarl () corpus, annotated with semantic roles following an adapted version of PropBank () guidelines.", "labels": [], "entities": [{"text": "Europarl () corpus", "start_pos": 71, "end_pos": 89, "type": "DATASET", "confidence": 0.8596878250439962}]}, {"text": "The authors perform annotation projection from English to French, using a joint model of syntax and semantics and employing heuristics for filtering.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7087730914354324}]}, {"text": "We use a model trained on the output of this projection system as one of the baselines.", "labels": [], "entities": []}, {"text": "The evaluation dataset is relatively small in this case, so we perform the transfer only one-way, from English to French.", "labels": [], "entities": []}, {"text": "The part-of-speech tags in all datasets were replaced with the universal POS tags of.", "labels": [], "entities": []}, {"text": "For Czech, we have augmented the mappings to account for the tags that were not present in the datasets from which the original mappings were derived.", "labels": [], "entities": []}, {"text": "Namely, tag \"t\" is mapped to \"VERB\" and \"Y\" -to \"PRON\".", "labels": [], "entities": [{"text": "VERB", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9263626933097839}]}, {"text": "We use parallel data to construct a bilingual dictionary used in word mapping, as well as in the projection baseline.", "labels": [], "entities": [{"text": "word mapping", "start_pos": 65, "end_pos": 77, "type": "TASK", "confidence": 0.792876660823822}]}, {"text": "For English-Czech 1 see http://www.ml4nlp.de/code-and-data/treex2conll and English-French, the data is drawn from Europarl (, for English-Chinese -from.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 114, "end_pos": 122, "type": "DATASET", "confidence": 0.9893993139266968}]}, {"text": "The word alignments were obtained using GIZA++) and the intersection heuristic.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.6726629883050919}]}, {"text": "We now evaluate the contribution of different aspects of the feature representation to the performance of the model.: EN-FR model transfer accuracy with different feature subsets, using original and transferred syntactic information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9787569642066956}]}, {"text": "The fact that the model performs slightly better with transferred syntax maybe explained by two factors.", "labels": [], "entities": []}, {"text": "Firstly, as we already mentioned, the original syntactic annotation is also produced automatically.", "labels": [], "entities": []}, {"text": "Secondly, in the model transfer setup it is more important how closely the syntacticsemantic interface on the target side resembles that on the source side than how well it matches the \"true\" structure of the target language, and in this respect a transferred dependency parser may have an advantage over one trained on target-language data.", "labels": [], "entities": []}, {"text": "The high impact of the Gloss features here maybe partly attributed to the fact that the mapping is derived from the same corpus as the evaluation data -Europarl ( -and partly by the similarity between English and French in terms of word order, usage of articles and prepositions.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 152, "end_pos": 160, "type": "DATASET", "confidence": 0.79297935962677}]}, {"text": "The moderate contribution of the crosslingual cluster features are likely due to the insufficient granularity of the clustering for this task.", "labels": [], "entities": []}, {"text": "For more distant language pairs, the contributions of individual feature groups are less interpretable, so we only highlight a few observations.", "labels": [], "entities": []}, {"text": "First of all, both EN-CZ and CZ-EN benefit noticeably from the use of the original syntactic annotation, including dependency relations, but not from the transferred syntax, most likely due to the low syntactic transfer performance.", "labels": [], "entities": []}, {"text": "Both perform better when lexical information is available, although the improvement is not as significant as in the case of French -only up to 5%.", "labels": [], "entities": []}, {"text": "The situation with Chinese is somewhat complicated in that adding lexical information here fails to yield an improvement in terms of the metric considered.", "labels": [], "entities": []}, {"text": "This is likely due to the fact that we consider only the core roles, which can usually be predicted with high accuracy based on syntactic information alone.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9927147030830383}]}], "tableCaptions": [{"text": " Table 3: Argument identification, transferred  model vs. projection baseline, F 1 .", "labels": [], "entities": [{"text": "Argument identification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.9366291463375092}, {"text": "F", "start_pos": 79, "end_pos": 80, "type": "METRIC", "confidence": 0.9846665859222412}]}, {"text": " Table 4: Argument classification, transferred  model vs. unsupervised baseline in terms of the  clustering metric F c  1 (see section 2.3).", "labels": [], "entities": [{"text": "Argument classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.9044086933135986}]}, {"text": " Table 5: Argument classification, transferred  model vs. projection baseline, accuracy.", "labels": [], "entities": [{"text": "Argument classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.9101170599460602}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9995793700218201}]}, {"text": " Table 6: Model accuracy on the source and target  language using original syntax. The source lan- guage scores for English vary between language  pairs because of the difference in syntactic anno- tation and role subset used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9986433386802673}]}, {"text": " Table 7: EN-CZ transfer (with original syntax), F 1 ,  recall and precision for the top-10 most frequent  roles.", "labels": [], "entities": [{"text": "EN-CZ transfer", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6986509412527084}, {"text": "F 1", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9952369928359985}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9992722868919373}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9994294047355652}]}, {"text": " Table 8: EN-FR model transfer accuracy with dif- ferent feature subsets, using original and trans- ferred syntactic information.", "labels": [], "entities": [{"text": "EN-FR model transfer", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.5292317569255829}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9477554559707642}]}]}