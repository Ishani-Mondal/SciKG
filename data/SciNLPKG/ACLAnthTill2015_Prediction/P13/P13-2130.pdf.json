{"title": [], "abstractContent": [{"text": "We present the first attempt to perform full FrameNet annotation with crowdsourcing techniques.", "labels": [], "entities": []}, {"text": "We compare two approaches: the first one is the standard annotation methodology of lexical units and frame elements in two steps, while the second is a novel approach aimed at acquiring frames in a bottom-up fashion, starting from frame element annotation.", "labels": [], "entities": []}, {"text": "We show that our methodology, relying on a single annotation step and on simplified role definitions , outperforms the standard one both in terms of accuracy and time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9991992115974426}]}], "introductionContent": [{"text": "Annotating frame information is a complex task, usually modeled in two steps: first annotators are asked to choose the situation (or frame) evoked by a given predicate (the lexical unit, LU) in a sentence, and then they assign the semantic roles (or frame elements, FEs) that describe the participants typically involved in the chosen frame.", "labels": [], "entities": [{"text": "Annotating frame information", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8225880265235901}, {"text": "FEs", "start_pos": 266, "end_pos": 269, "type": "METRIC", "confidence": 0.9710619449615479}]}, {"text": "Existing frame annotation tools, such as Salto () and the Berkeley system () foresee this two-step approach, in which annotators first select a frame from a large repository of possible frames (1,162 frames are currently listed in the online version of the resource), and then assign the FE labels constrained by the chosen frame to LU dependents.", "labels": [], "entities": [{"text": "FE", "start_pos": 288, "end_pos": 290, "type": "METRIC", "confidence": 0.9938468337059021}]}, {"text": "In this paper, we argue that such workflow shows some redundancy which can be addressed by radically changing the annotation methodology and performing it in one single step.", "labels": [], "entities": []}, {"text": "Our novel annotation approach is also more compliant with the definition of frames proposed in: in his seminal work, Fillmore postulated that the meanings of words can be understood on the basis of a semantic frame, i.e. a description of a type of event or entity and the participants in it.", "labels": [], "entities": []}, {"text": "This implies that frames can be distinguished one from another on the basis of the participants involved, thus it seems more cognitively plausible to start from the FE annotation to identify the frame expressed in a sentence, and not the contrary.", "labels": [], "entities": [{"text": "FE", "start_pos": 165, "end_pos": 167, "type": "METRIC", "confidence": 0.9456945061683655}]}, {"text": "The goal of our methodology is to provide full frame annotation in a single step and in a bottomup fashion.", "labels": [], "entities": []}, {"text": "Instead of choosing the frame first, we focus on FEs and let the frame emerge based on the chosen FEs.", "labels": [], "entities": [{"text": "FEs", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9637740254402161}, {"text": "FEs", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9213851094245911}]}, {"text": "We believe this approach complies better with the cognitive activity performed by annotators, while the 2-step methodology is more artificial and introduces some redundancy because part of the annotators' choices are replicated in the two steps (i.e. in order to assign a frame, annotators implicitly identify the participants also in the first step, even if they are annotated later).", "labels": [], "entities": []}, {"text": "Another issue we investigate in this work is how semantic roles should be annotated in a crowdsourcing framework.", "labels": [], "entities": []}, {"text": "This task is particularly complex, therefore it is usually performed by expert annotators under the supervision of linguistic experts and lexicographers, as in the case of FrameNet.", "labels": [], "entities": []}, {"text": "In NLP, different annotation efforts for encoding semantic roles have been carried out, each applying its own methodology and annotation guidelines (see for instance for for PropBank).", "labels": [], "entities": []}, {"text": "In this work, we present a pilot study in which we assess to what extent role descriptions meant for 'linguistics experts' are also suitable for annotators from the crowd.", "labels": [], "entities": []}, {"text": "Moreover, we show how a simplified version of these descriptions, less bounded to a specific linguistic theory, improve the annotation quality.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the anatomy and discuss the results of the tasks we outsourced to the crowd via the CrowdFlower 1 platform.", "labels": [], "entities": [{"text": "CrowdFlower 1 platform", "start_pos": 113, "end_pos": 135, "type": "DATASET", "confidence": 0.8296122948328654}]}, {"text": "Golden data Quality control of the collected judgements is a key factor for the success of the experiments.", "labels": [], "entities": []}, {"text": "Cheating risk is minimized by adding gold units, namely data for which the requester already knows the answer.", "labels": [], "entities": []}, {"text": "If a worker misses too many gold answers within a given threshold, he or she will be flagged as untrusted and his or her judgments will be automatically discarded.", "labels": [], "entities": []}, {"text": "Worker switching effect Depending on their accuracy in providing answers to gold units, workers may switch from a trusted to an untrusted status and vice versa.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9978607296943665}]}, {"text": "In practice, a worker submits his or her responses via a web page.", "labels": [], "entities": []}, {"text": "Each page contains one gold unit and a variable number of regular units that can beset by the requester during the calibration phase.", "labels": [], "entities": []}, {"text": "If a worker becomes un-1 https://crowdflower.com trusted, the platform collects another judgment to fill the gap.", "labels": [], "entities": []}, {"text": "If a worker moves back to the trusted status, his or her previous contribution is added to the results as free extra judgments.", "labels": [], "entities": []}, {"text": "Such phenomenon typically occurs when the complexity of gold units is high enough to induce low agreement in workers' answers.", "labels": [], "entities": []}, {"text": "Thus, the requester is constrained to review gold units and to eventually forgive workers who missed them.", "labels": [], "entities": []}, {"text": "This has massively happened in our experiments and is one of the main causes of the overall cost decrease and time increase.", "labels": [], "entities": []}, {"text": "Cost calibration The total cost of a generic crowdsourcing task is naturally bound to a data unit.", "labels": [], "entities": []}, {"text": "This represents an issue inmost of our experiments, as the number of questions per unit (i.e. a sentence) varies according to the number of frames and FEs evoked by the LU contained in a sentence.", "labels": [], "entities": [{"text": "FEs", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9884024262428284}]}, {"text": "In order to enable cost comparison, for each experiment we need to use the average number of questions per sentence as a multiplier to a constant cost per sentence.", "labels": [], "entities": []}, {"text": "We set the payment per working page to 5 $ cents and the number of sentences per page to 3, resulting in 1.83 $ cent per sentence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of the reproduced frame dis- crimination task as per (Hong and Baker, 2011)", "labels": [], "entities": [{"text": "frame dis- crimination", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.7550659775733948}]}, {"text": " Table 2: Overview of the experimental results.  FD stands for Frame Discrimination, FER for FEs  Recognition", "labels": [], "entities": [{"text": "FD", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9877534508705139}, {"text": "FER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9965687990188599}, {"text": "FEs  Recognition", "start_pos": 93, "end_pos": 109, "type": "METRIC", "confidence": 0.6734922677278519}]}]}