{"title": [{"text": "Learning Semantic Textual Similarity with Structural Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "Measuring semantic textual similarity (STS) is at the cornerstone of many NLP applications.", "labels": [], "entities": [{"text": "Measuring semantic textual similarity (STS)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7252121014254433}]}, {"text": "Different from the majority of approaches, where a large number of pairwise similarity features are used to represent a text pair, our model features the following: (i) it directly encodes input texts into relational syntactic structures; (ii) relies on tree kernels to handle feature engineering automatically; (iii) combines both structural and feature vector representations in a single scoring model, i.e., in Support Vector Regression (SVR); and (iv) delivers significant improvement over the best STS systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "In STS the goal is to learn a scoring model that given a pair of two short texts returns a similarity score that correlates with human judgement.", "labels": [], "entities": []}, {"text": "Hence, the key aspect of having an accurate STS framework is the design of features that can adequately represent various aspects of the similarity between texts, e.g., using lexical, syntactic and semantic similarity metrics.", "labels": [], "entities": []}, {"text": "The majority of approaches treat input text pairs as feature vectors where each feature is a score corresponding to a certain type of similarity.", "labels": [], "entities": []}, {"text": "This approach is conceptually easy to implement and the STS shared task at SemEval 2012 () (STS-2012) has shown that the best systems were built following this idea, i.e., a number of features encoding similarity of an input text pair were combined in a single scoring model, e.g., SVR.", "labels": [], "entities": [{"text": "STS shared task at SemEval 2012", "start_pos": 56, "end_pos": 87, "type": "DATASET", "confidence": 0.57838603357474}]}, {"text": "Nevertheless, one limitation of using only similarity features to represent a text pair is that of low representation power.", "labels": [], "entities": []}, {"text": "The novelty of our approach is that we treat the input text pairs as structural objects and rely on the power of kernel learning to extract relevant structures.", "labels": [], "entities": []}, {"text": "To link the documents in a pair we mark the nodes in the related structures with a special relational tag.", "labels": [], "entities": []}, {"text": "This way effective structural relational patterns are implicitly encoded in the trees and can be automatically learned by the kernel-based machines.", "labels": [], "entities": []}, {"text": "We combine our relational structural model with the features from two best systems of STS-2012.", "labels": [], "entities": []}, {"text": "Finally, we use the approach of classifier stacking to combine several structural models into the feature vector representation.", "labels": [], "entities": [{"text": "classifier stacking", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7993499040603638}]}, {"text": "The contribution of this paper is as follows: (i) it provides a convincing evidence that adding structural features automatically extracted by structural kernels yields a significant improvement inaccuracy; (ii) we define a combination kernel that integrates both structural and feature vector representations within a single scoring model, e.g., Support Vector Regression; (iii) we provide a simple way to construct relational structural models that can be built using off-the-shelf NLP tools; (iv) we experiment with four structural representations and show that constituency and dependency trees represent the best source for learning structural relationships; and (v) using a classifier stacking approach, structural models can be easily combined and integrated into existing feature-based STS models.", "labels": [], "entities": [{"text": "classifier stacking", "start_pos": 680, "end_pos": 699, "type": "TASK", "confidence": 0.695081040263176}]}], "datasetContent": [{"text": "We present the results of our model tested on the data from the Core STS task at SemEval 2012.", "labels": [], "entities": [{"text": "Core STS task at SemEval 2012", "start_pos": 64, "end_pos": 93, "type": "DATASET", "confidence": 0.7906131048997244}]}], "tableCaptions": [{"text": " Table 1: Results on STS-2012. First set of experiments studies the combination of fvec models from  UKP (U), Takelab (T) and (A). Next we show results for four structural representations: shallow (S),  constituency (C), dependency (D) and phrase-dependency (P) trees with STK and PTK; next row set  demonstrates the necessity of relational linking for two best structures, i.e. C and D (empty circle denotes  a structures with no relational linking.); finally, domain adaptation via bags of features (B) of the entire  pair and (M) manually encoded dataset type show the state of the art results.", "labels": [], "entities": [{"text": "UKP", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.9751167893409729}, {"text": "PTK", "start_pos": 281, "end_pos": 284, "type": "METRIC", "confidence": 0.8445490002632141}]}]}