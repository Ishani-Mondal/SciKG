{"title": [{"text": "Latent Semantic Tensor Indexing for Community-based Question Answering", "labels": [], "entities": [{"text": "Latent Semantic Tensor Indexing", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7108288928866386}, {"text": "Question Answering", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.6591558307409286}]}], "abstractContent": [{"text": "Retrieving similar questions is very important in community-based question answering(CQA).", "labels": [], "entities": [{"text": "Retrieving similar questions", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8870996832847595}, {"text": "community-based question answering(CQA)", "start_pos": 50, "end_pos": 89, "type": "TASK", "confidence": 0.7690972288449606}]}, {"text": "In this paper, we propose a unified question retrieval model based on latent semantic indexing with tensor analysis, which can capture word associations among different parts of CQA triples simultaneously.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7891668081283569}]}, {"text": "Thus, our method can reduce lexical chasm of question retrieval with the help of the information of question content and answer parts.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7815164625644684}]}, {"text": "The experimental result shows that our method out-performs the traditional methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Community-based (or collaborative) question answering(CQA) such as Yahoo!", "labels": [], "entities": [{"text": "question answering(CQA)", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7975027024745941}]}, {"text": "Answers 1 and Baidu Zhidao 2 has become a popular online service in recent years.", "labels": [], "entities": []}, {"text": "Unlike traditional question answering (QA), information seekers can post their questions on a CQA website which are later answered by other users.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.8731756508350372}]}, {"text": "However, with the increase of the CQA archive, there accumulate massive duplicate questions on CQA websites.", "labels": [], "entities": [{"text": "CQA archive", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.9816924631595612}]}, {"text": "One of the primary reasons is that information seekers cannot retrieve answers they need and thus post another new question consequently.", "labels": [], "entities": []}, {"text": "Therefore, it becomes more and more important to find semantically similar questions.", "labels": [], "entities": []}, {"text": "The major challenge for CQA retrieval is the lexical gap (or lexical chasm) among the questions (: An example on question retrieval as shown in.", "labels": [], "entities": [{"text": "CQA retrieval", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.7823669016361237}, {"text": "question retrieval", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.6841964870691299}]}, {"text": "Since question-answer pairs are usually short, the word mismatching problem is especially important.", "labels": [], "entities": []}, {"text": "However, due to the lexical gap between questions and answers as well as spam typically existing in user-generated content, filtering and ranking answers is very challenging.", "labels": [], "entities": []}, {"text": "The earlier studies mainly focus on generating redundant features, or finding textual clues using machine learning techniques; none of them ever consider questions and their answers as relational data but instead model them as independent information.", "labels": [], "entities": []}, {"text": "Moreover, they only consider the answers of the current question, and ignore any previous knowledge that would be helpful to bridge the lexical and semantic gap.", "labels": [], "entities": []}, {"text": "In recent years, many methods have been proposed to solve the word mismatching problem between user questions and the questions in a QA archive(), among which the translation-based ( or syntactic-based approaches () methods have been proven to improve the performance of CQA retrieval.", "labels": [], "entities": [{"text": "word mismatching problem between user questions", "start_pos": 62, "end_pos": 109, "type": "TASK", "confidence": 0.7920593768358231}, {"text": "CQA retrieval", "start_pos": 271, "end_pos": 284, "type": "TASK", "confidence": 0.7447434961795807}]}, {"text": "However, most of these approaches used pipeline methods: (1) modeling word association; (2) question retrieval combined with other models, such as vector space model (VSM), Okapi model ( or language model (LM).", "labels": [], "entities": [{"text": "word association", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.6687600910663605}, {"text": "question retrieval", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.8030471503734589}]}, {"text": "The pipeline methods often have many non-trivial experimental setting and result to be very hard to reproduce.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel unified retrieval model for CQA, latent semantic tensor indexing (LSTI), which is an extension of the conventional latent semantic indexing (LSI)).", "labels": [], "entities": [{"text": "latent semantic tensor indexing (LSTI)", "start_pos": 67, "end_pos": 105, "type": "TASK", "confidence": 0.6626450674874442}]}, {"text": "Similar to LSI, LSTI can integrate the two detached parts (modeling word association and question retrieval) into a single model.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7431113719940186}]}, {"text": "In traditional document retrieval, LSI is an effective method to overcome two of the most severe constraints on Boolean keyword queries: synonymy, that is, multiple words with similar meanings, and polysemy, or words with more than one meanings.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7766657173633575}]}, {"text": "Usually in a CQA archive, each entry (or question) is in the following triple form:\u27e8question title, question content, answer\u27e9.", "labels": [], "entities": [{"text": "CQA archive", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.9442177712917328}]}, {"text": "Because the performance based solely on the content or the answer part is less than satisfactory, many works proposed that additional relevant information should be provided to help question retrieval(.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.861539751291275}]}, {"text": "For example, if a question title contains the keyword \"why\", the CQA triple, which contains \"because\" or \"reason\" in its answer part, is more likely to be what the user looks for.", "labels": [], "entities": []}, {"text": "Since each triple in CQA has three parts, the natural representation of the CQA collection is a three-dimensional array, or 3rd-order tensor, rather than a matrix.", "labels": [], "entities": []}, {"text": "Based on the tensor decomposition, we can model the word association simultaneously in the pairs: questionquestion, question-body and question-answer.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 3 introduces the concept of LSI.", "labels": [], "entities": []}, {"text": "Section 4 presents our method.", "labels": [], "entities": []}, {"text": "Section 5 describes the experimental analysis.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collected the resolved CQA triples from the \"computer\" category of Yahoo!", "labels": [], "entities": []}, {"text": "Answers and Baidu Zhidao websites.", "labels": [], "entities": [{"text": "Answers", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9306005239486694}]}, {"text": "We just selected the resolved questions that already have been given their best answers.", "labels": [], "entities": []}, {"text": "The CQA triples are preprocessed with stopwords removal (Chinese sentences are segmented into words in advance by FudanNLP toolkit().", "labels": [], "entities": [{"text": "FudanNLP toolkit", "start_pos": 114, "end_pos": 130, "type": "DATASET", "confidence": 0.9042825400829315}]}, {"text": "In order to evaluate our retrieval system, we divide our dataset into two parts.", "labels": [], "entities": []}, {"text": "The first part is used as training dataset; the rest is used as test dataset for evaluation.", "labels": [], "entities": []}, {"text": "The datasets are shown in  We compare our method with two baseline methods: Okapi BM25 and LSI and two stateof-the-art methods: (Jeon et al., 2005b)(.", "labels": [], "entities": [{"text": "Okapi BM25", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.9056813418865204}]}, {"text": "In LSI, we regard each triple as a single document.", "labels": [], "entities": []}, {"text": "Three annotators are involved in the evaluation process.", "labels": [], "entities": []}, {"text": "Given a returned result, two annotators are asked to label it with \"relevant\" or \"irrelevant\".", "labels": [], "entities": []}, {"text": "If an annotator considers the returned result semantically equivalent to the queried question, he labels it as \"relevant\"; otherwise, it is labeled as \"irrelevant\".", "labels": [], "entities": []}, {"text": "If a conflict happens, the third annotator will make the final judgement.", "labels": [], "entities": []}, {"text": "We use mean average precision (MAP) to evaluate the effectiveness of each method.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 7, "end_pos": 35, "type": "METRIC", "confidence": 0.9314946830272675}]}, {"text": "The experiment results are illustrated in and 4, which show that our method outperforms the others on both datasets.", "labels": [], "entities": []}, {"text": "The primary reason is that we incorporate the content of the question body and the answer parts into the process of question retrieval, which should provide additional relevance information.", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.7818413078784943}]}, {"text": "Different to  It is worth noting that the problem of data sparsity is more crucial for LSTI since the size of a tensor in LSTI is larger than a termdocument matrix in LSI.", "labels": [], "entities": []}, {"text": "When the size of data is small, LSTI tends to just align the common words and thus cannot find the corresponding relations among the focus words in CQA triples.", "labels": [], "entities": []}, {"text": "Therefore, more CQA triples may result in better performance for our method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of Collected Datasets", "labels": [], "entities": [{"text": "Statistics of Collected Datasets", "start_pos": 10, "end_pos": 42, "type": "DATASET", "confidence": 0.5409540981054306}]}, {"text": " Table 3: Retrieval Performance on Dataset  from Yahoo! Answers", "labels": [], "entities": [{"text": "Retrieval", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9353927969932556}, {"text": "Yahoo! Answers", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.8093956311543783}]}, {"text": " Table 4: Retrieval Performance on Dataset  from Baidu Zhidao", "labels": [], "entities": [{"text": "Retrieval", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9340022802352905}]}]}