{"title": [], "abstractContent": [{"text": "During real-life interactions, people are naturally gesturing and modulating their voice to emphasize specific points or to express their emotions.", "labels": [], "entities": []}, {"text": "With the recent growth of social websites such as YouTube, Facebook, and Amazon, video reviews are emerging as anew source of multimodal and natural opinions that has been left almost untapped by automatic opinion analysis techniques.", "labels": [], "entities": []}, {"text": "This paper presents a method for multimodal sentiment classification , which can identify the sentiment expressed in utterance-level visual datas-treams.", "labels": [], "entities": [{"text": "multimodal sentiment classification", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.7094559669494629}]}, {"text": "Using anew multimodal dataset consisting of sentiment annotated utterances extracted from video reviews, we show that multimodal sentiment analysis can be effectively performed, and that the joint use of visual, acoustic, and linguistic modalities can lead to error rate reductions of up to 10.5% as compared to the best performing individual modality.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 118, "end_pos": 147, "type": "TASK", "confidence": 0.7586837609608968}, {"text": "error rate", "start_pos": 260, "end_pos": 270, "type": "METRIC", "confidence": 0.9595979750156403}]}], "introductionContent": [{"text": "Video reviews represent a growing source of consumer information that gained increasing interest from companies, researchers, and consumers.", "labels": [], "entities": []}, {"text": "Popular web platforms such as YouTube, Amazon, Facebook, and ExpoTV have reported a significant increase in the number of consumer reviews in video format over the past five years.", "labels": [], "entities": [{"text": "ExpoTV", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.9313338994979858}]}, {"text": "Compared to traditional text reviews, video reviews provide a more natural experience as they allow the viewer to better sense the reviewer's emotions, beliefs, and intentions through richer channels such as intonations, facial expressions, and body language.", "labels": [], "entities": []}, {"text": "Much of the work to date on opinion analysis has focused on textual data, and a number of resources have been created including lexicons (Wiebe and) or large annotated datasets).", "labels": [], "entities": [{"text": "opinion analysis", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.8432212769985199}]}, {"text": "Given the accelerated growth of other media on the Web and elsewhere, which includes massive collections of videos (e.g., YouTube, Vimeo, VideoLectures), images (e.g., Flickr, Picasa), audio clips (e.g., podcasts), the ability to address the identification of opinions in the presence of diverse modalities is becoming increasingly important.", "labels": [], "entities": [{"text": "identification of opinions", "start_pos": 242, "end_pos": 268, "type": "TASK", "confidence": 0.8388745387395223}]}, {"text": "This has motivated researchers to start exploring multimodal clues for the detection of sentiment and emotions in video content.", "labels": [], "entities": [{"text": "detection of sentiment and emotions in video content", "start_pos": 75, "end_pos": 127, "type": "TASK", "confidence": 0.7920419126749039}]}, {"text": "In this paper, we explore the addition of speech and visual modalities to text analysis in order to identify the sentiment expressed in video reviews.", "labels": [], "entities": []}, {"text": "Given the non homogeneous nature of full-video reviews, which typically include a mixture of positive, negative, and neutral statements, we decided to perform our experiments and analyses at the utterance level.", "labels": [], "entities": []}, {"text": "This is inline with earlier work on text-based sentiment analysis, where it has been observed that full-document reviews often contain both positive and negative comments, which led to a number of methods addressing opinion analysis at sentence level.", "labels": [], "entities": [{"text": "text-based sentiment analysis", "start_pos": 36, "end_pos": 65, "type": "TASK", "confidence": 0.6875537236531576}, {"text": "opinion analysis", "start_pos": 216, "end_pos": 232, "type": "TASK", "confidence": 0.7159778028726578}]}, {"text": "Our results show that relying on the joint use of linguistic, acoustic, and visual modalities allows us to better sense the sentiment being expressed as compared to the use of only one modality at a time.", "labels": [], "entities": []}, {"text": "Another important aspect of this paper is the introduction of anew multimodal opinion database annotated at the utterance level which is, to our knowledge, the first of its kind.", "labels": [], "entities": []}, {"text": "In our work, this dataset enabled a wide range of multimodal sentiment analysis experiments, addressing the relative importance of modalities and individual features.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.6707262297471365}]}, {"text": "The following section presents related work in text-based sentiment analysis and audio-visual emotion recognition.", "labels": [], "entities": [{"text": "text-based sentiment analysis", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.6505084733168284}, {"text": "audio-visual emotion recognition", "start_pos": 81, "end_pos": 113, "type": "TASK", "confidence": 0.6545390586058298}]}, {"text": "Section 3 describes our new multimodal datasets with utterance-level sentiment annotations.", "labels": [], "entities": []}, {"text": "Section 4 presents our multimodal sen-timent analysis approach, including details about our linguistic, acoustic, and visual features.", "labels": [], "entities": [{"text": "multimodal sen-timent analysis", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.6263914207617441}]}, {"text": "Our experiments and results on multimodal sentiment classification are presented in Section 5, with a detailed discussion and analysis in Section 6.", "labels": [], "entities": [{"text": "multimodal sentiment classification", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.7293284138043722}]}], "datasetContent": [{"text": "For our experiments, we created a dataset of utterances (named MOUD) containing product opinions expressed in Spanish.", "labels": [], "entities": [{"text": "MOUD) containing product opinions expressed in Spanish", "start_pos": 63, "end_pos": 117, "type": "TASK", "confidence": 0.7620995417237282}]}, {"text": "We chose to work with Spanish because it is a widely used language, and it is the native language of the main author of this paper.", "labels": [], "entities": []}, {"text": "We started by collecting a set of videos from the social media website YouTube, using several keywords likely to lead to a product review or recommendation.", "labels": [], "entities": []}, {"text": "Starting with the YouTube search page, videos were found using the following keywords: mis products favoritos (my favorite products), products que no recomiendo (non recommended products), mis perfumes favoritos (my favorite perfumes), peliculas recomendadas (recommended movies), peliculas que no recomiendo (non recommended movies) and libros recomendados (recommended books), libros que no recomiendo (non recommended books).", "labels": [], "entities": []}, {"text": "Notice that the keywords are not targeted at a specific product type; rather, we used a variety of product names, so that the dataset has some degree of generality within the broad domain of product reviews.", "labels": [], "entities": []}, {"text": "Publicly available from the authors webpage.", "labels": [], "entities": []}, {"text": "Among all the videos returned by the YouTube search, we selected only videos that respected the following guidelines: the speaker should be in front of the camera; her face should be clearly visible, with a minimum amount of face occlusion during the recording; there should not be any background music or animation.", "labels": [], "entities": []}, {"text": "The final video set includes 80 videos randomly selected from the videos retrieved from YouTube that also met the guidelines above.", "labels": [], "entities": []}, {"text": "The dataset includes 15 male and 65 female speakers, with their age approximately ranging from 20 to 60 years.", "labels": [], "entities": []}, {"text": "All the videos were first pre-processed to eliminate introductory titles and advertisements.", "labels": [], "entities": []}, {"text": "Since the reviewers often switched topics when expressing their opinions, we manually selected a 30 seconds opinion segment from each video to avoid having multiple topics in a single review.", "labels": [], "entities": []}, {"text": "We run our sentiment classification experiments on the MOUD dataset introduced earlier.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.954431414604187}, {"text": "MOUD dataset", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.8470478355884552}]}, {"text": "From the dataset, we remove utterances labeled as neutral, thus keeping only the positive and negative utterances with valid visual features.", "labels": [], "entities": []}, {"text": "The removal of neutral utterances is done for two main reasons.", "labels": [], "entities": [{"text": "removal of neutral utterances", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.7910878956317902}]}, {"text": "First, the number of neutral utterances in the dataset is rather small.", "labels": [], "entities": []}, {"text": "Second, previous work in subjectivity and sentiment analysis has demonstrated that a layered approach (where neutral statements are first separated from opinion statements followed by a separation between positive and negative statements) works better than a single three-way classification.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.9230921864509583}]}, {"text": "After this process, we are left with an experimental dataset of 412 utterances, 182 of which are labeled as positive, and 231 are labeled as negative.", "labels": [], "entities": []}, {"text": "From each utterance, we extract the linguistic, acoustic, and visual features described above, which are then combined using the early fusion (or feature-level fusion) approach (Hall and Llinas,.", "labels": [], "entities": []}, {"text": "In this approach, the features collected from all the multimodal streams are combined into a single feature vector, thus resulting in one vector for each utterance in the dataset which is used to make a decision about the sentiment orientation of the utterance.", "labels": [], "entities": []}, {"text": "We run several comparative experiments, using one, two, and three modalities at a time.", "labels": [], "entities": []}, {"text": "We use the entire set of 412 utterances and run tenfold cross validations using an SVM classifier, as implemented in the Weka toolkit.", "labels": [], "entities": [{"text": "Weka toolkit", "start_pos": 121, "end_pos": 133, "type": "DATASET", "confidence": 0.9490962624549866}]}, {"text": "In line with previous work on emotion recognition in speech where utterances are selected in a speaker dependent manner (i.e., utterances from the same speaker are included in both training and test), as well as work on sentence-level opinion classification where document boundaries are not considered in the split performed between the training and test sets, the training/test split for each fold is performed at utterance level regardless of the video they belong to. shows the results of the utterance-level sentiment classification experiments.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7652891576290131}, {"text": "sentence-level opinion classification", "start_pos": 220, "end_pos": 257, "type": "TASK", "confidence": 0.6422496338685354}, {"text": "utterance-level sentiment classification", "start_pos": 497, "end_pos": 537, "type": "TASK", "confidence": 0.7103803157806396}]}, {"text": "The baseline is obtained using the ZeroR classifier, which assigns the most frequent label by default, averaged over the ten folds.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Utterance-level sentiment classification  with linguistic, acoustic, and visual features.", "labels": [], "entities": [{"text": "Utterance-level sentiment classification", "start_pos": 10, "end_pos": 50, "type": "TASK", "confidence": 0.9091305136680603}]}, {"text": " Table 3: Correlations between several visual and acoustic features. Visual features: AU6 Cheek raise,  AU12 Lip corner pull, AU45 Blink eye and closure, AU1,1+4 Distress brow. Acoustic features: Pitch,  Voice probability, Intensity, Energy. *Correlation is significant at the 0.05 level (1-tailed)  .", "labels": [], "entities": [{"text": "AU6 Cheek raise", "start_pos": 86, "end_pos": 101, "type": "METRIC", "confidence": 0.7167636056741079}, {"text": "Blink eye and closure", "start_pos": 131, "end_pos": 152, "type": "METRIC", "confidence": 0.8597327619791031}, {"text": "Correlation", "start_pos": 243, "end_pos": 254, "type": "METRIC", "confidence": 0.9812626242637634}]}, {"text": " Table 4: Video-level sentiment classification with  linguistic, acoustic, and visual features.", "labels": [], "entities": [{"text": "Video-level sentiment classification", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.755160907904307}]}]}