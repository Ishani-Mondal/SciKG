{"title": [{"text": "Models of Semantic Representation with Visual Attributes", "labels": [], "entities": []}], "abstractContent": [{"text": "We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes.", "labels": [], "entities": []}, {"text": "We create anew large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images.", "labels": [], "entities": []}, {"text": "We use this dataset to train attribute classi-fiers and integrate their predictions with text-based distributional models of word meaning.", "labels": [], "entities": []}, {"text": "We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on hand-crafted norming data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen increased interest in grounded language acquisition, where the goal is to extract representations of the meaning of natural language tied to the physical world.", "labels": [], "entities": [{"text": "grounded language acquisition", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.6594826777776083}]}, {"text": "The language grounding problem has assumed several guises in the literature such as semantic parsing (), mapping natural language instructions to executable actions (), associating simplified language to perceptual data such as images or video, and learning the meaning of words based on linguistic and perceptual input (.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.7379200011491776}]}, {"text": "In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models.", "labels": [], "entities": []}, {"text": "The motivation for models that do not learn exclusively from text is twofold.", "labels": [], "entities": []}, {"text": "From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language.", "labels": [], "entities": []}, {"text": "From an engineering perspective, the ability to learn representations for multimodal data has many practical applications including image retrieval () and annotation), text illustration (), object and scene recognition, and robot navigation).", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 132, "end_pos": 147, "type": "TASK", "confidence": 0.7349604070186615}, {"text": "text illustration", "start_pos": 168, "end_pos": 185, "type": "TASK", "confidence": 0.8005927205085754}, {"text": "object and scene recognition", "start_pos": 190, "end_pos": 218, "type": "TASK", "confidence": 0.6122866272926331}, {"text": "robot navigation", "start_pos": 224, "end_pos": 240, "type": "TASK", "confidence": 0.8074271082878113}]}, {"text": "One strand of research uses feature norms as a stand-in for sensorimotor experience.", "labels": [], "entities": []}, {"text": "Feature norms are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word.", "labels": [], "entities": []}, {"text": "The attributes represent perceived physical and functional properties associated with the referents of words.", "labels": [], "entities": []}, {"text": "For example, apples are typically green or red, round, shiny, smooth, crunchy, tasty, and soon; dogs have four legs and bark, whereas chairs are used for sitting.", "labels": [], "entities": []}, {"text": "Feature norms are instrumental in revealing which dimensions of meaning are psychologically salient, however, their use as a proxy for people's perceptual representations can itself be problematic.", "labels": [], "entities": []}, {"text": "The number and types of attributes generated can vary substantially as a function of the amount of time devoted to each concept.", "labels": [], "entities": []}, {"text": "It is not entirely clear how people generate attributes and whether all of these are important for representing concepts.", "labels": [], "entities": []}, {"text": "Finally, multiple participants are required to create a representation for each con-cept, which limits elicitation studies to a small number of concepts and the scope of any computational model based on feature norms.", "labels": [], "entities": []}, {"text": "Another strand of research focuses exclusively on the visual modality, even though the grounding problem could involve auditory, motor, and haptic modalities as well.", "labels": [], "entities": []}, {"text": "This is not entirely surprising.", "labels": [], "entities": []}, {"text": "Visual input represents a major source of data from which humans can learn semantic representations of linguistic and non-linguistic communicative actions.", "labels": [], "entities": []}, {"text": "Furthermore, since images are ubiquitous, visual data can be gathered far easier than some of the other modalities.", "labels": [], "entities": []}, {"text": "Distributional models that integrate the visual modality have been learned from texts and images or from ImageNet (), e.g., by exploiting the fact that images in this database are hierarchically organized according to WordNet synsets (.", "labels": [], "entities": []}, {"text": "Images are typically represented on the basis of low-level features such as SIFT, whereas texts are treated as bags of words.", "labels": [], "entities": []}, {"text": "Our work also focuses on images as away of physically grounding the meaning of words.", "labels": [], "entities": []}, {"text": "We, however, represent them by high-level visual attributes instead of low-level image features.", "labels": [], "entities": []}, {"text": "Attributes are not concept or category specific (e.g., animals have stripes and so do clothing items; balls are round, and so are oranges and coins), and thus allow us to express similarities and differences across concepts more easily.", "labels": [], "entities": []}, {"text": "Furthermore, attributes allow us to generalize to unseen objects; it is possible to say something about them even though we cannot identify them (e.g., it has a beak and along tail).", "labels": [], "entities": []}, {"text": "We show that this attribute-centric approach to representing images is beneficial for distributional models of lexical meaning.", "labels": [], "entities": []}, {"text": "Our attributes are similar to those provided by participants in norming studies, however, importantly they are learned from training data (a database of images and their visual attributes) and thus generalize to new images without additional human involvement.", "labels": [], "entities": []}, {"text": "In the following we describe our efforts to create anew large-scale dataset that consists of 688K images that match the same concrete concepts used in the feature norming study of.", "labels": [], "entities": []}, {"text": "We derive a taxonomy of 412 visual attributes and explain how we learn attribute classifiers following recent work in computer vision.", "labels": [], "entities": []}, {"text": "Next, we show that this attribute-based image representation can be usefully integrated with textual data to create distributional models that give a better fit to human word association data over models that rely on human generated feature norms.", "labels": [], "entities": []}], "datasetContent": [{"text": "Concepts and Images We created a dataset of images and their visual attributes for the nouns contained in feature norms.", "labels": [], "entities": []}, {"text": "The norms cover a wide range of concrete concepts including animate and inanimate things (e.g., animals, clothing, vehicles, utensils, fruits, and vegetables) and were collected by presenting participants with words and asking them to list properties of the objects to which the words referred.", "labels": [], "entities": []}, {"text": "To avoid confusion, in the remainder of this paper we will use the term attribute to refer to properties of concepts and the term feature to refer to image features, such as color or edges.", "labels": [], "entities": []}, {"text": "Images for the concepts in McRae et al.'s (2005) production norms were harvested from ImageNet (, an ontology of images based on the nominal hierarchy of WordNet.", "labels": [], "entities": [{"text": "McRae et al.'s (2005) production norms", "start_pos": 27, "end_pos": 65, "type": "DATASET", "confidence": 0.7277605563402176}, {"text": "WordNet", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.9442512392997742}]}, {"text": "ImageNet has more than 14 million images spanning 21K WordNet synsets.", "labels": [], "entities": [{"text": "ImageNet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8878107070922852}, {"text": "WordNet synsets", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.9129036068916321}]}, {"text": "We chose this database due to its high coverage and the high quality of its images (i.e., cleanly labeled and high resolution).", "labels": [], "entities": []}, {"text": "McRae et al.'s norms contain 541 concepts out of which 516 appear in ImageNet 1 and are represented by 688K images overall.", "labels": [], "entities": []}, {"text": "The average number of images per concept is 1,310 with the most popular being closet (2,149 images) and the least popular prune  The images depicting each concept were randomly partitioned into a training, development, and test set.", "labels": [], "entities": []}, {"text": "For most concepts the development set contained a maximum of 100 images and the test set a maximum of 200 images.", "labels": [], "entities": []}, {"text": "Concepts with less than 800 images in total were split into 1/8 test and development set each, and 3/4 training set.", "labels": [], "entities": []}, {"text": "The development set was used for devising and refining our attribute annotation scheme.", "labels": [], "entities": []}, {"text": "The training and test sets were used for learning and evaluating, respectively, attribute classifiers (see Section 4).", "labels": [], "entities": []}, {"text": "Attribute Annotation Our aim was to develop a set of visual attributes that are both discriminating and cognitively plausible, i.e., humans would generally use them to describe a concrete concept.", "labels": [], "entities": []}, {"text": "As a starting point, we thus used the visual attributes from norming study.", "labels": [], "entities": []}, {"text": "Attributes capturing other primary sensory information (e.g., smell, sound), functional/motor properties, or encyclopaedic information were not taken into account.", "labels": [], "entities": []}, {"text": "For example, is purple is a valid visual attribute for an eggplant, whereas a vegetable is not, since it cannot be visualized.", "labels": [], "entities": []}, {"text": "Collating all the visual attributes in the norms resulted in a total of 673 which we further modified and extended during the annotation process explained below.", "labels": [], "entities": []}, {"text": "The annotation was conducted on a per-concept rather than a per-image basis (as for example in).", "labels": [], "entities": []}, {"text": "For each concept (e.g., bear or eggplant), we inspected the images in the development set and chose all visual attributes that applied.", "labels": [], "entities": []}, {"text": "If an attribute was generally true for the concept, but the images did not provide enough evidence, the attribute was nevertheless chosen and labeled with <no evidence>.", "labels": [], "entities": []}, {"text": "For example, a plum has a pit, but most images in ImageNet show plums where only the outer part of the fruit is visible.", "labels": [], "entities": []}, {"text": "Attributes supported by the image data but missing from the norms were added.", "labels": [], "entities": []}, {"text": "For example, has lights and has bumper are attributes of cars but are not included in the norms.", "labels": [], "entities": []}, {"text": "Attributes were grouped in eight general classes shown in.", "labels": [], "entities": []}, {"text": "Annotation proceeded on a category-by-category basis, e.g., first all foodrelated concepts were annotated, then animals, vehicles, and soon.", "labels": [], "entities": []}, {"text": "Two annotators (both co-authors of this paper) developed the set of attributes for each category.", "labels": [], "entities": []}, {"text": "One annotator first labeled concepts with their attributes, and the other annotator reviewed the annotations, making changes if needed.", "labels": [], "entities": []}, {"text": "Annotations were revised and compared per category in order to ensure consistency across all concepts of that category.", "labels": [], "entities": []}, {"text": "Our methodology is slightly different from in that we did not simply transfer the attributes from the norms to the concepts in question but refined and extended them according to the visual data.", "labels": [], "entities": []}, {"text": "There are several reasons for this.", "labels": [], "entities": []}, {"text": "Firstly, it makes sense to select attributes corroborated by the images.", "labels": [], "entities": []}, {"text": "Secondly, by looking at the actual images, we could eliminate errors in norms.", "labels": [], "entities": []}, {"text": "For example, eight study participants erroneously thought that a catfish has scales.", "labels": [], "entities": []}, {"text": "Thirdly, during the annotation process, we normalized synonymous attributes (e.g., has pit and has stone) and attributes that exhibited negligible variations has 2 pieces, has pointed end, has strap, has thumb, has buckles, has heels has shoe laces, has soles, is black, is brown, is white, made of leather, made of rubber climbs, climbs trees, crawls, hops, jumps, eats, eats nuts, is small, has bushy tail has 4 legs, has head, has neck, has nose, has snout, has tail, has claws has eyes, has feet, has toes, diff colours, has 2 legs, has 2 wheels, has windshield, has floorboard, has stand, has tank has mudguard, has seat, has exhaust pipe, has frame, has handlebar, has lights, has mirror has step-through frame, is black, is blue, is red, is white, made of aluminum, made of steel: Attribute predictions for sandals, squirrel, and motorcycle.", "labels": [], "entities": []}, {"text": "in meaning (e.g., has stem and has stalk).", "labels": [], "entities": []}, {"text": "Finally, our aim was to collect an exhaustive list of visual attributes for each concept which is consistent across all members of a category.", "labels": [], "entities": []}, {"text": "This is unfortunately not the casein McRae et al.'s norms.", "labels": [], "entities": [{"text": "McRae et al.", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.8974360823631287}]}, {"text": "Participants were asked to list up to 14 different properties that describe a concept.", "labels": [], "entities": []}, {"text": "As a result, the attributes of a concept denote the set of properties humans consider most salient.", "labels": [], "entities": []}, {"text": "For example, both, lemons and oranges have pulp.", "labels": [], "entities": []}, {"text": "But the norms provide this attribute only for the second concept.", "labels": [], "entities": []}, {"text": "On average, each concept was annotated with 19 attributes; approximately 14.5 of these were not part of the semantic representation created by participants for that concept even though they figured in the representations of other concepts.", "labels": [], "entities": []}, {"text": "Furthermore, on average two McRae et al. attributes per concept were discarded.", "labels": [], "entities": []}, {"text": "Examples of concepts and their attributes from our database 2 are shown in.", "labels": [], "entities": []}, {"text": "Evaluation Task We evaluated the distributional models presented in Section 5 on the word association norms collected by.", "labels": [], "entities": []}, {"text": "These were established by presenting a large number of participants with a cue word (e.g., rice) and asking them to name an associate From http://w3.usf.edu/FreeAssociation/.", "labels": [], "entities": []}, {"text": "word in response (e.g., Chinese, wedding, food, white).", "labels": [], "entities": []}, {"text": "For each cue, the norms provide a set of associates and the frequencies with which they were named.", "labels": [], "entities": []}, {"text": "We can thus compute the probability distribution over associates for each cue.", "labels": [], "entities": []}, {"text": "Analogously, we can estimate the degree of similarity between a cue and its associates using our models.", "labels": [], "entities": []}, {"text": "The norms contain 63,619 unique cueassociate pairs.", "labels": [], "entities": []}, {"text": "Of these, 435 pairs were covered by and our models.", "labels": [], "entities": []}, {"text": "We also experimented with 1,716 pairs that were not part of McRae et al.'s study but belonged to concepts covered by our attribute taxonomy (e.g., animals, vehicles), and were present in our corpus and ImageNet.", "labels": [], "entities": [{"text": "ImageNet", "start_pos": 202, "end_pos": 210, "type": "DATASET", "confidence": 0.9278347492218018}]}, {"text": "Using correlation analysis (Spearman's \u03c1), we examined the degree of linear relationship between the human cue-associate probabilities and the automatically derived similarity values.", "labels": [], "entities": []}, {"text": "8 Parameter Settings In order to integrate the visual attributes with the models described in Section 5 we must select the appropriate threshold value \u03b4 (see Eq.).", "labels": [], "entities": []}, {"text": "We optimized this value on the development set and obtained best results with \u03b4 = 0.", "labels": [], "entities": []}, {"text": "We also experimented with thresholding the attribute prediction scores and with excluding attributes with low precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9745365381240845}]}, {"text": "In both cases, we obtained best results when using all attributes.", "labels": [], "entities": []}, {"text": "We could apply CCA to the vectors representing each image separately and then compute a weighted centroid on the projected vectors.", "labels": [], "entities": []}, {"text": "We refrained from doing this as it involves additional parameters and assumes input different from the other models.", "labels": [], "entities": []}, {"text": "We measured the similarity between two words using the cosine of the angle.", "labels": [], "entities": []}, {"text": "For the attribute-topic model, the number of predefined components C was set to 10.", "labels": [], "entities": []}, {"text": "In this model, similarity was measured as defined by.", "labels": [], "entities": [{"text": "similarity", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.992094099521637}]}, {"text": "The underlying idea is that word association can be expressed as a conditional distribution.", "labels": [], "entities": [{"text": "word association", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7223450243473053}]}, {"text": "With regard to the textual attributes, we obtained a 9,394-dimensional semantic space after discarding word-attribute pairs with a log-likelihood ratio scoreless than 19.", "labels": [], "entities": []}, {"text": "We also discarded attributes co-occurring with less than two different words.: Correlation matrix for seen cue-associate pairs and five distributional models.", "labels": [], "entities": []}, {"text": "All correlation coefficients are statistically significant (p < 0.01, N = 435).", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9782350063323975}]}], "tableCaptions": [{"text": " Table 5: Correlation matrix for seen", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9505895972251892}]}, {"text": " Table 6: Correlation matrix for unseen", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9327495098114014}]}, {"text": " Table 7: Model performance on seen", "labels": [], "entities": []}, {"text": " Table 8: Model performance on a subset of Nelson  et al. (1998) cue-associate pairs. Seen are concepts  known to the attribute classifiers and covered by  MixLDA (N = 85). Unseen are concepts covered  by LDA but unknown to the attribute classifiers  (N = 388). All correlation coefficients are statisti- cally significant (p < 0.05).", "labels": [], "entities": [{"text": "MixLDA", "start_pos": 156, "end_pos": 162, "type": "DATASET", "confidence": 0.7467702031135559}]}]}