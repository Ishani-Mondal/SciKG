{"title": [{"text": "Identifying Bad Semantic Neighbors for Improving Distributional Thesauri", "labels": [], "entities": [{"text": "Identifying Bad Semantic Neighbors", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8703513592481613}, {"text": "Improving Distributional Thesauri", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.735170861085256}]}], "abstractContent": [{"text": "Distributional thesauri are now widely used in a large number of Natural Language Processing tasks.", "labels": [], "entities": [{"text": "Natural Language Processing tasks", "start_pos": 65, "end_pos": 98, "type": "TASK", "confidence": 0.7006730362772942}]}, {"text": "However, they are far from containing only interesting semantic relations.", "labels": [], "entities": []}, {"text": "As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures.", "labels": [], "entities": []}, {"text": "In this article, we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry.", "labels": [], "entities": []}, {"text": "This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts.", "labels": [], "entities": []}, {"text": "Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors.", "labels": [], "entities": []}, {"text": "We evaluate the interest of this method fora large set of English nouns with various frequencies.", "labels": [], "entities": []}], "introductionContent": [{"text": "The work we present in this article focuses on the automatic building of a thesaurus from a corpus.", "labels": [], "entities": []}, {"text": "As illustrated by, such thesaurus gives for each of its entries a list of words, called semantic neighbors, that are supposed to be semantically linked to the entry.", "labels": [], "entities": []}, {"text": "Generally, each neighbor is associated with a weight that characterizes the strength of its link with the entry and all the neighbors of an entry are sorted according to the decreasing order of their weight.", "labels": [], "entities": []}, {"text": "The term semantic neighbor is very generic and can have two main interpretations according to the kind of semantic relations it is based on: one relies only on paradigmatic relations, such as hypernymy or synonymy, while the other considers syntagmatic relations, called collocation relations by) in the context of lexical cohesion or \"non-classical relations\" by).", "labels": [], "entities": []}, {"text": "The distinction between these two interpretations refers to the distinction between the notions of semantic similarity and semantic relatedness as it was done in) or in ( for instance.", "labels": [], "entities": []}, {"text": "However, the limit between these two notions is sometimes hard to find in existing work as terms semantic similarity and semantic relatedness are often used interchangeably.", "labels": [], "entities": []}, {"text": "Moreover, semantic similarity is frequently considered as included into semantic relatedness and the two problems are often tackled by using the same methods.", "labels": [], "entities": []}, {"text": "In the remainder of this article, we will use the term semantic similarity with its generic sense and the term semantic relatedness for referring more specifically to similarity based on syntagmatic relations.", "labels": [], "entities": []}, {"text": "Following work such as, a widespread way to build a thesaurus from a corpus is to use a semantic similarity measure for extracting the semantic neighbors of the entries of the thesaurus.", "labels": [], "entities": []}, {"text": "Three main ways of implementing such measures can be distinguished.", "labels": [], "entities": []}, {"text": "The first one relies on handcrafted resources in which semantic relations are clearly identified.", "labels": [], "entities": []}, {"text": "Work based on WordNet-like lexical networks for building semantic similarity measures such as) or () falls into this category.", "labels": [], "entities": []}, {"text": "These measures typically exploit the hierarchical structure of these networks, based on hypernymy relations.", "labels": [], "entities": []}, {"text": "The second approach makes use of a less structured source of knowledge about words such as the definitions of classical dictionaries or the glosses of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 151, "end_pos": 158, "type": "DATASET", "confidence": 0.9626233577728271}]}, {"text": "WordNet's glosses were used to support Lesklike measures in ( and more recently, measures were also defined from Wikipedia or Wiktionaries (.", "labels": [], "entities": [{"text": "WordNet's glosses", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9398121237754822}]}, {"text": "The last option is the corpusbased approach, based on the distributional hypothesis: each word is characterized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share.", "labels": [], "entities": []}, {"text": "This perspective was first adopted by and and then, explored in details in, or.", "labels": [], "entities": []}, {"text": "The problem of improving the results of the \"classical\" implementation of the distributional approach as it can be found in) for instance was already tackled by some work.", "labels": [], "entities": []}, {"text": "A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (, in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by, that proposes a bootstrapping method for modifying the weights of context elements according to the semantic neighbors found by an initial distributional similarity measure.", "labels": [], "entities": []}, {"text": "However, another part of these proposals implies more radical changes.", "labels": [], "entities": []}, {"text": "The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in, the multi-prototype or examplar-based models, the Deep Learning approach of ( or the redefinition of the distributional approach in a Bayesian framework can be classified into this second category.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.717268779873848}, {"text": "Latent Semantic Analysis", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.5970110098520914}]}, {"text": "The work we present in this article takes place in the framework defined by for implementing the distributional approach but proposes anew method for improving a thesaurus builtin this context based on the identification of its bad semantic neighbors rather than on the adaptation of the weight of their features.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results of lead to three main observations.", "labels": [], "entities": []}, {"text": "First, the level of results heavily depends on the frequency range of target words: the best results are obtained for high frequency words while evaluation measures significantly decrease for words whose frequency is low.", "labels": [], "entities": []}, {"text": "Second, the characteristics of the reference resources have a significant impact on results.", "labels": [], "entities": []}, {"text": "WordNet provides a restricted number of synonyms for each noun while the Moby thesaurus contains for each entry a large number of synonyms and similar words.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9798986911773682}, {"text": "Moby thesaurus", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.9313709437847137}]}, {"text": "As a consequence, the precisions at different cut-offs have a significantly higher value with Moby as reference than with WordNet as reference.", "labels": [], "entities": [{"text": "precisions", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9958237409591675}, {"text": "WordNet", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.9444677829742432}]}, {"text": "Finally, the results of Table 2 are compatible with those of (Lin, 1998) for instance (R-prec.", "labels": [], "entities": []}, {"text": "= 11.6 and MAP = 8.1 with WM as reference for all entries of the thesaurus at http://webdocs.cs.ualberta.", "labels": [], "entities": [{"text": "MAP", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9987723231315613}, {"text": "WM", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9235666394233704}]}, {"text": "ca/lindek/Downloads/sim.tgz) if we take into account the fact that the thesaurus of Lin was built from a much larger corpus and with syntactic co-occurrences.", "labels": [], "entities": []}, {"text": "The first thing to notice is that at the global scale, all measures for all references are significantly improved , which means that our hypothesis about the possibility fora discriminative classifier to capture the meaning of a word tends to be validated.", "labels": [], "entities": []}, {"text": "It is an interesting result since the features upon which this classifier was built were taken from WSD and were not specifically selected for this task.", "labels": [], "entities": [{"text": "WSD", "start_pos": 100, "end_pos": 103, "type": "DATASET", "confidence": 0.8206832408905029}]}, {"text": "As a consequence, there is probably some room for improvement.", "labels": [], "entities": []}, {"text": "If we go into details, clearly shows two main trends.", "labels": [], "entities": []}, {"text": "First, the improvement of results is particularly effective for middle frequency entries, then for low frequency and finally, for high frequency entries.", "labels": [], "entities": []}, {"text": "Because of their already high level in the initial thesaurus, results for high frequency entries are difficult to improve but it is important to note that our selection of bad neighbors has a very low error rate, which at least preserves these results.", "labels": [], "entities": [{"text": "error rate", "start_pos": 201, "end_pos": 211, "type": "METRIC", "confidence": 0.9661427140235901}]}, {"text": "This is confirmed by the fact that, with WordNet as reference, only 744 neighbors were found wrongly downgraded, spread over 686 entries, which represents only 5% of all downgraded neighbors.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9721912741661072}]}, {"text": "The second main trend of: Results of the reranking of semantic neighbors cerns the type of semantic relations: results with Moby as reference are improved in a larger extent than results with WordNet as reference.", "labels": [], "entities": []}, {"text": "This suggests that our procedure is more effective for semantically related words than for semantically similar words, which can be considered as a little bit surprising since the notion of context in our discriminative classifier seems a priori more strict than in \"classical\" distributional contexts.", "labels": [], "entities": []}, {"text": "However, this point must be investigated further as a significant part of the relations in Moby, even if they do no represent the largest part of them, are paradigmatic relations.", "labels": [], "entities": []}, {"text": "WordNet respect, admiration, regard Moby admiration, appreciation, acceptance, dignity, regard, respect, account, adherence, consideration, estimate, estimation, fame, greatness, reverence + 79 words more initial cordiality, gratitude, admiration, comradeship, back-scratching, perplexity, respect, ruination, appreciation, neighbourliness . .", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8526764512062073}, {"text": "acceptance", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9193193316459656}, {"text": "estimate", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9864286780357361}]}, {"text": "reranking gratitude, admiration, respect, appreciation, neighborliness, trust, empathy, goodwill, reciprocity, half-staff, affection, self-esteem, reverence, longing, regard . .", "labels": [], "entities": []}, {"text": ".: Impact of our reranking for the entry esteem illustrates more precisely the impact of our reranking procedure for the middle frequency entry esteem.", "labels": [], "entities": []}, {"text": "Its WordNet row gives all the reference synonyms for this entry in WordNet while its Moby row gives the first reference related words for this entry in Moby.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9646534323692322}]}, {"text": "In our initial thesaurus, the first two neighbors of esteem that are present in our reference resources are admiration (rank 3) and respect (rank 7).", "labels": [], "entities": [{"text": "admiration", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9951338171958923}, {"text": "respect", "start_pos": 132, "end_pos": 139, "type": "METRIC", "confidence": 0.9877874255180359}]}, {"text": "The reranking produces a thesaurus in which these two words appear as the second and the third neighbors of the entry because neighbors without clear relation with it such as backscratching were downgraded while its third synonym in WordNet is raised from rank 22 to rank 15.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 233, "end_pos": 240, "type": "DATASET", "confidence": 0.9513674378395081}]}, {"text": "Moreover, the number of neighbors among the first 15 ones that are present in Moby increases from 3 to 5.", "labels": [], "entities": [{"text": "Moby", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.9271003603935242}]}], "tableCaptions": [{"text": " Table 3: Results of word-in-context classifiers", "labels": [], "entities": []}]}