{"title": [{"text": "Exploring Word Order Universals: a Probabilistic Graphical Model Approach", "labels": [], "entities": [{"text": "Exploring Word Order Universals", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6588911786675453}]}], "abstractContent": [{"text": "In this paper we propose a probabilistic graph-ical model as an innovative framework for studying typological universals.", "labels": [], "entities": []}, {"text": "We view language as a system and linguistic features as its components whose relationships are encoded in a Directed Acyclic Graph (DAG).", "labels": [], "entities": []}, {"text": "Taking discovery of the word order universals as a knowledge discovery task we learn the graph-ical representation of a word order subsystem which reveals a finer structure such as direct and indirect dependencies among word order features.", "labels": [], "entities": []}, {"text": "Then probabilistic inference enables us to seethe strength of such relationships: given the observed value of one feature (or combination of features), the probabilities of values of other features can be calculated.", "labels": [], "entities": []}, {"text": "Our model is not restricted to using only two values of a feature.", "labels": [], "entities": []}, {"text": "Using imputation technique and EM algorithm it can handle missing values well.", "labels": [], "entities": []}, {"text": "Model averaging technique solves the problem of limited data.", "labels": [], "entities": [{"text": "Model averaging", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8412640690803528}]}, {"text": "In addition the in-cremental and divide-and-conquer method addresses the areal and genetic effects simultaneously instead of separately as in Daum\u00e9 III and Campbell (2007).", "labels": [], "entities": []}], "introductionContent": [{"text": "Ever since proposed 45 universals of language based on a sample of 30 languages, typologists have been pursuing this topic actively for the past half century.", "labels": [], "entities": []}, {"text": "Since some of them do not agree with the term (or concept) of \"universal\" they use other terminology such as \"correlation\", \"co-occurrence\", \"dependency\", \"interaction\" and \"implication\" to refer to the relationships between/among linguistic feature pairs most of which concern morpheme and word order.", "labels": [], "entities": []}, {"text": "Indeed the definition of \"universals\" has never been clear until recently, when most typologists agreed that such universals should be statistical universals which are \"statistical tendencies\" discovered from data samples by using statistical methods as used in any other science.", "labels": [], "entities": []}, {"text": "Only those tendencies that can be extrapolated to make general conclusions about the population can be claimed to be \"universals\" since they reflect the global preferences of value distribution of linguistic features across genealogical hierarchy and geographical areas.", "labels": [], "entities": []}, {"text": "Previous statistical methods in the research of word order universals have yielded interesting results but they have to make strong assumptions and do a considerable amount of data preprocessing to make the data fit the statistical model.", "labels": [], "entities": [{"text": "word order universals", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.6469149192174276}]}, {"text": "Recent studies using probabilistic models are much more flexible and can handle noise and uncertainty better).", "labels": [], "entities": []}, {"text": "However these models still rely on strong theoretic assumptions and heavy data treatment, such as using only two values of word order pairs while discarding other values, purposefully selecting a subset of the languages to study, or selecting partial data with complete values.", "labels": [], "entities": []}, {"text": "In this paper we introduce a novel approach of using a probabilistic graphical model to study word order universals.", "labels": [], "entities": []}, {"text": "Using this model we can have a graphic representation of the structure of language as a complex system composed of linguistic features.", "labels": [], "entities": []}, {"text": "Then the relationship among these features can be quantified as probabilities.", "labels": [], "entities": []}, {"text": "Such a model does not rely on strong assumptions and has little constraint on data.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: in Section 2 we discuss the rationale of using a probabilistic graphic model to study word order universals and introduce our two models; Section 3 is about learning structures and parameters for the two models.", "labels": [], "entities": []}, {"text": "Section 4 discusses the quantitative analysis while Section 5 gives qualitative analysis of the results.", "labels": [], "entities": []}, {"text": "Section 6 is about inference such as MAP query and in Section 6 we discuss the advantage of using PGM to study word order universals.", "labels": [], "entities": []}], "datasetContent": [{"text": "Comparison with Greenberg's work is shown in (in Appendix A).", "labels": [], "entities": []}, {"text": "If the probability is above 0.5 we say it is a universal and mark it red.", "labels": [], "entities": []}, {"text": "We think values like 0.4-0.5 can also give us some suggestive estimates therefore we mark these green.", "labels": [], "entities": []}, {"text": "For Universal 2, 3, 4, 5, 10, 18 and 19, our results conform to Greenberg's.", "labels": [], "entities": []}, {"text": "But for others there are discrepancies of different degrees.", "labels": [], "entities": []}, {"text": "For example, for U12 our results show that \"VSO\" can predict \"Initial\" but not very strongly compared with \"SOV\" predicting \"Not_Initial\".", "labels": [], "entities": [{"text": "U12", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.7826877236366272}, {"text": "VSO", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.8806111812591553}, {"text": "Initial", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.8971462249755859}]}, {"text": "(in Appendix A) shows our comparison with Dryer (1992)'s work.", "labels": [], "entities": []}, {"text": "We noticed there is an asymmetry in terms of V_O's influence on other word order pairs, which was not discussed in previous work.", "labels": [], "entities": []}, {"text": "In the correlated pairs, only ADP_NP and G_N show bidirectional correlation with O_V while PoQPar becomes a noncorrelated pair.", "labels": [], "entities": []}, {"text": "In the non-correlated pairs, Dem_N becomes a correlated pair and other pairs also show correlation of weak strength.", "labels": [], "entities": []}, {"text": "Most of our results therefore do not confirm Dryer's findings.", "labels": [], "entities": []}, {"text": "We compared the probabilities of single value pairs of the top ten word order universals with Daum\u00e9 III and Campbell's results, which are shown in the following figures.", "labels": [], "entities": []}, {"text": "P(true) is the probability of having the particular implication; prob is the probability calculated in a different way which is not specified in Daum\u00e9 III and Campbell's work.", "labels": [], "entities": [{"text": "P", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9206651449203491}]}, {"text": "It can be seen that our model provides moderate numbers which fall between the two probabilities in Daum\u00e9 III and Campbell's results.", "labels": [], "entities": []}, {"text": "In the two universals that have the biggest gaps are: 9) Prepositions ->VO and 10) Adjective-Noun>Demonstrative-Noun.", "labels": [], "entities": []}, {"text": "In the three universals that have the biggest gaps are: 3) NounGenitive->Initial subordinator word, 6) NounGenitive->Prepositions and 8) OV->SV.", "labels": [], "entities": []}, {"text": "It is hard to tell which model does a better job just by doing comparison like this.", "labels": [], "entities": []}, {"text": "Daum\u00e9 III and Campbell's model computes the probabilities of 3442 feature pairs separately.", "labels": [], "entities": []}, {"text": "Their model with two values as nodes does not consider the more complex dependencies among more than two features.", "labels": [], "entities": []}, {"text": "Our model provides a better solution by trying to maximize the joint probabilities of all word order feature pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Comparison with Greenberg's work", "labels": [], "entities": []}]}