{"title": [{"text": "DERIVBASE: Inducing and Evaluating a Derivational Morphology Resource for German", "labels": [], "entities": [{"text": "DERIVBASE", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8210833072662354}]}], "abstractContent": [{"text": "Derivational models are still an under-researched area in computational morphology.", "labels": [], "entities": []}, {"text": "Even for German, a rather resource-rich language, there is alack of large-coverage derivational knowledge.", "labels": [], "entities": []}, {"text": "This paper describes a rule-based framework for inducing derivational families (i.e., clusters of lemmas in derivational relationships) and its application to create a high-coverage German resource, DERIVBASE, mapping over 280k lemmas into more than 17k non-singleton clusters.", "labels": [], "entities": [{"text": "DERIVBASE", "start_pos": 199, "end_pos": 208, "type": "DATASET", "confidence": 0.7431345582008362}]}, {"text": "We focus on the rule component and a qualitative and quantitative evaluation.", "labels": [], "entities": []}, {"text": "Our approach achieves up to 93% precision and 71% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9997158646583557}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9994229078292847}]}, {"text": "We attribute the high precision to the fact that our rules are based on information from grammar books.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9995473027229309}]}], "introductionContent": [{"text": "Morphological processing is generally recognized as an important step for many NLP tasks.", "labels": [], "entities": [{"text": "Morphological processing", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9449920356273651}, {"text": "NLP tasks", "start_pos": 79, "end_pos": 88, "type": "TASK", "confidence": 0.8881159722805023}]}, {"text": "Morphological analyzers such as lemmatizers and part of speech (POS) taggers are commonly the first NLP tools developed for any language.", "labels": [], "entities": [{"text": "part of speech (POS) taggers", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.631603160074779}]}, {"text": "They are also applied in NLP applications where little other linguistic analysis is performed, such as linguistic annotation of corpora or terminology acquisition; see for an informative summary.", "labels": [], "entities": [{"text": "terminology acquisition", "start_pos": 139, "end_pos": 162, "type": "TASK", "confidence": 0.7957309782505035}]}, {"text": "Most work on computational morphology has focused on inflectional morphology, that is, the handling of grammatically determined variation of form (), which can be understood, overimplifying somewhat, as a normalization step.", "labels": [], "entities": [{"text": "handling of grammatically determined variation of form", "start_pos": 91, "end_pos": 145, "type": "TASK", "confidence": 0.7793018903051104}]}, {"text": "Derivational morphology, which is concerned with the formation of new words from existing ones, has received less attention.", "labels": [], "entities": [{"text": "Derivational morphology", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9256525337696075}]}, {"text": "Examples are nominalization (to understand \u2192 the understanding), verbalization (the shelf \u2192 to shelve), and adjectivization (the size \u2192 sizable).", "labels": [], "entities": []}, {"text": "Part of the reason for the relative lack of attention lies in the morphological properties of English, such as the presence of many zero derivations (the fish \u2192 to fish), the dominance of suffixation, and the relative absence of stem changes in derivation.", "labels": [], "entities": []}, {"text": "For these reasons, simple stemming algorithms provide a cheap and accurate approximation to English derivation.", "labels": [], "entities": []}, {"text": "Two major NLP resources deal with derivation.", "labels": [], "entities": [{"text": "derivation", "start_pos": 34, "end_pos": 44, "type": "TASK", "confidence": 0.9743775129318237}]}, {"text": "WordNet lists so-called \"morphosemantic\" relations for English, and a number of proposals exist for extending WordNets in other languages with derivational relations.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9631803035736084}]}, {"text": "CatVar, the \"Categorial Variation Database of English\", is a lexicon aimed specifically at derivation.", "labels": [], "entities": [{"text": "CatVar", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9095093011856079}]}, {"text": "It groups English nouns, verbs, adjectives, and adverbs into derivational equivalence classes or derivational families such as ask V asker N asking N asking A Derivational families are commonly understood as groups of derivationally related lemmas ().", "labels": [], "entities": []}, {"text": "The lemmas in CatVar come from various open word classes, and multiple words maybe listed for the same POS.", "labels": [], "entities": []}, {"text": "The above family lists two nouns: an event noun (asking) and an agentive noun (asker).", "labels": [], "entities": []}, {"text": "However, CatVar does not consider prefixation, which is why, e.g., the adjective unasked is missing.", "labels": [], "entities": []}, {"text": "CatVar has found application in different areas of English NLP.", "labels": [], "entities": [{"text": "CatVar", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9150055646896362}]}, {"text": "Examples are the acquisition of paraphrases that cut across POS lines, applied, for example, in textual entailment.", "labels": [], "entities": []}, {"text": "Then there is the induction and extension of semantic roles resources for predicates of various parts of speech).", "labels": [], "entities": []}, {"text": "Finally, CatVar has been used as a lexical resource to generate sentence intersections (.", "labels": [], "entities": []}, {"text": "In this paper, we describe the project of obtaining derivational knowledge for German to enable similar applications.", "labels": [], "entities": []}, {"text": "Even though there are two derivational resources for this language, IMSLEX) and CELEX (, both have shortcomings.", "labels": [], "entities": [{"text": "IMSLEX", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.6497166156768799}, {"text": "CELEX", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.842775821685791}]}, {"text": "The former does not appear to be publicly available, and the latter has a limited coverage (50k lemmas) and does not explicitly represent derivational relationships within families, which are necessary for fine-grained optimization of families.", "labels": [], "entities": []}, {"text": "For this reason, we look into building a novel derivational resource for German.", "labels": [], "entities": []}, {"text": "Unfortuantely, the approach used to build CatVar cannot be adopted: it builds on a collection of high-quality lexical-semantic resources such as NOMLEX (), which are not available for German.", "labels": [], "entities": [{"text": "NOMLEX", "start_pos": 145, "end_pos": 151, "type": "DATASET", "confidence": 0.9188722372055054}]}, {"text": "Instead, we employ a rule-based framework to define derivation rules that cover both suffixation and prefixation and describes stem changes.", "labels": [], "entities": []}, {"text": "Following the work of, we define the derivational processes using derivational rules and higher-order string transformation functions.", "labels": [], "entities": []}, {"text": "The derivational rules induce a partition of the language's lemmas into derivational families.", "labels": [], "entities": []}, {"text": "Our method is applicable to many languages if the following are available: (1) a comprehensive set of lemmas (optionally including gender information); (2) knowledge about admissible derivational patterns, which can be gathered, for example, from linguistics textbooks.", "labels": [], "entities": []}, {"text": "The result is a freely available high-precision high-coverage resource for German derivational morphology that has a structure parallel to CatVar, but was obtained without using manually constructed lexical-semantic resources.", "labels": [], "entities": [{"text": "CatVar", "start_pos": 139, "end_pos": 145, "type": "DATASET", "confidence": 0.9295719861984253}]}, {"text": "We conduct a thorough evaluation of the induced derivational families both regarding precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9995039701461792}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9981023669242859}]}, {"text": "Section 2 discusses prior work.", "labels": [], "entities": []}, {"text": "Section 3 defines our derivation model that is applied to German in Section 4.", "labels": [], "entities": []}, {"text": "Sections 5 and 6 present our evaluation setup and results.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper and outlines future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The induction of derivational families could be evaluated globally as a clustering problem.", "labels": [], "entities": []}, {"text": "Unfortunately, cluster evaluation is a non-trivial task for which there is no consensus on the best approach (.", "labels": [], "entities": [{"text": "cluster evaluation", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.9398589730262756}]}, {"text": "We decided to perform our evaluation at the level of pairs: we manually judge fora set of pairs whether they are derivationally related or not.", "labels": [], "entities": []}, {"text": "We obtain the gold standard for this evaluation by sampling lemmas from the lemma list.", "labels": [], "entities": []}, {"text": "With random sampling, the evaluation would be unrealistic because avast majority of pairs would be derivationally unrelated and count as true negatives in our analysis.", "labels": [], "entities": []}, {"text": "Moreover, in order to reliably estimate the overall precision of the obtained derivational families, we need to evaluate on pairs sampled from these families.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9984003901481628}]}, {"text": "On the other hand, in order to assess recall, we need to sample from pairs that are not included in our derivational families.", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9961051940917969}]}, {"text": "To obtain reliable estimates of both precision and recall, we decided to draw two different samples: (1) a sample of lemma pairs sampled from the induced derivational families, on which we estimate precision (P-sample) and (2) a sample of lemma pairs sampled from the set of possibly derivationally related lemma pairs, on which we estimate recall (R-sample).", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9991770386695862}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9981775283813477}, {"text": "precision", "start_pos": 198, "end_pos": 207, "type": "METRIC", "confidence": 0.9984042048454285}, {"text": "recall", "start_pos": 341, "end_pos": 347, "type": "METRIC", "confidence": 0.9989006519317627}]}, {"text": "In both cases, pairs (l 1 , l 2 ) are sampled in two steps: first a lemma l 1 is drawn from a non-singleton family, then the second lemma l 2 is drawn from the derivational family of l 1 (P-sample) or the set of lemmas possibly related to l 1 (R-sample).", "labels": [], "entities": []}, {"text": "The set of possibly related lemmas is a union of the derivational family of l 1 , the clusters of l 1 obtained with the baseline methods, and k lemmas most similar to l 1 according to the two string distance measures.", "labels": [], "entities": []}, {"text": "We use k = 7 in our experiments.", "labels": [], "entities": []}, {"text": "This is based on preliminary experiments on the development set (cf. Section 4.1), which showed that k = 7 retrieves about 92% of the related lemmas retrieved fork = 20 with a much smaller number of true negatives.", "labels": [], "entities": []}, {"text": "Thus, the evaluation on the R-sample might overestimate the recall, but only slightly so, while the P-sample yields a reliable estimate of precision by reducing the number of true negatives in the sample.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9996929168701172}, {"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9994049072265625}]}, {"text": "Both samples contain 2400 lemma pairs each.", "labels": [], "entities": []}, {"text": "Lemmas included in the development set (Section 4.1) were excluded from sampling.", "labels": [], "entities": []}, {"text": "We measure the precision of our method on the P-sample and recall on the R-sample.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.999665379524231}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9996429681777954}]}, {"text": "For the baselines, precision was also computed on the R-sample (computing it on P-sample, which is obtained from the induced derivational families, would severely underestimate the number of false positives).", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9996065497398376}]}, {"text": "We omit the F 1 score because its use for precision and recall estimates from different samples is unclear.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9883584976196289}, {"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9991468191146851}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9967064261436462}]}, {"text": "DERIVBASE reaches 83% precision when using all rules and 93% precision when using only highly reliable rules.", "labels": [], "entities": [{"text": "DERIVBASE", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.46256887912750244}, {"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9988049268722534}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9992547631263733}]}, {"text": "DERIVBASE-L123 achieves the highest recall, outperforming other methods and variants by a large margin.", "labels": [], "entities": [{"text": "DERIVBASE-L123", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.5332753658294678}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9997310042381287}]}, {"text": "Refinement of the initial model has produced a significant improvement in recall without losses in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9995168447494507}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9985731840133667}]}, {"text": "The baselines perform worse than our method: the stemmer we use is rather conservative, which fragments the families and leads to a very low recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9979801774024963}]}, {"text": "The string distance-based approaches achieve more balanced precision and recall scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9983818531036377}, {"text": "recall scores", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9763260185718536}]}, {"text": "Note that for these methods, precision and recall can be traded off against each other by varying the number of clusters; we chose the number of clusters by optimizing the F 1 score on the calibration and validaton sets.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9993977546691895}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9991948008537292}, {"text": "F 1 score", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9676345785458883}]}, {"text": "All subsequent analyses refer to DERIVBASE-    L123, which is the model with the highest recall.", "labels": [], "entities": [{"text": "DERIVBASE-    L123", "start_pos": 33, "end_pos": 51, "type": "METRIC", "confidence": 0.8696466684341431}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9993064403533936}]}, {"text": "If optimal precision is required, DERIVBASE-L3 should however be preferred.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9992135763168335}, {"text": "DERIVBASE-L3", "start_pos": 34, "end_pos": 46, "type": "METRIC", "confidence": 0.9442679286003113}]}, {"text": "We cross-classified our rules according to high/low accuracy and high/low coverage based on the pairs in the P-sample.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9969351291656494}, {"text": "coverage", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9702959060668945}]}, {"text": "We only considered directly derivationally related (\u2192 D ) pairs and defined \"high accuracy\" and \"high coverage\" as all rules above the 25th percentile in terms of accuracy and coverage, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9871205687522888}, {"text": "coverage", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.955601692199707}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9992479681968689}, {"text": "coverage", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9946229457855225}]}, {"text": "The results are shown in: all high-coverage rules are also highly accurate.", "labels": [], "entities": [{"text": "accurate", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9619624614715576}]}, {"text": "Most rules are accurate but infrequent.", "labels": [], "entities": []}, {"text": "Only 21 rules have a low accuracy, but all of them apply infrequently.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.999427318572998}]}, {"text": "Analysis by parts of speech.", "labels": [], "entities": [{"text": "Analysis by parts of speech", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8235311508178711}]}, {"text": "shows precision and recall values for different part of speech combinations for the basis and derived words.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9993224143981934}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9979388117790222}]}, {"text": "High precision and recall are achieved for N-A derivations.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9996378421783447}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9997324347496033}]}, {"text": "The recall is lowest for V-V derivations, suggesting that the derivational phenomena for this POS combination are not yet covered satisfactorily.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9997221827507019}]}], "tableCaptions": [{"text": " Table 2: Breakdown of derivation rules by category  of the basis and the derived word", "labels": [], "entities": []}, {"text": " Table 3: Categories for lemma pair classification", "labels": [], "entities": [{"text": "lemma pair classification", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.7535313069820404}]}, {"text": " Table 4: Inter-annotator agreement on validation  sample", "labels": [], "entities": []}, {"text": " Table 5: Precision and recall on test samples", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9943506717681885}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.998728334903717}]}, {"text": " Table 7: Precision and recall across different part  of speech (first POS: basis; second POS: derived  word)", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9587454795837402}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9901136159896851}]}, {"text": " Table 6: all high-coverage  rules are also highly accurate. Most rules are ac- curate but infrequent. Only 21 rules have a low  accuracy, but all of them apply infrequently.", "labels": [], "entities": [{"text": "accurate", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9781191349029541}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9967443943023682}]}]}