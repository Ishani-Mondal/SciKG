{"title": [{"text": "Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages", "labels": [], "entities": []}], "abstractContent": [{"text": "Developing natural language processing tools for low-resource languages often requires creating resources from scratch.", "labels": [], "entities": []}, {"text": "While a variety of semi-supervised methods exist for training from incomplete data, there are open questions regarding what types of training data should be used and how much is necessary.", "labels": [], "entities": []}, {"text": "We discuss a series of experiments designed to shed light on such questions in the context of part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.7886583507061005}]}, {"text": "We obtain timed annotations from linguists for the low-resource languages Kinyarwanda and Malagasy (as well as English) and evaluate how the amounts of various kinds of data affect performance of a trained POS-tagger.", "labels": [], "entities": []}, {"text": "Our results show that annotation of word types is the most important , provided a sufficiently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus.", "labels": [], "entities": []}, {"text": "We also show that finite-state morphological analyzers are effective sources of type information when few labeled examples are available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Low-resource languages present a particularly difficult challenge for natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 70, "end_pos": 103, "type": "TASK", "confidence": 0.7142409235239029}]}, {"text": "For example, supervised learning methods can provide high accuracy for part-of-speech (POS) tagging), but they perform poorly when little supervision is available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9983300566673279}, {"text": "part-of-speech (POS) tagging", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.6059287488460541}]}, {"text": "Good results in weakly-supervised tagging have been obtained by training sequence models such as hidden Markov models (HMM) using the Expectation-Maximization algorithm (EM), however most work in this area has still relied on relatively large amounts of data, both annotated and unannotated, as well as an assumption that the annotations are very clean.", "labels": [], "entities": []}, {"text": "The ability to learn taggers using very little data is enticing: only a tiny fraction of the world's languages have enough data for standard supervised models to work well.", "labels": [], "entities": []}, {"text": "The collection or development of resources is a time-consuming and expensive process, creating a significant barrier for an under-studied language where there are few experts and little funding.", "labels": [], "entities": []}, {"text": "It is thus important to develop approaches that achieve good accuracy based on the amount of data that can be reasonably obtained, for example, in just a few hours by a linguist doing fieldwork on a non-native language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9979040622711182}]}, {"text": "Previous work explored learning taggers from weak information, but the type, amount, quality, and sources of data raise questions about the applicability of those results to real-world low-resource scenarios).", "labels": [], "entities": [{"text": "learning taggers from weak information", "start_pos": 23, "end_pos": 61, "type": "TASK", "confidence": 0.80265371799469}]}, {"text": "Most research simulated weak supervision with tag dictionaries extracted from existing large, expertly-annotated corpora.", "labels": [], "entities": []}, {"text": "These resources have been developed overlong periods of time by trained annotators who collaborate to produce high-quality analyses.", "labels": [], "entities": []}, {"text": "They are also biased towards including only the most likely tag for each word type, resulting in a cleaner dictionary than one would find in areal scenario.", "labels": [], "entities": []}, {"text": "As such, these experiments do not reflect real-world constraints.", "labels": [], "entities": []}, {"text": "One exception to this work is: they use a manually-constructed lexicon for Hebrew in order to learn an HMM tagger.", "labels": [], "entities": []}, {"text": "However, this lexicon was constructed by trained lexicographers over along period of time and achieves very high coverage of the language with very good quality, much better than could be achieved by our non-expert linguistics graduate student annotators in just a few hours.", "labels": [], "entities": []}, {"text": "learn a POS-tagger from existing linguistic resources, namely a dictionary and a reference grammar, but these resources are not available, much less digitized, for most under-studied languages.) develop a model in which a POS-tagger is learned from a list of POS tags and just three \"prototype\" word types for each tag, but their approach requires a vector space to compute the distributional similarity between prototypes and other word types in the corpus.", "labels": [], "entities": []}, {"text": "Such distributional models are not feasible for low-resource languages because they require immense amounts of raw text, much more than is available in these settings (.", "labels": [], "entities": []}, {"text": "Further, they extracted their prototype lists directly from a labeled corpus, something we are specifically avoiding.", "labels": [], "entities": []}, {"text": "evaluate the use of mixed type and token constraints generated by projecting information from a highresource language to a low-resource language via a parallel corpus.", "labels": [], "entities": []}, {"text": "However, large parallel corpora are not available for most low-resource languages.", "labels": [], "entities": []}, {"text": "These are also expensive resources to create and would take considerably more effort to produce than the monolingual resources that our annotators were able to generate in a four-hour timeframe.", "labels": [], "entities": []}, {"text": "Of course, if they are available, such parallel text links could be incorporated into our approach.", "labels": [], "entities": []}, {"text": "In our previous work, we developed a different strategy based on generalizing linguistic input with a computational model: linguists annotated either types or tokens for two hours, these annotations are projected onto a corpus of unlabeled tokens using label propagation and HMMs, and a final POS-tagger is trained on this larger autolabeled corpus (.", "labels": [], "entities": []}, {"text": "That approach uses much more realistic types and quantities of resources than previous work; nonetheless, it leaves many open questions regarding the effectiveness of incrementally more annotation, the role of unannotated data, and whether there is a good balance to be found using a combination of type-and token-supervision.", "labels": [], "entities": []}, {"text": "We also did not consider morphological analyzers as a form of type supervision, as suggested by.", "labels": [], "entities": [{"text": "morphological analyzers", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.662395790219307}, {"text": "type supervision", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.9045223295688629}]}, {"text": "This paper addresses these questions via a series of experiments designed to quantify the effect on performance given by the amount of time spent finding or annotating training materials.", "labels": [], "entities": []}, {"text": "We specifically look at the impact of four types of data collection: 1.", "labels": [], "entities": []}, {"text": "Time annotating sentences (token supervision) 2.", "labels": [], "entities": [{"text": "Time annotating sentences", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7241785625616709}]}, {"text": "Time creating tag dictionary (type supervision) 3.", "labels": [], "entities": []}, {"text": "Time constructing a finite state transducer (FST) to analyze word-type morphology 4.", "labels": [], "entities": []}, {"text": "Amount of raw data available for training We explore these strategies in the context of POStagging for Kinyarwanda and Malagasy.", "labels": [], "entities": [{"text": "POStagging", "start_pos": 88, "end_pos": 98, "type": "TASK", "confidence": 0.7605222463607788}]}, {"text": "We also include experiments for English, pretending as though it is a low-resource language.", "labels": [], "entities": []}, {"text": "The overwhelming takeaway from our results is that type supervision-when backed by an effective semisupervised learning approach-is the most important source of linguistic information.", "labels": [], "entities": []}, {"text": "Also, morphological analyzers help for morphologically rich languages when there are few labeled types or tokens (and, it never hurts to use them).", "labels": [], "entities": []}, {"text": "Finally, performance improves with more raw data, though we see diminishing returns past 400,000 tokens.", "labels": [], "entities": []}, {"text": "With just four hours of type annotation, our system obtains good accuracy across the three languages: 89.8% on English, 81.9% on Kinyarwanda, and 81.2% on Malagasy.", "labels": [], "entities": [{"text": "type annotation", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.8437024056911469}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9991554021835327}]}, {"text": "Our results compare favorably with previous work despite using considerably less supervision and a more difficult set of tags.", "labels": [], "entities": []}, {"text": "For example, use the entirety of English Wiktionary directly as a tag dictionary to obtain 87.1% accuracy on English, below our result.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9969972372055054}]}, {"text": "average 88.8% across 8 major languages, but for Turkish, a morphologically rich language, they achieve only 65.2%, significantly below our 81.9% for morphologically-rich Kinyarwanda.", "labels": [], "entities": []}], "datasetContent": [{"text": "To better understand the effect that each type of supervision has on tagger accuracy, we perform a series of experiments, with KIN and MLG as true low-resource languages.", "labels": [], "entities": [{"text": "tagger", "start_pos": 69, "end_pos": 75, "type": "TASK", "confidence": 0.9630425572395325}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.8966414332389832}]}, {"text": "English experiments, for which we had both experienced and novice annotators, allow for further exploration into issues concerning data collection and preparation.", "labels": [], "entities": [{"text": "data collection and preparation", "start_pos": 131, "end_pos": 162, "type": "TASK", "confidence": 0.7373163998126984}]}, {"text": "The overall best accuracies achieved by language are 81.9% for KIN using all types, 81.2% for MLG using half types and half tokens, and 89.8% for ENG using all types and the maximal amount of raw data.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9962499737739563}]}, {"text": "All of these best values were achieved using both FST and affix LP features.", "labels": [], "entities": [{"text": "FST", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.7328168153762817}]}, {"text": "All results described in this section are averaged over five folds of raw data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Annotations for each language and annotator as time increases. Shows the number of tag  dictionary entries from type annotation vs. token. (The count of labeled tokens is shown in parentheses).  For brevity, the table only shows hourly progress.", "labels": [], "entities": []}, {"text": " Table 3: Coverage of the English morphological  FST during development. For brevity, showing 2- hour increments instead of 30-minute segments.", "labels": [], "entities": [{"text": "English morphological  FST", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.5524842242399851}]}, {"text": " Table 4: Coverage and ambiguity of the final FST  for each language.", "labels": [], "entities": [{"text": "FST", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.5424753427505493}]}]}