{"title": [{"text": "Building Japanese Textual Entailment Specialized Data Sets for Inference of Basic Sentence Relations", "labels": [], "entities": [{"text": "Inference of Basic Sentence Relations", "start_pos": 63, "end_pos": 100, "type": "TASK", "confidence": 0.8619227766990661}]}], "abstractContent": [{"text": "This paper proposes a methodology for generating specialized Japanese data sets for textual entailment, which consists of pairs decomposed into basic sentence relations.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7313560843467712}]}, {"text": "We experimented with our methodology over a number of pairs taken from the RITE-2 data set.", "labels": [], "entities": [{"text": "RITE-2 data set", "start_pos": 75, "end_pos": 90, "type": "DATASET", "confidence": 0.9463589986165365}]}, {"text": "We compared our methodology with existing studies in terms of agreement, frequencies and times, and we evaluated its validity by investigating recognition accuracy.", "labels": [], "entities": [{"text": "agreement", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9557643532752991}, {"text": "validity", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9670286774635315}, {"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.8803247213363647}]}], "introductionContent": [{"text": "In recognizing textual entailment (RTE), automated systems assess whether a human reader would consider that, given a snippet of text t1 and some unspecified (but restricted) world knowledge, a second snippet of text t2 is true.", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE)", "start_pos": 3, "end_pos": 39, "type": "TASK", "confidence": 0.8927784164746603}]}, {"text": "An example is given below.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the RITE-2 formal run 4 , 15 teams used our specialized data set for the evaluation of their systems.", "labels": [], "entities": [{"text": "RITE-2 formal run", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.6363232433795929}]}, {"text": "shows the average of F 1 scores 5 for each BSR.", "labels": [], "entities": [{"text": "F 1", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9928599298000336}, {"text": "BSR", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.5436872839927673}]}, {"text": "Scrambling and Modifier yielded high scores (close to 90%).", "labels": [], "entities": []}, {"text": "The score of List was also    Inference, Spatial and Apposition yielded low scores (less than 50%).", "labels": [], "entities": [{"text": "Inference", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9204038977622986}, {"text": "Apposition", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9988081455230713}]}, {"text": "The scores of Disagreement: lexical, Nominalization and Disagreement: Meronymy were about 50-70%.", "labels": [], "entities": []}, {"text": "BSRs that yielded scores of less than 70% occurred less than 3 times, and those that yielded scores of not more than 70% occurred 3 times or more, except for Temporal and Transparent head.", "labels": [], "entities": []}, {"text": "Therefore, the frequencies of BSRs are related to F 1 scores, and we should consider how to build systems that recognize infrequent BSRs accurately.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9681970477104187}]}, {"text": "In addition, F 1 scores in Synonymy: phrasal and Entailment: phrasal are low, although these are labeled frequently.", "labels": [], "entities": [{"text": "F 1", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9911159873008728}, {"text": "Synonymy: phrasal", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.6643436352411906}]}, {"text": "This is one possible direction of future work.", "labels": [], "entities": []}, {"text": "also shows the number of pairs in BSR to which the two annotators assigned different labels.", "labels": [], "entities": [{"text": "BSR", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.7152379751205444}]}, {"text": "For example, one annotator labeled t2 while the other labeled t2 in the following pair:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Distribution of monothematic pairs with  respect to original Y/N pairs", "labels": [], "entities": []}, {"text": " Table 6: Distribution of BSRs in t1-t2 pairs in an  existing study and in the present study using our  methodology", "labels": [], "entities": [{"text": "Distribution of BSRs", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.5632627904415131}]}, {"text": " Table 7: Average F 1 scores in BSR and frequen- cies of misclassifications by annotators", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9371611277262369}, {"text": "BSR", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.6189624071121216}]}]}