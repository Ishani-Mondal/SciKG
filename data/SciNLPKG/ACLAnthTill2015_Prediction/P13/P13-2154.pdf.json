{"title": [{"text": "A New Set of Norms for Semantic Relatedness Measures", "labels": [], "entities": [{"text": "Semantic Relatedness Measures", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.8453525503476461}]}], "abstractContent": [{"text": "We have elicited human quantitative judgments of semantic relatedness for 122 pairs of nouns and compiled them into anew set of relatedness norms that we call Rel-122.", "labels": [], "entities": [{"text": "Rel-122", "start_pos": 159, "end_pos": 166, "type": "DATASET", "confidence": 0.9051889777183533}]}, {"text": "Judgments from individual subjects in our study exhibit high average correlation to the resulting relatedness means (r = 0.77, \u03c3 = 0.09, N = 73), although not as high as Resnik's (1995) upper bound for expected average human correlation to similarity means (r = 0.90).", "labels": [], "entities": []}, {"text": "This suggests that human perceptions of relatedness are less strictly constrained than perceptions of similarity and establishes a clearer expectation for what constitutes human-like performance by a computational measure of semantic relatedness.", "labels": [], "entities": []}, {"text": "We compare the results of several WordNet-based similarity and relatedness measures to our Rel-122 norms and demonstrate the limitations of WordNet for discovering general indications of semantic relatedness.", "labels": [], "entities": [{"text": "Rel-122 norms", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.9289614260196686}, {"text": "WordNet", "start_pos": 140, "end_pos": 147, "type": "DATASET", "confidence": 0.9394312500953674}]}, {"text": "We also offer a critique of the field's reliance upon similarity norms to evaluate relatedness measures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite the well-established technical distinction between semantic similarity and relatedness, comparison to established similarity norms from psychology remains part of the standard evaluative procedure for assessing computational measures of semantic relatedness.", "labels": [], "entities": []}, {"text": "Because similarity is only one particular type of relatedness, comparison to similarity norms fails to give a complete view of a relatedness measure's efficacy.", "labels": [], "entities": []}, {"text": "In keeping with observation that \"comparison with human judgments is the ideal way to evaluate a measure of similarity or relatedness,\" we have undertaken the creation of anew set of relatedness norms.", "labels": [], "entities": []}], "datasetContent": [{"text": "Each participant in our study was randomly assigned to one of four conditions.", "labels": [], "entities": []}, {"text": "Each condition contained 32 noun pairs for evaluation.", "labels": [], "entities": []}, {"text": "Of those pairs, 10 were randomly selected from from WordNet++ (Ponzetto and Navigli, 2010) and 10 from SGN ()-two semantic networks that categorically indicate strong relatedness between WordNet noun senses.", "labels": [], "entities": []}, {"text": "10 additional pairs were generated by randomly pairing words from a list of all nouns occurring in Wikipedia.", "labels": [], "entities": []}, {"text": "The nouns in the pairs we used from each of these three sources were matched for frequency of occurrence in Wikipedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 108, "end_pos": 117, "type": "DATASET", "confidence": 0.9267954230308533}]}, {"text": "We manually selected two additional pairs that appeared across all four conditions: leaves-rake and lion-cage.", "labels": [], "entities": []}, {"text": "These control pairs were included to ensure that each condition contained examples of strong semantic relatedness, and potentially to help identify and eliminate data from participants who assigned random relatedness scores.", "labels": [], "entities": []}, {"text": "Within each condition, the 32 word pairs were presented to all subjects in the same random order.", "labels": [], "entities": []}, {"text": "Across conditions, the two control pairs were always presented in the same positions in the word pair grid.", "labels": [], "entities": []}, {"text": "Each word pair was subjected to additional scrutiny before being included in our dataset.", "labels": [], "entities": []}, {"text": "We eliminated any pairs falling into one or more of the following categories: (a) pairs containing proper nouns, (b) pairs in which one or both nouns might easily be mistaken for adjectives or verbs, (c) pairs with advanced vocabulary or words that might require domain-specific knowledge in order to be properly evaluated, and (d) pairs with shared stems or common head nouns (e.g., first cousin-second cousin and sinner-sinning).", "labels": [], "entities": []}, {"text": "The latter were eliminated to prevent subjects from latching onto superficial lexical commonalities as indicators of strong semantic relatedness without reflecting upon meaning.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Excerpt of Rel-122 norms.", "labels": [], "entities": [{"text": "Rel-122 norms", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.8911576867103577}]}, {"text": " Table 2: Correlation of similarity and relatedness measures to Rel-122, M&C, and R&G. Starred rows  (*) are considered relatedness measures. All measures are WordNet-based, except for the scoring metric  of Szumlanski and Gomez (2010), which is based on lexical co-occurrence frequency in Wikipedia.", "labels": [], "entities": [{"text": "Rel-122", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9247915744781494}, {"text": "WordNet-based", "start_pos": 159, "end_pos": 172, "type": "DATASET", "confidence": 0.9464511871337891}]}, {"text": " Table 3: Comparison of relatedness means to M&C similarity means. Correlation is r = 0.91.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 67, "end_pos": 78, "type": "METRIC", "confidence": 0.9904006123542786}]}]}