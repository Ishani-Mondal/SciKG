{"title": [{"text": "Variational Inference for Structured NLP Models", "labels": [], "entities": [{"text": "Variational Inference", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8109415173530579}]}], "abstractContent": [{"text": "Description Historically, key breakthroughs in structured NLP models, such as chain CRFs or PCFGs, have relied on imposing careful constraints on the locality of features in order to permit efficient dynamic programming for computing expectations or finding the highest-scoring structures.", "labels": [], "entities": []}, {"text": "However, as modern structured models become more complex and seek to incorporate longer-range features, it is more and more often the case that performing exact inference is impossible (or at least impractical) and it is necessary to resort to some sort of approximation technique, such as beam search, pruning, or sampling.", "labels": [], "entities": [{"text": "beam search", "start_pos": 290, "end_pos": 301, "type": "TASK", "confidence": 0.8750740885734558}]}, {"text": "In the NLP community, one increasingly popular approach is the use of variational methods for computing approximate distributions.", "labels": [], "entities": []}, {"text": "The goal of the tutorial is to provide an introduction to variational methods for approximate inference , particularly mean field approximation and belief propagation.", "labels": [], "entities": [{"text": "mean field approximation", "start_pos": 119, "end_pos": 143, "type": "TASK", "confidence": 0.6438001394271851}, {"text": "belief propagation", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.7747859954833984}]}, {"text": "The intuition behind the mathematical derivation of variational methods is fairly simple: instead of trying to directly compute the distribution of interest, first consider some efficiently computable approximation of the original inference problem, then find the solution of the approximate inference problem that minimizes the distance to the true distribution.", "labels": [], "entities": []}, {"text": "Though the full derivations can be somewhat tedious, the resulting procedures are quite straightforward, and typically consist of an iterative process of individually updating specific components of the model, conditioned on the rest.", "labels": [], "entities": []}, {"text": "Although we will provide some theoretical background, the main goal of the tu-torial is to provide a concrete procedural guide to using these approximate inference techniques, illustrated with detailed walkthroughs of examples from recent NLP literature.", "labels": [], "entities": []}, {"text": "Once both variational inference procedures have been described in detail, we'll provide a summary comparison of the two, along with some intuition about which approach is appropriate when.", "labels": [], "entities": []}, {"text": "We'll also provide a guide to further exploration of the topic, briefly discussing other variational techniques , such as expectation propagation and convex relaxations, but concentrating mainly on providing pointers to additional resources for those who wish to learn more.", "labels": [], "entities": [{"text": "expectation propagation", "start_pos": 122, "end_pos": 145, "type": "TASK", "confidence": 0.742011085152626}]}, {"text": "Structured Models and Factor Graphs \u2022 Factor graph notation \u2022 Example structured NLP models \u2022 Inference 2.", "labels": [], "entities": []}, {"text": "Mean Field \u2022 Warmup (iterated conditional modes) \u2022 Mean field procedure \u2022 Derivation of mean field update \u2022 Example 3.", "labels": [], "entities": []}, {"text": "Structured Mean Field \u2022 Structured approximation \u2022 Computing structured updates \u2022 Example: Joint parsing and alignment 4.", "labels": [], "entities": [{"text": "Structured approximation", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.9439697861671448}, {"text": "Joint parsing", "start_pos": 91, "end_pos": 104, "type": "TASK", "confidence": 0.7617141306400299}, {"text": "alignment", "start_pos": 109, "end_pos": 118, "type": "TASK", "confidence": 0.9418063759803772}]}, {"text": "Belief Propagation \u2022 Intro \u2022 Messages and beliefs \u2022 Loopy BP 5.", "labels": [], "entities": [{"text": "Loopy BP 5", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.8395079374313354}]}, {"text": "Structured Belief Propagation \u2022 Warmup (efficient products for messages) \u2022 Example: Word alignment \u2022 Example: Dependency parsing 6.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.727696418762207}, {"text": "Dependency parsing", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.7758236527442932}]}, {"text": "Wrap-Up \u2022 Mean field vs BP \u2022 Other approximation techniques 9", "labels": [], "entities": [{"text": "BP", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9379652738571167}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}