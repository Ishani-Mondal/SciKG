{"title": [{"text": "Word Alignment Modeling with Context Dependent Deep Neural Network", "labels": [], "entities": [{"text": "Word Alignment Modeling", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7849022050698599}]}], "abstractContent": [{"text": "In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.6830573678016663}]}, {"text": "We describe in detail how we adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method introduced in speech recognition to the HMM-based word alignment model, in which bilingual word embedding is discrimina-tively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7360421270132065}, {"text": "HMM-based word alignment", "start_pos": 128, "end_pos": 152, "type": "TASK", "confidence": 0.7462536692619324}]}, {"text": "While being capable to model the rich bilingual correspondence , our method generates a very compact model with much fewer parameters.", "labels": [], "entities": []}, {"text": "Experiments on a large scale English-Chinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.", "labels": [], "entities": [{"text": "word alignment task", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7634520133336385}, {"text": "HMM", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.8723546862602234}, {"text": "IBM model 4 baselines", "start_pos": 119, "end_pos": 140, "type": "DATASET", "confidence": 0.8248093873262405}, {"text": "F-score", "start_pos": 156, "end_pos": 163, "type": "METRIC", "confidence": 0.9937570095062256}]}], "introductionContent": [{"text": "Recent years research communities have seen a strong resurgent interest in modeling with deep (multi-layer) neural networks.", "labels": [], "entities": []}, {"text": "This trending topic, usually referred under the name Deep Learning, is started by ground-breaking papers such as), in which innovative training procedures of deep structures are proposed.", "labels": [], "entities": []}, {"text": "Unlike shallow learning methods, such as Support Vector Machine, Conditional Random Fields, and Maximum Entropy, which need hand-craft features as input, DNN can learn suitable features (representations) automatically with raw input data, given a training objective.", "labels": [], "entities": []}, {"text": "DNN did not achieve expected success until 2006, when researchers discovered a proper way to intialize and train the deep architectures, which contains two phases: layer-wise unsupervised pretraining and supervised fine tuning.", "labels": [], "entities": [{"text": "DNN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8353770971298218}]}, {"text": "For pretraining, Restricted Boltzmann Machine (RBM) (), auto-encoding ( and sparse coding () are proposed and popularly used.", "labels": [], "entities": []}, {"text": "The unsupervised pretraining trains the network one layer at a time, and helps to guide the parameters of the layer towards better regions in parameter space.", "labels": [], "entities": []}, {"text": "Followed by fine tuning in this region, DNN is shown to be able to achieve state-of-the-art performance in various area, or even better () (.", "labels": [], "entities": []}, {"text": "DNN also achieved breakthrough results on the ImageNet dataset for objective recognition ().", "labels": [], "entities": [{"text": "DNN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9684017896652222}, {"text": "ImageNet dataset", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9339475631713867}, {"text": "objective recognition", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.6948150247335434}]}, {"text": "For speech recognition, ( proposed context-dependent neural network with large vocabulary, which achieved 16.0% relative error reduction.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8393757045269012}, {"text": "relative error reduction", "start_pos": 112, "end_pos": 136, "type": "METRIC", "confidence": 0.7870452801386515}]}, {"text": "DNN has also been applied in Natural Language Processing (NLP) field.", "labels": [], "entities": []}, {"text": "Most works convert atomic lexical entries into a dense, low dimensional, real-valued representation, called word embedding; Each dimension represents a latent aspect of a word, capturing its semantic and syntactic properties ().", "labels": [], "entities": []}, {"text": "Word embedding is usually first learned from huge amount of monolingual texts, and then fine-tuned with taskspecific objectives. and) further apply Recursive Neural Networks to address the structural prediction tasks such as tagging and parsing, and) explores the compositional aspect of word representations.", "labels": [], "entities": [{"text": "tagging and parsing", "start_pos": 225, "end_pos": 244, "type": "TASK", "confidence": 0.5903643369674683}]}, {"text": "Inspired by successful previous works, we propose anew DNN-based word alignment method, which exploits contextual and semantic similarities between words.", "labels": [], "entities": [{"text": "DNN-based word alignment", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.6352918148040771}]}, {"text": "As shown in example (a) of, in word pair {\"juda\" \u21d2\"mammoth\"}, the Chinese word \"juda\" is a common word, but the English word \"mammoth\" is not, so it is very hard to align them correctly.", "labels": [], "entities": []}, {"text": "If we know that \"mammoth\" has the similar meaning with \"big\", or \"huge\", it would be easier to find the corresponding word in the Chinese sentence.", "labels": [], "entities": []}, {"text": "As we mentioned in the last paragraph, word embedding (trained with huge monolingual texts) has the ability to map a word into a vector space, in which, similar words are near each other.", "labels": [], "entities": []}, {"text": "For example (b) in, for the word pair {\"yibula\" \u21d2 \"Yibula\"}, both the Chinese word \"yibula\" and English word \"Yibula\" are rare name entities, but the words around them are very common, which are {\"nongmin\", \"shuo\"} for Chinese side and {\"farmer\", \"said\"} for the English side.", "labels": [], "entities": []}, {"text": "The pattern of the context {\"nongmin X shuo\" \u21d2 \"farmer X said\"} may help to align the word pair which fill the variable X, and also, the pattern {\"yixiang X gongcheng\" \u21d2 \"a X job\"} is helpful to align the word pair {\"juda\" \u21d2\"mammoth\"} for example (a).", "labels": [], "entities": []}, {"text": "Based on the above analysis, in this paper, both the words in the source and target sides are firstly mapped to a vector via a discriminatively trained word embeddings, and word pairs are scored by a multi-layer neural network which takes rich contexts (surrounding words on both source and target sides) into consideration; and a HMM-like distortion model is applied on top of the neural network to characterize structural aspect of bilingual sentences.", "labels": [], "entities": []}, {"text": "In the rest of this paper, related work about DNN and word alignment are first reviewed in Section 2, followed by a brief introduction of DNN in Section 3.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.8081845045089722}]}, {"text": "We then introduce the details of leveraging DNN for word alignment, including the details of our network structure in Section 4 and the training method in Section 5.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.7763896882534027}]}, {"text": "The merits of our approach are illustrated with the experiments described in Section 6, and we conclude our paper in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct our experiment on Chinese-to-English word alignment task.", "labels": [], "entities": [{"text": "Chinese-to-English word alignment", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.5778904060522715}]}, {"text": "We use the manually aligned Chinese-English alignment corpus () which contains 491 sentence pairs as test set.", "labels": [], "entities": []}, {"text": "We adapt the segmentation on the Chinese side to fit our word segmentation standard.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.6797824800014496}]}], "tableCaptions": []}