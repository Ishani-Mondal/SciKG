{"title": [], "abstractContent": [{"text": "As one of the most challenging issues in NLP, metaphor identification and its interpretation have seen many models and methods proposed.", "labels": [], "entities": [{"text": "metaphor identification", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.9256895184516907}]}, {"text": "This paper presents a study on metaphor identification based on the semantic similarity between literal and non literal meanings of words that can appear at the same context.", "labels": [], "entities": [{"text": "metaphor identification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.8668321073055267}]}], "introductionContent": [{"text": "A metaphor is a literary figure of speech that describes a subject by asserting that it is, on some point of comparison, the same as another otherwise unrelated object.", "labels": [], "entities": []}, {"text": "Metaphor is a type of analogy and is closely related to other rhetorical figures of speech that achieve their effects via association, comparison or resemblance including allegory, hyperbole, and simile.", "labels": [], "entities": []}, {"text": "Rhetorical theorists and other scholars of language have discussed numerous dimensions of metaphors, though these nomenclatures are by no means universal nor necessarily mutually exclusive.", "labels": [], "entities": []}, {"text": "Avery challenging task in linguistics is the metaphor identification and the its interpretation.", "labels": [], "entities": [{"text": "metaphor identification", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.8474652171134949}]}, {"text": "Metaphor identification procedure (MIP) is a method for identifying metaphorically used words in discourse.", "labels": [], "entities": [{"text": "Metaphor identification procedure (MIP)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8517729540665945}]}, {"text": "It can be used to recognize metaphors in spoken and written language.", "labels": [], "entities": [{"text": "recognize metaphors in spoken and written language", "start_pos": 18, "end_pos": 68, "type": "TASK", "confidence": 0.8367295946393695}]}, {"text": "The procedure aims to determine the relationship of a particular lexical unit in the discourse and recognize its use in a particular context as possibly metaphorical.", "labels": [], "entities": []}, {"text": "Since many words can be considered metaphorical in different contexts, MIP requires a clear distinction between words that convey metaphorical meaning and those that do not, despite the fact that language generally differs in the degrees of metaphoricity.", "labels": [], "entities": [{"text": "MIP", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.98707515001297}]}, {"text": "In this paper we propose a method for identifying metaphorical usage in verbs.", "labels": [], "entities": [{"text": "identifying metaphorical usage in verbs", "start_pos": 38, "end_pos": 77, "type": "TASK", "confidence": 0.904396939277649}]}, {"text": "Our method is looking for semantic analogies in the context of a verb by comparing it against prior known instances of literal and non-literal usage of the same verb in different contexts.", "labels": [], "entities": []}, {"text": "After discussing the metaphor identication literature (Section 2), we proceed to present our research proposal (Section 3) and to present and discuss our first experiments based on WordNet similarity measures (Section 4).", "labels": [], "entities": []}, {"text": "Experiment results help us to draw conclusions and insights about analogical reasoning and memory-based learning for this task and to outline promising research paths (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate our method we search for common English verbs which can take either literal or non literal predicates.", "labels": [], "entities": []}, {"text": "As the most common verbs (be, have and do) can function as verbs and auxiliary verbs, we didn't use them for our experiments.", "labels": [], "entities": []}, {"text": "As a consequence, we chose common function verbs which can take a direct object as predicate.", "labels": [], "entities": []}, {"text": "More specifically, at our experiments we concentrated on literal and non literal predicates of the verbs: break, catch, cut, draw, drop, find, get, hate, hear, hold, keep, kill, leave, listen, lose, love, make, pay, put, save, see, take, want.", "labels": [], "entities": [{"text": "literal and non literal predicates of the verbs: break, catch, cut, draw, drop, find, get, hate, hear, hold, keep, kill, leave, listen, lose, love, make, pay, put, save, see, take, want", "start_pos": 57, "end_pos": 242, "type": "Description", "confidence": 0.8407091443185453}]}, {"text": "We used the VU Amsterdam Metaphor Corpus 1 1 Please see http://www.metaphorlab.", "labels": [], "entities": [{"text": "VU Amsterdam Metaphor Corpus 1 1", "start_pos": 12, "end_pos": 44, "type": "DATASET", "confidence": 0.9404671887556711}]}, {"text": "vu.nl/en/research/funded_research/ in order to extract data for our experiments.", "labels": [], "entities": []}, {"text": "We used shallow heuristics to match verbs and direct objects, with manually checking and correcting the result.", "labels": [], "entities": []}, {"text": "We have also used the British National Corpus (BNC), in order to take more samples, mostly literal.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 22, "end_pos": 51, "type": "DATASET", "confidence": 0.9692460795243582}]}, {"text": "In the case of he BNC, we were able to extract the direct object from the depency parses, but had manually controlled metaphorical vs. literal usage.", "labels": [], "entities": []}, {"text": "In all, we collected 124 instances of literal usage and 275 instances of non-literal usage involving 311 unique nouns.", "labels": [], "entities": []}, {"text": "With this body of literal and non-literal contexts, we tried every possible combination of one literal and one non-literal object for each verb as seed, and tested with the remaining words.", "labels": [], "entities": []}, {"text": "The mean results are collected in, where we see how the LCS-based measures by and performed the best.", "labels": [], "entities": []}, {"text": "One observation is that the differences between the different measures although significant, they are not as dramatic as to effect reversals in the decision.", "labels": [], "entities": []}, {"text": "This is apparent in the simple voting results (right-most column in) where all measures yield identical results.", "labels": [], "entities": []}, {"text": "Only when differences in the similarities accumulate before the comparison between literal and non-literal context is made (three left-most columns in), does the choice of similarity measure make a difference.", "labels": [], "entities": []}, {"text": "Another observation pertains to relaxing the dependency on WordNet so that method can be based on similarities defined over more widely available lexical resources.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9506188035011292}]}, {"text": "In this respect, the low F-score by the adapted Lesk measure is not very encouraging, as variations of the Lesk measure could be defined over the glosses in digital dictionaries without explicit WordNet-style relations.", "labels": [], "entities": [{"text": "F-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9971166849136353}]}, {"text": "Combined with the high valuation of methods using the LCS, this leads us to conclude that the relative taxonomic position is a very important factor.", "labels": [], "entities": [{"text": "valuation", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9656729698181152}]}, {"text": "Finally, and happily counter to our prior intuition, we would like to note the robustness of the method to the number of different senses test words have: plotting the F-score against the number of senses did not result in consistently deteriorating results as the senses multiply).", "labels": [], "entities": [{"text": "F-score", "start_pos": 168, "end_pos": 175, "type": "METRIC", "confidence": 0.9952104687690735}]}, {"text": "If this had happened, we would have con-VU-Amsterdam-Metaphor-Corpus 2 Although some of the nouns in our collection have as many as 33 senses, we have only plotted the data for up to 15 senses; the data is too sparse to be reasonably usuable beyond that point.", "labels": [], "entities": []}, {"text": "fronted a Catch-22 situation where disambiguation is needed in order to carryout metaphora identification, a disambiguation task itself.", "labels": [], "entities": [{"text": "metaphora identification", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.7928906381130219}]}, {"text": "The way things stand, our method can be successfully applied to shallow NLP tasks or as a pre-processing and optimization step for WSD and parsing.", "labels": [], "entities": [{"text": "WSD", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.9650233387947083}]}], "tableCaptions": [{"text": " Table 1: F\u03b2=1 scores for all combinations of seven different similarity measures and five ways of deriving  a single judgement on literal usage by testing all senses of a word against all senses of the seed words.", "labels": [], "entities": [{"text": "F\u03b2", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9981619715690613}]}]}