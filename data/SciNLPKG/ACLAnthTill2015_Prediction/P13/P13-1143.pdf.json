{"title": [{"text": "Grammatical Error Correction Using Integer Linear Programming", "labels": [], "entities": [{"text": "Grammatical Error Correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8828952113787333}]}], "abstractContent": [{"text": "We propose a joint inference algorithm for grammatical error correction.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6045862436294556}]}, {"text": "Different from most previous work where different error types are corrected independently, our proposed inference process considers all possible errors in a unied framework.", "labels": [], "entities": []}, {"text": "We use integer linear programming (ILP) to model the inference process, which can easily incorporate both the power of existing error classiers and prior knowledge on grammatical error correction.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 167, "end_pos": 195, "type": "TASK", "confidence": 0.656544437011083}]}, {"text": "Experimental results on the Helping Our Own shared task show that our method is competitive with state-of-the-art systems.", "labels": [], "entities": [{"text": "Helping Our Own shared task", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.8278050422668457}]}], "introductionContent": [{"text": "Grammatical error correction is an important task of natural language processing (NLP).", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7722405791282654}, {"text": "natural language processing (NLP)", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.8056255877017975}]}, {"text": "It has many potential applications and may help millions of people who learn English as a second language (ESL).", "labels": [], "entities": []}, {"text": "As a research eld, it faces the challenge of processing ungrammatical language, which is different from other NLP tasks.", "labels": [], "entities": []}, {"text": "The task has received much attention in recent years, and was the focus of two shared tasks on grammatical error correction in.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.6519213120142618}]}, {"text": "To detect and correct grammatical errors, two different approaches are typically used \u0097 knowledge engineering or machine learning.", "labels": [], "entities": []}, {"text": "The rst relies on handcrafting a set of rules.", "labels": [], "entities": []}, {"text": "For example, the superlative adjective best is preceded by the article the.", "labels": [], "entities": []}, {"text": "In contrast, the machine learning approach formulates the task as a classication problem based on learning from training data.", "labels": [], "entities": []}, {"text": "For example, an article classier takes a noun phrase (NP) as input and predicts its article using class labels a/an, the, or \u025b (no article).", "labels": [], "entities": []}, {"text": "Both approaches have their advantages and disadvantages.", "labels": [], "entities": []}, {"text": "One can readily handcraft a set of rules to incorporate various prior knowledge from grammar books and dictionaries, but rules often have exceptions and it is difcult to build rules for all grammatical errors.", "labels": [], "entities": []}, {"text": "On the other hand, the machine learning approach can learn from texts written by ESL learners where grammatical errors have been annotated.", "labels": [], "entities": []}, {"text": "However, training data maybe noisy and classiers may need prior knowledge to guide their predictions.", "labels": [], "entities": []}, {"text": "Another consideration in grammatical error correction is how to deal with multiple errors in an input sentence.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.6075862348079681}]}, {"text": "Most previous work deals with errors individually: different classiers (or rules) are developed for different types of errors (article classier, preposition classier, etc).", "labels": [], "entities": []}, {"text": "Classiers are then deployed independently.", "labels": [], "entities": []}, {"text": "An example is a pipeline system, where each classier takes the output of the previous classier as its input and proposes corrections of one error type.", "labels": [], "entities": []}, {"text": "One problem of this pipeline approach is that the relations between errors are ignored.", "labels": [], "entities": []}, {"text": "For example, assume that an input sentence contains a cats.", "labels": [], "entities": []}, {"text": "An article classier may propose to delete a, while a noun number classier may propose to change cats to cat.", "labels": [], "entities": []}, {"text": "A pipeline approach will choose one of the two corrections based purely on which error classier is applied rst.", "labels": [], "entities": []}, {"text": "Another problem is that when applying a classier, the surrounding words in the context are assumed to be correct, which is not true if grammatical errors appear close to each other in a sentence.", "labels": [], "entities": []}, {"text": "In this paper, we formulate grammatical error correction as a task suited for joint inference.", "labels": [], "entities": [{"text": "formulate grammatical error correction", "start_pos": 18, "end_pos": 56, "type": "TASK", "confidence": 0.7004802152514458}, {"text": "joint inference", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.7297585606575012}]}, {"text": "Given an input sentence, different types of errors are jointly corrected as follows.", "labels": [], "entities": []}, {"text": "For every possible error correction, we assign a score which measures how grammatical the resulting sentence is if the correction is accepted.", "labels": [], "entities": []}, {"text": "We then choose a set of corrections which will result in a corrected sentence that is judged to be the most grammatical.", "labels": [], "entities": []}, {"text": "The inference problem is solved by integer lin-ear programming (ILP).", "labels": [], "entities": []}, {"text": "Variables of ILP are indicators of possible grammatical error corrections, the objective function aims to select the best set of corrections, and the constraints help to enforce a valid and grammatical output.", "labels": [], "entities": [{"text": "grammatical error corrections", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.6031754612922668}]}, {"text": "Furthermore, ILP not only provides a method to solve the inference problem, but also allows fora natural integration of grammatical constraints into a machine learning approach.", "labels": [], "entities": []}, {"text": "We will show that ILP fully utilizes individual error classiers, while prior knowledge on grammatical error correction can be easily expressed using linear constraints.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 90, "end_pos": 118, "type": "TASK", "confidence": 0.6912813186645508}]}, {"text": "We evaluate our proposed ILP approach on the test data from the Helping Our Own (HOO) 2011 shared task ().", "labels": [], "entities": [{"text": "Helping Our Own (HOO) 2011 shared task", "start_pos": 64, "end_pos": 102, "type": "TASK", "confidence": 0.6371939844555325}]}, {"text": "Experimental results show that the ILP formulation is competitive with stateof-the-art grammatical error correction systems.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 87, "end_pos": 115, "type": "TASK", "confidence": 0.6955980857213339}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives the related work.", "labels": [], "entities": []}, {"text": "Section 3 introduces a basic ILP formulation.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 improve the basic ILP formulation with more constraints and second order variables, respectively.", "labels": [], "entities": [{"text": "ILP formulation", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.9158803224563599}]}, {"text": "Section 6 presents the experimental results.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments mainly focus on two aspects: how our ILP approach performs compared to other grammatical error correction systems; and how the different constraints and the second order variables affect the ILP performance.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.6750956078370413}]}, {"text": "We follow the evaluation setup in the HOO 2011 shared task on grammatical error correction ().", "labels": [], "entities": [{"text": "HOO 2011 shared task", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.7983508706092834}, {"text": "grammatical error correction", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.5153379638989767}]}, {"text": "The development set and test set in the shared task consist of conference and workshop papers taken from the Association for Computational Linguistics (ACL).", "labels": [], "entities": []}, {"text": "gives an overview of the data sets.", "labels": [], "entities": []}, {"text": "System performance is measured by precision, recall, and F measure: The difculty lies in how to generate the system edits from the system output.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9996733665466309}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9994294047355652}, {"text": "F measure", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9922679364681244}]}, {"text": "In the HOO 2011 shared task, participants can submit system edits directly or the corrected plain-text system output.", "labels": [], "entities": [{"text": "HOO 2011 shared task", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.7175948023796082}]}, {"text": "In the latter case, the ofcial HOO scorer will extract system edits based on the original (ungrammatical) input text and the corrected system output text, using GNU Wdiff . Consider an input sentence The data is similar with test set.", "labels": [], "entities": []}, {"text": "The gold-standard edits are with \u2192 to and \u025b \u2192 the.", "labels": [], "entities": []}, {"text": "That is, the grammatically correct sentence should be The data is similar to the test set.", "labels": [], "entities": []}, {"text": "Suppose the corrected output of a system to be evaluated is exactly this perfectly corrected sentence The data is similar to the test set.", "labels": [], "entities": []}, {"text": "However, the ofcial HOO scorer using GNU Wdiff will automatically extract only one system edit with \u2192 to the for this system output.", "labels": [], "entities": [{"text": "GNU Wdiff", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.8566696047782898}]}, {"text": "Since this single system edit does not match any of the two gold-standard edits, the HOO scorer returns an F measure of 0, even though the system output is perfectly correct.", "labels": [], "entities": [{"text": "HOO scorer", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.8782621026039124}, {"text": "F measure", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9891775846481323}]}, {"text": "In order to overcome this problem, the MaxMatch (M 2 ) scorer was proposed in.", "labels": [], "entities": [{"text": "MaxMatch (M 2 ) scorer", "start_pos": 39, "end_pos": 61, "type": "METRIC", "confidence": 0.6224195063114166}]}, {"text": "Given a set of gold-standard edits, the original (ungrammatical) input text, and the corrected system output text, the M 2 scorer searches for the system edits that have the largest overlap with the gold-standard edits.", "labels": [], "entities": []}, {"text": "For the above example, the system edits automatically determined by the M 2 scorer are identical to the goldstandard edits, resulting in an F measure of 1 as we would expect.", "labels": [], "entities": [{"text": "F measure", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9861006438732147}]}, {"text": "We will use the M 2 scorer in this paper to determine the best system edits.", "labels": [], "entities": [{"text": "M 2 scorer", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9446603854497274}]}, {"text": "Once the system edits are found, P , R, and F are computed using the standard denition (26).", "labels": [], "entities": [{"text": "F", "start_pos": 44, "end_pos": 45, "type": "METRIC", "confidence": 0.983138918876648}]}, {"text": "We compare our ILP approach with two other systems: the beam search decoder of) which achieves the best published performance to date on the HOO 2011 data set, and UI Run1 ( ) which achieves the best performance among all participating systems at the HOO 2011 shared task.", "labels": [], "entities": [{"text": "HOO 2011 data set", "start_pos": 141, "end_pos": 158, "type": "DATASET", "confidence": 0.9609008878469467}, {"text": "HOO 2011 shared task", "start_pos": 251, "end_pos": 271, "type": "TASK", "confidence": 0.7258113026618958}]}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "The HOO 2011 shared task provides two sets of gold-standard edits: the original gold-standard edits produced by the annotator, and the ofcial gold-", "labels": [], "entities": [{"text": "HOO 2011 shared task", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8872513920068741}]}], "tableCaptions": [{"text": " Table 1: Error types and corrections. The Applicable column indicates which parts of a sentence are  applicable to an error type. In the rst row, \u025b means deleting an article.", "labels": [], "entities": [{"text": "Applicable", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9974703788757324}]}, {"text": " Table 3: Overview of the HOO 2011 data sets.  Corrections are called edits in the HOO 2011  shared task.", "labels": [], "entities": [{"text": "HOO 2011 data sets", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.9726376682519913}, {"text": "HOO 2011  shared task", "start_pos": 83, "end_pos": 104, "type": "DATASET", "confidence": 0.8919390439987183}]}, {"text": " Table 4: Comparison of three grammatical error  correction systems.", "labels": [], "entities": [{"text": "grammatical error  correction", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.660735289255778}]}, {"text": " Table 5: Comparison of the beam search decoder and ILP inference. ILP is equipped with all constraints  (MC, ANA, DR) and default parameters. Second order variables related to article and noun number error  types are also used in the last row.", "labels": [], "entities": [{"text": "ANA", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9628725051879883}]}, {"text": " Table 6: The effects of different constraints and second order variables.", "labels": [], "entities": []}]}