{"title": [{"text": "Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain", "labels": [], "entities": [{"text": "Robust Abstractive Multi-Document Summarization", "start_pos": 8, "end_pos": 55, "type": "TASK", "confidence": 0.5572894513607025}]}], "abstractContent": [{"text": "In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text.", "labels": [], "entities": [{"text": "summarization", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.8591009378433228}]}, {"text": "Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extrac-tive summaries.", "labels": [], "entities": [{"text": "redundancy avoidance", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7884003520011902}, {"text": "sentence compression", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.7191591858863831}]}, {"text": "In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9912986159324646}]}, {"text": "We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes.", "labels": [], "entities": []}, {"text": "We show that model summaries (1) are more abstrac-tive and make use of more sentence aggre-gation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added.", "labels": [], "entities": []}, {"text": "These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.", "labels": [], "entities": []}], "introductionContent": [{"text": "In automatic summarization, centrality has been one of the guiding principles for content selection in extractive systems.", "labels": [], "entities": [{"text": "summarization", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.8386752605438232}, {"text": "content selection", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7294474244117737}]}, {"text": "We define centrality to be the idea that a summary should contain the parts of the source text that are most similar or representative of the source text.", "labels": [], "entities": []}, {"text": "This is most transparently illustrated by the Maximal Marginal Relevance (MMR) system of, which defines the summarization objective to be a linear combination of a centrality term and a non-redundancy term.", "labels": [], "entities": [{"text": "Maximal Marginal Relevance (MMR)", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.5705992927153906}]}, {"text": "Since MMR, much progress has been made on more sophisticated methods of measuring centrality and integrating it with non-redundancy (See Nenkova and McKeown (2011) fora recent survey).", "labels": [], "entities": [{"text": "MMR", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9286260604858398}]}, {"text": "For example, term weighting methods such as the signature term method of pick out salient terms that occur more often than would be expected in the source text based on frequencies in a background corpus.", "labels": [], "entities": [{"text": "term weighting", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7306223809719086}]}, {"text": "This method is a core component of the most successful summarization methods ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.9854190349578857}]}, {"text": "While extractive methods based on centrality have thus achieved success, there has long been recognition that abstractive methods are ultimately more desirable.", "labels": [], "entities": []}, {"text": "One line of work is in text simplification and sentence fusion, which focus on the ability of abstraction to achieve a higher compression ratio.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.8050180077552795}, {"text": "sentence fusion", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.79970383644104}]}, {"text": "A less examined issue is that of aggregation and information synthesis.", "labels": [], "entities": [{"text": "information synthesis", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.8071430027484894}]}, {"text": "A key part of the usefulness of summaries is that they provide some synthesis or analysis of the source text and make a more general statement that is of direct relevance to the user.", "labels": [], "entities": [{"text": "summaries", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.9584479928016663}]}, {"text": "For example, a series of related events can be aggregated and expressed as a trend.", "labels": [], "entities": []}, {"text": "The position of this paper is that centrality is not enough to make substantial progress towards abstractive summarization that is capable of this type of semantic inference.", "labels": [], "entities": []}, {"text": "Instead, summarization systems need to make more use of domain knowledge.", "labels": [], "entities": [{"text": "summarization", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.9788848757743835}]}, {"text": "We provide evidence for this in a series of studies on the TAC 2010 guided summarization data set that examines how the behaviour of automatic summarizers can or cannot be distinguished from human summarizers.", "labels": [], "entities": [{"text": "TAC 2010 guided summarization data set", "start_pos": 59, "end_pos": 97, "type": "DATASET", "confidence": 0.8621025681495667}]}, {"text": "First, we confirm that abstraction is a desirable goal, and provide a quantitative measure of the degree of sentence aggregation in a summarization system.", "labels": [], "entities": []}, {"text": "Second, we show that centrality-based measures are unlikely to lead to substantial progress towards abstractive summarization, because current topperforming systems already produce summaries that are more \"central\" than humans do.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 100, "end_pos": 125, "type": "TASK", "confidence": 0.5545511543750763}]}, {"text": "Third, we consider how domain knowledge maybe useful as a resource for an abstractive system, by showing that key parts of model summaries can be reconstructed from the source plus related in-domain documents.", "labels": [], "entities": []}, {"text": "Our contributions are novel in the following respects.", "labels": [], "entities": []}, {"text": "First, our analyses are performed at the level of caseframes, rather at the level of words or syntactic dependencies as in previous work.", "labels": [], "entities": []}, {"text": "Caseframes are shallow approximations of semantic roles which are well suited to characterizing a domain by its slots.", "labels": [], "entities": []}, {"text": "Furthermore, we take a developmental rather than evaluative perspective-our goal is not to develop anew evaluation measure as defined by correlation with human responsiveness judgments.", "labels": [], "entities": []}, {"text": "Instead, our studies reveal useful criteria with which to distinguish human-written and system summaries, helping to guide the development of future summarization systems.", "labels": [], "entities": [{"text": "summarization", "start_pos": 149, "end_pos": 162, "type": "TASK", "confidence": 0.9747109413146973}]}], "datasetContent": [{"text": "We conducted our experiments on the data and results of the TAC 2010 summarization workshop.", "labels": [], "entities": [{"text": "TAC 2010 summarization workshop", "start_pos": 60, "end_pos": 91, "type": "DATASET", "confidence": 0.8151049911975861}]}, {"text": "This data set contains 920 newspaper articles in 46 topics of 20 documents each.", "labels": [], "entities": []}, {"text": "Ten are used in an initial guided summarization task, and ten are used in an update summarization task, in which a summary must be produced assuming that the original ten documents had already been read.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8915519118309021}, {"text": "update summarization task", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.6184897820154825}]}, {"text": "All summaries have a word length limit of 100 words.", "labels": [], "entities": []}, {"text": "We analyzed the results of the two summarization tasks separately in our experiments.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8937646150588989}]}, {"text": "The 46 topics belong to five different categories or domains: Accidents and natural disasters, Criminal or terrorist attacks, Health and safety, Endangered resources, and Investigations and trials.", "labels": [], "entities": []}, {"text": "Each domain is associated with a template specifying the type of information that is expected in the domain, such as the participants in the event or the time that the event occurred.", "labels": [], "entities": []}, {"text": "In our study, we compared the characteristics of summaries generated by the eight human summarizers with those generated by the peer summaries, which are basically extractive systems.", "labels": [], "entities": [{"text": "summaries generated by the eight human summarizers", "start_pos": 49, "end_pos": 99, "type": "TASK", "confidence": 0.7036282292434147}]}, {"text": "There are 43 peer summarization systems, including two baselines defined by NIST.", "labels": [], "entities": [{"text": "NIST", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.9629745483398438}]}, {"text": "We refer to systems by their ID given by NIST, which are alphabetical for the human summarizers (A to H), and numeric for the peer summarizers (1 to 43).", "labels": [], "entities": [{"text": "NIST", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.9776766300201416}]}, {"text": "We removed two peer systems (systems 29 and 43) which did not generate any summary text in the workshop, presumably due to software problems.", "labels": [], "entities": []}, {"text": "For each measure that we consider, we compare the average among the human-written summaries to the three individual peer systems, which we chose in order to provide a representative sample of the average and best performance of the automatic systems according to current evaluation methods.", "labels": [], "entities": []}, {"text": "These systems are all primarily extractive, like most of the systems in the workshop: Peer average The average of the measure among the 41 peer summarizers.", "labels": [], "entities": [{"text": "Peer average", "start_pos": 86, "end_pos": 98, "type": "METRIC", "confidence": 0.9698600769042969}]}], "tableCaptions": [{"text": " Table 2: The average number of source text sen- tences needed to cover a summary sentence. The  model average is statistically significantly differ- ent from all the other conditions p < 10 \u22127  (Study 1).", "labels": [], "entities": []}, {"text": " Table 4: Density of signature caseframes after  merging to various threshold for the initial (Init.)  and update (Up.) summarization tasks (Study 2).", "labels": [], "entities": []}, {"text": " Table 6: The effect on caseframe coverage of  adding in-domain and out-of-domain documents.", "labels": [], "entities": [{"text": "caseframe coverage", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7451940476894379}]}]}