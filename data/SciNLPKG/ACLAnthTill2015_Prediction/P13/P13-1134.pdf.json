{"title": [{"text": "FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection", "labels": [], "entities": []}], "abstractContent": [{"text": "We present anew bilingual FrameNet lexicon for English and German.", "labels": [], "entities": []}, {"text": "It is created through a simple, but powerful approach to construct a FrameNet in any language using Wiktionary as an inter-lingual representation.", "labels": [], "entities": []}, {"text": "Our approach is based on a sense alignment of FrameNet and Wiktionary, and subsequent translation disambiguation into the target language.", "labels": [], "entities": []}, {"text": "We perform a detailed evaluation of the created resource and a discussion of Wiktionary as an interlingual connection for the cross-language transfer of lexical-semantic resources.", "labels": [], "entities": [{"text": "cross-language transfer of lexical-semantic resources", "start_pos": 126, "end_pos": 179, "type": "TASK", "confidence": 0.827627158164978}]}, {"text": "The created resource is publicly available at http://www.", "labels": [], "entities": []}], "introductionContent": [{"text": "FrameNet is a valuable resource for natural language processing (NLP): semantic role labeling (SRL) systems based on FrameNet provide semantic analysis for NLP applications, such as question answering () and information extraction.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9172424674034119}, {"text": "natural language processing (NLP)", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.8103877007961273}, {"text": "semantic role labeling (SRL)", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.7960672775904337}, {"text": "question answering", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.8845946788787842}, {"text": "information extraction", "start_pos": 208, "end_pos": 230, "type": "TASK", "confidence": 0.8671976029872894}]}, {"text": "However, their wide deployment has been prohibited by the poor coverage and limited availability of a similar resource in many languages.", "labels": [], "entities": []}, {"text": "Expert-built lexical-semantic resources are expensive to create.", "labels": [], "entities": []}, {"text": "Previous cross-lingual transfer of FrameNet used corpus-based approaches, or resource alignment with multilingual expert-built resources, such as EuroWordNet.", "labels": [], "entities": [{"text": "EuroWordNet", "start_pos": 146, "end_pos": 157, "type": "DATASET", "confidence": 0.9834787845611572}]}, {"text": "The latter indirectly also suffers from the high cost and constrained coverage of expert-built resources.", "labels": [], "entities": []}, {"text": "Recently, collaboratively created resources have been investigated for the multilingual extension of resources in NLP, beginning with Wikipedia.", "labels": [], "entities": []}, {"text": "They rely on the socalled \"Wisdom of the Crowds\", contributions by a large number of volunteers, which results in a continuously updated high-quality resource available in hundreds of languages.", "labels": [], "entities": []}, {"text": "Due to the encyclopedic nature of Wikipedia, previous work focused on encyclopedic information for Wikipedia entries, i.e., almost exclusively on nouns.", "labels": [], "entities": []}, {"text": "This is not enough for resources like FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.8409013152122498}]}, {"text": "Such resources need lexical-semantic information on various POS.", "labels": [], "entities": []}, {"text": "For FrameNet, information on the predicates associated with a semantic framemostly verbs, nouns, and adjectives -is crucial, for instance gloss or syntactic subcategorization.", "labels": [], "entities": []}, {"text": "A solution for the problem of multilingual extension of lexical semantic resources is to use Wiktionary, a collaboratively created dictionary, as connection between languages.", "labels": [], "entities": [{"text": "multilingual extension of lexical semantic resources", "start_pos": 30, "end_pos": 82, "type": "TASK", "confidence": 0.8081153829892477}]}, {"text": "It provides high-quality lexical information on all POS, for instance glosses, sense relations, syntactic subcategorization, etc.", "labels": [], "entities": []}, {"text": "Like Wikipedia, it is continuously extended and contains translations to hundreds of languages, including low-resource ones.", "labels": [], "entities": []}, {"text": "To our knowledge, Wiktionary has not been evaluated as an interlingual index for the cross-lingual extension of lexical-semantic resources.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel method for the creation of bilingual FrameNet lexicons based on an alignment to Wiktionary.", "labels": [], "entities": [{"text": "Wiktionary", "start_pos": 114, "end_pos": 124, "type": "DATASET", "confidence": 0.9146918654441833}]}, {"text": "We demonstrate our method on the language pair English-German and present the resulting resources, a lemma-based multilingual and a sense-disambiguated GermanEnglish FrameNet lexicon.", "labels": [], "entities": []}, {"text": "The understanding of lexical-semantic resources and their combinations, e.g., how alignment algorithms can be adapted to individual resource pairs and different POS, is essential for their effective use in NLP and a prerequisite for later in-task evaluation and application.", "labels": [], "entities": []}, {"text": "To enhance this understanding for the presented resource pair, we perform a detailed analysis of the created resource and compare it to existing FrameNet resources for German.", "labels": [], "entities": []}, {"text": "This is a major step towards the vision of this paper: a simple, but powerful approach to partially construct a FrameNet in any language using Wiktionary as an interlingual representation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We determined the best setting for the alignment of FrameNet and Wiktionary in a ten-fold crossvalidation on the gold standard.", "labels": [], "entities": []}, {"text": "Besides the parameters for the computation of the PPR vectors (we used the publicly available UKB tool by), the main parameter in the experiments is the textual information that is used to represent the senses.", "labels": [], "entities": [{"text": "UKB", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.977321445941925}]}, {"text": "For FrameNet senses, we used the lemma-pos, sense gloss, example sentences, frame name and frame definition as textual features; for Wiktionary senses, we considered lemma-pos, sense gloss, example sentences, hyponyms and synonyms.", "labels": [], "entities": []}, {"text": "We computed the similarity scores on tokenized, lemmatized and stop-word-filtered texts.", "labels": [], "entities": [{"text": "similarity", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9740878939628601}]}, {"text": "First, we evaluated models for COS and PPR independently based on various combinations of the textual features listed above.", "labels": [], "entities": [{"text": "COS", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.8303313851356506}]}, {"text": "We then used the parameter setting of the best-performing single models to train the model that jointly optimizes the thresholds for PPR and COS (see eqn.).", "labels": [], "entities": []}, {"text": "In, we report on the results of the best single models and the best joint model.", "labels": [], "entities": []}, {"text": "For the evaluation, we compute precision P, recall Rand F 1 on the positive class (aligned=true), e.g., precision P is the number of pairs correctly aligned divided by all aligned pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9979076385498047}, {"text": "recall Rand F 1", "start_pos": 44, "end_pos": 59, "type": "METRIC", "confidence": 0.8360098451375961}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.992944061756134}]}, {"text": "We achieved the highest precision and F 1 -score  for COS using all available features, but excluding FrameNet example sentences because they introduce too much noise.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.999728262424469}, {"text": "F 1 -score", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9902991205453873}, {"text": "COS", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.7872204184532166}]}, {"text": "Adding the frame name and frame definition to the often short glosses provides a richer sense representation for the COS measure.", "labels": [], "entities": [{"text": "COS", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.6717552542686462}]}, {"text": "The best-performing PPR configuration uses sense gloss and lemma-pos.", "labels": [], "entities": []}, {"text": "For the joint model, we employed the best single PPR configuration, and a COS configuration that uses sense gloss extended by Wiktionary hypernyms, synonyms and FrameNet frame name and frame definition, to achieve the highest score, an F 1 -score of 0.739.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 236, "end_pos": 246, "type": "METRIC", "confidence": 0.9829748123884201}]}, {"text": "We compared the performance of our alignment on the gold standard to a baseline which randomly selects one target sense from the candidate set of each source sense (Random-1).", "labels": [], "entities": []}, {"text": "We also consider the more competitive Wiktionary first sense baseline (WKT-1).", "labels": [], "entities": []}, {"text": "It is guided by the heuristic that more frequent senses are listed first in Wiktionary (.", "labels": [], "entities": []}, {"text": "It is a stronger baseline with an F 1 -score of 0.65 (see).", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9942841082811356}]}, {"text": "To derive the upper bound for the alignment performance (UBound), we computed the F 1 score from the average pairwise F 1 -score of the annotators according to.", "labels": [], "entities": [{"text": "UBound)", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9085119366645813}, {"text": "F 1 score", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9881029526392618}, {"text": "pairwise F 1 -score", "start_pos": 109, "end_pos": 128, "type": "METRIC", "confidence": 0.8311744213104248}]}, {"text": "As the evaluation set mirrors the POS distribution in FrameNet and is sufficiently large, unlike earlier alignments, we can analyze the performance by POS.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.869733452796936}]}, {"text": "The BEST JOINT model performs well on nouns, slightly better on adjectives, and worse on verbs, see.", "labels": [], "entities": [{"text": "BEST", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9107021689414978}]}, {"text": "For the baselines and the UBound the same applies, with the difference that adjectives receive even better results in comparison.", "labels": [], "entities": [{"text": "UBound", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.897369921207428}]}, {"text": "This fits in with the perceived degree of difficulty according to the observed polysemy for the POS: for verbs we have many candidate sets with two or more candidates, i.e., we observe higher polysemy, while for nouns and even stronger for adjectives, many small candidate sets occur, which stand for an easier alignment decision.", "labels": [], "entities": []}, {"text": "This is inline with the reported higher complexity of lexical resources with respect to verbs and greater difficulty in alignments and word sense disambiguation (Laparra and.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 135, "end_pos": 160, "type": "TASK", "confidence": 0.6184274554252625}]}, {"text": "The performance of BEST JOINT on all POS is F 1 =0.73, which is significantly higher than the WKT-1 baseline (p<0.05 according to McNemar's test).", "labels": [], "entities": [{"text": "BEST JOINT", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.5724460482597351}, {"text": "F 1", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9903929531574249}, {"text": "WKT-1 baseline", "start_pos": 94, "end_pos": 108, "type": "DATASET", "confidence": 0.8301323652267456}]}, {"text": "The performance on nouns (F 1 =0.775) is on par with the results reported by for nouns (F 1 =0.78).", "labels": [], "entities": [{"text": "F 1", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9844326078891754}, {"text": "F 1", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9653087556362152}]}, {"text": "Because the errors of two subsequently applied automatic methods can multiply, we provide a posthoc evaluation of the results.", "labels": [], "entities": []}, {"text": "To evaluate the quality of the German FrameNet lexicon, we collected the FrameNet senses fora list of 15 frames that were sampled by Pad\u00f3 and  were assigned correctly to their frames.", "labels": [], "entities": [{"text": "German FrameNet lexicon", "start_pos": 31, "end_pos": 54, "type": "DATASET", "confidence": 0.8213103612263998}]}, {"text": "This is higher than expected, considering the errors from the applied methods add up.", "labels": [], "entities": []}, {"text": "Further analysis revealed that both resource creation steps contribute equally to the 39 errors.", "labels": [], "entities": []}, {"text": "For 17 of the evaluated sense pairs, redundancy confirms their quality: they were obtained independently by two or three alignment-and-translation paths and do not contain alignment errors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Alignment performance by POS.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9191271662712097}]}, {"text": " Table 3: Post-hoc evaluation (precision P).", "labels": [], "entities": [{"text": "precision P)", "start_pos": 31, "end_pos": 43, "type": "METRIC", "confidence": 0.95598965883255}]}, {"text": " Table 4: Frame-semantic resources for German.", "labels": [], "entities": []}, {"text": " Table 5: Statistics after relation disambiguation.", "labels": [], "entities": [{"text": "relation disambiguation", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.8640881478786469}]}, {"text": " Table 6: Overlap of FNWKde with resource r.", "labels": [], "entities": [{"text": "FNWKde", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.5132500529289246}]}]}