{"title": [{"text": "Two-Neighbor Orientation Model with Cross-Boundary Global Contexts", "labels": [], "entities": []}], "abstractContent": [{"text": "Long distance reordering remains one of the greatest challenges in statistical machine translation research as the key con-textual information may well be beyond the confine of translation units.", "labels": [], "entities": [{"text": "Long distance reordering", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6441688438256582}, {"text": "statistical machine translation research", "start_pos": 67, "end_pos": 107, "type": "TASK", "confidence": 0.7318426072597504}]}, {"text": "In this paper, we propose Two-Neighbor Orientation (TNO) model that jointly models the orientation decisions between anchors and two neighboring multi-unit chunks which may cross phrase or rule boundaries.", "labels": [], "entities": []}, {"text": "We explicitly model the longest span of such chunks, referred to as Maximal Orientation Span, to serve as a global parameter that constrains underlying local decisions.", "labels": [], "entities": []}, {"text": "We integrate our proposed model into a state-of-the-art string-to-dependency translation system and demonstrate the efficacy of our proposal in a large-scale Chinese-to-English translation task.", "labels": [], "entities": [{"text": "Chinese-to-English translation task", "start_pos": 158, "end_pos": 193, "type": "TASK", "confidence": 0.7497033377488455}]}, {"text": "On NIST MT08 set, our most advanced model brings around +2.0 BLEU and-1.0 TER improvement.", "labels": [], "entities": [{"text": "NIST MT08 set", "start_pos": 3, "end_pos": 16, "type": "DATASET", "confidence": 0.8304845492045084}, {"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9984832406044006}, {"text": "TER", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.999687671661377}]}], "introductionContent": [{"text": "Long distance reordering remains one of the greatest challenges in Statistical Machine Translation (SMT) research.", "labels": [], "entities": [{"text": "Long distance reordering", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7039395769437155}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.8637808958689371}]}, {"text": "The challenge stems from the fact that an accurate reordering hinges upon the model's ability to make many local and global reordering decisions accurately.", "labels": [], "entities": []}, {"text": "Often, such reordering decisions require contexts that span across multiple translation units.", "labels": [], "entities": []}, {"text": "Unfortunately, previous approaches fall short in capturing such cross-unit contextual information that could be critical in reordering.", "labels": [], "entities": []}, {"text": "Specifically, the popular distortion or lexicalized reordering models in phrasebased SMT focus only on making good local prediction (i.e. predicting the orientation of immediate neighboring translation units), while translation rules in syntax-based SMT come with a strong context-free assumption, which model only the reordering within the confine of the rules.", "labels": [], "entities": [{"text": "phrasebased SMT", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.4976932853460312}]}, {"text": "In this paper, we argue that reordering modeling would greatly benefit from richer cross-boundary contextual information We introduce a reordering model that incorporates such contextual information, named the TwoNeighbor Orientation (TNO) model.", "labels": [], "entities": []}, {"text": "We first identify anchors as regions in the source sentences around which ambiguous reordering patterns frequently occur and chunks as regions that are consistent with word alignment which may span multiple translation units at decoding time.", "labels": [], "entities": []}, {"text": "Most notably, anchors and chunks in our model may not necessarily respect the boundaries of translation units.", "labels": [], "entities": []}, {"text": "Then, we jointly model the orientations of chunks that immediately precede and follow the anchors (hence, the name \"two-neighbor\") along with the maximal span of these chunks, to which we refer as Maximal Orientation Span (MOS).", "labels": [], "entities": []}, {"text": "As we will elaborate further in next sections, our models provide a stronger mechanism to make more accurate global reordering decisions for the following reasons.", "labels": [], "entities": []}, {"text": "First of all, we consider the orientation decisions on both sides of the anchors simultaneously, in contrast to existing works that only consider one-sided decisions.", "labels": [], "entities": []}, {"text": "In this way, we hope to upgrade the unigram formulation of existing reordering models to a higher order formulation.", "labels": [], "entities": []}, {"text": "Second of all, we capture the reordering of chunks that may cross translation units and maybe composed of multiple units, in contrast to ex-isting works that focus on the reordering between individual translation units.", "labels": [], "entities": []}, {"text": "In effect, MOS acts as a global reordering parameter that guides or constrains the underlying local reordering decisions.", "labels": [], "entities": []}, {"text": "To show the effectiveness of our model, we integrate our TNO model into a state-of-theart syntax-based SMT system, which uses synchronous context-free grammar (SCFG) rules to jointly model reordering and lexical translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9235256910324097}, {"text": "lexical translation", "start_pos": 204, "end_pos": 223, "type": "TASK", "confidence": 0.7576673328876495}]}, {"text": "The introduction of nonterminals in the SCFG rules provides some degree of generalization.", "labels": [], "entities": []}, {"text": "However as mentioned earlier, the context-free assumption ingrained in the syntax-based formalism often limits the model's ability to influence global reordering decision that involves cross-boundary contexts.", "labels": [], "entities": []}, {"text": "In integrating TNO, we hope to strengthen syntax-based system's ability to make more accurate global reordering decisions.", "labels": [], "entities": []}, {"text": "Our other contribution in this paper is a practical method for integrating the TNO model into syntax-based translations.", "labels": [], "entities": []}, {"text": "The integration is nontrivial since the decoding of syntax-based SMT proceeds in a bottom-up fashion, while our model is more natural for top-down parsing, thus the model's full context sometimes is often available only at the latest stage of decoding.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.7429246306419373}]}, {"text": "We implement an efficient shift-reduce algorithm that facilitates the accumulation of partial context in a bottom-up fashion, allowing our model to influence the translation process even in the absence of full context.", "labels": [], "entities": []}, {"text": "We show the efficacy of our proposal in a largescale Chinese-to-English translation task where the introduction of our TNO model provides a significant gain over a state-of-the-art string-todependency SMT system) that we enhance with additional state-of-the-art features.", "labels": [], "entities": [{"text": "Chinese-to-English translation task", "start_pos": 53, "end_pos": 88, "type": "TASK", "confidence": 0.696543296178182}, {"text": "SMT", "start_pos": 201, "end_pos": 204, "type": "TASK", "confidence": 0.9412590265274048}]}, {"text": "Even though the experimental results carried out in this paper employ SCFG-based SMT systems, we would like to point out that our models is applicable to other systems including phrasebased SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.7068589329719543}, {"text": "SMT", "start_pos": 190, "end_pos": 193, "type": "TASK", "confidence": 0.8329497575759888}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce the formulation of our TNO model.", "labels": [], "entities": []}, {"text": "In Section 3, we introduce and motivate the concept of Maximal Orientation Span.", "labels": [], "entities": [{"text": "Maximal Orientation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8333810567855835}]}, {"text": "In Section 4, we introduce four variants of the TNO model with different model complexities.", "labels": [], "entities": []}, {"text": "In Section 5, we describe the training procedure to estimate the parameters of our models.", "labels": [], "entities": []}, {"text": "In Section 6, we describe our shift-reduce algorithm which integrates our proposed TNO model into syntax-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.8668686151504517}]}, {"text": "In Section 7, we describe our experiments and present our results.", "labels": [], "entities": []}, {"text": "We wrap up with related work in Section 8 and conclusion in Section 9.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our baseline systems is a state-of-the-art stringto-dependency system).", "labels": [], "entities": []}, {"text": "The system is trained on 10 million parallel sentences that are available to the Phase 1 of the DARPA BOLT Chinese-English MT task.", "labels": [], "entities": [{"text": "DARPA BOLT Chinese-English MT task", "start_pos": 96, "end_pos": 130, "type": "TASK", "confidence": 0.6369869232177734}]}, {"text": "The training corpora include a mixed genre of newswire, weblog, broadcast news, broadcast conversation, discussion forums and comes from various sources such as LDC, HK Law, HK Hansard and UN data.", "labels": [], "entities": [{"text": "LDC", "start_pos": 161, "end_pos": 164, "type": "DATASET", "confidence": 0.9400256872177124}, {"text": "HK Law", "start_pos": 166, "end_pos": 172, "type": "DATASET", "confidence": 0.9253550171852112}, {"text": "HK Hansard and UN data", "start_pos": 174, "end_pos": 196, "type": "DATASET", "confidence": 0.7204292416572571}]}, {"text": "In total, our baseline model employs about 40 features, including four from our proposed Two-Neighbor Orientation model.", "labels": [], "entities": []}, {"text": "In addition to the standard features including the rule translation probabilities, we incorporate features that are found useful for developing a state-of-the-art baseline, such as the provenance features).", "labels": [], "entities": [{"text": "rule translation", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7528403401374817}]}, {"text": "We use a large 6-gram language model, which was trained on 10 billion English words from multiple corpora, including the English side of our parallel corpus plus other corpora such as Gigaword (LDC2011T07) and Google News.", "labels": [], "entities": [{"text": "Gigaword (LDC2011T07)", "start_pos": 184, "end_pos": 205, "type": "DATASET", "confidence": 0.8475848138332367}]}, {"text": "We also train a class-based language model) on two million English sentences selected from the parallel corpus.", "labels": [], "entities": []}, {"text": "As the backbone of our string-to-dependency system, we train 3-gram models for left and right dependencies and unigram for head using the target side of the bilingual training data.", "labels": [], "entities": []}, {"text": "To train our Two-Neighbor Orientation model, we select a subset of 5 million aligned sentence pairs.", "labels": [], "entities": [{"text": "Two-Neighbor Orientation", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.7440871298313141}]}, {"text": "For the tuning and development sets, we set aside 1275 and 1239 sentences selected from LDC2010E30 corpus.", "labels": [], "entities": [{"text": "LDC2010E30 corpus", "start_pos": 88, "end_pos": 105, "type": "DATASET", "confidence": 0.9735154509544373}]}, {"text": "We tune the decoding weights with PRO () to maximize BLEU-TER.", "labels": [], "entities": [{"text": "PRO", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9948227405548096}, {"text": "BLEU-TER", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.99850994348526}]}, {"text": "As for the blind test set, we report the performance on the NIST MT08 evaluation set, which consists of 691 sentences from newswire and 666 sentences from weblog.", "labels": [], "entities": [{"text": "NIST MT08 evaluation set", "start_pos": 60, "end_pos": 84, "type": "DATASET", "confidence": 0.920654758810997}]}, {"text": "We pick the weights that produce the highest development set scores to decode the test set.", "labels": [], "entities": []}, {"text": "summarizes the experimental results on NIST MT08 newswire and weblog.", "labels": [], "entities": [{"text": "NIST MT08 newswire", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.87252805630366}]}, {"text": "In column 2, we report the classification accuracy on a subset of training data.", "labels": [], "entities": [{"text": "classification", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.9130814671516418}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9730321168899536}]}, {"text": "Note that these numbers are for reference only and not directly comparable with each other since the features used in these classifiers include several gold standard information, such as the anchors' target words, the anchors' MOSrelated features ( and the orientation of the right MOS: The NIST MT08 results on newswire (nw) and weblog (wb) genres.", "labels": [], "entities": [{"text": "NIST MT08", "start_pos": 291, "end_pos": 300, "type": "DATASET", "confidence": 0.8881275653839111}]}, {"text": "S2D is the baseline string-to-dependency system (line 1), on top of which Two-Neighbor Orientation Model 1 to 4 are employed (line 2-5).", "labels": [], "entities": []}, {"text": "The best TER and BLEU results on each genre are in bold.", "labels": [], "entities": [{"text": "TER", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.985862672328949}, {"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9858813285827637}]}, {"text": "For BLEU, higher scores are better, while for TER, lower scores are better.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9986311793327332}, {"text": "TER", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9909825921058655}]}, {"text": "to be predicted at decoding time.", "labels": [], "entities": []}, {"text": "In columns 2 and 4, we report the BLEU scores, while in columns 3 and 5, we report the TER scores.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9754601716995239}, {"text": "TER scores", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9795964360237122}]}, {"text": "The performance of our baseline stringto-dependency syntax-based SMT is shown in the first line, followed by the performance of our TwoNeighbor Orientation model starting from Model 1 to Model 4.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.6670980453491211}]}, {"text": "As shown, the empirical results confirm our intuition that SMT can greatly benefit from reordering model that incorporate cross-unit contextual information.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9924904704093933}]}, {"text": "Model 1 provides most of the gain across the two genres of around +0.9 to +1.2 BLEU and -0.5 to -1.1 TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9981986880302429}, {"text": "TER", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9927074313163757}]}, {"text": "Model 2 which conditions PO Lon OR provides an additional +0.2 BLEU improvement on BLEU score consistently across the two genres.", "labels": [], "entities": [{"text": "PO Lon OR", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.8097858230272929}, {"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.998278021812439}, {"text": "BLEU score", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.977660745382309}]}, {"text": "As shown inline 4, we see a stronger improvement in the inclusion of MOS-related information as features in Model 3.", "labels": [], "entities": []}, {"text": "In newswire, Model 3 gives an additional +0.4 BLEU and -0.2 TER, while in weblog, it gives a stronger improvement of an additional +0.5 BLEU and -0.3 TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9983130693435669}, {"text": "TER", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.985062837600708}, {"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9964489936828613}, {"text": "TER", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.9708099961280823}]}, {"text": "The inclusion of explicit MOS modeling in Model 4 gives a significant BLEU score improvement of +0.5 but no TER improvement in newswire.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9772399365901947}, {"text": "TER", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9994868040084839}]}, {"text": "In weblog, Model 4 gives a mixed results of +0.2 BLEU score improvement and a hit of +0.6 TER.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9706709682941437}, {"text": "TER", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9919006824493408}]}, {"text": "We conjecture that the weblog text has a more ambiguous orientation span that are more challenging to learn.", "labels": [], "entities": []}, {"text": "In total, our TNO model gives an encouraging result.", "labels": [], "entities": []}, {"text": "Our most advanced model gives significant improvement of +1.8 BLEU/-0.8 TER in newswire domain and +2.1 BLEU/-1.0 TER over a strong string-to-dependency syntax-based SMT enhanced with additional state-of-the-art features.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9959631562232971}, {"text": "TER", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9325717687606812}, {"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9970184564590454}, {"text": "TER", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9100935459136963}, {"text": "SMT", "start_pos": 166, "end_pos": 169, "type": "TASK", "confidence": 0.8940936326980591}]}], "tableCaptions": [{"text": " Table 4: The NIST MT08 results on newswire (nw) and we- blog (wb) genres. S2D is the baseline string-to-dependency  system (line 1), on top of which Two-Neighbor Orientation  Model 1 to 4 are employed (line 2-5). The best TER and  BLEU results on each genre are in bold. For BLEU, higher  scores are better, while for TER, lower scores are better.", "labels": [], "entities": [{"text": "NIST MT08", "start_pos": 14, "end_pos": 23, "type": "DATASET", "confidence": 0.8225929439067841}, {"text": "BLEU", "start_pos": 232, "end_pos": 236, "type": "METRIC", "confidence": 0.9850549697875977}, {"text": "BLEU", "start_pos": 276, "end_pos": 280, "type": "METRIC", "confidence": 0.9965039491653442}, {"text": "TER", "start_pos": 319, "end_pos": 322, "type": "METRIC", "confidence": 0.9096869230270386}]}]}