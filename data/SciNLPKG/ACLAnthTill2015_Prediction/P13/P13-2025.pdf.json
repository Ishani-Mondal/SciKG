{"title": [{"text": "On the Predictability of Human Assessment: when Matrix Completion Meets NLP Evaluation", "labels": [], "entities": [{"text": "NLP Evaluation", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.6289024949073792}]}], "abstractContent": [{"text": "This paper tackles the problem of collecting reliable human assessments.", "labels": [], "entities": []}, {"text": "We show that knowing multiple scores for each example instead of a single score results in a more reliable estimation of a system quality.", "labels": [], "entities": []}, {"text": "To reduce the cost of collecting these multiple ratings, we propose to use matrix completion techniques to predict some scores knowing only scores of other judges and some common ratings.", "labels": [], "entities": []}, {"text": "Even if prediction performance is pretty low, decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example .", "labels": [], "entities": []}], "introductionContent": [{"text": "Human assessment is often considered as the best, if not the only, way to evaluate 'subjective' NLP tasks like MT or speech generation.", "labels": [], "entities": [{"text": "MT", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.9864119291305542}, {"text": "speech generation", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.654882088303566}]}, {"text": "However, human evaluations are doomed to be noisy and, sometimes, even contradictory as they depend on individual perception and understanding of the score scale that annotators generally use in remarkably different ways ().", "labels": [], "entities": []}, {"text": "Moreover, annotation is known to be along and frustrating process and annotator fatigue has been identified as another source of noise (.", "labels": [], "entities": []}, {"text": "In addition to defining and enforcing stricter guidelines, several solutions have been proposed to reduce the annotation effort and produce more reliable ratings.", "labels": [], "entities": []}, {"text": "For instance, to limit the impact of the score scale interpretation, in the WMT evaluation campaign, annotators are asked to rank translation hypotheses from best to worst instead of providing absolute scores (e.g. in terms of adequacy or fluency).", "labels": [], "entities": [{"text": "WMT evaluation", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.7036895751953125}]}, {"text": "Generalizing this approach, several works () have defined novel annotation protocols to reduce the number of judgments that need to be collected.", "labels": [], "entities": []}, {"text": "However, all these methods suffer from several limitations: first, they provide no interpretable information about the quality of the system (only a relative comparison between two systems is possible); second, has recently shown that the ranking they induce is not reliable.", "labels": [], "entities": []}, {"text": "In this work, we study an alternative approach to the problem of collecting reliable human assessments.", "labels": [], "entities": []}, {"text": "Our basic assumption, motivated by the success of ensemble methods, is that having several judgments for each example, even if they are noisy, will result in a more reliable decision than having a single judgment.", "labels": [], "entities": []}, {"text": "An evaluation campaign should therefore aim at gathering a score matrix, in which each example is rated by all judges instead of having each judge rate only a small subset of examples, thereby minimizing redundancy.", "labels": [], "entities": []}, {"text": "Obviously, the former approach requires a large annotation effort and is, in practice, not feasible.", "labels": [], "entities": []}, {"text": "That is why, to reduce the number of judgments that must be collected, we propose to investigate the possibility of using matrix completion techniques to recover the entire score matrix from a sample of its entries.", "labels": [], "entities": []}, {"text": "The question we try to answer is whether the missing scores of one judge can be predicted knowing only scores of other judges and some shared ratings.", "labels": [], "entities": []}, {"text": "The contributions of this paper are twofold: i) we show how knowing the full score matrix instead of a single score for each example provides a more reliable estimation of a system quality (Section 3); ii) we present preliminary experiments showing that missing data techniques can be used to recover the score matrix from a sample of its entries despite the low inter-rater agreement (Section 4).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Correlation between the rankings induced  by the recovered matrix and the original score ma- trix for the CE corpus", "labels": [], "entities": []}, {"text": " Table 1: Completion performance as evaluated by the MAE for the three prediction methods and the  three corpora considered.", "labels": [], "entities": [{"text": "Completion", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9158378839492798}, {"text": "MAE", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.4794515073299408}]}]}