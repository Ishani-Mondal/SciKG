{"title": [], "abstractContent": [{"text": "What do we want to learn from a translation competition and how do we learn it with confidence?", "labels": [], "entities": [{"text": "translation competition", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.9249322712421417}]}, {"text": "We argue that a disproportionate focus on ranking competition participants has led to lots of different rankings, but little insight about which rankings we should trust.", "labels": [], "entities": []}, {"text": "In response, we provide the first framework that allows an empirical comparison of different analyses of competition results.", "labels": [], "entities": []}, {"text": "We then use this framework to compare several analytical models on data from the Workshop on Machine Translation (WMT).", "labels": [], "entities": [{"text": "Machine Translation (WMT)", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.8429437637329101}]}], "introductionContent": [], "datasetContent": [{"text": "We organized the competition data as described at the end of Section 4.", "labels": [], "entities": []}, {"text": "To compare the preference models, we did the following: \u2022 Randomly chose a subset of k comparisons from the training set, fork \u2208 {100, 200, 400, 800, 1600, 3200}.", "labels": [], "entities": []}, {"text": "12 \u2022 Trained the preference model on these comparisons.", "labels": [], "entities": []}, {"text": "\u2022 Evaluated the perplexity of the trained model on the test preferences, as described in Section 4.", "labels": [], "entities": []}, {"text": "For each model and training size, we averaged the perplexities from 5 trials of each competition track.", "labels": [], "entities": []}, {"text": "We then plotted average perplexity as a function of training size.", "labels": [], "entities": []}, {"text": "These graphs are shown If k was greater than the total number of training comparisons, then we took the entire set.", "labels": [], "entities": []}, {"text": "For WMT10 and WMT11, the best models were the IRT models, with the Gaussian parameterization converging the most rapidly and reaching the lowest perplexity.", "labels": [], "entities": [{"text": "WMT10", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.9075754284858704}, {"text": "WMT11", "start_pos": 14, "end_pos": 19, "type": "DATASET", "confidence": 0.9023264050483704}]}, {"text": "For WMT12, in which reference translations were excluded from the competition, four models were nearly indistinguishable: the two IRT models and the two averaged Independent Student models.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7961884140968323}]}, {"text": "This somewhat validates the organizers' decision to exclude the references, particularly given WMT's use of the BOJAR ranking heuristic (the nucleus of the Independent Student models) for its official rankings.", "labels": [], "entities": [{"text": "WMT", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.8930003643035889}, {"text": "BOJAR", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.9849698543548584}]}, {"text": "Results for WMT10 exclude the German-English and English-German tracks, since we used these to tune our model hyperparameters.", "labels": [], "entities": [{"text": "WMT10", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.8358466625213623}]}, {"text": "These were set as follows.", "labels": [], "entities": []}, {"text": "The Dirichlet strength for each baseline was 1.", "labels": [], "entities": [{"text": "Dirichlet strength", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.9851703941822052}]}, {"text": "For IRT-Gaussian: \u03c30 = 1.0, \u03c3 obs = 1.0, \u03c3a = 0.5, and the decision radius was 0.4.", "labels": [], "entities": [{"text": "IRT-Gaussian", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.4285432994365692}]}, {"text": "For IRT-Categorical: \u039b = 8, \u03c3 obs = 1.0, \u03b1a = 0.5, and the decision radius was 0.", "labels": [], "entities": [{"text": "IRT-Categorical", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.38870707154273987}]}, {"text": "The IRT models proved the most robust at handling judge noise.", "labels": [], "entities": []}, {"text": "We repeated the WMT10 experiment using the same test sets, but using the unfiltered crowdsourced comparisons (rather than \"expert\" 14 comparisons) for training.", "labels": [], "entities": [{"text": "WMT10 experiment", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.8784745633602142}]}, {"text": "Whereas the crowdsourced noise considerably degraded the Geometric Independent Students model, the IRT models were remarkably robust.", "labels": [], "entities": []}, {"text": "IRT-Gaussian in particular came close to replicating the performance of Geometric Independent Students trained on the much cleaner expert data.", "labels": [], "entities": [{"text": "IRT-Gaussian", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.813213050365448}]}, {"text": "This is rather impressive, since the crowdsourced judges agree only 46.6% of the time, compared to a 65.8% agreement rate among I.e., machine translation specialists.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.771702229976654}]}, {"text": "Another nice property of the IRT models is that they explicitly model student ability, so they yield a natural ranking.", "labels": [], "entities": [{"text": "IRT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8298554420471191}]}, {"text": "For training size 1600 of the WMT11 English-Czech track, (left) shows the mean student abilities learned by the IRT-Gaussian model.", "labels": [], "entities": [{"text": "WMT11 English-Czech track", "start_pos": 30, "end_pos": 55, "type": "DATASET", "confidence": 0.9222110311190287}]}, {"text": "The error bars show one standard deviation of the ability means (recall that we performed 5 trials, each with a random training subset of size 1600).", "labels": [], "entities": []}, {"text": "These results provide further insight into a case analyzed by, which raised concern about the relative ordering of online-B, cu-bojar, and cu-marecek.", "labels": [], "entities": []}, {"text": "According to IRT-Gaussian's analysis of the data, these three students are so close in ability that any ordering is essentially arbitrary.", "labels": [], "entities": [{"text": "IRT-Gaussian", "start_pos": 13, "end_pos": 25, "type": "DATASET", "confidence": 0.8420297503471375}]}, {"text": "Short of a full ranking, the analysis does suggest four strata.", "labels": [], "entities": []}, {"text": "Viewing one of IRT-Gaussian's induced preference models as a heatmap 15 (, four bands are discernable.", "labels": [], "entities": []}, {"text": "First, the reference sentences are clearly the darkest (best).", "labels": [], "entities": []}, {"text": "Next come students 2-7, followed by the slightly lighter (weaker) students 8-10, followed by the lightest (weakest) student 11.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Dataset sizes for each competition track  (number of comparisons).", "labels": [], "entities": []}]}