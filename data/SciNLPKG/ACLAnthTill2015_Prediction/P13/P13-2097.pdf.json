{"title": [{"text": "Reducing Annotation Effort for Quality Estimation via Active Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "Quality estimation models provide feedback on the quality of machine translated texts.", "labels": [], "entities": [{"text": "Quality estimation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6506047993898392}]}, {"text": "They are usually trained on human-annotated datasets, which are very costly due to its task-specific nature.", "labels": [], "entities": []}, {"text": "We investigate active learning techniques to reduce the size of these datasets and thus annotation effort.", "labels": [], "entities": []}, {"text": "Experiments on a number of datasets show that with as little as 25% of the training instances it is possible to obtain similar or superior performance compared to that of the complete datasets.", "labels": [], "entities": []}, {"text": "In other words, our active learning query strategies cannot only reduce annotation effort but can also result in better quality predictors.", "labels": [], "entities": []}], "introductionContent": [{"text": "The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations.", "labels": [], "entities": [{"text": "machine translation (MT) quality estimation (QE)", "start_pos": 15, "end_pos": 63, "type": "TASK", "confidence": 0.8638437002897262}]}, {"text": "This task is usually addressed with machine learning models trained on datasets composed of source sentences, their machine translations, and a quality label assigned by humans.", "labels": [], "entities": []}, {"text": "A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence.", "labels": [], "entities": []}, {"text": "Since quality scores for the training of QE models are given by human experts, the annotation process is costly and subject to inconsistencies due to the subjectivity of the task.", "labels": [], "entities": []}, {"text": "To avoid inconsistencies because of disagreements among annotators, it is often recommended that a QE model is trained for each translator, based on labels given by such a translator.", "labels": [], "entities": []}, {"text": "This further increases the annotation costs because different datasets are needed for different tasks.", "labels": [], "entities": []}, {"text": "Therefore, strategies to reduce the demand for annotated data are needed.", "labels": [], "entities": []}, {"text": "Such strategies can also bring the possibility of selecting data that is less prone to inconsistent annotations, resulting in more robust and accurate predictions.", "labels": [], "entities": []}, {"text": "In this paper we investigate Active Learning (AL) techniques to reduce the size of the dataset while keeping the performance of the resulting QE models.", "labels": [], "entities": []}, {"text": "AL provides methods to select informative data points from a large pool which, if labelled, can potentially improve the performance of a machine learning algorithm.", "labels": [], "entities": []}, {"text": "The rationale behind these methods is to help the learning algorithm achieve satisfactory results from only on a subset of the available data, thus incurring less annotation effort.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments using four MT datasets manually annotated for quality: English-Spanish (en-es): 2, 254 sentences translated by Moses (, as provided by the WMT12 Quality Estimation shared task.", "labels": [], "entities": [{"text": "WMT12 Quality Estimation shared task", "start_pos": 162, "end_pos": 198, "type": "TASK", "confidence": 0.5778019309043885}]}, {"text": "Effort scores range from 1 (too bad to be post-edited) to 5 (no post-editing needed).", "labels": [], "entities": [{"text": "Effort", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9832687377929688}]}, {"text": "Three expert post-editors evaluated each sentence and the final score was obtained by a weighted average between the three scores.", "labels": [], "entities": []}, {"text": "We use the default split given in the shared task: 1, 832 sentences for training and 432 for test.", "labels": [], "entities": []}, {"text": "French-English (fr-en): 2, 525 sentences translated by Moses as provided in Specia (2011), annotated by a single translator.", "labels": [], "entities": [{"text": "Specia (2011)", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.8451782315969467}]}, {"text": "Human labels indicate post-editing effort ranging from 1 (too bad to be post-edited) to 4 (little or no post-editing needed).", "labels": [], "entities": []}, {"text": "We use a random split of 90% sentences for training and 10% for test.", "labels": [], "entities": []}, {"text": "Arabic-English (ar-en): 2, 585 sentences translated by two state-of-the-art SMT systems (denoted ar-en-1 and ar-en-2), as provided in . A random split of 90% sentences for training and 10% for testis used.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9728637337684631}]}, {"text": "Human labels indicate the adequacy of the translation ranging from 1 (completely inadequate) to 4 (adequate).", "labels": [], "entities": []}, {"text": "These datasets were annotated by two expert translators.", "labels": [], "entities": []}, {"text": "To build our QE models, we extracted the 17 features used by the baseline approach in the WMT12 QE shared task.", "labels": [], "entities": [{"text": "WMT12 QE shared task", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.6548018157482147}]}, {"text": "1 These features were used with a Support Vector Regressor (SVR) with radial basis function and fixed hyperparameters (C=5, \u03b3=0.01, =0.5), using the Scikit-learn toolkit (Pedregosa et al., 2011).", "labels": [], "entities": []}, {"text": "For each dataset and each query method, we performed 20 active learning simulation experiments and averaged the results.", "labels": [], "entities": []}, {"text": "We started with 50 randomly selected sentences from the training set and used all the remaining training sentences as our query pool, adding one new sentence to the training set at each iteration.", "labels": [], "entities": []}, {"text": "Results were evaluated by measuring Mean Absolute Error (MAE) scores on the test set.", "labels": [], "entities": [{"text": "Mean Absolute Error (MAE) scores", "start_pos": 36, "end_pos": 68, "type": "METRIC", "confidence": 0.9348889759608677}]}, {"text": "We also performed an \"oracle\" experiment: at each iteration, it selects the instance that minimises the MAE on the test set.", "labels": [], "entities": [{"text": "MAE", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9926372766494751}]}, {"text": "The oracle results give an upper bound in performance for each test set.", "labels": [], "entities": []}, {"text": "Since an SVR does not supply variance values for its predictions, we employ a technique known as query-by-bagging ().", "labels": [], "entities": []}, {"text": "The idea is to build an ensemble of N SVRs trained on sub-samples of the training data.", "labels": [], "entities": []}, {"text": "When selecting anew query, the ensemble is able to return N predictions for each instance, from where a variance value can be inferred.", "labels": [], "entities": []}, {"text": "We used 20 SVRs as our ensemble and 20 as the size of each training sub-sample.", "labels": [], "entities": []}, {"text": "The variance values are then used as-is in the case of US strategy and combined with query densities in case of the ID strategy.", "labels": [], "entities": []}, {"text": "shows the learning curves for all query methods and all datasets.", "labels": [], "entities": []}, {"text": "The \"random\" curves are our baseline since they are equivalent to passive learning (with various numbers of instances).", "labels": [], "entities": []}, {"text": "We first evaluated our methods in terms of how many instances they needed to achieve 99% of the MAE score on the full dataset.", "labels": [], "entities": [{"text": "MAE score", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9515181481838226}]}, {"text": "For three datasets, the AL methods significantly outperformed the random selection baseline, while no improvement was observed on the ar-en-1 dataset.", "labels": [], "entities": [{"text": "ar-en-1 dataset", "start_pos": 134, "end_pos": 149, "type": "DATASET", "confidence": 0.694787323474884}]}], "tableCaptions": [{"text": " Table 1: Number (proportion) of instances needed to achieve 99% of the performance of the full dataset.  Bold-faced values indicate the best performing datasets.", "labels": [], "entities": []}, {"text": " Table 2: Best MAE scores obtained in the AL experiments. For each method, the first column shows the  number (proportion) of instances used to obtain the best MAE, the second column shows the MAE score  obtained and the third column shows the MAE score for random instance selection at the same number  of instances. The last column shows the MAE obtained using the full dataset. Best scores are shown in  bold and are significantly better (paired t-test, p < 0.05) than both their randomly selected counterparts  and the full dataset MAE.", "labels": [], "entities": [{"text": "MAE score", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.9112952947616577}, {"text": "MAE", "start_pos": 244, "end_pos": 247, "type": "METRIC", "confidence": 0.9031728506088257}]}]}