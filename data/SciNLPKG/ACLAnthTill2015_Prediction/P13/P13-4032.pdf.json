{"title": [{"text": "VSEM: An open library for visual semantics representation", "labels": [], "entities": [{"text": "VSEM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7615318894386292}]}], "abstractContent": [{"text": "VSEM is an open library for visual semantics.", "labels": [], "entities": [{"text": "VSEM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9487892389297485}]}, {"text": "Starting from a collection of tagged images, it is possible to automatically construct an image-based representation of concepts by using off-the-shelf VSEM functionalities.", "labels": [], "entities": []}, {"text": "VSEM is entirely written in MATLAB and its object-oriented design allows a large flexibility and reusability.", "labels": [], "entities": [{"text": "VSEM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.935509204864502}]}, {"text": "The software is accompanied by a website with supporting documentation and examples.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the last years we have witnessed great progress in the area of automated image analysis.", "labels": [], "entities": [{"text": "automated image analysis", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.615386833747228}]}, {"text": "Important advances, such as the introduction of local features fora robust description of the image content (see fora systematic review) and the bag-of-visual-words method (BoVW) 1 fora standard representation across multiple images, have contributed to make image analysis ubiquitous, with applications ranging from robotics to biology, from medicine to photography.", "labels": [], "entities": [{"text": "image analysis", "start_pos": 259, "end_pos": 273, "type": "TASK", "confidence": 0.7659939229488373}]}, {"text": "Two facts have played a key role in the rapid advance of these ideas.", "labels": [], "entities": []}, {"text": "First, the introduction of very well defined challenges which have been attracting also a wide community of \"outsiders\" specialized in a variety of disciplines (e.g., machine learning, neural networks, graphical models and natural language processing).", "labels": [], "entities": []}, {"text": "Second, the sharing of effective, well documented implementations of cutting edge image analysis algorithms, such as OpenCV and VLFeat.", "labels": [], "entities": [{"text": "cutting edge image analysis", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.6130508184432983}, {"text": "OpenCV", "start_pos": 117, "end_pos": 123, "type": "DATASET", "confidence": 0.9344009757041931}]}, {"text": "A comparable story can be told about automatic text analysis.", "labels": [], "entities": [{"text": "automatic text analysis", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.6606211364269257}]}, {"text": "The last decades have seen along series of successes in the processing of large text corpora in order to extract more or less structured semantic knowledge.", "labels": [], "entities": []}, {"text": "In particular, under the assumption that meaning can be captured by patterns of co-occurrences of words, distributional semantic models such as Latent Semantic Analysis or Topic Models ( ) have been shown to be very effective both in general semantic tasks such as approximating human intuitions about meaning, as well as in more application-driven tasks such as information retrieval, word disambiguation and query expansion.", "labels": [], "entities": [{"text": "approximating human intuitions about meaning", "start_pos": 265, "end_pos": 309, "type": "TASK", "confidence": 0.804630708694458}, {"text": "information retrieval", "start_pos": 363, "end_pos": 384, "type": "TASK", "confidence": 0.773101806640625}, {"text": "word disambiguation", "start_pos": 386, "end_pos": 405, "type": "TASK", "confidence": 0.7740500271320343}, {"text": "query expansion", "start_pos": 410, "end_pos": 425, "type": "TASK", "confidence": 0.7536603510379791}]}, {"text": "And also in the case of automated text analysis, a wide range of method implementations are at the disposal of the scientific community.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.7231860905885696}]}, {"text": "Nowadays, given the parallel success of the two disciplines, there is growing interest in making the visual and textual channels interact for mutual benefit.", "labels": [], "entities": []}, {"text": "If we look at the image analysis community, we discover a well established tradition of studies that exploit both channels of information.", "labels": [], "entities": [{"text": "image analysis", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7794542908668518}]}, {"text": "For example, there is a relatively extended amount of literature about enhancing the performance on visual tasks such as object recognition or image retrieval by replacing a purely image-based pipeline with hybrid methods augmented with textual information ().", "labels": [], "entities": [{"text": "object recognition", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7457157075405121}, {"text": "image retrieval", "start_pos": 143, "end_pos": 158, "type": "TASK", "confidence": 0.6729411035776138}]}, {"text": "Unfortunately, the same cannot be said of the exploitation of image analysis from within the text community.", "labels": [], "entities": [{"text": "image analysis", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.728090763092041}]}, {"text": "Despite the huge potential that automatically induced visual features could represent as anew source of perceptually grounded semantic knowledge, image-enhanced models of semantics developed so far) have only scratched this great potential and are still considered as proof-of-concept studies only.", "labels": [], "entities": []}, {"text": "One possible reason of this delay with respect to the image analysis community might be ascribed to the high entry barriers that NLP researchers adopting image analysis methods have to face.", "labels": [], "entities": [{"text": "image analysis community", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.8471789757410685}]}, {"text": "Although many of the image analysis toolkits are open source and well documented, they mainly address users within the same community and therefore their use is not as intuitive for others.", "labels": [], "entities": [{"text": "image analysis", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.8423135876655579}]}, {"text": "The final goal of libraries such VLFeat and OpenCV is the representation and classification of images.", "labels": [], "entities": [{"text": "VLFeat", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.8617693781852722}, {"text": "OpenCV", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.9013340473175049}, {"text": "representation and classification of images", "start_pos": 58, "end_pos": 101, "type": "TASK", "confidence": 0.716141265630722}]}, {"text": "Therefore, they naturally lack of a series of complementary functionalities that are necessary to bring the visual representation to the level of semantic concepts.", "labels": [], "entities": []}, {"text": "To fill the gap we just described, we present hereby VSEM, 7 a novel toolkit which allows the extraction of image-based representations of concepts in an easy fashion.", "labels": [], "entities": [{"text": "VSEM, 7", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.7755140264829}]}, {"text": "VSEM is equipped with state-of-the-art algorithms, from low-level feature detection and description up to the BoVW representation of images, together with a set of new routines necessary to move from an image-wise to a concept-wise representation of image content.", "labels": [], "entities": [{"text": "VSEM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9255179762840271}, {"text": "feature detection and description", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.7233435660600662}]}, {"text": "Ina nutshell, VSEM extracts visual information in away that resembles how it is done for automatic text analysis.", "labels": [], "entities": [{"text": "VSEM extracts visual information", "start_pos": 14, "end_pos": 46, "type": "TASK", "confidence": 0.6664465814828873}, {"text": "text analysis", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.6618761718273163}]}, {"text": "Thanks to BoVW, the image content is indeed discretized and visual units somehow comparable to words in text are produced (the visual words).", "labels": [], "entities": [{"text": "BoVW", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.9374873638153076}]}, {"text": "In this way, from a corpus of images annotated with a set of concepts, it is possible to derive semantic vectors of co-occurrence counts of concepts and visual words akin to the representations of words in terms of textual collocates in standard distributional semantics.", "labels": [], "entities": []}, {"text": "Impor- In recent years, a conspicuous literature of studies has surfaced, wherein demonstration was made of how text based models are not sufficiently good at capturing the environment we acquire language from.", "labels": [], "entities": []}, {"text": "This is due to the fact that they are lacking of perceptual information (.", "labels": [], "entities": []}, {"text": "The authors of the aforementioned studies usually refer to words instead of concepts.", "labels": [], "entities": []}, {"text": "We chose to call them concepts to account for the both theoretical and practical differences standing between a word and the perceptual information it brings along, which we define its concept.", "labels": [], "entities": []}, {"text": "7 http://clic.cimec.unitn.it/vsem/ tantly, the obtained visual semantic vectors can be easily combined with more traditional text-based vectors to arrive at a multimodal representation of meaning (see e.g. (. It has been shown that the resulting multimodal models perform better than text-only models in semantic tasks such as approximating semantic similarity and relatedness).", "labels": [], "entities": [{"text": "approximating semantic similarity", "start_pos": 327, "end_pos": 360, "type": "TASK", "confidence": 0.8263304233551025}]}, {"text": "VSEM functionalities concerning image analysis is based on VLFeat (.", "labels": [], "entities": [{"text": "VSEM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7544147968292236}, {"text": "image analysis", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7571361660957336}, {"text": "VLFeat", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.9043740034103394}]}, {"text": "This guarantees that the image analysis underpinnings of the library are well maintained and state-of-the-art.", "labels": [], "entities": [{"text": "image analysis", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7221910953521729}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we introduce the procedure to obtain an image-based representation of a concept.", "labels": [], "entities": []}, {"text": "Section 3 describes the VSEM architecture.", "labels": [], "entities": []}, {"text": "Section 4 shows how to install and run VSEM through an example that uses the Pascal VOC data set.", "labels": [], "entities": [{"text": "Pascal VOC data set", "start_pos": 77, "end_pos": 96, "type": "DATASET", "confidence": 0.7293963730335236}]}, {"text": "Section 5 concludes summarizing the material and discussing further directions.", "labels": [], "entities": [{"text": "summarizing", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9702793955802917}]}], "datasetContent": [], "tableCaptions": []}