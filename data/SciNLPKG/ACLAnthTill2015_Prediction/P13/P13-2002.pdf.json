{"title": [{"text": "Exact Maximum Inference for the Fertility Hidden Markov Model", "labels": [], "entities": []}], "abstractContent": [{"text": "The notion of fertility in word alignment (the number of words emitted by a single state) is useful but difficult to model.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7604804635047913}]}, {"text": "Initial attempts at modeling fertility used heuristic search methods.", "labels": [], "entities": []}, {"text": "Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation.", "labels": [], "entities": []}, {"text": "Yet in practice we also need the single best alignment, which is difficult to find using Gibbs.", "labels": [], "entities": []}, {"text": "Building on recent advances in dual decomposition, this paper introduces an exact algorithm for finding the single best alignment with a fertility HMM.", "labels": [], "entities": [{"text": "dual decomposition", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9406057894229889}]}, {"text": "Finding the best alignment appears important , as this model leads to a substantial improvement in alignment quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word-based translation models intended to model the translation process have found new uses identifying word correspondences in sentence pairs.", "labels": [], "entities": [{"text": "Word-based translation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6416680961847305}]}, {"text": "These word alignments area crucial training component inmost machine translation systems.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.7557114064693451}, {"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7192181348800659}]}, {"text": "Furthermore, they are useful in other NLP applications, such as entailment identification.", "labels": [], "entities": [{"text": "entailment identification", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.9338079988956451}]}, {"text": "The simplest models may use lexical information alone.", "labels": [], "entities": []}, {"text": "The seminal Model 1 () has proved very powerful, performing nearly as well as more complicated models in some phrasal systems (.", "labels": [], "entities": []}, {"text": "With minor improvements to initialization) (which maybe important (), it can be quite competitive.", "labels": [], "entities": [{"text": "initialization", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.9867959022521973}]}, {"text": "Subsequent IBM models include more detailed information about context.", "labels": [], "entities": []}, {"text": "Models 2 and 3 incorporate a positional model based on the absolute position of the word; Models 4 and 5 use a relative position model instead (an English word tends to align to a French word that is nearby the French word aligned to the previous English word).", "labels": [], "entities": []}, {"text": "Models 3, 4, and 5 all incorporate a notion of \"fertility\": the number of French words that align to any English word.", "labels": [], "entities": []}, {"text": "Although these latter models covered abroad range of phenomena, estimation techniques and MAP inference were challenging.", "labels": [], "entities": [{"text": "estimation", "start_pos": 64, "end_pos": 74, "type": "TASK", "confidence": 0.9832752346992493}, {"text": "MAP inference", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.8267938494682312}]}, {"text": "The authors originally recommended heuristic procedures based on local search for both.", "labels": [], "entities": []}, {"text": "Such methods work reasonably well, but can be computationally inefficient and have few guarantees.", "labels": [], "entities": []}, {"text": "Thus, many researchers have switched to the HMM model ( and variants with more parameters.", "labels": [], "entities": []}, {"text": "This captures the positional information in the IBM models in a framework that admits exact parameter estimation inference, though the objective function is not concave: local maxima area concern.", "labels": [], "entities": []}, {"text": "Modeling fertility is challenging in the HMM framework as it violates the Markov assumption.", "labels": [], "entities": []}, {"text": "Where the HMM jump model considers only the prior state, fertility requires looking across the whole state space.", "labels": [], "entities": []}, {"text": "Therefore, the standard forward-backward and Viterbi algorithms do not apply.", "labels": [], "entities": []}, {"text": "Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.", "labels": [], "entities": []}, {"text": "However, they do not have a efficient means of MAP inference, which is necessary in many applications such as machine translation.", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9799054265022278}, {"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.8114262223243713}]}, {"text": "This paper introduces a method for exact MAP inference with the fertility HMM using dual decomposition.", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.9244443774223328}]}, {"text": "The resulting model leads to substantial improvements in alignment quality.", "labels": [], "entities": [{"text": "alignment", "start_pos": 57, "end_pos": 66, "type": "TASK", "confidence": 0.979824960231781}]}], "datasetContent": [{"text": "We explore the impact of this improved MAP inference procedure on a task in German-English word alignment.", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.906390368938446}, {"text": "German-English word alignment", "start_pos": 76, "end_pos": 105, "type": "TASK", "confidence": 0.5560246706008911}]}, {"text": "For training data we use the news commentary data from the WMT 2012 translation task.", "labels": [], "entities": [{"text": "WMT 2012 translation task", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.6925004720687866}]}, {"text": "1 120 of the training sentences were manually annotated with word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.6963913291692734}]}, {"text": "The results in compare several different algorithms on this same data.", "labels": [], "entities": []}, {"text": "The first line is a baseline HMM using exact posterior computation and inference with the standard dynamic programming algorithms.", "labels": [], "entities": []}, {"text": "The next line shows the fertility HMM with approximate posterior computation from Gibbs sampling but with final alignment selected by the Viterbi algorithm.", "labels": [], "entities": []}, {"text": "Clearly fertility modeling is improving alignment quality.", "labels": [], "entities": []}, {"text": "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (.", "labels": [], "entities": []}, {"text": "Here, however, the difference between a dual decomposition and Viterbi is significant: their results were likely due to search error.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results over the 120 evalu- ation sentences. Alignment error rates in both di- rections are provided here.", "labels": [], "entities": [{"text": "Alignment error", "start_pos": 68, "end_pos": 83, "type": "METRIC", "confidence": 0.8434489071369171}]}]}