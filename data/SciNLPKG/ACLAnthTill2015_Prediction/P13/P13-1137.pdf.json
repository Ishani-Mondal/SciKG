{"title": [{"text": "Domain-Independent Abstract Generation for Focused Meeting Summarization", "labels": [], "entities": [{"text": "Summarization", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.5180732011795044}]}], "abstractContent": [{"text": "We address the challenge of generating natural language abstractive summaries for spoken meetings in a domain-independent fashion.", "labels": [], "entities": []}, {"text": "We apply Multiple-Sequence Alignment to induce abstract generation templates that can be used for different domains.", "labels": [], "entities": [{"text": "Multiple-Sequence Alignment", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.7725217044353485}]}, {"text": "An Overgenerate-and-Rank strategy is utilized to produce and rank candidate abstracts.", "labels": [], "entities": []}, {"text": "Experiments using in-domain and out-of-domain training on disparate corpora show that our system uniformly outperforms state-of-the-art supervised extract-based approaches.", "labels": [], "entities": []}, {"text": "In addition, human judges rate our system summaries significantly higher than compared systems in fluency and overall quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Meetings area common way to collaborate, share information and exchange opinions.", "labels": [], "entities": []}, {"text": "Consequently, automatically generated meeting summaries could be of great value to people and businesses alike by providing quick access to the essential content of past meetings.", "labels": [], "entities": []}, {"text": "Focused meeting summaries have been proposed as particularly useful; in contrast to summaries of a meeting as a whole, they refer to summaries of a specific aspect of a meeting, such as the DECISIONS reached, PROBLEMS discussed, PROGRESS made or AC-TION ITEMS that emerged).", "labels": [], "entities": []}, {"text": "Our goal is to provide an automatic summarization system that can generate abstract-style focused meeting summaries to help users digest the vast amount of meeting content in an easy manner.", "labels": [], "entities": [{"text": "summarization", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9249890446662903}]}, {"text": "Existing meeting summarization systems remain largely extractive: their summaries are comprised exclusively of patchworks of utterances selected directly from the meetings to be summarized (.", "labels": [], "entities": []}, {"text": "Although relatively easy to construct, extractive approaches fall short of producing concise and readable summaries, largely due In contrast, human-written meeting summaries are typically in the form of abstracts -distillations of the original conversation written in new language.", "labels": [], "entities": []}, {"text": "A user study from showed that people demonstrate a strong preference for abstractive summaries over extracts when the text to be summarized is conversational.", "labels": [], "entities": []}, {"text": "Consider, for example, the two types of focused summary along with their associated dialogue snippets in.", "labels": [], "entities": []}, {"text": "We can see that extracts are likely to include unnecessary and noisy information from the meeting transcripts.", "labels": [], "entities": []}, {"text": "On the contrary, the manually composed summaries (abstracts) are more compact and readable, and are written in a distinctly non-conversational style.", "labels": [], "entities": []}, {"text": "To address the limitations of extract-based summaries, we propose a complete and fully automatic domain-independent abstract generation framework for focused meeting summarization.", "labels": [], "entities": [{"text": "summaries", "start_pos": 44, "end_pos": 53, "type": "TASK", "confidence": 0.8763856887817383}, {"text": "focused meeting summarization", "start_pos": 150, "end_pos": 179, "type": "TASK", "confidence": 0.47248371442159015}]}, {"text": "Following existing language generation research, we first perform content selection: given the dialogue acts relevant to one element of the meeting (e.g. a single decision or problem), we train a classifier to identify summary-worthy phrases.", "labels": [], "entities": [{"text": "language generation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7581183910369873}, {"text": "content selection", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7338935434818268}]}, {"text": "Next, we develop an \"overgenerate-and-rank\" strategy ( for surface realization, which generates and ranks candidate sentences for the abstract.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7765829265117645}]}, {"text": "After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element.", "labels": [], "entities": []}, {"text": "As described in subsequent sections, the generation framework allows us to identify and reformulate the important information for the focused summary.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 To the best of our knowledge, our system is the first fully automatic system to generate natural language abstracts for spoken meetings.", "labels": [], "entities": []}, {"text": "\u2022 We present a novel template extraction algorithm, based on Multiple Sequence Alignment (MSA) (, to induce domain-independent templates that guide abstract generation.", "labels": [], "entities": [{"text": "template extraction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.72004134953022}, {"text": "Multiple Sequence Alignment (MSA)", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.7136824826399485}]}, {"text": "MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs () and has also been employed for learning paraphrases ().", "labels": [], "entities": []}, {"text": "\u2022 Although our framework requires labeled training data for each type of focused summary (decisions, problems, etc.), we also make initial tries for domain adaptation so that our summarization method does not need human-written abstracts for each new meeting domain (e.g. faculty meetings, theater group meetings, project group meetings).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 149, "end_pos": 166, "type": "TASK", "confidence": 0.7681722044944763}]}, {"text": "We instantiate the abstract generation framework on two corpora from disparate domains -the AMI Meeting Corpus () and ICSI Meeting Corpus ( -and produce systems to generate focused summaries with regard to four types of meeting elements: DECISIONs, PROBLEMs, AC-TION ITEMSs, and PROGRESS.", "labels": [], "entities": [{"text": "AMI Meeting Corpus", "start_pos": 92, "end_pos": 110, "type": "DATASET", "confidence": 0.907182772954305}, {"text": "ICSI Meeting Corpus", "start_pos": 118, "end_pos": 137, "type": "DATASET", "confidence": 0.8986010551452637}]}, {"text": "Automatic evaluation (using ROUGE ( and BLEU ()) against manually generated focused summaries shows that our summarizers uniformly and statistically significantly outperform two baseline systems as well as a state-of-the-art supervised extraction-based system.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9912576675415039}, {"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9985509514808655}]}, {"text": "Human evaluation also indicates that the abstractive summaries produced by our systems are more linguistically appealing than those of the utterance-level extraction-based system, preferring them over summaries from the extractionbased system of comparable semantic correctness (62.3% vs. 37.7%).", "labels": [], "entities": []}, {"text": "Finally, we examine the generality of our model across domains for two types of focused summarization -decisions and problems -by training the summarizer on out-of-domain data (i.e. the AMI corpus for use on the ICSI meeting data, and vice versa).", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 186, "end_pos": 196, "type": "DATASET", "confidence": 0.9050959646701813}, {"text": "ICSI meeting data", "start_pos": 212, "end_pos": 229, "type": "DATASET", "confidence": 0.9397428631782532}]}, {"text": "The resulting systems yield results comparable to those from the same system trained on in-domain data, and statistically significantly outperform supervised extractive summarization approaches trained on in-domain data.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two disparate corpora are used for evaluation.", "labels": [], "entities": []}, {"text": "The AMI meeting corpus) contains 139 scenario-driven meetings, where groups of four people participate in a series of four meetings fora fictitious project of designing remote control.", "labels": [], "entities": [{"text": "AMI meeting corpus", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9027221202850342}]}, {"text": "The ICSI meeting corpus ( consists of 75 naturally occurring meetings, each of them has 4 to 10 participants.", "labels": [], "entities": [{"text": "ICSI meeting corpus", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9124165972073873}]}, {"text": "Compared to the fabricated topics in AMI, the conversations in ICSI tend to be specialized and technical, e.g. discussion about speech and language technology.", "labels": [], "entities": []}, {"text": "We use 57 meetings in ICSI and 139 meetings in AMI that include a short (usually one-sentence), manually constructed abstract summarizing each important output for every meeting.", "labels": [], "entities": [{"text": "ICSI", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.9293259382247925}, {"text": "AMI", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.9116599559783936}]}, {"text": "Decision and problem summaries are annotated for both corpora.", "labels": [], "entities": [{"text": "Decision and problem summaries", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.611736349761486}]}, {"text": "AMI has extra action item summaries, and ICSI has progress summaries.", "labels": [], "entities": [{"text": "AMI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9487043619155884}, {"text": "ICSI", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.4789678752422333}]}, {"text": "The set of dialogue acts that support each abstract are annotated as such.", "labels": [], "entities": []}, {"text": "We consider two system input settings.", "labels": [], "entities": []}, {"text": "In the True Clusterings setting, we use the annotations to create perfect partitions of the DAs for input to the system; in the System Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in ().", "labels": [], "entities": []}, {"text": "DAs are grouped according to a classifier trained beforehand.", "labels": [], "entities": [{"text": "DAs", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8551074266433716}]}, {"text": "We compare our system with (1) two unsupervised baselines, (2) two supervised extractive approaches, and (3) an oracle derived from the gold standard abstracts.", "labels": [], "entities": []}, {"text": "As in, the LONGEST DA in each cluster is selected as the summary.", "labels": [], "entities": [{"text": "LONGEST DA", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.7479516863822937}]}, {"text": "The second baseline picks the cluster prototype (i.e. the DA with the largest TF-IDF similarity with the cluster centroid) as the summary according to.", "labels": [], "entities": []}, {"text": "Although it is possible that important content is spread over multiple DAs, both baselines allow us to determine summary quality when summaries are restricted to a single utterance.", "labels": [], "entities": []}, {"text": "We also compare our approach to two supervised extractive summarization methods -Support Vector Machines) trained with the same fea-tures as our system (see) to identify the important DAs (no syntax features) ( to include into the summary . Oracle.", "labels": [], "entities": []}, {"text": "We compute an oracle consisting of the words from the DA cluster that also appear in the associated abstract to reflect the gap between the best possible extracts and the human abstracts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Domain adaptation evaluation. Systems trained on out-of-domain data are denoted with \"(OUT)\", oth-", "labels": [], "entities": [{"text": "Domain adaptation evaluation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8998557925224304}, {"text": "OUT", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9815525412559509}]}, {"text": " Table 4: Human evaluation results of Fluency and Se-", "labels": [], "entities": [{"text": "Se-", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.7086591273546219}]}]}