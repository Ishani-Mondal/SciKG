{"title": [{"text": "Towards Accurate Distant Supervision for Relational Facts Extraction", "labels": [], "entities": [{"text": "Accurate Distant Supervision", "start_pos": 8, "end_pos": 36, "type": "TASK", "confidence": 0.8178236285845438}, {"text": "Relational Facts Extraction", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.8199416399002075}]}], "abstractContent": [{"text": "Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus.", "labels": [], "entities": [{"text": "Distant supervision (DS)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7485076367855072}]}, {"text": "However, the accuracy is still not satisfying.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9997270703315735}]}, {"text": "In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles.", "labels": [], "entities": [{"text": "DS", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9691461324691772}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9988586902618408}, {"text": "valid entity type detection", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.6258906275033951}, {"text": "negative training examples construction", "start_pos": 144, "end_pos": 183, "type": "TASK", "confidence": 0.6064420118927956}]}, {"text": "We propose an approach to handle these factors.", "labels": [], "entities": []}, {"text": "By experimenting on Wikipedia articles to extract the facts in Freebase (the top 92 relations), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9734610915184021}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.998969554901123}]}], "introductionContent": [{"text": "Recently there are great efforts on building large structural knowledge bases (KB) such as Freebase, Yago, etc.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.9828366637229919}, {"text": "Yago", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.8760965466499329}]}, {"text": "They are composed of relational facts often represented in the form of a triplet, (SrcEntity, Relation, DstEntity), such as \"(Bill Gates, BornIn, Seattle)\".", "labels": [], "entities": []}, {"text": "An important task is to enrich such KBs by extracting more facts from text.", "labels": [], "entities": []}, {"text": "Specifically, this paper focuses on extracting facts for existing relations.", "labels": [], "entities": []}, {"text": "This is different from OpenIE () which needs to discover new relations.", "labels": [], "entities": []}, {"text": "Given large amounts of labeled sentences, supervised methods are able to achieve good performance ().", "labels": [], "entities": []}, {"text": "However, it is difficult to handle large scale corpus due to the high cost of labeling.", "labels": [], "entities": []}, {"text": "Recently an approach called distant supervision (DS) () was proposed, which does not require any labels on the text.", "labels": [], "entities": [{"text": "distant supervision (DS)", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.5720074534416199}]}, {"text": "It treats the extraction problem as classifying * The contact author.", "labels": [], "entities": []}, {"text": "a candidate entity pair to a relation.", "labels": [], "entities": []}, {"text": "Then an existing fact in a KB can be used as a labeled example whose label is the relation name.", "labels": [], "entities": []}, {"text": "Then the features of all the sentences (from a given text corpus) containing the entity pair are merged as the feature of the example.", "labels": [], "entities": []}, {"text": "Finally a multi-class classifier is trained.", "labels": [], "entities": []}, {"text": "However, the accuracy of DS is not satisfying.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9997162222862244}, {"text": "DS", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.5483325719833374}]}, {"text": "Some variants have been proposed to improve the performance ().", "labels": [], "entities": []}, {"text": "They argue that DS introduces a lot of noise into the training data by merging the features of all the sentences containing the same entity pair, because a sentence containing the entity pair of a relation may not talk about the relation. and introduce hidden variables to indicate whether a sentence is noise and try to infer them from the data.", "labels": [], "entities": []}, {"text": "design a generative model to identify noise patterns.", "labels": [], "entities": []}, {"text": "However, as shown in the experiments (Section 4), the above variants do not lead to much improvement inaccuracy.", "labels": [], "entities": []}, {"text": "In this paper, we point out and analyze some critical factors in DS which have great impact on the accuracy but has not been touched or well handled before.", "labels": [], "entities": [{"text": "DS", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9876492619514465}, {"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9993413090705872}]}, {"text": "First, each relation has its own schema definition, i.e., the source entity and the destination entity should be of valid types, which is overlooked in DS.", "labels": [], "entities": []}, {"text": "Therefore, we propose a component of entity type detection to check it.", "labels": [], "entities": [{"text": "entity type detection", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.7220037778218588}]}, {"text": "Second, DS introduces many false negative examples into the training set and we propose anew method to construct negative training examples.", "labels": [], "entities": []}, {"text": "Third, we find it is difficult fora single classifier to achieve high accuracy and hence we train multiple classifiers and ensemble them.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9983649849891663}]}, {"text": "We also notice that and utilize external information such as more facts from Yago and labeled sentences from ACE to improve the performance.", "labels": [], "entities": [{"text": "Yago", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.9320211410522461}, {"text": "ACE", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.8567792177200317}]}, {"text": "These methods can also be equipped with the approach proposed in this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We aimed to extract facts of the 92 most frequent relations in Freebase 2009.", "labels": [], "entities": [{"text": "Freebase 2009", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.9798804223537445}]}, {"text": "The facts of each relation were equally split to two parts for training and testing.", "labels": [], "entities": []}, {"text": "Wikipedia 2009 was used as the target corpus, where 800,000 articles were used for training and 400,000 for testing.", "labels": [], "entities": [{"text": "Wikipedia 2009", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9435077905654907}]}, {"text": "During the NED phrase, there are 94 unique entity types (they are also relations in Freebase) for the source and destination entities.", "labels": [], "entities": []}, {"text": "Note that some entity types contain too few entities and they are discarded.", "labels": [], "entities": []}, {"text": "We used 500,000 Wikipedia articles (2,000,000 sentences) for generating training data for the NED component.", "labels": [], "entities": []}, {"text": "We used Open NLP POS tagger, Standford NER () and MaltParser () to label/tag sentences.", "labels": [], "entities": [{"text": "Standford NER", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.8898526430130005}]}, {"text": "We employed liblinear as classifiers for NED and relation extraction and the solver is L2LR.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8029570579528809}]}], "tableCaptions": []}