{"title": [{"text": "Additive Neural Networks for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.8327701886494955}]}], "abstractContent": [{"text": "Most statistical machine translation (SMT) systems are modeled using a log-linear framework.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 5, "end_pos": 42, "type": "TASK", "confidence": 0.7925003717343012}]}, {"text": "Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interpreted to reach their potential.", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9956865310668945}]}, {"text": "A neural network is a reasonable method to address these pitfalls.", "labels": [], "entities": []}, {"text": "However, modeling SMT with a neural network is not trivial, especially when taking the decoding efficiency into consideration.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9958080053329468}]}, {"text": "In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9950666427612305}]}, {"text": "In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector.", "labels": [], "entities": []}, {"text": "Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks.", "labels": [], "entities": [{"text": "Japanese-to-English translation tasks", "start_pos": 114, "end_pos": 151, "type": "TASK", "confidence": 0.7515940566857656}]}], "introductionContent": [{"text": "Recently, great progress has been achieved in SMT, especially since proposed the log-linear model: almost all the stateof-the-art SMT systems are based on the log-linear model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9971064925193787}, {"text": "SMT", "start_pos": 130, "end_pos": 133, "type": "TASK", "confidence": 0.9704155921936035}]}, {"text": "Its most important advantage is that arbitrary features can be added to the model.", "labels": [], "entities": []}, {"text": "Thus, it casts complex translation between a pair of languages as feature engineering, which facilitates research and development for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9929682612419128}]}, {"text": "Regardless of how successful the log-linear model is in SMT, it still has some shortcomings.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9962020516395569}]}, {"text": "This joint work was done while the first author visited NICT.", "labels": [], "entities": [{"text": "NICT", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9685127139091492}]}, {"text": "On the one hand, features are required to be linear with respect to the objective of the translation model (), but it is not guaranteed that the potential features be linear with the model.", "labels": [], "entities": []}, {"text": "This induces modeling inadequacy, in which the translation performance may not improve, or may even decrease, after one integrates additional features into the model.", "labels": [], "entities": []}, {"text": "On the other hand, it cannot deeply interpret its surface features, and thus cannot efficiently develop the potential of these features.", "labels": [], "entities": []}, {"text": "What may happen is that a feature p does initially not improve the translation performance, but after a nonlinear operation, e.g. log(p), it does.", "labels": [], "entities": [{"text": "translation", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.955351710319519}]}, {"text": "The reason is not because this feature is useless but the model does not efficiently interpret and represent it.", "labels": [], "entities": []}, {"text": "Situations such as this confuse explanations for feature designing, since it is unclear whether such a feature contributes to a translation or not.", "labels": [], "entities": [{"text": "feature designing", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.8017631471157074}]}, {"text": "A neural network) is a reasonable method to overcome the above shortcomings.", "labels": [], "entities": []}, {"text": "However, it should take constraints, e.g. the decoding efficiency, into account in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9862592220306396}]}, {"text": "Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search).", "labels": [], "entities": [{"text": "SMT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.973658561706543}]}, {"text": "In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model.", "labels": [], "entities": []}, {"text": "Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem.", "labels": [], "entities": []}, {"text": "Actually, even for the (log-) linear model, efficient decoding with the language model is not trivial.", "labels": [], "entities": []}, {"text": "In this paper, we propose a variant of neural networks, i.e. additive neural networks (see Section 3 for details), for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.994293749332428}]}, {"text": "It consists of two components: a linear component which captures nonlocal (or state dependent) features and a non-linear component (i.e., neural nework) which encodes lo-X te \u0084 X \u00cb}\u00a8\\\u00cb}\u00a8\\ X X friendly cooperation over the last years: A bilingual tree with two synchronous rules, r 1 : X \u2192 \u00cb}\u00a8\\\u00cb}\u00a8\\;friendly cooperation and r 2 : X \u2192 te \u0084 X; X over the last years.", "labels": [], "entities": []}, {"text": "The inside rectangle denotes the partial derivation d 1 = {r 1 } with the partial translation e 1 =\"friendly cooperation\", and the outside rectangle denotes the derivation d 2 = {r 1 , r 2 } with the translation e 2 =\"friendly cooperation over the last years\".", "labels": [], "entities": []}, {"text": "cal (or state independent) features.", "labels": [], "entities": []}, {"text": "Compared with the log-linear model, it has more powerful expressive abilities and can deeply interpret and represent features with hidden units in neural networks.", "labels": [], "entities": []}, {"text": "Moreover, our method is simple to implement and its decoding efficiency is comparable to that of the log-linear model.", "labels": [], "entities": []}, {"text": "We also integrate word embedding into the model by representing each word as a feature vector.", "labels": [], "entities": []}, {"text": "Because of the thousands of parameters and the non-convex objective in our model, efficient training is not simple.", "labels": [], "entities": []}, {"text": "We propose an efficient training methodology: we apply the mini-batch conjugate sub-gradient algorithm () to accelerate the training; we also propose pre-training and post-training methods to avoid poor local minima.", "labels": [], "entities": []}, {"text": "The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model).", "labels": [], "entities": []}, {"text": "On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation.", "labels": [], "entities": [{"text": "Japanese-toEnglish translation tasks", "start_pos": 31, "end_pos": 67, "type": "TASK", "confidence": 0.6514436304569244}]}], "datasetContent": [{"text": "We conduct our experiments on the Chinese-toEnglish and Japanese-to-English translation tasks.", "labels": [], "entities": [{"text": "Japanese-to-English translation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.6648224145174026}]}, {"text": "For the Chinese-to-English task, the training data is the FBIS corpus (news domain) with about 240k sentence pairs; the development set is the NIST02 evaluation data; the development test set is NIST05; and the test datasets are NIST06, and NIST08.", "labels": [], "entities": [{"text": "FBIS corpus (news domain)", "start_pos": 58, "end_pos": 83, "type": "DATASET", "confidence": 0.9141110281149546}, {"text": "NIST02 evaluation data", "start_pos": 143, "end_pos": 165, "type": "DATASET", "confidence": 0.9058988094329834}, {"text": "NIST05", "start_pos": 195, "end_pos": 201, "type": "DATASET", "confidence": 0.9853046536445618}, {"text": "NIST06", "start_pos": 229, "end_pos": 235, "type": "DATASET", "confidence": 0.9861499071121216}, {"text": "NIST08", "start_pos": 241, "end_pos": 247, "type": "DATASET", "confidence": 0.9900784492492676}]}, {"text": "For the Japanese-to-English task, the training data with 300k sentence pairs is from the NTCIR-patent task; the development set, development test set, and two test sets are averagely extracted from a given development set with 4000 sentences, and these four datasets are called test1, test2, test3 and test4, respectively.", "labels": [], "entities": [{"text": "NTCIR-patent", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.9044391512870789}]}, {"text": "We run GIZA++) on the training corpus in both directions () to obtain the word alignment for each sentence pair.", "labels": [], "entities": []}, {"text": "Using the SRILM Toolkits) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data.", "labels": [], "entities": [{"text": "SRILM Toolkits", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.8048337996006012}, {"text": "English Gigaword corpus", "start_pos": 155, "end_pos": 178, "type": "DATASET", "confidence": 0.8128206332524618}]}, {"text": "In our experiments, the translation performances are measured by case-sensitive BLEU4 metric 4 ().", "labels": [], "entities": [{"text": "BLEU4 metric 4", "start_pos": 80, "end_pos": 94, "type": "METRIC", "confidence": 0.9678073724110922}]}, {"text": "The significance testing is performed by paired bootstrap re-sampling.", "labels": [], "entities": []}, {"text": "We use an in-house developed hierarchical phrase-based translation) for our baseline system, which shares the similar setting as Hiero (), e.g. beam-size=100, kbest-size=100, and is denoted as L-Hiero to emphasize its log-linear model.", "labels": [], "entities": []}, {"text": "We tune L-Hiero with two methods MERT and PRO implemented in the Moses toolkit.", "labels": [], "entities": [{"text": "MERT", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9943881630897522}, {"text": "PRO", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9462434649467468}]}, {"text": "On the same experiment settings, the performance of L-Hiero is comparable Seconds/Sent L-Hiero 1.77 AdNN-Hiero-E 1.88: The decoding time comparison on NIST05 between L-Hiero and AdNN-Hiero-E. to that of Moses: on the NIST05 test set, L-Hiero achieves 25.1 BLEU scores and Moses achieves 24.8.", "labels": [], "entities": [{"text": "Seconds/Sent L-Hiero 1.77 AdNN-Hiero-E 1.88", "start_pos": 74, "end_pos": 117, "type": "METRIC", "confidence": 0.675995260477066}, {"text": "NIST05", "start_pos": 151, "end_pos": 157, "type": "DATASET", "confidence": 0.9768496155738831}, {"text": "NIST05 test set", "start_pos": 217, "end_pos": 232, "type": "DATASET", "confidence": 0.982465406258901}, {"text": "BLEU", "start_pos": 256, "end_pos": 260, "type": "METRIC", "confidence": 0.9991494417190552}]}, {"text": "Further, we integrate the embedding features (See Section 3.3) into the log-linear model along with the default features as L-Hiero, which is called L-Hiero-E. Since L-Hiero-E has hundreds of features, we use PRO as its tuning toolkit.", "labels": [], "entities": []}, {"text": "AdNN-Hiero-E is our implementation of the AddNN model with embedding features, as discussed in Section 3, and it shares the same codebase and settings as L-Hiero.", "labels": [], "entities": []}, {"text": "We adopt the following setting for training AdNN-Hiero-E: u=10; batch-size=1000 and CGiter=3, as referred in), and T =200 in Algorithm 1; the pre-training and post-training methods as PRO; the regularizer \u03bb in Eq.", "labels": [], "entities": []}, {"text": "(6) as 10 and 30, and M axIter as 16 and 20 in Algorithm 2, for Chinese-to-English and Japanese-to-English tasks, respectively.", "labels": [], "entities": []}, {"text": "Although there are several parameters in AdNN which may limit its practicability, according to many of our internal studies, most parameters are insensitive to AdNN except \u03bb and M axIter, which are common in other tuning toolkits such as MIRA and can be tuned 5 on a development test dataset.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 238, "end_pos": 242, "type": "DATASET", "confidence": 0.7991181015968323}]}, {"text": "Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by for fairer comparisons.", "labels": [], "entities": [{"text": "MERT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.5608634948730469}, {"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9988403916358948}]}, {"text": "For AdNN, we report the averaged scores of five post-training runs, but both pre-training and training are performed only once.", "labels": [], "entities": [{"text": "AdNN", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8476155400276184}]}], "tableCaptions": [{"text": " Table 3: The effect of different feature setting on  AdNN model. + means the comparison is signifi- cant over AdNN-Hiero-D with p < 0.05.", "labels": [], "entities": []}]}