{"title": [{"text": "Mr. MIRA: Open-Source Large-Margin Structured Learning on MapReduce", "labels": [], "entities": [{"text": "MIRA", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9745144844055176}]}], "abstractContent": [{"text": "We present an open-source framework for large-scale online structured learning.", "labels": [], "entities": []}, {"text": "Developed with the flexibility to handle cost-augmented inference problems such as statistical machine translation (SMT), our large-margin learner can be used with any decoder.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 83, "end_pos": 120, "type": "TASK", "confidence": 0.7950402895609537}]}, {"text": "Integration with MapReduce using Hadoop streaming allows efficient scaling with increasing size of training data.", "labels": [], "entities": []}, {"text": "Although designed with a focus on SMT, the decoder-agnostic design of our learner allows easy future extension to other structured learning problems such as sequence labeling and parsing.", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9926046133041382}, {"text": "sequence labeling", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.6223751455545425}]}], "introductionContent": [{"text": "Structured learning problems such as sequence labeling or parsing, where the output has a rich internal structure, commonly arise in NLP.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.6877027750015259}, {"text": "parsing", "start_pos": 58, "end_pos": 65, "type": "TASK", "confidence": 0.9025625586509705}]}, {"text": "While batch learning algorithms adapted for structured learning such as CRFs () and structural SVMs) have received much attention, online methods such as the structured perceptron) and a family of Passive-Aggressive algorithms) have recently gained prominence across many tasks, including part-of-speech tagging, parsing) and statistical machine translation (SMT)), due to their ability to deal with large training sets and high-dimensional input representations.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 289, "end_pos": 311, "type": "TASK", "confidence": 0.7555463314056396}, {"text": "statistical machine translation (SMT))", "start_pos": 326, "end_pos": 364, "type": "TASK", "confidence": 0.8310419023036957}]}, {"text": "Unlike batch learners, which must consider all examples when optimizing the objective, online learners operate in rounds, optimizing using one example or a handful of examples at a time.", "labels": [], "entities": []}, {"text": "This online nature offers several attractive properties, facilitating scaling to large training sets while remaining simple and offering fast convergence.", "labels": [], "entities": []}, {"text": "Mr. MIRA, the open source system 1 described in this paper, implements an online largemargin structured learning algorithm based on MIRA ( \u00a72.1), for cost-augmented online largescale training in high-dimensional feature spaces.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7178804874420166}]}, {"text": "Our contribution lies in providing the first published decoder-agnostic parallelization of MIRA with Hadoop for structured learning.", "labels": [], "entities": []}, {"text": "While the current demonstrated application focuses on large-scale discriminative training for machine translation, the learning algorithm is general with respect to the inference algorithm employed.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7905628681182861}]}, {"text": "We are able to decouple our learner entirely from the MT decoder, allowing users to specify their own inference procedure through a simple text communication protocol ( \u00a72.2).", "labels": [], "entities": []}, {"text": "The learner only requires k-best output with feature vectors, as well as the specification of a cost function.", "labels": [], "entities": []}, {"text": "Standard automatic evaluation metrics for MT, such as BLEU () and TER), have already been implemented.", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9918409585952759}, {"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9991439580917358}, {"text": "TER", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9940260648727417}]}, {"text": "Furthermore, our system can be extended to other structured learning problems with a minimal amount of effort, simply by implementing a task-specific cost function and specifying an appropriate decoder.", "labels": [], "entities": []}, {"text": "Through Hadoop streaming, our system can take advantage of commodity clusters to handle large-scale training ( \u00a73), while also being capable of running in environments ranging from a single machine to a PBS-managed batch cluster.", "labels": [], "entities": []}, {"text": "Experimental results ( \u00a74) show that it scales linearly and makes fast parameter tuning on large tuning sets for SMT practical.", "labels": [], "entities": [{"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9955098628997803}]}], "datasetContent": [{"text": "We evaluated online learning in Hadoop MapReduce by applying it to German-English machine translation, using our hierarchical phrasebased translation system with cdec as the decoder ().", "labels": [], "entities": [{"text": "German-English machine translation", "start_pos": 67, "end_pos": 101, "type": "TASK", "confidence": 0.5934771001338959}]}, {"text": "The parallel training data consist of the Europarl and News Commentary corpora from the WMT12 translation task, containing 2.08M sentences.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9885545372962952}, {"text": "News Commentary corpora", "start_pos": 55, "end_pos": 78, "type": "DATASET", "confidence": 0.8936631679534912}, {"text": "WMT12 translation task", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.7471576531728109}]}, {"text": "A 5-gram language model was trained on the English side of the bitext along with 750M words of news using SRILM with modified Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.6819487810134888}]}, {"text": "We experimented with two feature sets: (1) a small set with standard MT features, including a large set containing the top 200k sparse features that might be useful to train on large numbers of instances: rule id and shape, target bigrams, insertions and deletions, and structural distortion features.", "labels": [], "entities": []}, {"text": "All experiments were conducted on a Hadoop cluster (running Cloudera's distribution, CDH 4.2.1) with 16 nodes, each with two quad-core 2.2 GHz Intel Nehalem Processors, 24 GB RAM, and three 2 TB drives.", "labels": [], "entities": [{"text": "Cloudera's distribution, CDH 4.2.1", "start_pos": 60, "end_pos": 94, "type": "DATASET", "confidence": 0.8992001513640085}]}, {"text": "In total, the cluster is configured fora capacity of 128 parallel workers, although we do not have direct control over the number of simultaneous mappers, which depends on the number of input splits.", "labels": [], "entities": []}, {"text": "If the number of splits is smaller than 128, then the cluster is under-utilized.", "labels": [], "entities": []}, {"text": "To note this, we report the number of splits for each setting in our experimental results.", "labels": [], "entities": []}, {"text": "We ran MIRA on a number of tuning sets, described in, in order to test the effectiveness and scalability of our system.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 7, "end_pos": 11, "type": "TASK", "confidence": 0.3420596122741699}]}, {"text": "First, we used the standard development set from WMT12, consisting of 3,003 sentences from news domain.", "labels": [], "entities": [{"text": "WMT12", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.8682991862297058}]}, {"text": "In order to show the scaling characteristics of our approach, we then used larger portions of the training bitext directly to tune parameters.", "labels": [], "entities": []}, {"text": "In order to avoid overfitting, we used a jackknifing method to split the training data into n = 10 folds, and built a translation system on n \u2212 1 folds, while adjusting the sampling rate to sample sentences from the other fold to obtain tuning sets ranging from 5,000 sentences to 50,000 sentences.", "labels": [], "entities": []}, {"text": "shows details of experimental results for each setting.", "labels": [], "entities": []}, {"text": "The second column shows the space each tuning set takes upon disk when we include reference translations and grammar files along with the sentences.", "labels": [], "entities": []}, {"text": "The reported tuning BLEU is from the iteration with best performance, and running times are reported from the top-scoring iteration as well.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.8683454990386963}]}, {"text": "Even though our focus in this evaluation is to show the scalability of our implementation to large input and feature sets, it is also worthwhile to mention the effectiveness aspect.", "labels": [], "entities": []}, {"text": "As we increase the tuning set size by sampling sentences from the training data, we see very little improvement in BLEU and TER with the smaller feature set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9990906715393066}, {"text": "TER", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.9974842071533203}]}, {"text": "This is not surprising, since sparse features are more likely to gain from additional tuning instances.", "labels": [], "entities": []}, {"text": "Indeed, tuning scores for all sets improve substantially with sparse features, accompanied by small increases on test.", "labels": [], "entities": []}, {"text": "While tuning on dev data results in better BLEU on test data than when tuning on the larger sets, it is important to note that although we are able to tune more features on the larger bitext tuning sets, they are not composed of the same genre as the dev and test sets, resulting in a domain mismatch.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9995036125183105}]}, {"text": "Therefore, we are actually comparing a smaller indomain tuning set with a larger out-of-domain set.", "labels": [], "entities": []}, {"text": "While this domain adaptation is problematic, the ability to discriminatively tune on larger sets remains highly desirable.", "labels": [], "entities": []}, {"text": "In terms of running time, we observe that the algorithm scales linearly with respect to input size, regardless of the feature set.", "labels": [], "entities": []}, {"text": "With more features, running time increases due to a more complex translation model, as well as larger intermediate output (i.e., amount of information passed from mappers to reducers).", "labels": [], "entities": []}, {"text": "The scaling characteristics point out the strength of our system: our scalable MIRA implementation allows one to tackle learning problems where there are many parameters, but also many training instances.", "labels": [], "entities": []}, {"text": "Comparing the wall clock time of parallelization with Hadoop to the standard mode of 10-20 learner parallelization, for the small 25k feature setting, after one iteration, which takes 4625 seconds using 15 learners on our PBS cluster, the tuning score is 19.5 BLEU, while in approximately the same time, we can perform five iterations with Hadoop and obtain 30.98 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 260, "end_pos": 264, "type": "METRIC", "confidence": 0.9981107711791992}, {"text": "BLEU", "start_pos": 364, "end_pos": 368, "type": "METRIC", "confidence": 0.9970979690551758}]}, {"text": "While this is not a completely fair comparison, as the two clusters utilize different resources and the number of learners, it suggests the practical benefits that Hadoop can provide.", "labels": [], "entities": []}, {"text": "Although increasing the number of learners on our PBS cluster to the number of mappers used in Hadoop would result in roughly equivalent performance, arbitrarily scaling out learners on the PBS cluster to handle larger training sets can be challenging since we'd have to manually coordinate the parallel processes in an ad-hoc manner.", "labels": [], "entities": []}, {"text": "In contrast, Hadoop provides scalable parallelization in a manageable framework, providing data distribution, synchronization, fault tolerance, as well as other features, \"for free\".", "labels": [], "entities": [{"text": "fault tolerance", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.6674987524747849}]}], "tableCaptions": [{"text": " Table 1: Evaluation of our Hadoop implementation of MIRA, showing running time as well as BLEU  and TER values for tuning and testing data.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.3868675231933594}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9994505047798157}, {"text": "TER", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9901232123374939}]}]}