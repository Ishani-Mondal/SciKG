{"title": [{"text": "A Tale about PRO and Monsters", "labels": [], "entities": [{"text": "PRO", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.8962239027023315}]}], "abstractContent": [{"text": "While experimenting with tuning on long sentences, we made an unexpected discovery: that PRO falls victim to monsters-overly long negative examples with very low BLEU+1 scores, which are unsuitable for learning and can cause testing BLEU to drop by several points absolute.", "labels": [], "entities": [{"text": "BLEU+1 scores", "start_pos": 162, "end_pos": 175, "type": "METRIC", "confidence": 0.9761798232793808}, {"text": "BLEU", "start_pos": 233, "end_pos": 237, "type": "METRIC", "confidence": 0.9936328530311584}]}, {"text": "We propose several effective ways to address the problem, using length-and BLEU+1-based cutoffs , outlier filters, stochastic sampling, and random acceptance.", "labels": [], "entities": [{"text": "length-and", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9330476522445679}, {"text": "BLEU+1-based cutoffs", "start_pos": 75, "end_pos": 95, "type": "METRIC", "confidence": 0.8886057436466217}]}, {"text": "The best of these fixes not only slay and protect against monsters, but also yield higher stability for PRO as well as improved test-time BLEU scores.", "labels": [], "entities": [{"text": "PRO", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9041831493377686}, {"text": "BLEU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9753618240356445}]}, {"text": "Thus, we recommend them to anybody using PRO, monster-believer or not.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We used a phrase-based SMT model ( as implemented in the Moses toolkit (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.8623846769332886}]}, {"text": "We trained on all Arabic-English data for NIST 2012 except for UN, we tuned on (the longest-50% of) the MT06 sentences, and we tested on MT09.", "labels": [], "entities": [{"text": "Arabic-English data for NIST 2012", "start_pos": 18, "end_pos": 51, "type": "DATASET", "confidence": 0.6871225237846375}, {"text": "UN", "start_pos": 63, "end_pos": 65, "type": "DATASET", "confidence": 0.47643429040908813}, {"text": "MT06", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.8087319135665894}, {"text": "MT09", "start_pos": 137, "end_pos": 141, "type": "DATASET", "confidence": 0.9508577585220337}]}, {"text": "We used the MADA ATB segmentation for Arabic () and truecasing for English, phrases of maximal length 7, Kneser-Ney smoothing, and lexicalized reordering (), and a 5-gram language model, trained on GigaWord v.5 using KenLM).", "labels": [], "entities": [{"text": "MADA ATB segmentation", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.589630643526713}]}, {"text": "We dropped unknown words both at tuning and testing, and we used minimum Bayes risk decoding at testing ().", "labels": [], "entities": []}, {"text": "We evaluated the output with NIST's scoring tool v.13a, cased.", "labels": [], "entities": [{"text": "NIST", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.9034894108772278}]}, {"text": "We used the Moses implementations of MERT, PRO and batch MIRA, with the -return-best-dev parameter for the latter.", "labels": [], "entities": [{"text": "MERT", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.5786154866218567}, {"text": "MIRA", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.6442579627037048}]}, {"text": "We ran these optimizers for up to 25 iterations and we used 1000-best lists.", "labels": [], "entities": []}, {"text": "For stability, we performed three reruns of each experiment (tuning + evaluation), and we report averaged scores.", "labels": [], "entities": []}, {"text": "presents the results for different selection alternatives.", "labels": [], "entities": []}, {"text": "The first two columns show the testing results: average BLEU and standard deviation over three reruns.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9952775239944458}, {"text": "standard", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9944065809249878}]}], "tableCaptions": [{"text": " Table 1: PRO iterations, tuning on long sentences.", "labels": [], "entities": [{"text": "PRO iterations", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7821872234344482}]}, {"text": " Table 2: PRO vs. MERT vs. MIRA.", "labels": [], "entities": [{"text": "MERT", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.634446382522583}, {"text": "MIRA", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.7538665533065796}]}, {"text": " Table 3: Some fixes to PRO (select pairs with highest BLEU+1 differential, also require at least 5  BLEU+1 points difference). A dagger (  \u2020 ) indicates selection fixes that successfully get rid of monsters.", "labels": [], "entities": [{"text": "PRO", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.5591303706169128}, {"text": "BLEU+1 differential", "start_pos": 55, "end_pos": 74, "type": "METRIC", "confidence": 0.9656444936990738}, {"text": "BLEU+1 points difference", "start_pos": 101, "end_pos": 125, "type": "METRIC", "confidence": 0.9573679566383362}]}, {"text": " Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (  \u2020 \u2020 ) indicates that  random acceptance kills monsters. The asterisk (  *  ) indicates improved stability over random acceptance.", "labels": [], "entities": [{"text": "PRO", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.40583598613739014}, {"text": "BLEU+1", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9769686857859293}]}]}