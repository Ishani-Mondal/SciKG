{"title": [{"text": "A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations", "labels": [], "entities": [{"text": "Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations", "start_pos": 21, "end_pos": 100, "type": "TASK", "confidence": 0.6714556753635407}]}], "abstractContent": [{"text": "We propose a joint model for unsuper-vised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level.", "labels": [], "entities": [{"text": "induction of sentiment, aspect and discourse information", "start_pos": 43, "end_pos": 99, "type": "TASK", "confidence": 0.7243993878364563}, {"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.9865272641181946}]}, {"text": "We deviate from the traditional view of discourse , as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters.", "labels": [], "entities": [{"text": "opinion analysis task", "start_pos": 151, "end_pos": 172, "type": "TASK", "confidence": 0.8139470418294271}]}, {"text": "The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9736402630805969}]}], "introductionContent": [{"text": "With the rapid growth of the Web, it is becoming increasingly difficult to discern useful from irrelevant information, particularly in user-generated content, such as product reviews.", "labels": [], "entities": []}, {"text": "To make it easier for the reader to separate the wheat from the chaff, it is necessary to structure the available information.", "labels": [], "entities": []}, {"text": "In the review domain, this is done in aspectbased sentiment analysis which aims at identifying text fragments in which opinions are expressed about ratable aspects of products, such as 'room quality' or 'service quality'.", "labels": [], "entities": [{"text": "aspectbased sentiment analysis", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.6677468717098236}]}, {"text": "Such fine-grained analysis can serve as the first step in aspect-based sentiment summarization (), a task with many practical applications.", "labels": [], "entities": [{"text": "aspect-based sentiment summarization", "start_pos": 58, "end_pos": 94, "type": "TASK", "confidence": 0.626100222269694}]}, {"text": "Aspect-based summarization is an active research area for which various techniques have been developed, both statistical ( and not (, and relying on different types of supervision sources, such as sentiment-annotated texts or polarity lexica).", "labels": [], "entities": [{"text": "Aspect-based summarization", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8957291543483734}]}, {"text": "Most methods rely on local information (bag-of-words, short ngrams or elementary syntactic fragments) and do not attempt to account for more complex interactions.", "labels": [], "entities": []}, {"text": "However, these local lexical representations by themselves are often not sufficient to infer a sentiment or aspect fora fragment of text.", "labels": [], "entities": []}, {"text": "For instance, in the following example taken from a TripAdvisor 1 review: Example 1.", "labels": [], "entities": [{"text": "TripAdvisor 1 review", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.8948027491569519}]}, {"text": "The room was nice but let's not talk about the view.", "labels": [], "entities": []}, {"text": "it is difficult to deduce on the basis of local lexical features alone that the opinion about the view is negative.", "labels": [], "entities": []}, {"text": "The clause let's not talk about the view could by itself be neutral or even positive given the right context (e.g., I've never seen such a fancy hotel room, my living room doesn't look that cool... and let's not talk about the view).", "labels": [], "entities": []}, {"text": "However, the contrast relation signaled by the connective but makes it clear that the second clause has a negative polarity.", "labels": [], "entities": []}, {"text": "The same observations can be made about transitions between aspects: changes in aspect are often clearly marked by discourse connectives.", "labels": [], "entities": []}, {"text": "Importantly, some of these cues are not discourse connectives in the strict linguistic sense and are specific to the review domain (e.g., the phrase I would also in a review indicates that the topic is likely to be changed).", "labels": [], "entities": []}, {"text": "In order to accurately predict sentiment and topic, 2 a model needs to ac-count for these discourse phenomena and cannot rely solely on local lexical information.", "labels": [], "entities": [{"text": "ac-count", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.954619288444519}]}, {"text": "These issues have not gone unnoticed to the research community.", "labels": [], "entities": []}, {"text": "Consequently, there has recently been an increased interest in models that leverage content and discourse structure in sentiment analysis tasks.", "labels": [], "entities": [{"text": "sentiment analysis tasks", "start_pos": 119, "end_pos": 143, "type": "TASK", "confidence": 0.9456659356753031}]}, {"text": "However, discourse-level information is typically incorporated in a pipeline architecture, either in the form of sentiment polarity shifters) that operate on the lexical level or by using discourse relations () that comply with discourse theories like Rhetorical Structure Theory (RST) (.", "labels": [], "entities": [{"text": "sentiment polarity shifters", "start_pos": 113, "end_pos": 140, "type": "TASK", "confidence": 0.6637954811255137}, {"text": "Rhetorical Structure Theory (RST)", "start_pos": 252, "end_pos": 285, "type": "TASK", "confidence": 0.748931477467219}]}, {"text": "Such approaches have a number of disadvantages.", "labels": [], "entities": []}, {"text": "First, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations.", "labels": [], "entities": []}, {"text": "These resources are available only fora handful of languages.", "labels": [], "entities": []}, {"text": "Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to error propagation.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.8567617535591125}]}, {"text": "Furthermore, these techniques will not necessarily be able to induce discourse relations informative for the sentiment analysis domain).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.9054040610790253}]}, {"text": "An alternative approach is to define a taskspecific scheme of discourse relations).", "labels": [], "entities": []}, {"text": "This previous work showed that task-specific discourse relations are helpful in predicting sentiment, however, in doing so they relied on gold-standard discourse annotation attest time rather than predicting it automatically or inducing it jointly with sentiment polarity.", "labels": [], "entities": [{"text": "predicting sentiment", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.9040440618991852}]}, {"text": "We take a different approach and induce discourse and sentiment information jointly in an unsupervised (or weakly supervised) manner.", "labels": [], "entities": []}, {"text": "This has the advantage of not having to pre-specify a mapping from discourse cues to discourse relations; our model induces this automatically, which makes it portable to new domains and languages.", "labels": [], "entities": []}, {"text": "Joint induction of discourse and sentiment structure also has the added benefit that the model is able to learn exactly those aspects of discourse structure that are relevant for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.9339902400970459}]}, {"text": "We start with a relatively standard joint model of sentiment and topic, which can be regarded as a cross-breed between the JST model () and the ASUM model (), changeably as well as sentiment levels and opinion polarity.", "labels": [], "entities": []}, {"text": "This model is weakly supervised, as it relies solely on documentlevel (i.e. not aspect-specific) opinion polarity labels to induce topics and sentiment on the subsentential level.", "labels": [], "entities": []}, {"text": "In order to test our hypothesis that discourse information is beneficial, we add a discourse modeling component.", "labels": [], "entities": [{"text": "discourse modeling", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7093303948640823}]}, {"text": "Note that in modeling discourse we do not exploit any kind of supervision.", "labels": [], "entities": []}, {"text": "We demonstrate that the resulting model outperforms the baseline on a product review dataset (see Section 5).", "labels": [], "entities": [{"text": "product review dataset", "start_pos": 70, "end_pos": 92, "type": "DATASET", "confidence": 0.5977803766727448}]}, {"text": "To the best of our knowledge, unsupervised joint induction of discourse structure, sentiment and topic information has not been considered before, particularly not in the context of the aspect-based sentiment analysis task.", "labels": [], "entities": [{"text": "aspect-based sentiment analysis task", "start_pos": 186, "end_pos": 222, "type": "TASK", "confidence": 0.7173403725028038}]}, {"text": "Importantly, our method for discourse modeling is a general method which can be integrated in virtually any LDA-style model of aspect and sentiment.", "labels": [], "entities": [{"text": "discourse modeling", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7470006346702576}]}], "datasetContent": [{"text": "To the best of our knowledge, this is the first work that aims at evaluating directly the joint information of the sentiment and aspect assignment at the sub-sentential level of full reviews; most existing studies either focus on indirect evaluation of the produced models (e.g., classifying the overall sentiment of sentences ( or even reviews (Nakagawa et al., 2010; Jo and Oh, 2011)) or evaluated solely at the sentential or even document level.", "labels": [], "entities": []}, {"text": "Consequently, in order to evaluate our methods, we created anew dataset which will be publicly released.", "labels": [], "entities": []}, {"text": "For creating the gold standard, 9 annotators annotated a random subset of our dataset (65 reviews, 1541 EDUs).", "labels": [], "entities": []}, {"text": "The annotators were presented with the whole review partitioned in EDUs and were asked to annotate every EDU with the aspect and sentiment (i.e. +1, 0 or \u22121) it expresses.", "labels": [], "entities": []}, {"text": "presents the distribution of aspects in the dataset.", "labels": [], "entities": []}, {"text": "The distribution of the sentiments is uniform.", "labels": [], "entities": []}, {"text": "The label rest captures cases where EDUs do not refer to any aspect or to a very rare aspect.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement (IAA), as measured in terms of Cohen's kappa score, was 66% for the aspect labeling, 70% for the sentiment annotation and 61% for the joint task of sentiment and aspect annotation.", "labels": [], "entities": [{"text": "inter-annotator agreement (IAA)", "start_pos": 4, "end_pos": 35, "type": "METRIC", "confidence": 0.8973173499107361}]}, {"text": "Though these scores may not seem very high, they are similar to the ones reported in related sentiment annotation efforts (see e.g.,).", "labels": [], "entities": []}, {"text": "Experimental setup In order to quantitatively evaluate the model predictions, we run two sets of experiments.", "labels": [], "entities": []}, {"text": "In the first, we treat the task as an unsupervised classification problem and evaluate the output of the models directly against the gold standard annotation.", "labels": [], "entities": []}, {"text": "This is a very challenging set-up, as the model has no prior information about the aspects defined).", "labels": [], "entities": []}, {"text": "In the second set of experiments, we show that aspects and sentiments induced by our model can be used to construct informative features for supervised classification.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 141, "end_pos": 166, "type": "TASK", "confidence": 0.6465870290994644}]}, {"text": "In: Separate evaluation (F1) of the \"marked\" and the \"unmarked\" EDUs.", "labels": [], "entities": [{"text": "F1)", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9336114227771759}]}, {"text": "all the cases, we compare the discourse-agnostic and the discourse-informed models.", "labels": [], "entities": []}, {"text": "In order to induce the model, we let the sampler run for 2000 iterations.", "labels": [], "entities": []}, {"text": "We use the last sample to define the labeling.", "labels": [], "entities": []}, {"text": "The number of topics K was set to 10 in order to match the number of aspects defined in our annotation scheme (see).", "labels": [], "entities": []}, {"text": "The hyperpriors were chosen in a qualitative experiment over a subset of our dataset by manually inspecting the produced languages models.", "labels": [], "entities": []}, {"text": "The resulting values are: \u03b1 = 10 \u22123 , \u03b2 = 5 * 10 \u22124 , \u03c4 = 5 * 10 \u22124 , \u03b7 = 10 \u22123 , \u03bd 4 = 10 3 , \u03bd\u00af 4 = 10 \u22124 , \u03c9 \u03b8 = 85 and \u03c9 \u03b8 = \u03c9 \u03c8 = \u03c9 \u03c8 = 5.", "labels": [], "entities": []}, {"text": "Our labels encoding aspect and sentiment level can be regarded as clusters.", "labels": [], "entities": []}, {"text": "Consequently we can apply techniques developed in the context of clustering evaluation.", "labels": [], "entities": [{"text": "clustering evaluation", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.9489056468009949}]}, {"text": "We use aversion of the standard metrics considered for the word sense induction task) where a clustering is converted to a classification problem.", "labels": [], "entities": [{"text": "word sense induction task", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.795343667268753}]}, {"text": "This is achieved by splitting the gold standard into two subsets; the training portion is used to choose oneto-one correspondence from the gold classes to the induced clusters and then the chosen mapping is applied to the testing portion.", "labels": [], "entities": []}, {"text": "We perform 10-fold cross validation and report precision, recall and F1 score.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9630093574523926}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.999761164188385}, {"text": "F1 score", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9769963622093201}]}, {"text": "Our dataset is very skewed and the majority class (rest) is arguably the least important, so we use macro-averaging over labels and then average those across folds to arrive to the reported numbers.", "labels": [], "entities": []}, {"text": "We compare the discourse-informed model (Discourse) against two baselines; the discourseagnostic SentAsp model and Random which assigns a random label to an EDU while respecting the distribution of labels in the training set.", "labels": [], "entities": []}, {"text": "presents the first analysis conducted on the full set of EDUs.", "labels": [], "entities": []}, {"text": "We observe that by incorporating latent discourse relation we improve per-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Distribution of aspects in the data.", "labels": [], "entities": []}, {"text": " Table 3: Results in terms of macro-averaged pre- cision, recall and F1.", "labels": [], "entities": [{"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9995877146720886}, {"text": "F1", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.9996767044067383}]}, {"text": " Table 4: Separate evaluation (F1) of the \"marked\"  and the \"unmarked\" EDUs.", "labels": [], "entities": [{"text": "F1)", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9205067157745361}]}, {"text": " Table 7: Supervised learning at the EDU level (accuracy)", "labels": [], "entities": [{"text": "EDU", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.5718687176704407}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9969258904457092}]}]}