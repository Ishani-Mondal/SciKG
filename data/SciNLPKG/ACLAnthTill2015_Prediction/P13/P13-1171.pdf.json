{"title": [{"text": "Question Answering Using Enhanced Lexical Semantic Models", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8043736517429352}]}], "abstractContent": [{"text": "In this paper, we study the answer sentence selection problem for question answering.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.6367416679859161}, {"text": "question answering", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.8398033380508423}]}, {"text": "Unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance using models of lexical semantic resources.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.7278499603271484}, {"text": "dependency tree matching", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.6397074063618978}]}, {"text": "Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, regardless of the choice of learning algorithms.", "labels": [], "entities": []}, {"text": "When evaluated on a benchmark dataset, the MAP and MRR scores are increased by 8 to 10 points, compared to one of our baseline systems using only surface-form matching.", "labels": [], "entities": [{"text": "MAP", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9325066804885864}, {"text": "MRR", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.8768644332885742}]}, {"text": "Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open-domain question answering (QA), which fulfills a user's information need by outputting direct answers to natural language queries, is a challenging but important problem.", "labels": [], "entities": [{"text": "Open-domain question answering (QA)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7827654480934143}]}, {"text": "State-of-the-art QA systems often implement a complicated pipeline architecture, consisting of question analysis, document or passage retrieval, answer selection and verification.", "labels": [], "entities": [{"text": "question analysis", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7480549514293671}, {"text": "document or passage retrieval", "start_pos": 114, "end_pos": 143, "type": "TASK", "confidence": 0.5844728052616119}, {"text": "answer selection", "start_pos": 145, "end_pos": 161, "type": "TASK", "confidence": 0.8083698451519012}]}, {"text": "In this paper, we focus on one of the key subtasks -answer sentence selection.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.8303025960922241}]}, {"text": "Given a question and a set of candidate sentences, the task is to choose the correct sentence that contains the exact answer and can sufficiently support the answer choice.", "labels": [], "entities": []}, {"text": "For instance, although both of the following sentences contain the answer \"Jack Lemmon\" to the question \"Who won the best actor Oscar in 1973?\" only the first sentence is correct.", "labels": [], "entities": [{"text": "Who won the best actor Oscar in 1973?\"", "start_pos": 105, "end_pos": 143, "type": "TASK", "confidence": 0.687864406241311}]}], "datasetContent": [{"text": "We present our experimental results in this section by first introducing the data and evaluation metrics, followed by the results of existing systems and some baseline methods.", "labels": [], "entities": []}, {"text": "We then show the positive impact of adding information of word relations from various lexical semantics models, with some discussion on the limitation of the word-matching approach.", "labels": [], "entities": []}, {"text": "The answer selection dataset we used was originally created by based on the QA track of past Text REtrieval Conferences (TREC-QA).", "labels": [], "entities": [{"text": "answer selection", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8572891354560852}, {"text": "Text REtrieval Conferences (TREC-QA)", "start_pos": 93, "end_pos": 129, "type": "TASK", "confidence": 0.6563807874917984}]}, {"text": "Questions in this dataset are short factoid questions, such as \"What is Crips' gang color?\"", "labels": [], "entities": []}, {"text": "In average, each question is associated with approximately 33 answer candidate sentences.", "labels": [], "entities": []}, {"text": "A pair of question and sentence is judged positive if the sentence contains the exact answer key and can provide sufficient context as supporting evidence.", "labels": [], "entities": []}, {"text": "The training set of the data contains manually labeled 5,919 question/sentence pairs from TREC 8-12.", "labels": [], "entities": [{"text": "TREC 8-12", "start_pos": 90, "end_pos": 99, "type": "DATASET", "confidence": 0.8808158338069916}]}, {"text": "The development and testing sets are both from TREC 13, which contain 1,374 and 1,866 pairs, respectively.", "labels": [], "entities": [{"text": "TREC 13", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.7347962558269501}]}, {"text": "The task is treated as a sentence ranking problem for each question and thus evaluated in Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), using the official TREC evaluation program.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 90, "end_pos": 118, "type": "METRIC", "confidence": 0.9759522080421448}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 123, "end_pos": 149, "type": "METRIC", "confidence": 0.962462991476059}]}, {"text": "Following (, candidate sentences with more than 40 words are removed from evaluation, as well as questions with only positive or negative candidate sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test set results of existing methods, taken  from", "labels": [], "entities": []}, {"text": " Table 2: Results of three baseline methods.", "labels": [], "entities": []}, {"text": " Table 3: Test results of various models and feature groups. Logistic regression (LR) and boosted decision  trees (BDT) are the two unstructured models. LCLR is the algorithm for learning latent structures.  Feature groups are identical word matching (I), lemma matching (L), WordNet (WN) and enhanced  Lexical Semantics (LS). All includes these four plus Named Entity matching (NE) and Answer type  checking (Ans).", "labels": [], "entities": [{"text": "Answer type  checking (Ans)", "start_pos": 387, "end_pos": 414, "type": "METRIC", "confidence": 0.6681821346282959}]}]}