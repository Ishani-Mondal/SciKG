{"title": [{"text": "A New Syntactic Metric for Evaluation of Machine Translation", "labels": [], "entities": [{"text": "Evaluation of Machine Translation", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.6158888339996338}]}], "abstractContent": [{"text": "Machine translation (MT) evaluation aims at measuring the quality of a candidate translation by comparing it with a reference translation.", "labels": [], "entities": [{"text": "Machine translation (MT) evaluation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9034104943275452}]}, {"text": "This comparison can be performed on multiple levels: lexical, syntactic or semantic.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew syntactic metric for MT evaluation based on the comparison of the dependency structures of the reference and the candidate translations.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9711811244487762}]}, {"text": "The dependency structures are obtained by means of a Weighted Constraints Dependency Grammar parser.", "labels": [], "entities": []}, {"text": "Based on experiments performed on English to German translations, we show that the new metric correlates well with human judgments at the system level.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research in automatic machine translation (MT) evaluation has the goal of developing a set of computer-based methods that measure accurately the correctness of the output generated by a MT system.", "labels": [], "entities": [{"text": "automatic machine translation (MT) evaluation", "start_pos": 12, "end_pos": 57, "type": "TASK", "confidence": 0.8642545001847404}, {"text": "MT", "start_pos": 186, "end_pos": 188, "type": "TASK", "confidence": 0.9556242227554321}]}, {"text": "However, this task is a difficult one mainly because there is no unique reference output that can be used in the comparison with the candidate translation.", "labels": [], "entities": []}, {"text": "One sentence can have several correct translations.", "labels": [], "entities": []}, {"text": "Thus, it is difficult to decide if the deviation from an existing reference translation is a matter of style (the use of synonymous words, different syntax etc.) or areal translation error.", "labels": [], "entities": []}, {"text": "Most of the automatic evaluation metrics developed so far are focused on the idea of lexical matching between the tokens of one or more reference translations and the tokens of a candidate translation.", "labels": [], "entities": []}, {"text": "However, structural similarity between a reference translation and a candidate one cannot be captured by lexical features.", "labels": [], "entities": []}, {"text": "Therefore, research in MT evaluation experiences a gradual shift of focus from lexical metrics to structural ones, whether they are syntactic or semantic or a combination of both.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9877435266971588}]}, {"text": "This paper introduces anew syntactic automatic MT evaluation method.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9348885416984558}]}, {"text": "At this stage of research the new metric is evaluating translations from any source language into German.", "labels": [], "entities": []}, {"text": "Given that a set of constraint-based grammar rules are available for that language, extensions to other target languages are anytime possible.", "labels": [], "entities": []}, {"text": "The chosen tool for providing syntactic information for German is the Weighted Constraints Dependency Grammar (WCDG) parser, which is preferred over other parsers because of its robustness to ungrammatical input, as it is typical for MT output.", "labels": [], "entities": [{"text": "Weighted Constraints Dependency Grammar (WCDG) parser", "start_pos": 70, "end_pos": 123, "type": "TASK", "confidence": 0.5959971621632576}, {"text": "MT output", "start_pos": 234, "end_pos": 243, "type": "TASK", "confidence": 0.9382204711437225}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 the state of the art in MT evaluation is presented, while in Section 3 the new syntactic metric is described.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9561277627944946}]}, {"text": "The experimental setup and results are presented in Section 4.", "labels": [], "entities": []}, {"text": "The last section deals with the conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to determine how accurate CESM is in capturing the similarity between references and translations, we evaluated it at the system level and at the segment level.", "labels": [], "entities": []}, {"text": "The evaluation was conducted using data provided by the NAACL 2012 WMT workshop ).", "labels": [], "entities": [{"text": "NAACL 2012 WMT workshop", "start_pos": 56, "end_pos": 79, "type": "DATASET", "confidence": 0.9029374420642853}]}, {"text": "The test data for the workshop consisted of 99 translated news articles in English, German, French, Spanish and Czech.", "labels": [], "entities": []}, {"text": "At the system level, the initial German test set provided at the workshop was filtered according to the length of segments.", "labels": [], "entities": [{"text": "German test set", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.75835253794988}]}, {"text": "This was done in order to limit the time requirements of WCDG.", "labels": [], "entities": [{"text": "WCDG", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.575808048248291}]}, {"text": "As a result, 500 segments with a length between 50 and 80 characters were extracted from the German reference file.", "labels": [], "entities": [{"text": "German reference file", "start_pos": 93, "end_pos": 114, "type": "DATASET", "confidence": 0.9645192424456278}]}, {"text": "In the next step, we arbitrarily selected the outputs of 7 of the 15 systems that were submitted for evaluation in the English to German translation task: DFKI (Vilar, 2012), JHU (),), UK (Zeman, 2012) and three anonymized system outputs referred to as OnlineA, OnlineB, OnlineC.", "labels": [], "entities": [{"text": "DFKI (Vilar, 2012)", "start_pos": 155, "end_pos": 173, "type": "DATASET", "confidence": 0.8472442428270975}, {"text": "JHU", "start_pos": 175, "end_pos": 178, "type": "DATASET", "confidence": 0.8255562782287598}, {"text": "UK (Zeman, 2012)", "start_pos": 185, "end_pos": 201, "type": "DATASET", "confidence": 0.8214607934157053}, {"text": "OnlineA", "start_pos": 253, "end_pos": 260, "type": "DATASET", "confidence": 0.9235594272613525}, {"text": "OnlineB", "start_pos": 262, "end_pos": 269, "type": "DATASET", "confidence": 0.8474062085151672}, {"text": "OnlineC", "start_pos": 271, "end_pos": 278, "type": "DATASET", "confidence": 0.9247787594795227}]}, {"text": "After this initial step of filtering the data, the 7 systems were evaluated by calculating the CESM score for every pair of reference and translation segments corresponding to a system.", "labels": [], "entities": [{"text": "CESM score", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9098415970802307}]}, {"text": "The average scores obtained are depicted in.", "labels": [], "entities": []}, {"text": "Evaluation of the metric at the system level was performed by measuring the correlation of the CESM metric with human judgments using Spearman's rank correlation coefficient \u03c1: where n represents the number of MT systems considered during evaluation, and d i 2 represents the difference between the ranks, assigned to a system, by the metric and the human judgments.", "labels": [], "entities": [{"text": "CESM metric", "start_pos": 95, "end_pos": 106, "type": "DATASET", "confidence": 0.858797550201416}, {"text": "rank correlation coefficient \u03c1", "start_pos": 145, "end_pos": 175, "type": "METRIC", "confidence": 0.7629483863711357}]}, {"text": "The minimum value of \u03c1 is -1, when there is no correlation between the two rankings, while the maximum value is 1, when the two rankings correlate perfectly . In order to compute the \u03c1 score, the scores attributed to every system by CESM, were converted into ranks.", "labels": [], "entities": [{"text": "CESM", "start_pos": 233, "end_pos": 237, "type": "DATASET", "confidence": 0.9335840940475464}]}, {"text": "From the different ranking strategies that were presented by the WMT12 workshop, the standard ranking order was chosen.", "labels": [], "entities": [{"text": "WMT12 workshop", "start_pos": 65, "end_pos": 79, "type": "DATASET", "confidence": 0.8096393346786499}]}, {"text": "The \u03c1 rank correlation coefficient was calculated as being \u03c1 = 0.92, which shows there is a strong correlation between the results of CESM and the human judgments.", "labels": [], "entities": [{"text": "\u03c1 rank correlation coefficient", "start_pos": 4, "end_pos": 34, "type": "METRIC", "confidence": 0.9313414543867111}]}, {"text": "In order to better assess the quality of CESM, the test set was also evaluated using NIST, which managed to obtain the same rank correlation coefficient of \u03c1 = 0.92.", "labels": [], "entities": [{"text": "NIST", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.9241263270378113}, {"text": "rank correlation coefficient", "start_pos": 124, "end_pos": 152, "type": "METRIC", "confidence": 0.8138799667358398}]}, {"text": "The first step in evaluating at the segment level was filtering the initial test set provided by the WMT12 workshop.", "labels": [], "entities": [{"text": "WMT12 workshop", "start_pos": 101, "end_pos": 115, "type": "DATASET", "confidence": 0.9118632376194}]}, {"text": "For this purpose, 2500 reference and translation segments were selected with a length between 50 and 80 characters.", "labels": [], "entities": []}, {"text": "The Kendall tau rank correlation coefficient was calculated in order to measure the correlation with human judgments, where Kendall tau ) is defined as:", "labels": [], "entities": [{"text": "Kendall tau rank correlation coefficient", "start_pos": 4, "end_pos": 44, "type": "METRIC", "confidence": 0.5786877512931824}]}], "tableCaptions": [{"text": " Table 1. System level evaluation results", "labels": [], "entities": []}]}