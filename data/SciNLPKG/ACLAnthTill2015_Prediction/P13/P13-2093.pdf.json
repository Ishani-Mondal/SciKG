{"title": [{"text": "Dual Training and Dual Prediction for Polarity Classification", "labels": [], "entities": [{"text": "Polarity Classification", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7326694130897522}]}], "abstractContent": [{"text": "Bag-of-words (BOW) is now the most popular way to model text in machine learning based sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.8995595574378967}]}, {"text": "However, the performance of such approach sometimes remains rather limited due to some fundamental deficiencies of the BOW model.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the polarity shift problem, and propose a novel approach, called dual training and dual prediction (DTDP), to address it.", "labels": [], "entities": [{"text": "dual prediction (DTDP)", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.7781935811042786}]}, {"text": "The basic idea of DTDP is to first generate artificial samples that are polarity-opposite to the original samples by polarity reversion, and then leverage both the original and opposite samples for (dual) training and (dual) prediction.", "labels": [], "entities": []}, {"text": "Experimental results on four datasets demonstrate the effectiveness of the proposed approach for polarity classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 97, "end_pos": 120, "type": "TASK", "confidence": 0.9003051817417145}]}], "introductionContent": [{"text": "The most popular text representation model in machine learning based sentiment classification is known as the bag-of-words (BOW) model, where apiece of text is represented by an unordered collection of words, based on which standard machine learning algorithms are employed as classifiers.", "labels": [], "entities": [{"text": "text representation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7087610065937042}, {"text": "machine learning based sentiment classification", "start_pos": 46, "end_pos": 93, "type": "TASK", "confidence": 0.6260790050029754}]}, {"text": "Although the BOW model is simple and has achieved great successes in topic-based text classification, it disrupts word order, breaks the syntactic structures and discards some kinds of semantic information that are possibly very important for sentiment classification.", "labels": [], "entities": [{"text": "BOW", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.6420871615409851}, {"text": "topic-based text classification", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.6032205522060394}, {"text": "sentiment classification", "start_pos": 243, "end_pos": 267, "type": "TASK", "confidence": 0.9467115104198456}]}, {"text": "Such disadvantages sometimes limit the performance of sentiment classification systems.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.9637289643287659}]}, {"text": "A lot of subsequent work focused on feature engineering that aims to find a set of effective features based on the BOW representation.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8536060154438019}]}, {"text": "However, there still remain some problems that are not well addressed.", "labels": [], "entities": []}, {"text": "Out of them, the polarity shift problem is the biggest one.", "labels": [], "entities": [{"text": "polarity shift", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7880011200904846}]}, {"text": "We refer to \"polarity shift\" as a linguistic phenomenon that the sentiment orientation of a text is reversed (from positive to negative or vice versa) because of some particular expressions called polarity shifters.", "labels": [], "entities": []}, {"text": "Negation words (e.g., \"no\", \"not\" and \"don't\") are the most important type of polarity shifter.", "labels": [], "entities": [{"text": "polarity shifter", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.7454871833324432}]}, {"text": "For example, by adding a negation word \"don't\" to a positive text \"I like this book\" in front of \"like\", the orientation of the text is reversed from positive to negative.", "labels": [], "entities": []}, {"text": "Naturally, handling polarity shift is very important for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.9657989740371704}]}, {"text": "However, the BOW representations of two polarity-opposite texts, e.g., \"I like this book\" and \"I don't like this book\", are considered to be very similar by most of machine learning algorithms.", "labels": [], "entities": []}, {"text": "Although some methods have been proposed in the literature to address the polarity shift problem, the state-of-the-art results are still far from satisfactory.", "labels": [], "entities": []}, {"text": "For example, the improvements are less than 2% after considering polarity shift in.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel approach, called dual training and dual prediction (DTDP), to address the polarity shift problem.", "labels": [], "entities": [{"text": "dual prediction (DTDP)", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7697248101234436}]}, {"text": "By taking advantage of the unique nature of polarity classification, DTDP is motivated by first generating artificial samples that are polarity-opposite to the original ones.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.747795045375824}]}, {"text": "For example, given the original sample \"I don't like this book.", "labels": [], "entities": []}, {"text": "It is boring,\" its polarity-opposite version, \"I like this book.", "labels": [], "entities": []}, {"text": "It is interesting\", is artificially generated.", "labels": [], "entities": []}, {"text": "Second, the original and opposite training samples are used together for training a sentiment classifier (called dual training), and the original and opposite test samples are used together for prediction (called dual prediction).", "labels": [], "entities": []}, {"text": "Experimental results prove that the procedure of DTDP is very effective at correcting the training and prediction errors caused by polarity shift, and it beats other alternative methods of considering polarity shift.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Multi-Domain Sentiment Datasets 2 are used for evaluations.", "labels": [], "entities": []}, {"text": "They consist of product reviews collected from four different domains: Book, DVD, Electronics and Kitchen.", "labels": [], "entities": [{"text": "Book", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.9504134058952332}]}, {"text": "Each of them contains 1,000 positive and 1,000 negative reviews.", "labels": [], "entities": []}, {"text": "Each of the datasets is randomly spit into 5 folds, with four folds serving as training data, and the remaining one fold serving as test data.", "labels": [], "entities": []}, {"text": "All of the following results are reported in terms of an average of 5-fold cross validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classification accuracy of different systems using unigram features", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9602136611938477}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9239305853843689}]}, {"text": " Table 2: Classification accuracy of different systems using both unigram and bigram features", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.948493242263794}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9508026838302612}]}]}