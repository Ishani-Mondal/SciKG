{"title": [{"text": "Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers", "labels": [], "entities": [{"text": "Forest-to-String Machine Translation Engine", "start_pos": 12, "end_pos": 55, "type": "TASK", "confidence": 0.7798851877450943}]}], "abstractContent": [{"text": "In this paper we describe Travatar, a forest-to-string machine translation (MT) engine based on tree transducers.", "labels": [], "entities": [{"text": "forest-to-string machine translation (MT) engine", "start_pos": 38, "end_pos": 86, "type": "TASK", "confidence": 0.8400129931313651}]}, {"text": "It provides an open-source C++ implementation for the entire forest-to-string MT pipeline, including rule extraction, tuning, decoding , and evaluation.", "labels": [], "entities": [{"text": "MT pipeline", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.909591555595398}, {"text": "rule extraction", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.8015895783901215}]}, {"text": "There area number of options for model training, and tuning includes advanced options such as hyper-graph MERT, and training of sparse features through online learning.", "labels": [], "entities": [{"text": "MERT", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.8700772523880005}]}, {"text": "The training pipeline is modeled after that of the popular Moses decoder, so users familiar with Moses should be able to get started quickly.", "labels": [], "entities": []}, {"text": "We perform a validation experiment of the decoder on English-Japanese machine translation, and find that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation.", "labels": [], "entities": [{"text": "English-Japanese machine translation", "start_pos": 53, "end_pos": 89, "type": "TASK", "confidence": 0.6383485297362009}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9978450536727905}]}, {"text": "As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder.", "labels": [], "entities": []}, {"text": "Travatar is available under the LGPL at", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the recent trends in statistical machine translation (SMT) is the popularity of models that use syntactic information to help solve problems of long-distance reordering between the source and target language text.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 28, "end_pos": 65, "type": "TASK", "confidence": 0.822007973988851}]}, {"text": "These techniques can be broadly divided into pre-ordering techniques, which first parse and reorder the source sentence into the target order before translating (, and treebased decoding techniques, which take a tree or forest as input and choose the reordering and translation jointly.", "labels": [], "entities": []}, {"text": "While pre-ordering is notable to consider both translation and reordering in a joint model, it is useful in that it is done before the actual translation process, so it can be performed with a conventional translation pipeline using a standard phrase-based decoder such as Moses (.", "labels": [], "entities": []}, {"text": "For tree-to-string systems, on the other hand, it is necessary to have available or create a decoder that is equipped with this functionality, which becomes a bottleneck in the research and development process.", "labels": [], "entities": []}, {"text": "In this demo paper, we describe Travatar, an open-source tree-to-string or forest-to-string translation system that can be used as a tool for translation using source-side syntax, and as a platform for research into syntax-based translation methods.", "labels": [], "entities": []}, {"text": "In particular, compared to other decoders which mainly implement syntax-based translation in the synchronous context-free grammar (SCFG) framework, Travatar is built upon the tree transducer framework (), a richer formalism that can help capture important distinctions between parse trees, as we show in Section 2.", "labels": [], "entities": []}, {"text": "Travatar includes a fully documented training and testing regimen that was modeled around that of Moses, making it possible for users familiar with Moses to get started with Travatar quickly.", "labels": [], "entities": []}, {"text": "The framework of the software is also designed to be extensible, so the toolkit is applicable for other tree-to-string transduction tasks.", "labels": [], "entities": []}, {"text": "In the evaluation of the decoder on EnglishJapanese machine translation, we perform a comparison to Moses's phrase-based, hierarchicalphrase-based, and SCFG-based tree-to-string translation.", "labels": [], "entities": [{"text": "EnglishJapanese machine translation", "start_pos": 36, "end_pos": 71, "type": "TASK", "confidence": 0.5586421092351278}, {"text": "SCFG-based tree-to-string translation", "start_pos": 152, "end_pos": 189, "type": "TASK", "confidence": 0.6035095552603403}]}, {"text": "Based on the results, we find that treeto-string, and particularly forest-to-string, translation using Travatar provides competitive or superior accuracy to all of these techniques.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9966537952423096}]}, {"text": "As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder.", "labels": [], "entities": []}], "datasetContent": [{"text": "For tuning the parameters of the model, Travatar natively supports minimum error rate training (MERT) and is extension to hypergraphs ().", "labels": [], "entities": [{"text": "minimum error rate training (MERT)", "start_pos": 67, "end_pos": 101, "type": "METRIC", "confidence": 0.8538465414728437}]}, {"text": "This tuning can be performed for evaluation measures including BLEU () and RIBES (, with an easily extendable interface that makes it simple to support other measures.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9966200590133667}, {"text": "RIBES", "start_pos": 75, "end_pos": 80, "type": "METRIC", "confidence": 0.9795640110969543}]}, {"text": "There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm, and regularized structured SVMs trained using FOBOS.", "labels": [], "entities": [{"text": "FOBOS", "start_pos": 161, "end_pos": 166, "type": "DATASET", "confidence": 0.7349138855934143}]}, {"text": "There are plans to implement more algorithms such as MIRA or AROW) in the near future.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.8742568492889404}, {"text": "AROW", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.8544167280197144}]}, {"text": "The Travatar toolkit also provides an evaluation program that can calculate the scores of translation output according to various evaluation measures, and calculate the significance of differences between systems using bootstrap resampling).", "labels": [], "entities": []}, {"text": "In our experiments, we validated the performance of the translation toolkit on English-Japanese translation of Wikipedia articles, as specified by the Kyoto Free Translation Task (KFTT).", "labels": [], "entities": [{"text": "English-Japanese translation of Wikipedia articles", "start_pos": 79, "end_pos": 129, "type": "TASK", "confidence": 0.7934668779373169}, {"text": "Kyoto Free Translation Task (KFTT)", "start_pos": 151, "end_pos": 185, "type": "TASK", "confidence": 0.5872664877346584}]}, {"text": "Training used the 405k sentences of training data of length under 60, tuning was performed on the development set, and testing was performed on the test set using the BLEU and RIBES measures.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9974141120910645}, {"text": "RIBES", "start_pos": 176, "end_pos": 181, "type": "METRIC", "confidence": 0.9515201449394226}]}, {"text": "As baseline systems we use the Moses 2 implementation of phrase-based (MOSES-PBMT), hierarchical phrase-based (MOSES-HIER), and treeto-string translation (MOSES-T2S).", "labels": [], "entities": [{"text": "treeto-string translation", "start_pos": 128, "end_pos": 153, "type": "TASK", "confidence": 0.7475603222846985}]}, {"text": "The phrasebased and hierarchical phrase-based models were trained with the default settings according to tutorials on each website.", "labels": [], "entities": []}, {"text": "For all systems, we use a 5-gram Kneser-Ney smoothed language model.", "labels": [], "entities": []}, {"text": "Alignment for each system was performed using either GIZA++ 3 or Nile with main results reported for the aligner that achieved the best accuracy on the dev set, and a further comparison shown in the auxiliary experiments in Section 4.3.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.7259109616279602}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9992843270301819}]}, {"text": "Tuning was performed with minimum error rate training to maximize BLEU over 200-best lists.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9538930654525757}, {"text": "error rate", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9140500128269196}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9907665252685547}]}, {"text": "Tokenization was performed with the Stanford tokenizer for English, and the KyTea word segmenter (Neubig et al., 2011) for Japanese.", "labels": [], "entities": [{"text": "Tokenization", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9615465402603149}]}, {"text": "For all tree-to-string systems we use Egret 5 as an English parser, as we found it to achieve high accuracy, and it allows for the simple output of forests.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9973266124725342}]}, {"text": "Rule extraction was performed using onebest trees, which were right-binarized, and lowercased post-parsing.", "labels": [], "entities": [{"text": "Rule extraction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8424800336360931}]}, {"text": "For Travatar, composed rules of up to size 4 and a maximum of 2 non-terminals and 7 terminals for each rule were used.", "labels": [], "entities": []}, {"text": "Nullaligned words were only attached to the top node, and no count normalization was performed, in contrast to Moses, which performs count normalization and exhaustive null word attachment.", "labels": [], "entities": [{"text": "exhaustive null word attachment", "start_pos": 157, "end_pos": 188, "type": "TASK", "confidence": 0.6398153528571129}]}, {"text": "Decoding was performed over either one-best trees (TRAV-T2S), or over forests including all edges included in the parser 200-best list (TRAV-F2S), and a pop limit of 1000 hypotheses was used for cube BLEU RIBES Rules Sent/s. pruning.", "labels": [], "entities": [{"text": "TRAV-T2S", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.8709714412689209}, {"text": "TRAV-F2S", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.6894991993904114}, {"text": "BLEU RIBES", "start_pos": 200, "end_pos": 210, "type": "METRIC", "confidence": 0.787127822637558}]}], "tableCaptions": [{"text": " Table 1: Translation results (BLEU, RIBES), rule  table size, and speed in sentences per second for  each system. Bold numbers indicate a statistically  significant difference over all other systems (boot- strap resampling with p > 0.05) (Koehn, 2004).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9955965876579285}, {"text": "RIBES", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9338318705558777}, {"text": "speed", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9701140522956848}]}, {"text": " Table 2: Translation results (BLEU, RIBES), for  several translation models (PBMT, Hiero, T2S,  F2S), aligners (GIZA++, Nile), and parsers (Stan- ford, Egret).", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9360694885253906}, {"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9980364441871643}, {"text": "RIBES", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9104907512664795}]}]}