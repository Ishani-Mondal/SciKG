{"title": [{"text": "Why-Question Answering using Intra-and Inter-Sentential Causal Relations", "labels": [], "entities": [{"text": "Why-Question Answering", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9058140814304352}]}], "abstractContent": [{"text": "In this paper, we explore the utility of intra-and inter-sentential causal relations between terms or clauses as evidence for answering why-questions.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work that uses both intra-and inter-sentential causal relations for why-QA.", "labels": [], "entities": []}, {"text": "We also propose a method for assessing the appropriate-ness of causal relations as answers to a given question using the semantic orientation of excitation proposed by Hashimoto et al.", "labels": [], "entities": []}, {"text": "By applying these ideas to Japanese why-QA, we improved precision by 4.4% against all the questions in our test set over the current state-of-the-art system for Japanese why-QA.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9996731281280518}]}, {"text": "In addition , unlike the state-of-the-art system, our system could achieve very high precision (83.2%) for 25% of all the questions in the test set by restricting its output to the confident answers only.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9988208413124084}]}], "introductionContent": [{"text": "\"Why-question answering\" (why-QA) is a task to retrieve answers from a given text archive fora why-question, such as \"Why are tsunamis generated?\"", "labels": [], "entities": [{"text": "Why-question answering\" (why-QA) is a task to retrieve answers from a given text archive fora why-question, such as \"Why are tsunamis generated?\"", "start_pos": 1, "end_pos": 146, "type": "Description", "confidence": 0.7529018747395483}]}, {"text": "The answers are usually text fragments consisting of one or more sentences.", "labels": [], "entities": []}, {"text": "Although much research exists on this task), its performance remains much lower than that of the state-of-the-art factoid QA systems, such as IBM's.", "labels": [], "entities": []}, {"text": "In this work, we propose a quite straightforward but novel approach for such difficult why-QA task.", "labels": [], "entities": []}, {"text": "Consider the sentence A1 in, which represents the causal relation between the cause, \"the ocean's water mass ..., waves are gen- erated,\" and its effect, \"Tsunamis ... are generated.\"", "labels": [], "entities": []}, {"text": "This is a good answer to the question, \"Why are tsunamis generated?\", since the effect part is more or less equivalent to the (propositional) content of the question.", "labels": [], "entities": []}, {"text": "Our method finds text fragments that include such causal relations with an effect part that resembles a given question and provides them as answers.", "labels": [], "entities": []}, {"text": "Since this idea looks quite intuitive, many people would probably consider it as a solution to why-QA.", "labels": [], "entities": []}, {"text": "However, to our surprise, we could not find any previous work on why-QA that took this approach.", "labels": [], "entities": []}, {"text": "Some methods utilized the causal relations between terms as evidence for finding answers (i.e., matching a cause term with an answer text and its effect term with a question).", "labels": [], "entities": []}, {"text": "Other approaches utilized such clue terms for causality as \"because\" as evidence for finding answers.", "labels": [], "entities": []}, {"text": "However, these algorithms did not check whether an answer candidate, i.e., a text fragment that maybe provided as an answer, explicitly contains a complex causal relation sen-tence with the effect part that resembles a question.", "labels": [], "entities": []}, {"text": "For example, A5 in is an incorrect answer to \"Why are tsunamis generated?\", but these previous approaches would probably choose it as a proper answer due to \"because\" and \"earthquake\" (i.e., a cause of tsunamis).", "labels": [], "entities": [{"text": "A5", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9717980027198792}]}, {"text": "At least in our experimental setting, our approach outperformed these simpler causality-based QA systems.", "labels": [], "entities": []}, {"text": "Perhaps this approach was previously deemed infeasible due to two non-trivial technical challenges.", "labels": [], "entities": []}, {"text": "The first challenge is to accurately identify a wide range of causal relations like those in in answer candidates.", "labels": [], "entities": []}, {"text": "To meet this challenge, we developed a sequence labeling method that identifies not only intra-sentential causal relations, i.e., the causal relations between two terms/phrases/clauses expressed in a single sentence (e.g., A1 in), but also the intersentential causal relations, which are the causal relations between two terms/phrases/clauses expressed in two adjacent sentences (e.g., A2) in a given text fragment.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.6725138276815414}]}, {"text": "The second challenge is assessing the appropriateness of each identified causal relation as an answer to a given question.", "labels": [], "entities": []}, {"text": "This is important since the causal relations identified in the answer candidates may have nothing to do with a given question.", "labels": [], "entities": []}, {"text": "In this case, we have to reject these causal relations because they are inappropriate as an answer to the question.", "labels": [], "entities": []}, {"text": "When a single answer candidate contains many causal relations, we also have to select the appropriate ones.", "labels": [], "entities": []}, {"text": "Consider the causal relations in A1-A4.", "labels": [], "entities": []}, {"text": "Those in A1-A3 are appropriate answers to \"Why are tsunamis generated?\", but not the one in A4.", "labels": [], "entities": []}, {"text": "To assess the appropriateness, the system must recognize textual entailment, i.e., \"tsunamis (are) generated\" in the question is entailed by all \"tsunamis are generated\" in A1, \"cause a tsunami\" in A2 and \"tsunamis are caused\" in A3 but not by \"tsunamis weaken\" in A4.", "labels": [], "entities": []}, {"text": "This quite difficult task is currently being studied by many researchers in the RTE field).", "labels": [], "entities": [{"text": "RTE", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9617305397987366}]}, {"text": "To meet this challenge, we developed a relatively simple method that can be seen as a lightweight approximation for this difficult RTE task, using excitation polarities ( ).", "labels": [], "entities": [{"text": "RTE task", "start_pos": 131, "end_pos": 139, "type": "TASK", "confidence": 0.9174718856811523}]}, {"text": "Through our experiments on Japanese why-QA, we show that a combination of the above methods can improve why-QA accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9688540697097778}]}, {"text": "In addition, our proposed method can be successfully combined with other approaches to why-QA and can contribute to higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9980282187461853}]}, {"text": "As a final result, we improved the precision by 4.4% against all the questions in our test set over the current state-of-the-art system of Japanese why-QA ( . The difference in the performance became much larger when we only compared the highly confident answers of each system.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9995997548103333}]}, {"text": "When we made our system provide only its confident answers according to their confidence score given by our system, the precision of these confident answers was 83.2% for 25% of all the questions in our test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9996107220649719}]}, {"text": "In the same setting, the precision of the state-of-the-art system ( ) was only 62.4%.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9997889399528503}]}], "datasetContent": [{"text": "We experimented with causal relation recognition and why-QA with our causal relation features.", "labels": [], "entities": [{"text": "causal relation recognition", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.6686608890692393}]}], "tableCaptions": [{"text": " Table 5: Results of causal relation recognition (%)", "labels": [], "entities": [{"text": "causal relation recognition", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.7838988502820333}]}, {"text": " Table 6: Ablation test results for causal relation  recognition (%)", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9978783130645752}, {"text": "causal relation  recognition", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.7007726530234019}]}, {"text": " Table 7: Why-QA results (%)", "labels": [], "entities": [{"text": "Why-QA", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9278594851493835}]}, {"text": " Table 8: Results with/without intra-and inter- sentential causal relations (%)", "labels": [], "entities": []}, {"text": " Table 9: Ablation test results for why-QA (%)", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989839196205139}, {"text": "why-QA", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9631292223930359}]}]}