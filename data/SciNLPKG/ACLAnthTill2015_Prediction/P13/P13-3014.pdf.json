{"title": [{"text": "Question Analysis for Polish Question Answering", "labels": [], "entities": [{"text": "Question Analysis", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7806914448738098}, {"text": "Polish Question Answering", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6910335620244344}]}], "abstractContent": [{"text": "This study is devoted to the problem of question analysis fora Polish question answering system.", "labels": [], "entities": [{"text": "question analysis", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.8508349359035492}, {"text": "Polish question answering system", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.7031338885426521}]}, {"text": "The goal of the question analysis is to determine its general structure , type of an expected answer and create a search query for finding relevant documents in a textual knowledge base.", "labels": [], "entities": []}, {"text": "The paper contains an overview of available solutions of these problems, description of their implementation and presents an evaluation based on a set of 1137 questions from a Polish quiz TV show.", "labels": [], "entities": []}, {"text": "The results help to understand how an environment of a Slavonic language affects the performance of methods created for English.", "labels": [], "entities": []}], "introductionContent": [{"text": "The main motivation for building Question Answering (QA) systems is that they relieve a user of a need to translate his problem to a machinereadable form.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.8487499952316284}]}, {"text": "To make it possible, we need to equip a computer system with an ability to understand requests in a natural language, find answers in a knowledge base and formulate them in the natural language.", "labels": [], "entities": []}, {"text": "The aim of this paper is to deal with the first of these steps, i.e. question analysis module.", "labels": [], "entities": [{"text": "question analysis module", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.86077348391215}]}, {"text": "It accepts the question as an input and returns a data structure containing relevant information, herein called question model.", "labels": [], "entities": []}, {"text": "It consists of two elements: a question type and a search query.", "labels": [], "entities": []}, {"text": "The question type classifies a question to one of the categories based on its structure.", "labels": [], "entities": []}, {"text": "A general question type takes one of the following values: verification (Czy Lee Oswald zabi\u0142 Johna Kennedy'ego?, Eng.", "labels": [], "entities": [{"text": "verification", "start_pos": 59, "end_pos": 71, "type": "METRIC", "confidence": 0.9881453514099121}]}, {"text": "Did Lee Oswald kill John Kennedy?), option choosing (Kt\u00f3ry z nich zabi\u0142 Johna Kennedy'ego: Lance Oswald czy Lee Oswald?, Eng.", "labels": [], "entities": []}, {"text": "Which one killed John Kennedy: Lance Oswald or Lee Oswald?), named entity (Kto zabi\u0142 Johna Kennedy'ego?, Eng.", "labels": [], "entities": []}, {"text": "Who killed John Kennedy?), unnamed entity (Czego u\u02d9 zy\u0142 Lee Oswald, \u02d9 zeby zabi\u00b4czabi\u00b4c Johna Kennedy'ego?, Eng.", "labels": [], "entities": []}, {"text": "What did Lee Oswald use to kill John Kennedy?), other name fora given named entity (Jakiego pseudonimu u\u02d9 zywa\u0142 John Kennedy w trakcie s\u0142u\u02d9 zby wojskowej?, Eng.", "labels": [], "entities": []}, {"text": "What nickname did John Kennedy use during his military service?) and multiple entities (Kt\u00f3rzy prezydenci Stan\u00f3w Zjednoczonych zostali zabici w trakcie kadencji?, Eng. Which U.S. presidents were assassinated in office?).", "labels": [], "entities": []}, {"text": "There are many others possible, such as definition or explanation questions, but they require specific techniques for answer finding and remain beyond the scope of this work.", "labels": [], "entities": [{"text": "definition or explanation questions", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.9022905230522156}, {"text": "answer finding", "start_pos": 118, "end_pos": 132, "type": "TASK", "confidence": 0.869148850440979}]}, {"text": "For example, the Question Answering for Machine Reading Evaluation (QA4MRE) competition) included these complex questions (e.g. What caused X?, How did X happen?, Why did X happen?).", "labels": [], "entities": [{"text": "Question Answering for Machine Reading Evaluation (QA4MRE) competition", "start_pos": 17, "end_pos": 87, "type": "TASK", "confidence": 0.8439920365810394}]}, {"text": "In case of named entity questions, it is also useful to find its named entity type, corresponding to a type of an entity which could be provided as an answer.", "labels": [], "entities": []}, {"text": "A list of possible options, suited to questions about general knowledge, is given in.", "labels": [], "entities": []}, {"text": "As some of the categories include others (e.g. CITY is a PLACE), the goal of a classifier is to find the narrowest available.", "labels": [], "entities": []}, {"text": "The need fora search query is motivated by performance reasons.", "labels": [], "entities": []}, {"text": "A linguistic analysis applied to a source text to find the expected answer is usually resource-consuming, so it cannot be performed on the whole corpus (in case of this experiment 839,269 articles).", "labels": [], "entities": []}, {"text": "To avoid it, we transform the question into the search query, which is subsequently used in a search engine, incorporating a full-text index of the corpus.", "labels": [], "entities": []}, {"text": "As a result we get a list of documents, possibly related to the question.", "labels": [], "entities": []}, {"text": "Although the query generation plays an auxiliary role, failure at this stage may lead both to too long processing times (in case of excessive number of returned documents) and lack of a final answer (in  case of not returning a relevant document).", "labels": [], "entities": []}], "datasetContent": [{"text": "For the purpose of evaluation, a set of 1137 questions from a Polish quiz TV show \"Jeden z dziesi\u02db eciu\", published in, has been manually reviewed and updated.", "labels": [], "entities": [{"text": "evaluation", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.9625357985496521}]}, {"text": "A general question type and a named entity type has been assigned to each of the questions.", "labels": [], "entities": []}, {"text": "presents the number of question types occurrences in the test set.", "labels": [], "entities": []}, {"text": "As a source corpus, a textual version of the Polish Wikipedia has been used.", "labels": [], "entities": [{"text": "Polish Wikipedia", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.8593628704547882}]}, {"text": "To evaluate query generation an article name has been assigned to those questions (1057), for which a single article in Wikipedia containing an answer exists.", "labels": [], "entities": [{"text": "query generation", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7870158851146698}]}, {"text": "Outputs of type classifiers have been gathered: Accuracy of the four question type classifiers: numbers of questions classified, percentages of correct answers and products of these two. and compared to the expected ones.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9928399324417114}]}, {"text": "The machine learning classifiers have been evaluated using 100-fold cross-validation . Four of the presented improvements of query generation tested here include: basic OR query, AND query with fallback to OR, focus segments removal and expansion with synonyms.", "labels": [], "entities": [{"text": "query generation", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.7606008350849152}, {"text": "focus segments removal", "start_pos": 210, "end_pos": 232, "type": "TASK", "confidence": 0.583030084768931}]}, {"text": "For each of those, three types of segment matching strategies have been applied: exact, stemming-based and fuzzy.", "labels": [], "entities": [{"text": "segment matching", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.8052147328853607}]}, {"text": "The recorded results include recall (percentage of result lists including the desired article among the first 100) and average position of the article in the list.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9996941089630127}]}], "tableCaptions": [{"text": " Table 1: The 6 general question types and the 31  named entity types and numbers of their occur- rences in the test set.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy of the four question type classi- fiers: numbers of questions classified, percentages  of correct answers and products of these two.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980584979057312}]}, {"text": " Table 3. It turns out that the  basic technique generally yields the best result.  Starting with an AND query and using OR only  in case of a failure leads to an improvement of the  expected article ranking position but the recall ra- tio drops significantly, which means that quite of- ten the results of a restrictive query do not include  the relevant article. The removal of the question  focus from the list of keywords also has a nega- tive impact on performance. The most surprising X X X X X X X X", "labels": [], "entities": [{"text": "recall ra-", "start_pos": 225, "end_pos": 235, "type": "METRIC", "confidence": 0.9469065268834432}]}, {"text": " Table 3: Results of the four considered query gen- eration techniques, each with the three types of  matching strategy. For each combination a recall  (measured by the presence of a given source docu- ment in the first 100 returned) and an average po- sition on the ranked list is given.", "labels": [], "entities": [{"text": "query gen- eration", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.637197308242321}, {"text": "recall", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.9987461566925049}]}]}