{"title": [{"text": "Joint Event Extraction via Structured Prediction with Global Features", "labels": [], "entities": [{"text": "Joint Event Extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7247596780459086}, {"text": "Structured Prediction", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.8648294806480408}]}], "abstractContent": [{"text": "Traditional approaches to the task of ACE event extraction usually rely on sequential pipelines with multiple stages, which suffer from error propagation since event triggers and arguments are predicted in isolation by independent local classifiers.", "labels": [], "entities": [{"text": "ACE event extraction", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.9179627299308777}]}, {"text": "By contrast, we propose a joint framework based on structured prediction which extracts triggers and arguments together so that the local predictions can be mutually improved.", "labels": [], "entities": []}, {"text": "In addition, we propose to incorporate global features which explicitly capture the dependencies of multiple triggers and arguments.", "labels": [], "entities": []}, {"text": "Experimental results show that our joint approach with local features outperforms the pipelined baseline, and adding global features further improves the performance significantly.", "labels": [], "entities": []}, {"text": "Our approach advances state-of-the-art sentence-level event extraction, and even outperforms previous argument labeling methods which use external knowledge from other sentences and documents.", "labels": [], "entities": [{"text": "sentence-level event extraction", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.6131591697533926}, {"text": "argument labeling", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.7534197568893433}]}], "introductionContent": [{"text": "Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments.", "labels": [], "entities": [{"text": "Event extraction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7886544466018677}, {"text": "Information Extraction (IE)", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.86556955575943}]}, {"text": "Most state-of-the-art approaches () use sequential pipelines as building blocks, which breakdown the whole task into separate subtasks, such as trigger identification/classification and argument identification/classification.", "labels": [], "entities": [{"text": "trigger identification/classification", "start_pos": 144, "end_pos": 181, "type": "TASK", "confidence": 0.8148785680532455}, {"text": "argument identification/classification", "start_pos": 186, "end_pos": 224, "type": "TASK", "confidence": 0.8299494534730911}]}, {"text": "As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream classifiers.", "labels": [], "entities": []}, {"text": "The downstream components, however, cannot impact earlier decisions.", "labels": [], "entities": []}, {"text": "For example, consider the following sentences with an ambiguous word \"fired\": (1) In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.", "labels": [], "entities": [{"text": "Palestine Hotel", "start_pos": 146, "end_pos": 161, "type": "DATASET", "confidence": 0.9298998713493347}]}, {"text": "(2) He has fired his air defense chief . In sentence (1), \"fired\" is a trigger of type Attack.", "labels": [], "entities": []}, {"text": "Because of the ambiguity, a local classifier may miss it or mislabel it as a trigger of End-Position.", "labels": [], "entities": []}, {"text": "However, knowing that \"tank\" is very likely to bean Instrument argument of Attack events, the correct event subtype assignment of \"fired\" is obviously Attack.", "labels": [], "entities": [{"text": "Instrument", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9559581279754639}]}, {"text": "Likewise, in sentence (2), \"air defense chief\" is a job title, hence the argument classifier is likely to label it as an Entity argument for End-Position trigger.", "labels": [], "entities": []}, {"text": "In addition, the local classifiers are incapable of capturing inter-dependencies among multiple event triggers and arguments.", "labels": [], "entities": []}, {"text": "Consider sentence (1) again.", "labels": [], "entities": []}, {"text": "depicts the corresponding event triggers and arguments.", "labels": [], "entities": []}, {"text": "The dependency between \"fired\" and \"died\" cannot be captured by the local classifiers, which may fail to attach \"cameraman\" to \"fired\" as a Target argument.", "labels": [], "entities": []}, {"text": "By using global features, we can propagate the Victim argument of the Die event to the Target argument of the Attack event.", "labels": [], "entities": []}, {"text": "As another example, knowing that an Attack event usually only has one Attacker argument, we could penalize assignments in which one trigger has more than one Attacker.", "labels": [], "entities": []}, {"text": "Such global features cannot be easily exploited by a local classifier.", "labels": [], "entities": []}, {"text": "Therefore, we take afresh look at this problem and formulate it, for the first time, as a structured learning problem.", "labels": [], "entities": []}, {"text": "We propose a novel joint event extraction algorithm to predict the triggers and arguments simultaneously, and use the structured perceptron to train the joint model.", "labels": [], "entities": [{"text": "joint event extraction", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.6598535180091858}]}, {"text": "This way we can capture the dependencies between triggers and argument as well as explore.", "labels": [], "entities": []}, {"text": "There are two event mentions that share three arguments, namely the Die event mention triggered by \"died\", and the Attack event mention triggered by \"fired\".", "labels": [], "entities": [{"text": "Die event mention triggered", "start_pos": 68, "end_pos": 95, "type": "METRIC", "confidence": 0.9058837890625}, {"text": "Attack event mention", "start_pos": 115, "end_pos": 135, "type": "METRIC", "confidence": 0.9157128731409708}]}, {"text": "arbitrary global features over multiple local predictions.", "labels": [], "entities": []}, {"text": "However, different from easier tasks such as part-of-speech tagging or noun phrase chunking where efficient dynamic programming decoding is feasible, here exact joint inference is intractable.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.7197272479534149}, {"text": "noun phrase chunking", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.6945187648137411}]}, {"text": "Therefore we employ beam search in decoding, and train the model using the early-update perceptron variant tailored for beam search.", "labels": [], "entities": [{"text": "beam search", "start_pos": 120, "end_pos": 131, "type": "TASK", "confidence": 0.8115894496440887}]}, {"text": "We make the following contributions: 1.", "labels": [], "entities": []}, {"text": "Different from traditional pipeline approach, we present a novel framework for sentencelevel event extraction, which predicts triggers and their arguments jointly (Section 3).", "labels": [], "entities": [{"text": "sentencelevel event extraction", "start_pos": 79, "end_pos": 109, "type": "TASK", "confidence": 0.6970714131991068}]}, {"text": "2. We develop a rich set of features for event extraction which yield promising performance even with the traditional pipeline (Section 3.4.1).", "labels": [], "entities": [{"text": "event extraction", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.7931329607963562}]}, {"text": "In this paper we refer to them as local features.", "labels": [], "entities": []}, {"text": "3. We introduce various global features to exploit dependencies among multiple triggers and arguments (Section 3.4.2).", "labels": [], "entities": []}, {"text": "Experiments show that our approach outperforms the pipelined approach with the same set of local features, and significantly advances the state-of-the-art with the addition of global features which brings a notable further improvement (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "We utilized the ACE 2005 corpus as our testbed.", "labels": [], "entities": [{"text": "ACE 2005 corpus", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.9745923280715942}]}, {"text": "For comparison, we used the same test set with 40 newswire articles (672 sentences) as in for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set.", "labels": [], "entities": []}, {"text": "The rest 529 documents (14, 840 sentences) are used for training.", "labels": [], "entities": []}, {"text": "Following previous work), we use the following criteria to determine the correctness of an predicted event mention: \u2022 A trigger is correct if its event subtype and offsets match those of a reference trigger.", "labels": [], "entities": []}, {"text": "\u2022 An argument is correctly identified if its event subtype and offsets match those of any of the reference argument mentions.", "labels": [], "entities": []}, {"text": "\u2022 An argument is correctly identified and classified if its event subtype, offsets and argument role match those of any of the reference argument mentions.", "labels": [], "entities": []}, {"text": "Finally we use Precision (P), Recall (R) and Fmeasure (F 1 ) to evaluate the overall performance.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 15, "end_pos": 28, "type": "METRIC", "confidence": 0.9575768858194351}, {"text": "Recall (R)", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9637180864810944}, {"text": "Fmeasure (F 1 )", "start_pos": 45, "end_pos": 60, "type": "METRIC", "confidence": 0.9718988537788391}]}], "tableCaptions": [{"text": " Table 3: Top 5 event subtypes that co-occur with  Attack event in the same sentence.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of training time and accuracy on the dev set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9995406866073608}]}, {"text": " Table 5: Comparison between the performance  (%) of standard-update and early-update with  global features. Here b stands for beam size.", "labels": [], "entities": []}, {"text": " Table 6: Overall performance with gold-standard entities, timex, and values.  \u2020 beyond sentence level.", "labels": [], "entities": []}, {"text": " Table 7: Overall performance (%) with predicted  entities, timex, and values. \u2193 indicates the perfor- mance drop from experiments with gold-standard  argument candidates (see", "labels": [], "entities": []}]}