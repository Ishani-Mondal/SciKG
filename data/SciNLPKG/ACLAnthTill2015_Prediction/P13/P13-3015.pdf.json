{"title": [{"text": "A Comparison of Techniques to Automatically Identify Complex Words", "labels": [], "entities": [{"text": "Automatically Identify Complex Words", "start_pos": 30, "end_pos": 66, "type": "TASK", "confidence": 0.7899266108870506}]}], "abstractContent": [{"text": "Identifying complex words (CWs) is an important, yet often overlooked, task within lexical simplification (The process of automatically replacing CWs with simpler alternatives).", "labels": [], "entities": [{"text": "Identifying complex words (CWs)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9155759215354919}]}, {"text": "If too many words are identified then substitutions maybe made erroneously, leading to a loss of meaning.", "labels": [], "entities": []}, {"text": "If too few words are identified then those which impede a user's understanding maybe missed, resulting in a complex final text.", "labels": [], "entities": []}, {"text": "This paper addresses the task of evaluating different methods for CW identification.", "labels": [], "entities": [{"text": "CW identification", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.9618914723396301}]}, {"text": "A corpus of sentences with annotated CWs is mined from Simple Wikipedia edit histories, which is then used as the basis for several experiments.", "labels": [], "entities": [{"text": "Simple Wikipedia edit histories", "start_pos": 55, "end_pos": 86, "type": "DATASET", "confidence": 0.7630893662571907}]}, {"text": "Firstly, the corpus design is explained and the results of the validation experiments using human judges are reported.", "labels": [], "entities": []}, {"text": "Experiments are carried out into the CW identification techniques of: simplifying everything , frequency thresholding and training a support vector machine.", "labels": [], "entities": [{"text": "CW identification", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.942644327878952}, {"text": "frequency thresholding", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.6117897480726242}]}, {"text": "These are based upon previous approaches to the task and show that thresholding does not perform significantly differently to the more na\u00a8\u0131vena\u00a8\u0131ve technique of simplifying everything.", "labels": [], "entities": []}, {"text": "The support vector machine achieves a slight increase in precision over the other two methods, but at the cost of a dramatic trade off in recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9994232654571533}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9984569549560547}]}], "introductionContent": [{"text": "Complex Word (CW) identification is an important task at the first stage of lexical simplification and errors introduced or avoided here will affect final results.", "labels": [], "entities": [{"text": "Complex Word (CW) identification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6268074661493301}, {"text": "lexical simplification", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.6897412091493607}]}, {"text": "This work looks at the process of automatically identifying difficult words fora lexical simplification system.", "labels": [], "entities": []}, {"text": "Lexical simplification is the task of identifying and replacing CWs in a text to improve the overall understandability and readability.", "labels": [], "entities": [{"text": "Lexical simplification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8829725682735443}]}, {"text": "This is a difficult task which is computationally expensive and often inadequately accurate.", "labels": [], "entities": []}, {"text": "Lexical simplification is just one method of text simplification and is often deployed alongside other simplification methods.", "labels": [], "entities": [{"text": "Lexical simplification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8608681559562683}, {"text": "text simplification", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7613376677036285}]}, {"text": "Syntactic simplification, statistical machine translation and semantic simplification (or explanation generation) are all current methods of text simplification.", "labels": [], "entities": [{"text": "Syntactic simplification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8659655451774597}, {"text": "statistical machine translation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.7378679712613424}, {"text": "semantic simplification", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.7237303107976913}, {"text": "explanation generation", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.7198144346475601}, {"text": "text simplification", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.7531571388244629}]}, {"text": "Text simplification is typically deployed as an assistive technology, although this is not always the case.", "labels": [], "entities": [{"text": "Text simplification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7783667743206024}]}, {"text": "It may also be used alongside other technologies such as summarisation to improve their final results.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.9925691485404968}]}, {"text": "Identifying CWs is a task which every lexical simplification system must perform, either explicitly or implicitly, before simplification can take place.", "labels": [], "entities": [{"text": "Identifying CWs", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.934302419424057}]}, {"text": "CWs are difficult to define, which makes them difficult to identify.", "labels": [], "entities": []}, {"text": "For example, take the following sentence: The four largest islands are Honshu, Hokkaido, Shikoku, and Kyushu, and there are approximately 3,000 smaller islands in the chain.", "labels": [], "entities": [{"text": "Honshu", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.8905925750732422}]}, {"text": "In the above sentence, we might identify the proper nouns (Honshu, Hokkaido, etc.) as complex (as they maybe unfamiliar) or we may choose to discount them from our scheme altogether, as proper nouns are unlikely to have any valid replacements.", "labels": [], "entities": []}, {"text": "If we discount the proper nouns then the other valid CW would be 'approximately'.", "labels": [], "entities": []}, {"text": "At 13 characters it is more than twice the average of 5.7 characters per word and has more syllables than any other word.", "labels": [], "entities": []}, {"text": "Further, CWs are often identified by their frequency (see Section 2.1) and here, 'approximately' exhibits a much lower frequency than the other words.", "labels": [], "entities": []}, {"text": "There are many reasons to evaluate the identification of CWs.", "labels": [], "entities": [{"text": "identification of CWs", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.875962495803833}]}, {"text": "This research stems primarily from the discovery that no previous comparison of current techniques exists.", "labels": [], "entities": []}, {"text": "It is hoped that by providing this, the community will be able to identify and evaluate new techniques using the methods proposed herein.", "labels": [], "entities": []}, {"text": "If CW identification is not performed well, then potential candidates maybe missed, and simple words maybe falsely identified.", "labels": [], "entities": [{"text": "CW identification", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.9030208587646484}]}, {"text": "This is dangerous as simplification will often result in a minor change in a text's semantics.", "labels": [], "entities": []}, {"text": "For example, the sentence: The United Kingdom is a state in northwest Europe.", "labels": [], "entities": []}, {"text": "May be simplified to give: The United Kingdom is a country in northwest Europe.", "labels": [], "entities": []}, {"text": "In this example from the corpus used in this research, the word \"state\" is simplified to give \"country\".", "labels": [], "entities": []}, {"text": "Whilst this is a valid synonym in the given context, state and country are not necessarily semantically identical.", "labels": [], "entities": []}, {"text": "Broadly speaking, state refers to apolitical entity, whereas country refers to a physical space within a set of borders.", "labels": [], "entities": []}, {"text": "This is an acceptable change and even necessary for simplification.", "labels": [], "entities": []}, {"text": "However, if applied blindly, then too many modifications maybe made, resulting in major deviations from the text's original semantics.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows: \u2022 A report on the corpus developed and used in the evaluation phase.", "labels": [], "entities": []}, {"text": "\u2022 The implementation of a support vector machine for the classification of CWs.", "labels": [], "entities": [{"text": "classification of CWs", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.8647243976593018}]}, {"text": "Section 2.6 \u2022 A comparison of common techniques on the same corpus.", "labels": [], "entities": []}, {"text": "\u2022 An analysis of the features used in the support vector machine.", "labels": [], "entities": []}], "datasetContent": [{"text": "Several systems for detecting CWs were implemented and evaluated using the CW corpus.", "labels": [], "entities": [{"text": "detecting CWs", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.7940201163291931}, {"text": "CW corpus", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.8535429835319519}]}, {"text": "The two main techniques that exist in the literature are simplifying everything) has been shown to give a higher score, however this data was not available during the course of this research. and frequency based thresholding ().", "labels": [], "entities": []}, {"text": "These were implemented as well as a support vector machine classifier.", "labels": [], "entities": []}, {"text": "This section describes the design decisions made during implementation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The results of different exper- iments on the SemEval lexical simplifi- cation data. These show that SUBTLEX  was the best word frequency measure for  rating lexical complexity. The other en- tries correspond to alternative word fre- quency measures. The Google Web 1T  data (Brants", "labels": [], "entities": [{"text": "SemEval lexical simplifi- cation data", "start_pos": 56, "end_pos": 93, "type": "DATASET", "confidence": 0.5331561813751856}, {"text": "Google Web 1T  data", "start_pos": 265, "end_pos": 284, "type": "DATASET", "confidence": 0.912133663892746}, {"text": "Brants", "start_pos": 286, "end_pos": 292, "type": "DATASET", "confidence": 0.5866946578025818}]}, {"text": " Table 3. The values presented are the mean of 5  trials and the error bars represent the standard de- viation.", "labels": [], "entities": []}, {"text": " Table 2: The correlation coefficients for  each feature. These show the correlation  against the language's simplicity and so  a positive correlation indicates that if that  feature is higher then the word will be  simpler.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9805926084518433}]}]}