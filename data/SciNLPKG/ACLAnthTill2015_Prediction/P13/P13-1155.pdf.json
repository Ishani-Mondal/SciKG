{"title": [{"text": "Social Text Normalization using Contextual Graph Random Walks", "labels": [], "entities": [{"text": "Social Text Normalization", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6406760712464651}]}], "abstractContent": [{"text": "We introduce asocial media text normal-ization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.8290719389915466}]}, {"text": "The proposed system is based on unsupervised learning of the normalization equivalences from unla-beled text.", "labels": [], "entities": []}, {"text": "The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus.", "labels": [], "entities": []}, {"text": "We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4).", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9986909031867981}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9996662139892578}]}, {"text": "When used as a preprocessing step fora state-of-the-art machine translation system, the translation quality on social media text improved by 6%.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7530745267868042}]}, {"text": "The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social Media text is usually very noisy and contains a lot of typos, ad-hoc abbreviations, phonetic substitutions, customized abbreviations and slang language.", "labels": [], "entities": []}, {"text": "The social media text is evolving with new entities, words and expressions.", "labels": [], "entities": []}, {"text": "Natural language processing and understanding systems such as Machine Translation, Information Extraction and Text-to-Speech are usually trained and optimized for clean data; therefore such systems would face a challenging problem with social media text.", "labels": [], "entities": [{"text": "Natural language processing and understanding", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.6780546128749847}, {"text": "Machine Translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7999185919761658}, {"text": "Information Extraction", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.7420452833175659}]}, {"text": "Various social media genres developed distinct characteristics.", "labels": [], "entities": []}, {"text": "For example, SMS developed a nature of shortening messages to avoid multiple keystrokes.", "labels": [], "entities": []}, {"text": "On the other hand, Facebook and instant messaging developed another genre where more emotional expressions and different abbreviations are very common.", "labels": [], "entities": []}, {"text": "Somewhere in between, Twitter's statuses come with some brevity similar to SMS along with the social aspect of Facebook.", "labels": [], "entities": []}, {"text": "On the same time, various social media genres share many characteristics and typo styles.", "labels": [], "entities": []}, {"text": "For example, repeating letters or punctuation for emphasizing and emotional expression such as \"'goooood morniiing\"'.", "labels": [], "entities": []}, {"text": "Using phonetic spelling in a generalized way or to reflect a local accent; such as \"'wuz up bro\"' (what is up brother).", "labels": [], "entities": []}, {"text": "Eliminating vowels such as \"'cm to c my luv\"'.", "labels": [], "entities": []}, {"text": "Substituting numbers for letters such as \"'4get\"' (forget) , \"'2morrow\"' (tomorrow), and \"'b4\"' (before).", "labels": [], "entities": []}, {"text": "Substituting phonetically similar letters such as \"'phone\"' (fon).", "labels": [], "entities": []}, {"text": "Slang abbreviations which usually abbreviates multi-word expression such as \"'LMS\"' (like my status) , \"'idk\"' (i do not know), \"'rofl\"' (rolling on floor laughing).", "labels": [], "entities": []}, {"text": "While social media genres share many characteristics, they have significant differences as well.", "labels": [], "entities": []}, {"text": "It is crucial to have a solution for text normalization that can adapt to such variations automatically.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8065276145935059}]}, {"text": "We propose a text normalization approach using an unsupervised method to induce normalization equivalences from noisy data which can adapt to any genre of social media.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7772833704948425}]}, {"text": "In this paper, we focus on providing a solution for social media text normalization as a preprocessing step for NLP applications.", "labels": [], "entities": [{"text": "social media text normalization", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.6140769347548485}]}, {"text": "However, this is a challenging problem for several reasons.", "labels": [], "entities": []}, {"text": "First, it is not straightforward to define the Out-ofVocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "Traditionally, an OOV word is defined as a word that does not exist in the vocabulary of a given system.", "labels": [], "entities": []}, {"text": "However, this definition is not adequate for the social media text which has a very dynamic nature.", "labels": [], "entities": []}, {"text": "Many words and named entities that do not exist in a given vocabulary should not be considered for normalization.", "labels": [], "entities": []}, {"text": "Second, same OOV word may have many appropriate normalization depending on the context and on the domain.", "labels": [], "entities": []}, {"text": "Third, text normalization as a preprocessing step should have very high precision; in other words, it should provide conservative and confident normalization and not overcorrect.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.8112552165985107}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9989374279975891}]}, {"text": "Moreover, the text normalization should have high recall, as well, to have a good impact on the NLP applications.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7899698913097382}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9995123147964478}]}, {"text": "In this paper, we introduce asocial media text normalization system which addresses the challenges mentioned above.", "labels": [], "entities": [{"text": "asocial media text normalization", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.6114515513181686}]}, {"text": "The proposed system is based on constructing a lattice from possible normalization candidates and finding the best normalization sequence according to an n-gram language model using a Viterbi decoder.", "labels": [], "entities": []}, {"text": "We propose an unsupervised approach to learn the normalization candidates from unlabeled text data.", "labels": [], "entities": []}, {"text": "The proposed approach uses Random Walks on a contextual similarity graph constructed form n-gram sequences on large unlabeled text corpus.", "labels": [], "entities": []}, {"text": "The proposed approach is very scalable, accurate and adaptive to any domain and language.", "labels": [], "entities": []}, {"text": "We evaluate the approach on the normalization task as well as machine translation task.", "labels": [], "entities": [{"text": "normalization task", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.9086193442344666}, {"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7546893954277039}]}, {"text": "The rest of this paper is organized as follows: Section(2) discusses the related work, Section(3) introduces the text normalization system and the baseline candidate generators, Section(4) introduces the proposed graph-based lexicon induction approach, Section(5) discusses the experiments and output analysis, and finally Section(6) concludes and discusses future work.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.6472139060497284}]}], "datasetContent": [{"text": "We collected large amount of social media data to generate the normalization lexicon using the ran-dom walk approach.", "labels": [], "entities": []}, {"text": "The data consists of 73 million Twitter statuses.", "labels": [], "entities": []}, {"text": "All tweets were collected from March/April 2012 using the Twitter Streaming APIs 2 . We augmented this data with 50 million sentences of clean data from English LDC Gigaword corpus . We combined both data, noisy and clean, together to induce the normalization dictionary from them.", "labels": [], "entities": [{"text": "English LDC Gigaword corpus", "start_pos": 153, "end_pos": 180, "type": "DATASET", "confidence": 0.7836898565292358}]}, {"text": "While the Gigaword clean data was used to train the language model to score the normalized lattice.", "labels": [], "entities": [{"text": "Gigaword clean data", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.8951513171195984}]}, {"text": "We constructed a test set of 1000 sentences of social media which had been corrected by a native human annotator, the main guidelines were to normalize noisy words to its corresponding clean words in a consistent way according to the evidences in the context.", "labels": [], "entities": []}, {"text": "We will refer to this test set as SM-Test.", "labels": [], "entities": []}, {"text": "Furthermore, we developed a test set for evaluating the effect of the normalization system when used as a preprocessing step for Machine translation.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.8888576030731201}]}, {"text": "The machine translation test set is composed of 500 sentences of social media English text translated to normalized Spanish text by a bi-lingual translator.", "labels": [], "entities": [{"text": "machine translation test set", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.7353937327861786}]}, {"text": "We experimented different candidate generators and compared it to the unsupervised lexicon approach.", "labels": [], "entities": []}, {"text": "In2), the first baseline is using a dictionary based spellchecker; which gets low precision and very low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9987308382987976}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9975516200065613}]}, {"text": "Similarly the trie approximate string match is doing a similar job with better recall though the precision is worst.", "labels": [], "entities": [{"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9994879961013794}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.999302864074707}]}, {"text": "Both of the baseline approaches are inadequate for social media text since both will try to correct any word that is similar to a word in the dictionary.", "labels": [], "entities": []}, {"text": "The Trie approximate match is doing better job on the recall since the approximate match is based on phonetic and lexical similarities.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9972999691963196}]}, {"text": "On the other hand, the induced normalization lexicon approach is doing much better even with a small amount of data as we can see with system RW1 which uses Lex1 generated from 20M sentences and has 123K lexicon entry.", "labels": [], "entities": []}, {"text": "Increasing the amount of training data does impact the performance positively especially the recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9993769526481628}]}, {"text": "On the other hand, increasing the number of steps has a good impact on the recall as well; but with a considerable impact on the precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9994321465492249}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9991353154182434}]}, {"text": "It is clear that increasing the amount of data and keeping the steps limit at \"'4\"' gives better precision and coverage as well.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9989199638366699}, {"text": "coverage", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9806070327758789}]}, {"text": "This is a preferred setting since the main objective of this approach is to have better precision to serve as a reliable preprocessing step for Machine Translation and other NLP applications.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9965502023696899}, {"text": "Machine Translation", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.8865171372890472}]}, {"text": "The final evaluation of the text normalization system is an extrinsic evaluation where we evaluate the effect of the text normalization task on asocial media text translating from English to Spanish using a large scale translation system trained on general domain data.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7432867884635925}, {"text": "text normalization", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7191820442676544}]}, {"text": "The system is trained on English-Spanish parallel data from WMT 2012 evaluation . The data consists of about 5M parallel sentences on news, europal and UN data.", "labels": [], "entities": [{"text": "WMT 2012 evaluation", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.8789672255516052}, {"text": "europal and UN data", "start_pos": 140, "end_pos": 159, "type": "DATASET", "confidence": 0.8333814740180969}]}, {"text": "The system is a state of the art phrase based system similar to.", "labels": [], "entities": []}, {"text": "We used The BLEU score () to evaluate the translation accuracy with and without the normalization.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9775705337524414}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.8976236581802368}]}], "tableCaptions": [{"text": " Table 1: Generated Lexicons, steps are the Ran- dom Walks maximum steps.", "labels": [], "entities": []}, {"text": " Table 2: Text Normalization with different lexi- cons", "labels": [], "entities": [{"text": "Text Normalization", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7517861127853394}]}, {"text": " Table 3: Text Normalization Results", "labels": [], "entities": [{"text": "Text Normalization", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6816713809967041}]}]}