{"title": [{"text": "Scalable Modified Kneser-Ney Language Model Estimation", "labels": [], "entities": [{"text": "Scalable Modified Kneser-Ney Language Model Estimation", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.5565273612737656}]}], "abstractContent": [{"text": "We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation.", "labels": [], "entities": []}, {"text": "Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk.", "labels": [], "entities": []}, {"text": "Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens.", "labels": [], "entities": []}, {"text": "Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8102038204669952}, {"text": "BLEU point", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9753103256225586}, {"text": "2013 Workshop on Machine Translation task", "start_pos": 116, "end_pos": 157, "type": "TASK", "confidence": 0.559450368086497}]}, {"text": "Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM.", "labels": [], "entities": [{"text": "RAM", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9955808520317078}, {"text": "SRILM", "start_pos": 148, "end_pos": 153, "type": "DATASET", "confidence": 0.9176090955734253}]}, {"text": "The code is open source as part of KenLM.", "labels": [], "entities": [{"text": "KenLM", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.8565198183059692}]}], "introductionContent": [{"text": "Relatively low perplexity has made modified Kneser-Ney smoothing) a popular choice for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7834482789039612}]}, {"text": "However, existing estimation methods require either large amounts of RAM) or machines (.", "labels": [], "entities": [{"text": "estimation", "start_pos": 18, "end_pos": 28, "type": "TASK", "confidence": 0.9665327072143555}]}, {"text": "As a result, practitioners have chosen to useless data) or simpler smoothing methods (.", "labels": [], "entities": []}, {"text": "Backoff-smoothed n-gram language models assign probability to a word w n in context w n\u22121 1 according to the recursive equation p(w n |w n\u22121 1 ) = p(w n |w n\u22121 1 ), if w n 1 was seen b(w n\u22121 1 )p(w n |w n 2 ), otherwise The task is to estimate probability p and backoff b from text for each seen entry w n 1 . This paper", "labels": [], "entities": []}], "datasetContent": [{"text": "After spam filtering), removing markup, selecting English, splitting sentences (, deduplicating, tokenizing (, and truecasing, 126 billion tokens remained.: Counts of unique n-grams (in millions) for the 5 orders in the large LM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Counts of unique n-grams (in millions)  for the 5 orders in the large LM.", "labels": [], "entities": [{"text": "Counts", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9623499512672424}]}, {"text": " Table 2: Uncased BLEU results from the 2013  Workshop on Machine Translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.973913848400116}, {"text": "2013  Workshop on Machine Translation", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.4986116111278534}]}]}