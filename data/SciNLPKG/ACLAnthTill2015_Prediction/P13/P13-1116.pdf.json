{"title": [{"text": "ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling", "labels": [], "entities": [{"text": "Implicit Semantic Role Labelling", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.5807688534259796}]}], "abstractContent": [{"text": "This paper presents a novel deterministic algorithm for implicit Semantic Role Labeling.", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7286003430684408}]}, {"text": "The system exploits a very simple but relevant discursive property, the argument coherence over different instances of a predicate.", "labels": [], "entities": []}, {"text": "The algorithm solves the implicit arguments sequentially, exploiting not only explicit but also the implicit arguments previously solved.", "labels": [], "entities": []}, {"text": "In addition, we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditionally, Semantic Role Labeling (SRL) systems have focused in searching the fillers of those explicit roles appearing within sentence boundaries).", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.7895011802514394}]}, {"text": "These systems limited their searchspace to the elements that share a syntactical relation with the predicate.", "labels": [], "entities": []}, {"text": "However, when the participants of a predicate are implicit this approach obtains incomplete predicative structures with null arguments.", "labels": [], "entities": []}, {"text": "The following example includes the gold-standard annotations fora traditional SRL process: The previous analysis includes annotations for the nominal predicate loss based on the NomBank structure ().", "labels": [], "entities": [{"text": "SRL process", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.889461874961853}]}, {"text": "In this case the annotator identifies, in the first sentence, the arguments arg0, the entity losing something, arg1, the thing lost, and arg3, the source of that loss.", "labels": [], "entities": []}, {"text": "However, in the second sentence there is another instance of the same predicate, loss, but in this case no argument has been associated with it.", "labels": [], "entities": []}, {"text": "Traditional SRL systems facing this type of examples are notable to fill the arguments of a predicate because their fillers are not in the same sentence of the predicate.", "labels": [], "entities": [{"text": "SRL", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9620554447174072}]}, {"text": "Moreover, these systems also let unfilled arguments occurring in the same sentence, like in the following example: (2) Quest Medical Inc said it adopted [arg1 a shareholders' rights] [np plan] in which rights to purchase shares of common stock will be distributed as a dividend to shareholders of record as of Oct 23.", "labels": [], "entities": [{"text": "Quest Medical Inc", "start_pos": 119, "end_pos": 136, "type": "DATASET", "confidence": 0.8804126779238383}]}, {"text": "For the predicate plan in the previous sentence, a traditional SRL process only returns the filler for the argument arg1, the theme of the plan.", "labels": [], "entities": [{"text": "SRL", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9496451616287231}]}, {"text": "However, in both examples, a reader could easily infer the missing arguments from the surrounding context of the predicate, and determine that in (1) both instances of the predicate share the same arguments and in (2) the missing argument corresponds to the subject of the verb that dominates the predicate, Quest Medical Inc.", "labels": [], "entities": [{"text": "Quest Medical Inc", "start_pos": 308, "end_pos": 325, "type": "DATASET", "confidence": 0.8827007611592611}]}, {"text": "Obviously, this additional annotations could contribute positively to its semantic analysis.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.8640568256378174}]}, {"text": "In fact, pointed out that implicit arguments can increase the coverage of argument structures in NomBank by 71%.", "labels": [], "entities": []}, {"text": "However, current automatic systems require large amounts of manually annotated training data for each predicate.", "labels": [], "entities": []}, {"text": "The effort required for this manual annotation explains the absence of generally applicable tools.", "labels": [], "entities": []}, {"text": "This problem has become a main concern for many NLP tasks.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 48, "end_pos": 57, "type": "TASK", "confidence": 0.8815614283084869}]}, {"text": "This fact explains anew trend to develop accurate unsupervised systems that exploit simple but robust linguistic principles.", "labels": [], "entities": []}, {"text": "In this work, we study the coherence of the predicate and argument realization in discourse.", "labels": [], "entities": []}, {"text": "In particular, we have followed a similar approach to the one proposed by who filled the arguments of anaphoric mentions of nominal predicates using previous mentions of the same predicate.", "labels": [], "entities": []}, {"text": "We present an extension of this idea assuming that in a coherent document the different ocurrences of a predicate, including both verbal and nominal forms, tend to be mentions of the same event, and thus, they share the same argument fillers.", "labels": [], "entities": []}, {"text": "Following this approach, we have developed a deterministic algorithm that obtains competitive results with respect to supervised methods.", "labels": [], "entities": []}, {"text": "That is, our system can be applied to any predicate without training data.", "labels": [], "entities": []}, {"text": "The main contributions of this work are the following: \u2022 We empirically prove that there exists a strong discourse relationship between the implicit and explicit argument fillers of the same predicates.", "labels": [], "entities": []}, {"text": "\u2022 We propose a deterministic approach that exploits this discoursive property in order to obtain the fillers of implicit arguments.", "labels": [], "entities": []}, {"text": "\u2022 We adapt to the implicit SRL problem a classic algorithm for pronoun resolution.", "labels": [], "entities": [{"text": "SRL problem", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.8784099817276001}, {"text": "pronoun resolution", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7767837941646576}]}, {"text": "\u2022 We develop a robust algorithm, ImpAr, that obtains very competitive results with respect to existing supervised systems.", "labels": [], "entities": [{"text": "ImpAr", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.8749622702598572}]}, {"text": "We release an open source prototype implementing this algorithm 1 . The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses the related work.", "labels": [], "entities": []}, {"text": "Section 3 presents in detail the data used in our experiments.", "labels": [], "entities": []}, {"text": "Section 4 describes our algorithm for implicit argument resolution.", "labels": [], "entities": [{"text": "implicit argument resolution", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.6962899565696716}]}, {"text": "Section 5 presents some experiments we have carried out to test the algorithm.", "labels": [], "entities": []}, {"text": "Section 6 discusses the results obtained.", "labels": [], "entities": []}, {"text": "Finally, section 7 offers some concluding remarks and presents some future research lines.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we have focused on the dataset developed in.", "labels": [], "entities": []}, {"text": "This dataset (hereinafter BNB which stands for \"Beyond NomBank\") extends existing predicate annotations for NomBank and ProbBank.", "labels": [], "entities": [{"text": "BNB", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9855790138244629}, {"text": "ProbBank", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.9732311367988586}]}, {"text": "BNB presented the first annotation work of implicit arguments based on PropBank and NomBank frames.", "labels": [], "entities": [{"text": "BNB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.970559298992157}, {"text": "PropBank", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9536362886428833}]}, {"text": "This annotation was an extension of the standard training, development and testing sections of Penn TreeBank that have been typically used for SRL evaluation and were already annotated with PropBank and NomBank predicate structures.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 95, "end_pos": 108, "type": "DATASET", "confidence": 0.9928116202354431}, {"text": "SRL evaluation", "start_pos": 143, "end_pos": 157, "type": "TASK", "confidence": 0.9647887945175171}, {"text": "PropBank", "start_pos": 190, "end_pos": 198, "type": "DATASET", "confidence": 0.9310146570205688}]}, {"text": "The authors selected a limited set of predicates.", "labels": [], "entities": []}, {"text": "These predicates are all nominalizations of other verbal predicates, without sense ambiguity, that appear frequently in the corpus and tend to have implicit arguments associated with their instances.", "labels": [], "entities": []}, {"text": "These constraints allowed them to model enough occurrences of each implicit argument in order to cover adequately all the possible cases appearing in a test document.", "labels": [], "entities": []}, {"text": "For each missing argument position they went overall the preceding sentences and annotated all mentions of the filler of that argument.", "labels": [], "entities": []}, {"text": "In tables 3 and 4 we show the list of predicates and the resulting figures of this annotation.", "labels": [], "entities": []}, {"text": "In this work we also use the corpus provided for the CoNLL-2008 task.", "labels": [], "entities": []}, {"text": "These corpora cover the same BNB documents and include annotated predictions for syntactic dependencies and SuperSense labels as semantic tags.", "labels": [], "entities": [{"text": "BNB documents", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9623088538646698}]}, {"text": "Unlike we do not use the constituent analysis from the Penn TreeBank.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.9949778318405151}]}, {"text": "In order to evaluate the performance of the ImpAr algorithm, we have followed the evaluation method presented by.", "labels": [], "entities": []}, {"text": "For every argument position in the goldstandard the scorer expects a single predicted constituent to fill in.", "labels": [], "entities": []}, {"text": "In order to evaluate the correct span of a constituent, a prediction is scored using the Dice coefficient: The function above relates the set of tokens that form a predicted constituent, P redicted, and the set of tokens that are part of an annotated constituent in the gold-standard, T rue.", "labels": [], "entities": []}, {"text": "For each missing argument, the gold-standard includes the whole coreference chain of the filler.", "labels": [], "entities": []}, {"text": "Therefore, the scorer selects from all coreferent mentions the highest Dice value.", "labels": [], "entities": []}, {"text": "If the predicted span does not cover the head of the annotated filler, the scorer returns zero.", "labels": [], "entities": []}, {"text": "Then, P recision is calculated by the sum of all prediction scores divided by the number of attempts carried out by the system.", "labels": [], "entities": [{"text": "P", "start_pos": 6, "end_pos": 7, "type": "METRIC", "confidence": 0.9871519804000854}, {"text": "recision", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.5066696405410767}]}, {"text": "Recall is equal to the sum of the prediction scores divided by the number of actual annotations in the goldstandard.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9960623383522034}]}, {"text": "F-measure is calculated as the harmonic mean of recall and precision.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9757786393165588}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9986768364906311}, {"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9973567724227905}]}, {"text": "Traditionally, there have been two approaches to develop SRL systems, one based on constituent trees and the other one based on syntactic dependencies.", "labels": [], "entities": [{"text": "SRL", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9887415766716003}]}, {"text": "Additionally, the evaluation of both types of systems has been performed differently.", "labels": [], "entities": []}, {"text": "For constituent based SRL systems the scorers evaluate the correct span of the filler, while for dependency based systems the scorer just check if the systems are able to capture the head token of the filler.", "labels": [], "entities": []}, {"text": "As shown above, previous works in implicit argument resolution proposed a metric that involves the correct identification of the whole span of the filler.", "labels": [], "entities": [{"text": "implicit argument resolution", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.7890043059984843}]}, {"text": "ImpAr algorithm works with syntactic dependencies and therefore it only returns the head token of the filler.", "labels": [], "entities": []}, {"text": "In order to compare our results with previous works, we had to apply some simple heuristics to guess the correct span of the filler.", "labels": [], "entities": []}, {"text": "Obviously, this process inserts some noise in the final evaluation.", "labels": [], "entities": []}, {"text": "We have performed a first evaluation over the test set used in (.", "labels": [], "entities": []}, {"text": "This dataset contains 437 predicate instances but just 246 argument positions are implicitly filled.", "labels": [], "entities": []}, {"text": "includes the results obtained by ImpAr, the results of the system presented by and the baseline proposed for the task.", "labels": [], "entities": [{"text": "ImpAr", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.6876384019851685}]}, {"text": "Best results are marked in bold . For all predicates, ImpAr improves over the baseline (19.3 points higher in the overall F 1 ).", "labels": [], "entities": [{"text": "ImpAr", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9949021339416504}, {"text": "F 1 )", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.9754814902941386}]}, {"text": "Our system also outperforms the one presented by   instance, our system obtains much higher results for the predicates bid and fund, while much lower for loss and loan.", "labels": [], "entities": []}, {"text": "In general, ImpAr seems to be more robust since it obtains similar performances for all predicates.", "labels": [], "entities": []}, {"text": "In fact, the standard deviation, \u03c3 , of F 1 measure is 10.98 for ImpAr while this value for the (Gerber and Chai, 2010) system is 20.00.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.9276098807652792}, {"text": "ImpAr", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.5941852331161499}]}, {"text": "Ina more recent work, Gerber and Chai (2012) presented some improvements of their previous results.", "labels": [], "entities": []}, {"text": "In this work, they extended the evaluation of their model using the whole dataset and not just the testing documents.", "labels": [], "entities": []}, {"text": "Applying a crossvalidated approach they tried to solve some problems that they found in the previous evaluation, like the small size of the testing set.", "labels": [], "entities": []}, {"text": "For this work, they also studied a wider set of features, specially, they experimented with some statistics learnt from parts of GigaWord automatically annotated.", "labels": [], "entities": []}, {"text": "shows that the improvement over their previous system was remarkable.", "labels": [], "entities": []}, {"text": "The system also seems to be more stable across predicates.", "labels": [], "entities": []}, {"text": "For comparison purposes, we also included the performance of ImpAr applied over the whole dataset.", "labels": [], "entities": [{"text": "ImpAr", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9377143383026123}]}, {"text": "The results in show that, although ImpAr still achieves the best results in some cases, this time, it cannot beat the overall results obtained by the supervised model.", "labels": [], "entities": [{"text": "ImpAr", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.5394956469535828}]}, {"text": "In fact, both systems obtain a very similar recall, but the system from (Gerber and Chai, 2012) obtains much higher precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.999359667301178}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9985381364822388}]}, {"text": "In both cases, the \u03c3 value of F 1 is reduced, 8.81 for ImpAr and 8.21 for (.", "labels": [], "entities": [{"text": "F 1", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9647260308265686}, {"text": "ImpAr", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.5824591517448425}]}, {"text": "However, ImpAr obtains very similar performance independently of the testing dataset what proves the robustness of the algorithm.", "labels": [], "entities": []}, {"text": "This suggests that our algorithm can obtain strong results also for other corpus and predicates.", "labels": [], "entities": []}, {"text": "Instead, the supervised approach would need a large amount of manual annotations for every predicate to be processed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Weights assigned to each salience factor.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation with the test. The results from (Gerber and Chai, 2010) are included.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation with the full dataset. The results from (Gerber and Chai, 2012) are included.", "labels": [], "entities": []}, {"text": " Table 5: Exp1, Exp2 and Exp3 correspond to ablations of the  components. Exp3 and Exp4 are experiments over the cases  that are not solved by explicit antecedents. Exp6 evaluates  the system capturing just the head tokens of the constituents.", "labels": [], "entities": []}]}