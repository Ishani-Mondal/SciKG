{"title": [{"text": "Learning a Phrase-based Translation Model from Mon- olingual Data with Application to Domain Adaptation", "labels": [], "entities": [{"text": "Phrase-based Translation Model", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.8292059302330017}]}], "abstractContent": [{"text": "Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.7954324930906296}]}, {"text": "However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power.", "labels": [], "entities": [{"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9827203750610352}]}, {"text": "Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora.", "labels": [], "entities": [{"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9814953207969666}, {"text": "word-based translation", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.6635733395814896}]}, {"text": "It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result.", "labels": [], "entities": [{"text": "bitext", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.93991619348526}, {"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9932222366333008}]}, {"text": "In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically induced translation lexicon or a manually-edited translation dictionary.", "labels": [], "entities": []}, {"text": "We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality.", "labels": [], "entities": [{"text": "domain adaptation task", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.8162577350934347}]}], "introductionContent": [{"text": "During the last decade, statistical machine translation has made great progress.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.824505885442098}]}, {"text": "Novel translation models, such as phrase-based models, hierarchical phrase-based models and linguistically syntax-based models (;) have been proposed and achieved higher and higher translation performance.", "labels": [], "entities": []}, {"text": "However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters.", "labels": [], "entities": []}, {"text": "It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair.", "labels": [], "entities": []}, {"text": "Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 163, "end_pos": 190, "type": "TASK", "confidence": 0.6579172909259796}, {"text": "statistical machine translation", "start_pos": 241, "end_pos": 272, "type": "TASK", "confidence": 0.6186855733394623}]}, {"text": "In the bilingual lexicon induction (, with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-parallel data only requiring that the monolingual data in each language are from a fairly comparable domain.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.6986400485038757}, {"text": "canonical correlation analysis (CCA)", "start_pos": 146, "end_pos": 182, "type": "TASK", "confidence": 0.681511382261912}]}, {"text": "The unsupervised statistical machine translation method () viewed the translation task as a decipherment problem and designed a generative model with the objective function to maximize the likelihood of the source language monolingual data.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.6274912257989248}]}, {"text": "To tackle the large-scale vocabulary, they mainly considered the word-based model (e.g. IBM Model 3) and applied the Bayesian method with Gibbs sampling or slice sampling.", "labels": [], "entities": []}, {"text": "Finally, they used the learned translation model directly to translate unseen data () or incorporated the learned bilingual lexicon as anew in-domain translation resource into the phrase-based model which is trained with out-of-domain data to improve the domain adaptation performance in machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 288, "end_pos": 307, "type": "TASK", "confidence": 0.7543171942234039}]}, {"text": "We can easily see that these unsupervised methods can only induce the word-based translation rules (bilingual lexicon) at present.", "labels": [], "entities": []}, {"text": "It is a big challenge that whether we can induce phrase 1, word reordering example: \u672c \u53d1\u660e \u7684 \u76ee\u7684 \u5728\u4e8e ||| the purpose of the invention is to ||| 0-0 0-3 1-4 2-2 3-1 4-5 4-6 2, idiom example: \u8fa8\u8bc6 \u771f\u4f2a \u7684 ||| distinguish the true from the false ||| 0-0 1-2 1-5 2-1 2-4 3, unknown word translation: \u53d1\u5149 \u4e8c\u6781\u7ba1 \u82af\u7247 \u7684 ||| of the light-emitting diode chip ||| 0-2 1-2 2-4 3-0 3-1: Examples of new translation knowledge learned with the proposed phrase pair induction method.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.7431465089321136}, {"text": "unknown word translation", "start_pos": 261, "end_pos": 285, "type": "TASK", "confidence": 0.7036882638931274}]}, {"text": "For the three fields separated by \"|||\", the first two are respectively Chinese and English phrase, and the last one is the word alignment between these two phrases.", "labels": [], "entities": []}, {"text": "level translation rules and learn a phrase-based model from the monolingual corpora.", "labels": [], "entities": []}, {"text": "In this paper, we focus on exploring this direction and propose a simple but effective method to induce the phrase-level translation rules from monolingual data.", "labels": [], "entities": [{"text": "phrase-level translation", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.7189340889453888}]}, {"text": "The main idea of our method is to divide the phrase-level translation rule induction into two steps: bilingual lexicon induction and phrase pair induction.", "labels": [], "entities": [{"text": "phrase-level translation rule induction", "start_pos": 45, "end_pos": 84, "type": "TASK", "confidence": 0.8142489939928055}, {"text": "bilingual lexicon induction", "start_pos": 101, "end_pos": 128, "type": "TASK", "confidence": 0.6157864034175873}, {"text": "phrase pair induction", "start_pos": 133, "end_pos": 154, "type": "TASK", "confidence": 0.7606364885965983}]}, {"text": "Since many researchers have studied the bilingual lexicon induction, in this paper, we mainly concentrate ourselves on phrase pair induction given a probabilistic bilingual lexicon and two in-domain large monolingual data (source and target language).", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.6873081922531128}, {"text": "phrase pair induction", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.7958784699440002}]}, {"text": "In addition, we will further introduce how to refine the induced phrase pairs and estimate the parameters of the induced phrase pairs, such as four standard translation features and phrase reordering feature used in the conventional phrase-based models ().", "labels": [], "entities": []}, {"text": "The induced phrase-based model will be used to help domain adaptation for machine translation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.7472639679908752}, {"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7689357399940491}]}, {"text": "In the rest of this paper, we first explain with examples to show what new translation knowledge can be learned with our proposed phrase pair induction method (Section 2), and then we introduce the approach for probabilistic bilingual lexicon acquisition in Section 3.", "labels": [], "entities": [{"text": "phrase pair induction", "start_pos": 130, "end_pos": 151, "type": "TASK", "confidence": 0.7753345966339111}, {"text": "probabilistic bilingual lexicon acquisition", "start_pos": 211, "end_pos": 254, "type": "TASK", "confidence": 0.6209182664752007}]}, {"text": "In Section 4 and 5, we respectively present our method for phrase pair induction and introduce an approach for phrase pair refinement and parameter estimation.", "labels": [], "entities": [{"text": "phrase pair induction", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.8206375042597452}, {"text": "phrase pair refinement", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.7535345355669657}]}, {"text": "Section 6 will show the detailed experiments for the task of domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.8127912878990173}]}, {"text": "We will introduce some related work in Section 7 and conclude this paper in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our purpose is to induce phrase pairs to improve translation quality for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.7325599640607834}]}, {"text": "We have introduced the out-of-domain data and the electronic in-domain lexicon in Section 3.", "labels": [], "entities": []}, {"text": "Here we introduce other information about the in-domain data.", "labels": [], "entities": []}, {"text": "Besides the in-domain lexicon, we have collected respectively 1 million monolingual sentences in electronic area from the web.", "labels": [], "entities": []}, {"text": "They are neither parallel nor comparable because we cannot even extract a small number of parallel sentence pairs from this monolingual data using the method of ().", "labels": [], "entities": []}, {"text": "We further employ experts to translate 2000 Chinese electronic sentences into English.", "labels": [], "entities": []}, {"text": "The first half is used as the tuning set (elec1000-tune) and the second half is employed as the testing set (elec1000-test).", "labels": [], "entities": []}, {"text": "We construct two kinds of phrase-based models using Moses (: one uses out-of-domain data and the other uses in-domain data.", "labels": [], "entities": []}, {"text": "For the out-of-domain data, we build the phrase table and reordering table using the 2.08 million Chinese-to-English sentence pairs, and we use the SRILM toolkit) to train the 5-gram English language model with the target part of the parallel sentences and the Xinhua portion of the English Gigaword.", "labels": [], "entities": []}, {"text": "For the in-domain electronic data, we first consider the lexicon as a phrase table in which we assign a constant 1.0 for each of the four probabilities, and then we combine this initial phrase table and the induced phrase pairs to form the new phrase table.", "labels": [], "entities": []}, {"text": "The in-domain reordering table is created for the induced phrase pairs.", "labels": [], "entities": []}, {"text": "An in-domain 5-gram English language model is trained with the target 1 million monolingual data.", "labels": [], "entities": []}, {"text": "We use BLEU () score with shortest length penalty as the evaluation metric and apply the pairwise re-sampling approach) to perform the significance test.", "labels": [], "entities": [{"text": "BLEU () score", "start_pos": 7, "end_pos": 20, "type": "METRIC", "confidence": 0.9216503898302714}]}, {"text": "In this section, we first conduct experiments to figure out how the translation performance degrades when the domain changes.", "labels": [], "entities": []}, {"text": "To better illustrate the comparison, we first use News data to evaluate the NIST evaluation tests and then use the same News data to evaluate the electronic test sets.", "labels": [], "entities": [{"text": "News data", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.903734415769577}, {"text": "NIST evaluation tests", "start_pos": 76, "end_pos": 97, "type": "DATASET", "confidence": 0.9038939277331034}, {"text": "News data", "start_pos": 120, "end_pos": 129, "type": "DATASET", "confidence": 0.9057809710502625}]}, {"text": "For the NIST evaluation, we employ Chinese-to-English NIST MT03 as the tuning set and NIST MT05 as the test set.", "labels": [], "entities": [{"text": "NIST", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.8254094123840332}, {"text": "NIST MT03", "start_pos": 54, "end_pos": 63, "type": "DATASET", "confidence": 0.7894214689731598}, {"text": "NIST MT05", "start_pos": 86, "end_pos": 95, "type": "DATASET", "confidence": 0.839228630065918}]}, {"text": "It is obvious that, it is relatively high when using the News training data to evaluate the same News test set.", "labels": [], "entities": [{"text": "News training data", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.9144764343897501}, {"text": "News test set", "start_pos": 97, "end_pos": 110, "type": "DATASET", "confidence": 0.9522697726885477}]}, {"text": "However, when the test domain is changed, the translation performance decreases to a large extent.", "labels": [], "entities": []}, {"text": "Given the in-domain bilingual lexicon and two monolingual data, previous works also proposed some good methods to explore the potential of the given data to improve the translation quality.", "labels": [], "entities": []}, {"text": "Here, we implement their approaches and use them as our strong baseline.", "labels": [], "entities": []}, {"text": "regards the in-domain lexicon with corpus translation probability as another phrase table and further use the in-domain language model besides the out-of-domain language model.", "labels": [], "entities": [{"text": "corpus translation", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7416783273220062}]}, {"text": "We can see from the table that the domain lexicon is much helpful and significantly outperforms the baseline with more than 4.0 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9985357522964478}]}, {"text": "When it is enhanced with the in-domain language model, it can further improve the translation performance by more than 2.5 BLEU points.", "labels": [], "entities": [{"text": "translation", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.9476730823516846}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9990566372871399}]}, {"text": "This method has made good use of in-domain lexicon and the target-side indomain monolingual data, but it does not take full advantage of the in-domain source-side monolingual data.", "labels": [], "entities": []}, {"text": "In order to use source-side monolingual data,,, and employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence.", "labels": [], "entities": []}, {"text": "With the source-side sentences and their translations, the new phrase table and reordering table are built.", "labels": [], "entities": []}, {"text": "Then, these resources are added into the best configuration.", "labels": [], "entities": []}, {"text": "The experimental results are presented in the last low of.", "labels": [], "entities": []}, {"text": "From the results, we see that transductive learning can further improve the translation performance significantly by 0.6 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9993146657943726}]}, {"text": "In tranductive learning, in-domain lexicon and both-side monolingual data have been explored.", "labels": [], "entities": []}, {"text": "However, this method does not take full advantage of both-side monolingual data because it uses source and target monolingual data individually.", "labels": [], "entities": []}, {"text": "In our method, we explore fully the source and target monolingual data to induce translation equivalence on the phrase level.", "labels": [], "entities": []}, {"text": "In order to make the phrase pair induction more efficient, we first sort all the sentences in the both-side monolingual data according to the word hit rate in the bilingual lexicon.", "labels": [], "entities": [{"text": "phrase pair induction", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7189396719137827}]}, {"text": "Then, we conduct six sets of experiments respectively on the first 100k, 200k, 300k, 500k and whole 1m sentences.", "labels": [], "entities": []}, {"text": "All the experiments are run based on the configuration with BLEU 13.41 in, and we call this configuration BestConfig.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9915871620178223}, {"text": "BestConfig", "start_pos": 106, "end_pos": 116, "type": "DATASET", "confidence": 0.9345458745956421}]}, {"text": "Note that the unknown words are only allowed if the source-side of a phrase pair has more than 3 words.", "labels": [], "entities": []}, {"text": "We can see from the table that our method obtains the best translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.9520581364631653}]}, {"text": "When using the first 100k sentences for phrase pair induction, it obtains a significant improvement over the BestConfig by 0.65 BLEU points and can outperform the transductive learning method.", "labels": [], "entities": [{"text": "phrase pair induction", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.7940898736317953}, {"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.998859167098999}]}, {"text": "When we use more monolingual data, the performance becomes even better.", "labels": [], "entities": []}, {"text": "The method of phrase pair induction using 300k sentences performs quite well.", "labels": [], "entities": [{"text": "phrase pair induction", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.8434344132741293}]}, {"text": "It outperforms the BestConfig significantly with an improvement of 1.42 BLEU points and it also performs much better than transductive learning method with gains of 0.82 BLEU points.", "labels": [], "entities": [{"text": "BestConfig", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.852806568145752}, {"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9984076619148254}, {"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.997002899646759}]}, {"text": "With the monolingual data larger and larger, the gains become smaller and smaller because the word hit rate gets lower and lower.", "labels": [], "entities": []}, {"text": "These experimental results empirically show the effectiveness of our proposed phrase pair induction method.", "labels": [], "entities": [{"text": "phrase pair induction", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.7921058734258016}]}, {"text": "A question remains that how many new phrase pairs are induced with different size of monolingual data.", "labels": [], "entities": []}, {"text": "Here, we give respectively the statistics before and after filtering with the 1000 test sentences.", "labels": [], "entities": []}, {"text": "We can see from the table that lots of new phrase pairs can be induced since the source and target monolingual data is in the same domain.", "labels": [], "entities": []}, {"text": "However, since the source and target monolingual data is far from parallel, most of the phrase pairs are not long.", "labels": [], "entities": []}, {"text": "For example, in the 108,948 distinct phrase pairs, we find that the phrase pair distribution according to source-side length is (3:50.6%, 4:35.6%, 5:3.3%, 6:9.8%, 7:0.7%).", "labels": [], "entities": []}, {"text": "It is easy to see that the phrase pairs whose source-side length longer than 4 account for only a very small part.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Experimental results using News training data to test NIST evaluation data and electronic data (numbers  denote BLEU score points in percent).", "labels": [], "entities": [{"text": "News training data", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.8522229592005411}, {"text": "NIST evaluation data", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.8833988706270853}, {"text": "BLEU score points", "start_pos": 122, "end_pos": 139, "type": "METRIC", "confidence": 0.9737684528032938}]}, {"text": " Table 4: Experimental results using News training data, in-domain lexicon, language model and transductive  learning. Bold figures mean that the results are statistically significant better than the baseline with p<0.01, and  \"++\" denotes the result is statistically significant better than baseline+in-domain lexicon. \"*\" means that the  result is statistically significant better than 13.41 with p<0.05.", "labels": [], "entities": [{"text": "News training data", "start_pos": 37, "end_pos": 55, "type": "DATASET", "confidence": 0.7680118481318156}]}, {"text": " Table 5: Experimental results of our phrase pair induction method. Bold figures denotes the corresponding  method significantly outperform the BestConfig with p<0.05. Bold and Italic figures means the results are sig- nificantly better than that of BestConfig with p<0.01. \"++\" denotes that the corresponding approach performs  significantly better than Transductive Learning with p<0.01.", "labels": [], "entities": [{"text": "phrase pair induction", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.6857254107793173}, {"text": "BestConfig", "start_pos": 144, "end_pos": 154, "type": "DATASET", "confidence": 0.9188988208770752}]}, {"text": " Table 6: the number of phrase pairs induced with different size of monolingual data.", "labels": [], "entities": []}]}