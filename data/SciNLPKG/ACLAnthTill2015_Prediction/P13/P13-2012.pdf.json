{"title": [{"text": "PARMA: A Predicate Argument Aligner", "labels": [], "entities": [{"text": "Predicate Argument Aligner", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.7486168543497721}]}], "abstractContent": [{"text": "We introduce PARMA, a system for cross-document, semantic predicate and argument alignment.", "labels": [], "entities": [{"text": "PARMA", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.6083624958992004}, {"text": "semantic predicate and argument alignment", "start_pos": 49, "end_pos": 90, "type": "TASK", "confidence": 0.5568196773529053}]}, {"text": "Our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering, integrating them into a simple discrimina-tive model.", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 94, "end_pos": 124, "type": "TASK", "confidence": 0.8005992770195007}, {"text": "question answering", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.8405384421348572}]}, {"text": "PARMA achieves state of the art results on an existing and anew dataset.", "labels": [], "entities": [{"text": "PARMA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7228589057922363}]}, {"text": "We suggest that previous efforts have focussed on data that is biased and too easy, and we provide a more difficult dataset based on translation data with a low base-line which we beat by 17% F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 192, "end_pos": 194, "type": "METRIC", "confidence": 0.995931088924408}]}], "introductionContent": [{"text": "A key step of the information extraction pipeline is entity disambiguation, in which discovered entities across many sentences and documents must be organized to represent real world entities.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7668204307556152}, {"text": "entity disambiguation", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.7136154025793076}]}, {"text": "The NLP community has along history of entity disambiguation both within and across documents.", "labels": [], "entities": []}, {"text": "While most information extraction work focuses on entities and noun phrases, there have been a few attempts at predicate, or event, disambiguation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.7780950963497162}, {"text": "predicate, or event, disambiguation", "start_pos": 111, "end_pos": 146, "type": "TASK", "confidence": 0.7099905014038086}]}, {"text": "Commonly a situational predicate is taken to correspond to either an event or a state, lexically realized in verbs such as \"elect\" or nominalizations such as \"election\".", "labels": [], "entities": []}, {"text": "Similar to entity coreference resolution, almost all of this work assumes unanchored mentions: predicate argument tuples are grouped together based on coreferent events.", "labels": [], "entities": [{"text": "entity coreference resolution", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.7450065016746521}]}, {"text": "The first work on event coreference dates back to.", "labels": [], "entities": [{"text": "event coreference", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.8284524381160736}]}, {"text": "More recently, this task has been considered by and.", "labels": [], "entities": []}, {"text": "As with unanchored entity disambiguation, these methods rely on clustering methods and evaluation metrics.", "labels": [], "entities": []}, {"text": "Another view of predicate disambiguation seeks to link or align predicate argument tuples to an existing anchored resource containing references to events or actions, similar to anchored entity disambiguation (entity linking) ().", "labels": [], "entities": [{"text": "predicate disambiguation", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.9348786771297455}]}, {"text": "The most relevant, and perhaps only, work in this area is that of who linked predicates across document pairs, measuring the F1 of aligned pairs.", "labels": [], "entities": [{"text": "F1", "start_pos": 125, "end_pos": 127, "type": "METRIC", "confidence": 0.9986322522163391}]}, {"text": "Here we present PARMA, anew system for predicate argument alignment.", "labels": [], "entities": [{"text": "PARMA", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.7824606895446777}, {"text": "predicate argument alignment", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.8288750449816386}]}, {"text": "As opposed to Roth and Frank, PARMA is designed as a a trainable platform for the incorporation of the sort of lexical semantic resources used in the related areas of Recognizing Textual Entailment (RTE) and Question Answering (QA).", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 167, "end_pos": 203, "type": "TASK", "confidence": 0.7474855879942576}, {"text": "Question Answering (QA)", "start_pos": 208, "end_pos": 231, "type": "TASK", "confidence": 0.8767481803894043}]}, {"text": "We demonstrate the effectiveness of this approach by achieving state of the art performance on the data of Roth and Frank despite having little relevant training data.", "labels": [], "entities": []}, {"text": "We then show that while the \"lemma match\" heuristic provides a strong baseline on this data, this appears to bean artifact of their data creation process (which was heavily reliant on word overlap).", "labels": [], "entities": []}, {"text": "In response, we evaluate on anew and more challenging dataset for predicate argument alignment derived from multiple translation data.", "labels": [], "entities": [{"text": "predicate argument alignment", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.7809775869051615}]}, {"text": "We release PARMA as anew framework for the incorporation and evaluation of new resources for predicate argument alignment.", "labels": [], "entities": [{"text": "predicate argument alignment", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.8607209523518881}]}], "datasetContent": [{"text": "We consider three datasets for evaluating PARMA.", "labels": [], "entities": [{"text": "PARMA", "start_pos": 42, "end_pos": 47, "type": "TASK", "confidence": 0.477801650762558}]}, {"text": "For richer annotations that include lemmatizations, part of speech, NER, and in-doc coreference, we pre-processed each of the datasets using tools 7 similar to those used to create the Annotated Gigaword corpus (.", "labels": [], "entities": [{"text": "Annotated Gigaword corpus", "start_pos": 185, "end_pos": 210, "type": "DATASET", "confidence": 0.8715582887331644}]}, {"text": "Extended Event Coreference Bank Based on the dataset of, introduced the Extended Event Coreference Bank (EECB) to evaluate cross-document event coreference.", "labels": [], "entities": [{"text": "Extended Event Coreference Bank (EECB)", "start_pos": 72, "end_pos": 110, "type": "METRIC", "confidence": 0.44783938356808256}]}, {"text": "EECB provides document clusters, within which entities and events may corefer.", "labels": [], "entities": [{"text": "EECB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9596173167228699}]}, {"text": "Our task is different from Lee et al. but we can modify the corpus setup to support our task.", "labels": [], "entities": []}, {"text": "To produce source and target document pairs, we select the first document within every cluster as the source and each of the remaining documents as target documents (i.e. N \u2212 1 pairs fora cluster of size N ).", "labels": [], "entities": []}, {"text": "This yielded 437 document pairs.", "labels": [], "entities": []}, {"text": "Roth and Frank The only existing dataset for our task is from Roth and Frank (2012) (RF), who annotated documents from the English Gigaword Fifth Edition corpus).", "labels": [], "entities": [{"text": "English Gigaword Fifth Edition corpus", "start_pos": 123, "end_pos": 160, "type": "DATASET", "confidence": 0.895680558681488}]}, {"text": "The data was generated by clustering similar news stories from Gigaword using TF-IDF cosine similarity of their headlines.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9221728444099426}]}, {"text": "This corpus is small, containing only 10 document pairs in the development set and 60 in the test set.", "labels": [], "entities": []}, {"text": "To increase the training size, we train PARMA with 150 randomly selected document pairs from both EECB and MTC, and the entire dev set from Roth and Frank using multidomain feature splitting.", "labels": [], "entities": [{"text": "EECB", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.9610063433647156}, {"text": "MTC", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.773071825504303}, {"text": "multidomain feature splitting", "start_pos": 161, "end_pos": 190, "type": "TASK", "confidence": 0.6695924202601115}]}, {"text": "We tuned the threshold \u03c4 on the Roth and Frank dev set, but choose the regularizer \u03bb based on a grid search on a 5-fold version of the EECB dataset.", "labels": [], "entities": [{"text": "EECB dataset", "start_pos": 135, "end_pos": 147, "type": "DATASET", "confidence": 0.9893918633460999}]}, {"text": "Multiple Translation Corpora We constructed anew predicate argument alignment dataset based on the LDC Multiple Translation Corpora (MTC), which consist of multiple English translations for foreign news articles.", "labels": [], "entities": [{"text": "predicate argument alignment", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6300809979438782}, {"text": "LDC Multiple Translation Corpora (MTC)", "start_pos": 99, "end_pos": 137, "type": "TASK", "confidence": 0.5765548859323774}]}, {"text": "Since these multiple translations are semantically equivalent, they provide a good resource for aligned predicate argument pairs.", "labels": [], "entities": []}, {"text": "However, finding good pairs is a challenge: we want pairs with significant overlap so that they have predicates and arguments that align, but not documents that are trivial rewrites of each other.", "labels": [], "entities": []}, {"text": "Roth and Frank selected document pairs based on clustering, meaning that the pairs had high lexical overlap, often resulting in minimal rewrites of each other.", "labels": [], "entities": []}, {"text": "As a result, despite ignoring all context, their baseline method (lemmaalignment) worked quite well.", "labels": [], "entities": []}, {"text": "To create a more challenging dataset, we selected document pairs from the multiple translations that minimize the lexical overlap (in English).", "labels": [], "entities": []}, {"text": "Because these are translations, we know that there are equivalent predicates and arguments in each pair, and that any lexical variation preserves meaning.", "labels": [], "entities": []}, {"text": "Therefore, we can select pairs with minimal lexical overlap in order to create a system that truly stresses lexically-based alignment systems.", "labels": [], "entities": []}, {"text": "Each document pair has a correspondence between sentences, and we run GIZA++ on these sentences to produce token-level alignments.", "labels": [], "entities": []}, {"text": "We take all aligned nouns as arguments and all aligned verbs (excluding be-verbs, light verbs, and reporting verbs) as predicates.", "labels": [], "entities": []}, {"text": "We then add negative examples by randomly substituting half of the sentences in one document with sentences from an- On the RF data set, performance is correlated with lexical similarity.", "labels": [], "entities": [{"text": "RF data set", "start_pos": 124, "end_pos": 135, "type": "DATASET", "confidence": 0.9622963666915894}]}, {"text": "On our more lexically diverse set, this is not the case.", "labels": [], "entities": []}, {"text": "This could be due to the fact that some of the documents in the RF sets are minor re-writes of the same newswire story, making them easy to align.", "labels": [], "entities": [{"text": "RF sets", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.8405222594738007}]}, {"text": "other corpus, guaranteed to be unrelated.", "labels": [], "entities": []}, {"text": "The amount of substitutions we perform can vary the \"relatedness\" of the two documents in terms of the predicates and arguments that they talk about.", "labels": [], "entities": []}, {"text": "This reflects our expectation of real world data, where we do not expect perfect overlap in predicates and arguments between a source and target document, as you would in translation data.", "labels": [], "entities": []}, {"text": "Lastly, we prune any document pairs that have more than 80 predicates or arguments or have a Jaccard index on bags of lemmas greater than 0.5, to give us a dataset of 328 document pairs.", "labels": [], "entities": []}, {"text": "Metric We use precision, recall, and F1.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9997883439064026}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9997256398200989}, {"text": "F1", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.999810516834259}]}, {"text": "For the RF dataset, we follow Roth and Frank (2012) and and evaluate on aversion of F1 that considers SURE and POSSIBLE links, which are available in the RF data.", "labels": [], "entities": [{"text": "RF dataset", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.7587838172912598}, {"text": "F1", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9981977343559265}, {"text": "SURE", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9923585057258606}, {"text": "POSSIBLE", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.8792340159416199}, {"text": "RF data", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.7783181965351105}]}, {"text": "Given an alignment to be scored A and a reference alignment B which contains SURE and POSSIBLE links, B sand B p respectively, precision and recall are: Lemma baseline Following Roth and Frank we include a lemma baseline, in which two predicates or arguments align if they have the same lemma.", "labels": [], "entities": [{"text": "SURE", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9269396066665649}, {"text": "precision", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9996067881584167}, {"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9994868040084839}]}], "tableCaptions": [{"text": " Table 1: PARMA outperforms the baseline lemma  matching system on the three test sets, drawn from  the Extended Event Coreference Bank, Roth and  Frank's data, and our set created from the Multiple  Translation Corpora. PARMA achieves a higher F1  and recall score than Roth and Frank's reported  result.", "labels": [], "entities": [{"text": "Extended Event Coreference Bank", "start_pos": 104, "end_pos": 135, "type": "DATASET", "confidence": 0.5470638647675514}, {"text": "Multiple  Translation", "start_pos": 190, "end_pos": 211, "type": "TASK", "confidence": 0.6847793757915497}, {"text": "F1", "start_pos": 245, "end_pos": 247, "type": "METRIC", "confidence": 0.999671459197998}, {"text": "recall score", "start_pos": 253, "end_pos": 265, "type": "METRIC", "confidence": 0.9761819839477539}]}]}