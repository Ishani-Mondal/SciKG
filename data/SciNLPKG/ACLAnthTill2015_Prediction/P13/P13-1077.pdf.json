{"title": [{"text": "An Infinite Hierarchical Bayesian Model of Phrasal Translation", "labels": [], "entities": []}], "abstractContent": [{"text": "Modern phrase-based machine translation systems make extensive use of word-based translation models for inducing alignments from parallel corpora.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 7, "end_pos": 39, "type": "TASK", "confidence": 0.6451495389143626}]}, {"text": "This is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation.", "labels": [], "entities": []}, {"text": "This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior.", "labels": [], "entities": [{"text": "phrase-based translation units", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.7474217514197031}]}, {"text": "Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations.", "labels": [], "entities": []}, {"text": "Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The phrase-based approach ( to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.8750804543495179}, {"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9799986481666565}]}, {"text": "Leading translation systems) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus.", "labels": [], "entities": [{"text": "multi-word translation", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7893060445785522}]}, {"text": "Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation.", "labels": [], "entities": []}, {"text": "Word-based translation models () remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from which phrasal translation units are extracted using a heuristic.", "labels": [], "entities": [{"text": "Word-based translation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6191764771938324}]}, {"text": "Although this approach demonstrably works, it suffers from a number of shortcomings.", "labels": [], "entities": []}, {"text": "Firstly, many phrase-based phenomena which do not decompose into word translations (e.g., idioms) will be missed, as the underlying word-based alignment model is unlikely to propose the correct alignments.", "labels": [], "entities": []}, {"text": "Secondly, the relationship between different phrase-pairs is not considered, such as between single word translations and larger multi-word phrase-pairs or where one large phrase-pair subsumes another.", "labels": [], "entities": []}, {"text": "This paper develops a phrase-based translation model which aims to address the above shortcomings of the phrase-based translation pipeline.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.7351701855659485}, {"text": "phrase-based translation pipeline", "start_pos": 105, "end_pos": 138, "type": "TASK", "confidence": 0.7790425618489584}]}, {"text": "Specifically, we formulate translation using inverse transduction grammar (ITG), and seek to learn an ITG from parallel corpora.", "labels": [], "entities": [{"text": "formulate translation", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7365987002849579}]}, {"text": "The novelty of our approach is that we develop a Bayesian prior over the grammar, such that a nonterminal becomes a 'cache' learning each production and its complete yield, which in turn is recursively composed of its child constituents.", "labels": [], "entities": []}, {"text": "This is closely related to adaptor grammars), which also generate full tree rewrites in a monolingual setting.", "labels": [], "entities": []}, {"text": "Our model learns translations of entire sentences while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations.", "labels": [], "entities": []}, {"text": "The model is richly parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrasepairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline.", "labels": [], "entities": [{"text": "MT pipeline", "start_pos": 280, "end_pos": 291, "type": "TASK", "confidence": 0.8914895057678223}]}, {"text": "We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling).", "labels": [], "entities": [{"text": "language modelling", "start_pos": 255, "end_pos": 273, "type": "TASK", "confidence": 0.7108705937862396}]}, {"text": "We are not the first to consider this idea; Neubig et al.", "labels": [], "entities": []}, {"text": "(2011) developed a similar approach for learning an ITG using a form of Pitman-Yor adaptor grammar.", "labels": [], "entities": []}, {"text": "However Neubig et al.'s work was flawed in a number of respects, most notably in terms of their heuristic beam sampling algorithm which does not meet either of the Markov Chain Monte Carlo criteria of ergodicity or detailed balance.", "labels": [], "entities": []}, {"text": "Consequently their approach does not constitute a valid Bayesian model.", "labels": [], "entities": []}, {"text": "In contrast, this paper provides a more rigorous and theoretically sound method.", "labels": [], "entities": []}, {"text": "Moreover our approach results in consistent translation improvements across a number of translation tasks compared to Neubig et al.'s method, and a competitive phrase-based baseline.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets We train our model across three language pairs: Urdu\u2192English (UR-EN), Farsi\u2192English (FA-EN), and Arabic\u2192English (AR-EN).", "labels": [], "entities": []}, {"text": "The corpora statistics of these translation tasks are summarised in.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.913809061050415}]}, {"text": "The UR-EN corpus comes from NIST 2009 translation evaluation.", "labels": [], "entities": [{"text": "UR-EN corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7942402362823486}, {"text": "NIST 2009 translation evaluation", "start_pos": 28, "end_pos": 60, "type": "DATASET", "confidence": 0.7636352628469467}]}, {"text": "The AR-EN training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02).", "labels": [], "entities": [{"text": "AR-EN training data", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.7578727801640829}, {"text": "eTIRR corpus", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.8071131110191345}, {"text": "Arabic news corpus (LDC2004T17", "start_pos": 71, "end_pos": 101, "type": "DATASET", "confidence": 0.7543864846229553}, {"text": "Ummah corpus", "start_pos": 108, "end_pos": 120, "type": "DATASET", "confidence": 0.8871775567531586}, {"text": "ISI automatically extracted web parallel corpus", "start_pos": 186, "end_pos": 233, "type": "DATASET", "confidence": 0.697163055340449}]}, {"text": "For FA-EN, we use TEP 8 Tehran English-Persian Parallel corpus (, which consists of conversational/informal text extracted The full model differs from the approximating grammar in that it accounts for inter-dependencies between subtrees by recursively tracking the changes in the customer and table counts while scoring the tree.", "labels": [], "entities": [{"text": "TEP 8 Tehran English-Persian Parallel corpus", "start_pos": 18, "end_pos": 62, "type": "DATASET", "confidence": 0.8328120211760203}]}, {"text": "Around 98% of samples were accepted in our experiments.", "labels": [], "entities": []}, {"text": "Sampler configuration Samplers are initialised with trees created from GIZA++ alignments constructed using a SCFG factorisation method ().", "labels": [], "entities": []}, {"text": "This algorithm represents the translation of a sentence as a large SCFG rule, which it then factorises into lower rank SCFG rules, a process akin to rule binarisation commonly used in SCFG decoding.", "labels": [], "entities": []}, {"text": "Rules that cannot be reduced to a rank-2 SCFG are simplified by dropping alignment edges until they can be factorised, the net result being an ITG derivation largely respecting the alignments.", "labels": [], "entities": []}, {"text": "The blocked sampler was run 1000 iterations for UR-EN, 100 iterations for FA-EN and AR-EN.", "labels": [], "entities": [{"text": "FA-EN", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.8270730972290039}]}, {"text": "After each full sampling iteration, we resample all the hyper-parameters using slice-sampling, with the following priors: a \u223c Beta(1, 1), b \u223c Gamma(10, 0.1).", "labels": [], "entities": []}, {"text": "shows the posterior probability improves with each full sampling iterations.", "labels": [], "entities": [{"text": "posterior probability", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.948358416557312}]}, {"text": "The alignment probability was set to \u03b7 = 0.99.", "labels": [], "entities": [{"text": "alignment probability", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.7627895474433899}, {"text": "\u03b7", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.990050733089447}]}, {"text": "The sampling was repeated for 5 independent runs, and we present results where we combine the outputs of these runs.", "labels": [], "entities": []}, {"text": "This is a form of Monte Carlo integration which allows us to represent the uncertainty in the posterior, while also representing multiple modes, if present.", "labels": [], "entities": [{"text": "Monte Carlo integration", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.6398371656735738}]}, {"text": "The time complexity of our inference algorithm is O(n 6 ), which can be prohibitive for large scale machine translation tasks.", "labels": [], "entities": [{"text": "O", "start_pos": 50, "end_pos": 51, "type": "METRIC", "confidence": 0.983465313911438}, {"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7338030636310577}]}, {"text": "We reduce the complexity by constraining the inside inference to consider only derivations which are compatible Hence the BLEU scores we get for the baselines may appear lower than what reported in the literature.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9993211030960083}]}, {"text": "Using the factorised alignments directly in a translation system resulted in a slight loss in BLEU versus using the unfactorised alignments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9991990923881531}]}, {"text": "Our baseline system uses the latter.", "labels": [], "entities": []}, {"text": "q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q  with high confidence alignments from GIZA++.", "labels": [], "entities": []}, {"text": "11 shows the sampling time with respect to the average sentence length, showing that our alignment-constrained sampling algorithm is better than the unconstrained algorithm with empirical complexity of n 4 . However, the time complexity is still high, so we set the maximum sentence length to 30 to keep our experiments practicable.", "labels": [], "entities": []}, {"text": "Presumably other means of inference maybe more efficient, such as Gibbs sampling (Levenberg et al., 2012) or auxiliary variable sampling ; we leave these extensions to future work.", "labels": [], "entities": []}, {"text": "Following (, we evaluate our model by using its output word alignments to construct a phrase table.", "labels": [], "entities": []}, {"text": "As a baseline, we train a phrasebased model using the moses toolkit 12 based on the word alignments obtained using GIZA++ in both directions and symmetrized using the growdiag-final-and heuristic.", "labels": [], "entities": []}, {"text": "This alignment is used as input to the rule factorisation algorithm, producing the ITG trees with which we initialise our sampler.", "labels": [], "entities": [{"text": "ITG trees", "start_pos": 83, "end_pos": 92, "type": "DATASET", "confidence": 0.8902805745601654}]}, {"text": "To put our results in the context of the previous work, we also compare against pialign (Neubig et al., 2011), an ITG algorithm using a Pitman-Yor process prior, as described in Section 2.", "labels": [], "entities": []}, {"text": "In the end-to-end MT pipeline we use a standard set of features: relative-frequency and lexical translation model probabilities in both directions; distance-based distortion model; language model and word count.", "labels": [], "entities": [{"text": "MT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9734441041946411}]}, {"text": "We set the distortion limit to 6 and max-phrase-length to 7 in all experiments.", "labels": [], "entities": [{"text": "distortion limit", "start_pos": 11, "end_pos": 27, "type": "METRIC", "confidence": 0.9696541428565979}, {"text": "max-phrase-length", "start_pos": 37, "end_pos": 54, "type": "METRIC", "confidence": 0.9813323020935059}]}, {"text": "We train 3-gram language models using modified Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "For AR-EN experiments the language model is trained on English data as, and for FA-EN and UR-EN the English data are the target sides of the bilingual training data.", "labels": [], "entities": [{"text": "FA-EN", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.960669994354248}]}, {"text": "We use minimum error rate training) with nbest list size 100 to optimize the feature weights for maximum: Fraction of rules with a given frequency, using a single sample grammar (UR-EN).", "labels": [], "entities": []}, {"text": "shows the BLEU scores for the three translation tasks UR/AR/FA\u2192EN based on our method against the baselines.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994334578514099}, {"text": "UR/AR/FA\u2192EN", "start_pos": 54, "end_pos": 65, "type": "METRIC", "confidence": 0.7679998363767352}]}, {"text": "For our models, we report the average BLEU score of the 5 independent runs as well as that of the aggregate phrase table generated by these 5 independent runs.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9828026592731476}]}, {"text": "There area number of interesting observations in.", "labels": [], "entities": []}, {"text": "Firstly, combining the phrase tables from independent runs results in increased BLEU scores, possibly due to the representation of uncertainty in the outputs, and the representation of different modes captured by the individual models.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.979421854019165}]}, {"text": "We believe this type of Monte Carlo model averaging should be considered in general when sampling techniques are employed for grammatical inference, e.g. in parsing and translation.", "labels": [], "entities": [{"text": "Monte Carlo model averaging", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.6823397874832153}, {"text": "parsing", "start_pos": 157, "end_pos": 164, "type": "TASK", "confidence": 0.9687526226043701}, {"text": "translation", "start_pos": 169, "end_pos": 180, "type": "TASK", "confidence": 0.6599790453910828}]}, {"text": "Secondly, our approach consistently improves over the Giza++ baseline often by a large margin, whereas pialign underperforms the GIZA++ baseline in many cases.", "labels": [], "entities": [{"text": "Giza++ baseline", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.8406510551770529}, {"text": "GIZA++ baseline", "start_pos": 129, "end_pos": 144, "type": "DATASET", "confidence": 0.8647792935371399}]}, {"text": "Thirdly, our model consistently outperforms pialign (except in AR-EN MT08 which is very close).", "labels": [], "entities": [{"text": "AR-EN MT08", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.8825041651725769}]}, {"text": "This highlights the modeling and inference differences between our method and the pialign.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The BLEU scores for the translation tasks of three language pairs. The individual column show  the average and 95% confidence intervals for 5 independent runs, whereas the combination column show  the results for combining the phrase tables of all these runs. The baselines are GIZA++ alignments and  those generated by the pialign (Neubig et al., 2011) bold: the best result.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9988589286804199}, {"text": "translation tasks", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.9036706984043121}]}, {"text": " Table 3.  Firstly, combining the phrase tables from indepen- dent runs results in increased BLEU scores, possi- bly due to the representation of uncertainty in the  outputs, and the representation of different modes  captured by the individual models. We believe this  type of Monte Carlo model averaging should be  considered in general when sampling techniques  are employed for grammatical inference, e.g. in  parsing and translation. Secondly, our approach  consistently improves over the Giza++ baseline  often by a large margin, whereas pialign under-", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 93, "end_pos": 104, "type": "METRIC", "confidence": 0.9812847375869751}, {"text": "possi- bly", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9582265814145406}, {"text": "parsing and translation", "start_pos": 414, "end_pos": 437, "type": "TASK", "confidence": 0.6247979700565338}]}]}