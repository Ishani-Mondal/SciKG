{"title": [{"text": "Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation", "labels": [], "entities": [{"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.6623327881097794}]}], "abstractContent": [{"text": "Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.795024186372757}]}, {"text": "Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7815741300582886}]}, {"text": "In this paper, we focus on further improving the performance of the reordering model (and thereby machine translation) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7236234098672867}]}, {"text": "The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine alignments being noisy.", "labels": [], "entities": []}, {"text": "To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorder-ings produced given noisy alignments and also improves word alignments using information from the reordering model.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 157, "end_pos": 172, "type": "TASK", "confidence": 0.7623326778411865}]}, {"text": "This approach generates alignments that are 2.6 f-Measure points better than a baseline supervised aligner.", "labels": [], "entities": []}, {"text": "The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and again of 5.2 BLEU points over a standard phrase-based baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9975715279579163}, {"text": "NIST MT-08 Urdu-English evaluation set", "start_pos": 109, "end_pos": 147, "type": "DATASET", "confidence": 0.9188220620155334}, {"text": "word alignments", "start_pos": 194, "end_pos": 209, "type": "TASK", "confidence": 0.7519191801548004}, {"text": "BLEU", "start_pos": 228, "end_pos": 232, "type": "METRIC", "confidence": 0.9885050654411316}]}], "introductionContent": [{"text": "Dealing with word order differences between source and target languages presents a significant challenge for machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.7910862267017365}]}, {"text": "Failing to produce target words in the correct order results in machine translation output that is not fluent and is often very hard to understand.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.6829818785190582}]}, {"text": "These problems are particularly severe when translating between languages which have very different structure.", "labels": [], "entities": [{"text": "translating between languages", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.8637981216112772}]}, {"text": "Phrase based systems () use lexicalized distortion models) and scores from the target language model to produce words in the correct order in the target language.", "labels": [], "entities": []}, {"text": "These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large.", "labels": [], "entities": []}, {"text": "There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying handwritten or automatically learned rules (.", "labels": [], "entities": []}, {"text": "Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed).", "labels": [], "entities": []}, {"text": "These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7412543594837189}]}, {"text": "In this paper, we build upon the approach in) which uses manual word alignments for learning a reordering model.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.7388936281204224}]}, {"text": "Specifically, we show that we can significantly improve reordering performance by using a large number of sentence pairs for which manual word alignments are not available.", "labels": [], "entities": []}, {"text": "The motivation for going beyond manual word alignments is clear: the reordering model can have millions of features and estimating weights for the features on thousands of sentences of manual word alignments is likely to be inadequate.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.732083797454834}]}, {"text": "One approach to deal with this problem would be to use only part-of-speech tags as features for all but the most frequent words.", "labels": [], "entities": []}, {"text": "This will cut down on the number of features and perhaps the model would be learnable with a small set of manual word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 113, "end_pos": 128, "type": "TASK", "confidence": 0.6937074065208435}]}, {"text": "Unfortunately, as we will see in the experimental section, leaving out lexical information from the models hurts performance even with a relatively small set of manual word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 168, "end_pos": 183, "type": "TASK", "confidence": 0.7341221570968628}]}, {"text": "Another option would be to collect more manual word alignments but this is undesirable because it is time consuming and expensive.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7004163563251495}]}, {"text": "The challenge in going beyond manual word alignments and using machine alignments is the noise in the machine alignments which affects the performance of the reordering model (see Section 5).", "labels": [], "entities": [{"text": "word alignments", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.7469691634178162}]}, {"text": "We illustrate this with the help of a motivating example.", "labels": [], "entities": []}, {"text": "Consider the example English sentence and its translation shown in.", "labels": [], "entities": []}, {"text": "He went to the stadium to play vaha khelne keliye stadium ko gaya A standard word alignment algorithm that we used) made the mistake of mis-aligning the Urdu ko and keliye (it switched the two).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.7129739373922348}]}, {"text": "Deriving reference reorderings from these wrong alignments would give us an incorrect reordering.", "labels": [], "entities": []}, {"text": "A reordering model trained on such incorrect reorderings would obviously perform poorly.", "labels": [], "entities": []}, {"text": "Our task is thus two-fold (i) improve the quality of machine alignments (ii) use these less noisy alignments to derive cleaner training data fora reordering model.", "labels": [], "entities": []}, {"text": "Before proceeding, we first point out that the two tasks, viz., reordering and word alignment are related: Having perfect reordering makes the alignment task easier while having perfect alignments in turn makes the task of finding reorderings trivial.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.7850683927536011}]}, {"text": "Motivated by this fact, we introduce models that allow us to connect the source/target reordering and the word alignments and show that these models help in mutually improving the performance of word alignments and reordering.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.6749456077814102}, {"text": "word alignments", "start_pos": 195, "end_pos": 210, "type": "TASK", "confidence": 0.7252470552921295}]}, {"text": "Specifically, we build two models: the first scores reorderings given the source sentence and noisy alignments, the second scores alignments given the noisy source and target reorderings and the source and target sentences themselves.", "labels": [], "entities": []}, {"text": "The second model helps produce better alignments, while we use the first model to help generate better reference reordering given noisy alignments.", "labels": [], "entities": []}, {"text": "These improved reference reorderings will then be used to train a reordering model.", "labels": [], "entities": []}, {"text": "Our experiments show that reordering models trained using these improved machine alignments perform significantly better than models trained only on manual word alignments.", "labels": [], "entities": []}, {"text": "This results in a 1.8 BLEU point gain in machine translation performance on an Urdu-English machine translation task over a preordering model trained using only manual word alignments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9994283318519592}, {"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.749226301908493}, {"text": "Urdu-English machine translation task", "start_pos": 79, "end_pos": 116, "type": "TASK", "confidence": 0.7025720626115799}]}, {"text": "In all, this increases the gain in performance by using the preordering model to 5.2 BLEU points over a standard phrasebased system with no preordering.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9990179538726807}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the main reordering issues in Urdu-English translation.", "labels": [], "entities": []}, {"text": "Section 3 introduces the reordering modeling framework that forms the basis for our work.", "labels": [], "entities": []}, {"text": "Section 4 describes the two models we use to tie together reordering and alignments and how we use these models to generate training data for training our reordering model.", "labels": [], "entities": []}, {"text": "Section 5 presents the experimental setup used for evaluating the models proposed in this paper on an Urdu-English machine translation task.", "labels": [], "entities": [{"text": "Urdu-English machine translation task", "start_pos": 102, "end_pos": 139, "type": "TASK", "confidence": 0.6695971488952637}]}, {"text": "Section 6 presents the results of our experiments.", "labels": [], "entities": []}, {"text": "We describe related work in Section 7 and finally present some concluding remarks and potential future work in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the experimental setup that we used to evaluate the models proposed in this paper.", "labels": [], "entities": []}, {"text": "All experiments were done on UrduEnglish and we evaluate reordering in two ways: Firstly, we evaluate reordering performance directly by comparing the reordered source sentence in Urdu with a reference reordering obtained from the manual word alignments using BLEU) (we call this measure monolingual BLEU or mBLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 260, "end_pos": 264, "type": "METRIC", "confidence": 0.9956039190292358}, {"text": "BLEU", "start_pos": 300, "end_pos": 304, "type": "METRIC", "confidence": 0.9742151498794556}]}, {"text": "All mBLEU results are reported on a small test set of about 400 sentences set aside from our set of sentences with manual word alignments.", "labels": [], "entities": []}, {"text": "Additionally, we evaluate the effect of reordering on our final systems for machine translation measured using BLEU.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7748633921146393}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9950445890426636}]}, {"text": "We use about 10K sentences (180K words) of manual word alignments which were created in house using part of the NIST MT-08 training data 3 to train our baseline reordering model and to train our supervised machine aligners.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7402014434337616}, {"text": "NIST MT-08 training data 3", "start_pos": 112, "end_pos": 138, "type": "DATASET", "confidence": 0.9124870061874389}]}, {"text": "We use a parallel corpus of 3.9M words consisting of 1.7M words from the NIST MT-08 training data set and 2.2M words extracted from parallel news stories on the 3 http://www.ldc.upenn.edu web 4 . The parallel corpus is used for building our phrased based machine translation system and to add training data for our reordering model.", "labels": [], "entities": [{"text": "NIST MT-08 training data set", "start_pos": 73, "end_pos": 101, "type": "DATASET", "confidence": 0.9271467447280883}, {"text": "phrased based machine translation", "start_pos": 241, "end_pos": 274, "type": "TASK", "confidence": 0.5944803059101105}]}, {"text": "For our English language model, we use the Gigaword English corpus in addition to the English side of our parallel corpus.", "labels": [], "entities": [{"text": "Gigaword English corpus", "start_pos": 43, "end_pos": 66, "type": "DATASET", "confidence": 0.9270086089769999}]}, {"text": "Our Part-of-Speech tagger is a Maximum Entropy Markov model tagger trained on roughly fifty thousand words from the CRULP corpus.", "labels": [], "entities": [{"text": "CRULP corpus", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.964009553194046}]}, {"text": "For our machine translation experiments, we used a standard phrase based system) with a lexicalized distortion model with a window size of +/-4 words . To extract phrases we use HMM alignments along with higher quality alignments from a supervised aligner).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7650031745433807}]}, {"text": "We report results on the (four reference) NIST MT-08 evaluation set in for the News and Web conditions.", "labels": [], "entities": [{"text": "NIST MT-08 evaluation", "start_pos": 42, "end_pos": 63, "type": "DATASET", "confidence": 0.8831785122553507}, {"text": "News and Web conditions", "start_pos": 79, "end_pos": 102, "type": "DATASET", "confidence": 0.909439817070961}]}, {"text": "The News and Web conditions each contain roughly 20K words in the test set, with the Web condition containing more informal text from the web.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: mBLEU scores for Urdu to English re- ordering using models trained on different data  sources and tested on a development set of 8017  Urdu tokens.", "labels": [], "entities": [{"text": "Urdu to English re- ordering", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.6516494999329249}]}, {"text": " Table 3. We also note a  gain of 2.0 BLEU points over a hierarchical phrase  based system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9990308284759521}]}, {"text": " Table 4: MT performance without preordering  (phrase based and hierarchical phrase based),  and with reordering models using different data  sources (phrase based).", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9892996549606323}]}]}