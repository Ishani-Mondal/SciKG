{"title": [{"text": "The effect of non-tightness on Bayesian estimation of PCFGs", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.5799550414085388}]}], "abstractContent": [{"text": "Probabilistic context-free grammars have the unusual property of not always defining tight distributions (i.e., the sum of the \"probabilities\" of the trees the grammar generates can be less than one).", "labels": [], "entities": []}, {"text": "This paper reviews how this non-tightness can arise and discusses its impact on Bayesian estimation of PCFGs.", "labels": [], "entities": []}, {"text": "We begin by presenting the notion of \"almost everywhere tight grammars\" and show that linear CFGs follow it.", "labels": [], "entities": []}, {"text": "We then propose three different ways of reinterpreting non-tight PCFGs to make them tight, show that the Bayesian estimators in Johnson et al.", "labels": [], "entities": []}, {"text": "(2007) are correct under one of them, and provide MCMC samplers for the other two.", "labels": [], "entities": [{"text": "MCMC samplers", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.8837504386901855}]}, {"text": "We conclude with a discussion of the impact of tightness empirically.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic Context-Free Grammars (PCFGs) play a special role in computational linguistics because they are perhaps the simplest probabilistic models of hierarchical structures.", "labels": [], "entities": [{"text": "Probabilistic Context-Free Grammars (PCFGs)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.698913554350535}]}, {"text": "Their simplicity enables us to mathematically analyze their properties to a detail that would be difficult with linguistically more accurate models.", "labels": [], "entities": []}, {"text": "Such analysis is useful because it is reasonable to expect more complex models to exhibit similar properties as well.", "labels": [], "entities": []}, {"text": "The problem of inferring PCFG rule probabilities from training data consisting of yields or strings alone is interesting from both cognitive and engineering perspectives.", "labels": [], "entities": []}, {"text": "Cognitively it is implausible that children can perceive the parse trees of the language they are learning, but it is more reasonable to assume that they can obtain the terminal strings or yield of these trees.", "labels": [], "entities": []}, {"text": "Unsupervised methods for learning a grammar from terminal strings alone is also interesting from an engineering perspective because such training data is cheap and plentiful, while the manually parsed data required by supervised methods are expensive to produce and relatively rare.", "labels": [], "entities": []}, {"text": "show that inferring PCFG rule probabilities from strings alone is computationally intractable, so we should not expect to find an efficient, general-purpose algorithm for the unsupervised problem.", "labels": [], "entities": []}, {"text": "Instead, approximation algorithms are standardly used.", "labels": [], "entities": []}, {"text": "For example, the Inside-Outside (IO) algorithm efficiently implements the Expectation-Maximization (EM) procedure for approximating a Maximum Likelihood estimator.", "labels": [], "entities": []}, {"text": "Bayesian estimators for PCFG rule probabilities have also been attracting attention because they provide a theoretically-principled way of incorporating prior information.", "labels": [], "entities": []}, {"text": "proposed a Variational Bayes estimator based on a meanfield approximation, and proposed MCMC samplers for the posterior distribution overrule probabilities and the parse trees of the training data strings.", "labels": [], "entities": [{"text": "MCMC samplers", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.8163057565689087}]}, {"text": "PCFGs have the interesting property (which we expect most linguistically more realistic models to also possess) that the distributions they define are not always properly normalized or \"tight\".", "labels": [], "entities": []}, {"text": "Ina non-tight PCFG the partition function (i.e., sum of the \"probabilities\" of all the trees generated by the PCFG) is less than one., called such non-tight PCFGs \"inconsistent\", but we follow in calling them \"non-tight\" to avoid confusion with the consistency of statistical estimators).", "labels": [], "entities": []}, {"text": "showed that renormalized non-tight PCFGs (which he called \"Gibbs CFGs\") define the same class of distributions over trees as do tight PCFGs with the same rules, and provided an algorithm for mapping any PCFG to a tight PCFG with the same rules that defines the same distribution over trees.", "labels": [], "entities": []}, {"text": "An obvious question is then: how does tightness affect the inference of PCFGs?", "labels": [], "entities": []}, {"text": "studied the question for Maximum Likelihood (ML) estimation, and showed that ML es-timates are always tight for both the supervised case (where the input consists of parse trees) and the unsupervised case (where the input consists of yields or terminal strings).", "labels": [], "entities": [{"text": "ML) estimation", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.6117803653081259}]}, {"text": "This means that ML estimators can simply ignore issues of tightness, and rest assured that the PCFGs they estimate are in fact tight.", "labels": [], "entities": [{"text": "ML estimators", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9205828905105591}]}, {"text": "The situation is more subtle with Bayesian estimators.", "labels": [], "entities": []}, {"text": "We show that for the special case of linear PCFGs (which include HMMs) with nondegenerate priors the posterior puts zero mass on non-tight PCFGs, so tightness is not an issue with Bayesian estimation of such grammars.", "labels": [], "entities": []}, {"text": "However, because all of the commonly used priors (such as the Dirichlet or the logistic normal) assign nonzero probability across the whole probability simplex, in general the posterior may assign non-zero probability to non-tight PCFGs.", "labels": [], "entities": []}, {"text": "We discuss three different possible approaches to this in this paper: 1.", "labels": [], "entities": []}, {"text": "the only-tight approach, where we modify the prior so it only assigns non-zero probability to tight PCFGs, 2.", "labels": [], "entities": []}, {"text": "the renormalization approach, where we renormalize non-tight PCFGs so they define a probability distribution over trees, and 3.", "labels": [], "entities": []}, {"text": "the sink-element approach, where we reinterpret non-tight PCFGs as assigning non-zero probability to a \"sink element\", so both tight and non-tight PCFGs are properly normalized.", "labels": [], "entities": []}, {"text": "We show how to modify the Gibbs sampler described by so it produces samples from the posterior distributions defined by the only-tight and renormalization approaches.", "labels": [], "entities": []}, {"text": "Perhaps surprisingly, we show that Gibbs sampler as defined by Johnson et al. actually produces samples from the posterior distributions defined by the sink-element approach.", "labels": [], "entities": []}, {"text": "We conclude by studying the effect of requiring tightness on the estimation of some simple PCFGs.", "labels": [], "entities": []}, {"text": "Because the Bayesian posterior converges around the (tight) ML estimate as the size of the data grows, requiring tightness only seems to make a difference with highly biased priors or with very small training corpora.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}