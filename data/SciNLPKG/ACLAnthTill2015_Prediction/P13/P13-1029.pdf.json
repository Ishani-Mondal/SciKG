{"title": [], "abstractContent": [{"text": "In this paper, we consider the problem of cross-formalism transfer in parsing.", "labels": [], "entities": [{"text": "cross-formalism transfer", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.7150270938873291}]}, {"text": "We are interested in parsing constituency-based grammars such as HPSG and CCG using a small amount of data specific for the target formalism, and a large quantity of coarse CFG annotations from the Penn Treebank.", "labels": [], "entities": [{"text": "parsing constituency-based grammars", "start_pos": 21, "end_pos": 56, "type": "TASK", "confidence": 0.8762301405270895}, {"text": "HPSG", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.9166785478591919}, {"text": "Penn Treebank", "start_pos": 198, "end_pos": 211, "type": "DATASET", "confidence": 0.9943186342716217}]}, {"text": "While all of the target formalisms share a similar basic syntactic structure with Penn Treebank CFG, they also encode additional constraints and semantic features.", "labels": [], "entities": [{"text": "Penn Treebank CFG", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.9886770248413086}]}, {"text": "To handle this apparent discrepancy, we design a probabilistic model that jointly generates CFG and target formalism parses.", "labels": [], "entities": [{"text": "CFG", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.789717972278595}]}, {"text": "The model includes features of both parses, allowing transfer between the formalisms, while preserving parsing efficiency.", "labels": [], "entities": []}, {"text": "We evaluate our approach on three constituency-based grammars-CCG, HPSG, and LFG, augmented with the Penn Treebank-1.", "labels": [], "entities": [{"text": "Penn Treebank-1", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.9955936372280121}]}, {"text": "Our experiments show that across all three formalisms , the target parsers significantly benefit from the coarse annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the last several decades, linguists have introduced many different grammars for describing the syntax of natural languages.", "labels": [], "entities": []}, {"text": "Moreover, the ongoing process of developing new formalisms is intrinsic to linguistic research.", "labels": [], "entities": []}, {"text": "However, before these grammars can be used for statistical parsing, they require annotated sentences for training.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.8304628729820251}]}, {"text": "The difficulty of obtaining such annotations is a key limiting factor that inhibits the effective use of these grammars.", "labels": [], "entities": []}, {"text": "The standard solution to this bottleneck has relied on manually crafted transformation rules that map readily available syntactic annotations (e.g, the Penn Treebank) to the desired formalism.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 152, "end_pos": 165, "type": "DATASET", "confidence": 0.9946211874485016}]}, {"text": "Designing these transformation rules is a major undertaking which requires multiple correction cycles and a deep understanding of the underlying grammar formalisms.", "labels": [], "entities": []}, {"text": "In addition, designing these rules frequently requires external resources such as Wordnet, and even involves correction of the existing treebank.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9682908654212952}]}, {"text": "This effort has to be repeated for each new grammar formalism, each new annotation scheme and each new language.", "labels": [], "entities": []}, {"text": "In this paper, we propose an alternative approach for parsing constituency-based grammars.", "labels": [], "entities": [{"text": "parsing constituency-based grammars", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.896636446317037}]}, {"text": "Instead of using manually-crafted transformation rules, this approach relies on a small amount of annotations in the target formalism.", "labels": [], "entities": []}, {"text": "Frequently, such annotations are available in linguistic texts that introduce the formalism.", "labels": [], "entities": []}, {"text": "For instance, a textbook on HPSG illustrates grammatical constructions using about 600 examples.", "labels": [], "entities": []}, {"text": "While these examples are informative, they are not sufficient for training.", "labels": [], "entities": []}, {"text": "To compensate for the annotation sparsity, our approach utilizes coarsely annotated data readily available in large quantities.", "labels": [], "entities": []}, {"text": "A natural candidate for such coarse annotations is context-free grammar (CFG) from the Penn Treebank, while the target formalism can be any constituency-based grammars, such as Combinatory Categorial Grammar (CCG)), Lexical Functional Grammar (LFG) or Head-Driven Phrase Structure Grammar (HPSG).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.9944153726100922}]}, {"text": "All of these formalisms share a similar basic syntactic structure with Penn Treebank CFG.", "labels": [], "entities": [{"text": "Penn Treebank CFG", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.9919256170590719}]}, {"text": "However, the target formalisms also encode additional constraints and semantic features.", "labels": [], "entities": []}, {"text": "For instance, Penn Treebank annotations do not make an explicit distinction between complement and adjunct, while all the above grammars mark these roles explicitly.", "labels": [], "entities": [{"text": "Penn Treebank annotations", "start_pos": 14, "end_pos": 39, "type": "DATASET", "confidence": 0.9706146717071533}]}, {"text": "Moreover, even the identical syntactic information is encoded differently in these formalisms.", "labels": [], "entities": []}, {"text": "An example of this phenomenon is the marking of subject.", "labels": [], "entities": []}, {"text": "In LFG, this information is captured in the mapping equation, namely \u2191 SBJ =\u2193, while Penn Treebank represents it as a functional tag, such as NP-SBJ.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.9888838231563568}]}, {"text": "shows derivations in the three target formalisms we consider, as well as a CFG derivation.", "labels": [], "entities": [{"text": "CFG", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.8296940326690674}]}, {"text": "We can see that the derivations of these formalisms share the same basic structure, while the formalism-specific information is mainly encoded in the lexical entries and node labels.", "labels": [], "entities": []}, {"text": "To enable effective transfer the model has to identify shared structural components between the formalisms despite the apparent differences.", "labels": [], "entities": []}, {"text": "Moreover, we do not assume parallel annotations.", "labels": [], "entities": []}, {"text": "To this end, our model jointly parses the two corpora according to the corresponding annotations, enabling transfer via parameter sharing.", "labels": [], "entities": []}, {"text": "In particular, we augment each target tree node with hidden variables that capture the connection to the coarse annotations.", "labels": [], "entities": []}, {"text": "Specifically, each node in the target tree has two labels: an entry which is specific to the target formalism, and a latent label containing a value from the Penn Treebank tagset, such as NP (see).", "labels": [], "entities": [{"text": "Penn Treebank tagset", "start_pos": 158, "end_pos": 178, "type": "DATASET", "confidence": 0.9888719717661539}]}, {"text": "This design enables us to represent three types of features: the target formalismspecific features, the coarse formalism features, and features that connect the two.", "labels": [], "entities": []}, {"text": "This modeling approach makes it possible to perform transfer to a range of target formalisms, without manually drafting formalism-specific rules.", "labels": [], "entities": []}, {"text": "We evaluate our approach on three constituency-based grammars -CCG, HPSG, and LFG.", "labels": [], "entities": []}, {"text": "As a source of coarse annotations, we use the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.996097981929779}]}, {"text": "Our results clearly demonstrate that for all three formalisms, parsing accuracy can be improved by training with additional coarse annotations.", "labels": [], "entities": [{"text": "parsing", "start_pos": 63, "end_pos": 70, "type": "TASK", "confidence": 0.9770068526268005}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9757349491119385}]}, {"text": "For instance, the model trained on 500 HPSG sentences achieves labeled dependency F-score of 72.3%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.6606101989746094}]}, {"text": "Adding 15,000 Penn Treebank sentences during training leads to 78.5% labeled dependency F-score, an absolute improvement of 6.2%.", "labels": [], "entities": [{"text": "Penn Treebank sentences", "start_pos": 14, "end_pos": 37, "type": "DATASET", "confidence": 0.9841578404108683}, {"text": "labeled dependency F-score", "start_pos": 69, "end_pos": 95, "type": "METRIC", "confidence": 0.8219272096951803}]}, {"text": "To achieve similar performance in the absence of coarse annotations, the parser has to be trained on about 1,500 sentences, namely three times what is needed when using coarse annotations.", "labels": [], "entities": []}, {"text": "Similar results are While the Penn Treebank-2 contains richer annotations, we decided to use the Penn Treebank-1 to demonstrate the feasibility of transfer from coarse annotations.", "labels": [], "entities": [{"text": "Penn Treebank-2", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.9947678744792938}, {"text": "Penn Treebank-1", "start_pos": 97, "end_pos": 112, "type": "DATASET", "confidence": 0.9949460029602051}]}], "datasetContent": [{"text": "All of these corpora were derived via conversion of Penn Treebank to the target formalisms.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9954812824726105}]}, {"text": "In particular, our CCG and HPSG datasets were converted from the Penn Treebank based on hand- crafted rules.", "labels": [], "entities": [{"text": "HPSG datasets", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.9445201456546783}, {"text": "Penn Treebank", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9940249621868134}]}, {"text": "shows which sections of the treebanks were used in training, testing and development for both formalisms.", "labels": [], "entities": []}, {"text": "Our LFG training dataset was constructed in a similar fashion).", "labels": [], "entities": [{"text": "LFG training dataset", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8648414015769958}]}, {"text": "However, we choose to use PARC700 as our LFG tesing and development datasets, following the previous work by).", "labels": [], "entities": [{"text": "PARC700", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.853333592414856}, {"text": "LFG tesing", "start_pos": 41, "end_pos": 51, "type": "TASK", "confidence": 0.6588394045829773}]}, {"text": "It contains 700 manually annotated sentences that are randomly selected from Penn Treebank Section 23.", "labels": [], "entities": [{"text": "Penn Treebank Section 23", "start_pos": 77, "end_pos": 101, "type": "DATASET", "confidence": 0.9920825809240341}]}, {"text": "The split of PARC700 follows the setting in ().", "labels": [], "entities": [{"text": "PARC700", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.8963088393211365}]}, {"text": "Since our model does not assume parallel data, we use distinct sentences in the source and target treebanks.", "labels": [], "entities": []}, {"text": "Following previous work, we only consider sentences not exceeding 40 words, except on PARC700 where all sentences are used.", "labels": [], "entities": [{"text": "PARC700", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9599406123161316}]}, {"text": "Evaluation Metrics: We use two evaluation metrics.", "labels": [], "entities": []}, {"text": "First, following previous work, we evaluate our method using the labeled and unlabeled predicate-argument dependency F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.7859099507331848}]}, {"text": "This metric is commonly used to measure parsing quality for the formalisms considered in this paper.", "labels": [], "entities": []}, {"text": "The detailed definition of this measure as applied for each formalism is provided in).", "labels": [], "entities": []}, {"text": "For CCG, we use the evaluation script from the C&C tools.", "labels": [], "entities": []}, {"text": "For HPSG, we evaluate all types of dependencies, including punctuations.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8033062815666199}]}, {"text": "For LFG, we consider the preds-only dependencies, which are the dependencies between pairs of words.", "labels": [], "entities": []}, {"text": "Secondly, we also evaluate using unlabeled PARSEVAL, a standard measure for PCFG parsing.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9561154246330261}, {"text": "PCFG parsing", "start_pos": 76, "end_pos": 88, "type": "TASK", "confidence": 0.7131581902503967}]}, {"text": "The dependency F-score captures both the target-5 Available at http://svn.ask.it.usyd.edu.au/trac/candc/wiki grammar labels and tree-structural relations.", "labels": [], "entities": [{"text": "F-score", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.8726686835289001}]}, {"text": "The unlabeled PARSEVAL is used as an auxiliary measure that enables us to separate these two aspects by focusing on the structural relations exclusively.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9440468549728394}]}, {"text": "Training without CFG Data: To assess the impact of coarse data in the experiments below, we also consider the model trained only on formalism-specific annotations.", "labels": [], "entities": [{"text": "CFG Data", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.9173020124435425}]}, {"text": "When no CFG sentences are available, we assign all the CFG labels to a special value shared by all the nodes.", "labels": [], "entities": []}, {"text": "In this set-up, the model reduces to a normal loglinear model for the target formalism.", "labels": [], "entities": []}, {"text": "Parameter Settings: During training, all the feature parameters \u03b8 are initialized to zero.", "labels": [], "entities": []}, {"text": "The hyperparameters used in the model are tuned on the development sets.", "labels": [], "entities": []}, {"text": "We noticed, however, that the resulting values are consistent across different formalisms.", "labels": [], "entities": []}, {"text": "In particular, we set the l 2 -norm weight to \u03bb = 1.0, the supertagger threshold to \u03b2 = 0.01, and the PCFG pruning threshold to \u03b1 = 0.002.", "labels": [], "entities": []}, {"text": "Impact of Coarse Annotations on Target Formalism: To analyze the effectiveness of annotation transfer, we fix the number of annotated trees in the target formalism and vary the amount of coarse annotations available to the algorithm during training.", "labels": [], "entities": [{"text": "annotation transfer", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7156031131744385}]}, {"text": "In particular, we use 500 sentences with formalism-specific annotations, and vary the number of CFG trees from zero to 15,000.", "labels": [], "entities": [{"text": "CFG trees", "start_pos": 96, "end_pos": 105, "type": "DATASET", "confidence": 0.8149798214435577}]}, {"text": "As shows, CFG data boosts parsing accuracy for all the target formalisms.", "labels": [], "entities": [{"text": "CFG data", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.773217111825943}, {"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9742156863212585}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9740678668022156}]}, {"text": "For instance, there is again of 6.2% in labeled dependency F-score for HPSG formalism when 15,000 CFG trees are used.", "labels": [], "entities": [{"text": "F-score", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.881098747253418}]}, {"text": "Moreover, increasing the number of coarse annotations used in training leads to further improvement on different evaluation metrics.", "labels": [], "entities": []}, {"text": "Tradeoff between Target and Coarse Annotations: We also assess the relative contribution of coarse annotations when the size of annotated training corpus in the target formalism varies.", "labels": [], "entities": []}, {"text": "In this set of experiments, we fix the number of CFG trees to 15,000 and vary the number of target annotations from 500 to 4,000.", "labels": [], "entities": [{"text": "CFG trees", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.8216289281845093}]}, {"text": "shows the relative contribution of formalism-specific annotations compared to that of the coarse annotations.", "labels": [], "entities": []}, {"text": "For instance, shows that the parsing performance achieved using 2,000 CCG sentences can be achieved using approximately 500 CCG sentences when coarse annotations are available for training.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9705356955528259}]}, {"text": "More generally, the result convincingly demonstrates that coarse annotations are helpful for all the sizes of formalism-specific training data.", "labels": [], "entities": []}, {"text": "As expected, the improvement margin decreases when more formalism-specific data is used.", "labels": [], "entities": [{"text": "improvement margin", "start_pos": 17, "end_pos": 35, "type": "METRIC", "confidence": 0.9511408805847168}]}, {"text": "also illustrates a slightly different characteristics of transfer performance between two evaluation metrics.", "labels": [], "entities": []}, {"text": "Across all three grammars, we can observe that adding CFG data has a more pronounced effect on the PARSEVAL measure than the dependency F-score.", "labels": [], "entities": [{"text": "CFG data", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.8356252014636993}, {"text": "PARSEVAL measure", "start_pos": 99, "end_pos": 115, "type": "METRIC", "confidence": 0.9634765088558197}, {"text": "F-score", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.7981366515159607}]}, {"text": "This phenomenon can be explained as follows.", "labels": [], "entities": []}, {"text": "The unlabeled PARSEVAL score-f) mainly relies on the coarse structural information.", "labels": [], "entities": [{"text": "PARSEVAL score-f)", "start_pos": 14, "end_pos": 31, "type": "METRIC", "confidence": 0.9494361281394958}]}, {"text": "On the other hand, predicate-argument dependency Fscore also relies on the target grammar information.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.6868860721588135}]}, {"text": "Because that our model only transfers structural information from the source treebank, the gains of PARSEVAL are expected to be larger than that of dependency F-score.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9449649453163147}, {"text": "F-score", "start_pos": 159, "end_pos": 166, "type": "METRIC", "confidence": 0.8383879661560059}]}], "tableCaptions": [{"text": " Table 4: The labeled/unlabeled dependency F- score comparisons between our model and state- of-the-art parsers.", "labels": [], "entities": [{"text": "F- score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9587955872217814}]}]}