{"title": [{"text": "Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation *", "labels": [], "entities": [{"text": "Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation", "start_pos": 22, "end_pos": 93, "type": "TASK", "confidence": 0.7350303360394069}]}], "abstractContent": [{"text": "Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.6942636469999949}]}, {"text": "In this paper, we propose a novel approach to finding translations for oov words.", "labels": [], "entities": []}, {"text": "We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases.", "labels": [], "entities": []}, {"text": "Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations.", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7909971177577972}]}, {"text": "Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7345563471317291}]}], "introductionContent": [{"text": "Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.7114063302675883}]}, {"text": "SMT systems usually copy unknown words verbatim to the target language output.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9831013083457947}]}, {"text": "Although this is helpful in translating a small fraction of oovs such as named entities for languages with same writing systems, it harms the translation in other types of oovs and distant language pairs.", "labels": [], "entities": []}, {"text": "In general, copied-over oovs area hindrance to fluent, high quality translation, and we can see evidence of this in automatic measures such as BLEU () and also inhuman evaluation scores such as HTER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9987941980361938}, {"text": "HTER", "start_pos": 194, "end_pos": 198, "type": "DATASET", "confidence": 0.6005194187164307}]}, {"text": "The problem becomes more severe when only a limited amount of parallel text is available for training or when the training and test data are from different domains.", "labels": [], "entities": []}, {"text": "Even noisy translation of oovs can aid the language model to better * This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant.", "labels": [], "entities": [{"text": "NSERC, Canada (RGPIN: 264905) grant", "start_pos": 114, "end_pos": 149, "type": "DATASET", "confidence": 0.8116561969121298}]}, {"text": "The third author was supported by an early career research award from Monash University to visit Simon Fraser University.", "labels": [], "entities": [{"text": "Simon Fraser University", "start_pos": 97, "end_pos": 120, "type": "DATASET", "confidence": 0.9209639032681783}]}, {"text": "re-order the words in the target language (.", "labels": [], "entities": []}, {"text": "Increasing the size of the parallel data can reduce the number of oovs.", "labels": [], "entities": []}, {"text": "However, there will always be some words or phrases that are new to the system and finding ways to translate such words or phrases will be beneficial to the system.", "labels": [], "entities": []}, {"text": "Researchers have applied a number of approaches to tackle this problem.", "labels": [], "entities": []}, {"text": "Some approaches use pivot languages) while others use lexicon-induction-based approaches from source language monolingual corpora (.", "labels": [], "entities": []}, {"text": "Pivot language techniques tackle this problem by taking advantage of available parallel data between the source language and a third language.", "labels": [], "entities": []}, {"text": "Using a pivot language, oovs are translated into a third language and back into the source language and thereby paraphrases to those oov words are extracted).", "labels": [], "entities": []}, {"text": "For each oov, the system can be augmented by aggregating the translations of all its paraphrases and assign them to the oov.", "labels": [], "entities": []}, {"text": "However, these methods require parallel corpora between the source language and one or multiple pivot languages.", "labels": [], "entities": []}, {"text": "Another line of work exploits spelling and morphological variants of oov words.", "labels": [], "entities": []}, {"text": "presents techniques for online handling of oov words for Arabic to English such as spelling expansion and morphological expansion.", "labels": [], "entities": [{"text": "spelling expansion", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7530972063541412}, {"text": "morphological expansion", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.7435649931430817}]}, {"text": "proposes a method to combine sublexical/constituent translations of an oov word or phrase to generate its translations.", "labels": [], "entities": []}, {"text": "Several researchers have applied lexiconinduction methods to create a bilingual lexicon for those oovs.", "labels": [], "entities": []}, {"text": "use a monolingual text on the source side to find paraphrases to oov words for which the translations are available.", "labels": [], "entities": []}, {"text": "The translations for these paraphrases are then used as the translations of the oov word.", "labels": [], "entities": []}, {"text": "These methods are based on the distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning.", "labels": [], "entities": []}, {"text": "showed that this method improves over the baseline system where oovs are untranslated.", "labels": [], "entities": []}, {"text": "We propose a graph propagation-based extension to the approach of in which a graph is constructed from source language monolingual text 1 and the source-side of the available parallel data.", "labels": [], "entities": []}, {"text": "Nodes that have related meanings are connected together and nodes for which we have translations in the phrase-table are annotated with target-side translations and their feature values.", "labels": [], "entities": []}, {"text": "A graph propagation algorithm is then used to propagate translations from labeled nodes to unlabeled nodes (phrases appearing only in the monolingual text and oovs).", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.7865195274353027}]}, {"text": "This provides a general purpose approach to handle several types of oovs, including morphological variants, spelling variants and synonyms 2 . Constructing such a huge graph and propagating messages through it pose severe computational challenges.", "labels": [], "entities": []}, {"text": "Throughout the paper, we will see how these challenges are dealt with using scalable algorithms.", "labels": [], "entities": []}, {"text": "introduced the notion of a distributional profile in bilingual lexicon induction from monolingual data.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.642793744802475}]}, {"text": "A distributional profile (DP) of a word or phrase type is a co-occurrence vector created by combining all co-occurrence vectors of the tokens of that phrase type.", "labels": [], "entities": []}, {"text": "Each distributional profile can be seen as a point in a |V |-dimensional space where V is the vocabulary where each word type represents a unique axis.", "labels": [], "entities": []}, {"text": "Points (i.e. phrase types) that are close to one another in this highdimensional space can represent paraphrases.", "labels": [], "entities": []}, {"text": "This approach has also been used in machine translation to find in-vocabulary paraphrases for oov words on the source side and find away to translate them.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8024324476718903}]}], "datasetContent": [{"text": "We experimented with two different domains for the bilingual data: Europarl corpus From the dev and test sets, we extract all source words that do not appear in the phrase-table constructed from the parallel data.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.9780077040195465}]}, {"text": "From the oovs, we exclude numbers as well as named entities.", "labels": [], "entities": []}, {"text": "We apply a simple heuristic to detect named entities: basically words that are capitalized in the original dev/test set that do not appear at the beginning of a sentence are named entities.", "labels": [], "entities": []}, {"text": "For the end-to-end MT pipeline, we used Moses () with these standard features: relative-frequency and lexical translation model (TM) probabilities in both directions; distortion model; language model (LM) and word count.", "labels": [], "entities": [{"text": "MT pipeline", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9214351177215576}]}, {"text": "Word alignment is done using GIZA++.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.718836784362793}]}, {"text": "We used distortion limit of 6 and max-phrase-length of 10 in all the experiments.", "labels": [], "entities": [{"text": "distortion limit", "start_pos": 8, "end_pos": 24, "type": "METRIC", "confidence": 0.9788975417613983}, {"text": "max-phrase-length", "start_pos": 34, "end_pos": 51, "type": "METRIC", "confidence": 0.9897359609603882}]}, {"text": "For the language model, we used the KenLM toolkit) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 111, "end_pos": 126, "type": "DATASET", "confidence": 0.9932737946510315}]}, {"text": "If we have a list of possible translations for oovs with their probabilities, we become able to evaluate different methods we discussed.", "labels": [], "entities": []}, {"text": "We wordaligned the dev/test sets by concatenating them to a large parallel corpus and running GIZA++ on the whole set.", "labels": [], "entities": []}, {"text": "The resulting word alignments are used to extract the translations for each oov.", "labels": [], "entities": []}, {"text": "The correctness of this gold standard is limited to the size of the parallel data used as well as the quality of the word alignment software toolkit, and is not 100% precise.", "labels": [], "entities": [{"text": "correctness", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9793922305107117}, {"text": "word alignment software toolkit", "start_pos": 117, "end_pos": 148, "type": "TASK", "confidence": 0.8220837116241455}]}, {"text": "However, it gives a good estimate of how each oov should be translated without the need for human judgments.", "labels": [], "entities": []}, {"text": "For evaluating our baseline as well as graphbased approaches, we use both intrinsic and extrinsic evaluations.", "labels": [], "entities": []}, {"text": "Two intrinsic evaluation metrics that we use to evaluate the possible translations for oovs are Mean Reciprocal Rank (MRR) and Recall.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 96, "end_pos": 122, "type": "METRIC", "confidence": 0.9667546451091766}, {"text": "Recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9895452857017517}]}, {"text": "Intrinsic evaluation metrics are faster to apply and are used to optimize different hyper-parameters of the approach (e.g. window size, phrase length, etc.).", "labels": [], "entities": []}, {"text": "Once we come up with the optimized values for the hyper-parameters, we extrinsically evaluate different approaches by adding the new translations to the phrase-table and run it through the MT pipeline.", "labels": [], "entities": [{"text": "MT pipeline", "start_pos": 189, "end_pos": 200, "type": "TASK", "confidence": 0.6512021124362946}]}], "tableCaptions": [{"text": " Table 1: Statistics of training sets in different domains.", "labels": [], "entities": []}, {"text": " Table 2: number of oovs in dev and test sets for Europarl and  EMEA systems.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.9875642657279968}, {"text": "EMEA", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.6481572985649109}]}, {"text": " Table 3: Results of intrinsic evaluations (MRR and Recall)  on Europarl, window size 4 and paraphrase length 2", "labels": [], "entities": [{"text": "MRR", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9934276342391968}, {"text": "Recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9863805174827576}, {"text": "Europarl", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.9925819635391235}]}, {"text": " Table 4: Intrinsic results of different types of graphs when  using unigram nodes on Europarl.", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.957443118095398}, {"text": "Europarl", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.9923415184020996}]}, {"text": " Table 5: Results on using unigram or bigram nodes.", "labels": [], "entities": []}, {"text": " Table 6: Bleu scores for different domains with or without using oov translations.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9771695733070374}]}]}