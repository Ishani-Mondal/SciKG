{"title": [], "abstractContent": [{"text": "Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparse-ness.", "labels": [], "entities": [{"text": "Natural language parsing", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6527856091658274}]}, {"text": "Instead, we introduce a Compo-sitional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations.", "labels": [], "entities": []}, {"text": "The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%.", "labels": [], "entities": [{"text": "CVG", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7576436996459961}, {"text": "PCFG", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.8310437798500061}, {"text": "Stanford Parser", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.8614098429679871}, {"text": "F1 score", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.988938570022583}]}, {"text": "It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser.", "labels": [], "entities": []}, {"text": "The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactic parsing is a central task in natural language processing because of its importance in mediating between linguistic expression and meaning.", "labels": [], "entities": [{"text": "Syntactic parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9289140403270721}, {"text": "natural language processing", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.6846282482147217}]}, {"text": "For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling () and paraphrase detection.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.8833177387714386}, {"text": "semantic role labeling", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.6580351094404856}, {"text": "paraphrase detection", "start_pos": 157, "end_pos": 177, "type": "TASK", "confidence": 0.9424946010112762}]}, {"text": "Syntactic descriptions standardly use coarse discrete categories such as NP for noun phrases or PP for prepositional phrases.", "labels": [], "entities": []}, {"text": "However, recent work has shown that parsing results can be greatly improved by defining more fine-grained syntactic  categories, which better capture phrases with similar behavior, whether through manual feature engineering () or automatic learning ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9683634042739868}]}, {"text": "However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity.", "labels": [], "entities": []}, {"text": "Two strands of work therefore attempt to go further.", "labels": [], "entities": []}, {"text": "First, recent work in discriminative parsing has shown gains from careful engineering of features (.", "labels": [], "entities": [{"text": "discriminative parsing", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.6728853285312653}]}, {"text": "Features in such parsers can be seen as defining effective dimensions of similarity between categories.", "labels": [], "entities": []}, {"text": "Second, lexicalized parsers) associate each category with a lexical item.", "labels": [], "entities": []}, {"text": "This gives a fine-grained notion of semantic similarity, which is useful for tackling problems like ambiguous attachment decisions.", "labels": [], "entities": []}, {"text": "However, this approach necessitates complex shrinkage estimation schemes to deal with the sparsity of observations of the lexicalized categories.", "labels": [], "entities": []}, {"text": "In many natural language systems, single words and n-grams are usefully described by their distributional similarities), among many others.", "labels": [], "entities": []}, {"text": "But, even with large corpora, many n-grams will never be seen during training, especially when n is large.", "labels": [], "entities": []}, {"text": "In these cases, one cannot simply use distributional similarities to represent unseen phrases.", "labels": [], "entities": []}, {"text": "In this work, we present anew solution to learn features and phrase representations even for very long, unseen n-grams.", "labels": [], "entities": []}, {"text": "We introduce a Compositional Vector Grammar Parser (CVG) for structure prediction.", "labels": [], "entities": [{"text": "structure prediction", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.8245838582515717}]}, {"text": "Like the above work on parsing, the model addresses the problem of representing phrases and categories.", "labels": [], "entities": []}, {"text": "Unlike them, it jointly learns how to parse and how to represent phrases as both discrete categories and continuous vectors as illustrated in.", "labels": [], "entities": []}, {"text": "CVGs combine the advantages of standard probabilistic context free grammars (PCFG) with those of recursive neural networks (RNNs).", "labels": [], "entities": []}, {"text": "The former can capture the discrete categorization of phrases into NP or PP while the latter can capture fine-grained syntactic and compositional-semantic information on phrases and words.", "labels": [], "entities": []}, {"text": "This information can help in cases where syntactic ambiguity can only be resolved with semantic information, such as in the PP attachment of the two sentences: They ate udon with forks. vs. They ate udon with chicken.", "labels": [], "entities": []}, {"text": "Previous RNN-based parsers used the same (tied) weights at all nodes to compute the vector representing a constituent.", "labels": [], "entities": []}, {"text": "This requires the composition function to be extremely powerful, since it has to combine phrases with different syntactic head words, and it is hard to optimize since the parameters form a very deep neural network.", "labels": [], "entities": []}, {"text": "We generalize the fully tied RNN to one with syntactically untied weights.", "labels": [], "entities": []}, {"text": "The weights at each node are conditionally dependent on the categories of the child constituents.", "labels": [], "entities": []}, {"text": "This allows different composition functions when combining different types of phrases and is shown to result in a large improvement in parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 135, "end_pos": 142, "type": "TASK", "confidence": 0.967892050743103}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9169638156890869}]}, {"text": "Our compositional distributed representation allows a CVG parser to make accurate parsing decisions and capture similarities between phrases and sentences.", "labels": [], "entities": []}, {"text": "Any PCFG-based parser can be improved with an RNN.", "labels": [], "entities": []}, {"text": "We use a simplified version of the Stanford Parser ( as the base PCFG and improve its accuracy from 86.56 to 90.44% labeled F1 on all sentences of the WSJ section 23.", "labels": [], "entities": [{"text": "Stanford Parser", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.8332800567150116}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.999397873878479}, {"text": "F1", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.9767909049987793}, {"text": "WSJ section 23", "start_pos": 151, "end_pos": 165, "type": "DATASET", "confidence": 0.9623420238494873}]}, {"text": "The code of our parser is available at nlp.stanford.edu.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the CVG in two ways: First, by a standard parsing evaluation on Penn Treebank WSJ and then by analyzing the model errors in detail.", "labels": [], "entities": [{"text": "Penn Treebank WSJ", "start_pos": 76, "end_pos": 93, "type": "DATASET", "confidence": 0.9855712056159973}]}], "tableCaptions": [{"text": " Table 1: Comparison of parsers with richer state  representations on the WSJ. The last line is the  self-trained re-ranked Charniak parser.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.9703014492988586}]}]}