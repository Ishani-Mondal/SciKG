{"title": [{"text": "Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation", "labels": [], "entities": [{"text": "Crowd Prefers", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7467817664146423}, {"text": "Turker Biases in Query Segmentation", "start_pos": 74, "end_pos": 109, "type": "TASK", "confidence": 0.7019407093524933}]}], "abstractContent": [{"text": "Query segmentation, like text chunking, is the first step towards query understanding.", "labels": [], "entities": [{"text": "Query segmentation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.862311065196991}, {"text": "text chunking", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.7122582793235779}, {"text": "query understanding", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.8364307582378387}]}, {"text": "In this study, we explore the effectiveness of crowdsourcing for this task.", "labels": [], "entities": []}, {"text": "Through carefully designed control experiments and Inter Annotator Agreement metrics for analysis of experimental data, we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal (often only two) parts.", "labels": [], "entities": [{"text": "query segmentation", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.8775786757469177}]}, {"text": "Similarly , in the case of hierarchical or nested segmentation, turkers have a strong preference towards balanced binary trees.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text chunking of Natural Language (NL) sentences is a well studied problem that is an essential preprocessing step for many NLP applications.", "labels": [], "entities": [{"text": "Text chunking of Natural Language (NL) sentences", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8733447160985734}]}, {"text": "In the context of Web search queries, query segmentation is similarly the first step towards analysis and understanding of queries).", "labels": [], "entities": [{"text": "query segmentation", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7514501512050629}]}, {"text": "The task in both the cases is to divide the sentence or the query into contiguous segments or chunks of words such that the words from a segment are related to each other more strongly than words from different segments (.", "labels": [], "entities": []}, {"text": "It is typically assumed that the segments are structurally and semantically coherent and, therefore, the information contained in them can be processed holistically.", "labels": [], "entities": []}, {"text": "A majority of work on query segmentation relies on manually segmented queries by human experts for training and evaluation of segmentation algorithms.", "labels": [], "entities": [{"text": "query segmentation", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8714491426944733}]}, {"text": "These are typically small datasets and even with detailed annotation guidelines and/or close supervision, low Inter Annotator Agreement (IAA) remains an issue.", "labels": [], "entities": [{"text": "Inter Annotator Agreement (IAA)", "start_pos": 110, "end_pos": 141, "type": "METRIC", "confidence": 0.8260353704293569}]}, {"text": "For instance, illustrates the variation in flat segmentation by 10 annotators.", "labels": [], "entities": []}, {"text": "This confusion is mainly because the definition of a segment in a query is ambiguous and of an unspecified granularity.", "labels": [], "entities": []}, {"text": "This is further compounded by the fact that other than easily recognizable and agreed upon segments such as Named Entities or Multi-Word Expressions, there is no established notion of linguistic grouping such as phrases and clauses in a query.", "labels": [], "entities": []}, {"text": "Although there is little work on the use of crowdsourcing for query segmentation, the idea that the crowd could be a potential (and cheaper) source for reliable segmentation seems a reasonable assumption.", "labels": [], "entities": [{"text": "query segmentation", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7935409545898438}]}, {"text": "The need for larger datasets makes this an attractive proposition.", "labels": [], "entities": []}, {"text": "Also, a larger number of annotations could be appropriately distilled to obtain better quality segmentations.", "labels": [], "entities": []}, {"text": "In this paper we explore crowdsourcing as an option for query segmentation through experiments designed using Amazon Mechanical Turk (AMT) . We compare the results against gold datasets created by trained annotators.", "labels": [], "entities": [{"text": "query segmentation", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.8656666576862335}, {"text": "Amazon Mechanical Turk (AMT)", "start_pos": 110, "end_pos": 138, "type": "DATASET", "confidence": 0.8554026981194814}]}, {"text": "We address the issues pertaining to disagreements due to both ambiguity and granularity and attempt to objectively quantify their role in IAA.", "labels": [], "entities": [{"text": "IAA", "start_pos": 138, "end_pos": 141, "type": "TASK", "confidence": 0.9297741055488586}]}, {"text": "To this end, we also conduct similar annotation experiments for NL sentences and randomly generated queries.", "labels": [], "entities": []}, {"text": "While queries are not as structured as NL sentences they are not simply a set of random words.", "labels": [], "entities": []}, {"text": "Thus, it is necessary to compare query segmentation to th\u00eb uber-structure of NL sentences as well as the unter-structure of random n-grams.", "labels": [], "entities": [{"text": "query segmentation", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.8021474480628967}]}, {"text": "This has important implications for understanding any inherent biases annotators may have as a result of the apparent lack of structure of the queries.", "labels": [], "entities": []}, {"text": "To quantify the effect of granularity on segmentation, we also ask annotators to provide hierarchical or nested segmentations for real and random queries, as well as sentences.", "labels": [], "entities": []}, {"text": "Following proposal for hierarchical chunking of NL, we ask the annotators to group exactly two words or segments at a time to recursively form bigger segments.", "labels": [], "entities": [{"text": "hierarchical chunking of NL", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.6875347197055817}]}, {"text": "The concept is illustrated in. shows annotations from 10 Turkers.", "labels": [], "entities": []}, {"text": "It is important to constrain the joining of exactly two segments or words at a time to avoid the issue of fuzziness in granularity.", "labels": [], "entities": []}, {"text": "We shall refer to this style of annotation as Nested segmentation, whereas the non-hierarchical nonconstrained chunking will be referred to as Flat segmentation.", "labels": [], "entities": [{"text": "Nested segmentation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.684028148651123}]}, {"text": "Through statistical analysis of the experimental data we show that crowdsourcing may not be the best practice for query segmentation, not only because of ambiguity and granularity issues, but because there exist very strong biases amongst annotators to divide a query into two roughly equal parts that result in misleadingly high agreements.", "labels": [], "entities": [{"text": "query segmentation", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.8545089364051819}]}, {"text": "As apart of our analysis framework, we introduce anew IAA metric for comparison across flat and nested segmentations.", "labels": [], "entities": [{"text": "IAA", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.7145678997039795}]}, {"text": "This versatile metric can be 1 https://www.mturk.com/mturk/welcome readily adapted for measuring IAA for other linguistic annotation tasks, especially when done using crowdsourcing.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Sec 2 provides a brief overview of related work.", "labels": [], "entities": []}, {"text": "Sec 3 describes the experiment design and procedure.", "labels": [], "entities": []}, {"text": "In Sec 4, we introduce anew metric for IAA, that could be uniformly applied across flat and nested segmentations.", "labels": [], "entities": [{"text": "IAA", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.7784184813499451}]}, {"text": "Results of the annotation experiments are reported in Sec 5.", "labels": [], "entities": []}, {"text": "In Sec 6, we analyze the possible statistical and linguistic biases in annotation.", "labels": [], "entities": []}, {"text": "Sec 7 concludes the paper by summarizing the work and discussing future research directions.", "labels": [], "entities": [{"text": "summarizing", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9610957503318787}]}, {"text": "All the annotated datasets used in this research are freely available for non-commercial research purposes 2 .", "labels": [], "entities": []}], "datasetContent": [{"text": "The annotation experiments have been designed to systematically study the various aspects of query segmentation.", "labels": [], "entities": [{"text": "query segmentation", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.8044025003910065}]}, {"text": "In order to verify the effectiveness and reliability of crowdsourcing, we designed an AMT experiment for flat segmentation of Web search queries.", "labels": [], "entities": [{"text": "flat segmentation of Web search queries", "start_pos": 105, "end_pos": 144, "type": "TASK", "confidence": 0.7696122229099274}]}, {"text": "As a baseline, we would like to compare these annotations with those from human experts trained for the task.", "labels": [], "entities": []}, {"text": "We shall refer to this baseline as the Gold annotation set.", "labels": [], "entities": [{"text": "Gold annotation set", "start_pos": 39, "end_pos": 58, "type": "DATASET", "confidence": 0.9212584296862284}]}, {"text": "Since we believe that the issue of granularity could be the prime reason for previously reported low IAA for segmentation, we also designed AMT-based nested segmentation experiments for the same set of queries, and obtained the corresponding gold annotations.", "labels": [], "entities": [{"text": "IAA", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9952167272567749}, {"text": "segmentation", "start_pos": 109, "end_pos": 121, "type": "TASK", "confidence": 0.9794716835021973}, {"text": "AMT-based nested segmentation", "start_pos": 140, "end_pos": 169, "type": "TASK", "confidence": 0.7335657874743143}]}, {"text": "Finally, to estimate the role of ambiguity inherent in the structure of Web search queries on IAA, we conducted two more control experiments, both through crowdsourcing.", "labels": [], "entities": []}, {"text": "First, flat and nested segmentation of well-formed English, i.e., NL sentences of similar length distribution; and second, flat and nested segmentation of randomly generated queries.", "labels": [], "entities": []}, {"text": "Higher IAA for NL sentences would lead us to conclude that ambiguity and lack of structure in queries is the main reason for low agreements.", "labels": [], "entities": [{"text": "IAA", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.9989604949951172}]}, {"text": "On the other hand high or comparable IAA for random queries would mean that annotations have strong biases.", "labels": [], "entities": [{"text": "IAA", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9918203353881836}]}, {"text": "Thus, we have the following four pairs of annotation experiments: flat and nested segmentation of queries from crowdsourcing, corresponding flat and nested gold annotations, flat and nested segmentation of English sentences from crowdsourcing, and flat and nested segmentations for randomly generated queries through crowdsourcing.", "labels": [], "entities": []}, {"text": "For our experiments, we need a set of Web search queries and well-formed English sentences.", "labels": [], "entities": []}, {"text": "Furthermore, for generating the random queries, we will use search query logs to learn n-gram models.", "labels": [], "entities": []}, {"text": "In particular, we use the following datasets: Q500, QG500: Saha Roy et al. released a dataset of 500 queries, 5 to 8 words long, for evaluation of various segmentation algorithms.", "labels": [], "entities": [{"text": "Q500", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.8730898499488831}, {"text": "QG500", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.8578696250915527}]}, {"text": "This dataset has flat segmentations from three annotators obtained under controlled experimental settings, and can be considered as Gold annota- tions.", "labels": [], "entities": []}, {"text": "Hence, we select this set for our experiments as well.", "labels": [], "entities": []}, {"text": "We procured the corresponding nested segmentation for these queries from two human experts, who are regular search engine users, between 20 and 30 years old, and familiar with various linguistic annotation tasks.", "labels": [], "entities": []}, {"text": "They annotated the data under supervision.", "labels": [], "entities": []}, {"text": "They were trained and paid for the task.", "labels": [], "entities": []}, {"text": "We shall refer to the set of flat and nested gold annotations as QG500, whereas Q500 will be reserved for AMT experiments.", "labels": [], "entities": [{"text": "QG500", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.842799723148346}, {"text": "AMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.7504827380180359}]}, {"text": "Q700: Since 500 queries may not be enough for reliable conclusion and since the queries may not have been chosen specifically for the purpose of annotation experiments, we expanded the set with another 700 queries sampled from a slice of the query logs of Bing Australia 4 containing 16.7 million queries issued over a period of one month (May 2010).", "labels": [], "entities": [{"text": "Q700", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9441837668418884}, {"text": "query logs of Bing Australia 4", "start_pos": 242, "end_pos": 272, "type": "DATASET", "confidence": 0.6912161111831665}]}, {"text": "We picked, uniformly at random, queries that are 4 to 8 words long, have only English letters and numerals, and a high click entropy because \"a query with a larger click entropy value is more likely to bean informational or ambiguous query\" (.", "labels": [], "entities": []}, {"text": "Q500 consists of tailish queries with frequency between 5 and 15 that have at least one multiword named entity; but unlike the case of Q700, click-entropy was not considered during sampling.", "labels": [], "entities": []}, {"text": "As we shall see, this difference is clearly reflected in the results.", "labels": [], "entities": []}, {"text": "S300: We randomly selected 300 English sentences from a collection of full texts of public domain books 5 that were 5 to 15 words long, and checked them for well-formedness.", "labels": [], "entities": [{"text": "S300", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6502624154090881}]}, {"text": "This set will be referred to as S300.", "labels": [], "entities": []}, {"text": "QRand: Instead of generating search queries by throwing in words randomly, we thought it will be more interesting to explore annotation of  queries generated using n-gram models for n = 1, 2, 3.", "labels": [], "entities": [{"text": "QRand", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7408775091171265}]}, {"text": "We estimated the models from the Bing Australia log of 16.7 million queries.", "labels": [], "entities": [{"text": "Bing Australia log", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.9357258478800455}]}, {"text": "We generated 250 queries each of desired length distribution using the 1, 2 and 3-gram models.", "labels": [], "entities": []}, {"text": "We shall refer to these as U250, B250, T250 (for Uni, Bi and Trigram) respectively, and the whole dataset as QRand.", "labels": [], "entities": []}, {"text": "shows the query and sentence length distribution for the various sets.", "labels": [], "entities": []}, {"text": "We used AMT to get our annotations through crowdsourcing.", "labels": [], "entities": [{"text": "AMT", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.8625538349151611}]}, {"text": "Pilot experiments were carried out to test the instruction set and examples presented.", "labels": [], "entities": []}, {"text": "Based on the feedback, the precise instructions for the final experiments were designed.", "labels": [], "entities": []}, {"text": "Two separate AMT Human Intelligence Tasks (HITs) were designed for flat and nested query segmentation.", "labels": [], "entities": [{"text": "AMT Human Intelligence Tasks (HITs)", "start_pos": 13, "end_pos": 48, "type": "TASK", "confidence": 0.8605502077511379}, {"text": "flat and nested query segmentation", "start_pos": 67, "end_pos": 101, "type": "TASK", "confidence": 0.7042194485664368}]}, {"text": "Also, the experiments for queries (Q500+Q700) were conducted separately from S300 and QRand.", "labels": [], "entities": [{"text": "QRand", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.9243833422660828}]}, {"text": "Thus, we had six HITs in all.", "labels": [], "entities": []}, {"text": "The concept of flat and nested segmentation was introduced to the Turkers with the help of examples presented in two short videos . When in doubt regarding the meaning of a query, the Turkers were advised to issue the query on a search engine of their choice and find out its possible interpretation(s).", "labels": [], "entities": []}, {"text": "Note that we intentionally kept definitions of flat and nested segmentation fuzzy because (a) it would require very long instruction manuals to coverall possible cases and (b) Turkers do not tend to read verbose and complex instructions.", "labels": [], "entities": []}, {"text": "summarizes other specifics of HITs.", "labels": [], "entities": []}, {"text": "Honey pots or trap questions whose answers are known a priori are often included in a HIT to identify turkers who are unable to solve the task appropriately leading to incorrect annotations.", "labels": [], "entities": []}, {"text": "However, this trick cannot be employed in our case because there is no notion of an absolutely correct segmentation.", "labels": [], "entities": []}, {"text": "We observe that even with unambiguous queries, even expert annotators may dis-agree on some of the segment boundaries.", "labels": [], "entities": []}, {"text": "Hence, we decided to include annotations from all the turkers, except for those that were syntactically illformed (e.g., non-binary nested segmentation).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example of flat segmentation by Turkers.  f is the frequency of annotations; segment bound- aries are represented by |.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9361661672592163}]}, {"text": " Table 2: Example of nested segmentation by Turk- ers. f is the frequency of annotations.", "labels": [], "entities": [{"text": "Turk- ers", "start_pos": 44, "end_pos": 53, "type": "DATASET", "confidence": 0.8289935986200968}]}, {"text": " Table 4: Agreement Statistics: \u03b1(S).", "labels": [], "entities": [{"text": "Agreement Statistics", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.7847543954849243}, {"text": "\u03b1", "start_pos": 32, "end_pos": 33, "type": "METRIC", "confidence": 0.8686129450798035}]}, {"text": " Table 5: Average height for nested segmentation.", "labels": [], "entities": [{"text": "Average height", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9593162536621094}, {"text": "nested segmentation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.6218679398298264}]}, {"text": " Table 6: Percentages of positions of segment  boundaries with respect to prepositions. Prepo- sitions occurring in the beginning or end of a  query/sentence have been excluded from the anal- ysis; hence, numbers in a column do not total 100.", "labels": [], "entities": []}]}