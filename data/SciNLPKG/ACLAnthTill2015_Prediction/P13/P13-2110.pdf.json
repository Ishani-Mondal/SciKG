{"title": [{"text": "A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing", "labels": [], "entities": [{"text": "Joint Chinese Word Segmentation", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.508590504527092}, {"text": "POS Tagging", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.6992031186819077}, {"text": "Parsing", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.7542449831962585}]}], "abstractContent": [{"text": "For the cascaded task of Chinese word seg-mentation, POS tagging and parsing, the pipeline approach suffers from error propagation while the joint learning approach suffers from inefficient decoding due to the large combined search space.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.7923720479011536}, {"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.8265883326530457}]}, {"text": "In this paper, we present a novel lattice-based framework in which a Chinese sentence is first segmented into a word lattice, and then a lattice-based POS tagger and a lattice based parser are used to process the lattice from two different viewpoints: sequential POS tagging and hierarchical tree building.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 263, "end_pos": 274, "type": "TASK", "confidence": 0.7489209175109863}]}, {"text": "A strategy is designed to exploit the complementary strengths of the tagger and parser, and encourage them to predict agreed structures.", "labels": [], "entities": []}, {"text": "Experimental results on Chinese Treebank show that our lattice-based framework significantly improves the accuracy of the three sub-tasks.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.9836632311344147}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9989340901374817}]}], "introductionContent": [{"text": "Previous work on syntactic parsing generally assumes a processing pipeline where an input sentence is first tokenized, POS-tagged and then parsed.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.8187343776226044}]}, {"text": "This approach works well for languages like English where automatic tokenization and POS tagging can be performed with high accuracy without the guidance of the highlevel syntactic structure.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.787266880273819}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9933007955551147}]}, {"text": "Such an approach, however, is not optimal for languages like Chinese where there are no natural delimiters for word boundaries, and word segmentation (or tokenization) is a non-trivial research problem by itself.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.7269362807273865}]}, {"text": "Errors in word segmentation would propagate to later processing stages such as POS tagging and syntactic parsing.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7584001421928406}, {"text": "POS tagging", "start_pos": 79, "end_pos": 90, "type": "TASK", "confidence": 0.8351534903049469}, {"text": "syntactic parsing", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7410858571529388}]}, {"text": "More importantly, Chinese is a language that lacks the morphological clues that help determine the POS tag of a word.", "labels": [], "entities": [{"text": "POS tag", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.8582741022109985}]}, {"text": "For example, \u8c03\u67e5 ( \" investigate/investigation\") can either be a verb (\"investigate\") or a noun (\"investigation\"), and there is no morphological variation between its verbal form and nominal form.", "labels": [], "entities": []}, {"text": "This contributes to the relatively low accuracy (95% or below) in Chinese POS tagging when evaluated as a stand-alone task, and the noun/verb ambiguity is a major source of error.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9993997812271118}, {"text": "POS tagging", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.7871988415718079}]}, {"text": "More recently, joint inference approaches have been proposed to address the shortcomings of the pipeline approach.", "labels": [], "entities": []}, {"text": "proposed a joint inference approach where syntactic parsing can provide feedback to word segmentation and POS tagging and showed that the joint inference approach leads to improvements in all three sub-tasks.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7744852006435394}, {"text": "word segmentation", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.7369196861982346}, {"text": "POS tagging", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.778276264667511}]}, {"text": "However, a major challenge for joint inference approach is that the large combined search space makes efficient decoding and parameter estimation very hard.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 125, "end_pos": 145, "type": "TASK", "confidence": 0.6929598450660706}]}, {"text": "In this paper, we present a novel lattice-based framework for Chinese.", "labels": [], "entities": []}, {"text": "An input Chinese sentence is first segmented into a word lattice, which is a compact representation of a small set of high-quality word segmentations.", "labels": [], "entities": []}, {"text": "Then, a lattice-based POS tagger and a lattice-based parser are used to process the word lattice from two different viewpoints.", "labels": [], "entities": []}, {"text": "We next employ the dual decomposition method to exploit the complementary strengths of the tagger and parser, and encourage them to predict agreed structures.", "labels": [], "entities": []}, {"text": "Experimental results show that our lattice-based framework significantly improves the accuracies of the three sub-tasks 2 The Lattice-based Framework gives the organization of the framework.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9769729375839233}]}, {"text": "There are four types of linguistic structures: a Chinese sentence, the word lattice, tagged word sequence and parse tree of the Chinese sentence.", "labels": [], "entities": []}, {"text": "An example for each structure is provided in.", "labels": [], "entities": []}, {"text": "We can see that the terminals and preterminals of a parse tree constitute a tagged word sequence.", "labels": [], "entities": []}, {"text": "Therefore, we define a comparator between a tagged word sequence and a parse tree: if they contain the same word sequence and POS tags, they are equal, otherwise unequal.", "labels": [], "entities": []}, {"text": "also shows the workflow of the framework.", "labels": [], "entities": []}, {"text": "First, the Chinese sentence is segmented into a word lattice using the word segmentation system.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7259817719459534}]}, {"text": "Then the word lattice is fed into the lattice-based POS tagger to produce a tagged word sequence and into the latticebased parser to separately produce a parse tree . We then compare with to see whether they are equal.", "labels": [], "entities": []}, {"text": "If they are equal, we output as the final result.", "labels": [], "entities": []}, {"text": "Otherwise, the guidance generator generates some guidance orders based on the difference between and , and guides the tagger and the parser to process the lattice again.", "labels": [], "entities": []}, {"text": "This procedure may iterate many times until the tagger and parser predict equal structures.", "labels": [], "entities": []}, {"text": "The motivation to design such a framework is as follows.", "labels": [], "entities": []}, {"text": "First, state-of-the-art word segmentation systems can now perform with high accuracy.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7246118038892746}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9938852190971375}]}, {"text": "We can easily get an F1 score greater than 96%, and an oracle (upper bound) F1 score greater than 99% for the word lattice (.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9856248497962952}, {"text": "F1 score", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9834066927433014}]}, {"text": "Therefore, a word lattice provides us a good enough search space to allow sufficient interaction among word segmentation, POS tagging and parsing systems.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.7054697722196579}, {"text": "POS tagging", "start_pos": 122, "end_pos": 133, "type": "TASK", "confidence": 0.8284139931201935}]}, {"text": "Second, both the lattice-based POS tagger and the lattice-based parser can select word segmentation from the word lattice and predict POS tags, but they do so from two different perspectives.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7240123301744461}]}, {"text": "The lattice-based POS tagger looks at a path in a word lattice as a sequence and performs sequence labeling based on linear local context, while the lattice-based parser builds the parse trees in a hierarchical manner.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 18, "end_pos": 28, "type": "TASK", "confidence": 0.718052864074707}]}, {"text": "They have different strengths with regard to word segmentation and POS tagging.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7821279168128967}, {"text": "POS tagging", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.8797122240066528}]}, {"text": "We hypothesize that exploring the complementary strengths of the tagger and parser would improve each of the sub-tasks.", "labels": [], "entities": []}, {"text": "We build a character-based model) for the word segmentation system, and treat segmentation as a sequence labeling task, where each Chinese character is labeled with a tag.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7698328197002411}]}, {"text": "We use the tag set provided in and use the same feature templates.", "labels": [], "entities": []}, {"text": "We use the Maximum Entropy (ME) model to estimate the feature weights.", "labels": [], "entities": [{"text": "Maximum Entropy (ME)", "start_pos": 11, "end_pos": 31, "type": "METRIC", "confidence": 0.7946829080581665}]}, {"text": "To get a word lattice, we first generate N-best word segmentation results, and then compact the N-best lists into a word lattice by collapsing all the identical words into one edge.", "labels": [], "entities": [{"text": "N-best word segmentation", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.6278468867142996}]}, {"text": "We also assign a probability to each edge, which is calculated by multiplying the tagging probabilities of each character in the word.", "labels": [], "entities": []}, {"text": "The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : \u2022 ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.729078084230423}]}, {"text": "We use the same non-local feature templates used in and a similar decoding algorithm.", "labels": [], "entities": []}, {"text": "We use the perceptron algorithm) for parameter estimation.", "labels": [], "entities": []}, {"text": "proposed a lattice-based parser for Heberw based on the PCFG-LA model ().", "labels": [], "entities": [{"text": "Heberw", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.9291446805000305}, {"text": "PCFG-LA model", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9566932916641235}]}, {"text": "We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments.", "labels": [], "entities": []}, {"text": "Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability.", "labels": [], "entities": []}, {"text": "In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a parse tree is refined into [ ], where X is the latent annotation vector for all non-terminals in . The probability of [ ] is calculated as: where the three terms are products of all syntactic rule probabilities, lexical rule probabilities and word probabilities in [ ] respectively.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Lattice-based framework evaluation.", "labels": [], "entities": []}]}