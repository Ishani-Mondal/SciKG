{"title": [{"text": "A Statistical NLG Framework for Aggregated Planning and Realization", "labels": [], "entities": [{"text": "Aggregated Planning and Realization", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.5631411299109459}]}], "abstractContent": [{"text": "We present a hybrid natural language generation (NLG) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.8233478963375092}]}, {"text": "Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain.", "labels": [], "entities": []}, {"text": "First, we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis.", "labels": [], "entities": []}, {"text": "Each sentence is then organized into semantically similar groups (representing a domain specific concept) by k-means clustering.", "labels": [], "entities": []}, {"text": "After this semi-automatic processing (human review of cluster assignments), a number of corpus-level statistics are compiled and used as features by a ranking SVM to develop model weights from a training corpus.", "labels": [], "entities": []}, {"text": "At generation time, a set of input data, the collection of semantically organized templates, and the model weights are used to select optimal templates.", "labels": [], "entities": []}, {"text": "Our system is evaluated with automatic , non-expert crowdsourced and expert evaluation metrics.", "labels": [], "entities": []}, {"text": "We also introduce a novel automatic metric-syntactic variability that represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents.", "labels": [], "entities": []}, {"text": "The metrics for generated weather and biography texts fall within acceptable ranges.", "labels": [], "entities": []}, {"text": "In sum, we argue that our statistical approach to NLG reduces the need for complicated knowledge-based ar-chitectures and readily adapts to different domains with reduced development time.", "labels": [], "entities": []}, {"text": "* *Ravi Kondadadi is now affiliated with Nuance Communications , Inc.", "labels": [], "entities": []}], "introductionContent": [{"text": "NLG is the process of generating natural-sounding text from non-linguistic inputs.", "labels": [], "entities": []}, {"text": "A typical NLG system contains three main components: (1) Document (Macro) Planning -deciding what content should be realized in the output and how it should be structured; (2) Sentence (Micro) planninggenerating a detailed sentence specification and selecting appropriate referring expressions; and (3) Surface Realization -generating the final text after applying morphological modifications based on syntactic rules (see e.g.,,).", "labels": [], "entities": [{"text": "Surface Realization", "start_pos": 303, "end_pos": 322, "type": "TASK", "confidence": 0.7385909557342529}]}, {"text": "However, document planning is arguably one of the most crucial components of an NLG system and is responsible for making the texts express the desired communicative goal in a coherent structure.", "labels": [], "entities": [{"text": "document planning", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7194375395774841}]}, {"text": "If the document planning stage fails, the communicative goal of the generated text will not be met even if the other two stages are perfect.", "labels": [], "entities": []}, {"text": "While most traditional systems simplify development by using a pipelined approach where are executed in a sequence, this can result in errors atone stage propagating to successive stages (see e.g.,).", "labels": [], "entities": []}, {"text": "We propose a hybrid framework that combines by converting data to text in one single process.", "labels": [], "entities": []}, {"text": "Most NLG systems fall into two broad categories: knowledge-based and statistical.", "labels": [], "entities": []}, {"text": "Knowledge-based systems heavily depend on having domain expertise to come up with handcrafted rules at each stage of a pipeline.", "labels": [], "entities": []}, {"text": "Although knowledge-based systems can produce high quality text, they are (1) very expensive to build, involving a lot of discussion with the end users of the system for the document planning stage alone; (2) have limited linguistic coverage, as it is time consuming to capture linguistic variation; and (3) one has to start from scratch for each new domain because the developed components cannot be reused.", "labels": [], "entities": []}, {"text": "Statistical systems, on the other hand, are fairly inexpensive, more adaptable and rely on having historical data for the given domain.", "labels": [], "entities": []}, {"text": "Coverage is likely to be high if more historical data is available.", "labels": [], "entities": []}, {"text": "The main disadvantage with statistical systems is that they are more prone to errors and the output text may not be coherent as there are less constraints on the generated text.", "labels": [], "entities": []}, {"text": "Our framework is a hybrid of statistical and template-based systems.", "labels": [], "entities": []}, {"text": "Many knowledge-based systems use templates to generate text.", "labels": [], "entities": []}, {"text": "A template structure contains \"gaps\" that are filled to generate the output.", "labels": [], "entities": []}, {"text": "The idea is to create a lot of templates from the historical data and select the right template based on some constraints.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first hybrid statistical-template-based system that combines all three stages of NLG.", "labels": [], "entities": []}, {"text": "Experiments with different variants of our system (for biography and weather subject matter domains) demonstrate that our system generates reasonable texts.", "labels": [], "entities": []}, {"text": "Also, in addition to the standard metrics used to evaluate NLG systems (e.g., BLEU, NIST, etc.), we present a unique text evaluation metric called syntactic variability to measure the linguistic variation of generated texts.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9742360711097717}, {"text": "NIST", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.9301993250846863}]}, {"text": "This metric applies to the document collection level and is based on computing the number of unique template sequences among all the generated texts.", "labels": [], "entities": [{"text": "document collection", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.6642404645681381}]}, {"text": "A higher number indicates the texts are more variable and naturalsounding whereas a lower number shows they are more redundant.", "labels": [], "entities": []}, {"text": "We argue that this metric is useful for evaluating template-based systems and for any type of text generation for domains where linguistic variability is favored (e.g., the user is expected to go through more than one document in the same session).", "labels": [], "entities": [{"text": "text generation", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.755178302526474}]}, {"text": "The main contributions of this paper are (1) A statistical NLG system that combines document and sentence planning and surface realization into one single process; and (2) A new metric -syntactic variability -is proposed to measure the syntactic and morphological variability of the generated texts.", "labels": [], "entities": []}, {"text": "We believe this is the first work to propose an automatic metric to measure linguistic variability of generated texts in NLG.", "labels": [], "entities": []}, {"text": "Section 2 provides an overview of related work on NLG.", "labels": [], "entities": []}, {"text": "We present our main system in Section 3.", "labels": [], "entities": []}, {"text": "The system is evaluated and discussed in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 5 and point out future directions of research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first discuss the corpus data used to train and generate texts.", "labels": [], "entities": []}, {"text": "Then, the results of both automatic and human evaluations of our system's generations against the original and baseline texts are considered as a means of determining performance.", "labels": [], "entities": []}, {"text": "For all experiments reported in this section, the baseline system selects the most frequent conceptual unit at the given position, chooses the most likely template for the conceptual unit, and fills the template with input data.", "labels": [], "entities": []}, {"text": "The above process is repeated until the number of sentences is less than or equal to the average number of sentences for the given domain.", "labels": [], "entities": []}, {"text": "Two sets of crowdsourced human evaluation tasks (run on CrowdFlower) were constructed to compare against the automatic metrics: (1) an understandability evaluation of the entire text on a threepoint scale: Fluent = no grammatical or informative barriers; Understandable = some grammatical or informative barriers; Disfluent = significant grammatical or informative barriers; and (2) a sentence-level preference between sentence pairs (e.g., \"Do you prefer Sentence A (from DocOrig) or the corresponding Sentence B (from DocBase/DocSys)\").", "labels": [], "entities": [{"text": "Fluent", "start_pos": 206, "end_pos": 212, "type": "METRIC", "confidence": 0.9752065539360046}]}, {"text": "Over 100 native English speakers contributed, each one restricted to providing no more than 50 responses and only after they successfully answered 4 \"gold data\" questions correctly.", "labels": [], "entities": []}, {"text": "We also omitted those evaluators with a disproportionately high response rate.", "labels": [], "entities": []}, {"text": "No other data was collected on the contributors (although geographic data (country, region, city) and IP addresses were available).", "labels": [], "entities": []}, {"text": "For the sentence-level preference task, the pair orderings were randomized to prevent click bias.", "labels": [], "entities": []}, {"text": "For the text-understandability task, 40 documents were chosen at random from the DocOrig test set along with the corresponding 40 DocSys and DocBase generations (240 documents total/120 for each domain).", "labels": [], "entities": [{"text": "DocOrig test set", "start_pos": 81, "end_pos": 97, "type": "DATASET", "confidence": 0.9532720049222311}]}, {"text": "8 judgments per document were solicited from the crowd (1920 total judgments, 69.51 average agreement) and are summarized in (biography and weather respectively).", "labels": [], "entities": [{"text": "agreement", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.7847016453742981}]}, {"text": "If the system is performing well and the ranking model is actually contributing to increased performance, the accepted trend should be that the DocOrig texts are more fluent and preferred compared to both the DocSys and DocBase systems.", "labels": [], "entities": [{"text": "DocOrig texts", "start_pos": 144, "end_pos": 157, "type": "DATASET", "confidence": 0.9250991642475128}, {"text": "DocSys", "start_pos": 209, "end_pos": 215, "type": "DATASET", "confidence": 0.9382935762405396}]}, {"text": "However, the differences between DocOrig and DocSys will not be significant, the differences between DocOrig and DocBase and DocSys and DocBase will be significantly different.", "labels": [], "entities": [{"text": "DocOrig", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9448065757751465}, {"text": "DocSys", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.8294379711151123}, {"text": "DocOrig", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9535766243934631}, {"text": "DocBase", "start_pos": 113, "end_pos": 120, "type": "DATASET", "confidence": 0.8009545207023621}, {"text": "DocSys", "start_pos": 125, "end_pos": 131, "type": "DATASET", "confidence": 0.9394503831863403}, {"text": "DocBase", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.9455533623695374}]}, {"text": "Focusing on fluency ratings, it is expected that the DocOrig generations will have the highest fluency (as they are human generated).", "labels": [], "entities": [{"text": "DocOrig", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.911258339881897}]}, {"text": "Further, if the DocSys is performing well, it is expected that the fluency rating will be less than the DocOrig and higher than DocBase., which shows the biography text evaluations, demonstrates this acceptable distribution of performances.", "labels": [], "entities": [{"text": "DocSys", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.9598916172981262}, {"text": "DocOrig", "start_pos": 104, "end_pos": 111, "type": "DATASET", "confidence": 0.9554994702339172}, {"text": "DocBase.", "start_pos": 128, "end_pos": 136, "type": "DATASET", "confidence": 0.9711892008781433}]}, {"text": "For the weather discourses, as evident from, the acceptable trend holds between the DocSys and DocBase generations, and the DocSys generation fluency is actually slightly higher than DocOrig.", "labels": [], "entities": [{"text": "DocSys", "start_pos": 84, "end_pos": 90, "type": "DATASET", "confidence": 0.9605134725570679}, {"text": "DocBase generations", "start_pos": 95, "end_pos": 114, "type": "DATASET", "confidence": 0.8850500285625458}, {"text": "DocOrig", "start_pos": 183, "end_pos": 190, "type": "DATASET", "confidence": 0.9697848558425903}]}, {"text": "This is possibly because the DocOrig texts are from a particular subject matterweather forecasts for offshore oil rigs in the U.K. -which maybe difficult for people in general to understand.", "labels": [], "entities": [{"text": "DocOrig texts", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9444311261177063}]}, {"text": "Nonetheless, the demonstrated trend is favorable to our system.", "labels": [], "entities": []}, {"text": "In terms of significance, there are no statistically significant differences between the systems for weather (DocOrig vs. DocSys -\u03c7 2 =.347, d.f.=1, p=.555; DocOrig vs. DocBase -\u03c7 2 =.090, d.f.=1, p=.764; DocSys vs. DocBase -\u03c7 2 =.790, d.f.=1, p=.373).", "labels": [], "entities": []}, {"text": "While this is a good result for comparing DocOrig and DocSys generations, it is not for the other pairs.", "labels": [], "entities": [{"text": "DocOrig", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9359440207481384}, {"text": "DocSys generations", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.857636034488678}]}, {"text": "However, numerically, the trend is in the right direction despite not being able to demonstrate significance.", "labels": [], "entities": []}, {"text": "For biography, the trend fits nicely both numerically and in terms of statistical significance (DocOrig vs. DocSys -\u03c7 2 =5.094, d.f.=1, p=.024; DocOrig vs. DocBase -\u03c7 2 =35.171, d.f.=1, p<.0001; DocSys vs. DocBase -\u03c7 2 =14.000, d.f.=1, p<.0001).", "labels": [], "entities": []}, {"text": "For the sentence preference task, equivalent sentences across the 120 documents were chosen at random (80 sentences from biography and 74 sentences from weather).", "labels": [], "entities": []}, {"text": "8 judgments per comparison were solicited from the crowd (3758 total judgments, 75.87 average agreement) and are summarized in (biography and weather, respectively).", "labels": [], "entities": [{"text": "agreement", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.805458128452301}]}, {"text": "Similar to the text-understandability task, an acceptable performance pattern should include the DocOrig texts being preferred to both DocSys and DocBase generations and the DocSys generation preferred to the DocBase.", "labels": [], "entities": [{"text": "DocOrig texts", "start_pos": 97, "end_pos": 110, "type": "DATASET", "confidence": 0.9223086535930634}, {"text": "DocBase", "start_pos": 146, "end_pos": 153, "type": "DATASET", "confidence": 0.7013320922851562}, {"text": "DocBase", "start_pos": 209, "end_pos": 216, "type": "DATASET", "confidence": 0.977561891078949}]}, {"text": "The closer the DocSys generation is to the DocOrig, the better DocSys is performing.", "labels": [], "entities": [{"text": "DocOrig", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9731976985931396}, {"text": "DocSys", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.9101226925849915}]}, {"text": "The biography domain illus- trates this scenario () where the results are similar to the text-understandability experiments.", "labels": [], "entities": [{"text": "biography domain illus", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.9253924091657003}]}, {"text": "In contrast, for weather domain, sentences from DocBase system were preferred to our system's ().", "labels": [], "entities": [{"text": "DocBase system", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.9295068383216858}]}, {"text": "We looked at the cases where the preferences were in favor of DocBase.", "labels": [], "entities": [{"text": "DocBase", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9626244902610779}]}, {"text": "It appears that because of high syntactic variability, our system can produce quite complex sentences whereas the non-experts seem to prefer shorter and simpler sentences because of the complexity of the text.", "labels": [], "entities": []}, {"text": "In terms of significance, there are no statistically significant differences between the systems for weather (DocOrig vs. DocSys -\u03c7 2 =6.48, d.f.=1, p=.011; DocOrig vs. DocBase -\u03c7 2 =.720, d.f.=1, p=.396; DocSys vs. DocBase -\u03c7 2 =.720, d.f.=1, p=.396).", "labels": [], "entities": []}, {"text": "The trend is different compared to the fluency metric above in that the DocBase system is outperforming the DocOrig generations to an almost statistically significant difference -the remaining comparisons follow the trend.", "labels": [], "entities": [{"text": "DocBase", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9036079049110413}, {"text": "DocOrig generations", "start_pos": 108, "end_pos": 127, "type": "DATASET", "confidence": 0.9290541708469391}]}, {"text": "We believe that this is for similar reasons stated above -i.e., the generation maybe a more digestible version of a technical document.", "labels": [], "entities": []}, {"text": "More problematic is the results of the biography evaluations.", "labels": [], "entities": []}, {"text": "Here there is a statistically significant difference between the DocSys and DocOrig and no statistically significant difference between the DocSys and DocBase generations (DocOrig vs. DocSys -\u03c7 2 =76.880, d.f.=1, p<.0001; DocOrig vs. DocBase -\u03c7 2 =38.720, d.f.=1, p<.0001; DocSys vs. DocBase -\u03c7 2 =.720, d.f.=1, p=.396).", "labels": [], "entities": [{"text": "DocBase", "start_pos": 151, "end_pos": 158, "type": "DATASET", "confidence": 0.7761645317077637}]}, {"text": "Again, this distribution of preferences is numerically similar to the trend we would like to see, but the statistical significance indicates that there is some ground to makeup.", "labels": [], "entities": []}, {"text": "Expert evaluations are potentially informative for identifying specific shortcomings and how best to address them.", "labels": [], "entities": []}, {"text": "We performed expert evaluations for the biography domain only as we do not have access to weather experts.", "labels": [], "entities": []}, {"text": "The four biography reviewers are journalists who write short biographies for news archives.", "labels": [], "entities": []}, {"text": "For the biography domain, evaluations of the texts were largely similar to the evaluations of the non-expert crowd (76.22 average agreement for the sentence-preference task and 72.95 for the text-understandability task).", "labels": [], "entities": [{"text": "agreement", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9359565377235413}]}, {"text": "For example, the disfluent ratings were highest for the DocBase generations and lowest for the DocOrig generations.", "labels": [], "entities": [{"text": "DocBase", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.9682983756065369}, {"text": "DocOrig", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9428549408912659}]}, {"text": "Also, the fluent ratings were highest for the DocOrig generations, and while the combined fluent and understandable are higher for DocSys as compared to DocBase, the DocBase generations had a 10% higher fluent score (58.22%) as compared to the DocSys fluent score (47.97%).", "labels": [], "entities": [{"text": "DocOrig", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9233497977256775}, {"text": "DocSys", "start_pos": 131, "end_pos": 137, "type": "DATASET", "confidence": 0.9388530850410461}, {"text": "DocBase", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.9822008609771729}, {"text": "DocBase", "start_pos": 166, "end_pos": 173, "type": "DATASET", "confidence": 0.9523100852966309}, {"text": "DocSys fluent score", "start_pos": 244, "end_pos": 263, "type": "DATASET", "confidence": 0.774123469988505}]}, {"text": "Based on notes from the reviewers, the succinctness of the the DocBase generations are preferred in some ways as they are in keeping with certain editorial standards.", "labels": [], "entities": [{"text": "DocBase generations", "start_pos": 63, "end_pos": 82, "type": "DATASET", "confidence": 0.9718233346939087}]}, {"text": "This is further reflected in the sentence preferences being 70% in favor of the DocBase generations as compared to the DocSys generations (all other sentence comparisons were consistent with the non-expert crowd).", "labels": [], "entities": [{"text": "DocBase generations", "start_pos": 80, "end_pos": 99, "type": "DATASET", "confidence": 0.9540898203849792}, {"text": "DocSys generations", "start_pos": 119, "end_pos": 137, "type": "DATASET", "confidence": 0.9135929644107819}]}, {"text": "These expert evaluations provide much needed clarity to the NLG process.", "labels": [], "entities": [{"text": "NLG process", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.8519652485847473}]}, {"text": "Overall, our system is generating clearly acceptable texts.", "labels": [], "entities": []}, {"text": "Further, there are enough parameters inherent in the system to tune to different domain expectations.", "labels": [], "entities": []}, {"text": "This is an encouraging result considering that no experts were involved in the development of the systema key contrast to many other existing (especially rule-based) NLG systems.", "labels": [], "entities": []}], "tableCaptions": []}