{"title": [{"text": "Cross-lingual Projections between Languages from Different Families", "labels": [], "entities": [{"text": "Cross-lingual Projections between Languages from Different Families", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.8621490938322884}]}], "abstractContent": [{"text": "Cross-lingual projection methods can benefit from resource-rich languages to improve performances of NLP tasks in resources-scarce languages.", "labels": [], "entities": [{"text": "Cross-lingual projection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7327506840229034}]}, {"text": "However, these methods confronted the difficulty of syntactic differences between languages especially when the pair of languages varies greatly.", "labels": [], "entities": []}, {"text": "To make the projection method well-generalize to diverse languages pairs, we enhance the projection method based on word alignments by introducing target-language word representations as features and proposing a novel noise removing method based on these word representations.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 116, "end_pos": 131, "type": "TASK", "confidence": 0.6882630884647369}]}, {"text": "Experiments showed that our methods improve the performances greatly on projections between English and Chinese.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most NLP studies focused on limited languages with large sets of annotated data.", "labels": [], "entities": []}, {"text": "English and Chinese are examples of these resource-rich languages.", "labels": [], "entities": []}, {"text": "Unfortunately, it is impossible to build sufficient labeled data for all tasks in all languages.", "labels": [], "entities": []}, {"text": "To address NLP tasks in resource-scarce languages, cross-lingual projection methods were proposed, which make use of existing resources in resource-rich language (also called source language) to help NLP tasks in resource-scarce language (also named as target language).", "labels": [], "entities": []}, {"text": "There are several types of projection methods.", "labels": [], "entities": []}, {"text": "One intuitive and effective method is to build a common feature space for all languages, so that the model trained on one language could be directly used on other languages).", "labels": [], "entities": []}, {"text": "We call it direct projection, which becomes very popular recently.", "labels": [], "entities": [{"text": "direct projection", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7482966780662537}]}, {"text": "The main limitation of these methods is that target language has to be similar to source language.", "labels": [], "entities": []}, {"text": "Otherwise the performance will degrade especially when the orders of phrases between source and target languages differ a lot.", "labels": [], "entities": []}, {"text": "Another common type of projection methods map labels from resource-rich language sentences to resource-scarce ones in a parallel corpus using word alignment information ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.7132092416286469}]}, {"text": "We refer them as projection based on word alignments in this paper.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.6976142078638077}]}, {"text": "Compared to other types of projection methods, this type of methods is more robust to syntactic differences between languages since it trained models on the target side thus following the topology of the target language.", "labels": [], "entities": []}, {"text": "This paper aims to build an accurate projection method with strong generality to various pairs of languages, even when the languages are from different families and are typologically divergent.", "labels": [], "entities": []}, {"text": "As far as we know, only a few works focused on this topic.", "labels": [], "entities": []}, {"text": "We adopted the projection method based on word alignments since it is less affected by language differences.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7080772519111633}]}, {"text": "However, such methods also have some disadvantages.", "labels": [], "entities": []}, {"text": "Firstly, the models trained on projected data could only cover words and cases appeared in the target side of parallel corpus, making it difficult to generalize to test data in broader domains.", "labels": [], "entities": []}, {"text": "Secondly, the performances of these methods are limited by the accuracy of word alignments, especially when words between two languages are not one-one aligned.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9974281191825867}, {"text": "word alignments", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.6897785663604736}]}, {"text": "So the obtained labeled data contains a lot of noises, making the models built on them less accurate.", "labels": [], "entities": []}, {"text": "This paper aims to build an accurate projection method with strong generality to various pairs of languages.", "labels": [], "entities": []}, {"text": "We built the method on top of projection method based on word alignments because of its advantage of being less affected by syntactic differences, and proposed two solutions to solve the above two difficulties of this type of methods.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.7075386196374893}]}, {"text": "Firstly, we introduce Brown clusters of target language to make the projection models cover broader cases.", "labels": [], "entities": []}, {"text": "Brown clustering is a kind of word representations, which assigns word with similar functions to the same cluster.", "labels": [], "entities": []}, {"text": "They can be efficiently learned on large-scale unlabeled data in target language, which is much easier to acquire even when the scales of parallel corpora of minor languages are limited.", "labels": [], "entities": []}, {"text": "Brown clusters have been first introduced to the field of cross-lingual projections in) and have achieved great improvements on projection between European languages.", "labels": [], "entities": [{"text": "projection between European languages", "start_pos": 128, "end_pos": 165, "type": "TASK", "confidence": 0.8220347762107849}]}, {"text": "However, their work was based on the direct projection methods so that it do notwork very well between languages from different families as will be shown in Section 3.", "labels": [], "entities": []}, {"text": "Secondly, to reduce the noises in projection, we propose a noise removing method to detect and correct noisy projected labels.", "labels": [], "entities": [{"text": "noise removing", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.719953864812851}]}, {"text": "The method was also built on Brown clusters, based on the assumption that instances with similar representations of Brown clusters tend to have similar labels.", "labels": [], "entities": []}, {"text": "As far as we know, no one has done any research on removing noises based on the space of word representations in the field of NLP.", "labels": [], "entities": []}, {"text": "Using above techniques, we achieved a projection method that adapts well on different language pairs even when the two languages differ enormously.", "labels": [], "entities": []}, {"text": "Experiments of NER and POS tagging projection from English to Chinese proved the effectiveness of our methods.", "labels": [], "entities": [{"text": "NER", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8846806883811951}, {"text": "POS tagging projection", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.7700762748718262}]}, {"text": "In the rest of our paper, Section 2 describes the proposed cross-lingual projection method.", "labels": [], "entities": [{"text": "cross-lingual projection", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.7734710276126862}]}, {"text": "Evaluations are in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 gives concluding remarks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performances of NER projection.", "labels": [], "entities": [{"text": "NER projection", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.9620538353919983}]}, {"text": " Table 3: Performances of noise removing methods", "labels": [], "entities": [{"text": "noise removing", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7322079092264175}]}, {"text": " Table 4: Performances of POS tagging projection.", "labels": [], "entities": [{"text": "POS tagging projection", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.8725354472796122}]}]}