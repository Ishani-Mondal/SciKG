{"title": [], "abstractContent": [{"text": "We propose anew optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011).", "labels": [], "entities": [{"text": "summarization", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.9920079708099365}]}, {"text": "In our framework the sum-marization desideratum is expressed as a sum of a submodular function and a non-submodular function, which we call dispersion ; the latter uses inter-sentence dis-similarities in different ways in order to ensure non-redundancy of the summary.", "labels": [], "entities": []}, {"text": "We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases.", "labels": [], "entities": []}, {"text": "We conduct experiments on two corpora-DUC 2004 and user comments on news articles-and show that the performance of our algorithm outperforms those that rely only on submodularity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Summarization is a classic text processing problem.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9841916561126709}, {"text": "text processing", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7062758058309555}]}, {"text": "Broadly speaking, given one or more documents, the goal is to obtain a concise piece of text that contains the most salient points in the given document(s).", "labels": [], "entities": []}, {"text": "Thanks to the omnipresent information overload facing all of us, the importance of summarization is gaining; semiautomatically summarized content is increasingly becoming user-facing: many newspapers equip editors with automated tools to aid them in choosing a subset of user comments to show.", "labels": [], "entities": [{"text": "summarization", "start_pos": 83, "end_pos": 96, "type": "TASK", "confidence": 0.9792342185974121}]}, {"text": "Summarization has been studied for the past in various settings-a large single document, multiple documents on the same topic, and user-generated content.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9803460240364075}]}, {"text": "Each domain throws up its own set of idiosyncrasies and challenges for the summarization task.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9151158332824707}]}, {"text": "On one hand, in the multi-document case (say, different news reports on the same event), the text is often very long and detailed.", "labels": [], "entities": []}, {"text": "The precision/recall requirements are higher in this domain and a semantic representation of the text might be needed to avoid redundancy.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994074106216431}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9809679388999939}]}, {"text": "On the other hand, in the case of user-generated content (say, comments on a news article), even though the text is short, one is faced with a different set of problems: volume (popular articles generate more than 10,000 comments), noise (most comments are vacuous, linguistically deficient, and tangential to the article), and redundancy (similar views are expressed by multiple commenters).", "labels": [], "entities": []}, {"text": "In both cases, there is a delicate balance between choosing the salient, relevant, popular, and diverse points (e.g., sentences) versus minimizing syntactic and semantic redundancy.", "labels": [], "entities": []}, {"text": "While there have been many approaches to automatic summarization (see Section 2), our work is directly inspired by the recent elegant framework of (Lin and Bilmes, 2011).", "labels": [], "entities": [{"text": "summarization", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.8290697336196899}]}, {"text": "They employed the powerful theory of submodular functions for summarization: submodularity embodies the \"diminishing returns\" property and hence is a natural vocabulary to express the summarization desiderata.", "labels": [], "entities": [{"text": "summarization", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.9921455979347229}, {"text": "summarization desiderata", "start_pos": 184, "end_pos": 208, "type": "TASK", "confidence": 0.909307062625885}]}, {"text": "In this framework, each of the constraints (relevance, redundancy, etc.) is captured as a submodular function and the objective is to maximize their sum.", "labels": [], "entities": []}, {"text": "A simple greedy algorithm is guaranteed to produce an approximately optimal summary.", "labels": [], "entities": []}, {"text": "They used this framework to obtain the best results on the DUC 2004 corpus.", "labels": [], "entities": [{"text": "DUC 2004 corpus", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.9877202113469442}]}, {"text": "Even though the submodularity framework is quite general, it has limitations in its expressivity.", "labels": [], "entities": []}, {"text": "In particular, it cannot capture redundancy constraints that depend on pairwise dissimilarities between sentences.", "labels": [], "entities": []}, {"text": "For example, a natural constraint on the summary is that the sum or the minimum of pairwise dissimilarities between sentences chosen in the summary should be maximized; this, unfortunately, is not a submodular function.", "labels": [], "entities": []}, {"text": "We call functions that depend on inter-sentence pair-wise dissimilarities in the summary as dispersion functions.", "labels": [], "entities": []}, {"text": "Our focus in this work is on significantly furthering the submodularity-based summarization framework to incorporate such dispersion functions.", "labels": [], "entities": []}, {"text": "We propose a very general graph-based summarization framework that combines a submodular function with a non-submodular dispersion function.", "labels": [], "entities": [{"text": "summarization", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.8686850070953369}]}, {"text": "We consider three natural dispersion functions on the sentences in a summary: sum of all-pair sentence dissimilarities, the weight of the minimum spanning tree on the sentences, and the minimum of all-pair sentence dissimilarities.", "labels": [], "entities": []}, {"text": "These three functions represent three different ways of using the sentence dissimilarities.", "labels": [], "entities": []}, {"text": "We then show that a greedy algorithm can obtain approximately optimal summary in each of the three cases; the proof exploits some nice combinatorial properties satisfied by the three dispersion functions.", "labels": [], "entities": []}, {"text": "We then conduct experiments on two corpora: the DUC 2004 corpus and a corpus of user comments on news articles.", "labels": [], "entities": [{"text": "DUC 2004 corpus", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9772482713063558}]}, {"text": "On DUC 2004, we obtain performance that matches (, without any serious parameter tuning; note that their framework does not have the dispersion function.", "labels": [], "entities": [{"text": "DUC 2004", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.9714773297309875}]}, {"text": "On the comment corpus, we outperform their method, demonstrating that value of dispersion functions.", "labels": [], "entities": []}, {"text": "As part of our methodology, we also use anew structured representation for summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 75, "end_pos": 84, "type": "TASK", "confidence": 0.9753947257995605}]}], "datasetContent": [{"text": "For each summarization task, we compare the system output (i.e., summaries automatically produced by the algorithm) against the humangenerated summaries and evaluate the performance in terms of ROUGE score), a standard recall-based evaluation measure used in summarization.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.9065195620059967}, {"text": "ROUGE score", "start_pos": 194, "end_pos": 205, "type": "METRIC", "confidence": 0.982258528470993}, {"text": "recall-based", "start_pos": 219, "end_pos": 231, "type": "METRIC", "confidence": 0.9837090969085693}, {"text": "summarization", "start_pos": 259, "end_pos": 272, "type": "TASK", "confidence": 0.9882166981697083}]}, {"text": "A system that produces higher ROUGE scores generates better quality summary and vice versa.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 30, "end_pos": 42, "type": "METRIC", "confidence": 0.9717580378055573}]}, {"text": "We use the following evaluation settings in our experiments for each summarization task: (1) For multi-document summarization, we compute the ROUGE-1 5 scores that was the main evaluation criterion for DUC 2004 evaluations.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.9081899225711823}, {"text": "ROUGE-1 5 scores", "start_pos": 142, "end_pos": 158, "type": "METRIC", "confidence": 0.9744464953740438}, {"text": "DUC 2004 evaluations", "start_pos": 202, "end_pos": 222, "type": "TASK", "confidence": 0.7039889494578043}]}, {"text": "(2) For comment summarization, the collection of user comments associated with a given article is typically much larger.", "labels": [], "entities": [{"text": "comment summarization", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.6587038040161133}]}, {"text": "Additionally, individual comments are noisy, wordy, diverse, and informally written.", "labels": [], "entities": []}, {"text": "Hence for this task, we use a slightly different evaluation criterion that is inspired from the DUC 2005-2007 summarization evaluation tasks.", "labels": [], "entities": [{"text": "DUC 2005-2007 summarization evaluation tasks", "start_pos": 96, "end_pos": 140, "type": "DATASET", "confidence": 0.8159681916236877}]}, {"text": "We represent the content within each comment c (i.e., all sentences S(c) comprising the comment) as a single node in the graph.", "labels": [], "entities": []}, {"text": "We then run our summarization algorithm on the instantiated graph to produce a summary for each news article.", "labels": [], "entities": []}, {"text": "In addition, each news article and corresponding set of comments were presented to three human annotators.", "labels": [], "entities": []}, {"text": "They were asked to select a subset of comments (at most 20 comments) that best represented a summary capturing the most popular as well as diverse set of views and opinions expressed by different users that are relevant to the given news article.", "labels": [], "entities": []}, {"text": "We then compare the automatically generated comment summaries against the human-generated summaries and compute the ROUGE-1 and ROUGE-2 scores.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.9831410646438599}, {"text": "ROUGE-2", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.894672691822052}]}, {"text": "This summarization task is particularly hard for even human annotators since user-generated comments are typically noisy and there are several hundreds of comments per article.", "labels": [], "entities": [{"text": "summarization", "start_pos": 5, "end_pos": 18, "type": "TASK", "confidence": 0.98336261510849}]}, {"text": "Similar to existing work in the literature (, we computed inter-annotator agreement for the humans by comparing their summaries against each other on a small held-out set of articles.", "labels": [], "entities": []}, {"text": "The average ROUGE-1 F-scores observed for humans was much higher (59.7) than that of automatic systems measured against the human-generated summaries (our best system achieved a score of 28.9 ROUGE-1 on the same dataset).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9906893968582153}, {"text": "F-scores", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.6442623138427734}, {"text": "ROUGE-1", "start_pos": 192, "end_pos": 199, "type": "METRIC", "confidence": 0.9706804752349854}]}, {"text": "This shows that even though this is anew type of summarization task, humans tend to generate more consistent summaries and hence their annotations are reliable for evaluation purposes as in multi-document summarization.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.9119482040405273}]}], "tableCaptions": [{"text": " Table 1: Performance on DUC 2004.", "labels": [], "entities": [{"text": "DUC 2004", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8743634223937988}]}, {"text": " Table 4. We observe a sim- ilar pattern as with multi-document summariza- tion. The full system using all components out- perform all other parameter settings, achieving the  best ROUGE-1 and ROUGE-2 scores. The table  also shows that incorporating dispersion into the  objective function yields an improvement in sum- marization quality (row 4 versus row 5).", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 181, "end_pos": 188, "type": "METRIC", "confidence": 0.9845852255821228}, {"text": "ROUGE-2", "start_pos": 193, "end_pos": 200, "type": "METRIC", "confidence": 0.8972267508506775}]}, {"text": " Table 4: Performance with different parameters  (comments).", "labels": [], "entities": []}]}