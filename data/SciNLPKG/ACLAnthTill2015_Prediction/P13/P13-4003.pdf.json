{"title": [], "abstractContent": [{"text": "Given the increasing interest and development of computational and quantitative methods in historical linguistics, it is important that scholars have a basis for documenting , testing, evaluating, and sharing complex workflows.", "labels": [], "entities": []}, {"text": "We present a novel open-source toolkit for quantitative tasks in historical linguistics that offers these features.", "labels": [], "entities": []}, {"text": "This toolkit also serves as an interface between existing software packages and frequently used data formats , and it provides implementations of new and existing algorithms within a homogeneous framework.", "labels": [], "entities": []}, {"text": "We illustrate the toolkit's functionality with an exemplary workflow that starts with raw language data and ends with automatically calculated phonetic alignments, cognates and borrowings.", "labels": [], "entities": []}, {"text": "We then illustrate evaluation metrics on gold standard datasets that are provided with the toolkit.", "labels": [], "entities": [{"text": "gold standard datasets", "start_pos": 41, "end_pos": 63, "type": "DATASET", "confidence": 0.6605806946754456}]}], "introductionContent": [{"text": "Since the turn of the 21 st century, there has been an increasing amount of research that applies computational and quantitative approaches to historicalcomparative linguistic processes.", "labels": [], "entities": []}, {"text": "Among these are: phonetic alignment algorithms, statistical tests for genealogical relatedness), methods for phylogenetic reconstruction), and automatic detection of cognates (), borrowings, and proto-forms.", "labels": [], "entities": [{"text": "phonetic alignment", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8166168928146362}, {"text": "phylogenetic reconstruction", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.8243908882141113}]}, {"text": "In contrast to traditional approaches to language comparison, quantitative methods are often emphasized as advantageous with regard to objectivity, transparency and replicability of results.", "labels": [], "entities": [{"text": "language comparison", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7026141285896301}]}, {"text": "It is striking then that given the multitude of new approaches, very few are publicly available as executable code.", "labels": [], "entities": []}, {"text": "Thus in order to replicate a study, researchers have to rebuild workflows from published descriptions and reimplement their approaches and algorithms.", "labels": [], "entities": []}, {"text": "These challenges make the replication of results difficult, or even impossible, and they hinder not only the evaluation and comparison of existing algorithms, but also the development of new approaches that build on them.", "labels": [], "entities": []}, {"text": "Another problem is that quantitative approaches that have been released as software are largely incompatible with each other and they show great differences in regard to their input and out formats, application range and flexibility.", "labels": [], "entities": []}, {"text": "1 Given the breadth of research questions involved in determining language relatedness, this is not surprising.", "labels": [], "entities": [{"text": "determining language relatedness", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.6617919305960337}]}, {"text": "Furthermore, the linguistic datasets upon which many analyses and tools are based are only -if at all -available in disparate formats that need manual or semi-automatic re-editing before they can be used as input elsewhere.", "labels": [], "entities": []}, {"text": "Scholars who want to analyze a dataset with different approaches often have to (time-consumingly) convert it into various input formats and they have to familiarize themselves with many different kinds of software.", "labels": [], "entities": []}, {"text": "As a result, errors may occur during data conversion processes and the output from different tools must also be converted into a comparable format.", "labels": [], "entities": [{"text": "data conversion", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.7095854878425598}]}, {"text": "For the comparison of different output formats or 1 There is the STARLING database program for lexicostatistical and glottochronological analyses).", "labels": [], "entities": [{"text": "STARLING", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.7290676832199097}]}, {"text": "The Rug/L04 software aligns sound sequences and calculates phonetic distances using the Levensthein distance.", "labels": [], "entities": []}, {"text": "The ASJP-Software also computes the Levenshtein distance), but its results are based on previously executed phonetic analyses.", "labels": [], "entities": [{"text": "ASJP-Software", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8691496253013611}]}, {"text": "The ALINE software carries out pairwise alignment analyses).", "labels": [], "entities": []}, {"text": "There are also software packages from evolutionary biology, which are adapted for linguistic purposes, such as MrBayes (, PHYLIP, and SplitsTree for the evaluation of competing quantitative approaches, gold standard datasets are desirable.", "labels": [], "entities": [{"text": "MrBayes", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.6870080232620239}, {"text": "PHYLIP", "start_pos": 122, "end_pos": 128, "type": "DATASET", "confidence": 0.5189706683158875}]}, {"text": "Towards a solution to these problems, we have developed a toolkit that (a) serves as an interface between existing software packages and data formats frequently used in quantitative approaches, (b) provides high-quality implementations of new and existing approaches within a homogeneous framework, and (c) offers a solid basis for testing, documenting, evaluating, and sharing complex workflows in quantitative historical linguistics.", "labels": [], "entities": []}, {"text": "We call this open source toolkit LingPy.", "labels": [], "entities": [{"text": "LingPy", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.9322559833526611}]}], "datasetContent": [{"text": "In order to improve the performance of quantitative approaches, it is of crucial importance to test and evaluate them.", "labels": [], "entities": []}, {"text": "Evaluation is usually done by comparing how well a given approach performs on a reference dataset, i.e. a gold standard, where the results of the analysis are known in advance.", "labels": [], "entities": []}, {"text": "LingPy comes with a module for the evaluation of In, the performance of the four abovementioned approaches to automatic cognate detection are compared with the gold standard cognate judgments of a dataset covering 207 concepts translated into 20 Indo-European languages taken from the Indo-European Lexical Cognacy (IELex) database.", "labels": [], "entities": [{"text": "LingPy", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9379600286483765}, {"text": "automatic cognate detection", "start_pos": 110, "end_pos": 137, "type": "TASK", "confidence": 0.7317847808202108}]}, {"text": "The pair scores, implemented in LingPy after the description in, were used as an evaluation measure.", "labels": [], "entities": [{"text": "LingPy", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.9488561153411865}]}, {"text": "For all approaches we chose the respective thresholds that tend to yield the best results on all of the gold standards.", "labels": [], "entities": []}, {"text": "As shown in both the SCA and LexStat methods show a higher accuracy than the Turchin and NED methods, with LexStat slightly outperforming SCA.", "labels": [], "entities": [{"text": "LexStat", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.8541901707649231}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9992843270301819}, {"text": "Turchin", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.8780807852745056}, {"text": "LexStat", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.7803424596786499}]}, {"text": "However, the generally bad performance Gold standard here means that the cognate judgments were carried out manually by the compilers of the IELex database. of all approaches on this dataset shows that there is a clear need for improving automatic cognate detection approaches, especially in cases of remote relationship, such as Indo-European.", "labels": [], "entities": [{"text": "IELex database.", "start_pos": 141, "end_pos": 156, "type": "DATASET", "confidence": 0.9034754037857056}]}], "tableCaptions": [{"text": " Table 4: Cognate Detection in LingPy", "labels": [], "entities": [{"text": "Cognate Detection", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7473325431346893}, {"text": "LingPy", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.7978518605232239}]}]}