{"title": [{"text": "Subtree Extractive Summarization via Submodular Maximization", "labels": [], "entities": [{"text": "Subtree Extractive Summarization", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7942370971043905}, {"text": "Submodular Maximization", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.754017174243927}]}], "abstractContent": [{"text": "This study proposes a text summarization model that simultaneously performs sentence extraction and compression.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7697857320308685}, {"text": "sentence extraction", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7605305314064026}]}, {"text": "We translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.6558563411235809}]}, {"text": "We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary.", "labels": [], "entities": []}, {"text": "In order to handle the subtree extraction problem, we investigate anew class of submodular maximization problem, and anew algorithm that has the approximation ratio 1 2 (1 \u2212 e \u22121).", "labels": [], "entities": [{"text": "subtree extraction", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.81598961353302}]}, {"text": "Our experiments with the NTCIR ACLIA test collections show that our approach outper-forms a state-of-the-art algorithm.", "labels": [], "entities": [{"text": "NTCIR ACLIA test collections", "start_pos": 25, "end_pos": 53, "type": "DATASET", "confidence": 0.9283766746520996}]}], "introductionContent": [{"text": "Text summarization is often addressed as a task of simultaneously performing sentence extraction and sentence compression.", "labels": [], "entities": [{"text": "Text summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7456245422363281}, {"text": "sentence extraction", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7272166013717651}, {"text": "sentence compression", "start_pos": 101, "end_pos": 121, "type": "TASK", "confidence": 0.7261525243520737}]}, {"text": "Joint models of sentence extraction and compression have a great benefit in that they have a large degree of freedom as far as controlling redundancy goes.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7795748710632324}]}, {"text": "In contrast, conventional two-stage approaches), which first generate candidate compressed sentences and then use them to generate a summary, have less computational complexity than joint models.", "labels": [], "entities": []}, {"text": "However, two-stage approaches are suboptimal for text summarization.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7595394253730774}]}, {"text": "For example, when we compress sentences first, the compressed sentences may fail to contain important pieces of information due to the length limit imposed on each sentence.", "labels": [], "entities": []}, {"text": "On the other hand, when we extract sentences first, an important sentence may fail to be selected, simply because it is long.", "labels": [], "entities": []}, {"text": "Enumerating a huge number of compressed sentences is also infeasible.", "labels": [], "entities": []}, {"text": "Joint models can prune unimportant or redundant descriptions without resorting to enumeration.", "labels": [], "entities": []}, {"text": "Meanwhile, submodular maximization has recently been applied to the text summarization task, and the methods thereof have performed very well (.", "labels": [], "entities": [{"text": "text summarization task", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.8506491780281067}]}, {"text": "Formalizing summarization as a submodular maximization problem has an important benefit inthat the problem can be solved by using a greedy algorithm with a performance guarantee.", "labels": [], "entities": [{"text": "summarization", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.9309202432632446}]}, {"text": "We therefore decided to formalize the task of simultaneously performing sentence extraction and compression as a submodular maximization problem.", "labels": [], "entities": [{"text": "sentence extraction and compression", "start_pos": 72, "end_pos": 107, "type": "TASK", "confidence": 0.7531044855713844}]}, {"text": "That is, we extract subsentences for making the summary directly from all available subsentences in the documents and not in a stepwise fashion.", "labels": [], "entities": []}, {"text": "However, there is a difficulty with such a formalization.", "labels": [], "entities": []}, {"text": "In the past, the resulting maximization problem has been often accompanied by thousands of linear constraints representing logical relations between words.", "labels": [], "entities": []}, {"text": "The existing greedy algorithm for solving submodular maximization problems cannot work in the presence of such numerous constraints although monotone and nonmonotone submodular maximization with constraints other than budget constraints have been studied (.", "labels": [], "entities": []}, {"text": "In this study, we avoid this difficulty by reducing the task to one of extracting dependency subtrees from sentences in the source documents.", "labels": [], "entities": []}, {"text": "The reduction replaces the difficulty of numerous linear constraints with another difficulty wherein two subtrees can share the same word to-ken when they are selected from the same sentence, and as a result, the cost of the union of the two subtrees is not always the mere sum of their costs.", "labels": [], "entities": []}, {"text": "We can overcome this difficulty by tackling anew class of submodular maximization problem: a budgeted monotone nondecreasing submodular function maximization with a cost function, where the cost of an extraction unit varies depending on what other extraction units are selected.", "labels": [], "entities": []}, {"text": "By formalizing the subtree extraction problem as this new maximization problem, we can treat the constraints regarding the grammaticality of the compressed sentences in a straightforward way and use an arbitrary monotone submodular word score function for words including our word score function (shown later).", "labels": [], "entities": [{"text": "subtree extraction", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7796949446201324}]}, {"text": "We also propose anew greedy algorithm that solves this new class of maximization problem with a performance guarantee 1 2 (1 \u2212 e \u22121 ).", "labels": [], "entities": []}, {"text": "We evaluated our method on by using it to perform query-oriented summarization (.", "labels": [], "entities": []}, {"text": "Experimental results show that it is superior to state-of-the-art methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on Japanese QA test collections from NTCIR-7 ACLIA1 and NTCIR-8 ACLIA2 ().", "labels": [], "entities": [{"text": "Japanese QA test collections", "start_pos": 26, "end_pos": 54, "type": "DATASET", "confidence": 0.7629391998052597}, {"text": "NTCIR-7 ACLIA1", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.8627862632274628}, {"text": "NTCIR-8 ACLIA2", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.88480544090271}]}, {"text": "The collections contain questions and weighted answer nuggets.", "labels": [], "entities": []}, {"text": "Our experimental settings followed the settings of (Morita et al., 2011), except for the maximum summary length.", "labels": [], "entities": []}, {"text": "We generated summaries consisting of 140 Japanese characters or less, with the question as the query terms.", "labels": [], "entities": []}, {"text": "We did this because our aim is to use our method in mobile situations.", "labels": [], "entities": []}, {"text": "We used \"ACLIA1 test data\" to tune the parameters, and evaluated our method on \"ACLIA2 test\" data.", "labels": [], "entities": [{"text": "ACLIA1 test data", "start_pos": 9, "end_pos": 25, "type": "DATASET", "confidence": 0.9139376878738403}, {"text": "ACLIA2 test\" data", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.9091552942991257}]}, {"text": "Since KNP internally has a flag that indicates either an \"obligatory case\" or an \"adjacent case\", we regarded dependency relations flagged by KNP as obligatory in the sentence compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 167, "end_pos": 187, "type": "TASK", "confidence": 0.7143625468015671}]}, {"text": "KNP utilizes Kyoto University's case frames () as the resource for detecting obligatory or adjacent cases.", "labels": [], "entities": [{"text": "KNP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7464263439178467}, {"text": "Kyoto University's case frames", "start_pos": 13, "end_pos": 43, "type": "DATASET", "confidence": 0.9386063575744629}]}, {"text": "To evaluate the summaries, we followed the practices of the TAC summarization tasks and NTCIR ACLIA tasks, and computed pyramid-based precision with the allowance parameter, recall, and F \u03b2 (where \u03b2 is 1 or 3) scores.", "labels": [], "entities": [{"text": "summaries", "start_pos": 16, "end_pos": 25, "type": "TASK", "confidence": 0.9719559550285339}, {"text": "TAC summarization tasks", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.7453356782595316}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9745287299156189}, {"text": "allowance", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9566091299057007}, {"text": "recall", "start_pos": 174, "end_pos": 180, "type": "METRIC", "confidence": 0.9982179999351501}, {"text": "F \u03b2", "start_pos": 186, "end_pos": 189, "type": "METRIC", "confidence": 0.9813069999217987}]}, {"text": "The allowance parameter was determined from the average nugget length for each question type of the ACLIA2 collection (.", "labels": [], "entities": [{"text": "allowance", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9798617362976074}, {"text": "ACLIA2 collection", "start_pos": 100, "end_pos": 117, "type": "DATASET", "confidence": 0.9267177283763885}]}, {"text": "Precision and recall are computed from the nuggets that the summary covered along with their weights.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9917884469032288}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9992687106132507}]}, {"text": "One of the authors of this paper manually evaluated whether each nugget matched the summary.", "labels": [], "entities": []}, {"text": "We also used the automatic evaluation measure, POURPRE).", "labels": [], "entities": [{"text": "POURPRE", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9734886288642883}]}, {"text": "POURPRE is based on word matching of reference nuggets and system outputs.", "labels": [], "entities": [{"text": "word matching of reference nuggets", "start_pos": 20, "end_pos": 54, "type": "TASK", "confidence": 0.7983676075935364}]}, {"text": "We regarded as stopwords the most frequent 100 words in Mainichi articles from 1991 to 2005 (the document frequency was used to measure the frequency).", "labels": [], "entities": [{"text": "Mainichi articles", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.9550771713256836}]}, {"text": "We also set the threshold of nugget matching as 0.5 and binarized the nugget matching, following the previous study.", "labels": [], "entities": [{"text": "nugget matching", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7466446757316589}]}, {"text": "We tuned the parameters by using POUR-PRE on the development dataset.", "labels": [], "entities": [{"text": "POUR-PRE", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9932865500450134}]}, {"text": "Lin and Bilmes (2011) designed a monotone submodular function for query-oriented summarization.", "labels": [], "entities": []}, {"text": "Their succinct method performed well in DUC from 2004 to 2007.", "labels": [], "entities": [{"text": "DUC", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.7617452144622803}]}, {"text": "They proposed a positive diversity reward function in order to define a monotone submodular objective function for generating a non-redundant summary.", "labels": [], "entities": []}, {"text": "The diversity reward gives a smaller gain fora biased summary, because it consists of gains based on three clusters and calculates a square root score with respect to each sentence.", "labels": [], "entities": []}, {"text": "The reward also contains a score for the similarity of a sentence to the query, for purposes of query-oriented summa-  rization.", "labels": [], "entities": []}, {"text": "Their objective function also includes a coverage function based on the similarity w i,j between sentences.", "labels": [], "entities": []}, {"text": "In the coverage function min function limits the maximum gain \u03b1 \u2211 i\u2208V w i,j , which is a small fraction \u03b1 of the similarity between a sentence j and the all source documents.", "labels": [], "entities": []}, {"text": "The objective function is the sum of the positive reward Rand the coverage function L over the source documents V , as follows: where \u03b1, \u03b2 and \u03bb k are parameters, and r j,Q represents the similarity between sentence j and query Q.", "labels": [], "entities": []}, {"text": "We tuned the parameters on the development dataset.", "labels": [], "entities": []}, {"text": "used three clusters C k with different granularities, which were calculated in advance.", "labels": [], "entities": []}, {"text": "We set the granularity to (0.2N , 0.15N , 0.05N ) according to the settings of them, where N is the number of sentences in a document.", "labels": [], "entities": []}, {"text": "We also regarded as stopwords \" (tell),\" \" (know),\" \" (what)\" and their conjugated forms, which are excessively common in questions.", "labels": [], "entities": []}, {"text": "For the query expansion in the baseline, we used Japanese WordNet to obtain synonyms and hypernyms of query terms.", "labels": [], "entities": [{"text": "Japanese WordNet", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.7462893128395081}]}, {"text": "\"Subtree extraction (SbE)\" is our method, and \"Sentence extraction (NC)\" is aversion of our method without compression.", "labels": [], "entities": [{"text": "Subtree extraction", "start_pos": 1, "end_pos": 19, "type": "TASK", "confidence": 0.7115165442228317}, {"text": "Sentence extraction (NC)\"", "start_pos": 47, "end_pos": 72, "type": "METRIC", "confidence": 0.6698584735393525}]}, {"text": "The NC has the same objective function but only extracts sentences.", "labels": [], "entities": []}, {"text": "The F1-measure and F3-measure of our method are 0.159 and 0.190 respectively, while those of the state-of-the-art baseline are 0.135 and 0.174 respectively.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9985865354537964}, {"text": "F3-measure", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9225467443466187}]}, {"text": "Unfortunately, since the document set is small, the difference is not statistically significant.", "labels": [], "entities": []}, {"text": "Comparing our method with the one without compression, we can see that there are improvements in the F1 and F3 scores of the human evaluation, whereas the POURPRE score of the version of our method without compression is higher than that of our method with compression.", "labels": [], "entities": [{"text": "F1", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9994063377380371}, {"text": "F3", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.8886089324951172}, {"text": "POURPRE score", "start_pos": 155, "end_pos": 168, "type": "METRIC", "confidence": 0.9799994826316833}]}, {"text": "The compression improved the precision of our method, but slightly decreased the recall.", "labels": [], "entities": [{"text": "compression", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9457401037216187}, {"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9996079802513123}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9995492100715637}]}], "tableCaptions": [{"text": " Table 1: Results on ACLIA2 test data.", "labels": [], "entities": [{"text": "ACLIA2 test data", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.9360000888506571}]}, {"text": " Table 2: Effect of sentence compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.7712821364402771}]}]}