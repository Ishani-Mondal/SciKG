{"title": [{"text": "Beam Search for Solving Substitution Ciphers", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we address the problem of solving substitution ciphers using abeam search approach.", "labels": [], "entities": []}, {"text": "We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models.", "labels": [], "entities": []}, {"text": "We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6-gram language model.", "labels": [], "entities": []}, {"text": "We also apply our approach to the famous Zodiac-408 cipher and obtain slightly better (and near to optimal) results than previously published.", "labels": [], "entities": []}, {"text": "Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible deci-pherments, our approach only uses a letter-based 6-gram language model.", "labels": [], "entities": []}, {"text": "Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8% to 6.0% error rate.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 148, "end_pos": 163, "type": "DATASET", "confidence": 0.9583315551280975}]}], "introductionContent": [{"text": "State-of-the-art statistical machine translation (SMT) systems use large amounts of parallel data to estimate translation models.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.797153557340304}]}, {"text": "However, parallel corpora are expensive and not available for every domain.", "labels": [], "entities": []}, {"text": "Recently different works have been published that train translation models using only nonparallel data.", "labels": [], "entities": []}, {"text": "Although first practical applications of these approaches have been shown, the overall decipherment accuracy of the proposed algorithms is still low.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9920885562896729}]}, {"text": "Improving the core decipherment algorithms is an important step for making decipherment techniques useful for practical applications.", "labels": [], "entities": []}, {"text": "In this paper we present an effective beam search algorithm which provides high decipherment accuracies while having low computational requirements.", "labels": [], "entities": [{"text": "beam search", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.8976507782936096}]}, {"text": "The proposed approach allows using high order n-gram language models, is scalable to large vocabulary sizes and can be adjusted to account fora given amount of computational resources.", "labels": [], "entities": []}, {"text": "We show significant improvements in decipherment accuracy in a variety of experiments while being computationally more effective than previous published works.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9833148717880249}]}], "datasetContent": [{"text": "We conduct experiments on letter based 1:1 substitution ciphers, the homophonic substitution cipher Zodiac-408, and word based 1:1 substitution ciphers.", "labels": [], "entities": []}, {"text": "For a given reference mapping \u03c6 ref , we evaluate candidate mappings \u03c6 using two error measures: Mapping Error Rate MER(\u03c6, \u03c6 ref ) and Symbol Error Rate SER(\u03c6, \u03c6 ref ).", "labels": [], "entities": [{"text": "Error Rate MER", "start_pos": 105, "end_pos": 119, "type": "METRIC", "confidence": 0.8115201791127523}, {"text": "Symbol Error Rate SER", "start_pos": 135, "end_pos": 156, "type": "METRIC", "confidence": 0.7952405363321304}]}, {"text": "Roughly speaking, SER reports the fraction of symbols in the deciphered text that are not correct, while MER reports the fraction of incorrect mappings in \u03c6.", "labels": [], "entities": [{"text": "SER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9729875326156616}, {"text": "MER", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9202380776405334}]}, {"text": "Given a set of symbols V eval with unigram counts N (v) for v \u2208 V eval , and the total amount of running symbols Thus the SER can be seen as a weighted form of the MER, emphasizing errors for frequent words.", "labels": [], "entities": [{"text": "SER", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9721004962921143}, {"text": "MER", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.7641027569770813}]}, {"text": "In decipherment experiments, SER will often be lower than MER, since it is often easier to decipher frequent words.", "labels": [], "entities": [{"text": "SER", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9778512120246887}, {"text": "MER", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9940179586410522}]}], "tableCaptions": [{"text": " Table 1: Symbol error rates (SER), Mapping er- ror rates (MER) and runtimes (RT) in dependence  of language model order (ORDER) and histogram  pruning size (BEAM) for decipherment of letter  substitution ciphers of length 128. Runtimes are  reported on a single core machine. Results for  beam size \"\u221e\" were obtained using A  *  search.", "labels": [], "entities": [{"text": "error rates (SER)", "start_pos": 17, "end_pos": 34, "type": "METRIC", "confidence": 0.8865353345870972}, {"text": "Mapping er- ror rates (MER) and runtimes (RT)", "start_pos": 36, "end_pos": 81, "type": "METRIC", "confidence": 0.7347824137944442}, {"text": "language model order (ORDER)", "start_pos": 100, "end_pos": 128, "type": "METRIC", "confidence": 0.6307007173697153}, {"text": "histogram  pruning size (BEAM)", "start_pos": 133, "end_pos": 163, "type": "METRIC", "confidence": 0.740793988108635}]}, {"text": " Table 2: Symbol error rates (SER), Mapping er- ror rates (MER) and runtimes (RT) in dependence  of language model order (ORDER) and histogram  pruning size (BEAM) for the decipherment of the  Zodiac-408 cipher. Runtimes are reported on a  128-core machine.", "labels": [], "entities": [{"text": "error rates (SER)", "start_pos": 17, "end_pos": 34, "type": "METRIC", "confidence": 0.911468505859375}, {"text": "Mapping er- ror rates (MER) and runtimes (RT)", "start_pos": 36, "end_pos": 81, "type": "METRIC", "confidence": 0.7669376501670251}, {"text": "language model order (ORDER)", "start_pos": 100, "end_pos": 128, "type": "METRIC", "confidence": 0.7085047215223312}, {"text": "histogram  pruning size (BEAM)", "start_pos": 133, "end_pos": 163, "type": "METRIC", "confidence": 0.7667624404033025}]}, {"text": " Table 3: Word error rates (WER), Mapping error  rates (MER) and runtimes (RT) for iterative deci- pherment run on the (TOP) most frequent words.  Error rates are evaluated on the full vocabulary.  Runtimes are reported on a 128-core machine.", "labels": [], "entities": [{"text": "Word error rates (WER)", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8441354433695475}, {"text": "Mapping error  rates (MER)", "start_pos": 34, "end_pos": 60, "type": "METRIC", "confidence": 0.8007312516371409}, {"text": "runtimes (RT)", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.8899832665920258}]}]}