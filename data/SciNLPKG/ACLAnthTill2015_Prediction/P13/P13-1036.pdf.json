{"title": [{"text": "Scalable Decipherment for Machine Translation via Hash Sampling", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7740776836872101}, {"text": "Hash Sampling", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.6758971661329269}]}], "abstractContent": [{"text": "In this paper, we propose anew Bayesian inference method to train statistical machine translation systems using only non-parallel corpora.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.6332371830940247}]}, {"text": "Following a probabilis-tic decipherment approach, we first introduce anew framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models.", "labels": [], "entities": [{"text": "estimating translation models", "start_pos": 236, "end_pos": 265, "type": "TASK", "confidence": 0.7489938934644064}]}, {"text": "In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al.", "labels": [], "entities": []}, {"text": "The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocab-ulary/corpora sizes.", "labels": [], "entities": []}, {"text": "We show empirical results on the OPUS data-our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster).", "labels": [], "entities": [{"text": "OPUS data-our", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.8254198431968689}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.999075174331665}]}, {"text": "We also report for the first time-BLEU score results fora large-scale MT task using only non-parallel data (EMEA corpus).", "labels": [], "entities": [{"text": "MT task", "start_pos": 70, "end_pos": 77, "type": "TASK", "confidence": 0.9326518177986145}, {"text": "EMEA corpus", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.8208961188793182}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) systems these days are built using large amounts of bilingual parallel corpora.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8342730502287546}]}, {"text": "The parallel corpora are used to estimate translation model parameters involving word-to-word translation tables, fertilities, distortion, phrase translations, syntactic transformations, etc.", "labels": [], "entities": [{"text": "phrase translations", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.7153943926095963}]}, {"text": "But obtaining parallel data is an expensive process and not available for all language pairs or domains.", "labels": [], "entities": []}, {"text": "On the other hand, monolingual data (in written form) exists and is easier to obtain for many languages.", "labels": [], "entities": []}, {"text": "Learning translation models from monolingual corpora could help address the challenges faced by modern-day MT systems, especially for low resource language pairs.", "labels": [], "entities": [{"text": "MT", "start_pos": 107, "end_pos": 109, "type": "TASK", "confidence": 0.9866781234741211}]}, {"text": "Recently, this topic has been receiving increasing attention from researchers and new methods have been proposed to train statistical machine translation models using only monolingual data in the source and target language.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 122, "end_pos": 153, "type": "TASK", "confidence": 0.6262761553128561}]}, {"text": "The underlying motivation behind most of these methods is that statistical properties for linguistic elements are shared across different languages and some of these similarities (mappings) could be automatically identified from large amounts of monolingual data.", "labels": [], "entities": []}, {"text": "The MT literature does cover some prior work on extracting or augmenting partial lexicons using non-parallel corpora.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9495792984962463}]}, {"text": "However, none of these methods attempt to train end-to-end MT models, instead they focus on mining bilingual lexicons from monolingual corpora and often they require parallel seed lexicons as a starting point.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9808393716812134}]}, {"text": "Some of them () also rely on additional linguistic knowledge such as orthography, etc.", "labels": [], "entities": []}, {"text": "to mine word translation pairs across related languages (e.g., Spanish/English).", "labels": [], "entities": [{"text": "word translation pairs", "start_pos": 8, "end_pos": 30, "type": "TASK", "confidence": 0.7767065266768137}]}, {"text": "Unsupervised training methods have also been proposed in the past for related problems in decipherment) where the goal is to decode unknown scripts or ciphers.", "labels": [], "entities": []}, {"text": "The body of work that is more closely related to ours include that of who introduced a decipherment approach for training translation models using only monolingual cor-pora.", "labels": [], "entities": []}, {"text": "Their best performing method uses an EM algorithm to train a word translation model and they show results on a Spanish/English task.", "labels": [], "entities": [{"text": "word translation", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7283046096563339}]}, {"text": "extend the former approach and improve training efficiency by pruning translation candidates prior to EM training with the help of context similarities computed from monolingual corpora.", "labels": [], "entities": []}, {"text": "In this work we propose anew Bayesian inference method for estimating translation models from scratch using only monolingual corpora.", "labels": [], "entities": [{"text": "estimating translation", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.6281559467315674}]}, {"text": "Secondly, we introduce anew feature-based representation for sampling translation candidates that allows one to incorporate any amount of additional features (beyond simple bag-of-words) as sideinformation during decipherment training.", "labels": [], "entities": []}, {"text": "Finally, we also derive anew accelerated sampling mechanism using locality sensitive hashing inspired by recent work on fast, probabilistic inference for unsupervised clustering).", "labels": [], "entities": []}, {"text": "The new sampler allows us to perform fast, efficient inference with more complex translation models (than previously used) and scale better to large vocabulary and corpora sizes compared to existing methods as evidenced by our experimental results on two different corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test our method on two different corpora.", "labels": [], "entities": []}, {"text": "To evaluate translation quality, we use BLEU score (), a standard evaluation measure used in machine translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.9587172269821167}, {"text": "BLEU score", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9908907115459442}, {"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8195878863334656}]}, {"text": "First, we present MT results on non-parallel Spanish/English data from the OPUS corpus which was used by and.", "labels": [], "entities": [{"text": "MT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9717381000518799}, {"text": "OPUS corpus", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.9202600717544556}]}, {"text": "We show that our method achieves the best performance (BLEU scores) on this task while being significantly faster than both the previous approaches.", "labels": [], "entities": [{"text": "BLEU scores)", "start_pos": 55, "end_pos": 67, "type": "METRIC", "confidence": 0.9625729521115621}]}, {"text": "We then apply our method to a much larger non-parallel French/Spanish corpus constructed from the EMEA corpus.", "labels": [], "entities": [{"text": "EMEA corpus", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.934688001871109}]}, {"text": "Here the vocabulary sizes are much larger and we show how our new Bayesian decipherment method scales well to this task inspite of using complex translation models.", "labels": [], "entities": []}, {"text": "We also report the first BLEU results on such a large-scale MT task under truly non-parallel settings (without using any parallel data or seed lexicon).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9984447360038757}, {"text": "MT task", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9266173541545868}]}, {"text": "For both the MT tasks, we also report BLEU scores fora baseline system using identity translations for common words (words appearing in both source/target vocabularies) and random translations for other words.", "labels": [], "entities": [{"text": "MT tasks", "start_pos": 13, "end_pos": 21, "type": "TASK", "confidence": 0.9170714318752289}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9988731741905212}]}], "tableCaptions": [{"text": " Table 1: Statistics of non-parallel corpora used  here.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours)  on the Spanish/English OPUS corpus using only non-parallel corpora for training. For the Bayesian  methods 4a and 4b, the samplers were run for 1000 iterations each on a single machine (1.8GHz Intel  processor). For 1a, 2a, 2b, we list the training times as reported by Nuhn et al. (2012) based on their EM  implementation for different settings.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9671829342842102}, {"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.979805052280426}, {"text": "Spanish/English OPUS corpus", "start_pos": 104, "end_pos": 131, "type": "DATASET", "confidence": 0.5923369407653809}]}, {"text": " Table 3: MT results on the French/Spanish EMEA  corpus using the new hash sampling method.  *  The  last row displays results when we sample target  translations from a pruned candidate set (most fre- quent 1k Spanish words + identity translation can- didates) which enables the sampler to run much  faster when using more complex models.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9400023221969604}, {"text": "French/Spanish EMEA  corpus", "start_pos": 28, "end_pos": 55, "type": "DATASET", "confidence": 0.8339930891990661}]}]}