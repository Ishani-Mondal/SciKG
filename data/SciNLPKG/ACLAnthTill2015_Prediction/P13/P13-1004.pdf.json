{"title": [{"text": "Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation", "labels": [], "entities": [{"text": "Modelling Annotator Bias", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8733123739560446}, {"text": "Machine Translation Quality Estimation", "start_pos": 79, "end_pos": 117, "type": "TASK", "confidence": 0.8523593246936798}]}], "abstractContent": [{"text": "Annotating linguistic data is often a complex , time consuming and expensive endeavour.", "labels": [], "entities": []}, {"text": "Even with strict annotation guidelines, human subjects often deviate in their analyses, each bringing different biases, interpretations of the task and levels of consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 162, "end_pos": 173, "type": "METRIC", "confidence": 0.9707805514335632}]}, {"text": "We present novel techniques for learning from the outputs of multiple annotators while accounting for annotator specific behaviour.", "labels": [], "entities": []}, {"text": "These techniques use multi-task Gaussian Processes to learn jointly a series of annotator and metadata specific models, while explicitly representing correlations between models which can be learned directly from data.", "labels": [], "entities": []}, {"text": "Our experiments on two machine translation quality estimation datasets show uniform significant accuracy gains from multi-task learning, and consistently out-perform strong baselines.", "labels": [], "entities": [{"text": "machine translation quality estimation", "start_pos": 23, "end_pos": 61, "type": "TASK", "confidence": 0.7786034792661667}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9980059266090393}]}], "introductionContent": [{"text": "Most empirical work in Natural Language Processing (NLP) is based on supervised machine learning techniques which rely on human annotated data of some form or another.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.7745460073153178}]}, {"text": "The annotation process is often time consuming, expensive, and prone to errors; moreover there is often considerable disagreement amongst annotators.", "labels": [], "entities": []}, {"text": "In general, the predominant perspective to deal with these data annotation issues in previous work has been that there is a single underlying ground truth, and that the annotations collected are noisy and/or biased samples of this.", "labels": [], "entities": []}, {"text": "The challenge is then one of quality control, in order to process the data by filtering, averaging or similar to distil the truth.", "labels": [], "entities": []}, {"text": "We posit that this perspective is too limiting, especially with respect to linguistic data, where each individual's idiolect and linguistic background can give rise to many different -and yet equally valid -truths.", "labels": [], "entities": []}, {"text": "Particularly in highly subjective annotation tasks, the differences between annotators cannot be captured by simple models such as scaling all instances of a certain annotator by a factor.", "labels": [], "entities": []}, {"text": "They can originate from a number of nuanced aspects.", "labels": [], "entities": []}, {"text": "This is the case, for example, of annotations on the quality of sentences generated using machine translation (MT) systems, which are often used to build quality estimation models () -our application of interest.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.8411531329154969}]}, {"text": "In addition to annotators' own perceptions and expectations with respect to translation quality, a number of factors can affect their judgements on specific sentences.", "labels": [], "entities": []}, {"text": "For example, certain annotators may prefer translations produced by rulebased systems as these tend to be more grammatical, while others would prefer sentences produced by statistical systems with more adequate lexical choices.", "labels": [], "entities": []}, {"text": "Likewise, some annotators can be biased by the complexity of the source sentence: lengthy sentences are often (subconsciously) assumed to be of low quality by some annotators.", "labels": [], "entities": []}, {"text": "An extreme case is the judgement of quality through post-editing time: annotators have different typing speeds, as well as levels of expertise in the task of post-editing, proficiency levels in the language pair, and knowledge of the terminology used in particular sentences.", "labels": [], "entities": []}, {"text": "These variations result in time measurements that are not comparable across annotators.", "labels": [], "entities": []}, {"text": "Thus far, the use of post-editing time has been done on an per-annotator basis, or simply averaged across multiple translators, both strategies far from ideal.", "labels": [], "entities": []}, {"text": "Overall, these myriad of factors affecting quality judgements make the modelling of multiple annotators a very challenging problem.", "labels": [], "entities": []}, {"text": "This problem is exacerbated when annotations are provided by non-professional annotators, e.g., through crowdsourcing -a common strategy used to make annotation cheaper and faster, however at the cost of less reliable outcomes.", "labels": [], "entities": []}, {"text": "Most related work on quality assurance for data annotation has been developed in the context of crowdsourcing.", "labels": [], "entities": []}, {"text": "Common practices include filtering out annotators who substantially deviate from a gold-standard set or present unexpected behaviours (, or who disagree with others using, e.g., majority or consensus labelling (.", "labels": [], "entities": []}, {"text": "Another relevant strand of work aims to model legitimate, systematic biases in annotators (including both non-experts and experts), such as the fact that some annotators tend to be more negative than others, and that some annotators use a wider or narrower range of values (.", "labels": [], "entities": []}, {"text": "However, with a few exceptions in Computer Vision (e.g.,,), existing work disregard metadata and its impact on labelling.", "labels": [], "entities": [{"text": "Computer Vision", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.7197820246219635}]}, {"text": "In this paper we model the task of predicting the quality of sentence translations using datasets that have been annotated by several judges with different levels of expertise and reliability, containing translations from a variety of MT systems and on a range of different types of sentences.", "labels": [], "entities": [{"text": "predicting the quality of sentence translations", "start_pos": 35, "end_pos": 82, "type": "TASK", "confidence": 0.6029629111289978}, {"text": "MT", "start_pos": 235, "end_pos": 237, "type": "TASK", "confidence": 0.941867470741272}]}, {"text": "We address this problem using multi-task learning in which we learn individual models for each context (the task, incorporating the annotator and other metadata: translation system and the source sentence) while also modelling correlations between tasks such that related tasks can mutually inform one another.", "labels": [], "entities": []}, {"text": "Our use of multi-task learning allows the modelling of a diversity of truths, while also recognising that they are rarely independent of one another (annotators often agree) by explicitly accounting for inter-annotator correlations.", "labels": [], "entities": []}, {"text": "Our approach is based on Gaussian Processes (GPs)), a kernelised Bayesian non-parametric learning framework.", "labels": [], "entities": []}, {"text": "We develop multi-task learning models by representing intra-task transfer simply and explicitly as part of a parameterised kernel function.", "labels": [], "entities": []}, {"text": "GPs are an extremely flexible probabilistic framework and have been successfully adapted for multi-task learning in a number of ways, e.g., by learning multi-task correlations (, modelling per-task variance) or perannotator biases.", "labels": [], "entities": []}, {"text": "Our method builds on the work of by explicitly modelling intra-task transfer, which is learned automatically from the data, in order to robustly handle outlier tasks and task variances.", "labels": [], "entities": []}, {"text": "We show in our experiments on two translation quality datasets that these multi-task learning strategies are far superior to training individual per-task models or a single pooled model, and moreover that our multi-task learning approach can achieve similar performance to these baselines using only a fraction of the training data.", "labels": [], "entities": []}, {"text": "In addition to showing empirical performance gains on quality estimation applications, an important contribution of this paper is in introducing Gaussian Processes to the NLP community, 1 a technique that has great potential to further performance in a wider range of NLP applications.", "labels": [], "entities": []}, {"text": "Moreover, the algorithms proposed herein can be adapted to improve future annotation efforts, and subsequent use of noisy crowd-sourced data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use two freely available QE datasets to experiment with the techniques proposed in this paper: 2 WMT12: This dataset was distributed as part of the WMT12 shared task on QE).", "labels": [], "entities": [{"text": "QE datasets", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.8034655153751373}]}, {"text": "It contains 1, 832 instances for training, and 422 for test.", "labels": [], "entities": []}, {"text": "The English source sentences area subset of WMT09-12 test sets.", "labels": [], "entities": [{"text": "WMT09-12 test sets", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.9336718519528707}]}, {"text": "The Spanish MT outputs were created using a standard PBSMT Moses engine.", "labels": [], "entities": [{"text": "MT", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.872915506362915}]}, {"text": "Each instance was annotated with post-editing effort scores from highest effort (score 1) to lowest effort (score 5), where each score identifies an estimated percentage of the MT output that needs to be corrected.", "labels": [], "entities": [{"text": "MT", "start_pos": 177, "end_pos": 179, "type": "TASK", "confidence": 0.9513460397720337}]}, {"text": "The post-editing effort scores were produced independently by three professional translators based on a previously post-edited translation by a fourth translator.", "labels": [], "entities": []}, {"text": "In an attempt to accommodate for systematic biases among annotators, the final effort score was computed as the weighted average between the three PE-effort scores, with more weight given to the judges with higher standard deviation from their own mean score.", "labels": [], "entities": []}, {"text": "This resulted in scores spread more evenly in the range.", "labels": [], "entities": []}, {"text": "WPTP12: This dataset was distributed by.", "labels": [], "entities": [{"text": "WPTP12", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9614819884300232}]}, {"text": "It contains 299 English sentences translated into Spanish using two or more of eight MT systems randomly selected from all system submissions for WMT11).", "labels": [], "entities": [{"text": "WMT11", "start_pos": 146, "end_pos": 151, "type": "DATASET", "confidence": 0.8718529343605042}]}, {"text": "These MT systems range from online and customised SMT systems to commercial rule-based systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.9885048270225525}, {"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9840203523635864}]}, {"text": "Translations were post-edited by humans while time was recorded.", "labels": [], "entities": [{"text": "Translations", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9512738585472107}]}, {"text": "The labels are the number of seconds spent by a translator editing a sentence normalised by source sentence length.", "labels": [], "entities": []}, {"text": "The post-editing was done by eight native speakers of Spanish, including five professional translators and three translation students.", "labels": [], "entities": []}, {"text": "Only 20 translations were edited by all eight annotators, with the remaining translations randomly distributed amongst them.", "labels": [], "entities": []}, {"text": "The resulting dataset contains 1, 624 instances, which were randomly split into 1, 300 for training and 300 for test.", "labels": [], "entities": []}, {"text": "According to the analysis in (, while on average certain translators were found to be faster than others, their speed in post-editing individual sentences varies considerably, i.e., certain translators are faster at certain sentences.", "labels": [], "entities": []}, {"text": "To our knowledge, no previous work has managed to successfully model the prediction of post-editing time from datasets with multiple annotators.", "labels": [], "entities": []}, {"text": "Feature sets: In all experiments we use 17 shallow QE features that have been shown to perform well in previous work.", "labels": [], "entities": []}, {"text": "These were used by a highly competitive baseline entry in the WMT12 shared task, and were extracted here using the system provided by that shared task.", "labels": [], "entities": [{"text": "WMT12 shared task", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.7207881410916647}]}, {"text": "They include simple counts, e.g., the tokens in sentences, as well as source and target language model probabilities.", "labels": [], "entities": []}, {"text": "Each feature was scaled to have zero mean and unit standard deviation on the training set.", "labels": [], "entities": [{"text": "mean", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9547444581985474}]}, {"text": "Baselines: The baselines use the SVM regression algorithm with radial basis function kernel and parameters \u03b3, and C optimised through gridsearch and 5-fold cross validation on the training set.", "labels": [], "entities": []}, {"text": "This is generally a very strong baseline: in the WMT12 QE shared task, only five out of 19 submissions were able to significantly outperform it, and only by including many complex additional features, tree kernels, etc.", "labels": [], "entities": [{"text": "WMT12 QE shared task", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.6175397783517838}]}, {"text": "We also present \u00b5, a trivial baseline based on predicting for each test instance the training mean (overall, and for specific tasks).", "labels": [], "entities": [{"text": "\u00b5", "start_pos": 16, "end_pos": 17, "type": "METRIC", "confidence": 0.9488024115562439}]}, {"text": "GP: All GP models were implemented using the GPML Matlab toolbox.", "labels": [], "entities": [{"text": "GP", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.9226592779159546}, {"text": "GPML Matlab toolbox", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.9537033637364706}]}, {"text": "Hyperparameter optimisation was performed using conjugate gradient ascent of the log marginal likelihood function, with up to 100 iterations.", "labels": [], "entities": []}, {"text": "The simpler models were initialised with all hyperparameters set to one, while more complex models were initialised using the: Single-task learning results on the WMT12 dataset, trained and evaluated against the weighted averaged response variable.", "labels": [], "entities": [{"text": "WMT12 dataset", "start_pos": 163, "end_pos": 176, "type": "DATASET", "confidence": 0.984127402305603}]}, {"text": "\u00b5 is a baseline which predicts the training mean, SVM uses the same system as the WMT12 QE task, and the remainder are GP regression models with different kernels (all include additive noise).", "labels": [], "entities": [{"text": "SVM", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.8471131324768066}, {"text": "WMT12 QE task", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.758039395014445}]}, {"text": "For instance, models using ARD kernels were initialised from an equivalent isotropic kernel (which ties all the hyperparameters together), and independent per-task noise models were initialised from a single noise model.", "labels": [], "entities": []}, {"text": "This approach was more reliable than random restarts in terms of accuracy and runtime efficiency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9995132684707642}]}, {"text": "Evaluation: We evaluate predictive accuracy using two measures: mean absolute error, , where y i are the gold standard response values and\u02c6yand\u02c6 and\u02c6y i are the model predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.90036541223526}, {"text": "mean absolute error", "start_pos": 64, "end_pos": 83, "type": "METRIC", "confidence": 0.870023767153422}]}], "tableCaptions": [{"text": " Table 1: Single-task learning results on the  WMT12 dataset, trained and evaluated against  the weighted averaged response variable. \u00b5 is a  baseline which predicts the training mean, SVM  uses the same system as the WMT12 QE task, and  the remainder are GP regression models with dif- ferent kernels (all include additive noise).", "labels": [], "entities": [{"text": "WMT12 dataset", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9820559918880463}, {"text": "WMT12 QE task", "start_pos": 218, "end_pos": 231, "type": "DATASET", "confidence": 0.7938050429026285}]}, {"text": " Table 2: Results on the WMT12 dataset, trained  and evaluated over all three annotator's judge- ments. Shown above are the training mean base- line \u00b5, single-task learning approaches, and multi- task learning models, with the columns showing  macro average error rates over all three response  values. All systems use a squared exponential  ARD kernel in a product with the named task- kernel, and with added noise (per-task noise is de- noted {N}, otherwise has shared noise).", "labels": [], "entities": [{"text": "WMT12 dataset", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9556068778038025}, {"text": "training mean base- line \u00b5", "start_pos": 124, "end_pos": 150, "type": "METRIC", "confidence": 0.7877438267072042}]}]}