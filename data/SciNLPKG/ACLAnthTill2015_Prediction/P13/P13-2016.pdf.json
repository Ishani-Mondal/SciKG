{"title": [{"text": "Learning to Order Natural Language Texts", "labels": [], "entities": [{"text": "Learning to Order Natural Language Texts", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6918208847443262}]}], "abstractContent": [{"text": "Ordering texts is an important task for many NLP applications.", "labels": [], "entities": [{"text": "Ordering texts", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8767994940280914}]}, {"text": "Most previous works on summary sentence ordering rely on the contex-tual information (e.g. adjacent sentences) of each sentence in the source document.", "labels": [], "entities": [{"text": "summary sentence ordering", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.6886268655459086}]}, {"text": "In this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information.", "labels": [], "entities": []}, {"text": "We introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences.", "labels": [], "entities": []}, {"text": "We also propose to use the genetic algorithm to determine the total order of all sentences.", "labels": [], "entities": []}, {"text": "Evaluation results on a news corpus show the effectiveness of our proposed method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ordering texts is an important task in many natural language processing (NLP) applications.", "labels": [], "entities": []}, {"text": "It is typically applicable in the text generation field, both for concept-to-text generation and text-totext generation, such as multiple document summarization (MDS), question answering and soon.", "labels": [], "entities": [{"text": "text generation", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.7163811922073364}, {"text": "concept-to-text generation", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.7770141065120697}, {"text": "text-totext generation", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.6911506056785583}, {"text": "multiple document summarization (MDS)", "start_pos": 129, "end_pos": 166, "type": "TASK", "confidence": 0.7894096175829569}, {"text": "question answering", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.8730937838554382}]}, {"text": "However, ordering a set of sentences into a coherent text is still a hard and challenging problem for computers.", "labels": [], "entities": [{"text": "ordering a set of sentences into a coherent text", "start_pos": 9, "end_pos": 57, "type": "TASK", "confidence": 0.8168280985620286}]}, {"text": "Previous works on sentence ordering mainly focus on the MDS task (;.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7975426018238068}]}, {"text": "In this task, each summary sentence is extracted from a source document.", "labels": [], "entities": []}, {"text": "The timestamp of the source documents and the adjacent sentences in the source documents can be used as important clues for ordering summary sentences.", "labels": [], "entities": []}, {"text": "In this study, we investigate a more challenging and more general task of ordering a set of unordered sentences (e.g. randomly shuffle the * Xiaojun Wan is the corresponding author.", "labels": [], "entities": []}, {"text": "sentences in a text paragraph) without any contextual information.", "labels": [], "entities": []}, {"text": "This task can be applied to almost all text generation applications without restriction.", "labels": [], "entities": [{"text": "text generation", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7478807866573334}]}, {"text": "In order to address this challenging task, we first introduce a few useful features to characterize the order and coherence of natural language texts, and then propose to use the learning to rank algorithm to determine the order of two sentences.", "labels": [], "entities": []}, {"text": "Moreover, we propose to use the genetic algorithm to decide the overall text order.", "labels": [], "entities": []}, {"text": "Evaluations are conducted on a news corpus, and the results show the prominence of our method.", "labels": [], "entities": []}, {"text": "Each component technique or feature in our method has also been validated.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data Set and Evaluation Metric: We conducted the experiments on the North American News Text Corpus 2 . We trained the model on 80 thousand paragraphs and tested with 200 shuffled paragraphs.", "labels": [], "entities": [{"text": "North American News Text Corpus 2", "start_pos": 68, "end_pos": 101, "type": "DATASET", "confidence": 0.9083666602770487}]}, {"text": "We use Kendall's \u03c4 as the evaluation metric, which is based on the number of inversions in the rankings.", "labels": [], "entities": []}, {"text": "Comparisons: It is incomparable with other methods for summary sentence ordering based on special summarization corpus, so we implemented Lapata's probability model for comparison, which is considered the state of the art for this task.", "labels": [], "entities": [{"text": "summary sentence ordering", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.6991264919439951}]}, {"text": "In addition, we implemented a random ordering as a baseline.", "labels": [], "entities": []}, {"text": "We also tried to use a classification model in place of the ranking model.", "labels": [], "entities": []}, {"text": "In the classification model, sentence pairs like 1 a a s s + \ud97b\udf59 were viewed as positive examples and all other pairs were viewed as negative examples.", "labels": [], "entities": []}, {"text": "When deciding the overall order for either ranking or classification model we used three search strategies: greedy, genetic and exhaustive (or brutal) algorithms.", "labels": [], "entities": []}, {"text": "In addition, we conducted a series of experiments to evaluate the effect of each feature.", "labels": [], "entities": []}, {"text": "For each feature, we tested in two experiments, one of which only contained the single feature and the other one contained all the other features.", "labels": [], "entities": []}, {"text": "For comparative analysis of features, we tested with an exhaustive search algorithm to determine the overall order.", "labels": [], "entities": []}, {"text": "The comparison results in show that our Ranking SVM based method improves the performance over the baselines and the classification based method with any of the search algorithms.", "labels": [], "entities": []}, {"text": "We can also seethe greedy search strategy does not perform well and the genetic algorithm can provide a good approximate solution to obtain optimal results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Effects of different features.  It can be seen in", "labels": [], "entities": []}, {"text": " Table 4: Results of GA with different parameters.", "labels": [], "entities": [{"text": "GA", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9603877067565918}]}]}