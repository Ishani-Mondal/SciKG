{"title": [{"text": "A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel transition-based, greedy dependency parser which implements a flexible mix of bottom-up and top-down strategies.", "labels": [], "entities": []}, {"text": "The new strategy allows the parser to postpone difficult decisions until the relevant information becomes available.", "labels": [], "entities": []}, {"text": "The novel parser has a \u223c12% error reduction in unlabeled attachment score over an arc-eager parser, with a slowdown factor of 2.8.", "labels": [], "entities": [{"text": "error reduction in unlabeled attachment score", "start_pos": 28, "end_pos": 73, "type": "METRIC", "confidence": 0.7779198636611303}]}], "introductionContent": [{"text": "Dependency-based methods for syntactic parsing have become increasingly popular during the last decade or so.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.8775353729724884}]}, {"text": "This development is probably due to many factors, such as the increased availability of dependency treebanks and the perceived usefulness of dependency structures as an interface to downstream applications, but a very important reason is also the high efficiency offered by dependency parsers, enabling web-scale parsing with high throughput.", "labels": [], "entities": []}, {"text": "The most efficient parsers are greedy transition-based parsers, which only explore a single derivation for each input and relies on a locally trained classifier for predicting the next parser action given a compact representation of the derivation history, as pioneered by,,, and others.", "labels": [], "entities": []}, {"text": "However, while these parsers are capable of processing tens of thousands of tokens per second with the right choice of classifiers, they are also known to perform slightly below the state-ofthe-art because of search errors and subsequent error propagation, and recent research on transition-based dependency parsing has therefore explored different ways of improving their accuracy.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 280, "end_pos": 315, "type": "TASK", "confidence": 0.6084111928939819}, {"text": "accuracy", "start_pos": 373, "end_pos": 381, "type": "METRIC", "confidence": 0.9946921467781067}]}, {"text": "The most common approach is to use beam search instead of greedy decoding, in combination with a globally trained model that tries to minimize the loss over the entire sentence instead of a locally trained classifier that tries to maximize the accuracy of single decisions (given no previous errors), as first proposed by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 244, "end_pos": 252, "type": "METRIC", "confidence": 0.9934584498405457}]}, {"text": "With these methods, transition-based parsers have reached state-of-the-art accuracy fora number of languages (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9985663294792175}]}, {"text": "However, the drawback with this approach is that parsing speed is proportional to the size of the beam, which means that the most accurate transition-based parsers are not nearly as fast as the original greedy transition-based parsers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.9743775129318237}]}, {"text": "Another line of research tries to retain the efficiency of greedy classifier-based parsing by instead improving the way in which classifiers are learned from data.", "labels": [], "entities": []}, {"text": "While the classical approach limits training data to parser states that result from oracle predictions (derived from a treebank), these novel approaches allow the classifier to explore states that result from its own (sometimes erroneous) predictions (.", "labels": [], "entities": []}, {"text": "In this paper, we explore an orthogonal approach to improving the accuracy of transition-based parsers, without sacrificing their advantage in efficiency, by introducing anew type of transition system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9989472031593323}]}, {"text": "While all previous transition systems assume a static parsing strategy with respect to top-down and bottom-up processing, our new system allows a dynamic strategy for ordering parsing decisions.", "labels": [], "entities": []}, {"text": "This has the advantage that the parser can postpone difficult decisions until the relevant information becomes available, in away that is not possible in existing transition systems.", "labels": [], "entities": []}, {"text": "A second advantage of dynamic parsing is that we can extend the feature inventory of previous systems.", "labels": [], "entities": [{"text": "dynamic parsing", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.6548973917961121}]}, {"text": "Our experiments show that these advantages lead to significant improvements in parsing accuracy, compared to a baseline parser that uses the arc-eager transition system of, which is one of the most widely used static transition systems.", "labels": [], "entities": [{"text": "parsing", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.981610894203186}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9793475270271301}]}], "datasetContent": [{"text": "Performance evaluation is carried out on the Penn Treebank () converted to Stanford basic dependencies).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9944257140159607}]}, {"text": "We use sections 2-21 for training, 22 as development set, and 23 as test set.", "labels": [], "entities": []}, {"text": "The part-of-speech tags are assigned by an automatic tagger with accuracy 97.1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9994720816612244}]}, {"text": "The tagger used on the training set is trained on the same data set by using four-way jackknifing, while the tagger used on the development and test sets is trained on all the training set.", "labels": [], "entities": []}, {"text": "We train an arc-labeled version of our parser.", "labels": [], "entities": []}, {"text": "In the first three lines of we compare: Definition of C(c, t) = (s 3 , s 2 , s 1 , q 1 , q 2 , gp, gg), for c = (\u03c3 |\u03c3 3 |\u03c3 2 |\u03c3 1 , b 1 |b 2 |\u03b2, A) and t of type sh or la k , ra k , k \u2265 1.", "labels": [], "entities": []}, {"text": "Symbols u j,k and v j,k are the k-th nodes in the left and right spines, respectively, of stack element \u03c3 j , with u j,1 = v j,1 being the shared root of \u03c3 j ; none is an artificial element used when some context's placeholder is not available.", "labels": [], "entities": []}, {"text": "Figure 5: Extraction of atomic features for context C(c, la 2 ) = (s 3 , s 2 , s 1 , q 1 , q 2 , gp, gg), c = (\u03c3, \u03b2, A: Accuracy on test set, excluding punctuation, for unlabeled attachment score (UAS), labeled attachment score (LAS), unlabeled exact match (UEM).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9870886206626892}, {"text": "unlabeled attachment score (UAS)", "start_pos": 169, "end_pos": 201, "type": "METRIC", "confidence": 0.6998741378386816}, {"text": "labeled attachment score (LAS)", "start_pos": 203, "end_pos": 233, "type": "METRIC", "confidence": 0.7682867596546809}, {"text": "exact match (UEM)", "start_pos": 245, "end_pos": 262, "type": "METRIC", "confidence": 0.9275483846664428}]}, {"text": "the accuracy of our parser against our implementation of the arc-eager and arc-standard parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999409556388855}]}, {"text": "For the arc-eager parser, we use the feature template of Zhang and Nivre (2011).", "labels": [], "entities": []}, {"text": "The same template is adapted to the arc-standard parser, by removing the top-down parent features and by adding the right child features for the first stack element.", "labels": [], "entities": []}, {"text": "It turns out that our feature template, described in \u00a74.3, is the exact merge of the templates used for the arc-eager and the arc-standard parsers.", "labels": [], "entities": []}, {"text": "We train all parsers up to 30 iterations, and for each parser we select the weight vector \u03c9 from the iteration with the best accuracy on the development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9990405440330505}]}, {"text": "All our parsers attach the root node at the end of the parsing process, following the 'None' approach discussed by.", "labels": [], "entities": []}, {"text": "Punctuation is excluded in all evaluation metrics.", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9563742280006409}]}, {"text": "Considering UAS, our parser provides an improvement of 1.15 over the arc-eager parser and an improvement of 1.31 over the arc-standard parser, that is an error reduction of \u223c12% and \u223c13%, respectively.", "labels": [], "entities": [{"text": "UAS", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.7465074062347412}, {"text": "error reduction", "start_pos": 154, "end_pos": 169, "type": "METRIC", "confidence": 0.9798235297203064}]}, {"text": "Considering LAS, we achieve improvements of 1.33 and 1.47, with an error reduction of \u223c11% and \u223c12%, over the arc-eager and the arc-standard parsers, respectively.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 67, "end_pos": 82, "type": "METRIC", "confidence": 0.9795801043510437}]}, {"text": "We speculate that the observed improvement of our parser can be ascribed to two distinct components.", "labels": [], "entities": []}, {"text": "The first component is the left-/rightspine representation for stack elements, introduced in \u00a73.3.", "labels": [], "entities": []}, {"text": "The second component is the easy-first strategy, implemented on the basis of the spurious ambiguity of our parser and the definition of correct/incorrect transitions in \u00a74.2.", "labels": [], "entities": []}, {"text": "In this perspective, we observe that our parser can indeed be viewed as an arc-standard model augmented with (i) the spine representation, and (ii) the easy-first strategy.", "labels": [], "entities": []}, {"text": "More specifically, (i) generalizes the la/ra transitions to the la k /ra k transitions, introducing a top-down component into the purely bottom-up arc-standard.", "labels": [], "entities": []}, {"text": "On the other hand, (ii) drops the limitation of canonical computations for the arc-standard, and leverages on the spurious ambiguity of the parser to enlarge the search space.", "labels": [], "entities": []}, {"text": "The two components above are mutually independent, meaning that we can individually implement each component on top of an arc-standard model.", "labels": [], "entities": []}, {"text": "More precisely, the arc-standard + spine model uses the transitions la k /ra k but retains the definition of canonical computation, defined by applying each la k /ra k transition as soon as possible.", "labels": [], "entities": []}, {"text": "On the other hand, the arc-standard + easy-first model retains the original la/ra transitions but is trained allowing any correct transition at each configuration.", "labels": [], "entities": []}, {"text": "In this case the characterization of correct and incorrect configurations in Lemma 1 has been adapted to transitions la/ra, taking into account the bottom-up constraint.", "labels": [], "entities": []}, {"text": "With the purpose of incremental comparison, we report accuracy results for the two 'incremental' models in the last two lines of.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9993245601654053}]}, {"text": "Analyzing these results, and comparing with the plain arcstandard, we see that the spine representation and the easy-first strategy individually improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9969934225082397}]}, {"text": "Moreover, their combination into our model (third line of) works very well, with an overall improvement larger than the sum of the individual contributions.", "labels": [], "entities": []}, {"text": "We now turn to a computational analysis.", "labels": [], "entities": []}, {"text": "At each iteration our parser evaluates a number of transitions bounded by \u03b3 + 1, with \u03b3 the maximum value of the sum of the lengths of the left spine in \u03c3 1 and of the right spine in \u03c3 2 . Quantity \u03b3 is bounded by the length n of the input sentence.", "labels": [], "entities": []}, {"text": "Since the parser applies exactly 2n + 1 transitions, worst case running time is O(n 2 ).", "labels": [], "entities": [{"text": "O", "start_pos": 80, "end_pos": 81, "type": "METRIC", "confidence": 0.9766281843185425}]}, {"text": "We have computed the average value of \u03b3 on our English data set, resulting in 2.98 (variance 2.15) for training set, and 2.95 (variance 1.96) for development set.", "labels": [], "entities": [{"text": "English data set", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.9267621040344238}]}, {"text": "We conclude that, in the expected case, running time is O(n), with a slowdown constant which is rather small, in comparison to standard transition-based parsers.", "labels": [], "entities": [{"text": "O", "start_pos": 56, "end_pos": 57, "type": "METRIC", "confidence": 0.9932643175125122}]}, {"text": "Accordingly, when running our parser against our implementation of the arc-eager and arc-standard models, we measured a slow-down of 2.8 and 2.2, respectively.", "labels": [], "entities": []}, {"text": "Besides the change in representation, this slow-down is also due to the increase in the number of features in our system.", "labels": [], "entities": []}, {"text": "We have also checked the worst case value of \u03b3 in our data set.", "labels": [], "entities": []}, {"text": "Interestingly, we have seen that for strings of length smaller than 40 this value linearly grows with n, and for longer strings the growth stops, with a maximum worst case observed value of 22.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy on test set, excluding punc- tuation, for unlabeled attachment score (UAS),  labeled attachment score (LAS), unlabeled exact  match (UEM).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9984934329986572}, {"text": "unlabeled attachment score (UAS)", "start_pos": 61, "end_pos": 93, "type": "METRIC", "confidence": 0.7859496424595515}, {"text": "labeled attachment score (LAS)", "start_pos": 96, "end_pos": 126, "type": "METRIC", "confidence": 0.803454652428627}, {"text": "exact  match (UEM)", "start_pos": 138, "end_pos": 156, "type": "METRIC", "confidence": 0.9340733408927917}]}]}