{"title": [{"text": "A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art", "labels": [], "entities": [{"text": "Automatic Content Evaluation of News Summaries", "start_pos": 12, "end_pos": 58, "type": "TASK", "confidence": 0.7744460652271906}]}], "abstractContent": [{"text": "How good are automatic content metrics for news summary evaluation?", "labels": [], "entities": [{"text": "news summary evaluation", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.7219595611095428}]}, {"text": "Here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content.", "labels": [], "entities": []}, {"text": "Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy , precision and recall in finding significantly different systems.", "labels": [], "entities": [{"text": "Text Analysis Conference", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.7892216642697653}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9994230270385742}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9987591505050659}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9993463158607483}]}, {"text": "Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies, are most accurate across the years; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable.", "labels": [], "entities": []}, {"text": "We also test combinations of ROUGE variants and find that they considerably improve the accuracy of automatic prediction.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9256368279457092}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9994751811027527}, {"text": "automatic prediction", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.48121178150177}]}], "introductionContent": [{"text": "ROUGE) is a suite of automatic evaluations for summarization and was introduced a decade ago as a reasonable substitute for costly and slow human evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9379866719245911}, {"text": "summarization", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.9897193908691406}]}, {"text": "The scores it produces are based on n-gram or syntactic overlap between an automatic summary and a set of human reference summaries.", "labels": [], "entities": []}, {"text": "However, the field does not have a good grasp of which of the many evaluation scores is most accurate in replicating human judgements.", "labels": [], "entities": [{"text": "replicating human judgements", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.8694883584976196}]}, {"text": "This state of uncertainty has led to problems in comparing published work, as different researchers choose to publish different variants of scores.", "labels": [], "entities": []}, {"text": "In this paper we reassess the strengths of ROUGE variants using the data from four years of Text Analysis Conference (TAC) evaluations, 2008 to 2011.", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC) evaluations", "start_pos": 92, "end_pos": 134, "type": "TASK", "confidence": 0.7984218512262616}]}, {"text": "To assess the performance of the automatic evaluations, we focus on determining statistical significance 1 between systems, where the gold-standard comes from comparing the systems using manual pyramid and responsiveness evaluations.", "labels": [], "entities": [{"text": "statistical significance 1", "start_pos": 80, "end_pos": 106, "type": "METRIC", "confidence": 0.7314031720161438}]}, {"text": "In this setting, computing correlation coefficients between manual and automatic scores is not applicable as it does not take into account the statistical significance of the differences nor does it allow the use of more powerful statistical tests which use pairwise comparisons of performance on individual document sets.", "labels": [], "entities": []}, {"text": "Instead, we report on the accuracy of decisions on pairs of systems, as well as the precision and recall of identifying pairs of systems which exhibit statistically significant differences in content selection performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9979767203330994}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9993788003921509}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9985595345497131}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of pairs of significantly different  systems among the top 30 across the years. There  is a total of 435 pairs in each year.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy, Precision, Recall, and Balanced Accuracy of each ROUGE variant, averaged across  all eight tasks in 2008-2011, with and (without) significance.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994837045669556}, {"text": "Precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9932700991630554}, {"text": "Recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9855898022651672}, {"text": "Balanced", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9955251812934875}, {"text": "Accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.566220760345459}, {"text": "ROUGE", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.7652183175086975}]}, {"text": " Table 3: Accuracy, Precision, Recall, and Bal- anced Accuracy of each ROUGE combination on  TAC 2008-2010 pyramid.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994667172431946}, {"text": "Precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9981532692909241}, {"text": "Recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9916292428970337}, {"text": "Bal- anced", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9696138699849447}, {"text": "Accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.5272790789604187}, {"text": "ROUGE", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.8887174725532532}, {"text": "TAC 2008-2010 pyramid", "start_pos": 93, "end_pos": 114, "type": "DATASET", "confidence": 0.9270600080490112}]}, {"text": " Table 4: Best performing AESOP systems from TAC 2011; Scores within the 95% confidence interval  of the best are in bold face.", "labels": [], "entities": [{"text": "TAC 2011", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9183744192123413}]}]}