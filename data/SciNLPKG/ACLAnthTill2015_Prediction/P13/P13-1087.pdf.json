{"title": [{"text": "Density Maximization in Context-Sense Metric Space for All-words WSD", "labels": [], "entities": [{"text": "Density Maximization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8375690579414368}, {"text": "WSD", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.45114490389823914}]}], "abstractContent": [{"text": "This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disam-biguation from untagged corpora.", "labels": [], "entities": []}, {"text": "By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit sense-interdependency among the words in the same text string.", "labels": [], "entities": []}, {"text": "Through the smoothing, all the optimal senses are obtained atone time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word.", "labels": [], "entities": []}, {"text": "Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense base-line performance where none of SemEval-2 unsupervised systems reached.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is a task to identify the intended sense of a word based on its context.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7710112581650416}]}, {"text": "All-words WSD is its variant, where all the unrestricted running words in text are expected to be disambiguated.", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.46546807885169983}]}, {"text": "In the all-words task, all the senses in a dictionary are potentially the target destination of classification, and purely supervised approaches inherently suffer from data-sparsity problem.", "labels": [], "entities": []}, {"text": "The all-words task is also characterized by sense-interdependency of target words.", "labels": [], "entities": []}, {"text": "As the target words are typically taken from the same text string, they are naturally expected to be interrelated.", "labels": [], "entities": []}, {"text": "Disambiguation of a word should affect other words as an important clue.", "labels": [], "entities": [{"text": "Disambiguation of a word", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.872014582157135}]}, {"text": "From such characteristics of the task, knowledge-based unsupervised approaches have been extensively studied.", "labels": [], "entities": []}, {"text": "They compute dictionary-based sense similarity to find the most related senses among the words within a certain range of text.", "labels": [], "entities": []}, {"text": "(For reviews, see.)", "labels": [], "entities": []}, {"text": "In recent years, graph-based methods have attracted considerable attentions ().", "labels": [], "entities": []}, {"text": "On the graph structure of lexical knowledge base (LKB), random-walk or other well-known graph-based techniques have been applied to find mutually related senses among target words.", "labels": [], "entities": []}, {"text": "Unlike earlier studies disambiguating word-by-word, the graph-based methods obtain sense-interdependent solution for target words.", "labels": [], "entities": []}, {"text": "However, those methods mainly focus on modeling sense distribution and have less attention to contextual smoothing/generalization beyond immediate context.", "labels": [], "entities": [{"text": "contextual smoothing/generalization", "start_pos": 94, "end_pos": 129, "type": "TASK", "confidence": 0.7097923010587692}]}, {"text": "There exist several studies that enrich immediate context with large corpus statistics.", "labels": [], "entities": []}, {"text": "proposed a method to combine sense similarity with distributional similarity and configured predominant sense score.", "labels": [], "entities": [{"text": "predominant sense score", "start_pos": 92, "end_pos": 115, "type": "METRIC", "confidence": 0.5913310547669729}]}, {"text": "Distributional similarity was used to weight the influence of context words, based on large-scale statistics.", "labels": [], "entities": []}, {"text": "The method achieved successful WSD accuracy.", "labels": [], "entities": [{"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9030609130859375}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9729833602905273}]}, {"text": "used a k-nearest words on distributional similarity as context words.", "labels": [], "entities": []}, {"text": "They apply a LKB graph-based WSD to a target word together with the distributional context words, and showed that it yields better results on a domain dataset than just using immediate context words.", "labels": [], "entities": []}, {"text": "Though these studies are word-by-word WSD for target words, they demonstrated the effectiveness to enrich immediate context by corpus statistics.", "labels": [], "entities": [{"text": "WSD", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.7920860648155212}]}, {"text": "This paper proposes a smoothing model that integrates dictionary-based semantic similarity and corpus-based context statistics, where a combinatorial optimization scheme is employed to deal with sense interdependency of the all-words WSD task.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first describe our smoothing model in the following section.", "labels": [], "entities": []}, {"text": "The combinatorial optimization method with the model is described in Section 3.", "labels": [], "entities": [{"text": "combinatorial optimization", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.7343851029872894}]}, {"text": "Section 4 describes a specific implementation for evaluation.", "labels": [], "entities": []}, {"text": "The evaluation is performed with the SemEval-2 English all-words dataset.", "labels": [], "entities": [{"text": "SemEval-2 English all-words dataset", "start_pos": 37, "end_pos": 72, "type": "DATASET", "confidence": 0.5824417620897293}]}, {"text": "We present the performance in Section 5.", "labels": [], "entities": []}, {"text": "In Section 6 we discuss whether the intended context-to-sense mapping and the sense-interdependency are properly modeled.", "labels": [], "entities": []}, {"text": "Finally we review related studies in Section 7 and conclude in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "To confirm the effect of the proposed smoothing model and its combinatorial optimization scheme, we conducted WSD evaluations.", "labels": [], "entities": [{"text": "WSD", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.670283854007721}]}, {"text": "The primary evaluations compare our method with conventional ones, in Section 5.2.", "labels": [], "entities": []}, {"text": "Supplementary evaluations are described in the subsequent sections that include the comparison with SemEval-2 participating systems, and the analysis of model dynamics with the experimental data.", "labels": [], "entities": []}, {"text": "To make the evaluation comparable to state-ofthe-art systems, we used the official dataset of the SemEval-2 English all-words WSD task ( , which is currently the latest public dataset available with published results.", "labels": [], "entities": [{"text": "SemEval-2 English all-words WSD task", "start_pos": 98, "end_pos": 134, "type": "TASK", "confidence": 0.5247338473796844}]}, {"text": "The dataset consists of test data and background documents of the same environment domain.", "labels": [], "entities": []}, {"text": "The test data consists of 1,398 target words (1,032 nouns and 366 verbs) in 5.3K running words.", "labels": [], "entities": []}, {"text": "The background documents consists of 2.7M running words, which was used to compute distributional similarity.", "labels": [], "entities": []}, {"text": "Precisions and recalls were all computed using the official evaluation tool scorer2 in finegrained measure.", "labels": [], "entities": [{"text": "Precisions", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9902468919754028}, {"text": "recalls", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9913020133972168}, {"text": "scorer2", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.8591476082801819}, {"text": "finegrained measure", "start_pos": 87, "end_pos": 106, "type": "METRIC", "confidence": 0.9597139060497284}]}, {"text": "The tool accepts answers either in probabilistic format (senses with probabilities for each target word) or in deterministic format (most likely senses, with no score information).", "labels": [], "entities": []}, {"text": "As the proposed method is a probability model, we evaluated in the probabilistic way unless explicitly noted otherwise.", "labels": [], "entities": []}, {"text": "For this reason, we evaluated all the sense probabilities as they were.", "labels": [], "entities": []}, {"text": "Disambiguations were executed in separate runs for nouns and verbs, because no interaction takes place across POS in this metric implementation.", "labels": [], "entities": []}, {"text": "The two runs' results were combined later to a single answer to be input to scorer2.", "labels": [], "entities": [{"text": "scorer2", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.6437280774116516}]}, {"text": "The context metric space was composed by knearest neighbor words of distributional similarity, as is described in Section 4.", "labels": [], "entities": []}, {"text": "The value of k was evaluated for.", "labels": [], "entities": []}, {"text": "As for sense metric space, we evaluated two measures i.e.,) denoted as JCN, and denoted as Lesk.", "labels": [], "entities": []}, {"text": "In every condition, stopping criterion of iteration is always the number of iteration (500 times), irrespective of the convergence in likelihood.", "labels": [], "entities": [{"text": "stopping criterion", "start_pos": 20, "end_pos": 38, "type": "METRIC", "confidence": 0.9784015715122223}]}, {"text": "Primary evaluations compared our method with two conventional methods.", "labels": [], "entities": []}, {"text": "Those methods differ to ours only in scoring schemes.", "labels": [], "entities": []}, {"text": "The first one is the method by, which determines the word sense based on sense similarity and distributional similarity to the k-nearest neighbor words of a target word by distributional similarity.", "labels": [], "entities": []}, {"text": "Our major advantage is the combinatorial optimization framework, while the conventional one employs word-by-word scheme.", "labels": [], "entities": [{"text": "combinatorial optimization", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.6570479720830917}]}, {"text": "The second one is based on the method by, which determines the word sense by maximizing the sum of sense similarity to the k immediate neighbor words of a target word.", "labels": [], "entities": []}, {"text": "The k words were forced to be selected from other target words of the same POS to the word of interest, so as to make information resource equivalent to the other comparable two methods.", "labels": [], "entities": []}, {"text": "It is also a wordby-word method.", "labels": [], "entities": []}, {"text": "It exploits no distributional similarity.", "labels": [], "entities": []}, {"text": "Our major advantages are the combinatorial optimization scheme and the smoothing model to integrate distributional similarity.", "labels": [], "entities": []}, {"text": "In the following section, these comparative methods are referred to as Mc2004 and Pat2007, respectively.", "labels": [], "entities": [{"text": "Mc2004", "start_pos": 71, "end_pos": 77, "type": "DATASET", "confidence": 0.8319789171218872}, {"text": "Pat2007", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.7114861607551575}]}], "tableCaptions": [{"text": " Table 1: Comparison with the top-5 knowledge- based systems in SemEval-2 (JCN/k = 5).", "labels": [], "entities": []}]}