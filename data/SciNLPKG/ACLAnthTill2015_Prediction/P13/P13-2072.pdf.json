{"title": [{"text": "Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.790439635515213}]}], "abstractContent": [{"text": "In this paper we show how to automatically induce non-linear features for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7927741408348083}]}, {"text": "The new features are selected to approximately maximize a BLEU-related objective and decompose on the level of local phrases, which guarantees that the asymptotic complexity of machine translation decoding does not increase.", "labels": [], "entities": [{"text": "BLEU-related", "start_pos": 58, "end_pos": 70, "type": "METRIC", "confidence": 0.9949872493743896}]}, {"text": "We achieve this by applying gradient boosting machines (Friedman, 2000) to learn new weak learners (features) in the form of regression trees, using a differen-tiable loss function related to BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9961940050125122}]}, {"text": "Our results indicate that small gains in performance can be achieved using this method but we do not seethe dramatic gains observed using feature induction for other important machine learning tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "The linear model for machine translation) has become the de-facto standard in the field.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8142909109592438}]}, {"text": "Recently, researchers have proposed a large number of additional features) and parameter tuning methods ( which are better able to scale to the larger parameter space.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.848363608121872}]}, {"text": "However, a significant feature engineering effort is still required from practitioners.", "labels": [], "entities": []}, {"text": "When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example,).", "labels": [], "entities": []}, {"text": "In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (; Wu * This research was conducted during the author's internship at Microsoft Research et al., 2010).", "labels": [], "entities": [{"text": "web search ranking", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.6234594980875651}]}, {"text": "Here we adapt the main insights of such work to the machine translation setting and share results on two language pairs.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7737999260425568}]}, {"text": "Some recent works have attempted to relax the linearity assumption on MT features, by defining non-parametric models on complete translation hypotheses, for use in an nbest re-ranking setting.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9730527400970459}]}, {"text": "In this paper we develop a framework for inducing non-linear features in the form of regression decision trees, which decompose locally and can be integrated efficiently in decoding.", "labels": [], "entities": []}, {"text": "The regression trees encode nonlinear feature combinations of the original features.", "labels": [], "entities": []}, {"text": "We build on the work by which shows how to induce features to minimize any differentiable loss function.", "labels": [], "entities": []}, {"text": "In our application the features are regression decision trees, and the loss function is the pairwise ranking log-loss from the PRO method for parameter tuning).", "labels": [], "entities": []}, {"text": "Additionally, we show how to design the learning process such that the induced features are local on phrase-pairs and their language model and reordering context, and thus can be incorporated in decoding efficiently.", "labels": [], "entities": []}, {"text": "Our results using re-ranking on two language pairs show that the feature induction approach can bring small gains in performance.", "labels": [], "entities": [{"text": "feature induction", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.695000410079956}]}, {"text": "Overall, even though the method shows some promise, we do not seethe dramatic gains that have been seen for the web search ranking task (.", "labels": [], "entities": [{"text": "web search ranking task", "start_pos": 112, "end_pos": 135, "type": "TASK", "confidence": 0.7109452709555626}]}, {"text": "Further improvements in the original feature set and the induction algorithm, as well as full integration in decoding are needed to potentially result in substantial performance improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report experimental results on two language pairs: Chinese-English, and Finnish-English.", "labels": [], "entities": []}, {"text": "Table 1 summarizes statistics about the data.", "labels": [], "entities": []}, {"text": "For each language pair, we used a training set (Train) for extracting phrase tables and language models, a Dev-Train set for tuning feature weights and inducing features, a Dev-Select set for selecting hyperparameters of PRO tuning and selecting a stopping point and other hyperparameters of the boosting method, and a Test set for reporting final results.", "labels": [], "entities": []}, {"text": "For Chinese-English, the training corpus consists of approximately one million sentence pairs from the FBIS and HongKong portions of the LDC data for the NIST MT evaluation and the Dev-Train and Test sets are from NIST competitions.", "labels": [], "entities": [{"text": "FBIS", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.9036867022514343}, {"text": "LDC data", "start_pos": 137, "end_pos": 145, "type": "DATASET", "confidence": 0.7766518890857697}, {"text": "NIST MT evaluation", "start_pos": 154, "end_pos": 172, "type": "DATASET", "confidence": 0.7761165301005045}]}, {"text": "The MT system is a phrasal system with a 4-gram language model, trained on the Xinhua portion of the English Gigaword corpus.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.7592774629592896}, {"text": "English Gigaword corpus", "start_pos": 101, "end_pos": 124, "type": "DATASET", "confidence": 0.7844323714574178}]}, {"text": "The phrase table has maximum phrase length of 7 words on either side.", "labels": [], "entities": []}, {"text": "For Finnish-English we used a dataset from a technical domain of software manuals.", "labels": [], "entities": []}, {"text": "For this language pair we used two language models: one very large model trained on billions of words, and another language model trained from the target side of the parallel training set.", "labels": [], "entities": []}, {"text": "We report performance using the BLEU-SBP metric proposed in).", "labels": [], "entities": [{"text": "BLEU-SBP", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9975464940071106}]}, {"text": "This is a variant of BLEU () with strict brevity penalty, where along translation for one sentence cannot be used to counteract the brevity penalty for another sentence with a short translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9979528188705444}]}, {"text": "showed that this metric overcomes several undesirable properties of BLEU and has better correlation with human judgements.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9968519806861877}]}, {"text": "In our experiments with different feature sets and hyperparameters we observed more stable results and better correlation of Dev-Train, Dev-Select, and Test results using BLEU-SBP.", "labels": [], "entities": [{"text": "BLEU-SBP", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9810352921485901}]}, {"text": "For our experiments, we first trained weights for the base feature sets described in Section 2.2 using MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.5696595311164856}]}, {"text": "We then decoded the Dev-Train, Dev-Select, and Test datasets, generating 500-best lists for each set.", "labels": [], "entities": [{"text": "Test datasets", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.7772655785083771}]}, {"text": "All results in report performance of re-ranking on these 500-best lists using different feature sets and parameter tuning methods.", "labels": [], "entities": []}, {"text": "The baseline (base feature set) performance using MERT and PRO tuning on the two language pairs is shown on the first two lines.", "labels": [], "entities": [{"text": "MERT", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.8057372570037842}]}, {"text": "In line with prior work, PRO tuning achieves a bit lower scores on the tuning set but higher scores on the test set, compared to MERT.", "labels": [], "entities": [{"text": "PRO tuning", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.9377264082431793}, {"text": "MERT", "start_pos": 129, "end_pos": 133, "type": "DATASET", "confidence": 0.5379418134689331}]}, {"text": "The large feature set additionally contains over 170 manually specified features, described in Section 2.2.", "labels": [], "entities": []}, {"text": "It was infeasible to run MERT training on this feature set.", "labels": [], "entities": [{"text": "MERT training", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.872552901506424}]}, {"text": "The test set results using PRO tuning for the large set are about a quarter of a BLEU-SBP point higher than the results using the base feature set on both language pairs.", "labels": [], "entities": [{"text": "BLEU-SBP", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9991834759712219}]}, {"text": "Finally, the last two rows show the performance of the gradient boosting method.", "labels": [], "entities": [{"text": "gradient boosting", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7041630148887634}]}, {"text": "In addition to learning locally decomposable features boost-local, we also implemented boost-global, where we are learning combinations of the global feature values and lose decomposability.", "labels": [], "entities": []}, {"text": "The features learned by boost-global cannot be computed exactly on partial hypotheses in decoding and thus this method has a speed disadvantage, but we wanted to compare the performance of boostlocal and boost-global on n-best list re-ranking to seethe potential accuracy gain of the two methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 263, "end_pos": 271, "type": "METRIC", "confidence": 0.9985792636871338}]}, {"text": "We see that boost-local is slightly better in performance, in addition to being amenable to efficient decoder integration.", "labels": [], "entities": []}, {"text": "The gradient boosting results are mixed; for Finnish-English, we see around .2 gain of the boost-local model over the large feature set.", "labels": [], "entities": []}, {"text": "There is no improvement on Chinese-English, and the boost-global method brings slight degradation.", "labels": [], "entities": []}, {"text": "We did not see a large difference in performance among models using different decision tree leaf node types and different maximum numbers of leaf nodes.", "labels": [], "entities": []}, {"text": "The selected boost-local model for FIN-ENU used trees with maximum of 2 leaf nodes and linear leaf values; 25 new features were induced before performance started to degrade on the Dev-Select set.", "labels": [], "entities": [{"text": "FIN-ENU", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9297124147415161}]}, {"text": "The induced features for Finnish included combinations of language model and channel model scores, combinations of word count and channel model scores, and combinations of channel and lexicalized reordering scores.", "labels": [], "entities": []}, {"text": "For example, one feature increases the contribution of the relative frequency channel score for phrases with many target words, and decreases the channel model contribution for shorter phrases.", "labels": [], "entities": []}, {"text": "The best boost-local model for Chs-Enu used trees with a maximum of 2 constant-values leaf nodes, and induced 24 new tree features.", "labels": [], "entities": []}, {"text": "The features effectively promoted and demoted phrasepairs in context based on whether an input feature's value was smaller than a determined cutoff.", "labels": [], "entities": []}, {"text": "In conclusion, we proposed anew method to induce feature combinations for machine translation, which do not increase the decoding complexity.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7653824388980865}]}, {"text": "There were small improvements on one language pair in a re-ranking setting.", "labels": [], "entities": []}, {"text": "Further improvements in the original feature set and the induction algorithm, as well as full integration in decoding are needed to result in substantial performance improvements.", "labels": [], "entities": []}, {"text": "This work did not consider alternative ways of generating non-linear features, such as taking products of two or more input features.", "labels": [], "entities": []}, {"text": "It would be interesting to compare such alternatives to the regression tree features we explored.", "labels": [], "entities": []}, {"text": "Qiang Wu, Christopher J. Burges, Krysta M. Svore, and Jianfeng Gao.", "labels": [], "entities": []}, {"text": "2010. Adapting boosting for information retrieval measures.", "labels": [], "entities": [{"text": "Adapting boosting", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.934715747833252}, {"text": "information retrieval", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.8285883367061615}]}, {"text": "Information Retrieval, 13(3), June.", "labels": [], "entities": [{"text": "Information Retrieval, 13(3), June", "start_pos": 0, "end_pos": 34, "type": "DATASET", "confidence": 0.7373932003974915}]}], "tableCaptions": [{"text": " Table 1: Data sets for the two language pairs Chinese-", "labels": [], "entities": []}, {"text": " Table 2: Results for the two language pairs using different", "labels": [], "entities": []}]}