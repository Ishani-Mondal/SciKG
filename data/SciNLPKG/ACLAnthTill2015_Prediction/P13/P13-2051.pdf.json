{"title": [{"text": "Using Lexical Expansion to Learn Inference Rules from Sparse Data", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatic acquisition of inference rules for predicates is widely addressed by computing distributional similarity scores between vectors of argument words.", "labels": [], "entities": [{"text": "distributional similarity scores", "start_pos": 89, "end_pos": 121, "type": "METRIC", "confidence": 0.7328264911969503}]}, {"text": "In this scheme, prior work typically refrained from learning rules for low frequency predicates associated with very sparse argument vectors due to expected low reliability.", "labels": [], "entities": []}, {"text": "To improve the learning of such rules in an unsupervised way, we propose to lexically expand sparse argument word vectors with semantically similar words.", "labels": [], "entities": []}, {"text": "Our evaluation shows that lexical expansion significantly improves performance in comparison to state-of-the-art baselines.", "labels": [], "entities": [{"text": "lexical expansion", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.6975314170122147}]}], "introductionContent": [{"text": "The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA)) and Information Extraction (IE)).", "labels": [], "entities": [{"text": "Question Answering (QA))", "start_pos": 113, "end_pos": 137, "type": "TASK", "confidence": 0.8440881490707397}, {"text": "Information Extraction (IE))", "start_pos": 142, "end_pos": 170, "type": "TASK", "confidence": 0.8065169930458069}]}, {"text": "For example, the inference rule 'X treat Y \u2192 X relieve Y', between the templates 'X treat Y' and 'X relieve Y' maybe useful to identify the answer to \"Which drugs relieve stomach ache?\".", "labels": [], "entities": []}, {"text": "The predominant unsupervised approach for learning inference rules between templates is via distributional similarity ().", "labels": [], "entities": []}, {"text": "Specifically, each argument slot in a template is represented by an argument vector, containing the words (or terms) that instantiate this slot in all of the occurrences of the template in a learning corpus.", "labels": [], "entities": []}, {"text": "Two templates are then deemed semantically similar if the argument vectors of their corresponding slots are similar.", "labels": [], "entities": []}, {"text": "Ideally, inference rules should be learned for all templates that occur in the learning corpus.", "labels": [], "entities": []}, {"text": "However, many templates are rare and occur only few times in the corpus.", "labels": [], "entities": []}, {"text": "This is atypical NLP phenomenon that can be associated with either a small learning corpus, as in the cases of domain specific corpora and resource-scarce languages, or with templates with rare terms or long multi-word expressions such as 'X be also a risk factor to Y' or 'X finish second in Y', which capture very specific meanings.", "labels": [], "entities": []}, {"text": "Due to few occurrences, the slots of rare templates are represented with very sparse argument vectors, which in turn lead to low reliability in distributional similarity scores.", "labels": [], "entities": [{"text": "reliability", "start_pos": 129, "end_pos": 140, "type": "METRIC", "confidence": 0.9822721481323242}]}, {"text": "A common practice in prior work for learning predicate inference rules is to simply disregard templates below a minimal frequency threshold ().", "labels": [], "entities": []}, {"text": "Yet, acquiring rules for rare templates maybe beneficial both in terms of coverage, but also in terms of more accurate rule application, since rare templates are less ambiguous than frequent ones.", "labels": [], "entities": [{"text": "coverage", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9439480900764465}]}, {"text": "We propose to improve the learning of rules between infrequent templates by expanding their argument vectors.", "labels": [], "entities": []}, {"text": "This is done via a \"dual\" distributional similarity approach, in which we consider two words to be similar if they instantiate similar sets of templates.", "labels": [], "entities": []}, {"text": "We then use these similarities to expand the argument vector of each slot with words that were identified as similar to the original arguments in the vector.", "labels": [], "entities": []}, {"text": "Finally, similarities between templates are computed using the expanded vectors, resulting in a 'smoothed' version of the original similarity measure.", "labels": [], "entities": []}, {"text": "Evaluations on a rule application task show that our lexical expansion approach significantly improves the performance of the state-of-the-art DIRT algorithm ().", "labels": [], "entities": []}, {"text": "In addition, our approach outperforms a similarity measure based on vectors of latent topics instead of word vectors, a common way to avoid sparseness issues by means of dimensionality reduction.", "labels": [], "entities": []}], "datasetContent": [{"text": "We constructed a relatively small learning corpus for investigating the sparseness issues of such corpora.", "labels": [], "entities": []}, {"text": "To this end, we used a random sample from the large scale web-based ReVerb corpus), comprising tuple extractions of predicate templates with their argument instantiations.", "labels": [], "entities": [{"text": "ReVerb corpus", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.8891736567020416}]}, {"text": "We applied some clean-up preprocessing to these extractions, discarding stop words, rare words and non-alphabetical words that instantiated either the X or the Y argument slots.", "labels": [], "entities": []}, {"text": "In addition, we discarded templates that co-occur with less than 5 unique argument words in either of their slots, assuming that such few arguments cannot convey reliable semantic information, even with expansion.", "labels": [], "entities": []}, {"text": "Our final corpus consists of around 350,000 extractions and 14,000 unique templates.", "labels": [], "entities": []}, {"text": "In this corpus around one third of the extractions refer to templates that co-occur with at most 35 unique arguments in both their slots.", "labels": [], "entities": []}, {"text": "We evaluated the quality of inference rules using the dataset constructed by Zeichner et al.", "labels": [], "entities": []}, {"text": ", which contains about 6,500 manually annotated template rule applications, each labeled as corrector not.", "labels": [], "entities": []}, {"text": "For example, 'The game develop eye-hand coordination The game launch eye-hand coordination' is a rule application in this dataset of the rule 'X develop Y \u2192 X launch Y', labeled as incorrect, and 'Captain Cook sail to Australia \u2192 Captain Cook depart for Australia' is a rule application of the rule 'X sail to Y \u2192 X depart for Y', labeled as correct.", "labels": [], "entities": []}, {"text": "Specifically, we induced two datasets from Zeichner et al.'s dataset, denoted DS-5-35 and DS-5-50, which consist of all rule applications whose templates are present in our learning corpus and co-occurred with at least 5 and at most 35 and 50 unique argument words in both their slots, respectively.", "labels": [], "entities": []}, {"text": "DS-5-35 includes 311 rule applications (104 correct and 207 incorrect) and DS-5-50 includes 502 rule applications (190 correct and 312 incorrect).", "labels": [], "entities": [{"text": "DS-5-35", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9685544967651367}, {"text": "DS-5-50", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.9211981296539307}]}, {"text": "Our evaluation task is to rank all rule applications in each test set based on the similarity scores of the applied rules.", "labels": [], "entities": []}, {"text": "Optimal performance would rank all correct rule applications above the incorrect ones.", "labels": [], "entities": []}, {"text": "As a baseline for rule scoring we used the DIRT algorithm scheme, denoted DIRT-LE-None.", "labels": [], "entities": [{"text": "rule scoring", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.9516855776309967}]}, {"text": "We then compared between the performance of this baseline and its expanded versions, testing two similarity measures for generating the expansion sets of arguments: Lin and Cover.", "labels": [], "entities": []}, {"text": "We denote these expanded methods DIRT-LE-SIM-N, where SIM is the similarity measure used to generate the expansion sets and N is the lexical expansion degree, e.g. DIRT-LE-Lin-2.", "labels": [], "entities": []}, {"text": "We remind the reader that our scheme utilizes two similarity measures.", "labels": [], "entities": []}, {"text": "The first measure assesses the similarity between the argument vectors of the two templates in the rule.", "labels": [], "entities": []}, {"text": "This measure is kept constant in our experiments and is identical to DIRT's similarity measure (Lin).", "labels": [], "entities": [{"text": "similarity measure (Lin)", "start_pos": 76, "end_pos": 100, "type": "METRIC", "confidence": 0.796059501171112}]}, {"text": "The second measure assesses the similarity between words and is used for the lexical expansion of argument vectors.", "labels": [], "entities": []}, {"text": "Since this is the research goal of this paper, we experimented with two different measures for lexical expansion: asymmetric measure (Lin) and an asymmetric measure (Cover).", "labels": [], "entities": [{"text": "lexical expansion", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.6908688694238663}, {"text": "asymmetric measure (Lin)", "start_pos": 114, "end_pos": 138, "type": "METRIC", "confidence": 0.90400550365448}, {"text": "asymmetric measure (Cover)", "start_pos": 146, "end_pos": 172, "type": "METRIC", "confidence": 0.6836602091789246}]}, {"text": "To this end we evaluated their effect on DIRT's rule ranking performance and compared them to a vanilla version of DIRT without lexical expansion.", "labels": [], "entities": []}, {"text": "As another baseline, we follow Dinu and Lapata (2010) inducing LDA topic vectors for template slots and computing predicate template inference rule scores based on similarity between these vectors.", "labels": [], "entities": []}, {"text": "We use standard hyperparameters for learning the LDA model ().", "labels": [], "entities": []}, {"text": "This method is denoted LDA-K, where K is the number of topics in the model.", "labels": [], "entities": []}], "tableCaptions": []}