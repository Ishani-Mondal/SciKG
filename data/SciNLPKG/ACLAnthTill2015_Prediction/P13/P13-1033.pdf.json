{"title": [{"text": "A Markov Model of Machine Translation using Non-parametric Bayesian Inference", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7482651174068451}]}], "abstractContent": [{"text": "Most modern machine translation systems use phrase pairs as translation units, allowing for accurate modelling of phrase-internal translation and reordering.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7104182243347168}, {"text": "phrase-internal translation", "start_pos": 114, "end_pos": 141, "type": "TASK", "confidence": 0.7171365022659302}]}, {"text": "However phrase-based approaches are much less able to model sentence level effects between different phrase-pairs.", "labels": [], "entities": []}, {"text": "We propose anew model to address this imbalance , based on a word-based Markov model of translation which generates target translations left-to-right.", "labels": [], "entities": []}, {"text": "Our model encodes word and phrase level phenomena by conditioning translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing.", "labels": [], "entities": []}, {"text": "This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source.", "labels": [], "entities": []}, {"text": "Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9980542659759521}]}], "introductionContent": [{"text": "Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based () and syntax-based approaches;).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6943629086017609}]}, {"text": "These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches.", "labels": [], "entities": [{"text": "sentence translation", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7392796277999878}, {"text": "phrase translation", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.7376655042171478}]}, {"text": "In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9856797456741333}]}, {"text": "This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on context.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.7250757217407227}]}, {"text": "On one hand, the use of phrases can memorize local context and hence helps to generate better translation compared to word-based models.", "labels": [], "entities": []}, {"text": "On the other hand, this mechanism requires each phrase to be matched strictly and to be used as a whole, which precludes the use of discontinuous phrases and leads to poor generalisation to unseen data (where large phrases tend not to match).", "labels": [], "entities": []}, {"text": "In this paper we propose anew model to drop the independence assumption, by instead modelling correlations between translation decisions, which we use to induce translation derivations from aligned sentences (akin to word alignment).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 217, "end_pos": 231, "type": "TASK", "confidence": 0.7147568911314011}]}, {"text": "We develop a Markov model over translation decisions, in which each decision is conditioned on previous n most recent decisions.", "labels": [], "entities": [{"text": "translation decisions", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.9325880706310272}]}, {"text": "Our approach employs a sophisticated Bayesian non-parametric prior, namely the hierarchical Pitman-Yor Process) to represent backoff from larger to smaller contexts.", "labels": [], "entities": []}, {"text": "As a result, we need only use very simple translation units -primarily single words, but can still describe complex multi-word units through correlations between their component translation decisions.", "labels": [], "entities": []}, {"text": "We further decompose the process of generating each target word into component factors: finishing the translating, jumping elsewhere in the source, emitting a target word and deciding the fertility of the source words.", "labels": [], "entities": []}, {"text": "Overall our model has the following features: 1.", "labels": [], "entities": []}, {"text": "enabling model parameters to be shared between similar translation decisions, thereby obtaining more reliable statistics and generalizing better from small training sets.", "labels": [], "entities": []}, {"text": "2. learning a much richer set of translation fragments, such as gapping phrases, e.g., the translation for the German werde . .", "labels": [], "entities": []}, {"text": "ankommen in English is will arrive . .", "labels": [], "entities": []}, {"text": "3. providing a unifying framework spanning word-based and phrase-based model of translation, while incorporating explicit transla-tion, insertion, deletion and reordering components.", "labels": [], "entities": []}, {"text": "We demonstrate our model on Chinese-English and Arabic-English translation datasets.", "labels": [], "entities": []}, {"text": "The model produces uniformly better translations than those of a competitive phrase-based baseline, amounting to an improvement of up to 3.4 BLEU points absolute.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9991397857666016}]}], "datasetContent": [{"text": "In principle our model could be directly used as a MT decoder or as a feature in a decoder.", "labels": [], "entities": [{"text": "MT decoder", "start_pos": 51, "end_pos": 61, "type": "TASK", "confidence": 0.7698909044265747}]}, {"text": "However in this paper we limit our focus to inducing word alignments, i.e., by using the model to infer alignments which are then used in a standard phrasebased translation pipeline.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7304119169712067}, {"text": "phrasebased translation pipeline", "start_pos": 149, "end_pos": 181, "type": "TASK", "confidence": 0.7598986029624939}]}, {"text": "We leave full decoding for later work, which we anticipate would further improve performance by exploiting gapping phrases and other phenomena that implicitly form part of our model but are not represented in the phrase-based decoder.", "labels": [], "entities": []}, {"text": "Decoding under our model would be straight-forward in principle, as the generative process was designed to closely parallel the search procedure in the phrase-based model.", "labels": [], "entities": []}, {"text": "Three data sets were used in the experiments: two Chinese to English data sets on small (IWSLT) and larger corpora (FBIS), and Arabic to English translation.", "labels": [], "entities": []}, {"text": "Our experiments seek to test how the model compares to a GIZA++ baseline, quantifies the effect of each factor in the probabilistic model (i.e., jump, fertility), and the effect of different initialisations of the sampler.", "labels": [], "entities": [{"text": "GIZA++ baseline", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.8523309032122294}]}, {"text": "We present results on translation quality and word alignment.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8161531984806061}, {"text": "word alignment", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7787077724933624}]}], "tableCaptions": [{"text": " Table 2: Impact of adding factors to our Markov  model, showing BLEU scores on IWSLT. Key: f s - finish e-emission j-jump f t -fertility.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9993461966514587}, {"text": "IWSLT", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.8548779487609863}, {"text": "finish e-emission j-jump f t -fertility", "start_pos": 98, "end_pos": 137, "type": "METRIC", "confidence": 0.6485266004289899}]}, {"text": " Table 3: Machine translation performance in  BLEU % on the IWSLT 2005 Chinese-English test  set. The Gibbs samplers were initialized with three  different alignments, shown as columns.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7699398100376129}, {"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9982494711875916}, {"text": "IWSLT 2005 Chinese-English test  set", "start_pos": 60, "end_pos": 96, "type": "DATASET", "confidence": 0.9729589700698853}]}, {"text": " Table 4: Translation performance on Chinese to  English translation, showing BLEU% for models  trained on the FBIS data set.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9673448204994202}, {"text": "Chinese to  English translation", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.6578783839941025}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9997231364250183}, {"text": "FBIS data set", "start_pos": 111, "end_pos": 124, "type": "DATASET", "confidence": 0.9180832902590433}]}, {"text": " Table 5: Translation performance on Arabic to  English translation, showing BLEU%. Also shown  is word-alignment alignment accuracy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9998043179512024}, {"text": "word-alignment alignment", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.6990640759468079}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9651955366134644}]}]}