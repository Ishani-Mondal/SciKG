{"title": [{"text": "Modeling of term-distance and term-occurrence information for im- proving n-gram language model performance", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7104800194501877}]}, {"text": "We attempt to extract this information from history-contexts of up to ten words in size, and found it complements well the n-gram model, which inherently suffers from data scarcity in learning long history contexts.", "labels": [], "entities": []}, {"text": "Evaluated on the WSJ corpus, bi-gram and trigram model perplexity were reduced up to 23.5% and 14.0%, respectively.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.9757408797740936}]}, {"text": "Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models have been extensively studied in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.6570131182670593}]}, {"text": "The role of a language model is to measure how probably a (target) word would occur based on some given evidence extracted from the history-context.", "labels": [], "entities": []}, {"text": "The commonly used n-gram model () takes the immediately preceding history-word sequence, of length , as the evidence for prediction.", "labels": [], "entities": [{"text": "prediction", "start_pos": 121, "end_pos": 131, "type": "TASK", "confidence": 0.9642614722251892}]}, {"text": "Although n-gram models are simple and effective, modeling long history-contexts lead to severe data scarcity problems.", "labels": [], "entities": []}, {"text": "Hence, the context length is commonly limited to as short as three, i.e. the trigram model, and any useful information beyond this window is neglected.", "labels": [], "entities": []}, {"text": "In this work, we explore the possibility of modeling the presence of a history-word in terms of: (1) the distance and (2) the co-occurrence, with a target-word.", "labels": [], "entities": []}, {"text": "These two attributes will be exploited and modeled independently from each other, i.e. the distance is described regardless the actual frequency of the history-word, while the co-occurrence is described regardless the actual position of the history-word.", "labels": [], "entities": []}, {"text": "We refer to these two attributes as the term-distance (TD) and the term-occurrence (TO) components, respectively.", "labels": [], "entities": [{"text": "term-distance (TD)", "start_pos": 40, "end_pos": 58, "type": "METRIC", "confidence": 0.7971494942903519}, {"text": "term-occurrence (TO)", "start_pos": 67, "end_pos": 87, "type": "METRIC", "confidence": 0.762613520026207}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "The following section presents the most relevant related works.", "labels": [], "entities": []}, {"text": "Section 3 introduces and motivates our proposed approach.", "labels": [], "entities": []}, {"text": "Section 4 presents in detail the derivation of both TD and TO model components.", "labels": [], "entities": []}, {"text": "Section 5 presents some perplexity evaluation results.", "labels": [], "entities": []}, {"text": "Finally, section 6 presents our conclusions and proposed future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "A perplexity test was run on the BLLIP WSJ corpus (Charniak 2000) with the standard 5K vocabulary.", "labels": [], "entities": [{"text": "BLLIP WSJ corpus (Charniak 2000)", "start_pos": 33, "end_pos": 65, "type": "DATASET", "confidence": 0.9151401604924884}]}, {"text": "The entire WSJ '87 data (740K sentences 18M words) was used as train-set to train the n-gram, TD, and TO models.", "labels": [], "entities": [{"text": "WSJ '87 data", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.9360209405422211}]}, {"text": "The dev-set and the test-set, each comprising 500 sentences and about 12K terms, were selected randomly from WSJ '88 data.", "labels": [], "entities": [{"text": "WSJ '88 data", "start_pos": 109, "end_pos": 121, "type": "DATASET", "confidence": 0.9593897610902786}]}, {"text": "We used them for parameter finetuning and performance evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Perplexities of the n-gram model (NG)  of order (N) two to six and their combinations  with the TD, TO, and TD-TO models.", "labels": [], "entities": []}, {"text": " Table 2. Perplexities of the distant bigram (DBG)  and TD models when interpolated with a stan- dard bigram model.", "labels": [], "entities": []}]}