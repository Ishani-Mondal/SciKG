{"title": [{"text": "What causes a causal relation? Detecting Causal Triggers in Biomedical Scientific Discourse", "labels": [], "entities": [{"text": "Detecting Causal Triggers in Biomedical Scientific Discourse", "start_pos": 31, "end_pos": 91, "type": "TASK", "confidence": 0.8355495844568525}]}], "abstractContent": [{"text": "Current domain-specific information extraction systems represent an important resource for biomedical researchers, who need to process vaster amounts of knowledge in short times.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7248260527849197}]}, {"text": "Automatic discourse causality recognition can further improve their workload by suggesting possible causal connections and aiding in the curation of pathway models.", "labels": [], "entities": [{"text": "discourse causality recognition", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.6694397230943044}]}, {"text": "We here describe an approach to the automatic identification of discourse causality triggers in the biomedical domain using machine learning.", "labels": [], "entities": [{"text": "automatic identification of discourse causality triggers", "start_pos": 36, "end_pos": 92, "type": "TASK", "confidence": 0.7217118442058563}]}, {"text": "We create several baselines and experiment with various parameter settings for three algorithms, i.e., Conditional Random Fields (CRF), Support Vector Machines (SVM) and Random Forests (RF).", "labels": [], "entities": []}, {"text": "Also, we evaluate the impact of lexical, syntactic and semantic features on each of the algorithms and look at errors.", "labels": [], "entities": []}, {"text": "The best performance of 79.35% F-score is achieved by CRFs when using all three feature types.", "labels": [], "entities": [{"text": "F-score", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.998897910118103}]}], "introductionContent": [{"text": "The need to provide automated, efficient and accurate means of retrieving and extracting user-oriented biomedical knowledge has significantly increased according to the ever-increasing amount of knowledge pusblished daily in the form of research articles (.", "labels": [], "entities": []}, {"text": "Biomedical text mining has seen significant recent advancements in recent years (, including named entity recognition (), coreference resolution () and relation () and event extraction ().", "labels": [], "entities": [{"text": "Biomedical text mining", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8478947877883911}, {"text": "named entity recognition", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.6496789654095968}, {"text": "coreference resolution", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.9513894617557526}, {"text": "event extraction", "start_pos": 168, "end_pos": 184, "type": "TASK", "confidence": 0.7746813297271729}]}, {"text": "Using biomedical text mining technology, text can now be enriched via the addition of semantic metadata and thus can support tasks such as analysing molecular pathways () and semantic searching ().", "labels": [], "entities": [{"text": "biomedical text mining", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.638645718495051}, {"text": "semantic searching", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.785247266292572}]}, {"text": "However, more complex tasks, such as question answering and automatic summarisation, require the extraction of information that spans across several sentences, together with the recognition of relations that exist across sentence boundaries, in order to achieve high levels of performance.", "labels": [], "entities": [{"text": "question answering", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8972959816455841}, {"text": "summarisation", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.6766098141670227}]}, {"text": "The notion of discourse can be defined as a coherent sequence of clauses and sentences.", "labels": [], "entities": []}, {"text": "These are connected in a logical manner by discourse relations, such as causal, temporal and conditional, which characterise how facts in text are related.", "labels": [], "entities": []}, {"text": "In turn, these help readers infer deeper, more complex knowledge about the facts mentioned in the discourse.", "labels": [], "entities": []}, {"text": "These relations can be either explicit or implicit, depending whether or not they are expressed in text using overt discourse connectives (also known as triggers).", "labels": [], "entities": []}, {"text": "Take, for instance, the casein example (1), where the trigger Therefore signals a justification between the two sentences: because \"a normal response to mild acid pH from PmrB requires both a periplasmic histidine and several glutamic acid residues\", the authors believe that the \"regulation of PmrB activity could involve protonation of some amino acids\".", "labels": [], "entities": []}, {"text": "(1) In the case of PmrB, a normal response to mild acid pH requires not only a periplasmic histidine but also several glutamic acid residues.", "labels": [], "entities": []}, {"text": "Therefore, regulation of PmrB activity may involve protonation of one or more of these amino acids.", "labels": [], "entities": []}, {"text": "Thus, by identifying this causal relation, search engines become able to discover relations between biomedical entities and events or between experimental evidence and associated conclusions.", "labels": [], "entities": []}, {"text": "However, phrases acting as causal triggers in certain contexts may not denote causality in all cases.", "labels": [], "entities": []}, {"text": "Therefore, a dictionary-based approach is likely to produce a very high number of false positives.", "labels": [], "entities": []}, {"text": "In this paper, we explore several supervised machinelearning approaches to the automatic identification of triggers that actually denote causality.", "labels": [], "entities": []}], "datasetContent": [{"text": "We explored with various machine learning algorithms and various settings for the task of identifying causal triggers.", "labels": [], "entities": []}, {"text": "On the one hand, we experimented with CRF (), a probabilistic modelling framework commonly used for sequence labelling tasks.", "labels": [], "entities": [{"text": "sequence labelling tasks", "start_pos": 100, "end_pos": 124, "type": "TASK", "confidence": 0.7294718821843466}]}, {"text": "In this work, we employed the CRFSuite implementation 1 . On the other hand, we modelled trigger detection as a classification task, using Support Vector Machines and Random Forests.", "labels": [], "entities": [{"text": "trigger detection", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.8820627927780151}]}, {"text": "More specifically, we employed the implementation in Weka () for RFs, and Lib-SVM (Chang and Lin, 2011) for SVMs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of various classifiers in identifying  causal connectives", "labels": [], "entities": [{"text": "identifying  causal connectives", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.7294345498085022}]}, {"text": " Table 2: Effect of feature types on the sequence labelling  task, given in percentages.", "labels": [], "entities": [{"text": "sequence labelling  task", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.7127176423867544}]}, {"text": " Table 3: Effect of feature types on Random Forests.", "labels": [], "entities": []}, {"text": " Table 4. As can be observed,  the best performance is obtained when combining  the lexical and semantic feature types (69.85% F- score). The combination of all features produces the  best precision, whilst the best recall is obtained by  combining lexical and semantic features.", "labels": [], "entities": [{"text": "F- score", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9929878115653992}, {"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.999278724193573}, {"text": "recall", "start_pos": 216, "end_pos": 222, "type": "METRIC", "confidence": 0.9987336993217468}]}, {"text": " Table 4: Effect of feature types on SVM.", "labels": [], "entities": []}]}