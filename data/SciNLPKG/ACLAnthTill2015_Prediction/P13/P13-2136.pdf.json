{"title": [{"text": "An Information Theoretic Approach to Bilingual Word Clustering", "labels": [], "entities": [{"text": "Bilingual Word Clustering", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.6971947948137919}]}], "abstractContent": [{"text": "We present an information theoretic objective for bilingual word clustering that incorporates both monolingual distributional evidence as well as cross-lingual evidence from parallel corpora to learn high quality word clusters jointly in any number of languages.", "labels": [], "entities": [{"text": "bilingual word clustering", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6183522343635559}]}, {"text": "The monolingual component of our objective is the average mutual information of clusters of adjacent words in each language, while the bilingual component is the average mutual information of the aligned clusters.", "labels": [], "entities": []}, {"text": "To evaluate our method, we use the word clusters in an NER system and demonstrate a statistically significant improvement in F 1 score when using bilingual word clusters instead of monolingual clusters.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9915181994438171}]}], "introductionContent": [{"text": "A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group.", "labels": [], "entities": []}, {"text": "Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (, and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages.", "labels": [], "entities": [{"text": "Word clustering", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6743285357952118}, {"text": "modeling of translational correspondences", "start_pos": 198, "end_pos": 239, "type": "TASK", "confidence": 0.6682814806699753}]}, {"text": "In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters.", "labels": [], "entities": []}, {"text": "We propose a novel bilingual word clustering objective ( \u00a72).", "labels": [], "entities": [{"text": "bilingual word clustering", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.6052373349666595}]}, {"text": "The first term deals with each language independently and ensures that the data is well-explained by the clustering in a sequence model ( \u00a72.1).", "labels": [], "entities": []}, {"text": "The second term ensures that the cluster alignments induced by a word alignment have high mutual information across languages ( \u00a72.2).", "labels": [], "entities": []}, {"text": "Since the objective consists of terms representing the entropy monolingual data (for each language) and parallel bilingual data, it is particularly attractive for the usual situation in which there is much more monolingual data available than parallel data.", "labels": [], "entities": []}, {"text": "Because of its similarity to the variation of information metric, we call this bilingual term in the objective the aligned variation of information.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation of clustering is not a trivial problem.", "labels": [], "entities": []}, {"text": "One branch of work seeks to recast the problem as the of part-of-speech (POS) induction and attempts to match linguistic intuitions.", "labels": [], "entities": [{"text": "part-of-speech (POS) induction", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.6690712749958039}]}, {"text": "However, hard clusters are particularly useful for downstream tasks (.", "labels": [], "entities": []}, {"text": "We therefore chose to focus our evaluation on the latter problem.", "labels": [], "entities": []}, {"text": "For our evaluation, we use our word clusters as an input to a named entity recognizer which uses these clusters as a source of features.", "labels": [], "entities": []}, {"text": "Our evaluation task is the German corpus with NER annotation that was created for the shared task at CoNLL-2003 3 . The training set contains approximately 220,000 tokens and the development set and test set contains 55,000 tokens each.", "labels": [], "entities": [{"text": "German corpus", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.8646099865436554}, {"text": "CoNLL-2003 3", "start_pos": 101, "end_pos": 113, "type": "DATASET", "confidence": 0.8546137809753418}]}, {"text": "We use Stanford's Named Entity Recognition system 4 which uses a linear-chain conditional random field to predict the most likely sequence of NE labels ().", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.6196573277314504}]}, {"text": "Corpora for Clustering: We used parallel corpora for {Arabic, English, French, Korean & Turkish}-German pairs from WIT-3 corpus (Cettolo et al., 2012) 5 , which is a collection of translated transcriptions of TED talks.", "labels": [], "entities": [{"text": "WIT-3 corpus (Cettolo et al., 2012) 5", "start_pos": 115, "end_pos": 152, "type": "DATASET", "confidence": 0.8286762058734893}]}, {"text": "Each language pair contained around 1.5 million German words.", "labels": [], "entities": []}, {"text": "The corpus was word aligned in two directions using an unsupervised word aligner (, then the intersected alignment points were taken.", "labels": [], "entities": []}, {"text": "Monolingual Clustering: For every language pair, we train German word clusters on the monolingual German data from the parallel data.", "labels": [], "entities": []}, {"text": "Note that the parallel corpora are of different sizes and hence the monolingual German data from every parallel corpus is different.", "labels": [], "entities": []}, {"text": "We treat the F 1 score obtained using monolingual word clusters (\u03b2 = 0) as the baseline.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9695539673169454}]}, {"text": "shows the F 1 score of NER 6 when trained on these monolingual German word clusters.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.991153339544932}, {"text": "NER 6", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.6336031556129456}]}, {"text": "Bilingual Clustering: While we have formulated a joint objective that enables using both monolingual and bilingual evidence, it is possible to create word clusters using the bilingual signal only by removing the first term in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 226, "end_pos": 228, "type": "DATASET", "confidence": 0.9028517007827759}]}, {"text": "Table 1 shows the performance of NER when the word clusters are obtained using only the bilingual information for different language pairs.", "labels": [], "entities": [{"text": "NER", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9736126661300659}]}, {"text": "As can be seen, these clusters are helpful for all the language pairs.", "labels": [], "entities": []}, {"text": "For Turkish the F 1 score improves by 1.0 point over when there are no distributional clusters which clearly shows that the word alignment information improves the clustering quality.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9884179830551147}, {"text": "word alignment", "start_pos": 124, "end_pos": 138, "type": "TASK", "confidence": 0.6957319378852844}]}, {"text": "We now need to supplement the bilingual information with monolingual information to see if the improvement sustains.", "labels": [], "entities": []}, {"text": "We varied the weight of the bilingual objective (\u03b2) from 0.05 to 0.9 and observed the effect in NER performance on English-German language pair.", "labels": [], "entities": [{"text": "bilingual objective (\u03b2)", "start_pos": 28, "end_pos": 51, "type": "METRIC", "confidence": 0.8217367172241211}, {"text": "NER", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9474210143089294}]}, {"text": "The F 1 score is maximum for \u03b2 = 0.1 and decreases monotonically when \u03b2 is either increased or decreased.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9895841876665751}]}, {"text": "This indicates that bilingual information is helpful, but less valuable than monolingual information.", "labels": [], "entities": []}, {"text": "Preliminary experiments showed that the value of \u03b2 = 0.1 is fairly robust across other language pairs and hence we fix it to that for all the experiments.", "labels": [], "entities": []}, {"text": "We run our bilingual clustering model show that for the size of our generalization data in German-NER, K = 100 should give us the optimum value.", "labels": [], "entities": []}, {"text": "0.1) across all language pairs and note the F 1 scores.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9770647684733073}]}, {"text": "(unrefined) shows that except for Arabic-German & French-German, all other language pairs deliver a better F 1 score than only using monolingual German data.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9914109309514364}]}, {"text": "In case of ArabicGerman there is a drop in score by 0.25 points.", "labels": [], "entities": [{"text": "ArabicGerman", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.9636626243591309}, {"text": "drop in score", "start_pos": 35, "end_pos": 48, "type": "METRIC", "confidence": 0.7684086163838705}]}, {"text": "Although, we have observed improvement in F 1 score over the monolingual case, the gains do not reach significance according to McNemar's test).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9914359450340271}, {"text": "McNemar's test", "start_pos": 128, "end_pos": 142, "type": "DATASET", "confidence": 0.8268930117289225}]}, {"text": "Thus we propose to further refine the quality of word alignment links as follows: Let x be a word in language \u03a3 and y be a word in language \u2126 and let there exists an alignment link between x and y.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.6752409040927887}]}, {"text": "Recall that A(x, y) is the count of the alignment links between x and y observed in the parallel data, and A(x) and A(y) are the respective marginal counts.", "labels": [], "entities": [{"text": "A", "start_pos": 12, "end_pos": 13, "type": "METRIC", "confidence": 0.9934074878692627}, {"text": "A", "start_pos": 107, "end_pos": 108, "type": "METRIC", "confidence": 0.9774093627929688}]}, {"text": "Then we define an edge association weight e(x, y) = 2\u00d7A(x,y) A(x)+A(y) This quantity is an association of the strength of the relationship between x and y, and we use it to remove all alignment links whose e(x, y) is below a given threshold before running the bilingual clustering model.", "labels": [], "entities": []}, {"text": "We vary e from 0.1 to 0.7 and observe the new F 1 scores on the development data.: NER performance using different word clustering models.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9521589875221252}, {"text": "NER", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.9396303296089172}, {"text": "word clustering", "start_pos": 115, "end_pos": 130, "type": "TASK", "confidence": 0.6901500672101974}]}, {"text": "Bold indicates an improvement over the monolingual (\u03b2 = 0) baseline; \u2020 indicates a significant improvement (McNemar's test, p < 0.01).", "labels": [], "entities": [{"text": "McNemar's test", "start_pos": 108, "end_pos": 122, "type": "METRIC", "confidence": 0.5684238970279694}]}, {"text": "see that the optimal value of e changes from one language pair to another.", "labels": [], "entities": []}, {"text": "For French and English e = 0.1 gives the best results whereas for Turkish and Arabic e = 0.5 and for Korean e = 0.7.", "labels": [], "entities": []}, {"text": "Are these thresholds correlated with anything?", "labels": [], "entities": []}, {"text": "We suggest that higher values of e correspond to more intrinsically noisy alignments.", "labels": [], "entities": []}, {"text": "Since alignment models are parameterized based on the vocabularies of the languages they are aligning, larger vocabularies are more prone to degenerate solutions resulting from overfitting.", "labels": [], "entities": []}, {"text": "So we are not surprised to see that sparser alignments (resulting from higher values of e) are required by languages like Korean, while languages like French and English make due with denser alignments.", "labels": [], "entities": []}, {"text": "Evaluation on Test Set: We now verify our results on the test set.", "labels": [], "entities": []}, {"text": "We take the best bilingual word clustering model obtained for every language pair (e = 0.1 for En, Fr. e = 0.5 for Ar, Tr. e = 0.7 for Ko) and train NER classifiers using these.", "labels": [], "entities": [{"text": "bilingual word clustering", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.6642687618732452}, {"text": "NER classifiers", "start_pos": 149, "end_pos": 164, "type": "TASK", "confidence": 0.7019415199756622}]}, {"text": "shows the performance of German NER classifiers on the test set.", "labels": [], "entities": []}, {"text": "All the values shown in bold are better than the monolingual baselines.", "labels": [], "entities": []}, {"text": "English again has a statistically significant improvement over the baseline.", "labels": [], "entities": []}, {"text": "French and Turkish show the next best improvements.", "labels": [], "entities": []}, {"text": "The English-German cluster model performs better than the mkcls 7 tool (72.83%).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: NER performance using different word clustering models. Bold indicates an improvement over  the monolingual (\u03b2 = 0) baseline;  \u2020 indicates a significant improvement (McNemar's test, p < 0.01).", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9755254983901978}, {"text": "word clustering", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7076443731784821}, {"text": "McNemar's test", "start_pos": 176, "end_pos": 190, "type": "DATASET", "confidence": 0.7141479253768921}]}]}