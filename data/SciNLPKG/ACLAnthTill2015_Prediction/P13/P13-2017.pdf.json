{"title": [{"text": "Universal Dependency Annotation for Multilingual Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "We present anew collection of treebanks with homogeneous syntactic dependency annotation for six languages: German, English, Swedish, Spanish, French and Korean.", "labels": [], "entities": []}, {"text": "To show the usefulness of such a resource, we present a case study of cross-lingual transfer parsing with more reliable evaluation than has been possible before.", "labels": [], "entities": [{"text": "cross-lingual transfer parsing", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.7958653569221497}]}, {"text": "This 'universal' treebank is made freely available in order to facilitate research on multilingual dependency parsing.", "labels": [], "entities": [{"text": "multilingual dependency parsing", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.6610631545384725}]}], "introductionContent": [{"text": "In recent years, syntactic representations based on head-modifier dependency relations between words have attracted a lot of interest.", "labels": [], "entities": []}, {"text": "Research in dependency parsing -computational methods to predict such representations -has increased dramatically, due in large part to the availability of dependency treebanks in a number of languages.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8378499448299408}]}, {"text": "In particular, the CoNLL shared tasks on dependency parsing have provided over twenty data sets in a standardized format.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7492244839668274}]}, {"text": "While these data sets are standardized in terms of their formal representation, they are still heterogeneous treebanks.", "labels": [], "entities": []}, {"text": "That is to say, despite them all being dependency treebanks, which annotate each sentence with a dependency tree, they subscribe to different annotation schemes.", "labels": [], "entities": []}, {"text": "This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions.", "labels": [], "entities": []}, {"text": "Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/.", "labels": [], "entities": [{"text": "1 Downloadable", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.755608469247818}]}, {"text": "analysis of coordination, verb groups, subordinate clauses, and multi-word expressions).", "labels": [], "entities": []}, {"text": "These data sets can be sufficient if one's goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate.", "labels": [], "entities": []}, {"text": "First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components.", "labels": [], "entities": []}, {"text": "Second, consistent syntactic representations are desirable in the evaluation of unsupervised () or cross-lingual syntactic parsers ().", "labels": [], "entities": []}, {"text": "In the cross-lingual study of, where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source.", "labels": [], "entities": []}, {"text": "In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes.", "labels": [], "entities": [{"text": "parsing Swedish", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.8601897358894348}]}, {"text": "In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages.", "labels": [], "entities": []}, {"text": "In terms of automatic construction, attempt to harmonize a large number of dependency treebanks by mapping their annotation to aversion of the Prague Dependency Treebank scheme.", "labels": [], "entities": [{"text": "Prague Dependency Treebank scheme", "start_pos": 143, "end_pos": 176, "type": "DATASET", "confidence": 0.9688163250684738}]}, {"text": "Additionally, there have been efforts to manually or semimanually construct resources with common syn-tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (.", "labels": [], "entities": []}, {"text": "In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish.", "labels": [], "entities": [{"text": "multilingual syntactic analysis", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.7120670676231384}]}, {"text": "This resource is freely available and we plan to extend it to include more data and languages.", "labels": [], "entities": []}, {"text": "In the context of part-of-speech tagging, universal representations, such as that of, have already spurred numerous examples of improved empirical cross-lingual systems (.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.752579003572464}]}, {"text": "We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.6778087317943573}]}, {"text": "First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of.", "labels": [], "entities": []}, {"text": "Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, suggesting that most of these studies underestimate true accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9582908749580383}]}, {"text": "Finally, unlike all previous cross-lingual studies, we can report full labeled accuracies and not just unlabeled structural accuracies.", "labels": [], "entities": []}], "datasetContent": [{"text": "One of the motivating factors in creating such a data set was improved cross-lingual transfer evaluation.", "labels": [], "entities": [{"text": "cross-lingual transfer evaluation", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.8567445079485575}]}, {"text": "To test this, we use a cross-lingual transfer parser similar to that of.", "labels": [], "entities": []}, {"text": "In particular, it is a perceptron-trained shift-reduce parser with abeam of size 8.", "labels": [], "entities": []}, {"text": "We use the features of Zhang and Nivre (2011), except that all lexical identities are dropped from the templates during training and testing, hence inducing a 'delexicalized' model that employs only 'universal' properties from source-side treebanks, such as part-ofspeech tags, labels, head-modifier distance, etc.", "labels": [], "entities": []}, {"text": "We ran a number of experiments, which can be seen in.", "labels": [], "entities": []}, {"text": "For these experiments we randomly split each data set into training, development and testing sets.", "labels": [], "entities": []}, {"text": "The one exception is English, where we used the standard splits.", "labels": [], "entities": []}, {"text": "Each row in represents a source training language and each column a target evaluation language.", "labels": [], "entities": []}, {"text": "We report both unlabeled attachment score (UAS) and labeled attachment score (LAS)).", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 15, "end_pos": 47, "type": "METRIC", "confidence": 0.8146282235781351}, {"text": "labeled attachment score (LAS))", "start_pos": 52, "end_pos": 83, "type": "METRIC", "confidence": 0.8692780137062073}]}, {"text": "This is likely the first reliable cross-lingual parsing evaluation.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.6859807670116425}]}, {"text": "In particular, previous studies could not even report LAS due to differences in treebank annotations.", "labels": [], "entities": [{"text": "LAS", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.61517733335495}]}, {"text": "We can make several interesting observations.", "labels": [], "entities": []}, {"text": "Most notably, for the Germanic and Romance target languages, the best source language is from the same language group.", "labels": [], "entities": []}, {"text": "This is in stark contrast to the results of, who observe that this is rarely the case with the heterogenous CoNLL treebanks.", "labels": [], "entities": [{"text": "CoNLL treebanks", "start_pos": 108, "end_pos": 123, "type": "DATASET", "confidence": 0.9032504260540009}]}, {"text": "Among the Germanic languages, it is interesting to note that Swedish is the best source language for both German and English, which makes sense from a typological point of view, because Swedish is intermediate between German and English in terms of word order properties.", "labels": [], "entities": []}, {"text": "For Romance languages, the crosslingual parser is approaching the accuracy of the supervised setting, confirming that for these languages much of the divergence is lexical and not structural, which is not true for the Germanic languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9995918869972229}]}, {"text": "Finally, Korean emerges as a very clear outlier (both as a source and as a target language), which again is supported by typological considerations as well as by the difference in tokenization.", "labels": [], "entities": []}, {"text": "With respect to evaluation, it is interesting to compare the absolute numbers to those reported in   mon to both studies (DE, EN, SV and ES).", "labels": [], "entities": [{"text": "DE", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9695034027099609}, {"text": "SV", "start_pos": 130, "end_pos": 132, "type": "METRIC", "confidence": 0.793464720249176}, {"text": "ES", "start_pos": 137, "end_pos": 139, "type": "METRIC", "confidence": 0.7382708191871643}]}, {"text": "In that study, UAS was in the 38-68% range, as compared to 55-75% here.", "labels": [], "entities": [{"text": "UAS", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.92561936378479}]}, {"text": "For Swedish, we can even measure the difference exactly, because the test sets are the same, and we see an increase from 58.3% to 70.6%.", "labels": [], "entities": []}, {"text": "This suggests that most cross-lingual parsing studies have underestimated accuracies.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7464110255241394}, {"text": "accuracies", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9904555678367615}]}], "tableCaptions": [{"text": " Table 1: Harmonized label set based on Stanford dependencies (De Marneffe et al., 2006).", "labels": [], "entities": []}, {"text": " Table 2: Data set statistics.  *  Automatically con- verted WSJ section of the PTB. The data release  includes scripts to generate this data, not the data  itself.  \u2020 Automatically converted Talbanken sec- tion of the Swedish Treebank. N=News, B=Blogs,  R=Consumer Reviews.", "labels": [], "entities": [{"text": "PTB", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.5127829313278198}, {"text": "Swedish Treebank", "start_pos": 219, "end_pos": 235, "type": "DATASET", "confidence": 0.9563985466957092}]}, {"text": " Table 3. For these experiments we ran-", "labels": [], "entities": []}, {"text": " Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.", "labels": [], "entities": [{"text": "Cross-lingual transfer parsing", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.825317899386088}]}]}