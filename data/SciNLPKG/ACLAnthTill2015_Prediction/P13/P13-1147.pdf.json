{"title": [{"text": "Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction", "labels": [], "entities": [{"text": "Domain Adaptation of Relation Extraction", "start_pos": 50, "end_pos": 90, "type": "TASK", "confidence": 0.8534892559051513}]}], "abstractContent": [{"text": "Relation Extraction (RE) is the task of extracting semantic relationships between entities in text.", "labels": [], "entities": [{"text": "Relation Extraction (RE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9034977674484252}, {"text": "extracting semantic relationships between entities in text", "start_pos": 40, "end_pos": 98, "type": "TASK", "confidence": 0.8056072081838336}]}, {"text": "Recent studies on relation extraction are mostly supervised.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.9632328152656555}]}, {"text": "The clear drawback of supervised methods is the need of training data: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to.", "labels": [], "entities": []}, {"text": "This is the problem of domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7628768384456635}]}, {"text": "In this paper, we propose to combine (i) term generalization approaches such as word clustering and latent semantic analysis (LSA) and (ii) structured kernels to improve the adaptability of relation ex-tractors to new text genres/domains.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.7556637823581696}, {"text": "latent semantic analysis (LSA)", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.7598429868618647}]}, {"text": "The empirical evaluation on ACE 2005 domains shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation.", "labels": [], "entities": [{"text": "ACE 2005 domains", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.950790802637736}, {"text": "domain adaptation", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.761788010597229}]}], "introductionContent": [{"text": "Relation extraction is the task of extracting semantic relationships between entities in text, e.g. to detect an employment relationship between the person Larry Page and the company Google in the following text snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9105064272880554}]}, {"text": "Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy; * The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper.).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.9270060360431671}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.997394323348999}]}, {"text": "However, the clear drawback of supervised methods is the need of training data, which can slowdown the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to.", "labels": [], "entities": []}, {"text": "Approaches that can cope with domain changes are essential.", "labels": [], "entities": []}, {"text": "This is the problem of domain adaptation (DA) or transfer learning (TL).", "labels": [], "entities": [{"text": "domain adaptation (DA)", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.8314748167991638}, {"text": "transfer learning (TL)", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.8231452107429504}]}, {"text": "Technically, domain adaptation addresses the problem of learning when the assumption of independent and identically distributed (i.i.d.) samples is violated.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.777298241853714}]}, {"text": "Domain adaptation has been studied extensively during the last couple of years for various NLP tasks, e.g. two shared tasks have been organized on domain adaptation for dependency parsing.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7868140339851379}, {"text": "dependency parsing", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.8346845209598541}]}, {"text": "Results were mixed, thus it is still a very active research area.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, there is almost no work on adapting relation extraction (RE) systems to new domains.", "labels": [], "entities": [{"text": "relation extraction (RE)", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.8341664493083953}]}, {"text": "There are some prior studies on the related tasks of multi-task transfer learning ( and distant supervision (), which are clearly related but different: the former is the problem of how to transfer knowledge from old to new relation types, while distant supervision tries to learn new relations from unlabeled text by exploiting weak-supervision in the form of a knowledge resource (e.g. Freebase).", "labels": [], "entities": [{"text": "multi-task transfer learning", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.8446640173594157}]}, {"text": "We assume the same relation types but a shift in the underlying data distribution.", "labels": [], "entities": []}, {"text": "Weak supervision is a promising approach to improve a relation extraction system, especially to increase its coverage in terms of types of relations covered.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8635568618774414}]}, {"text": "In this paper we examine the related issue of changes in the underlying data distribution, while keeping the relations fixed.", "labels": [], "entities": []}, {"text": "Even a weakly supervised system is expected to perform well when applied to any kind of text (other domain/genre), thus ideally, we believe that combining domain adaptation with weak supervision is the way to go in the future.", "labels": [], "entities": []}, {"text": "This study is a first step towards this.", "labels": [], "entities": []}, {"text": "We focus on unsupervised domain adaptation, i.e. no labeled target data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7519806325435638}]}, {"text": "Moreover, we consider a particular domain adaptation setting: singlesystem DA, i.e. learning a single system able to cope with different but related domains.", "labels": [], "entities": []}, {"text": "Most studies on DA so far have focused on building a specialized system for every specific target domain, e.g..", "labels": [], "entities": [{"text": "DA", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9779943823814392}]}, {"text": "In contrast, the goal here is to build a single system that can robustly handle several domains, which is inline with the setup of the recent shared task on parsing the web (.", "labels": [], "entities": []}, {"text": "Participants were asked to build a single system that can robustly parse all domains (reviews, weblogs, answers, emails, newsgroups), rather than to build several domain-specific systems.", "labels": [], "entities": []}, {"text": "We consider this as a shift in what was considered domain adaptation in the past (adapt from source to a specific target) and what can be considered a somewhat different recent view of DA, that became widespread since 2011/2012.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7541956603527069}, {"text": "DA", "start_pos": 185, "end_pos": 187, "type": "TASK", "confidence": 0.9235090017318726}]}, {"text": "The latter assumes that the target domain(s) is/are not really known in advance.", "labels": [], "entities": []}, {"text": "In this setup, the domain adaptation problem boils down to finding a more robust system, i.e. one wants to build a system that can robustly handle any kind of data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.737619087100029}]}, {"text": "We propose to combine (i) term generalization approaches and (ii) structured kernels to improve the performance of a relation extractor on new domains.", "labels": [], "entities": []}, {"text": "Previous studies have shown that lexical and syntactic features are both very important ().", "labels": [], "entities": []}, {"text": "We combine structural features with lexical information generalized by clusters or similarity.", "labels": [], "entities": []}, {"text": "Given the complexity of feature engineering, we exploit kernel methods).", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7808904051780701}]}, {"text": "We encode word clusters or similarity in tree kernels, which, in turn, produce spaces of tree fragments.", "labels": [], "entities": []}, {"text": "For example, \"president\", \"vice-president\" and \"Texas\", \"US\", are terms indicating an employment relation between a person and a location.", "labels": [], "entities": []}, {"text": "Rather than only matching the surface string of words, lexical similarity enables soft matches between similar words in convolution tree kernels.", "labels": [], "entities": []}, {"text": "In the empirical evaluation on Automatic Content Extraction (ACE) data, we evaluate the impact of convolution tree kernels embedding lexical semantic similarities.", "labels": [], "entities": [{"text": "Automatic Content Extraction (ACE)", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.779346098502477}]}, {"text": "The latter is derived in two ways with: (a) Brown word clustering; and (b) Latent Semantic Analysis (LSA).", "labels": [], "entities": [{"text": "Brown word clustering", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.5497637291749319}]}, {"text": "We first show that our system aligns well with the state of the art on the ACE 2004 benchmark.", "labels": [], "entities": [{"text": "ACE 2004 benchmark", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.9570658008257548}]}, {"text": "Then, we test our RE system on the ACE 2005 data, which exploits kernels, structures and similarities for domain adaptation.", "labels": [], "entities": [{"text": "ACE 2005 data", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.985835333665212}, {"text": "domain adaptation", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7237387150526047}]}, {"text": "The results show that combining the huge space of tree fragments generalized at the lexical level provides an effective model for adapting RE systems to new domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "We treat relation extraction as a multi-class classification problem and use SVM-light-TK 4 to train the binary classifiers.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.881686806678772}]}, {"text": "The output of the classifiers is combined using the one-vs-all approach.", "labels": [], "entities": []}, {"text": "We modified the SVM-light-TK package to include the semantic tree kernels and instance weighting.", "labels": [], "entities": []}, {"text": "The entire software package is publicly available.", "labels": [], "entities": []}, {"text": "For the SVMs, we use the same parameters as: \u03bb = 0.4, c = 2.4 using the Collins Kernel).", "labels": [], "entities": [{"text": "Collins Kernel", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.9684962928295135}]}, {"text": "The precision/recall trade-off parameter for the none class was found on held-out data: j = 0.2.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9990026354789734}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9592165946960449}]}, {"text": "Evaluation metrics are standard micro average Precision, Recall and balanced Fscore (F1).", "labels": [], "entities": [{"text": "Precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.6856062412261963}, {"text": "Recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9939749836921692}, {"text": "balanced Fscore (F1)", "start_pos": 68, "end_pos": 88, "type": "METRIC", "confidence": 0.8451874017715454}]}, {"text": "To compute statistical significance, we use the approximate randomization test.", "labels": [], "entities": []}, {"text": "In all our experiments, we model argument order of the relations explicitly.", "labels": [], "entities": []}, {"text": "Thus, for instance for the 7 coarse ACE 2004 relations, we build 14 coarse-grained classifiers (two for each coarse ACE 2004 relation type except for PER-SOC, which is symmetric, and one classifier for the none relation).).", "labels": [], "entities": []}, {"text": "Moreover, the annotation guidelines have changed (for example, ACE 2005 contains no discourse relation, some relation (sub)types have changed/moved, and care must betaken for differences in SGM markup, etc.).", "labels": [], "entities": [{"text": "ACE 2005", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9547011852264404}]}], "tableCaptions": [{"text": " Table 1: Overview of the ACE 2005 data.", "labels": [], "entities": [{"text": "ACE 2005 data", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.9783226450284322}]}, {"text": " Table 2: For each domain the percentage of target  domain words (types) that are unseen in the source  together with the most frequent OOV words.", "labels": [], "entities": []}, {"text": " Table 3: Comparison to previous work on the 7 re- lations of ACE 2004. K: kernel-based; F: feature- based; yes/no: models argument order explicitly.", "labels": [], "entities": [{"text": "7 re- lations of ACE 2004", "start_pos": 45, "end_pos": 70, "type": "DATASET", "confidence": 0.7809435044016156}]}, {"text": " Table 4: Brown clusters in tree kernels (cf.", "labels": [], "entities": []}, {"text": " Table 5: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005.  PET and BOW are abbreviated by P and B, respectively. If not specified BOW is marked.", "labels": [], "entities": [{"text": "ACE 2005", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.9646541476249695}, {"text": "PET", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.852020800113678}, {"text": "BOW", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9857590198516846}, {"text": "BOW", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.9959068298339844}]}, {"text": " Table 6: F1 per coarse relation type", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9983099699020386}]}]}