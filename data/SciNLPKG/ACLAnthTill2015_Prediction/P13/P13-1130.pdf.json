{"title": [{"text": "Language Acquisition and Probabilistic Models: keeping it simple", "labels": [], "entities": []}], "abstractContent": [{"text": "Hierarchical Bayesian Models (HBMs) have been used with some success to capture empirically observed patterns of under-and overgeneralization in child language acquisition.", "labels": [], "entities": []}, {"text": "However, as is well known, HBMs are \"ideal\" learning systems, assuming access to unlimited computational resources that may not be available to child language learners.", "labels": [], "entities": []}, {"text": "Consequently, it remains crucial to carefully assess the use of HBMs along with alternative, possibly simpler, candidate models.", "labels": [], "entities": []}, {"text": "This paper presents such an evaluation fora language acquisition domain where explicit HBMs have been proposed: the acquisition of English dative constructions.", "labels": [], "entities": [{"text": "acquisition of English dative constructions", "start_pos": 116, "end_pos": 159, "type": "TASK", "confidence": 0.6832170784473419}]}, {"text": "In particular, we present a detailed, empiricallygrounded model-selection comparison of HBMs vs. a simpler alternative based on clustering along with maximum likelihood estimation that we call linear competition learning (LCL).", "labels": [], "entities": []}, {"text": "Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, with advances in probability and estimation theory, there has been much interest in Bayesian models (BMs); Jones and Love, 2011) and their application to child language acquisition with its challenging combination of structured information and incomplete knowledge, as they offer several advantages in this domain.", "labels": [], "entities": [{"text": "estimation theory", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7955988943576813}, {"text": "child language acquisition", "start_pos": 171, "end_pos": 197, "type": "TASK", "confidence": 0.630577822526296}]}, {"text": "They can readily handle the evident noise and ambiguity of acquisition input, while at the same time providing efficiency via priors that mirror known pre-existing language biases.", "labels": [], "entities": []}, {"text": "Further, hierarchical Bayesian Models (HBMs) can combine distinct abstraction levels of linguistic knowledge, from variation at the level of individual lexical items, to cross-item variation, using hyper-parameters to capture observed patterns of both under-and over-generalization as in the acquisition of e.g. dative alternations in English (, and verb frames in a controlled artificial language.", "labels": [], "entities": []}, {"text": "HBMs can thus be viewed as providing a \"rational\" upper bound on language learnability, yielding optimal models that account for observed data while minimizing any required prior information.", "labels": [], "entities": []}, {"text": "In addition, the clustering implicit in HBM modeling introduces additional parameters that can be tuned to specific data patterns.", "labels": [], "entities": [{"text": "HBM modeling", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.9499176144599915}]}, {"text": "However, this comes at a well-known price: HBMs generally are also ideal learning systems, known to be computationally infeasible.", "labels": [], "entities": []}, {"text": "Approximations proposed to ensure computational tractability, like reducing the number of classes that need to be learned may also be linguistically and cognitively implausible.", "labels": [], "entities": []}, {"text": "For instance, in terms of verb learning, this could take the form of reducing the number of subcategorization frames to the relevant subset, as in, where only 2 frames are considered for 'take', when in fact it is listed in 6 frames by.", "labels": [], "entities": []}, {"text": "Finally, comparison of various Bayesian models of the same task is rare (Jones and Love, 2011) and Bayesian inference generally can be demonstrated as simply one class of regularization or smoothing techniques among many others; given the problem at hand, there may well be other, equally compelling regularization methods for dealing with the bias-variance dilemma (e.g., SVMs).", "labels": [], "entities": []}, {"text": "Consequently, the relevance of HBMs for cognitively accurate accounts of human learning remains uncertain and needs to be carefully assessed.", "labels": [], "entities": []}, {"text": "Here we argue that the strengths of HBMs fora given task must be evaluated in light of their computational and cognitive costs, and compared to other viable alternatives.", "labels": [], "entities": []}, {"text": "The focus should be on finding the simplest statistical models consistent with a given behavior, particularly one that aligns with known cognitive limitations.", "labels": [], "entities": []}, {"text": "In the case of many language acquisition tasks this behavior often takes the form of overgeneralization, but with eventual convergence to some target language given exposure to more data.", "labels": [], "entities": []}, {"text": "In particular, in this paper we consider how children acquire English dative verb constructions, comparing HBMs to a simpler alternative, a linear competition learning (LCL) algorithm that models the behavior of a given verb as the linear competition between the evidence for that verb, and the average behavior of verbs belonging to its same class.", "labels": [], "entities": [{"text": "English dative verb constructions", "start_pos": 62, "end_pos": 95, "type": "TASK", "confidence": 0.6227439567446709}]}, {"text": "The results show that combining simple clustering methods along with ordinary maximum likelihood estimation yields a result comparable to HBM performance, providing an alternative account of the same facts, without the computational costs incurred by HBM models that must rely, for example, on Markov Chain Monte Carlo (MCMC) methods for numerically integrating complex likelihood integrals, or on Chinese Restaurant Process (CRP) for producing partitions.", "labels": [], "entities": []}, {"text": "In terms of Marr's hierarchy learning verb alternations is an abstract computational problem (Marr's type I), solvable by many type II methods combining representations (models, viz. HBMs or LCLs) with particular algorithms.", "labels": [], "entities": [{"text": "Marr's hierarchy learning verb alternations", "start_pos": 12, "end_pos": 55, "type": "TASK", "confidence": 0.6117597917715708}]}, {"text": "The HBM convention of adopting ideal learning amounts to invoking unbounded algorithmic resources, solvability in principle, even though in practice such methods, even approximate ones, are provably NP-hard).", "labels": [], "entities": []}, {"text": "Assuming cognitive plausibility as a desideratum, we therefore examine whether HBMs can also be approximated by another type II method (LCLs) that does not demand such intensive computation.", "labels": [], "entities": []}, {"text": "Any algorithm that approximates an HBM can be viewed as implementing a somewhat different underlying model; if it replicates HBM prediction performance but is simpler and less computationally complex then we assume it is preferable.", "labels": [], "entities": [{"text": "HBM prediction", "start_pos": 125, "end_pos": 139, "type": "TASK", "confidence": 0.840712159872055}]}, {"text": "This paper is organized as follows: we start with a discussion of formalizations of language acquisition tasks, \u00a72.", "labels": [], "entities": [{"text": "language acquisition tasks", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.7626374264558157}]}, {"text": "We present our experimental framework for the dative acquisition task, formalizing a range of learning models from simple MLE methods to HBM techniques, \u00a73, and a computational evaluation of each model, \u00a74.", "labels": [], "entities": [{"text": "dative acquisition task", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.9014283617337545}]}, {"text": "We finish with conclusions and possibilities for future work, \u00a75.", "labels": [], "entities": []}], "datasetContent": [{"text": "The learning task consists of estimating the probability that a given verb occurs in a particular frame, using previous occurrences as the basis for this estimation.", "labels": [], "entities": []}, {"text": "In this context, overgeneralization can be viewed as the model's predictions that a given verb seen only in one frame (say, a PD) can also occur in the other (say, a DOD) as well, and it decreases as the learner receives more data.", "labels": [], "entities": []}, {"text": "In one extreme we have MLE, which does not overgeneralize, and in the other the L1 model, which assigns uniform probability for all unseen cases.", "labels": [], "entities": [{"text": "MLE", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.973362922668457}]}, {"text": "The other 3 models fall somewhere in between, overgeneralizing beyond the observed data, using the prior and class-based smoothing to assign some (low) probability mass to an unseen verb-frame pair.", "labels": [], "entities": []}, {"text": "The relevant models' 1 Other clustering algorithms were also used; here we report X-means results as representative of these models.", "labels": [], "entities": []}, {"text": "X-means is available from http://www.cs. waikato.ac.nz/ml/weka/ predictions for each of the target verbs in the DOD frame, given the full corpus, are in figure 3.", "labels": [], "entities": [{"text": "DOD frame", "start_pos": 112, "end_pos": 121, "type": "DATASET", "confidence": 0.8030364215373993}]}, {"text": "In either end of the figure are the verbs that were attested in only one of the frames (PD only at the left-hand end, and DOD only at the right-hand end).", "labels": [], "entities": [{"text": "PD", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9582101106643677}, {"text": "DOD", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9598624110221863}]}, {"text": "For these verbs, LCL and HBM exhibit similar behavior.", "labels": [], "entities": []}, {"text": "When the low-frequency threshold is applied, MLE \u03b1\u03b2 , HBM and LCL work equally well, figure 4.", "labels": [], "entities": [{"text": "LCL", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.8740032315254211}]}, {"text": "To examine how overgeneralization progresses during the course of learning as the models were exposed to increasing amounts of data, we used the corpus divided by cumulative epochs, as described in \u00a73.1.", "labels": [], "entities": []}, {"text": "For each epoch, verbs seen in only one of the frames were divided in 5 frequency bins, and the models were assessed as to how much overgeneralization they displayed for each of these verbs.", "labels": [], "entities": []}, {"text": "Following overgeneralization is calculated as the absolute difference between the models predicted \u03b8 and \u03b8 MLE , for each of the epochs,, and for comparative purposes their alternating/non-alternating classification is also adopted.", "labels": [], "entities": [{"text": "MLE", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9815362095832825}]}, {"text": "For non-alternating verbs, overgeneralization reflects the degree of smoothing of each model.", "labels": [], "entities": []}, {"text": "As expected, the more frequent a verb is, the more confident the model is in the indirect negative evidence it has for that verb, and the less it overgeneralizes, shown in the lighter bars in all epochs.", "labels": [], "entities": []}, {"text": "In addition, the overall effect of larger amounts of data are indicated by a reduction in overgeneralization epoch by epoch.", "labels": [], "entities": []}, {"text": "The effects of class-based smoothing can be assessed comparing L1, a model without clustering which displays a constant degree of overgeneralization regardless of the epoch, while HBM uses a distribution over clusters and the other models X-means.", "labels": [], "entities": []}, {"text": "If a low-frequency threshold is applied, the differences between the models decrease significantly and so does the degree of overgeneralization in the models' predictions, as shown in the 3 lighter bars in the figure.", "labels": [], "entities": []}, {"text": "While the models differ somewhat in their predictions, the quantitative differences need to be assessed more carefully.", "labels": [], "entities": []}, {"text": "To compare the models and provide an overall difference measure, we use the predictions of the more complex model, HBM, as a baseline and then calculate the difference between its predictions and those of the other models.", "labels": [], "entities": [{"text": "HBM", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.8717870712280273}]}, {"text": "We used three different measures for comparing models, one for their standard difference; one that prioritizes agreement for high frequency verbs; and one that focuses more on low frequency verbs.", "labels": [], "entities": []}, {"text": "The first measure, denoted Difference, captures a direct comparison between two models, M 1 and M 2 as the average prediction difference among the verbs, and is defined as: This measure treats all differences uniformly, regardless of whether they relate to high or low frequency verbs in the learning sample (e.g. for bring with 150 counts and serve with only 1 have the same weight).", "labels": [], "entities": []}, {"text": "To focus on high frequency verbs, we also define the Weighted Difference between two models as: Here we expect D n < D since models tend to agree as the amount of evidence for each verb increases.", "labels": [], "entities": []}, {"text": "Conversely, our third measure, denoted Inverted, prioritizes the agreement between two models on low frequency verbs, defined as follows: D 1/n captures the degree of similarity in overgeneralization between two models.", "labels": [], "entities": [{"text": "Inverted", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9940750598907471}, {"text": "D 1/n", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9490439742803574}]}, {"text": "The results of applying these three difference measures are shown in for the relevant models, where grey is for D(M 1 , M 2 ), black for D n (M 1 , M 2 ) and white for D 1/n (M 1 , M 2 ).", "labels": [], "entities": []}, {"text": "Given the probabilistic nature of Monte Carlo methods, there is also a variation between different runs of the HBM model (HBM to HBM-2), and this indicates that models that perform within these bounds can be considered to be equivalent (e.g. HBMs and ME-MLE \u03b1\u03b2 for Weighted Difference, and the HBMs and X-MLE \u03b1\u03b2 for the Inverted Difference).", "labels": [], "entities": []}, {"text": "Comparing the prediction agreement, the strong influence of clustering is clear: the models that have compatible clusters have similar performances.", "labels": [], "entities": []}, {"text": "For instance, all the models that adopt the ME clusters for the data perform closest to HBMs.", "labels": [], "entities": []}, {"text": "Moreover, the weighted differences tend to be smaller than 0.01 and around 0.02 for the inverted differences.", "labels": [], "entities": []}, {"text": "The results for these measures become even closer inmost cases when the low frequency threshold is adopted,, as the  To examine the decay of overgeneralization with the increase in evidence for these models, two simulated scenarios are defined fora single generic verb: one where the evidence for DOD amounts to 75% of the data (dashed lines) and in the other to 100% (solid lines), figures 9 and 8.", "labels": [], "entities": []}, {"text": "Unsurprisingly, the performance of the models is dependent on the amount of evidence available.", "labels": [], "entities": []}, {"text": "This is a consequence of the decrease in the influence of the priors as the sample size increases in a rate of 1/N, as shown in for the decrease in overgeneralization.", "labels": [], "entities": []}, {"text": "Ultimately it is the ev- These results suggest that while these models all differ slightly in the degree of overgeneralization for low frequency data and noise, these differences are small, and as evidence reaches approximately 10 examples per verb, the overall performance for all models approaches that of MLE.", "labels": [], "entities": []}], "tableCaptions": []}