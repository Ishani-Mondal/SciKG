{"title": [{"text": "Smoothed marginal distribution constraints for language modeling", "labels": [], "entities": [{"text": "language modeling", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7114382684230804}]}], "abstractContent": [{"text": "We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of well-known Kneser-Ney (1995) smoothing.", "labels": [], "entities": []}, {"text": "Unlike Kneser-Ney, our approach is designed to be applied to any given smoothed backoff model, including models that have already been heavily pruned.", "labels": [], "entities": []}, {"text": "As a result, the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al., 2007; Chelba et al., 2010), while retaining the benefits of such marginal distribution constraints.", "labels": [], "entities": []}, {"text": "We present experimental results for heavily pruned backoff n-gram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods.", "labels": [], "entities": [{"text": "word error rate reductions", "start_pos": 105, "end_pos": 131, "type": "METRIC", "confidence": 0.8139734715223312}]}, {"text": "An open-source version of the algorithm has been released as part of the OpenGrm ngram library.", "labels": [], "entities": [{"text": "OpenGrm ngram library", "start_pos": 73, "end_pos": 94, "type": "DATASET", "confidence": 0.9421810706456503}]}], "introductionContent": [{"text": "Smoothed n-gram language models are the defacto standard statistical models of language fora wide range of natural language applications, including speech recognition and machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.7227055132389069}, {"text": "machine translation", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.801224023103714}]}, {"text": "Such models are trained on large text corpora, by counting the frequency of n-gram collocations, then normalizing and smoothing (regularizing) the resulting multinomial distributions.", "labels": [], "entities": []}, {"text": "Standard techniques store the observed n-grams and derive probabilities of unobserved n-grams via their longest observed suffix and \"backoff\" costs associated with the prefix histories of the unobserved suffixes.", "labels": [], "entities": []}, {"text": "Hence the size of the model grows with the number of observed n-grams, which is very large for typical training corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "All results presented here are for English Broadcast News.", "labels": [], "entities": [{"text": "English Broadcast News", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.969723622004191}]}, {"text": "We received scripts for replicating the results from the authors, and we report statistics on our replication of their paper's results in.", "labels": [], "entities": []}, {"text": "The scripts are distributed in such away that the user supplies the data from LDC98T31 (1996 CSR HUB4 Language Model corpus) and the script breaks the collection into training and testing sets, normalizes the text, and  trains and prunes the language models using the SRILM toolkit).", "labels": [], "entities": [{"text": "LDC98T31 (1996 CSR HUB4 Language Model corpus", "start_pos": 78, "end_pos": 123, "type": "DATASET", "confidence": 0.8483463376760483}, {"text": "SRILM toolkit", "start_pos": 268, "end_pos": 281, "type": "DATASET", "confidence": 0.8556414842605591}]}, {"text": "Presumably due to minor differences in text normalization, resulting in very slightly fewer n-grams in all conditions, we achieve negligibly lower perplexities (one or two tenths of a point) in all conditions, as can be seen when comparing with.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7030586302280426}]}, {"text": "All of the same trends result, thus that paper's result is successfully replicated here.", "labels": [], "entities": []}, {"text": "Note that we ran our Kneser-Ney pruning (noted with a \u2020 in the table), using the new -prune-history-lm switch in SRILM -created in response to the paper -which allows the use of another model to calculate the state marginals for pruning.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.8176555633544922}]}, {"text": "This fixes part of the problem -perplexity does not degrade as much as the Kneser-Ney pruned model in -but, as argued earlier in this paper, this is not the sole reason for the degradation and the perplexity remains extremely inflated.", "labels": [], "entities": []}, {"text": "We follow in training and test set definition, vocabulary size, and parameters for reporting perplexity.", "labels": [], "entities": []}, {"text": "Note that unigrams in the models are never pruned, hence all models assign probabilities over an identical vocabulary and perplexity is comparable across models.", "labels": [], "entities": []}, {"text": "For all results reported here, we use the SRILM toolkit for baseline model training and pruning, then convert from the resulting ARPA format model to an OpenFst format, as used in the OpenGrm n-gram library).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.8355993330478668}, {"text": "OpenGrm n-gram library", "start_pos": 184, "end_pos": 206, "type": "DATASET", "confidence": 0.8345878918965658}]}, {"text": "We then apply the marginal distribution constraints, and convert the result back to ARPA format for perplexity evaluation with the SRILM toolkit.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 131, "end_pos": 144, "type": "DATASET", "confidence": 0.8898473083972931}]}, {"text": "All models are subjected to full normalization sanity checks, as with typical model functions in the OpenGrm library.", "labels": [], "entities": []}, {"text": "Recall that our algorithm assumes that, for every n-gram in the model, all prefix and suffix ngrams are also in the model.", "labels": [], "entities": []}, {"text": "For pruned models, the SRILM toolkit does not impose such a requirement, hence explicit arcs are added to the: Perplexity reductions achieved with marginal distribution constraints (MDC) on the heavily pruned models from, and a mixture model.", "labels": [], "entities": []}, {"text": "WFST ngram counts are slightly higher than ARPA format in due to adding prefix and suffix n-grams.", "labels": [], "entities": [{"text": "WFST ngram counts", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.7197957833607992}]}, {"text": "model during conversion, with probability equal to what they would receive in the the original model.", "labels": [], "entities": []}, {"text": "The resulting model is equivalent, but with a small number of additional arcs in the explicit representation (around 1% for the most heavily pruned models).", "labels": [], "entities": []}, {"text": "presents perplexity results for models that result from applying our marginal distribution constraints to the four heavily pruned models from.", "labels": [], "entities": []}, {"text": "In all four cases, we get perplexity reductions of around 10 points.", "labels": [], "entities": [{"text": "perplexity reductions", "start_pos": 26, "end_pos": 47, "type": "METRIC", "confidence": 0.9320743083953857}]}, {"text": "We present the number of n-grams represented explicitly in the WFST, which is a slight increase from those presented in due to the reintroduction of prefix and suffix n-grams.", "labels": [], "entities": [{"text": "WFST", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.9112765789031982}]}, {"text": "In addition to the four models reported in, we produced a mixture model by interpolating (with equal weight) smoothed ngram probabilities from the full (unpruned) absolute discounting, Witten-Bell and Katz models, which share the same set of n-grams.", "labels": [], "entities": []}, {"text": "After renormalizing and pruning to approximately the same size as the other models, we get commensurate gains using this model as with the other models.", "labels": [], "entities": []}, {"text": "demonstrates the importance of iterating the steady state history calculation.", "labels": [], "entities": []}, {"text": "All of the methods achieve perplexity reductions with subsequent iterations.", "labels": [], "entities": []}, {"text": "Katz and absolute discounting achieve very little reduction in the first iteration, but catch backup in the second iteration.", "labels": [], "entities": []}, {"text": "The other iterative part of the algorithm, discussed in Section 4.3, is the denominator of equation 8, which changes due to adjustments in the backoff weights required by the revised n-gram probabilities.", "labels": [], "entities": []}, {"text": "If we do not iteratively update the backoff weights when reestimating the weights, the 'Pruned+MDC' perplexities in increase by between 0.2-0.4 points.", "labels": [], "entities": [{"text": "Pruned+MDC' perplexities", "start_pos": 88, "end_pos": 112, "type": "METRIC", "confidence": 0.9264132499694824}]}, {"text": "Hence, iterating the steady state probability calculation is quite important, as illustrated by; iterating the denominator calculation much less so, at least for these models.", "labels": [], "entities": []}, {"text": "We noted in Section 3 that a key difference between our approach and is that their approach treated the denominator as a constant.", "labels": [], "entities": []}, {"text": "If we do this, the 'Pruned+MDC' perplexities increase by between 4.5-5.6 points, i.e., about half of the perplexity reduction is lost for each method.", "labels": [], "entities": [{"text": "Pruned+MDC' perplexities", "start_pos": 20, "end_pos": 44, "type": "METRIC", "confidence": 0.9120702385902405}]}, {"text": "Thus, while iteration of denominator calculation may not be critical, it should not be treated as a constant.", "labels": [], "entities": [{"text": "denominator calculation", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.674529641866684}]}, {"text": "We now look at the impacts on system performance we can achieve with these new models 4 , and whether the perplexity differences that we observe translate to real error rate reductions.", "labels": [], "entities": []}, {"text": "For automatic speech recognition experiments, we used as test set the 1997 Hub4 evaluation set consisting of 32,689 words.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6226275662581126}, {"text": "Hub4 evaluation set", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.841584324836731}]}, {"text": "The acoustic model is a tied-state triphone GMM-based HMM whose input features are 9-frame stacked 13-dimensional PLP-cepstral coefficients projected down to 39 dimensions using LDA.", "labels": [], "entities": []}, {"text": "The model was trained on the 1996 and 1997 Hub4 acoustic model training sets (about 150 hours of data) using semi-tied covariance modeling and CMLLR-based speaker adaptive training and 4 iterations of boosted MMI.", "labels": [], "entities": [{"text": "Hub4 acoustic model training sets", "start_pos": 43, "end_pos": 76, "type": "DATASET", "confidence": 0.9443643450736999}]}, {"text": "We used a multi-pass decoding strategy: two quick passes for adaptation supervision, CMLLR and MLLR estimation; then a slower full decoding pass running about 3 times slower than real time.", "labels": [], "entities": [{"text": "MLLR estimation", "start_pos": 95, "end_pos": 110, "type": "METRIC", "confidence": 0.8500605821609497}]}, {"text": "presents recognition results for the heavily pruned models that we have been considering, both for first pass decoding and rescoring of the resulting lattices using failure transitions rather than epsilon backoff approximations.: WER reductions achieved with marginal distribution constraints (MDC) on the heavily pruned models from, and a mixture model.", "labels": [], "entities": [{"text": "WER", "start_pos": 230, "end_pos": 233, "type": "METRIC", "confidence": 0.7423402667045593}]}, {"text": "KneserNey results are shown for: a) original pruning; and b) with -prune-history-lm switch.", "labels": [], "entities": []}, {"text": "The perplexity reductions that were achieved for these models do translate to real word error rate reductions at both stages of between 0.5 and 0.9 percent absolute.", "labels": [], "entities": [{"text": "word error rate reductions", "start_pos": 83, "end_pos": 109, "type": "METRIC", "confidence": 0.6779710352420807}]}, {"text": "All of these gains are statistically significant at p < 0.0001 using the stratified shuffling test).", "labels": [], "entities": []}, {"text": "For pruned Kneser-Ney models, fixing the state marginals with the -prune-history-lm switch reduces the WER versus the original pruned model, but no reductions were achieved vs. baseline methods.", "labels": [], "entities": [{"text": "WER", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9961230158805847}]}, {"text": "presents perplexity and WER results for less heavily pruned models, where the pruning thresholds were set to yield approximately 1.5 million n-grams (4 times more than the previous models); and another set at around 5 million n-grams, as well as the full, unpruned models.", "labels": [], "entities": [{"text": "WER", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9990317821502686}]}, {"text": "While the robust gains we've observed up to now persist with the 1.5M n-gram models (WER reductions significant, Witten-Bell at p < 0.02, others at p < 0.0001), the larger models yield diminishing gains, with no real WER improvements.", "labels": [], "entities": [{"text": "WER reductions", "start_pos": 85, "end_pos": 99, "type": "METRIC", "confidence": 0.8672196567058563}]}, {"text": "Performance of Witten-Bell models with the marginal distribution constraints degrade badly for the larger models, indicating that this method of regularization, unmodified by aggressive pruning, does not provide a well suited distribution for this sort of optimization.", "labels": [], "entities": []}, {"text": "We speculate that this is due to underregularization, having noted some floating point precision issues when allowing the backoff recalculation to run indefinitely.", "labels": [], "entities": [{"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.6742336750030518}]}], "tableCaptions": [{"text": " Table 1: Reformatted version of Table 3 in Chelba et al. (2010), demonstrating perplexity degradation of Kneser-Ney", "labels": [], "entities": []}, {"text": " Table 2: Replication of Chelba et al. (2010) using provided", "labels": [], "entities": [{"text": "Replication", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.972380518913269}]}, {"text": " Table 3: Perplexity reductions achieved with marginal dis-", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9251849055290222}]}, {"text": " Table 4: WER reductions achieved with marginal dis-", "labels": [], "entities": [{"text": "WER reductions", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8163646161556244}]}, {"text": " Table 5: Perplexity (PPL) and both first pass (FP) and rescoring (RS) WER reductions for less heavily pruned models using", "labels": [], "entities": [{"text": "first pass (FP) and rescoring (RS) WER reductions", "start_pos": 36, "end_pos": 85, "type": "METRIC", "confidence": 0.8669566189249357}]}]}