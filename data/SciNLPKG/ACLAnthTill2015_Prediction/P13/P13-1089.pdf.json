{"title": [], "abstractContent": [{"text": "Given that structured output prediction is typically performed over entire datasets, one natural question is whether it is possible to re-use computation from earlier inference instances to speedup inference for future instances.", "labels": [], "entities": [{"text": "structured output prediction", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.7424521644910177}]}, {"text": "Amortized inference has been proposed as away to accomplish this.", "labels": [], "entities": []}, {"text": "In this paper, first, we introduce anew amortized inference algorithm called the Margin-based Amortized Inference, which uses the notion of structured margin to identify inference problems for which previous solutions are provably optimal.", "labels": [], "entities": [{"text": "Margin-based Amortized Inference", "start_pos": 81, "end_pos": 113, "type": "TASK", "confidence": 0.8042482634385427}]}, {"text": "Second , we introduce decomposed amortized inference, which is designed to address very large inference problems, where earlier amortization methods become less effective.", "labels": [], "entities": []}, {"text": "This approach works by decomposing the output structure and applying amortization piece-wise, thus increasing the chance that we can re-use previous solutions for parts of the output structure.", "labels": [], "entities": []}, {"text": "These parts are then combined to a global coherent solution using Lagrangian relaxation.", "labels": [], "entities": []}, {"text": "In our experiments, using the NLP tasks of semantic role labeling and entity-relation extraction, we demonstrate that with the margin-based algorithm, we need to call the inference engine only fora third of the test examples.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.6550061504046122}, {"text": "entity-relation extraction", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.7398494780063629}]}, {"text": "Further, we show that the decomposed variant of margin-based amortized inference achieves a greater reduction in the number of inference calls.", "labels": [], "entities": []}], "introductionContent": [{"text": "A wide variety of NLP problems can be naturally cast as structured prediction problems.", "labels": [], "entities": []}, {"text": "For * These authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "some structures like sequences or parse trees, specialized and tractable dynamic programming algorithms have proven to be very effective.", "labels": [], "entities": []}, {"text": "However, as the structures under consideration become increasingly complex, the computational problem of predicting structures can become very expensive, and in the worst case, intractable.", "labels": [], "entities": [{"text": "predicting structures", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.9082715511322021}]}, {"text": "In this paper, we focus on an inference technique called amortized inference (, where previous solutions to inference problems are used to speedup new instances.", "labels": [], "entities": []}, {"text": "The main observation that leads to amortized inference is that, very often, for different examples of the same size, the structures that maximize the score are identical.", "labels": [], "entities": []}, {"text": "If we can efficiently identify that two inference problems have the same solution, then we can re-use previously computed structures for newer examples, thus giving us a speedup.", "labels": [], "entities": []}, {"text": "This paper has two contributions.", "labels": [], "entities": []}, {"text": "First, we describe a novel algorithm for amortized inference called margin-based amortization.", "labels": [], "entities": []}, {"text": "This algorithm is on an examination of the structured margin of a prediction.", "labels": [], "entities": []}, {"text": "For anew inference problem, if this margin is larger than the sum of the decrease in the score of the previous prediction and any increase in the score of the second best one, then the previous solution will be the highest scoring one for the new problem.", "labels": [], "entities": []}, {"text": "We formalize this intuition to derive an algorithm that finds provably optimal solutions and show that this approach is a generalization of previously identified schemes (based on Theorem 1 of ().", "labels": [], "entities": []}, {"text": "Second, we argue that the idea of amortization is best exploited at the level of parts of the structures rather than the entire structure because we expect a much higher redundancy in the parts.", "labels": [], "entities": []}, {"text": "We introduce the notion of decomposed amortized inference, whereby we can attain a significant improvement in speedup by considering repeated sub-structures across the dataset and applying any amortized inference algorithm for the parts.", "labels": [], "entities": []}, {"text": "We evaluate the two schemes and their combination on two NLP tasks where the output is encoded as a structure: PropBank semantic role labeling () and the problem of recognizing entities and relations in text.", "labels": [], "entities": [{"text": "PropBank semantic role labeling", "start_pos": 111, "end_pos": 142, "type": "TASK", "confidence": 0.6056176349520683}]}, {"text": "In these problems, the inference problem has been framed as an integer linear program (ILP).", "labels": [], "entities": []}, {"text": "We compare our methods with previous amortized inference methods and show that margin-based amortization combined with decomposition significantly outperforms existing methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments show two results: 1.", "labels": [], "entities": []}, {"text": "The marginbased scheme outperforms the amortized inference approaches from ().", "labels": [], "entities": []}, {"text": "2. Decomposed amortized inference gives further gains in terms of re-using previous solutions.", "labels": [], "entities": []}, {"text": "We follow the experimental setup of) and simulate a long-running NLP process by caching problems and solutions from the Gigaword corpus.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 120, "end_pos": 135, "type": "DATASET", "confidence": 0.9515368938446045}]}, {"text": "We used a database engine to cache ILP and their solutions along with identifiers for the equivalence class and the value of \u03b4.", "labels": [], "entities": []}, {"text": "For the margin-based algorithm and the Theorem 1 from (, fora new inference problem p \u223c [P ], we retrieve all inference problems from the database that belong to the same equivalence class as the test problem p and find the cached assignment y that has the highest score according to the coefficients of p.", "labels": [], "entities": []}, {"text": "We only consider cached ILPs whose solution is y for checking the conditions of the theorem.", "labels": [], "entities": []}, {"text": "This optimization ensures that we only process a small number of cached coefficient vectors.", "labels": [], "entities": []}, {"text": "Ina second efficiency optimization, we pruned the database to remove redundant inference problems.", "labels": [], "entities": []}, {"text": "A problem is redundant if solution to that problem can be inferred from the other problems stored in the database that have the same solution and belong to the same equivalence class.", "labels": [], "entities": []}, {"text": "However, this pruning can be computationally expensive if the number of problems with the same solution and the same equivalence class is very large.", "labels": [], "entities": []}, {"text": "In that case, we first sampled a 5000 problems randomly and selected the non-redundant problems from this set to keep in the database.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Reduction in number of inference calls", "labels": [], "entities": []}]}