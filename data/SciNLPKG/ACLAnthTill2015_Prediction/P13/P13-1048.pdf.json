{"title": [{"text": "Combining Intra-and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis", "labels": [], "entities": [{"text": "Document-level Discourse Analysis", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.6960096557935079}]}], "abstractContent": [{"text": "We propose a novel approach for developing a two-stage document-level discourse parser.", "labels": [], "entities": [{"text": "document-level discourse parser", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.6593280831972758}]}, {"text": "Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intra-sentential parsing and the other for multi-sentential parsing.", "labels": [], "entities": []}, {"text": "We present two approaches to combine these two stages of discourse parsing effectively.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7141653895378113}]}, {"text": "A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the state-of-the-art, often by a wide margin.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We experiment with our discourse parser on the two datasets using our two different parsing approaches, namely 1S-1S and the sliding window.", "labels": [], "entities": []}, {"text": "We compare our approach with HILDA (Hernault et al., 2010) on RST-DT, and with the ILP-based approach of) on the Instructional corpus, since they are the state-ofthe-art on the respective genres.", "labels": [], "entities": [{"text": "HILDA", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.6655679941177368}, {"text": "Instructional corpus", "start_pos": 113, "end_pos": 133, "type": "DATASET", "confidence": 0.7326233237981796}]}, {"text": "On RST-DT, the standard split was used for training and testing purposes.", "labels": [], "entities": [{"text": "RST-DT", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.7388529181480408}]}, {"text": "The results for HILDA were obtained by running the system with default settings on the same inputs we provided to our system.", "labels": [], "entities": [{"text": "HILDA", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.5188197493553162}]}, {"text": "Since we could not run the ILP-based system of (Subba and Di-Eugenio, 2009) (not publicly available) on the Instructional corpus, we report the performances presented in their paper.", "labels": [], "entities": [{"text": "Instructional corpus", "start_pos": 108, "end_pos": 128, "type": "DATASET", "confidence": 0.8303583860397339}]}, {"text": "They used 151 documents for training and 25 documents for testing.", "labels": [], "entities": []}, {"text": "Since we did not have access to their particular split, we took 5 random samples of 151 documents for training and 25 documents for testing, and report the average performance over the 5 test sets.", "labels": [], "entities": []}, {"text": "To evaluate the parsing performance, we use the standard unlabeled (i.e., hierarchical spans) and labeled (i.e., nuclearity and relation) precision, recall and F-score as described in.", "labels": [], "entities": [{"text": "parsing", "start_pos": 16, "end_pos": 23, "type": "TASK", "confidence": 0.961287260055542}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9252721667289734}, {"text": "recall", "start_pos": 149, "end_pos": 155, "type": "METRIC", "confidence": 0.9986957907676697}, {"text": "F-score", "start_pos": 160, "end_pos": 167, "type": "METRIC", "confidence": 0.9983758926391602}]}, {"text": "To compare with previous studies, our experiments on RST-DT use the 18 coarser relations.", "labels": [], "entities": [{"text": "coarser", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9758384823799133}]}, {"text": "After attaching the nuclearity statuses (NS, SN, NN) to these relations, we get 41 distinct relations.", "labels": [], "entities": []}, {"text": "Following on the Instructional corpus, we use 26 relations, and treat the reversals of non-commutative relations as separate relations.", "labels": [], "entities": [{"text": "Instructional corpus", "start_pos": 17, "end_pos": 37, "type": "DATASET", "confidence": 0.879217118024826}]}, {"text": "That is, Goal-Act and Act-Goal are considered as two different relations.", "labels": [], "entities": []}, {"text": "Attaching the nuclearity statuses to these relations gives 76 distinct relations.", "labels": [], "entities": []}, {"text": "Analogous to previous studies, we map the n-ary relations (e.g., Joint) into nested right-branching binary relations.", "labels": [], "entities": []}, {"text": "presents F-score parsing results for our parsers and the existing systems on the two corpora.", "labels": [], "entities": [{"text": "F-score parsing", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.8440655767917633}]}, {"text": "On both corpora, our parser, namely, 1S-1S (TSP 1-1) and sliding window (TSP SW), outperform existing systems by a wide margin (p<7.1e-05).", "labels": [], "entities": []}, {"text": "3 On RST-DT, our parsers achieve absolute F-score improvements of 8%, 9.4% and 11.4% in span, nuclearity and relation, respectively, over HILDA.", "labels": [], "entities": [{"text": "F-score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.8998163342475891}, {"text": "span", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9820188283920288}, {"text": "relation", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9266930222511292}]}, {"text": "This represents relative error reductions of 32%, 23% and 21% in span, nuclearity and relation, respectively.", "labels": [], "entities": [{"text": "span", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9301900267601013}]}, {"text": "Our results are also close to the upper bound, i.e. human agreement on this corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Parsing results of different models using manual (gold) segmentation. Performances significantly superior to HILDA", "labels": [], "entities": [{"text": "HILDA", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.49265459179878235}]}]}