{"title": [{"text": "Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions", "labels": [], "entities": [{"text": "Broadcast News Story Segmentation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7522142380475998}]}], "abstractContent": [{"text": "We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions.", "labels": [], "entities": [{"text": "broadcast news story segmentation", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.6590445339679718}]}, {"text": "The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 43, "end_pos": 76, "type": "METRIC", "confidence": 0.8287630875905355}]}, {"text": "We employ Laplacian Eigenmap-s (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure.", "labels": [], "entities": []}, {"text": "We evaluate two approaches employing LDA and prob-abilistic latent semantic analysis (PLSA) distributions respectively.", "labels": [], "entities": []}, {"text": "The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied.", "labels": [], "entities": []}, {"text": "Experimental results show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach.", "labels": [], "entities": []}, {"text": "The proposed approach provides the best performance with the highest F1-measure of 0.7860.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9996598958969116}]}], "introductionContent": [{"text": "Story segmentation refers to partitioning a multimedia stream into homogenous segments each embodying a main topic or coherent story).", "labels": [], "entities": [{"text": "Story segmentation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.748613566160202}]}, {"text": "With the explosive growth of multimedia data, it becomes difficult to retrieve the most relevant components.", "labels": [], "entities": []}, {"text": "For indexing broadcast news programs, it is desirable to divide each of them into a number of independent stories.", "labels": [], "entities": [{"text": "indexing broadcast news programs", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.9235345721244812}]}, {"text": "Manual segmentation is accurate but labor-intensive and costly.", "labels": [], "entities": []}, {"text": "Therefore, automatic story segmentation approaches are highly demanded.", "labels": [], "entities": [{"text": "story segmentation", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7021485269069672}]}, {"text": "Lexical-cohesion based approaches have been widely studied for automatic broadcast news story segmentation (; * corresponding author).", "labels": [], "entities": [{"text": "automatic broadcast news story segmentation", "start_pos": 63, "end_pos": 106, "type": "TASK", "confidence": 0.6895192384719848}]}, {"text": "In this kind of approaches, the audio portion of the data stream is passed to an automatic speech recognition (ASR) system.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 81, "end_pos": 115, "type": "TASK", "confidence": 0.7976273496945699}]}, {"text": "Lexical cues are extracted from the ASR transcripts.", "labels": [], "entities": []}, {"text": "Lexical cohesion is the phenomenon that different stories tend to employ different sets of terms.", "labels": [], "entities": []}, {"text": "Term repetition is one of the most common appearances.", "labels": [], "entities": [{"text": "Term repetition", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8126172721385956}]}, {"text": "These rigid lexical-cohesion based approaches simply take term repetition into consideration, while term association in lexical cohesion is ignored.", "labels": [], "entities": []}, {"text": "Moreover, polysemy and synonymy are not considered.", "labels": [], "entities": []}, {"text": "To deal with these problems, some topic model techniques which provide conceptual level matching have been introduced to text and story segmentation task.", "labels": [], "entities": [{"text": "text and story segmentation task", "start_pos": 121, "end_pos": 153, "type": "TASK", "confidence": 0.6887842237949371}]}, {"text": "Probabilistic latent semantic analysis (PLSA)) is atypical instance and used widely.", "labels": [], "entities": [{"text": "Probabilistic latent semantic analysis (PLSA))", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7595174525465284}]}, {"text": "PLSA is the probabilistic variant of latent semantic analysis (LSA) (), and offers a more solid statistical foundation.", "labels": [], "entities": [{"text": "latent semantic analysis (LSA)", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.7696480602025986}]}, {"text": "PLSA provides more significant improvement than LSA for story segmentation (.", "labels": [], "entities": [{"text": "PLSA", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.4524626135826111}, {"text": "story segmentation", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.802663117647171}]}, {"text": "Despite the success of PLSA, there are concerns that the number of parameters in PLSA grows linearly with the size of the corpus.", "labels": [], "entities": []}, {"text": "This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems.", "labels": [], "entities": [{"text": "PLSA", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.9171717762947083}]}, {"text": "To deal with this issue, Latent Dirichlet Allocation (L-DA) () has been proposed.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (L-DA)", "start_pos": 25, "end_pos": 59, "type": "METRIC", "confidence": 0.9336150089899699}]}, {"text": "LDA has been proved to be effective in many segmentation tasks.", "labels": [], "entities": [{"text": "segmentation tasks", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9185740649700165}]}, {"text": "Recent studies have shown that intrinsic dimensionality of natural text corpus is significantly lower than its ambient Euclidean space).", "labels": [], "entities": []}, {"text": "Therefore, Laplacian Eigenmaps (LE) was proposed to compute corresponding natural low-dimensional structure.", "labels": [], "entities": [{"text": "Laplacian Eigenmaps (LE)", "start_pos": 11, "end_pos": 35, "type": "METRIC", "confidence": 0.7663945257663727}]}, {"text": "LE is a geometrically motivated dimensionality reduction method.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.7188722193241119}]}, {"text": "It projects data into a low-dimensional representation while preserving the intrinsic local geometric structure information ().", "labels": [], "entities": []}, {"text": "The locality preserving property attempts to make the lowdimensional data representation more robust to the noise from ASR errors (.", "labels": [], "entities": [{"text": "ASR", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9382352232933044}]}, {"text": "To further improve the segmentation performance, using latent topic distributions and LE instead of term frequencies to represent text blocks is studied in this paper.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.9722228646278381}, {"text": "LE", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9528177976608276}]}, {"text": "We study the effects of the size of training data and the number of latent topics on the LDA-based and the PLSA-based approaches.", "labels": [], "entities": []}, {"text": "Another related work ( is to use local geometric information to regularize the log-likelihood computation in PLSA.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were evaluated on the ASR transcripts provided in TDT2 English Broadcast news corpus 1 , which involved 1033 news programs.", "labels": [], "entities": [{"text": "TDT2 English Broadcast news corpus 1", "start_pos": 66, "end_pos": 102, "type": "DATASET", "confidence": 0.93840425213178}]}, {"text": "We separated this corpus into three non-overlapping sets: a training set of 500 programs for parameter estimation in topic modeling and LE, a development set of 133 programs for empirical tuning and a test set of 400 programs for performance evaluation.", "labels": [], "entities": [{"text": "LE", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.9181109666824341}]}, {"text": "In the training stage, ASR transcripts with manually labeled boundary tags were provided.", "labels": [], "entities": [{"text": "ASR transcripts", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.8942616581916809}]}, {"text": "Text 1 http://projects.ldc.upenn.edu/TDT2/ streams were broken into block units according to the given boundary tags, with each text block being a complete story.", "labels": [], "entities": []}, {"text": "In the segmentation stage, we divided test data into text blocks using the time labels of pauses in the transcripts.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 7, "end_pos": 19, "type": "TASK", "confidence": 0.9749463200569153}]}, {"text": "If the pause duration between two blocks last for more than 1.0 sec, it was considered as a boundary candidate.", "labels": [], "entities": []}, {"text": "To avoid the segmentation being suffered from ASR errors and the out-of-vocabulary issue, phoneme bigram was used as the basic term unit (.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.9751683473587036}, {"text": "ASR", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9480806589126587}]}, {"text": "Since the ASR transcripts were at word level, we performed word-to-phoneme conversion to obtain the phoneme bigram basic units.", "labels": [], "entities": [{"text": "ASR", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.938503086566925}, {"text": "word-to-phoneme conversion", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.6762702167034149}]}, {"text": "The following approaches, in which DP was used in story boundary detection, were evaluated in the experiments: \u2022 PLSA-DP: PLSA topic distributions were used to compute sentence cohesive strength.", "labels": [], "entities": [{"text": "story boundary detection", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.7692224184672037}]}, {"text": "\u2022 LDA-DP: LDA topic distributions were used to compute sentence cohesive strength.", "labels": [], "entities": []}, {"text": "\u2022 PLSA-LE-DP: PLSA topic distributions followed by LE projection were used to compute sentence cohesive strength.", "labels": [], "entities": []}, {"text": "\u2022 LDA-LE-DP: LDA topic distributions followed by LE projection were used to compute sentence cohesion strength.", "labels": [], "entities": []}, {"text": "For LDA, we used the implementation from David M. Blei's webpage 2 . For PLSA, we used the Lemur Toolkit 3 . F1-measure was used as the evaluation criterion.We followed the evaluation rule: a detected boundary candidate is considered correct if it lies within a 15 sec tolerant window on each side of a reference boundary.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9944328665733337}]}, {"text": "A number of parameters were set through empirical tuning on the developent set.", "labels": [], "entities": []}, {"text": "The penalty factor was set to 0.8.", "labels": [], "entities": []}, {"text": "When evaluating the effects of different size of the training set, the number of latent topics in topic modeling process was set to 64.", "labels": [], "entities": []}, {"text": "After the number of latent topics was fixed, the dimensionality after LE projection was set to 32.", "labels": [], "entities": []}, {"text": "When evaluating the effects of different number of latent topics in topic modeling computation, we fixed the size of the training set to 500 news programs and changed the number of latent topics from 16 to 256.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.758337140083313}]}, {"text": "We used the training set from 100 programs to 500 programs (adding 100 programs in each step) to e-valuate the effects of different size of training data in both PLSA-based and LDA-based approaches.", "labels": [], "entities": []}, {"text": "shows the results on the development set and the test set.", "labels": [], "entities": []}], "tableCaptions": []}