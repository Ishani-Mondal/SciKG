{"title": [{"text": "Improving pairwise coreference models through feature space hierarchy learning", "labels": [], "entities": [{"text": "Improving pairwise coreference", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.863702654838562}]}], "abstractContent": [{"text": "This paper proposes anew method for significantly improving the performance of pairwise coreference models.", "labels": [], "entities": []}, {"text": "Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models.", "labels": [], "entities": []}, {"text": "In effect , our approach finds an optimal feature space (derived from abase feature set and indicator set) for discriminating coref-erential mention pairs.", "labels": [], "entities": []}, {"text": "Although our approach explores a very large space of possible feature spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators.", "labels": [], "entities": []}, {"text": "Our experiments on the CoNLL-2012 Shared Task English datasets (gold mentions) indicate that our method is robust relative to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task English datasets", "start_pos": 23, "end_pos": 62, "type": "DATASET", "confidence": 0.8709093451499939}]}, {"text": "Our best system obtains a competitive 67.2 of average F1 over MUC, B 3 , and CEAF which, despite its simplicity, places it above the mean score of other systems on these datasets.", "labels": [], "entities": [{"text": "F1", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9987919926643372}, {"text": "MUC", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.9223805069923401}, {"text": "CEAF", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.9104212522506714}]}], "introductionContent": [{"text": "Coreference resolution is the problem of partitioning a sequence of noun phrases (or mentions), as they occur in a natural language text, into a set of referential entities.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9223361611366272}, {"text": "partitioning a sequence of noun phrases (or mentions)", "start_pos": 41, "end_pos": 94, "type": "TASK", "confidence": 0.763973879814148}]}, {"text": "A common approach to this problem is to separate it into two modules: on the one hand, one defines a model for evaluating coreference links, in general a discriminative classifier that detects coreferential mention pairs.", "labels": [], "entities": []}, {"text": "On the other hand, one designs a method for grouping the detected links into a coherent global output (i.e. a partition over the set of entity mentions).", "labels": [], "entities": []}, {"text": "This second step is typically achieved using greedy heuristics, although more sophisticated clustering approaches have been used, too, such as cutting graph methods and Integer Linear Programming (ILP) formulations.", "labels": [], "entities": []}, {"text": "Despite its simplicity, this two-step strategy remains competitive even when compared to more complex models utilizing a global loss.", "labels": [], "entities": []}, {"text": "In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pairwise classifier.", "labels": [], "entities": []}, {"text": "Consequently, a lot of research effort on coreference resolution has focused on trying to boost the performance of the pairwise classifier.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.9764000475406647}]}, {"text": "Numerous studies are concerned with feature extraction, typically trying to enrich the classifier with more linguistic knowledge and/or more world knowledge (.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7453490793704987}]}, {"text": "A second line of work explores the use of distinct local models for different types of mentions, specifically for different types of anaphoric mentions based on their grammatical categories (such as pronouns, proper names, definite descriptions).", "labels": [], "entities": []}, {"text": "An important justification for such spe-cialized models is (psycho-)linguistic and comes from theoretical findings based on salience or accessibility.", "labels": [], "entities": []}, {"text": "It is worth noting that, from a machine learning point of view, this is related to feature extraction in that both approaches in effect recast the pairwise classification problem in higher dimensional feature spaces.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8028610050678253}]}, {"text": "In this paper, we claim that mention pairs should not be processed by a single classifier, and instead should be handled through specific models.", "labels": [], "entities": []}, {"text": "But we are furthermore interested in learning how to construct and select such differential models.", "labels": [], "entities": []}, {"text": "Our argument is therefore based on statistical considerations, rather than on purely linguistic ones . The main question we raise is, given a set of indicators (such as grammatical types, distance between two mentions, or named entity types), how to best partition the pool of mention pair examples in order to best discriminate coreferential pairs from non coreferential ones.", "labels": [], "entities": []}, {"text": "In effect, we want to learn the \"best\" subspaces for our different models: that is, subspaces that are neither too coarse (i.e., unlikely to separate the data well) nor too specific (i.e., prone to data sparseness and noise).", "labels": [], "entities": []}, {"text": "We will see that this is also equivalent to selecting a single large adequate feature space by using the data.", "labels": [], "entities": []}, {"text": "Our approach generalizes earlier approaches in important ways.", "labels": [], "entities": []}, {"text": "For one thing, the definition of the different models is no longer restricted to grammatical typing (our model allows for various other types of indicators) or to the sole typing of the anaphoric mention (our models can also be specific to a particular type antecedent or to the two types of the mention pair).", "labels": [], "entities": []}, {"text": "More importantly, we propose an original method for learning the best set of models that can be built from a given set of indicators and a training set.", "labels": [], "entities": []}, {"text": "These models are organized in a hierarchy, wherein each leaf corresponds to a mutually disjoint subset of mention pair examples and the classifier that can be trained from it.", "labels": [], "entities": []}, {"text": "Our models are trained using the Online Passive-Aggressive algorithm or PA (), a large margin version of the perceptron.", "labels": [], "entities": [{"text": "PA", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9009664058685303}]}, {"text": "Our method is exact in that it explores the full space of hierarchies (of size at least 2 2 n ) definable on an indicator sequence, while remaining scalable by exploiting the particular structure of these during the training of the distinct local models).", "labels": [], "entities": []}, {"text": "3 However it should be underlined that the statistical viewpoint is complementary to the linguistic work.", "labels": [], "entities": []}, {"text": "This approach also performs well, and it largely outperforms the single model.", "labels": [], "entities": []}, {"text": "As will be shown based on a variety of experiments on the CoNLL-2012 Shared Task English datasets, these improvements are consistent across different evaluation metrics and for the most part independent of the clustering decoder that was used.", "labels": [], "entities": [{"text": "CoNLL-2012 Shared Task English datasets", "start_pos": 58, "end_pos": 97, "type": "DATASET", "confidence": 0.9100700974464416}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses the underlying statistical hypotheses of the standard pairwise model and defines a simple alternative framework that uses a simple separation of mention pairs based on grammatical types.", "labels": [], "entities": []}, {"text": "Next, in section 3, we generalize the method by introducing indicator hierarchies and explain how to learn the best models associated with them.", "labels": [], "entities": []}, {"text": "Section 4 provides a brief system description and Section 5 evaluates the various models on CoNLL-2012 English datasets.", "labels": [], "entities": [{"text": "CoNLL-2012 English datasets", "start_pos": 92, "end_pos": 119, "type": "DATASET", "confidence": 0.9639760057131449}]}], "datasetContent": [{"text": "We use the three metrics that are most commonly used 9 , namely: MUC (Vilain et al., 1995) computes for each true entity cluster the number of system clusters that are needed to cover it.", "labels": [], "entities": [{"text": "MUC", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.7875917553901672}]}, {"text": "Precision is this quantity divided by the true cluster size minus one.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.991265594959259}]}, {"text": "Recall is obtained by reversing true and predicated clusters.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9592949151992798}]}, {"text": "F1 is the harmonic mean.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9957559704780579}]}, {"text": "B 3 (Bagga and Baldwin, 1998) computes recall and precision scores for each mention, based on the intersection between the system/true clusters for that mention.", "labels": [], "entities": [{"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9987592697143555}, {"text": "precision scores", "start_pos": 50, "end_pos": 66, "type": "METRIC", "confidence": 0.9689204096794128}]}, {"text": "Precision is the ratio of the intersection and the true cluster sizes, while recall is the ratio of the intersection to the system cluster sizes.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9916453957557678}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9996534585952759}]}, {"text": "Global recall, precision, and F1 scores are obtained by averaging over the mention scores.", "labels": [], "entities": [{"text": "recall", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9929863810539246}, {"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9992620348930359}, {"text": "F1 scores", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9802658557891846}]}, {"text": "CEAF (Luo, 2005) scores are obtained by computing the best one-to-one mapping between the system/true partitions, which is equivalent to finding the best optimal alignment in the bipartite graph formed out of these partitions.", "labels": [], "entities": [{"text": "CEAF (Luo, 2005) scores", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.8731702821595329}]}, {"text": "We use the \u03c6 4 similarity function from (.", "labels": [], "entities": [{"text": "\u03c6 4 similarity function", "start_pos": 11, "end_pos": 34, "type": "METRIC", "confidence": 0.8554870933294296}]}, {"text": "These metrics were recently used in the CoNLL-2011 and -2012 Shared Tasks.", "labels": [], "entities": [{"text": "CoNLL-2011", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.9107893109321594}]}, {"text": "In addition, these campaigns use an unweighted average over the F1 scores given by the three metrics.", "labels": [], "entities": [{"text": "F1", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.9990837574005127}]}, {"text": "Following common practice, we use micro-averaging when reporting our scores for entire datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pairwise scores on CoNLL-2012 test.", "labels": [], "entities": [{"text": "CoNLL-2012 test", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.9543120861053467}]}, {"text": " Table 2: CoNLL-2012 test (gold mentions): Closest-First, Best-First and Aggressive-Merge decoders.", "labels": [], "entities": [{"text": "CoNLL-2012 test", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.8125339448451996}]}]}