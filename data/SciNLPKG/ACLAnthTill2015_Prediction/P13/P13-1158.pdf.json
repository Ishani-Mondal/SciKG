{"title": [{"text": "Paraphrase-Driven Learning for Open Question Answering", "labels": [], "entities": [{"text": "Paraphrase-Driven Learning", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8031500279903412}, {"text": "Open Question Answering", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.6501244306564331}]}], "abstractContent": [{"text": "We study question answering as a machine learning problem, and induce a function that maps open-domain questions to queries over a database of web extractions.", "labels": [], "entities": [{"text": "question answering", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.8054344356060028}]}, {"text": "Given a large, community-authored, question-paraphrase corpus, we demonstrate that it is possible to learn a semantic lexicon and linear ranking function without manually annotating questions.", "labels": [], "entities": []}, {"text": "Our approach automatically generalizes a seed lexicon and includes a scal-able, parallelized perceptron parameter estimation scheme.", "labels": [], "entities": []}, {"text": "Experiments show that our approach more than quadruples the recall of the seed lexicon, with only an 8% loss in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.999161958694458}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9987466335296631}]}], "introductionContent": [{"text": "Open-domain question answering (QA) is a longstanding, unsolved problem.", "labels": [], "entities": [{"text": "Open-domain question answering (QA)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8178186019261678}]}, {"text": "The central challenge is to automate every step of QA system construction, including gathering large databases and answering questions against these databases.", "labels": [], "entities": [{"text": "QA system construction", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.9048075079917908}]}, {"text": "While there has been significant work on large-scale information extraction (IE) from unstructured text (, the problem of answering questions with the noisy knowledge bases that IE systems produce has received less attention.", "labels": [], "entities": [{"text": "information extraction (IE) from unstructured text", "start_pos": 53, "end_pos": 103, "type": "TASK", "confidence": 0.8838788717985153}]}, {"text": "In this paper, we present an approach for learning to map questions to formal queries over a large, open-domain database of extracted facts).", "labels": [], "entities": []}, {"text": "Our system learns from a large, noisy, questionparaphrase corpus, where question clusters have a common but unknown query, and can span a diverse set of topics.", "labels": [], "entities": []}, {"text": "shows example paraphrase clusters fora set of factual questions.", "labels": [], "entities": []}, {"text": "Such data provides strong signal for learning about lexical variation, but there area number Who wrote the Winnie the Pooh books?", "labels": [], "entities": []}, {"text": "Who is the author of winnie the pooh?", "labels": [], "entities": []}, {"text": "What was the name of the authur of winnie the pooh?", "labels": [], "entities": []}, {"text": "Who wrote the series of books for Winnie the poo?", "labels": [], "entities": []}, {"text": "Who wrote the children's storybook 'Winnie the Pooh'?", "labels": [], "entities": []}, {"text": "What is the best cure fora hangover?", "labels": [], "entities": []}, {"text": "The best way to recover from a hangover?", "labels": [], "entities": []}, {"text": "What takes away a hangover?", "labels": [], "entities": []}, {"text": "How do you lose a hangover?", "labels": [], "entities": []}, {"text": "What are social networking sites used for?", "labels": [], "entities": []}, {"text": "Why do people use social networking sites worldwide?", "labels": [], "entities": []}, {"text": "Advantages of using social network sites?", "labels": [], "entities": []}, {"text": "Why do people use social networks a lot?", "labels": [], "entities": []}, {"text": "Why do people communicate on social networking sites?", "labels": [], "entities": []}, {"text": "What are the pros and cons of social networking sites?", "labels": [], "entities": []}, {"text": "How do you say Santa Claus in Sweden?", "labels": [], "entities": [{"text": "Santa Claus in Sweden", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.5913813039660454}]}, {"text": "Say santa clause in sweden?", "labels": [], "entities": []}, {"text": "How do you say santa clause in swedish?", "labels": [], "entities": []}, {"text": "How do they say santa in Sweden?", "labels": [], "entities": []}, {"text": "In Sweden what is santa called?", "labels": [], "entities": []}, {"text": "Who is sweden santa?: Examples of paraphrase clusters from the WikiAnswers corpus.", "labels": [], "entities": [{"text": "WikiAnswers corpus", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.8513949513435364}]}, {"text": "Within each cluster, there is a wide range of syntactic and lexical variations. of challenges.", "labels": [], "entities": []}, {"text": "Given that the data is communityauthored, it will inevitably be incomplete, contain incorrectly tagged paraphrases, non-factual questions, and other sources of noise.", "labels": [], "entities": []}, {"text": "Our core contribution is anew learning approach that scalably sifts through this paraphrase noise, learning to answer abroad class of factual questions.", "labels": [], "entities": []}, {"text": "We focus on answering open-domain questions that can be answered with single-relation queries, e.g. all of the paraphrases of \"Who wrote Winnie the Pooh?\" and \"What cures a hangover?\" in.", "labels": [], "entities": []}, {"text": "The algorithm answers such questions by mapping them to executable queries over a tuple store containing relations such as authored(milne, winnie-the-pooh) and treat(bloody-mary, hangover-symptoms).", "labels": [], "entities": [{"text": "authored", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9701240062713623}, {"text": "treat", "start_pos": 160, "end_pos": 165, "type": "METRIC", "confidence": 0.9881889820098877}]}, {"text": "The approach automatically induces lexical structures, which are combined to build queries for unseen questions.", "labels": [], "entities": []}, {"text": "It learns lexical equivalences for relations (e.g., wrote, authored, and creator), entities (e.g., Winnie the Pooh or Pooh Bear), and question templates (e.g., Who r thee books? and Who is the r of e?).", "labels": [], "entities": []}, {"text": "Crucially, the approach does not require any explicit labeling of the questions in our paraphrase corpus.", "labels": [], "entities": []}, {"text": "Instead, we use 16 seed question templates and string-matching to find high-quality queries fora small subset of the questions.", "labels": [], "entities": []}, {"text": "The algorithm uses learned word alignments to aggressively generalize the seeds, producing a large set of possible lexical equivalences.", "labels": [], "entities": []}, {"text": "We then learn a linear ranking model to filter the learned lexical equivalences, keeping only those that are likely to answer questions well in practice.", "labels": [], "entities": []}, {"text": "Experimental results on 18 million paraphrase pairs gathered from WikiAnswers 1 demonstrate the effectiveness of the overall approach.", "labels": [], "entities": []}, {"text": "We performed an end-to-end evaluation against a database of 15 million facts automatically extracted from general web text).", "labels": [], "entities": []}, {"text": "On known-answerable questions, the approach achieved 42% recall, with 77% precision, more than quadrupling the recall over a baseline system.", "labels": [], "entities": [{"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9993236064910889}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.999406099319458}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9989458918571472}]}, {"text": "In sum, we make the following contributions: \u2022 We introduce PARALEX, an end-to-end opendomain question answering system.", "labels": [], "entities": [{"text": "PARALEX", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9711816906929016}, {"text": "question answering", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7381217926740646}]}, {"text": "\u2022 We describe scalable learning algorithms that induce general question templates and lexical variants of entities and relations.", "labels": [], "entities": []}, {"text": "These algorithms require no manual annotation and can be applied to large, noisy databases of relational triples.", "labels": [], "entities": []}, {"text": "\u2022 We evaluate PARALEX on the end-task of answering questions from WikiAnswers using a database of web extractions, and show that it outperforms baseline systems.", "labels": [], "entities": [{"text": "PARALEX", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.7844428420066833}]}, {"text": "\u2022 We release our learned lexicon and question-paraphrase dataset to the research community, available at http://openie.cs.washington.edu.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the following systems: \u2022 PARALEX: the full system, using the lexical learning and parameter learning algorithms from Section 5.", "labels": [], "entities": [{"text": "PARALEX", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9836087822914124}]}, {"text": "\u2022 NoParam: PARALEX without the learned parameters.", "labels": [], "entities": [{"text": "PARALEX", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9728425145149231}]}, {"text": "\u2022 InitOnly: PARALEX using only the initial seed lexicon.", "labels": [], "entities": [{"text": "PARALEX", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.971191942691803}]}, {"text": "We evaluate the systems' performance on the endtask of QA on WikiAnswers questions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Performance on WikiAnswers questions  known to be answerable using REVERB.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9947417974472046}]}, {"text": " Table 5: Ablation of the learned lexical items.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.96500563621521}]}]}