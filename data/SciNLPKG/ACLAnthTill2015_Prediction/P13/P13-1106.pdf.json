{"title": [{"text": "Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.6775894314050674}, {"text": "Bilingual Named Entity Recognition", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.6395856887102127}]}], "abstractContent": [{"text": "Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.7968818843364716}]}, {"text": "However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors.", "labels": [], "entities": [{"text": "bilingual tagging", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.8136270344257355}]}, {"text": "We observe that NER label information can be used to correct alignment mistakes , and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two uni-directional alignment models.", "labels": [], "entities": [{"text": "NER tagging", "start_pos": 136, "end_pos": 147, "type": "TASK", "confidence": 0.9072531163692474}, {"text": "word alignment", "start_pos": 161, "end_pos": 175, "type": "TASK", "confidence": 0.7250025272369385}]}, {"text": "We introduce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions.", "labels": [], "entities": []}, {"text": "We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space.", "labels": [], "entities": []}, {"text": "Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolin-gual baselines.", "labels": [], "entities": [{"text": "OntoNotes dataset", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.8901286423206329}, {"text": "NER", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.9323869347572327}, {"text": "word alignment", "start_pos": 113, "end_pos": 127, "type": "TASK", "confidence": 0.729058712720871}]}], "introductionContent": [{"text": "We study the problem of Named Entity Recognition (NER) in a bilingual context, where the goal is to annotate parallel bi-texts with named entity tags.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.8233025968074799}]}, {"text": "This is a particularly important problem for machine translation (MT) since entities such as person names, locations, organizations, etc.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.8543817520141601}]}, {"text": "carry much of the information expressed in the source sentence.", "labels": [], "entities": []}, {"text": "Recognizing them provides useful information for phrase detection and word sense disambiguation (e.g., \"melody\" as in a female name has a different translation from the word \"melody\" in a musical sense), and can be directly leveraged to improve translation quality).", "labels": [], "entities": [{"text": "phrase detection", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.8757619559764862}, {"text": "word sense disambiguation", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.6865478356679281}]}, {"text": "We can also automatically construct a named entity translation lexicon by annotating and extracting entities from bi-texts, and use it to improve MT performance ().", "labels": [], "entities": [{"text": "named entity translation lexicon", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.7932362854480743}, {"text": "MT", "start_pos": 146, "end_pos": 148, "type": "TASK", "confidence": 0.9814964532852173}]}, {"text": "Previous work such as, and have also demonstrated that bitexts annotated with NER tags can provide useful additional training sources for improving the performance of standalone monolingual taggers.", "labels": [], "entities": []}, {"text": "Because human translation in general preserves semantic equivalence, bi-texts represent two perspectives on the same semantic content.", "labels": [], "entities": []}, {"text": "As a result, we can find complementary cues in the two languages that help to disambiguate named entity mentions (.", "labels": [], "entities": []}, {"text": "For example, the English word \"Jordan\" can be either a last name or a country.", "labels": [], "entities": []}, {"text": "Without sufficient context it can be difficult to distinguish the two; however, in Chinese, these two senses are disambiguated: \"\u4e54\u4e39\" as a last name, and \"\u7ea6\u65e6\" as a country name.: Example of NER labels between two word-aligned bilingual parallel sentences.", "labels": [], "entities": [{"text": "NER labels between two word-aligned bilingual parallel sentences", "start_pos": 189, "end_pos": 253, "type": "TASK", "confidence": 0.7639246508479118}]}, {"text": "The tag is an example of a wrong tag assignment.", "labels": [], "entities": []}, {"text": "The dashed alignment link between e 3 and f 2 is an example of alignment error.", "labels": [], "entities": []}, {"text": "previous applications of the DD method in NLP, where the model typically factors over two components and agreement is to besought between the two (, our method decomposes the larger graphical model into many overlapping components where each alignment edge forms a separate factor.", "labels": [], "entities": []}, {"text": "We design clique potentials over the alignment-based edges to encourage entity tag agreements.", "labels": [], "entities": []}, {"text": "Our method does not require any manual annotation of word alignments or named entities over the bilingual training data.", "labels": [], "entities": [{"text": "word alignments or named entities", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.8174962401390076}]}, {"text": "The aforementioned BI-NER model assumes fixed alignment input given by an underlying word aligner.", "labels": [], "entities": []}, {"text": "But the entity span and type predictions given by the NER models contain complementary information for correcting alignment errors.", "labels": [], "entities": [{"text": "correcting alignment", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.8376286327838898}]}, {"text": "To capture this source of information, we present a novel extension that combines the BI-NER model with two uni-directional HMM-based alignment models, and perform joint decoding of NER and word alignments.", "labels": [], "entities": [{"text": "BI-NER", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9061474204063416}]}, {"text": "The new model (denoted as BI-NER-WA) factors over five components: one NER model and one word alignment model for each language, plus a joint NER-alignment model which not only enforces NER label agreements but also facilitates message passing among the other four components.", "labels": [], "entities": [{"text": "BI-NER-WA", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.951440691947937}, {"text": "message passing", "start_pos": 228, "end_pos": 243, "type": "TASK", "confidence": 0.7031444609165192}]}, {"text": "An extended DD decoding algorithm is again employed to perform approximate inference.", "labels": [], "entities": [{"text": "approximate inference", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.8177562654018402}]}, {"text": "We give a formal definition of the Bi-NER model in Section 2, and then move to present the Bi-NER-WA model in Section 3.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate on the large OntoNotes (v4.0) corpus () which contains manually annotated NER tags for both Chinese and English.", "labels": [], "entities": [{"text": "OntoNotes (v4.0) corpus", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.8217318415641784}]}, {"text": "Document pairs are sentence aligned using the Champollion Tool Kit).", "labels": [], "entities": [{"text": "Champollion Tool Kit", "start_pos": 46, "end_pos": 66, "type": "DATASET", "confidence": 0.8720366954803467}]}, {"text": "After discarding sentences with no aligned counterpart, a total of 402 documents and 8,249 parallel sentence pairs were used for evaluation.", "labels": [], "entities": []}, {"text": "We will refer to this evaluation set as full-set.", "labels": [], "entities": []}, {"text": "We use odd-numbered documents as the dev set and evennumbered documents as the blind test set.", "labels": [], "entities": []}, {"text": "We did not perform parameter tuning on the dev set to optimize performance, instead we fix the initial learning rate to 0.5 and maximum iterations to 1,000 in all DD experiments.", "labels": [], "entities": []}, {"text": "We only use the dev set for model development.", "labels": [], "entities": [{"text": "model development", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.6862612366676331}]}, {"text": "The Stanford CRF-based NER tagger was used as the monolingual component in our models ().", "labels": [], "entities": [{"text": "NER tagger", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.6821580827236176}]}, {"text": "It also serves as a stateof-the-art monolingual baseline for both English and Chinese.", "labels": [], "entities": []}, {"text": "For English, we use the default tagger setting from.", "labels": [], "entities": []}, {"text": "For Chinese, we use an improved set of features over the default tagger, which includes distributional similarity features trained on large amounts of nonoverlapping data.", "labels": [], "entities": []}, {"text": "We train the two CRF models on all portions of the OntoNotes corpus that are annotated with named entity tags, except the parallel-aligned portion which we reserve for development and test purposes.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.910869687795639}]}, {"text": "In total, there are about 660 training documents (\u223c16k sentences) for Chinese and 1,400 documents (\u223c39k sentences) for English.", "labels": [], "entities": []}, {"text": "Out of the 18 named entity types that are annotated in OntoNotes, which include person, location, date, money, and soon, we select the four most commonly seen named entity types for evaluation.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.8919450640678406}]}, {"text": "They are person, location, organization and GPE.", "labels": [], "entities": [{"text": "GPE", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9919306635856628}]}, {"text": "All entities of these four types are converted to the standard BIO format, and background tokens and all other entity types are marked with tag O.", "labels": [], "entities": []}, {"text": "When we consider label agreements over aligned word pairs in all bilingual agreement models, we ignore the distinction between B-and Itags.", "labels": [], "entities": []}, {"text": "We report standard NER measures (entity precision (P), recall (R) and F 1 score) on the test set.", "labels": [], "entities": [{"text": "NER", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8170616626739502}, {"text": "entity precision (P)", "start_pos": 33, "end_pos": 53, "type": "METRIC", "confidence": 0.8058105707168579}, {"text": "recall (R)", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9716033935546875}, {"text": "F 1 score", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9835885365804037}]}, {"text": "Statistical significance tests are done using the paired bootstrap resampling method.", "labels": [], "entities": []}, {"text": "For alignment experiments, we train two uni-directional HMM models as our baseline and monolingual alignment models.", "labels": [], "entities": [{"text": "alignment", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9701429605484009}]}, {"text": "The parameters of the HMM were initialized by IBM Model 1 using the agreement-based EM training algorithms from.", "labels": [], "entities": []}, {"text": "Each model is trained for 2 iterations over a parallel corpus of 12 million English words and Chinese words, almost twice as much data as used in previous work that yields state-of-the-art unsupervised alignment results (.", "labels": [], "entities": []}, {"text": "Word alignment evaluation is done over the sections of OntoNotes that have matching goldstandard word alignment annotations from GALE Y1Q4 dataset.", "labels": [], "entities": [{"text": "Word alignment evaluation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8308943708737692}, {"text": "OntoNotes", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.9082052707672119}, {"text": "GALE Y1Q4 dataset", "start_pos": 129, "end_pos": 146, "type": "DATASET", "confidence": 0.9226330121358236}]}, {"text": "This subset contains 288 documents and 3,391 sentence pairs.", "labels": [], "entities": []}, {"text": "We will refer to this subset as wa-subset.", "labels": [], "entities": []}, {"text": "This evaluation set is over 20 times larger than the 150 sentences set used inmost past evaluations.", "labels": [], "entities": []}, {"text": "Alignments input to the BI-NER model are produced by thresholding the averaged posterior probability at 0.5.", "labels": [], "entities": [{"text": "BI-NER", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.643308699131012}]}, {"text": "In joint NER and alignment experiments, instead of posterior thresholding, we take the direct intersection of the Viterbi-best alignment of the two directional models.", "labels": [], "entities": [{"text": "NER", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.983630895614624}]}, {"text": "We report the standard P, R, F 1 and Alignment Error Rate (AER) measures for alignment experiments.", "labels": [], "entities": [{"text": "P, R, F 1", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.7064692129691442}, {"text": "Alignment Error Rate (AER)", "start_pos": 37, "end_pos": 63, "type": "METRIC", "confidence": 0.9469417234261831}, {"text": "alignment", "start_pos": 77, "end_pos": 86, "type": "TASK", "confidence": 0.9755867719650269}]}, {"text": "An important past work to make comparisons with is.", "labels": [], "entities": []}, {"text": "Their method is similar to ours in that they also model bilingual agreement in conjunction with two CRFbased monolingual models.", "labels": [], "entities": []}, {"text": "But instead of using just the PMI scores of bilingual NE pairs, as in our work, they employed a feature-rich log-linear model to capture bilingual correlations.", "labels": [], "entities": []}, {"text": "Parameters in their log-linear model require training with bilingually annotated data, which is not readily available.", "labels": [], "entities": []}, {"text": "To counter this problem, they proposed an \"up-training\" method which simulates a supervised learning environment by pairing a weak classifier with strong classifiers, and train the bilingual model to rank the output of the strong classifier highly among the N-best outputs of the weak classifier.", "labels": [], "entities": []}, {"text": "In order to compare directly with their method, we obtained the code behind: NER results on bilingual parallel test set.", "labels": [], "entities": []}, {"text": "Best numbers on each measure that are statistically significantly better than the monolingual baseline and are highlighted in bold.", "labels": [], "entities": []}, {"text": "training the reranker, and the reranker model selection was performed on the development dataset.", "labels": [], "entities": [{"text": "reranker model selection", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.5392814576625824}]}], "tableCaptions": [{"text": " Table 1: NER results on bilingual parallel test set.  Best numbers on each measure that are statistically  significantly better than the monolingual baseline  and", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9232515096664429}]}, {"text": " Table 2: Joint alignment and NER test results. +NC means incorporating additional neighbor constraints  from DeNero and Macherey (2011) to the model. Best number in each column is highlighted in bold.", "labels": [], "entities": [{"text": "Joint alignment", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6727483570575714}, {"text": "NER", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9667780995368958}, {"text": "NC", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9675427079200745}]}]}