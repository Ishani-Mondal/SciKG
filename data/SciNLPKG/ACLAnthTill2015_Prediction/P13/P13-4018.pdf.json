{"title": [{"text": "A Java Framework for Multilingual Definition and Hypernym Extraction", "labels": [], "entities": [{"text": "Hypernym Extraction", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.6843068152666092}]}], "abstractContent": [{"text": "In this paper we present a demonstration of a multilingual generalization of Word-Class Lattices (WCLs), a supervised lattice-based model used to identify textual definitions and extract hypernyms from them.", "labels": [], "entities": []}, {"text": "Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia.", "labels": [], "entities": []}, {"text": "We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences.", "labels": [], "entities": [{"text": "definition and hypernym extraction from user-provided sentences", "start_pos": 151, "end_pos": 214, "type": "TASK", "confidence": 0.7475063034466335}]}], "introductionContent": [{"text": "Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (), taxonomy learning, domain Word Sense Disambiguation (, automatic acquisition of semantic predicates, relation extraction () and, more in general, knowledge acquisition (.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 165, "end_pos": 183, "type": "TASK", "confidence": 0.8371366858482361}, {"text": "domain Word Sense Disambiguation", "start_pos": 207, "end_pos": 239, "type": "TASK", "confidence": 0.6492201089859009}, {"text": "automatic acquisition of semantic predicates", "start_pos": 243, "end_pos": 287, "type": "TASK", "confidence": 0.6494152307510376}, {"text": "relation extraction", "start_pos": 289, "end_pos": 308, "type": "TASK", "confidence": 0.7849119901657104}, {"text": "knowledge acquisition", "start_pos": 334, "end_pos": 355, "type": "TASK", "confidence": 0.7895159125328064}]}, {"text": "Unfortunately, constructing and updating such resources requires the effort of a team of experts.", "labels": [], "entities": []}, {"text": "Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains.", "labels": [], "entities": []}, {"text": "Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of interest.", "labels": [], "entities": []}, {"text": "Whilst it is not feasible to search texts manually for definitions in several languages, the task of extracting definitional information can be automatized by means of Machine Learning (ML) and Natural Language Processing (NLP) techniques.", "labels": [], "entities": []}, {"text": "Many approaches (, inter alia) build upon lexicosyntactic patterns, inspired by the seminal work of.", "labels": [], "entities": []}, {"text": "However, these methods suffer from two signifiicant drawbacks: on the one hand, low recall (as definitional sentences occur in highly variable syntactic structures), and, on the other hand, noise (because the most frequent definitional pattern -X is a Y -is inherently very noisy).", "labels": [], "entities": [{"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9989503026008606}]}, {"text": "A recent approach to definition and hypernym extraction, called Word-Class Lattices, overcomes these issues by addressing the variability of definitional sentences and providing a flexible way of automatically extracting hypernyms from them.", "labels": [], "entities": [{"text": "definition and hypernym extraction", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.8670550286769867}]}, {"text": "To do so, lattice-based classifiers are learned from a training set of textual definitions.", "labels": [], "entities": []}, {"text": "Training sentences are automatically clustered by similarity and, for each such cluster, a lattice classifier is learned which models the variants of the definition template detected.", "labels": [], "entities": []}, {"text": "A lattice is a directed acyclic graph, a subclass of non-deterministic finite state automata.", "labels": [], "entities": []}, {"text": "The purpose of the lattice structure is to preserve (in a compact form) the salient differences among distinct sequences.", "labels": [], "entities": []}, {"text": "In this paper we present a demonstration of Word-Class Lattices by providing a Java API and a Web application for online usage.", "labels": [], "entities": []}, {"text": "Since multilinguality is a key need in today's information society, and because WCLs have been tested overwhelmingly only with the English language, we provide experiments for three different languages, namely English, French and Italian.", "labels": [], "entities": []}, {"text": "To do so, in contrast to, who created a manually annotated training set of definitions, we provide a heuristic method for the automatic acquisition of reliable training sets from Wikipedia, and use them to determine the robustness and generalization power of WCLs.", "labels": [], "entities": []}, {"text": "We show high performance in definition and hypernym extraction for our three languages.", "labels": [], "entities": [{"text": "hypernym extraction", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.6587663888931274}]}], "datasetContent": [{"text": "We tested the newly acquired training dataset against two test datasets.", "labels": [], "entities": []}, {"text": "The first dataset was our random sampling of 1000 Wikipedia test sentences which we had set aside for each language (no intersection with the training set, see previous section).", "labels": [], "entities": []}, {"text": "The second dataset was the same one used in, made up of sentences from the ukWaC Web corpus) and used to estimate the definition and hypernym extraction performance on an open text corpus.", "labels": [], "entities": [{"text": "ukWaC Web corpus", "start_pos": 75, "end_pos": 91, "type": "DATASET", "confidence": 0.9846970240275065}, {"text": "hypernym extraction", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.6462628394365311}]}, {"text": "shows the results obtained on definition (column 2-4) and hypernym extraction (column 5-7) in terms of precision (P), recall (R) and accuracy (A) on our first dataset.", "labels": [], "entities": [{"text": "hypernym extraction", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.6794173866510391}, {"text": "precision (P)", "start_pos": 103, "end_pos": 116, "type": "METRIC", "confidence": 0.9508535861968994}, {"text": "recall (R)", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9505622237920761}, {"text": "accuracy (A)", "start_pos": 133, "end_pos": 145, "type": "METRIC", "confidence": 0.9411923736333847}]}, {"text": "Note that accuracy also takes into account candidate definitions in the test set which were tagged as non-definitional (see Section 3.1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991934895515442}]}, {"text": "In the Table we compare the performance of our English WCL trained from Wikipedia sentences using our automatic procedure against the original performance of WCL We used the 21-09-2012 (EN), 17-09-2012 (FR), 21-09-2012 (IT) dumps.", "labels": [], "entities": [{"text": "IT", "start_pos": 220, "end_pos": 222, "type": "METRIC", "confidence": 0.8913754820823669}]}, {"text": "Note that the first sentence of a Wikipedia page might seldom be non-definitional, such as \"Basmo fortress is located in the north-western part . .", "labels": [], "entities": [{"text": "Basmo fortress", "start_pos": 92, "end_pos": 106, "type": "DATASET", "confidence": 0.6605688631534576}]}, {"text": "\".: Estimated WCL definition extraction precision (P) and recall (R), testing a sample of ukWaC sentences.", "labels": [], "entities": [{"text": "WCL definition extraction precision (P)", "start_pos": 14, "end_pos": 53, "type": "METRIC", "confidence": 0.8351003101893834}, {"text": "recall (R)", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9708410948514938}, {"text": "ukWaC", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.9524187445640564}]}], "tableCaptions": [{"text": " Table 2: The number of Wikipedia pages and the  resulting automatically annotated definitions.", "labels": [], "entities": []}, {"text": " Table 3: Precision (P), recall (R) and accuracy  (A) of definition and hypernym extraction when  testing on our dataset of 1000 randomly sam- pled Wikipedia first-paragraph sentences. EN  (2010) refers to the WCL learned from the origi- nal manually-curated training set from Navigli and  Velardi (2010), while EN, FR and IT refer to WCL  trained, respectively, with one of the three training  sets automatically acquired from Wikipedia.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9278493076562881}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9363178163766861}, {"text": "accuracy  (A)", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.8645485490560532}, {"text": "hypernym extraction", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7199321389198303}, {"text": "FR", "start_pos": 316, "end_pos": 318, "type": "METRIC", "confidence": 0.9839438199996948}, {"text": "IT", "start_pos": 323, "end_pos": 325, "type": "METRIC", "confidence": 0.9182748794555664}]}, {"text": " Table 4: Estimated WCL definition extraction  precision (P) and recall (R), testing a sample of  ukWaC sentences.", "labels": [], "entities": [{"text": "Estimated WCL definition extraction  precision (P)", "start_pos": 10, "end_pos": 60, "type": "METRIC", "confidence": 0.8278200551867485}, {"text": "recall (R)", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.975048691034317}, {"text": "ukWaC", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.9415333271026611}]}]}