{"title": [], "abstractContent": [{"text": "We propose anew variant of Tree-Adjoining Grammar that allows adjunction of full wrapping trees but still bears only context-free expressivity.", "labels": [], "entities": []}, {"text": "We provide a transformation to context-free form, and a further reduction in probabilistic model size through factorization and pooling of parameters.", "labels": [], "entities": []}, {"text": "This collapsed context-free form is used to implement efficient grammar estimation and parsing algorithms.", "labels": [], "entities": [{"text": "grammar estimation", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7204225659370422}]}, {"text": "We perform parsing experiments the Penn Treebank and draw comparisons to Tree-Substitution Grammars and between different variations in probabilistic model design.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.9962811470031738}]}, {"text": "Examination of the most probable derivations reveals examples of the linguistically relevant structure that our variant makes possible.", "labels": [], "entities": []}], "introductionContent": [{"text": "While it is widely accepted that natural language is not context-free, practical limitations of existing algorithms motivate Context-Free Grammars (CFGs) as a good balance between modeling power and asymptotic performance.", "labels": [], "entities": []}, {"text": "In constituent-based parsing work, the prevailing technique to combat this divide between efficient models and real world data has been to selectively strengthen the dependencies in a CFG by increasing the grammar size through methods such as symbol refinement ().", "labels": [], "entities": [{"text": "constituent-based parsing", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.4985847622156143}, {"text": "symbol refinement", "start_pos": 243, "end_pos": 260, "type": "TASK", "confidence": 0.7174383401870728}]}, {"text": "Another approach is to employ a more powerful grammatical formalism and devise constraints and transformations that allow use of essential efficient algorithms such as the Inside-Outside algorithm and CYK parsing.", "labels": [], "entities": [{"text": "CYK parsing", "start_pos": 201, "end_pos": 212, "type": "TASK", "confidence": 0.7808576822280884}]}, {"text": "Tree-Adjoining Grammar (TAG) is a natural starting point for such methods as it is the canonical member of the mildly context-sensitive family, falling just above CFGs in the hierarchy of formal grammars.", "labels": [], "entities": []}, {"text": "TAG has a crucial advantage over CFGs in its ability to represent long distance interactions in the face of the interposing variations that commonly manifest in natural language).", "labels": [], "entities": []}, {"text": "Consider, for example, the sentences These pretzels are making me thirsty.", "labels": [], "entities": []}, {"text": "These pretzels are not making me thirsty.", "labels": [], "entities": []}, {"text": "These pretzels that I ate are making me thirsty.", "labels": [], "entities": []}, {"text": "Using a context-free language model with proper phrase bracketing, the connection between the words pretzels and thirsty must be recorded with three separate patterns, which can lead to poor generalizability and unreliable sparse frequency estimates in probabilistic models.", "labels": [], "entities": [{"text": "phrase bracketing", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7642281949520111}]}, {"text": "While these problems can be overcome to some extent with large amounts of data, redundant representation of patterns is particularly undesirable for systems that seek to extract coherent and concise information from text.", "labels": [], "entities": []}, {"text": "TAG allows a linguistically motivated treatment of the example sentences above by generating the last two sentences through modification of the first, applying operations corresponding to negation and the use of a subordinate clause.", "labels": [], "entities": [{"text": "TAG", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.3941936194896698}]}, {"text": "Unfortunately, the added expressive power of TAG comes with O(n 6 ) time complexity for essential algorithms on sentences of length n, as opposed to O(n 3 ) for the CFG.", "labels": [], "entities": [{"text": "O(n 6 ) time complexity", "start_pos": 60, "end_pos": 83, "type": "METRIC", "confidence": 0.9061494043895176}, {"text": "CFG", "start_pos": 165, "end_pos": 168, "type": "DATASET", "confidence": 0.9659702181816101}]}, {"text": "This makes TAG infeasible to analyze real world data in a reasonable time frame.", "labels": [], "entities": [{"text": "TAG", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9494069814682007}]}, {"text": "In this paper, we define OSTAG, anew way to constrain TAG in a conceptually simple way so that it can be reduced to a CFG, allowing the use of traditional cubic-time algorithms.", "labels": [], "entities": []}, {"text": "The reduction is reversible, so that the original TAG derivation can be recovered exactly from the CFG parse.", "labels": [], "entities": [{"text": "CFG parse", "start_pos": 99, "end_pos": 108, "type": "DATASET", "confidence": 0.9412609338760376}]}, {"text": "We provide this reduction in detail below and highlight the compression afforded by this TAG variant on synthetic formal languages.", "labels": [], "entities": []}, {"text": "We evaluate OSTAG on the familiar task of parsing the Penn Treebank.", "labels": [], "entities": [{"text": "OSTAG", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.5546838045120239}, {"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.98427814245224}, {"text": "Penn Treebank", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.8896205723285675}]}, {"text": "Using an automatically induced Tree-Substitution Grammar (TSG), we heuristically extract an OSTAG and estimate its parameters from data using models with various reduced probabilistic models of adjunction.", "labels": [], "entities": []}, {"text": "We contrast these models and investigate the use of adjunction in the most probable derivations of the test corpus, demonstating the superior modeling performance of OSTAG over TSG.", "labels": [], "entities": []}], "datasetContent": [{"text": "As a proof of concept, we investigate OSTAG in the context of the classic Penn Treebank statistical parsing setup; training on section 2-21 and testing on section 23.", "labels": [], "entities": [{"text": "OSTAG", "start_pos": 38, "end_pos": 43, "type": "TASK", "confidence": 0.6282952427864075}, {"text": "Penn Treebank", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9893179535865784}, {"text": "statistical parsing", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7119372189044952}]}, {"text": "For preprocessing, words that occur only once in the training data are mapped to the unknown categories employed in the parser of.", "labels": [], "entities": []}, {"text": "We also applied the annotation from that appends \"-U\" to each nonterminal node with a single child, drastically reducing the presence of looping unary chains.", "labels": [], "entities": []}, {"text": "This allows the use of a coarse to fine parsing strategy) in which a sentence is first parsed with the Maximum Likelihood PCFG and only constituents whose probability exceeds a cutoff of 10 \u22124 are allowed in the OSTAG chart.", "labels": [], "entities": [{"text": "OSTAG chart", "start_pos": 212, "end_pos": 223, "type": "DATASET", "confidence": 0.8794201612472534}]}, {"text": "Designed to facilitate sister adjunction, we define our binarization scheme by example in which the added nodes, indicated by @, record both the parent and head child of the rule., retaining all TSG rules that appear in at least one derivation in after 1000 iterations of sampling.", "labels": [], "entities": []}, {"text": "We use EM to estimate the parameters of this grammar on sections 2-21, and use this as our baseline.", "labels": [], "entities": [{"text": "EM", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.6492785215377808}]}, {"text": "To generate a set of TAG rules, we consider each rule in our baseline TSG and find all possi- Figure 5: Parsing F-Score for the models under comparison for both the full test set and sentences of length 40 or less.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 112, "end_pos": 119, "type": "METRIC", "confidence": 0.7972481846809387}]}, {"text": "For the OSTAG models, we list the number of adjunctions in the MPD of the full test set, as well as the number of wrapping adjunctions.", "labels": [], "entities": [{"text": "OSTAG", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.7861264944076538}]}, {"text": "ble auxiliary root and foot node pairs it contains.", "labels": [], "entities": []}, {"text": "For each such root/foot pair, we include the TAG rule implied by removal of the structure above the root and below the foot.", "labels": [], "entities": [{"text": "TAG", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9987024068832397}]}, {"text": "We also include the TSG rule left behind when the adjunction of this auxiliary tree is removed.", "labels": [], "entities": [{"text": "TSG", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9527304768562317}]}, {"text": "To be sure that experimental gains are not due to this increased number of TSG initial trees, we calculate parameters using EM for this expanded TSG and use it as a second baseline (TSG ).", "labels": [], "entities": [{"text": "EM", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.8349044322967529}]}, {"text": "With our full set of initial and auxiliary trees, we use EM and the PCFG reduction described above to estimate the parameters of an OSTAG.", "labels": [], "entities": [{"text": "EM", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.8185366988182068}, {"text": "OSTAG", "start_pos": 132, "end_pos": 137, "type": "DATASET", "confidence": 0.7962749600410461}]}, {"text": "We investigate three models for the probability of adjunction at anode.", "labels": [], "entities": []}, {"text": "The first uses a conservative number of parameters, with a Bernoulli variable for each symbol (OSTAG 1 ).", "labels": [], "entities": []}, {"text": "The second employs more parameters, conditioning on both the node's symbol and the symbol of its leftmost child (OSTAG 2 ).The third is highly parameterized but most prone to data sparsity, with a separate Bernoulli distribution for each Goodman index \u03b7 (OSTAG 3 ).", "labels": [], "entities": [{"text": "OSTAG", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.8815336227416992}]}, {"text": "We report results for Most Probable Derivation (MPD) parses of section 23 in.", "labels": [], "entities": [{"text": "Most Probable Derivation (MPD) parses", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.6096066364220211}]}, {"text": "Our results show that OSTAG outperforms both baselines.", "labels": [], "entities": [{"text": "OSTAG", "start_pos": 22, "end_pos": 27, "type": "TASK", "confidence": 0.5950414538383484}]}, {"text": "Furthermore, the various parameterizations of adjunction with OSTAG indicate that, at least in the case of the Penn Treebank, the finer grained modeling of a full table of adjunction probabilities for each Goodman index OSTAG 3 overcomes the danger of sparse data estimates.", "labels": [], "entities": [{"text": "OSTAG", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.9214161038398743}, {"text": "Penn Treebank", "start_pos": 111, "end_pos": 124, "type": "DATASET", "confidence": 0.9948144257068634}]}, {"text": "Not only does such a model lead to better parsing performance, but it uses adjunction more extensively than its more lightly parameterized alternatives.", "labels": [], "entities": [{"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9681246280670166}]}, {"text": "While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang  The OSTAG constraint can be relaxed as described in Section 4.2 to allow any finite number of on-spine adjunctions without sacrificing contextfree form.", "labels": [], "entities": []}, {"text": "However, the increase to the grammar constant quickly makes parsing with such models an arduous task.", "labels": [], "entities": [{"text": "parsing", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9790291786193848}]}, {"text": "To determine the effect of such a relaxation, we allow a single level of on-spine adjunction using the adjunction model of OSTAG 1 , and estimate this model with EM on the training data.", "labels": [], "entities": [{"text": "OSTAG 1", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.8682520091533661}, {"text": "EM", "start_pos": 162, "end_pos": 164, "type": "METRIC", "confidence": 0.9957000017166138}]}, {"text": "We parse sentences of length 40 or less in section 23 and observe that on-spine adjunction is never used in the MPD parses.", "labels": [], "entities": [{"text": "MPD parses", "start_pos": 112, "end_pos": 122, "type": "TASK", "confidence": 0.5799391269683838}]}, {"text": "This suggests that the OSTAG constraint is reasonable, at least for the domain of English news text.", "labels": [], "entities": [{"text": "OSTAG", "start_pos": 23, "end_pos": 28, "type": "TASK", "confidence": 0.5947684049606323}]}, {"text": "We performed further examination of the MPD using OSTAG for each of the sentences in the test corpus.", "labels": [], "entities": [{"text": "OSTAG", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.5830315351486206}]}, {"text": "As an artifact of the English language, the majority have their foot node on the left spine and would also be usable by TIG, and so we discuss the instances of wrapping auxiliary trees in these derivations that are uniquely available to OSTAG.", "labels": [], "entities": [{"text": "TIG", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.6257075667381287}, {"text": "OSTAG", "start_pos": 237, "end_pos": 242, "type": "DATASET", "confidence": 0.8866258859634399}]}, {"text": "We remove binarization for clarity and denote the foot node with an asterisk.", "labels": [], "entities": []}, {"text": "A frequent use of wrapping adjunction is to coordinate symbols such as quotes, parentheses, and dashes on both sides of a noun phrase.", "labels": [], "entities": []}, {"text": "One common wrapping auxiliary tree in our experiments is NP \" NP* \" PP This is used frequently in the news text of the Wall Street Journal for reported speech when avoiding a full quotation.", "labels": [], "entities": [{"text": "news text of the Wall Street Journal", "start_pos": 102, "end_pos": 138, "type": "DATASET", "confidence": 0.7374291249683925}]}, {"text": "This sentence is an example of the way the rule is employed, using what Joshi and Schabes (1997) referred to as \"factoring recursion from linguistic constraints\" with TAG.", "labels": [], "entities": []}, {"text": "Note that replacing the quoted noun phrase and its following prepositional phrase with the noun phrase itself yields a valid sentence, inline with the linguistic theory underlying TAG.", "labels": [], "entities": []}, {"text": "Another frequent wrapping rule, shown below, allows direct coordination between the contents of an appositive with the rest of the sentence.", "labels": [], "entities": []}], "tableCaptions": []}