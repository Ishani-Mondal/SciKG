{"title": [{"text": "The Role of Syntax in Vector Space Models of Compositional Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing.", "labels": [], "entities": []}, {"text": "In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Com-binatory Categorial Autoencoders.", "labels": [], "entities": []}, {"text": "This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence.", "labels": [], "entities": []}, {"text": "We use this model to learn high dimensional em-beddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since Frege stated his 'Principle of Semantic Compositionality' in 1892 researchers have pondered both how the meaning of a complex expression is determined by the meanings of its parts, and how those parts are combined..", "labels": [], "entities": []}, {"text": "Over a hundred years on the choice of representational unit for this process of compositional semantics, and how these units combine, remain open questions.", "labels": [], "entities": []}, {"text": "Frege's principle maybe debatable from a linguistic and philosophical standpoint, but it has provided a basis fora range of formal approaches to semantics which attempt to capture meaning in logical models.", "labels": [], "entities": []}, {"text": "The Montague grammar) is a prime example for this, building a model of composition based on lambdacalculus and formal logic.", "labels": [], "entities": [{"text": "Montague grammar", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9008896052837372}]}, {"text": "More recent work in this field includes the Combinatory Categorial Grammar (CCG), which also places increased emphasis on syntactic coverage.", "labels": [], "entities": [{"text": "Combinatory Categorial Grammar (CCG)", "start_pos": 44, "end_pos": 80, "type": "TASK", "confidence": 0.7736810048421224}]}, {"text": "Recently those searching for the right representation for compositional semantics have drawn inspiration from the success of distributional models of lexical semantics.", "labels": [], "entities": []}, {"text": "This approach represents single words as distributional vectors, implying that a word's meaning is a function of the environment it appears in, be that its syntactic role or co-occurrences with other words (.", "labels": [], "entities": []}, {"text": "While distributional semantics is easily applied to single words, sparsity implies that attempts to directly extract distributional representations for larger expressions are doomed to fail.", "labels": [], "entities": []}, {"text": "Only in the past few years has it been attempted to extend these representations to semantic composition.", "labels": [], "entities": []}, {"text": "Most approaches here use the idea of vector-matrix composition to learn larger representations from single-word encodings (.", "labels": [], "entities": []}, {"text": "While these models have proved very promising for compositional semantics, they make minimal use of linguistic information beyond the word level.", "labels": [], "entities": [{"text": "compositional semantics", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.8960688710212708}]}, {"text": "In this paper we bridge the gap between recent advances in machine learning and more traditional approaches within computational linguistics.", "labels": [], "entities": []}, {"text": "We achieve this goal by employing the CCG formalism to consider compositional structures at any point in a parse tree.", "labels": [], "entities": []}, {"text": "CCG is attractive both for its transparent interface between syntax and semantics, and a small but powerful set of combinatory operators with which we can parametrise our nonlinear transformations of compositional meaning.", "labels": [], "entities": [{"text": "CCG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8573978543281555}]}, {"text": "We present a novel class of recursive models, the Combinatory Categorial Autoencoders (CCAE), which marry a semantic process provided by a recursive autoencoder with the syntactic representations of the CCG formalism.", "labels": [], "entities": []}, {"text": "Through this model we seek to answer two ques-tions: Can recursive vector space models be reconciled with a more formal notion of compositionality; and is there a role for syntax in guiding semantics in these types of models?", "labels": [], "entities": []}, {"text": "CCAEs make use of CCG combinators and types by conditioning each composition function on its equivalent step in a CCG proof.", "labels": [], "entities": []}, {"text": "In terms of learning complexity and space requirements, our models strike a balance between simpler greedy approaches () and the larger recursive vector-matrix models).", "labels": [], "entities": []}, {"text": "We show that this combination of state of the art machine learning and an advanced linguistic formalism translates into concise models with competitive performance on a variety of tasks.", "labels": [], "entities": []}, {"text": "In both sentiment and compound similarity experiments we show that our CCAE models match or better comparable recursive autoencoder models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe a number of standard evaluations to determine the comparative performance of our model.", "labels": [], "entities": []}, {"text": "The first task of sentiment analysis allows us to compare our CCG-conditioned RAE with similar, existing models.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.9599999189376831}, {"text": "CCG-conditioned RAE", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.5489726513624191}]}, {"text": "Ina second experiment, we apply our model to a compound similarity evaluation, which allows us to evaluate our models against a larger class of vector-based models).", "labels": [], "entities": []}, {"text": "We conclude with some qualitative analysis to get a better idea of whether the combination of CCG and RAE can learn semantically expressive embeddings.", "labels": [], "entities": []}, {"text": "In our experiments we use the hyperbolic tangent as nonlinearity g.", "labels": [], "entities": []}, {"text": "Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by based on the model of.", "labels": [], "entities": []}, {"text": "We use the C&C parser  to generate CCG parse trees for the data used in our experiments.", "labels": [], "entities": []}, {"text": "For models CCAE-C and CCAE-D we use the 25 most frequent CCG categories (as extracted from the British National Corpus) with an additional general weight matrix in order to catchall remaining types.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 95, "end_pos": 118, "type": "DATASET", "confidence": 0.9466975132624308}]}, {"text": "In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by.", "labels": [], "entities": []}, {"text": "The results of this evaluation are in.", "labels": [], "entities": []}, {"text": "While we achieve the best performance on the MPQA corpus, the results on the SP corpus are less convincing.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.9477241635322571}, {"text": "SP corpus", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.7413448095321655}]}, {"text": "Perhaps surprisingly, the simplest model CCAE-A outperforms the other models on this dataset.", "labels": [], "entities": []}, {"text": "When considering the two datasets, sparsity seems a likely explanation for this difference in results: In the MPQA experiment most instances are very short with an average length of 3 words, while the average sentence length in the SP corpus is 21 words.", "labels": [], "entities": [{"text": "SP corpus", "start_pos": 232, "end_pos": 241, "type": "DATASET", "confidence": 0.6810465157032013}]}, {"text": "The MPQA task is further simplified through the use or an additional sentiment lexicon.", "labels": [], "entities": []}, {"text": "Considering dictionary size, the SP corpus has a dictionary of 22k words, more than three times the size of the MPQA dictionary.", "labels": [], "entities": [{"text": "MPQA dictionary", "start_pos": 112, "end_pos": 127, "type": "DATASET", "confidence": 0.9031244516372681}]}, {"text": "This issue of sparsity is exacerbated in the more complex CCAE models, where the training points are spread across different CCG types and rules.", "labels": [], "entities": []}, {"text": "While the initialization of the word vectors with previously learned embeddings (as was previously shown by) helps the models, all other model variables such as composition weights and biases are still initialised randomly and thus highly dependent on the amount of training data available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Accuracy of sentiment classification on  the sentiment polarity (SP) and MPQA datasets.  For NB we only display the best result among a  larger group of models analysed in that paper.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9569979906082153}, {"text": "sentiment classification", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.9345923960208893}, {"text": "MPQA datasets", "start_pos": 83, "end_pos": 96, "type": "DATASET", "confidence": 0.95709028840065}]}, {"text": " Table 5: Correlation coefficients of model predic- tions for the compound similarity task. Numbers  show Spearman's rank correlation coefficient (\u03c1).", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient (\u03c1)", "start_pos": 106, "end_pos": 149, "type": "METRIC", "confidence": 0.7423927038908005}]}]}