{"title": [{"text": "Scaling Semi-supervised Naive Bayes with Feature Marginals", "labels": [], "entities": [{"text": "Scaling Semi-supervised Naive Bayes", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7921428680419922}, {"text": "Marginals", "start_pos": 49, "end_pos": 58, "type": "TASK", "confidence": 0.28678059577941895}]}], "abstractContent": [{"text": "Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data.", "labels": [], "entities": [{"text": "Semi-supervised learning (SSL)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7268482625484467}]}, {"text": "SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available.", "labels": [], "entities": [{"text": "SSL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9034112691879272}, {"text": "text classification", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8494676947593689}]}, {"text": "However, existing SSL techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today.", "labels": [], "entities": []}, {"text": "In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets.", "labels": [], "entities": [{"text": "text classification", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7212460190057755}]}, {"text": "We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus.", "labels": [], "entities": []}, {"text": "In experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previous work.", "labels": [], "entities": [{"text": "text topic classification", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.7661572098731995}, {"text": "sentiment analysis", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9603665769100189}]}], "introductionContent": [{"text": "Semi-supervised Learning (SSL) is a Machine Learning (ML) approach that utilizes large amounts of unlabeled data, combined with a smaller amount of labeled data, to learn a target function).", "labels": [], "entities": [{"text": "Semi-supervised Learning (SSL)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.745892995595932}]}, {"text": "SSL is motivated by a simple reality: the amount of available machine-readable data is exploding, while human capacity for hand-labeling data for any given ML task remains relatively constant.", "labels": [], "entities": [{"text": "SSL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9687660336494446}]}, {"text": "Experiments in text classification and other domains have demonstrated that by leveraging unlabeled data, SSL techniques improve machine learning performance when human input is limited (e.g., ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7774693965911865}]}, {"text": "However, current SSL techniques have scalability limitations.", "labels": [], "entities": [{"text": "SSL", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9586580395698547}]}, {"text": "Typically, for each target concept to be learned, a semi-supervised classifier is trained using iterative techniques that execute multiple passes over the unlabeled data (e.g., Expectation-Maximization () or Label Propagation ().", "labels": [], "entities": []}, {"text": "This is problematic for text classification overlarge unlabeled corpora like the Web: new target concepts (new tasks and new topics of interest) arise frequently, and performing even a single pass over a large corpus for each new target concept is intractable.", "labels": [], "entities": [{"text": "text classification", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7861792743206024}]}, {"text": "In this paper, we present anew SSL text classification approach that scales to large corpora.", "labels": [], "entities": [{"text": "SSL text classification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.9062396883964539}]}, {"text": "Instead of utilizing unlabeled examples directly for each given target concept, our approach is to precompute a small set of statistics over the unlabeled data in advance.", "labels": [], "entities": []}, {"text": "Then, fora given target class and labeled data set, we utilize the statistics to improve a classifier.", "labels": [], "entities": []}, {"text": "Specifically, we introduce a method that extends Multinomial Naive Bayes (MNB) to leverage marginal probability statistics P (w) of each word w, computed over the unlabeled data.", "labels": [], "entities": []}, {"text": "The marginal statistics are used as a constraint to improve the class-conditional probability estimates P (w|+) and P (w|\u2212) for the positive and negative classes, which are often noisy when estimated over sparse labeled data sets.", "labels": [], "entities": []}, {"text": "We refer to the technique as MNB with Frequency Marginals (MNB-FM).", "labels": [], "entities": []}, {"text": "In experiments with large unlabeled data sets and sparse labeled data, we find that MNB-FM is both faster and more accurate on average than standard SSL methods from previous work, including Label Propagation, MNB with Expectation-Maximization,, and the recent Semisupervised Frequency Estimate (SFE) algorithm).", "labels": [], "entities": []}, {"text": "We also analyze how MNB-343 FM improves accuracy, and find that surprisingly MNB-FM is especially useful for improving classconditional probability estimates for words that never occur in the training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9991809725761414}]}, {"text": "The paper proceeds as follows.", "labels": [], "entities": []}, {"text": "We formally define the task in Section 2.", "labels": [], "entities": []}, {"text": "Our algorithm is defined in Section 3.", "labels": [], "entities": []}, {"text": "We present experimental results in Section 4, and analysis in Section 5.", "labels": [], "entities": []}, {"text": "We discuss related work in Section 6 and conclude in Section 7 with a discussion of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe our experiments quantifying the accuracy and scalability of our proposed technique.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9992109537124634}]}, {"text": "Across multiple domains, we find that MNB-FM outperforms a variety of approaches from previous work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: F1, training size in parentheses", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9893943667411804}, {"text": "training size", "start_pos": 14, "end_pos": 27, "type": "METRIC", "confidence": 0.8521575033664703}]}, {"text": " Table 6: RCV1: F1, |D L |= 100", "labels": [], "entities": [{"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.9945077300071716}]}, {"text": " Table 7: Runtimes of SSL methods (sec.)", "labels": [], "entities": []}, {"text": " Table 8: Analysis of Feature Marginal Improvement of MNB-FM over MNB (|D L | = 10). \"Known\"  indicates words occurring in both positive and negative training examples, \"Half Known\" indicates words  occurring in only positive or negative training examples, while \"Unknown\" indicates words that never  occur in labelled examples. Data is for the RCV1 MCAT category. MNB-FM improves estimates by a  substantial amount for unknown words and also the most common known and half-known words.", "labels": [], "entities": [{"text": "MNB", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.843913733959198}, {"text": "RCV1 MCAT category", "start_pos": 345, "end_pos": 363, "type": "DATASET", "confidence": 0.7735979557037354}]}, {"text": " Table 9: Analysis of Feature Marginal Improvement of MNB-FM over MNB (|D L | = 100). Data is  for the RCV1 MCAT category (see", "labels": [], "entities": [{"text": "Feature Marginal Improvement", "start_pos": 22, "end_pos": 50, "type": "METRIC", "confidence": 0.7117020885149637}, {"text": "D L", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.8691100776195526}, {"text": "RCV1 MCAT category", "start_pos": 103, "end_pos": 121, "type": "DATASET", "confidence": 0.7388468782107035}]}, {"text": " Table 10: R-Precision, training size in parentheses", "labels": [], "entities": [{"text": "R-Precision", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.962345540523529}, {"text": "training size", "start_pos": 24, "end_pos": 37, "type": "METRIC", "confidence": 0.9240599274635315}]}, {"text": " Table 11: RCV1: R-Precision, D L = 10", "labels": [], "entities": [{"text": "R-Precision", "start_pos": 17, "end_pos": 28, "type": "METRIC", "confidence": 0.9189233779907227}, {"text": "D L", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.8798511028289795}]}, {"text": " Table 12: RCV1: R-Precision, D L = 100  349", "labels": [], "entities": [{"text": "R-Precision", "start_pos": 17, "end_pos": 28, "type": "METRIC", "confidence": 0.8895657062530518}, {"text": "D L = 100  349", "start_pos": 30, "end_pos": 44, "type": "METRIC", "confidence": 0.7630751132965088}]}]}