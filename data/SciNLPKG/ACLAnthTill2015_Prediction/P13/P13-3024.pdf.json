{"title": [{"text": "A corpus-based evaluation method for Distributional Semantic Models", "labels": [], "entities": [{"text": "Distributional Semantic Models", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.8106222152709961}]}], "abstractContent": [{"text": "Evaluation methods for Distributional Semantic Models typically rely on behav-iorally derived gold standards.", "labels": [], "entities": []}, {"text": "These methods are difficult to deploy in languages with scarce linguistic/behavioral resources.", "labels": [], "entities": []}, {"text": "We introduce a corpus-based measure that evaluates the stability of the lexical semantic similarity space using a pseudo-synonym same-different detection task and no external resources.", "labels": [], "entities": []}, {"text": "We show that it enables to predict two behavior-based measures across a range of parameters in a Latent Semantic Analysis model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional Semantic Models (DSM) can be traced back to the hypothesis proposed by whereby the meaning of a word can be inferred from its context.", "labels": [], "entities": [{"text": "Distributional Semantic Models (DSM)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7465740541617075}]}, {"text": "Several implementations of Harris's hypothesis have been proposed in the last two decades (see fora review), but comparatively little has been done to develop reliable evaluation tools for these implementations.", "labels": [], "entities": []}, {"text": "Models evaluation is however an issue of crucial importance for practical applications, i.g., when trying to optimally set the model's parameters fora given task, and for theoretical reasons, i.g., when using such models to approximate semantic knowledge.", "labels": [], "entities": [{"text": "Models evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8012087047100067}]}, {"text": "Some evaluation techniques involve assigning probabilities to different models given the observed corpus and applying maximum likelihood estimation (.", "labels": [], "entities": []}, {"text": "However, computational complexity may prevent the application of such techniques, besides these probabilities may not be the best predictor for the model performance on a specific task.", "labels": [], "entities": []}, {"text": "Other commonly used methods evaluate DSMs by comparing their semantic representation to a behaviorally derived gold standard.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.9748213887214661}]}, {"text": "Some standards are derived from the TOEFL synonym test, or the Nelson word associations norms ().", "labels": [], "entities": [{"text": "TOEFL synonym test", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.4524281322956085}]}, {"text": "Others use results from semantic priming experiments ( or lexical substitutions errors (.", "labels": [], "entities": []}, {"text": "setup a more refined gold standard for English specifying different kinds of semantic relationship based on dictionary resources (like WordNet and ConceptNet).", "labels": [], "entities": []}, {"text": "These behavior-based evaluation methods are all resource intensive, requiring either linguistic expertise or human-generated data.", "labels": [], "entities": []}, {"text": "Such methods might not always be available, especially in languages with fewer resources than English.", "labels": [], "entities": []}, {"text": "In this situation, researchers usually select a small set of high-frequency target words and examine their nearest neighbors (the most similar to the target) using their own intuition.", "labels": [], "entities": []}, {"text": "This is used in particular to set the model parameters.", "labels": [], "entities": []}, {"text": "However, this rather informal method represents a \"cherry picking\" risk), besides it is only possible for languages that the researcher speaks.", "labels": [], "entities": [{"text": "cherry picking", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.6819121688604355}]}, {"text": "Here we introduce a method that aims at providing a rapid and quantitative evaluation for DSMs using an internal gold standard and requiring no external resources.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 90, "end_pos": 94, "type": "TASK", "confidence": 0.9552050828933716}]}, {"text": "It is based on a simple same-different task which detects pseudosynonyms randomly introduced in the corpus.", "labels": [], "entities": []}, {"text": "We claim that this measure evaluates the intrinsic ability of the model to capture lexical semantic similarity.", "labels": [], "entities": []}, {"text": "We validate it against two behaviorbased evaluations (Free association norms and the TOEFL synonym test) on semantic representations extracted from a Wikipedia corpus using one of the most commonly used distributional semantic models : the Latent Semantic Analysis (LSA,).", "labels": [], "entities": [{"text": "TOEFL synonym test", "start_pos": 85, "end_pos": 103, "type": "METRIC", "confidence": 0.8026748299598694}]}, {"text": "In this model, we construct a word-document matrix.", "labels": [], "entities": []}, {"text": "Each word is represented by a row, and each document is represented by a column.", "labels": [], "entities": []}, {"text": "Each matrix cell indicates the occurrence frequency of a given word in a given context.", "labels": [], "entities": []}, {"text": "Singular value decomposition (a kind of matrix factorization) is used to extract a reduced representation by truncating the matrix to a certain size (which we call the semantic dimension of the model).", "labels": [], "entities": [{"text": "Singular value decomposition", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6408248841762543}]}, {"text": "The cosine of the angle between vectors of the resulting space is used to measure the semantic similarity between words.", "labels": [], "entities": []}, {"text": "Two words end up with similar vectors if they co-occur multiple times in similar contexts.", "labels": [], "entities": []}], "datasetContent": [{"text": "We constructed three successively larger corpora of 1, 2 and 4 million words by randomly selecting articles from the original \"Wikicorpus\" made freely available on the internet by.", "labels": [], "entities": []}, {"text": "Wikicorpus is itself based on articles from the collaborative encyclopedia Wikipedia.", "labels": [], "entities": [{"text": "Wikicorpus", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8948136568069458}]}, {"text": "We selected the upper bound of 4 M words to be comparable with the typical corpus size used in theoretical studies on LSA (see for instance and).", "labels": [], "entities": []}, {"text": "For each corpus, we kept only words that occurred at least 10 times and we excluded a stop list of high frequency words with no conceptual content such as: the, of, to, and ...", "labels": [], "entities": []}, {"text": "This left us with a vocabulary of 8 643, 14 147 and 23 130 words respectively.", "labels": [], "entities": []}, {"text": "For the simulations, we used the free software Gensim) that provides an online Python implementation of LSA.", "labels": [], "entities": []}, {"text": "We first reproduced the results of, from which we derived the behavior-based measure.", "labels": [], "entities": []}, {"text": "Then, we computed our corpus-based measure with the same models.", "labels": [], "entities": []}], "tableCaptions": []}