{"title": [{"text": "Unsupervised joke generation from big data", "labels": [], "entities": [{"text": "Unsupervised joke generation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5633613467216492}]}], "abstractContent": [{"text": "Humor generation is a very hard problem.", "labels": [], "entities": [{"text": "Humor generation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9828176498413086}]}, {"text": "It is difficult to say exactly what makes a joke funny, and solving this problem al-gorithmically is assumed to require deep semantic understanding, as well as cultural and other contextual cues.", "labels": [], "entities": []}, {"text": "We depart from previous work that tries to model this knowledge using ad-hoc manually created databases and labeled training examples.", "labels": [], "entities": []}, {"text": "Instead we present a model that uses large amounts of unannotated data to generate I like my X like I like my Y, Z jokes, where X, Y, and Z are variables to be filled in.", "labels": [], "entities": []}, {"text": "This is, to the best of our knowledge, the first fully unsupervised humor generation system.", "labels": [], "entities": [{"text": "humor generation", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.6868344843387604}]}, {"text": "Our model significantly outper-forms a competitive baseline and generates funny jokes 16% of the time, compared to 33% for human-generated jokes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Generating jokes is typically considered to be a very hard natural language problem, as it implies a deep semantic and often cultural understanding of text.", "labels": [], "entities": [{"text": "Generating jokes", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8924131095409393}]}, {"text": "We deal with generating a particular type of joke -I like my X like I like my Y, Z -where X and Y are nouns and Z is typically an attribute that describes X and Y.", "labels": [], "entities": []}, {"text": "An example of such a joke is I like my men like I like my tea, hot and Britishthese jokes are very popular online.", "labels": [], "entities": [{"text": "Britishthese", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.8452559113502502}]}, {"text": "While this particular type of joke is not interesting from a purely generational point of view (the syntactic structure is fixed), the content selection problem is very challenging.", "labels": [], "entities": [{"text": "content selection", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.7652314603328705}]}, {"text": "Indeed, most of the X, Y, and Z triples, when used in the context of this joke, will not be considered funny.", "labels": [], "entities": []}, {"text": "Thus, the main challenge in this work is to \"fill in\" the slots in the joke template in away that the whole phrase is considered funny.", "labels": [], "entities": []}, {"text": "Unlike the previous work in humor generation, we do not rely on labeled training data or handcoded rules, but instead on large quantities of unannotated data.", "labels": [], "entities": [{"text": "humor generation", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.8635375499725342}]}, {"text": "We present a machine learning model that expresses our assumptions about what makes these types of jokes funny and show that by using this fairly simple model and large quantities of data, we are able to generate jokes that are considered funny by human raters in 16% of cases.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is, to the best of our knowledge, the first fully unsupervised joke generation system.", "labels": [], "entities": [{"text": "joke generation", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.7335779666900635}]}, {"text": "We rely only on large quantities of unlabeled data, suggesting that generating jokes does not always require deep semantic understanding, as usually thought.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model in two stages.", "labels": [], "entities": []}, {"text": "Firstly, using automatic evaluation with a set of jokes collected from Twitter, and secondly, by comparing our approach to human-generated jokes.", "labels": [], "entities": []}, {"text": "In the automatic evaluation we measure the effect of the different factors in the model, as laid out in Section 3.1.", "labels": [], "entities": []}, {"text": "We use two metrics for this evaluation.", "labels": [], "entities": []}, {"text": "The first is similar to log-likelihood, i.e., the log of the probability that our model assigns to a triple.", "labels": [], "entities": []}, {"text": "However, because we do not compute it on all the data, just on the data that contains the Xs from our development set, it is not exactly equal to the log-likelihood.", "labels": [], "entities": []}, {"text": "It is a local approximation to log-likelihood, and we therefore dub it LOcal Log-likelihood, or LOL-likelihood for short.", "labels": [], "entities": [{"text": "LOcal", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9136359691619873}, {"text": "LOL-likelihood", "start_pos": 96, "end_pos": 110, "type": "METRIC", "confidence": 0.8507674336433411}]}, {"text": "Our second metric computes the rank of the humangenerated jokes in the distribution of all possible jokes sorted decreasingly by their LOL-likelihood.", "labels": [], "entities": [{"text": "LOL-likelihood", "start_pos": 135, "end_pos": 149, "type": "METRIC", "confidence": 0.9847114086151123}]}, {"text": "This Rank OF Likelihood (ROFL) is computed relative to the number of all possible jokes, and like LOL-likelihood is averaged overall the jokes in our development data.", "labels": [], "entities": [{"text": "Rank OF Likelihood (ROFL)", "start_pos": 5, "end_pos": 30, "type": "METRIC", "confidence": 0.8427008986473083}, {"text": "LOL-likelihood", "start_pos": 98, "end_pos": 112, "type": "METRIC", "confidence": 0.961205005645752}]}, {"text": "One advantage of ROFL is that it is designed with the way we generate jokes in mind (cf. Section 5.3), and thus more directly measures the quality of generated jokes than LOL-likelihood.", "labels": [], "entities": [{"text": "ROFL", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.6038761138916016}]}, {"text": "For measuring LOL-likelihood and ROFL we use a set of 48 jokes randomly sampled from Twitter that fit the I like my X like I like my Y, Z pattern.", "labels": [], "entities": [{"text": "LOL-likelihood", "start_pos": 14, "end_pos": 28, "type": "METRIC", "confidence": 0.9517570734024048}, {"text": "ROFL", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9945375323295593}]}, {"text": "shows the effect of the different factors on the two metrics.", "labels": [], "entities": []}, {"text": "We use a model with only noun-attribute similarity (factors \u03c6(X, Z) and \u03c6(Y, Z)) as the baseline.", "labels": [], "entities": []}, {"text": "We see that the single biggest improvement comes from the attribute surprisal factor, i.e., from using rarer attributes.", "labels": [], "entities": []}, {"text": "The best combination of the factors, according to automatic metrics, is using all factors except for the noun similarity (Model 1), while using all the factors is the second best combination (Model 2).", "labels": [], "entities": []}, {"text": "The main evaluation of our model is in terms of human ratings, put simply: do humans find the jokes generated by our model funny?", "labels": [], "entities": []}, {"text": "We compare four models: the two best models from Section 5.: Effect of different factors.", "labels": [], "entities": []}, {"text": "(one that uses all the factors (Model 2), and one that uses all factors except for the noun dissimilarity (Model 1)), a baseline model that uses only the noun-attribute similarity, and jokes generated by humans, collected from Twitter.", "labels": [], "entities": []}, {"text": "We sample a further 32 jokes from Twitter, making sure that there was no overlap with the development set.", "labels": [], "entities": []}, {"text": "To generate a joke fora particular x we keep the top n most probable jokes according to the model, renormalize their probabilities so they sum to one, and sample from this reduced distribution.", "labels": [], "entities": []}, {"text": "This allows our model to focus on the jokes that it considers \"funny\".", "labels": [], "entities": []}, {"text": "In our experiments, we use n = 30, which ensures that we can still generate a variety of jokes for any given x.", "labels": [], "entities": []}, {"text": "In our experiments we showed five native English speakers the jokes from all the systems in a random, per rater, order.", "labels": [], "entities": []}, {"text": "The raters were asked to score each joke on a 3-point Likert scale: 1 (funny), 2 (somewhat funny), and 3 (not funny).", "labels": [], "entities": []}, {"text": "Naturally, the raters did not know which approach each joke was coming from.", "labels": [], "entities": []}, {"text": "Our model was used to sample Y and Z variables, given the same Xs used in the jokes collected from Twitter.", "labels": [], "entities": []}, {"text": "The second column shows the inter-rater agreement, and we can see that it is generally good, but that it is lower on the set of human jokes.", "labels": [], "entities": []}, {"text": "We inspected the human-generated jokes with high disagreement and found that the disagreement maybe partly explained by raters missing cultural references in the jokes (e.g., a sonic screwdriver is Doctor Who's tool of choice, which might be lost on those who are not familiar with the show).", "labels": [], "entities": []}, {"text": "We do not explicitly model cultural references, and are thus less likely to generate such jokes, leading to higher agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9668271541595459}]}, {"text": "The third column shows the mean joke score (lower is better), and we can see that human-generated jokes were rated the funniest, jokes from the baseline model the least funny, and that the model which uses all the Model  factors (Model 2) outperforms the model that was best according to the automatic evaluation (Model 1).", "labels": [], "entities": []}, {"text": "Finally, the last column shows the percentage of jokes the raters scored as funny (i.e., the number of funny scores divided by the total number of scores).", "labels": [], "entities": []}, {"text": "This is a metric that we are ultimately interested in -telling a joke that is somewhat funny is not useful, and we should only reward generating a joke that is found genuinely funny by humans.", "labels": [], "entities": []}, {"text": "The last column shows that humangenerated jokes are considered funnier than the machine-generated ones, but also that our model with all the factors does much better than the other two models.", "labels": [], "entities": []}, {"text": "Model 2 is significantly better than the baseline at p = 0.05 using a sign test, and human jokes are significantly better than all three models at p = 0.05 (because we were testing multiple hypotheses, we employed Holm-Bonferroni correction).", "labels": [], "entities": []}, {"text": "In the end, our best model generated jokes that were found funny by humans in 16% of cases, compared to 33% obtained by human-generated jokes.", "labels": [], "entities": []}, {"text": "Finally, we note that the funny jokes generated by our system are not simply repeats of the human jokes, but entirely new ones that we were notable to find anywhere online.", "labels": [], "entities": []}, {"text": "Examples of the funny jokes generated by Model 2 are shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Effect of different factors.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of different models on the  task of generating Y and Z given X.", "labels": [], "entities": []}]}