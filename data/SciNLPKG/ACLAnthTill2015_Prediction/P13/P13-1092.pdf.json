{"title": [], "abstractContent": [{"text": "We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7770820558071136}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9910736680030823}]}, {"text": "Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a proba-bilistic grammar using EM.", "labels": [], "entities": [{"text": "semantic parse", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7374813556671143}]}, {"text": "To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision.", "labels": [], "entities": [{"text": "GUSP", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.792605459690094}]}, {"text": "On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by supervised approaches.", "labels": [], "entities": [{"text": "ATIS dataset", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9336755871772766}, {"text": "GUSP", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9596342444419861}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9994932413101196}]}], "introductionContent": [{"text": "Semantic parsing maps text to a formal meaning representation such as logical forms or structured queries.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7953078746795654}]}, {"text": "Recently, there has been a burgeoning interest in developing machine-learning approaches for semantic parsing), but the predominant paradigm uses supervised learning, which requires example annotations that are costly to obtain.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 93, "end_pos": 109, "type": "TASK", "confidence": 0.8154478669166565}]}, {"text": "More recently, several groundedlearning approaches have been proposed to alleviate the annotation burden).", "labels": [], "entities": []}, {"text": "In particular, and proposed methods to learn from questionanswer pairs alone, which represents a significant advance.", "labels": [], "entities": []}, {"text": "However, although these methods exonerate annotators from mastering specialized logical forms, finding the answers for complex questions still requires non-trivial effort.", "labels": [], "entities": []}, {"text": "Poon & Domingos) proposed the USP system for unsupervised semantic parsing, which learns a parser by recursively clustering and composing synonymous expressions.", "labels": [], "entities": [{"text": "USP", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.7422263026237488}, {"text": "unsupervised semantic parsing", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.6908923586209615}]}, {"text": "While their approach completely obviates the need for direct supervision, their target logic forms are selfinduced clusters, which do not align with existing database or ontology.", "labels": [], "entities": []}, {"text": "As a result, USP cannot be used directly to answer complex questions against an existing database.", "labels": [], "entities": []}, {"text": "More importantly, it misses the opportunity to leverage database for indirect supervision.", "labels": [], "entities": []}, {"text": "In this paper, we present the GUSP system, which combines unsupervised semantic parsing with grounded learning from a database.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.7543193399906158}]}, {"text": "GUSP starts with the dependency tree of a sentence and produces a semantic parse by annotating the nodes and edges with latent semantic states derived from the database.", "labels": [], "entities": []}, {"text": "Given a set of natural-language questions and a database, GUSP learns a probabilistic semantic grammar using EM.", "labels": [], "entities": []}, {"text": "To compensate for the lack of direct supervision, GUSP constrains the search space using the database schema, and bootstraps learning using lexical scores computed from the names and values of database elements.", "labels": [], "entities": []}, {"text": "Unlike previous grounded-learning approaches, GUSP does not require ambiguous annotations or oracle answers, but rather focuses on leveraging database contents that are readily available.", "labels": [], "entities": []}, {"text": "Unlike USP, GUSP predetermines the target logical forms based on the database schema, which alleviates the difficulty in learning and ensures that the output semantic parses can be directly used in querying the database.", "labels": [], "entities": [{"text": "USP", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.8659798502922058}]}, {"text": "To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation by augmenting the state space to represent semantic relations beyond immediate dependency neighborhood.", "labels": [], "entities": []}, {"text": "This representation also factorizes over nodes and edges, enabling linear-time exact inference in GUSP.", "labels": [], "entities": []}, {"text": "We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing.", "labels": [], "entities": [{"text": "question answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.708266869187355}, {"text": "ATIS dataset", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.978336364030838}, {"text": "semantic parsing", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.7534407377243042}]}, {"text": "Compared to other standard datasets such as GEO and JOBS, ATIS features a database that is an order of magnitude larger in the numbers of relations and instances, as well as a more irregular language (ATIS questions were derived from spoken dialogs).", "labels": [], "entities": [{"text": "GEO", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.9666147828102112}, {"text": "JOBS", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.6885125637054443}]}, {"text": "Despite these challenges, GUSP attains an accuracy of 84% in end-to-end question answering, effectively tying with the stateof-the-art supervised approaches (85% by, 83% by).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9996196031570435}, {"text": "question answering", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7300714254379272}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of semantic parsing accu- racy on the ATIS test dataset. Both ZC07 and  FUBL used annotated logical forms in training,  whereas GUSP-FULL and GUSP++ did not. The  numbers for GUSP-FULL and GUSP++ are end- to-end question answering accuracy, whereas the  numbers for ZC07 and FUBL are recall on exact  match in logical forms.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.713783010840416}, {"text": "ATIS test dataset", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.9810543258984884}, {"text": "ZC07", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.9591052532196045}, {"text": "FUBL", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.5866923928260803}, {"text": "question answering", "start_pos": 233, "end_pos": 251, "type": "TASK", "confidence": 0.6595915108919144}, {"text": "accuracy", "start_pos": 252, "end_pos": 260, "type": "METRIC", "confidence": 0.7847209572792053}, {"text": "ZC07", "start_pos": 287, "end_pos": 291, "type": "DATASET", "confidence": 0.9766386151313782}, {"text": "FUBL", "start_pos": 296, "end_pos": 300, "type": "DATASET", "confidence": 0.6372078061103821}, {"text": "recall", "start_pos": 305, "end_pos": 311, "type": "METRIC", "confidence": 0.9873495697975159}]}, {"text": " Table 2: Comparison of question answering accu- racy in ablation experiments.", "labels": [], "entities": [{"text": "question answering accu- racy", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.7850344359874726}]}]}