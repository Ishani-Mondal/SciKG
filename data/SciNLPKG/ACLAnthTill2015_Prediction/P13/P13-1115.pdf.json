{"title": [{"text": "A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation *", "labels": [], "entities": [{"text": "Selectional Preferences", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.9326450526714325}]}], "abstractContent": [{"text": "This paper presents an unsupervised random walk approach to alleviate data spar-sity for selectional preferences.", "labels": [], "entities": []}, {"text": "Based on the measure of preferences between predicates and arguments, the model aggregates all the transitions from a given predicate to its nearby predicates, and propagates their argument preferences as the given predi-cate's smoothed preferences.", "labels": [], "entities": []}, {"text": "Experimental results show that this approach out-performs several state-of-the-art methods on the pseudo-disambiguation task, and it better correlates with human plausibility judgements.", "labels": [], "entities": []}], "introductionContent": [{"text": "Selectional preferences (SP) or selectional restrictions capture the plausibility of predicates and their arguments fora given relation.", "labels": [], "entities": [{"text": "Selectional preferences (SP)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6166871428489685}]}, {"text": "describe that predicates and their arguments have strict boolean restrictions, either satisfied or violated.", "labels": [], "entities": []}, {"text": "Sentences are semantically anomalous and not consistent in reading if they violated the restrictions.", "labels": [], "entities": []}, {"text": "argues that \"rejecting utterances is just what humans do not.", "labels": [], "entities": []}, {"text": "They try to understand them.\"", "labels": [], "entities": []}, {"text": "He further states selectional restrictions as preferences between the predicates and arguments, where the violation can be less preferred, but not fatal.", "labels": [], "entities": []}, {"text": "For instance, given the predicate word eat, word food is likely to be its object, iPhone is likely to be implausible for it, and tiger is less preferred but not curious.", "labels": [], "entities": []}, {"text": "SP have been proven to help many natural language processing tasks that involve attachment de- * Partial of this work was done when the first author visiting at Language Technologies Institute of Carnegie Mellon University sponsored by the China Scholarship Council.", "labels": [], "entities": [{"text": "China Scholarship Council", "start_pos": 240, "end_pos": 265, "type": "DATASET", "confidence": 0.9391304055849711}]}, {"text": "cisions, such as semantic role labeling), word sense disambiguation, human plausibility judgements), syntactic disambiguation (), word compositionality (, textual entailment ( and pronoun resolution () etc.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.6497011582056681}, {"text": "word sense disambiguation", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.6886434356371561}, {"text": "syntactic disambiguation", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.7152845859527588}, {"text": "word compositionality", "start_pos": 130, "end_pos": 151, "type": "TASK", "confidence": 0.7712608873844147}, {"text": "pronoun resolution", "start_pos": 180, "end_pos": 198, "type": "TASK", "confidence": 0.7713983654975891}]}, {"text": "A direct approach to acquire SP is to extract triples (q, r, a) of predicates, relations, and arguments from a syntactically analyzed corpus, and then conduct maximum likelihood estimation (M-LE) on the data.", "labels": [], "entities": [{"text": "acquire SP", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.6147879064083099}]}, {"text": "However, this strategy is infeasible for many plausible triples due to data sparsity.", "labels": [], "entities": []}, {"text": "For example, given the relation <verb-dobjnoun> in a corpus, we may see plausible triples: eat -{food, cake, apple, banana, candy...} But we may not see plausible and implausible triples such as: eat -{watermelon, ziti, escarole, iPhone...} Then how to use a smooth model to alleviate data sparsity for SP?", "labels": [], "entities": []}, {"text": "Random walk models have been successfully applied to alleviate the data sparsity issue on collaborative filtering in recommender systems.", "labels": [], "entities": []}, {"text": "Many online businesses, such as Netflix, Amazon.com, and Facebook, have used recommender systems to provide personalized suggestions on the movies, books, or friends that the users may prefer and interested in.", "labels": [], "entities": []}, {"text": "In this paper, we present an extension of using the random walk model to alleviate data sparsity for SP.", "labels": [], "entities": [{"text": "SP", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.9611284732818604}]}, {"text": "The main intuition is to aggregate all the transitions from a given predicate to its nearby predicates, and propagate their preferences on arguments as the given predicate's smoothed argu-ment preferences.", "labels": [], "entities": []}, {"text": "Our work and contributions are summarized as follows: \u2022 We present a framework of random walk approach to SP.", "labels": [], "entities": [{"text": "SP", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.9774494767189026}]}, {"text": "It contains four components with flexible configurations.", "labels": [], "entities": []}, {"text": "Each component is corresponding to a specific functional operation on the bipartite and monopartite graphs which representing the SP data; \u2022 We propose an adjusted preference ranking method to measure SP based on the popularity and association of predicate-argument pairs.", "labels": [], "entities": []}, {"text": "It better correlates with human plausibility judgements.", "labels": [], "entities": []}, {"text": "It also helps to discover similar predicates more precisely; \u2022 We introduce a probability function for random walk based on the predicate distances.", "labels": [], "entities": []}, {"text": "It controls the influence of nearby and distant predicates to achieve more accurate results; \u2022 We find out that propagate the measured preferences of predicate-argument pairs is more proper and natural for SP smooth.", "labels": [], "entities": []}, {"text": "It helps to improve the final performance significantly.", "labels": [], "entities": []}, {"text": "We conduct experiments using two sections of the LDC English gigaword corpora as the generalization data.", "labels": [], "entities": [{"text": "LDC English gigaword corpora", "start_pos": 49, "end_pos": 77, "type": "DATASET", "confidence": 0.9051663726568222}]}, {"text": "For the pseudo-disambiguation task, we evaluate it on the Penn TreeBank-3 data.", "labels": [], "entities": [{"text": "Penn TreeBank-3 data", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.9957829912503561}]}, {"text": "Results show that our model outperforms several previous methods.", "labels": [], "entities": []}, {"text": "We further investigate the correlations of smoothed scores with human plausibility judgements.", "labels": [], "entities": []}, {"text": "Again our method achieves better correlations on two third party data.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: Section 2 introduces related work.", "labels": [], "entities": []}, {"text": "Section 3 briefly formulates the overall framework of our method.", "labels": [], "entities": []}, {"text": "Section 4 describes the detailed model configurations, with discussions on their roles and implications.", "labels": [], "entities": []}, {"text": "Section 5 provides experiments on both the pseudo-disambiguation task and human plausibility judgements.", "labels": [], "entities": []}, {"text": "Finally, Section 6 summarizes the conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Test Data: For pseudo-disambiguation, we employ Penn TreeBank-3 (PTB) as the test data (Marcus et al., 1999) 2 . We collect the 36, 400 manually annotated verb-dobj-noun dependencies (with 23, 553 distinct ones) from PTB.", "labels": [], "entities": [{"text": "Penn TreeBank-3 (PTB)", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.9634390115737915}, {"text": "PTB", "start_pos": 217, "end_pos": 220, "type": "DATASET", "confidence": 0.9541950225830078}]}, {"text": "We keep dependencies whose predicates and arguments are seen in the generalization data.", "labels": [], "entities": []}, {"text": "We randomly select 20% of these dependencies as the test set.", "labels": [], "entities": []}, {"text": "We split the test set equally into two parts: one as the development set and the other as the final test set.", "labels": [], "entities": []}, {"text": "Human Plausibility Judgements Data: We employ two human plausibility judgements data for the correlation evaluation.", "labels": [], "entities": [{"text": "Human Plausibility Judgements", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6340424021085104}]}, {"text": "In each they collect a set of predicate-argument pairs, and annotate with two kinds of human ratings: one for an argument takes the role as the patient of a predicate, and the other for the argument as the agent.", "labels": [], "entities": []}, {"text": "The rating values are between 1 and 7: e.g. they assign hunter-subj-shoot with a rating 6.9 but 2.8 for shoot-dobj-hunter.", "labels": [], "entities": []}, {"text": "\u2022 PBP: develop a set of human plausibility ratings on the basis of the Penn TreeBank and FrameNet respectively.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9948501586914062}]}, {"text": "We refer PBP as their 212 patient ratings from the Penn TreeBank.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9951130151748657}]}, {"text": "\u2022 MRP: This data are originally contributed by.", "labels": [], "entities": [{"text": "MRP", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.7386295199394226}]}, {"text": "We use all their 723 patient-nn ratings.", "labels": [], "entities": []}, {"text": "Without explicit explanation, we remove all the selected PTB tests and human plausibility pairs from AFP and NYT to treat them unseen.", "labels": [], "entities": [{"text": "AFP", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.8039392828941345}, {"text": "NYT", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.5849380493164062}]}], "tableCaptions": [{"text": " Table 1: Comparing different ranking functions.", "labels": [], "entities": []}, {"text": " Table 2: Pseudo-disambiguation results of different smooth models. Macro and micro Accuracy.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9702997207641602}]}, {"text": " Table 3: Correlation results on the human plausibility judgements data.", "labels": [], "entities": []}]}