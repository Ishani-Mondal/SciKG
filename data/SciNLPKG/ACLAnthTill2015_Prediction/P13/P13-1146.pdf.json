{"title": [{"text": "Fine-grained Semantic Typing of Emerging Entities", "labels": [], "entities": []}], "abstractContent": [{"text": "Methods for information extraction (IE) and knowledge base (KB) construction have been intensively studied.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.8540602147579193}, {"text": "knowledge base (KB) construction", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.6531385680039724}]}, {"text": "However, a largely under-explored case is tapping into highly dynamic sources like news streams and social media, where new entities are continuously emerging.", "labels": [], "entities": []}, {"text": "In this paper, we present a method for discovering and semantically typing newly emerging out-of-KB entities, thus improving the freshness and recall of ontology-based IE and improving the precision and semantic rigor of open IE.", "labels": [], "entities": [{"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.996189534664154}, {"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9983906745910645}]}, {"text": "Our method is based on a prob-abilistic model that feeds weights into integer linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints.", "labels": [], "entities": []}, {"text": "Our experimental evaluation, based on crowd-sourced user studies, show our method performing significantly better than prior work.", "labels": [], "entities": []}], "introductionContent": [{"text": "A large number of knowledge base (KB) construction projects have recently emerged.", "labels": [], "entities": []}, {"text": "Prominent examples include Freebase which powers the Google Knowledge Graph, ConceptNet,, and others.", "labels": [], "entities": []}, {"text": "These KBs contain many millions of entities, organized in hundreds to hundred thousands of semantic classes, and hundred millions of relational facts between entities.", "labels": [], "entities": []}, {"text": "However, despite these impressive advances, there are still major limitations regarding coverage and freshness.", "labels": [], "entities": [{"text": "coverage", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9344130158424377}]}, {"text": "Most KB projects focus on entities that appear in Wikipedia (or other reference collections such as IMDB), and very few have tried to gather entities \"in the long tail\" beyond prominent sources.", "labels": [], "entities": []}, {"text": "Virtually all projects miss out on newly emerging entities that appear only in the latest news or social media.", "labels": [], "entities": []}, {"text": "For example, the Greenlandic singer Nive Nielsen has gained attention only recently and is not included in any KB (a former Wikipedia article was removed because it \"does not indicate the importance or significance of the subject\"), and the resignation of BBC director Entwistle is a recently new entity (of type event).", "labels": [], "entities": [{"text": "BBC director Entwistle", "start_pos": 256, "end_pos": 278, "type": "DATASET", "confidence": 0.8203089237213135}]}, {"text": "Our goal in this paper is to discover emerging entities of this kind on the fly as they become noteworthy in news and social-media streams.", "labels": [], "entities": []}, {"text": "A similar theme is pursued in research on open information extraction (open IE) (, which yields higher recall compared to ontologystyle KB construction with canonicalized and semantically typed entities organized in prespecified classes.", "labels": [], "entities": [{"text": "open information extraction (open IE)", "start_pos": 42, "end_pos": 79, "type": "TASK", "confidence": 0.6581519714423588}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9992297887802124}]}, {"text": "However, state-of-the-art open IE methods extract all noun phrases that are likely to denote entities.", "labels": [], "entities": []}, {"text": "These phrases are not canonicalized, so the same entity may appear under many different names, e.g., \"Mr. Entwistle\", \"George Entwistle\", \"the BBC director\", \"BBC head Entwistle\", and soon.", "labels": [], "entities": [{"text": "BBC director", "start_pos": 143, "end_pos": 155, "type": "DATASET", "confidence": 0.9014634490013123}, {"text": "BBC head Entwistle", "start_pos": 159, "end_pos": 177, "type": "DATASET", "confidence": 0.8544051051139832}]}, {"text": "This is a problem because names and titles are ambiguous, and this hampers precise search and concise results.", "labels": [], "entities": []}, {"text": "Our aim is for all recognized and newly discovered entities to be semantically interpretable by having fine-grained types that connect them to KB classes.", "labels": [], "entities": []}, {"text": "The expectation is that this will boost the disambiguation of known entity names and the grouping of new entities, and will also strengthen the extraction of relational facts about entities.", "labels": [], "entities": [{"text": "disambiguation of known entity names", "start_pos": 44, "end_pos": 80, "type": "TASK", "confidence": 0.8644136786460876}, {"text": "grouping of new entities", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.8361209779977798}]}, {"text": "For informative knowledge, new entities must be typed in a fine-grained manner (e.g., guitar player, blues band, concert, as opposed to crude types like person, organization, event).", "labels": [], "entities": []}, {"text": "Strictly speaking, the new entities that we cap-ture are typed noun phrases.", "labels": [], "entities": []}, {"text": "We do not attempt any cross-document co-reference resolution, as this would hardly work with the long-tail nature and sparse observations of emerging entities.", "labels": [], "entities": [{"text": "cross-document co-reference resolution", "start_pos": 22, "end_pos": 60, "type": "TASK", "confidence": 0.6300614575544993}]}, {"text": "Therefore, our setting resembles the established task of fine-grained typing for noun phrases, with the difference being that we disregard common nouns and phrases for prominent in-KB entities and instead exclusively focus on the difficult case of phrases that likely denote new entities.", "labels": [], "entities": []}, {"text": "The baselines to which we compare our method are state-of-the-art methods for nounphrase typing.", "labels": [], "entities": [{"text": "nounphrase typing", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.9552220702171326}]}, {"text": "The solution presented in this paper, called PEARL, leverages a repository of relational patterns that are organized in a typesignature taxonomy.", "labels": [], "entities": []}, {"text": "More specifically, we harness the PATTY collection consisting of more than 300,000 typed paraphrases).", "labels": [], "entities": [{"text": "PATTY collection", "start_pos": 34, "end_pos": 50, "type": "DATASET", "confidence": 0.7544420659542084}]}, {"text": "An example of PATTY's expressive phrases is: musician * cover * song fora musician performing someone else's song.", "labels": [], "entities": [{"text": "PATTY's expressive phrases", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.5158909410238266}]}, {"text": "When extracting noun phrases, PEARL also collects the cooccurring PATTY phrases.", "labels": [], "entities": [{"text": "PEARL", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.7943419814109802}]}, {"text": "The type signatures of the relational phrases are cues for the type of the entity denoted by the noun phrase.", "labels": [], "entities": []}, {"text": "For example, an entity named Snoop Dogg that frequently cooccurs with the singer * distinctive voice in * song pattern is likely to be a singer.", "labels": [], "entities": []}, {"text": "Moreover, if one entity in a relational triple is in the KB and can be properly disambiguated (e.g., a singer), we can use a partially bound pattern to infer the type of the other entity (e.g., a song) with higher confidence.", "labels": [], "entities": []}, {"text": "In this line of reasoning, we also leverage the common situation that many input sentences contain one entity registered in the KB and one novel or unknown entity.", "labels": [], "entities": []}, {"text": "Known entities are recognized and mapped to the KB using a recent tool for named entity disambiguation ).", "labels": [], "entities": [{"text": "named entity disambiguation", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.6672972440719604}]}, {"text": "For cleaning out false hypotheses among the type candidates fora new entity, we devised probabilistic models and an integer linear program that considers incompatibilities and correlations among entity types.", "labels": [], "entities": []}, {"text": "In summary, our contribution in this paper is a model for discovering and ontologically typing out-of-KB entities, using a fine-grained type system and harnessing relational paraphrases with type signatures for probabilistic weight computation.", "labels": [], "entities": []}, {"text": "Crowdsourced quality assessments demonstrate the accuracy of our model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9994592070579529}]}], "datasetContent": [{"text": "To define a suitable corpus of test data, we obtained a stream of news documents by subscribing to Google News RSS feeds fora few topics over a six-month period.", "labels": [], "entities": [{"text": "Google News RSS feeds", "start_pos": 99, "end_pos": 120, "type": "DATASET", "confidence": 0.8975293636322021}]}, {"text": "This produced 318, 434 documents.", "labels": [], "entities": []}, {"text": "The topics we subscribed to are: Angela Merkel, Barack Obama, Business, Entertainment, Hillary Clinton, Joe Biden, Mitt Romney, Newt Gingrich, Rick Santorum, SciTech and Top News.", "labels": [], "entities": [{"text": "Top News", "start_pos": 170, "end_pos": 178, "type": "DATASET", "confidence": 0.9052815735340118}]}, {"text": "All our experiments were carried out on this data.", "labels": [], "entities": []}, {"text": "The type system used is that of YAGO2, which is derived from WordNet.", "labels": [], "entities": [{"text": "YAGO2", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.7770962119102478}, {"text": "WordNet", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9743899703025818}]}, {"text": "Human evaluations were carried out on Amazon Mechanical Turk (MTurk), which is a platform for crowd-sourcing tasks that require human input.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk)", "start_pos": 38, "end_pos": 68, "type": "DATASET", "confidence": 0.9246948262055715}]}, {"text": "Tasks on MTurk are small questionnaires consisting of a description and a set of questions.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 9, "end_pos": 14, "type": "TASK", "confidence": 0.7232203483581543}]}, {"text": "We compared PEARL against two state-of-the-art baselines: i).", "labels": [], "entities": []}, {"text": "NNPLB (No Noun Phrase Left Behind), is the method presented in (Lin 2012), based on the propagation of types for known entities through salient patterns occurring with both known and unknown entities.", "labels": [], "entities": [{"text": "NNPLB", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8136629462242126}, {"text": "No Noun Phrase Left Behind)", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.5566790302594503}]}, {"text": "We implemented the algorithm in (Lin 2012) in our framework, using the relational patterns of PATTY (Nakashole 2012) for comparability.", "labels": [], "entities": [{"text": "PATTY (Nakashole 2012)", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.7377534806728363}]}, {"text": "For assessment we sampled from the top-5 highest ranked types for each entity.", "labels": [], "entities": []}, {"text": "In our experiments, our implementation of NNPLB achieved precision values comparable to those reported in (Lin 2012).", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.999309778213501}]}, {"text": "HYENA (Hierarchical tYpe classification for Entity NAmes), the method of (Yosef 2012), based on a feature-rich classifier for fine-grained, hierarchical type tagging.", "labels": [], "entities": [{"text": "HYENA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6930633783340454}]}, {"text": "This is a state-of-the-art representative of similar methods such as (Rahman 2010; Ling 2012).", "labels": [], "entities": []}, {"text": "To evaluate the quality of types assigned to emerging entities, we presented turkers with sentences from the news tagged with outof-KB entities and the types inferred by the methods under test.", "labels": [], "entities": []}, {"text": "The turkers task was to assess the correctness of types assigned to an entity mention.", "labels": [], "entities": []}, {"text": "To make it easy to understand the task for the turkers, we combined the extracted entity and type into a sentence.", "labels": [], "entities": []}, {"text": "For example if PEARL inferred that Brussels Summit is an political event, we generate and present the sentence: Brussels Summit is an event.", "labels": [], "entities": [{"text": "PEARL inferred that Brussels Summit is an political event", "start_pos": 15, "end_pos": 72, "type": "TASK", "confidence": 0.5945727792051103}, {"text": "Brussels Summit", "start_pos": 112, "end_pos": 127, "type": "DATASET", "confidence": 0.9305896759033203}]}, {"text": "We allowed four possible assessment values: a) Very good output corresponds to a perfect result.", "labels": [], "entities": []}, {"text": "b) Good output exhibits minor errors.", "labels": [], "entities": []}, {"text": "For instance, the description G20 Summit is an organization is wrong, because the summit is an event, but G20 is indeed an organization.", "labels": [], "entities": [{"text": "G20 Summit", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.8791179955005646}]}, {"text": "The problem in this example is incorrect segmentation of a named entity.", "labels": [], "entities": [{"text": "segmentation of a named entity", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.7768789172172547}]}, {"text": "c) Wrong for incorrect types (e.g., Brussels Summit is a politician).", "labels": [], "entities": [{"text": "Brussels Summit", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9354077875614166}]}, {"text": "d) Not sure / do not know for other cases.", "labels": [], "entities": []}, {"text": "Per method, turkers evaluated 105 entity-type pair test samples.", "labels": [], "entities": []}, {"text": "We first sampled among out-of-KB entities that were mentioned frequently in the news corpus: in at least 20 different news articles.", "labels": [], "entities": []}, {"text": "Each test sample was given to 3 different turkers for assessment.", "labels": [], "entities": []}, {"text": "Since the turkers did not always agree if the type fora sample is good or not, we aggregate their answers.", "labels": [], "entities": []}, {"text": "We use voting to decide whether the type was assigned correctly to an entity.", "labels": [], "entities": []}, {"text": "We consider the following voting variants: i) majority \"very good\" or \"good\", a conservative notion of precision: precision lower . ii) at least one \"very good\" or \"good\", a liberal notion of precision: precision upper . shows precision for PEARL-hard, PEARL-soft, NNPLB, and HYENA, with a 0.9-confidence Wilson score interval.", "labels": [], "entities": [{"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9983028173446655}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9872608184814453}, {"text": "precision", "start_pos": 192, "end_pos": 201, "type": "METRIC", "confidence": 0.9972681999206543}, {"text": "precision", "start_pos": 203, "end_pos": 212, "type": "METRIC", "confidence": 0.9239639639854431}, {"text": "precision", "start_pos": 227, "end_pos": 236, "type": "METRIC", "confidence": 0.9992338418960571}, {"text": "HYENA", "start_pos": 276, "end_pos": 281, "type": "DATASET", "confidence": 0.7176874876022339}]}, {"text": "PEARL-hard outperformed PEARL-soft and also both baselines.", "labels": [], "entities": [{"text": "PEARL-hard", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9071718454360962}, {"text": "PEARL-soft", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.7398964762687683}]}, {"text": "HYENA's relatively poor performance can be attributed to the fact that its features are mainly syntactic such as bi-grams and part-of-speech tags.", "labels": [], "entities": [{"text": "HYENA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.827906608581543}]}, {"text": "Web data is challenging, it has a lot of variations in syntactic formulations.", "labels": [], "entities": []}, {"text": "This introduces a fair amount of ambiguity which can easily mislead syntactic features.", "labels": [], "entities": []}, {"text": "Leveraging semantic features as done by PEARL could improve HYENA's performance.", "labels": [], "entities": []}, {"text": "While the NNPLB method performs better than HYENA, in comparison to PEARL-hard, there is room for improvement.", "labels": [], "entities": [{"text": "HYENA", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.6136243939399719}]}, {"text": "Like HYENA, NNPLB assigns negatively correlated types to the same entity.", "labels": [], "entities": []}, {"text": "This limitation could be addressed by applying PEARL's ILPs and probabilistic weights to the candidate types suggested by NNPLB.", "labels": [], "entities": [{"text": "PEARL's ILPs", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.7688209017117819}, {"text": "NNPLB", "start_pos": 122, "end_pos": 127, "type": "DATASET", "confidence": 0.9548982977867126}]}, {"text": "To compute inter-judge agreement we calculated Fleiss' kappa and Cohen's kappa \u03ba, which are standard measures.", "labels": [], "entities": []}, {"text": "The usual assumption for Fleiss'\u03ba is that labels are categorical, so that each disagreement counts the same.", "labels": [], "entities": []}, {"text": "This is not the casein our settings, where different labels may indicate partial agreement (\"good\", \"very good\").", "labels": [], "entities": []}, {"text": "There-  fore the \u03ba values in are lower-bound estimates of agreement in our experiments; the \"true agreement\" seems higher.", "labels": [], "entities": [{"text": "agreement", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9484615921974182}]}, {"text": "Nevertheless, the observed Fleiss \u03ba values show that the task was fairly clear to the turkers; values > 0.2 are generally considered as acceptable.", "labels": [], "entities": [{"text": "Fleiss \u03ba", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9792745113372803}]}, {"text": "Cohen's \u03ba is also not directly applicable to our setting.", "labels": [], "entities": []}, {"text": "We approximated it by finding pairs of judges who assessed a significant number of the same entity-type pairs.", "labels": [], "entities": []}, {"text": "Precision lower Precision upper Freq.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9093202948570251}, {"text": "Freq", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.5895283818244934}]}, {"text": "mentions 0.77\u00b10.08 0.88\u00b10.06 All mentions 0.65\u00b10.09 0.77\u00b10.08  Weights).", "labels": [], "entities": []}, {"text": "From, it is clear that both the ILP and the weighting model contribute significantly to PEARL's ability to make precise type assignments.", "labels": [], "entities": []}, {"text": "Sample results from PEARL-hard are shown in.", "labels": [], "entities": [{"text": "PEARL-hard", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.4658893048763275}]}, {"text": "For a given entity mention e, an entitytyping system returns a ranked list of types {t 1 , t 2 , ..., tn }.", "labels": [], "entities": []}, {"text": "We evaluated ranking quality using the top-5 ranks for each method.", "labels": [], "entities": []}, {"text": "These assessments were aggregated into the normalized discounted cumulative gain (NDCG), a widely used measure for ranking quality.", "labels": [], "entities": [{"text": "normalized discounted cumulative gain (NDCG)", "start_pos": 43, "end_pos": 87, "type": "METRIC", "confidence": 0.7586223142487662}]}, {"text": "The NDCG values obtained are 0.53, 0.16, and 0.16, for PEARLhard, HYENA, and NNPLB, respectively.", "labels": [], "entities": [{"text": "NDCG", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.6754776835441589}, {"text": "PEARLhard", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.49031585454940796}, {"text": "HYENA", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.7669446468353271}, {"text": "NNPLB", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.8310589790344238}]}, {"text": "PEARL clearly outperforms the baselines on ranking quality, too.", "labels": [], "entities": [{"text": "PEARL", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9304796457290649}]}], "tableCaptions": [{"text": " Table 1: Comparison of PEARL to baselines.", "labels": [], "entities": []}]}