{"title": [{"text": "Improving machine translation by training against an automatic semantic frame based evaluation metric", "labels": [], "entities": [{"text": "Improving machine translation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8780632615089417}]}], "abstractContent": [{"text": "We present the first ever results showing that tuning a machine translation system against a semantic frame based objective function, MEANT, produces more robustly adequate translations than tuning against BLEU or TER as measured across commonly used metrics and human subjective evaluation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.6873080730438232}, {"text": "MEANT", "start_pos": 134, "end_pos": 139, "type": "METRIC", "confidence": 0.9887823462486267}, {"text": "BLEU", "start_pos": 206, "end_pos": 210, "type": "METRIC", "confidence": 0.9979230761528015}, {"text": "TER", "start_pos": 214, "end_pos": 217, "type": "METRIC", "confidence": 0.9364302754402161}]}, {"text": "Moreover, for informal web forum data, human evalua-tors preferred MEANT-tuned systems over BLEU-or TER-tuned systems by a significantly wider margin than that for formal newswire-even though automatic semantic parsing might be expected to fare worse on informal language.", "labels": [], "entities": [{"text": "MEANT-tuned", "start_pos": 67, "end_pos": 78, "type": "METRIC", "confidence": 0.9235805869102478}, {"text": "BLEU-or TER-tuned", "start_pos": 92, "end_pos": 109, "type": "METRIC", "confidence": 0.7873860001564026}, {"text": "semantic parsing", "start_pos": 202, "end_pos": 218, "type": "TASK", "confidence": 0.7635288834571838}]}, {"text": "We argue that by preserving the meaning of the translations as captured by semantic frames right in the training process, an MT system is constrained to make more accurate choices of both lexical and reordering rules.", "labels": [], "entities": [{"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9816762208938599}]}, {"text": "As a result, MT systems tuned against semantic frame based MT evaluation metrics produce output that is more adequate.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.979724645614624}, {"text": "MT evaluation metrics", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.8539011279741923}]}, {"text": "Tuning a machine translation system against a semantic frame based objective function is independent of the translation model paradigm, so, any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7331604659557343}]}], "introductionContent": [{"text": "We present the first ever results of tuning a statistical machine translation (SMT) system against a semantic frame based objective function in order to produce a more adequate output.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.7713232139746348}]}, {"text": "We compare the performance of our system with that of two baseline SMT systems tuned against BLEU and TER, the commonly used n-gram and edit distance based metrics.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9892353415489197}, {"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9981575608253479}, {"text": "TER", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9907295107841492}]}, {"text": "Our system performs better than the baseline across seven commonly used evaluation metrics and subjective human evaluation on adequacy.", "labels": [], "entities": []}, {"text": "Surprisingly, tuning against a semantic MT evaluation metric also significantly outperforms the baseline on the domain of informal web forum data wherein automatic semantic parsing might be expected to fare worse.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.8188490569591522}, {"text": "semantic parsing", "start_pos": 164, "end_pos": 180, "type": "TASK", "confidence": 0.7455235123634338}]}, {"text": "These results strongly indicate that using a semantic frame based objective function for tuning would drive development of MT towards direction of higher utility.", "labels": [], "entities": [{"text": "MT", "start_pos": 123, "end_pos": 125, "type": "TASK", "confidence": 0.9917801022529602}]}, {"text": "Glaring errors caused by semantic role confusion that plague the state-of-the-art MT systems area consequence of using fast and cheap lexical n-gram based objective functions like BLEU to drive their development.", "labels": [], "entities": [{"text": "Glaring", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.912158191204071}, {"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9780417084693909}, {"text": "BLEU", "start_pos": 180, "end_pos": 184, "type": "METRIC", "confidence": 0.9525898098945618}]}, {"text": "Despite enforcing fluency it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely;).", "labels": [], "entities": []}, {"text": "We argue that instead of BLEU, a metric that focuses on getting the meaning right should be used as an objective function for tuning SMT so as to drive continuing progress towards higher utility.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9985387325286865}, {"text": "SMT", "start_pos": 133, "end_pos": 136, "type": "TASK", "confidence": 0.9723129868507385}]}, {"text": "MEANT, is an automatic semantic MT evaluation metric that measures similarity between the MT output and the reference translation via semantic frames.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9020390510559082}, {"text": "MT evaluation", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.8679554462432861}]}, {"text": "It correlates better with human adequacy judgment than other automatic MT evaluation metrics.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.930727481842041}]}, {"text": "Since a high MEANT score is contingent on correct lexical choices as well as syntactic and semantic structures, we believe that tuning against MEANT would improve both translation adequacy and fluency.", "labels": [], "entities": [{"text": "MEANT score", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9650438129901886}, {"text": "MEANT", "start_pos": 143, "end_pos": 148, "type": "METRIC", "confidence": 0.6386398673057556}]}, {"text": "Incorporating semantic structures into SMT by tuning against a semantic frame based evaluation metric is independent of the MT paradigm.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9807235598564148}, {"text": "MT paradigm", "start_pos": 124, "end_pos": 135, "type": "TASK", "confidence": 0.8236659467220306}]}, {"text": "Therefore, systems from different MT paradigms (such as hierarchical, phrase based, transduction grammar based) can benefit from the semantic information incorporated through our approach.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9700543284416199}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Translation quality of MT system tuned against MEANT, BLEU and TER on newswire data", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9481423497200012}, {"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9758821725845337}, {"text": "MEANT", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9943326711654663}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9988597631454468}, {"text": "TER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9971703886985779}]}, {"text": " Table 2: Translation quality of MT system tuned against MEANT, BLEU and TER on forum data", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9492345452308655}, {"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.970180332660675}, {"text": "MEANT", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.986242413520813}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9985463619232178}, {"text": "TER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9949483871459961}]}, {"text": " Table 3: MEANT scores of each system in the 150- sentence manual evaluation set.", "labels": [], "entities": [{"text": "MEANT", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9988105297088623}]}, {"text": " Table 4: No. of sentences ranked the most ade- quate by human evaluators for each system.", "labels": [], "entities": []}, {"text": " Table 5: Significance level of accepting the alter- native hypothesis.", "labels": [], "entities": []}]}