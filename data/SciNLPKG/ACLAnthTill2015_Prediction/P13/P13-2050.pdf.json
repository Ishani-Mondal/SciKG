{"title": [{"text": "Building Comparable Corpora Based on Bilingual LDA Model", "labels": [], "entities": []}], "abstractContent": [{"text": "Comparable corpora are important basic resources in cross-language information processing.", "labels": [], "entities": [{"text": "cross-language information processing", "start_pos": 52, "end_pos": 89, "type": "TASK", "confidence": 0.7192060351371765}]}, {"text": "However, the existing methods of building comparable corpora, which use inter-translate words and relative features, cannot evaluate the topical relation between document pairs.", "labels": [], "entities": []}, {"text": "This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages.", "labels": [], "entities": []}, {"text": "Experiments show that the novel method can obtain similar documents with consistent topics own better adaptability and stability performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "Comparable corpora can be mined fine-grained translation equivalents, such as bilingual terminologies, named entities and parallel sentences, to support the bilingual lexicography, statistical machine translation and cross-language information retrieval).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 181, "end_pos": 212, "type": "TASK", "confidence": 0.7007464071114858}, {"text": "cross-language information retrieval", "start_pos": 217, "end_pos": 253, "type": "TASK", "confidence": 0.7051564455032349}]}, {"text": "Comparable corpora are defined as pairs of monolingual corpora selected according to the criteria of content similarity but non-direct translation in different languages, which reduces limitation of matching source language and target language documents.", "labels": [], "entities": []}, {"text": "Thus comparable corpora have the advantage over parallel corpora in which they are more up-to-date, abundant and accessible.", "labels": [], "entities": []}, {"text": "Many works, which focused on the exploitation of building comparable corpora, were proposed in the past years.", "labels": [], "entities": []}, {"text": "acquired comparable corpora based on the truth that terms are inter-translation in different languages if they have similar frequency correlation at the same time periods.", "labels": [], "entities": []}, {"text": "extracted appropriate keywords from the source language documents and translated them into the target language, which were regarded as the query words to retrieve similar target documents.", "labels": [], "entities": []}, {"text": "analyzed document similarity based on the publication dates, linguistic independent units, bilingual dictionaries and word frequency distributions.", "labels": [], "entities": []}, {"text": "took advantage of the translation equivalents inserted in Wikipedia by means of interlanguage links to extract similar articles.", "labels": [], "entities": []}, {"text": "proposed a comparability measure based on the expectation of finding the translation for each word.", "labels": [], "entities": []}, {"text": "The above studies rely on the high coverage of the original bilingual knowledge and a specific data source together with the translation vocabularies, co-occurrence information and language links.", "labels": [], "entities": []}, {"text": "However, the severest problem is that they cannot understand semantic information.", "labels": [], "entities": []}, {"text": "The new studies seek to match similar documents on topic level to solve the traditional problems.", "labels": [], "entities": []}, {"text": "Preiss (2012) transformed the source language topical model to the target language and classified probability distribution of topics in the same language, whose shortcoming is that the effect of model translation seriously hampers the comparable corpora quality.", "labels": [], "entities": []}, {"text": "adapted monolingual topic model to bilingual topic model in which the documents of a concept unit in different languages were assumed to share identical topic distribution.", "labels": [], "entities": []}, {"text": "Bilingual topic model is widely adopted to mine translation equivalents from multi-language documents ().", "labels": [], "entities": []}, {"text": "Based on the bilingual topic model, this paper predicts the topical structure of documents in different languages and calculates the similarity of topics over documents to build comparable corpora.", "labels": [], "entities": []}, {"text": "The paper concretely includes: 1) Introduce the Bilingual LDA (Latent Dirichlet Allocation) model which builds comparable corpora and improves the efficiency of matching similar documents; 2) Design a novel method of TFIDF (Topic Frequency-Inverse Document Frequency) to enhance the distinguishing ability of topics from different documents; 3) Propose a tailored method of conditional probability to calculate document similarity; 4) Address a languageindependent study which isn't limited to a particular data source in any language.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments are conducted on two sets of Chinese-English comparable corpora.", "labels": [], "entities": []}, {"text": "The first dataset is news corpora with 3254 comparable document pairs, from which 200 pairs are randomly selected as the test dataset News-Test and the remainder is the training dataset News-Train.", "labels": [], "entities": [{"text": "News-Test", "start_pos": 134, "end_pos": 143, "type": "DATASET", "confidence": 0.7093990445137024}, {"text": "training dataset News-Train", "start_pos": 169, "end_pos": 196, "type": "DATASET", "confidence": 0.6599607865015665}]}, {"text": "The second dataset contains 8317 bilingual Wikipedia entry pairs, from which 200 pairs are randomly selected as the test dataset Wiki-Test and the remainder is the training dataset Wiki-Train.", "labels": [], "entities": [{"text": "Wiki-Test", "start_pos": 129, "end_pos": 138, "type": "DATASET", "confidence": 0.7565259337425232}, {"text": "Wiki-Train", "start_pos": 181, "end_pos": 191, "type": "DATASET", "confidence": 0.5018312931060791}]}, {"text": "Then News-Train and Wiki-Train are merged into the training dataset NW-Train.", "labels": [], "entities": [{"text": "News-Train", "start_pos": 5, "end_pos": 15, "type": "DATASET", "confidence": 0.932292640209198}, {"text": "training dataset NW-Train", "start_pos": 51, "end_pos": 76, "type": "DATASET", "confidence": 0.6876171429951986}]}, {"text": "And the hand-labeled gold standard namely NW-Test is composed of News-Test and Wiki-Test.", "labels": [], "entities": [{"text": "NW-Test", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.7741446495056152}, {"text": "News-Test", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.9322648644447327}]}, {"text": "used five levels of relevance to assess the alignments as follows: Same Story, Related Story, Shared Aspect, Common Terminology and Unrelated.", "labels": [], "entities": []}, {"text": "The paper selects the documents with Same Story and Related Story as comparable corpora.", "labels": [], "entities": [{"text": "Same Story", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.8096449673175812}]}, {"text": "Let C p be the comparable corpora in the building result and Cl be the comparable corpora in the labeled result.", "labels": [], "entities": []}, {"text": "The Precision (P), Recall (R) and F-measure (F) are defined as:", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9496688842773438}, {"text": "Recall (R)", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9544103294610977}, {"text": "F-measure (F)", "start_pos": 34, "end_pos": 47, "type": "METRIC", "confidence": 0.9599346071481705}]}], "tableCaptions": [{"text": " Table 1: Sensitivity of Data Source", "labels": [], "entities": []}, {"text": " Table 2: Existing Methods Comparison", "labels": [], "entities": [{"text": "Comparison", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.36392080783843994}]}]}