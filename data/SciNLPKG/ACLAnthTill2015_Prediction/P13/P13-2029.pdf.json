{"title": [{"text": "Automatic Coupling of Answer Extraction and Information Retrieval", "labels": [], "entities": [{"text": "Answer Extraction", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.746002584695816}, {"text": "Information Retrieval", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.7158728539943695}]}], "abstractContent": [{"text": "Information Retrieval (IR) and Answer Extraction are often designed as isolated or loosely connected components in Question Answering (QA), with repeated over-engineering on IR, and not necessarily performance gain for QA.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8422517478466034}, {"text": "Answer Extraction", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8973260819911957}, {"text": "Question Answering (QA)", "start_pos": 115, "end_pos": 138, "type": "TASK", "confidence": 0.837871515750885}]}, {"text": "We propose to tightly integrate them by coupling automatically learned features for answer extraction to a shallow-structured IR model.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.9086533486843109}]}, {"text": "Our method is very quick to implement, and significantly improves IR for QA (measured in Mean Average Precision and Mean Reciprocal Rank) by 10%-20% against an uncoupled retrieval baseline in both document and passage retrieval, which further leads to a downstream 20% improvement in QA F 1 .", "labels": [], "entities": [{"text": "IR", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.8409239053726196}, {"text": "Mean Average Precision", "start_pos": 89, "end_pos": 111, "type": "METRIC", "confidence": 0.9550938407580057}, {"text": "Mean Reciprocal Rank)", "start_pos": 116, "end_pos": 137, "type": "METRIC", "confidence": 0.905378594994545}, {"text": "passage retrieval", "start_pos": 210, "end_pos": 227, "type": "TASK", "confidence": 0.7050023376941681}, {"text": "QA F 1", "start_pos": 284, "end_pos": 290, "type": "METRIC", "confidence": 0.7894269029299418}]}], "introductionContent": [{"text": "The overall performance of a Question Answering system is bounded by its Information Retrieval (IR) front end, resulting in research specifically on Information Retrieval for Question Answering (IR4QA).", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7997133433818817}, {"text": "Information Retrieval for Question Answering (IR4QA)", "start_pos": 149, "end_pos": 201, "type": "TASK", "confidence": 0.7115276865661144}]}, {"text": "Common approaches such as query expansion, structured retrieval, and translation models show patterns of complicated engineering on the IR side, or isolate the upstream passage retrieval from downstream answer extraction.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7925908267498016}, {"text": "translation", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.972512423992157}, {"text": "answer extraction", "start_pos": 203, "end_pos": 220, "type": "TASK", "confidence": 0.7418584227561951}]}, {"text": "We argue that: 1.", "labels": [], "entities": []}, {"text": "an IR front end should deliver exactly what a QA 1 back end needs; 2.", "labels": [], "entities": []}, {"text": "many intuitions employed by QA should be and can be re-used in IR, rather than re-invented.", "labels": [], "entities": [{"text": "IR", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.969373345375061}]}, {"text": "We propose a coupled retrieval method with prior knowledge of its downstream QA component, that feeds QA with exactly the information needed.", "labels": [], "entities": []}, {"text": "As a motivating example, using the question When was Alaska purchased from the TREC 2002 QA track as the query to the Indri search engine, the top sentence retrieved from the accompanying AQUAINT corpus is: Eventually Alaska Airlines will allow all travelers who have purchased electronic tickets through any means.", "labels": [], "entities": [{"text": "Alaska purchased from the TREC 2002 QA track", "start_pos": 53, "end_pos": 97, "type": "DATASET", "confidence": 0.7180294655263424}, {"text": "AQUAINT corpus", "start_pos": 188, "end_pos": 202, "type": "DATASET", "confidence": 0.7313725352287292}]}, {"text": "While this relates Alaska and purchased, it is not a useful passage for the given question.", "labels": [], "entities": []}, {"text": "It is apparent that the question asks fora date.", "labels": [], "entities": []}, {"text": "Prior work proposed predictive annotation (): text is first annotated in a predictive manner (of what types of questions it might answer) with 20 answer types and then indexed.", "labels": [], "entities": []}, {"text": "A question analysis component (consisting of 400 question templates) maps the desired answer type to one of the 20 existing answer types.", "labels": [], "entities": [{"text": "question analysis", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.7902534306049347}]}, {"text": "Retrieval is then performed with both the question and predicated answer types in the query.", "labels": [], "entities": [{"text": "Retrieval", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8493725657463074}]}, {"text": "However, predictive annotation has the limitation of being labor intensive and assuming the underlying NLP pipeline to be accurate.", "labels": [], "entities": [{"text": "predictive annotation", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.9292629659175873}]}, {"text": "We avoid these limitations by directly asking the downstream QA system for the information about which entities answer which questions, via two steps: 1.", "labels": [], "entities": []}, {"text": "reusing the question analysis components from QA; 2.", "labels": [], "entities": []}, {"text": "forming a query based on the most relevant answer features given a question from the learned QA model.", "labels": [], "entities": []}, {"text": "There is no query-time overhead and no manual template creation.", "labels": [], "entities": []}, {"text": "Moreover, this approach is more robust against, e.g., entity recognition errors, because answer typing knowledge is learned from how the data was actually labeled, not from how the data was assumed to be labeled (e.g., manual templates usually assume perfect labeling of named entities, but often it is not the casein practice).", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7866072654724121}, {"text": "answer typing", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.8126142024993896}]}, {"text": "We use our statistically-trained QA system () that recognizes the association between question type and expected answer types through various features.", "labels": [], "entities": []}, {"text": "The QA system employs a linear chain Conditional Random Field (CRF) () and tags each token as either an answer (ANS) or not (O).", "labels": [], "entities": []}, {"text": "This will be our offthe-shelf QA system, which recognizes the association between question type and expected answer types through various features based on e.g., partof-speech tagging (POS) and named entity recognition (NER).", "labels": [], "entities": [{"text": "partof-speech tagging (POS)", "start_pos": 162, "end_pos": 189, "type": "TASK", "confidence": 0.8177893579006195}, {"text": "named entity recognition (NER)", "start_pos": 194, "end_pos": 224, "type": "TASK", "confidence": 0.783931573232015}]}, {"text": "With weights optimized by CRF training (Table 1), we can learn how answer features are correlated with question features.", "labels": [], "entities": []}, {"text": "These features, whose weights are optimized by the CRF training, directly reflect what the most important answer types associated with each question type are.", "labels": [], "entities": []}, {"text": "For instance, line 2 in says that if there is a when question, and the current token's NER label is DATE, then it is likely that this token is tagged as ANS.", "labels": [], "entities": [{"text": "DATE", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9788389801979065}, {"text": "ANS", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.80107182264328}]}, {"text": "IR can easily make use of this knowledge: fora when question, IR retrieves sentences with tokens labeled as DATE by NER, or POS tagged as CD.", "labels": [], "entities": [{"text": "IR retrieves sentences with tokens labeled as DATE", "start_pos": 62, "end_pos": 112, "type": "TASK", "confidence": 0.760288655757904}]}, {"text": "The only extra processing is to pre-tag and index the text with POS and NER labels.", "labels": [], "entities": []}, {"text": "The analyzing power of discriminative answer features for IR comes for free from a trained QA system.", "labels": [], "entities": [{"text": "IR", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9918455481529236}]}, {"text": "Unlike predictive annotation, statistical evidence determines the best answer features given the question, with no manual pattern or templates needed.", "labels": [], "entities": []}, {"text": "To compare again predictive annotation with our approach: predictive annotation works in a forward mode, downstream QA is tailored for upstream IR, i.e., QA works on whatever IR retrieves.", "labels": [], "entities": []}, {"text": "Our method works in reverse (backward): downstream QA dictates upstream IR, i.e., IR retrieves what QA wants.", "labels": [], "entities": []}, {"text": "Moreover, our approach extends easily beyond fixed answer types such as named entities: we are already using POS tags as a demonstration.", "labels": [], "entities": []}, {"text": "We can potentially use any helpful answer features in retrieval.", "labels": [], "entities": []}, {"text": "For instance, if the QA system learns that in order to is highly correlated with why question through lexicalized features, or some certain dependency relations are helpful in answering questions with specific structures, then it is natural and easy for the IR component to incorporate them.", "labels": [], "entities": []}, {"text": "There is also a distinction between our method and the technique of learning to rank applied in  to the label of current token (indexed by) in a CRF.", "labels": [], "entities": []}, {"text": "The larger the weight, the more \"important\" is this feature to help tag the current token with the corresponding label.", "labels": [], "entities": []}, {"text": "For instance, line 1 says when answering a when question, and the POS of current token is CD (cardinal number), it is likely (large weight) that the token is tagged as ANS.", "labels": [], "entities": [{"text": "POS", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9531586170196533}]}, {"text": "Our method is a QA-driven approach that provides supervision for IR from a learned QA model, while learning to rank is essentially an IR-driven approach: the supervision for IR comes from a labeled ranking list of retrieval results.", "labels": [], "entities": [{"text": "IR", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.976348340511322}, {"text": "IR", "start_pos": 174, "end_pos": 176, "type": "TASK", "confidence": 0.9755762219429016}]}, {"text": "Overall, we make the following contributions: \u2022 Our proposed method tightly integrates QA with IR and the reuse of analysis from QA does not put extra overhead on the IR queries.", "labels": [], "entities": []}, {"text": "This QA-driven approach provides a holistic solution to the task of IR4QA.", "labels": [], "entities": [{"text": "IR4QA", "start_pos": 68, "end_pos": 73, "type": "TASK", "confidence": 0.5612369775772095}]}, {"text": "\u2022 We learn statistical evidence about what the form of answers to different questions look like, rather than using manually authored templates.", "labels": [], "entities": []}, {"text": "This provides great flexibility in using answer features in IR queries.", "labels": [], "entities": [{"text": "IR queries", "start_pos": 60, "end_pos": 70, "type": "TASK", "confidence": 0.8999383449554443}]}, {"text": "We give a full spectrum evaluation of all three stages of IR+QA: document retrieval, passage retrieval and answer extraction, to examine thoroughly the effectiveness of the method.", "labels": [], "entities": [{"text": "IR+QA", "start_pos": 58, "end_pos": 63, "type": "TASK", "confidence": 0.8948432405789694}, {"text": "document retrieval", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7540791630744934}, {"text": "passage retrieval", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.878778725862503}, {"text": "answer extraction", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.8787670731544495}]}, {"text": "3 All of our code and datasets are publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "We want to measure and compare the performance of the following retrieval techniques: 1.", "labels": [], "entities": []}, {"text": "uncoupled retrieval with an off-the-shelf IR engine by using the question as query (baseline), 2.", "labels": [], "entities": []}, {"text": "QA-driven coupled retrieval (proposed), and 3.", "labels": [], "entities": [{"text": "QA-driven coupled retrieval", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6005697250366211}]}, {"text": "answer-bearing retrieval by using both the question and known answer as query, only evaluated for answer extraction (upper bound), at the three stages of question answering: 1.", "labels": [], "entities": [{"text": "answer-bearing retrieval", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7927844524383545}, {"text": "answer extraction", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.6909644156694412}, {"text": "question answering", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.740376204252243}]}, {"text": "Document retrieval (for relevant docs from corpus), measured by Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR)..", "labels": [], "entities": [{"text": "Document retrieval", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7889469563961029}, {"text": "Mean Average Precision (MAP)", "start_pos": 64, "end_pos": 92, "type": "METRIC", "confidence": 0.977355440457662}, {"text": "Mean Reciprocal Rank (MRR).", "start_pos": 97, "end_pos": 124, "type": "METRIC", "confidence": 0.9538183410962423}]}, {"text": "Note that only 88 questions out of MIT99 have an answer from the top 10 query results.", "labels": [], "entities": [{"text": "MIT99", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.9716084599494934}]}, {"text": "Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (), dependencyparsed by the Stanford Parser (, and NER-tagged by the Illinois Named Entity Tagger () with an 18-label typeset.", "labels": [], "entities": []}, {"text": "Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 questions are based, was processed in exactly the same manner as was the QA training set.", "labels": [], "entities": [{"text": "AQUAINT (LDC2002T31) corpus", "start_pos": 32, "end_pos": 59, "type": "DATASET", "confidence": 0.72071573138237}, {"text": "MIT99 questions", "start_pos": 74, "end_pos": 89, "type": "DATASET", "confidence": 0.9291000366210938}, {"text": "QA training set", "start_pos": 153, "end_pos": 168, "type": "DATASET", "confidence": 0.80449112256368}]}, {"text": "But only sentence boundaries, POS tags and NER labels were kept as the annotation of the corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Coupled vs. uncoupled document/sentence re-", "labels": [], "entities": []}, {"text": " Table 2. The test sentences were ob- tained with answer-bearing queries. This is as- suming almost perfect IR. The gap between the  top two and other lines signals more room for im- provements for IR in terms of better coverage and  better rank for answer-bearing sentences.", "labels": [], "entities": [{"text": "IR", "start_pos": 198, "end_pos": 200, "type": "TASK", "confidence": 0.9800921678543091}, {"text": "coverage", "start_pos": 220, "end_pos": 228, "type": "METRIC", "confidence": 0.9639227390289307}]}]}