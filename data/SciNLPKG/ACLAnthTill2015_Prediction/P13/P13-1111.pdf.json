{"title": [{"text": "Handling Ambiguities of Bilingual Predicate-Argument Structures for Statistical Machine Translation", "labels": [], "entities": [{"text": "Handling Ambiguities of Bilingual Predicate-Argument Structures", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.5888115614652634}, {"text": "Statistical Machine Translation", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.8408984740575155}]}], "abstractContent": [{"text": "Predicate-argument structure (PAS) has been demonstrated to be very effective in improving SMT performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9977090358734131}]}, {"text": "However, since a source-side PAS might correspond to multiple different target-side PASs, there usually exist many PAS ambiguities during translation.", "labels": [], "entities": []}, {"text": "In this paper , we group PAS ambiguities into two types: role ambiguity and gap ambiguity.", "labels": [], "entities": [{"text": "PAS ambiguities", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.926858127117157}]}, {"text": "Then we propose two novel methods to handle the two PAS ambiguities for SMT accordingly: 1) inside context integration; 2) a novel maximum entropy PAS disambiguation (MEPD) model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9954690933227539}, {"text": "inside context integration", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.5863225956757864}, {"text": "PAS disambiguation", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.774990439414978}]}, {"text": "In this way, we incorporate rich context information of PAS for disambiguation.", "labels": [], "entities": []}, {"text": "Then we integrate the two methods into a PAS-based translation framework.", "labels": [], "entities": [{"text": "PAS-based translation", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7581777274608612}]}, {"text": "Experiments show that our approach helps to achieve significant improvements on translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.9584367275238037}]}], "introductionContent": [{"text": "Predicate-argument structure (PAS) depicts the relationship between a predicate and its associated arguments, which indicates the skeleton structure of a sentence on semantic level.", "labels": [], "entities": [{"text": "Predicate-argument structure (PAS)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8039981067180634}]}, {"text": "Basically, PAS agrees much better between two languages than syntax structure (;).", "labels": [], "entities": [{"text": "PAS", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.7247608304023743}]}, {"text": "Considering that current syntaxbased translation models are always impaired by cross-lingual structure divergence, PAS is really a better representation of a sentence pair to model the bilingual structure mapping.", "labels": [], "entities": [{"text": "syntaxbased translation", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.6565938591957092}, {"text": "cross-lingual structure divergence", "start_pos": 79, "end_pos": 113, "type": "TASK", "confidence": 0.6766710082689921}]}, {"text": "However, since a source-side PAS might correspond to multiple different target-side PASs, there usually exist many PAS ambiguities during translation.", "labels": [], "entities": []}, {"text": "For example, in, (a) and (b) carry the same source-side PAS <[A0] [Pred(\u662f)] [A1] 3 > for Chinese predicate \"\u662f\".", "labels": [], "entities": [{"text": "PAS", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9876208901405334}, {"text": "Pred(\u662f)] [A1] 3", "start_pos": 67, "end_pos": 82, "type": "METRIC", "confidence": 0.7954325874646505}]}, {"text": "An example of ambiguous PASs.", "labels": [], "entities": []}, {"text": "Meanwhile, also depicts another kind of PAS ambiguity.", "labels": [], "entities": [{"text": "PAS ambiguity", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.8225739002227783}]}, {"text": "From, we can see that (a) and (c) get the same source-side PAS and target-side-like PAS.", "labels": [], "entities": []}, {"text": "However, they are different because in(c), there is a gap string \"\u5bf9 \u8fd0\u52a8\u5458\" between [A0] and.", "labels": [], "entities": []}, {"text": "Generally, the gap strings are due to the low recall of automatic semantic role labeling (SRL) or complex sentence structures.", "labels": [], "entities": [{"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9986273050308228}, {"text": "semantic role labeling (SRL)", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.7530098706483841}]}, {"text": "For example, in(c), the gap string \"\u5bf9 \u8fd0\u52a8\u5458\" is actually an argument \"AM-PRP\" of the PAS, but the SRL system has ignored it.", "labels": [], "entities": []}, {"text": "We call this kind of PAS ambiguity gap ambiguity.", "labels": [], "entities": [{"text": "PAS ambiguity gap ambiguity", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.831044539809227}]}, {"text": "During translation, these PAS ambiguities will greatly affect the PAS-based translation models.", "labels": [], "entities": [{"text": "translation", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.9706693291664124}, {"text": "PAS-based translation", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7835380434989929}]}, {"text": "Therefore, in order to incorporate the bilingual PAS into machine translation effectively, we need to decide which target-side-like PAS should be chosen fora specific source-side PAS.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.6841955929994583}]}, {"text": "We call this task PAS disambiguation.", "labels": [], "entities": [{"text": "PAS disambiguation", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7111884653568268}]}, {"text": "In this paper, we propose two novel methods to incorporate rich context information to handle PAS ambiguities.", "labels": [], "entities": [{"text": "PAS ambiguities", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.9167024493217468}]}, {"text": "Towards the gap ambiguity, we adopt a method called inside context integration to extend PAS to IC-PAS.", "labels": [], "entities": [{"text": "inside context integration", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.642414371172587}]}, {"text": "In terms of IC-PAS, the gap strings are combined effectively to deal with the gap ambiguities.", "labels": [], "entities": []}, {"text": "As to the role ambiguity, we design a novel maximum entropy PAS disambiguation (MEPD) model to combine various context features, such as context words of PAS.", "labels": [], "entities": []}, {"text": "For each ambiguous source-side PAS, we build a specific MEPD model to select appropriate target-side-like PAS for translation.", "labels": [], "entities": []}, {"text": "We will detail the two methods in Section 3 and 4 respectively.", "labels": [], "entities": []}, {"text": "Finally, we integrate the above two methods into a PAS-based translation framework ().", "labels": [], "entities": [{"text": "PAS-based translation", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.7401522099971771}]}, {"text": "Experiments show that the two PAS disambiguation methods significantly improve the baseline translation system.", "labels": [], "entities": [{"text": "PAS disambiguation", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8256746530532837}]}, {"text": "The main contribution of this work can be concluded as follows: 1) We define two kinds of PAS ambiguities: role ambiguity and gap ambiguity.", "labels": [], "entities": []}, {"text": "To our best knowledge, we are the first to handle these PAS ambiguities for SMT.", "labels": [], "entities": [{"text": "PAS ambiguities", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7692837417125702}, {"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9901596903800964}]}, {"text": "2) Towards the two different ambiguities, we design two specific methods for PAS disambiguation: inside context integration and the novel MEPD model.", "labels": [], "entities": [{"text": "PAS disambiguation", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.954357773065567}, {"text": "inside context integration", "start_pos": 97, "end_pos": 123, "type": "TASK", "confidence": 0.6384190917015076}]}], "datasetContent": [{"text": "We perform Chinese-to-English translation to demonstrate the effectiveness of our PAS disambiguation method.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.5978057533502579}, {"text": "PAS disambiguation", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.8530686497688293}]}, {"text": "The training data contains about 260K sentence pairs . To get accurate SRL results, we ensure that the length of each sentence in the training data is among 10 and 30 words.", "labels": [], "entities": [{"text": "SRL", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9335848093032837}]}, {"text": "We run GIZA++ and then employ the grow-diag-final-and (gdfa) strategy to produce symmetric word alignments.", "labels": [], "entities": []}, {"text": "The development set and test set come from the NIST evaluation test data).", "labels": [], "entities": [{"text": "NIST evaluation test data", "start_pos": 47, "end_pos": 72, "type": "DATASET", "confidence": 0.9594953060150146}]}, {"text": "Similar to the training set, we also only retain the sentences whose lengths are among 10 and 30 words.", "labels": [], "entities": []}, {"text": "Finally, the development set includes 595 sentences from NIST MT03 and the test set contains 1,786 sentences from NIST MT04 and MT05.", "labels": [], "entities": [{"text": "NIST MT03", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.8186131417751312}, {"text": "NIST MT04", "start_pos": 114, "end_pos": 123, "type": "DATASET", "confidence": 0.8835302293300629}, {"text": "MT05", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.8821899890899658}]}, {"text": "We train a 5-gram language model with the Xinhua portion of English Gigaword corpus and target part of the training data.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 60, "end_pos": 83, "type": "DATASET", "confidence": 0.8464144666989645}]}, {"text": "The translation quality is evaluated by case-insensitive BLEU-4 with shortest length penalty.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9527136087417603}, {"text": "BLEU-4", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9865171909332275}]}, {"text": "The statistical significance testis performed by the re-sampling approach.", "labels": [], "entities": []}, {"text": "We perform SRL on the source part of the training set, development set and test set by the Chinese SRL system used in.", "labels": [], "entities": [{"text": "SRL", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.984772264957428}]}, {"text": "To relieve the negative effect of SRL errors, we get the multiple SRL results by providing the SRL system with 3-best parse trees of Berkeley parser, 1-best parse tree of Bikel parser and Stanford parser.", "labels": [], "entities": [{"text": "SRL", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9700791239738464}]}, {"text": "Therefore, at last, we can get 5 SRL result for each sentence.", "labels": [], "entities": [{"text": "SRL", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9854889512062073}]}, {"text": "For the training set, we use these SRL results to do rule extraction respectively.", "labels": [], "entities": [{"text": "SRL", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.7428855299949646}, {"text": "rule extraction", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.8337284922599792}]}, {"text": "We combine the obtained rules together to get a combined rule set.", "labels": [], "entities": []}, {"text": "We discard the rules with fewer than 5 appearances.", "labels": [], "entities": []}, {"text": "Using this set, we can train our MEPD model directly.", "labels": [], "entities": [{"text": "MEPD", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.6445507407188416}]}, {"text": "As to translation, we match the 5 SRL results with transformation rules respectively, and then apply the resulting target-side-like PASs for decoding.", "labels": [], "entities": [{"text": "translation", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.9842490553855896}]}, {"text": "As we mentioned in section 2.3, we use the state-of-the-art BTG system to translate the non-PAS spans.", "labels": [], "entities": []}, {"text": "source-side PAS counts number of classes.", "labels": [], "entities": [{"text": "PAS", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.7363259196281433}]}, {"text": "The top 10 frequent source-side PASs in the dev and test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2.  As we can see, after employing PAS for transla- tion, all systems outperform the baseline BTG  system significantly. This comparison verifies  the conclusion of (", "labels": [], "entities": [{"text": "PAS", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.8276048898696899}]}, {"text": " Table 2. Result of baseline system and the MT sys- tems using our PAS-based disambiguation method.  The \"*\" and \"#\" denote that the result is significantly  better than BTG and PASTR respectively (p<0.01).", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.7219337224960327}, {"text": "BTG", "start_pos": 170, "end_pos": 173, "type": "DATASET", "confidence": 0.533375084400177}, {"text": "PASTR", "start_pos": 178, "end_pos": 183, "type": "METRIC", "confidence": 0.7421669960021973}]}, {"text": " Table 3. Statistics on the matching PAS.", "labels": [], "entities": [{"text": "PAS", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.34881243109703064}]}]}